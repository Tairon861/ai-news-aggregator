<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 11 Nov 2025 01:46:40 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>Baseten takes on hyperscalers with new AI training platform that lets you own your model weights (AI | VentureBeat)</title><link>https://venturebeat.com/ai/baseten-takes-on-hyperscalers-with-new-ai-training-platform-that-lets-you</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.baseten.co/"&gt;&lt;u&gt;Baseten&lt;/u&gt;&lt;/a&gt;, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.&lt;/p&gt;&lt;p&gt;The San Francisco-based company announced Thursday the general availability of &lt;a href="https://www.baseten.co/products/training/"&gt;&lt;u&gt;Baseten Training&lt;/u&gt;&lt;/a&gt;, an infrastructure platform designed to help companies fine-tune open-source AI models without the operational headaches of managing GPU clusters, multi-node orchestration, or cloud capacity planning. The move is a calculated expansion beyond Baseten&amp;#x27;s core inference business, driven by what CTO Amir Haghighat describes as relentless customer demand and a strategic imperative to capture the full lifecycle of AI deployment.&lt;/p&gt;&lt;p&gt;&amp;quot;We had a captive audience of customers who kept coming to us saying, &amp;#x27;Hey, I hate this problem,&amp;#x27;&amp;quot; Haghighat said in an interview. &amp;quot;One of them told me, &amp;#x27;Look, I bought a bunch of H100s from a cloud provider. I have to SSH in on Friday, run my fine-tuning job, then check on Monday to see if it worked. Sometimes I realize it just hasn&amp;#x27;t been working all along.&amp;#x27;&amp;quot;&lt;/p&gt;&lt;p&gt;The launch comes at a critical inflection point in enterprise AI adoption. As open-source models from &lt;a href="https://huggingface.co/meta-llama"&gt;&lt;u&gt;Meta&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://huggingface.co/Alibaba-NLP"&gt;&lt;u&gt;Alibaba&lt;/u&gt;&lt;/a&gt;, and others increasingly rival proprietary systems in performance, companies face mounting pressure to reduce their reliance on expensive API calls to services like OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; or Anthropic&amp;#x27;s &lt;a href="https://claude.ai/"&gt;&lt;u&gt;Claude&lt;/u&gt;&lt;/a&gt;. But the path from off-the-shelf open-source model to production-ready custom AI remains treacherous, requiring specialized expertise in machine learning operations, infrastructure management, and performance optimization.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s answer: provide the infrastructure rails while letting companies retain full control over their training code, data, and model weights. It&amp;#x27;s a deliberately low-level approach born from hard-won lessons.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How a failed product taught Baseten what AI training infrastructure really needs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;This isn&amp;#x27;t Baseten&amp;#x27;s first foray into training. The company&amp;#x27;s previous attempt, a product called Blueprints launched roughly two and a half years ago, failed spectacularly — a failure Haghighat now embraces as instructive.&lt;/p&gt;&lt;p&gt;&amp;quot;We had created the abstraction layer a little too high,&amp;quot; he explained. &amp;quot;We were trying to create a magical experience, where as a user, you come in and programmatically choose a base model, choose your data and some hyperparameters, and magically out comes a model.&amp;quot;&lt;/p&gt;&lt;p&gt;The problem? Users didn&amp;#x27;t have the intuition to make the right choices about base models, data quality, or hyperparameters. When their models underperformed, they blamed the product. Baseten found itself in the consulting business rather than the infrastructure business, helping customers debug everything from dataset deduplication to model selection.&lt;/p&gt;&lt;p&gt;&amp;quot;We became consultants,&amp;quot; Haghighat said. &amp;quot;And that&amp;#x27;s not what we had set out to do.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.baseten.co/"&gt;&lt;u&gt;Baseten&lt;/u&gt;&lt;/a&gt; killed Blueprints and refocused entirely on inference, vowing to &amp;quot;earn the right&amp;quot; to expand again. That moment arrived earlier this year, driven by two market realities: the vast majority of Baseten&amp;#x27;s inference revenue comes from custom models that customers train elsewhere, and competing training platforms were using restrictive terms of service to lock customers into their inference products.&lt;/p&gt;&lt;p&gt;&amp;quot;Multiple companies who were building fine-tuning products had in their terms of service that you as a customer cannot take the weights of the fine-tuned model with you somewhere else,&amp;quot; Haghighat said. &amp;quot;I understand why from their perspective — I still don&amp;#x27;t think there is a big company to be made purely on just training or fine-tuning. The sticky part is in inference, the valuable part where value is unlocked is in inference, and ultimately the revenue is in inference.&amp;quot;&lt;/p&gt;&lt;p&gt;Baseten took the opposite approach: customers own their weights and can download them at will. The bet is that superior inference performance will keep them on the platform anyway.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Multi-cloud GPU orchestration and sub-minute scheduling set Baseten apart from hyperscalers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The new &lt;a href="https://www.baseten.co/products/training/"&gt;&lt;u&gt;Baseten Training&lt;/u&gt;&lt;/a&gt; product operates at what Haghighat calls &amp;quot;the infrastructure layer&amp;quot; — lower-level than the failed Blueprints experiment, but with opinionated tooling around reliability, observability, and integration with Baseten&amp;#x27;s inference stack.&lt;/p&gt;&lt;p&gt;Key technical capabilities include multi-node training support across clusters of &lt;a href="https://www.nvidia.com/en-us/data-center/h100/"&gt;&lt;u&gt;NVIDIA H100&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://www.baseten.co/blog/accelerating-inference-nvidia-b200-gpus/?utm_term=b200%20gpu&amp;amp;utm_campaign=Search+-Hosting+Inference&amp;amp;utm_source=adwords&amp;amp;utm_medium=ppc&amp;amp;hsa_acc=9990356727&amp;amp;hsa_cam=21607833837&amp;amp;hsa_grp=179204207220&amp;amp;hsa_ad=747940377782&amp;amp;hsa_src=g&amp;amp;hsa_tgt=kwd-2402895176571&amp;amp;hsa_kw=b200%20gpu&amp;amp;hsa_mt=e&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;gad_source=1&amp;amp;gad_campaignid=21607833837&amp;amp;gbraid=0AAAAAqCKh1tBBfE-A9FM_fms6m5z6IuPk&amp;amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6S_fxqaV-EfbGshbTKib8rt2sQl81yPs6M0y40aoYf1Zy5mV_ECflBoCjPsQAvD_BwE"&gt;&lt;u&gt;B200 GPUs&lt;/u&gt;&lt;/a&gt;, automated checkpointing to protect against node failures, sub-minute job scheduling, and integration with Baseten&amp;#x27;s proprietary &lt;a href="https://www.baseten.co/blog/how-we-built-multi-cloud-capacity-management/"&gt;&lt;u&gt;Multi-Cloud Management (MCM) &lt;/u&gt;&lt;/a&gt;system. That last piece is critical: MCM allows Baseten to dynamically provision GPU capacity across multiple cloud providers and regions, passing cost savings to customers while avoiding the capacity constraints and multi-year contracts typical of hyperscaler deals.&lt;/p&gt;&lt;p&gt;&amp;quot;With hyperscalers, you don&amp;#x27;t get to say, &amp;#x27;Hey, give me three or four B200 nodes while my job is running, and then take it back from me and don&amp;#x27;t charge me for it,&amp;#x27;&amp;quot; Haghighat said. &amp;quot;They say, &amp;#x27;No, you need to sign a three-year contract.&amp;#x27; We don&amp;#x27;t do that.&amp;quot;&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s approach mirrors broader trends in cloud infrastructure, where abstraction layers increasingly allow workloads to move fluidly across providers. When AWS experienced a major outage several weeks ago, Baseten&amp;#x27;s inference services remained operational by automatically routing traffic to other cloud providers — a capability now extended to training workloads.&lt;/p&gt;&lt;p&gt;The technical differentiation extends to Baseten&amp;#x27;s observability tooling, which provides per-GPU metrics for multi-node jobs, granular checkpoint tracking, and a refreshed UI that surfaces infrastructure-level events. The company also introduced an &amp;quot;&lt;a href="https://github.com/basetenlabs/ml-cookbook"&gt;&lt;u&gt;ML Cookbook&lt;/u&gt;&lt;/a&gt;&amp;quot; of open-source training recipes for popular models like Gemma, GPT OSS, and Qwen, designed to help users reach &amp;quot;training success&amp;quot; faster.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Early adopters report 84% cost savings and 50% latency improvements with custom models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Two early customers illustrate the market Baseten is targeting: AI-native companies building specialized vertical solutions that require custom models.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.oxen.ai/"&gt;&lt;u&gt;Oxen AI&lt;/u&gt;&lt;/a&gt;, a platform focused on dataset management and model fine-tuning, exemplifies the partnership model Baseten envisions. CEO Greg Schoeninger articulated a common strategic calculus, telling VentureBeat: &amp;quot;Whenever I&amp;#x27;ve seen a platform try to do both hardware and software, they usually fail at one of them. That&amp;#x27;s why partnering with Baseten to handle infrastructure was the obvious choice.&amp;quot;&lt;/p&gt;&lt;p&gt;Oxen built its customer experience entirely on top of Baseten&amp;#x27;s infrastructure, using the &lt;a href="https://www.baseten.co/resources/changelog/authenticate-from-cli-with-truss-login/"&gt;&lt;u&gt;Baseten CLI&lt;/u&gt;&lt;/a&gt; to programmatically orchestrate training jobs. The system automatically provisions and deprovisions GPUs, fully concealing Baseten&amp;#x27;s interface behind Oxen&amp;#x27;s own. For one Oxen customer, &lt;a href="https://alliumai.com/"&gt;&lt;u&gt;AlliumAI&lt;/u&gt;&lt;/a&gt; — a startup bringing structure to messy retail data — the integration delivered 84% cost savings compared to previous approaches, reducing total inference costs from $46,800 to $7,530.&lt;/p&gt;&lt;p&gt;&amp;quot;Training custom LoRAs has always been one of the most effective ways to leverage open-source models, but it often came with infrastructure headaches,&amp;quot; said Daniel Demillard, CEO of AlliumAI. &amp;quot;With Oxen and Baseten, that complexity disappears. We can train and deploy models at massive scale without ever worrying about CUDA, which GPU to choose, or shutting down servers after training.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://parsed.com/"&gt;&lt;u&gt;Parsed&lt;/u&gt;&lt;/a&gt;, another early customer, tackles a different pain point: helping enterprises reduce dependence on OpenAI by creating specialized models that outperform generalist LLMs on domain-specific tasks. The company works in mission-critical sectors like healthcare, finance, and legal services, where model performance and reliability aren&amp;#x27;t negotiable.&lt;/p&gt;&lt;p&gt;&amp;quot;Prior to switching to Baseten, we were seeing repetitive and degraded performance on our fine-tuned models due to bugs with our previous training provider,&amp;quot; said Charles O&amp;#x27;Neill, Parsed&amp;#x27;s co-founder and chief science officer. &amp;quot;On top of that, we were struggling to easily download and checkpoint weights after training runs.&amp;quot;&lt;/p&gt;&lt;p&gt;With Baseten, Parsed achieved 50% lower end-to-end latency for transcription use cases, spun up HIPAA-compliant EU deployments for testing within 48 hours, and kicked off more than 500 training jobs. The company also leveraged Baseten&amp;#x27;s modified &lt;a href="https://docs.baseten.co/examples/vllm"&gt;&lt;u&gt;vLLM inference framework&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/"&gt;&lt;u&gt;speculative decoding&lt;/u&gt;&lt;/a&gt; — a technique that generates draft tokens to accelerate language model output — to cut latency in half for custom models.&lt;/p&gt;&lt;p&gt;&amp;quot;Fast models matter,&amp;quot; O&amp;#x27;Neill said. &amp;quot;But fast models that get better over time matter more. A model that&amp;#x27;s 2x faster but static loses to one that&amp;#x27;s slightly slower but improving 10% monthly. Baseten gives us both — the performance edge today and the infrastructure for continuous improvement.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why training and inference are more interconnected than the industry realizes&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Parsed example illuminates a deeper strategic rationale for Baseten&amp;#x27;s training expansion: the boundary between training and inference is blurrier than conventional wisdom suggests.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s model performance team uses the training platform extensively to create &amp;quot;draft models&amp;quot; for speculative decoding, a cutting-edge technique that can dramatically accelerate inference. The company recently announced it achieved 650+ tokens per second on OpenAI&amp;#x27;s &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;&lt;u&gt;GPT OSS 120B model&lt;/u&gt;&lt;/a&gt; — a 60% improvement over its launch performance — using &lt;a href="https://arxiv.org/abs/2503.01840"&gt;&lt;u&gt;EAGLE-3&lt;/u&gt;&lt;/a&gt; speculative decoding, which requires training specialized small models to work alongside larger target models.&lt;/p&gt;&lt;p&gt;&amp;quot;Ultimately, inference and training plug in more ways than one might think,&amp;quot; Haghighat said. &amp;quot;When you do speculative decoding in inference, you need to train the draft model. Our model performance team is a big customer of the training product to train these EAGLE heads on a continuous basis.&amp;quot;&lt;/p&gt;&lt;p&gt;This technical interdependence reinforces Baseten&amp;#x27;s thesis that owning both training and inference creates defensible value. The company can optimize the entire lifecycle: a model trained on Baseten can be deployed with a single click to inference endpoints pre-optimized for that architecture, with deployment-from-checkpoint support for chat completion and audio transcription workloads.&lt;/p&gt;&lt;p&gt;The approach contrasts sharply with vertically integrated competitors like &lt;a href="https://replicate.com/"&gt;&lt;u&gt;Replicate&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://modal.com/"&gt;&lt;u&gt;Modal&lt;/u&gt;&lt;/a&gt;, which also offer training and inference but with different architectural tradeoffs. Baseten&amp;#x27;s bet is on lower-level infrastructure flexibility and performance optimization, particularly for companies running custom models at scale.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;As open-source AI models improve, enterprises see fine-tuning as the path away from OpenAI dependency&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Underpinning Baseten&amp;#x27;s entire strategy is a conviction about the trajectory of open-source AI models — namely, that they&amp;#x27;re getting good enough, fast enough, to unlock massive enterprise adoption through fine-tuning.&lt;/p&gt;&lt;p&gt;&amp;quot;Both closed and open-source models are getting better and better in terms of quality,&amp;quot; Haghighat said. &amp;quot;We don&amp;#x27;t even need open source to surpass closed models, because as both of them are getting better, they unlock all these invisible lines of usefulness for different use cases.&amp;quot;&lt;/p&gt;&lt;p&gt;He pointed to the proliferation of reinforcement learning and supervised fine-tuning techniques that allow companies to take an open-source model and make it &amp;quot;as good as the closed model, not at everything, but at this narrow band of capability that they want.&amp;quot;&lt;/p&gt;&lt;p&gt;That trend is already visible in Baseten&amp;#x27;s &lt;a href="https://www.baseten.co/products/model-apis/"&gt;&lt;u&gt;Model APIs business&lt;/u&gt;&lt;/a&gt;, launched alongside Training earlier this year to provide production-grade access to open-source models. The company was the first provider to offer access to &lt;a href="https://api-docs.deepseek.com/news/news1226"&gt;&lt;u&gt;DeepSeek V3&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://api-docs.deepseek.com/news/news250120"&gt;&lt;u&gt;R1&lt;/u&gt;&lt;/a&gt;, and has since added models like &lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;&lt;u&gt;Llama 4&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://qwen.ai/research"&gt;&lt;u&gt;Qwen 3&lt;/u&gt;&lt;/a&gt;, optimized for performance and reliability. Model APIs serves as a top-of-funnel product: companies start with off-the-shelf open-source models, realize they need customization, move to Training for fine-tuning, and ultimately deploy on Baseten&amp;#x27;s &lt;a href="https://www.baseten.co/products/dedicated-deployments/"&gt;&lt;u&gt;Dedicated Deployments&lt;/u&gt;&lt;/a&gt; infrastructure.&lt;/p&gt;&lt;p&gt;Yet Haghighat acknowledged the market remains &amp;quot;fuzzy&amp;quot; around which training techniques will dominate. Baseten is hedging by staying close to the bleeding edge through its &lt;a href="https://www.baseten.co/blog/forward-deployed-engineering/"&gt;&lt;u&gt;Forward Deployed Engineering team&lt;/u&gt;&lt;/a&gt;, which works hands-on with select customers on reinforcement learning, supervised fine-tuning, and other advanced techniques.&lt;/p&gt;&lt;p&gt;&amp;quot;As we do that, we will see patterns emerge about what a productized training product can look like that really addresses the user&amp;#x27;s needs without them having to learn too much about how RL works,&amp;quot; he said. &amp;quot;Are we there as an industry? I would say not quite. I see some attempts at that, but they all seem like almost falling to the same trap that Blueprints fell into—a bit of a walled garden that ties the hands of AI folks behind their back.&amp;quot;&lt;/p&gt;&lt;p&gt;The roadmap ahead includes potential abstractions for common training patterns, expansion into image, audio, and video fine-tuning, and deeper integration of advanced techniques like prefill-decode disaggregation, which separates the initial processing of prompts from token generation to improve efficiency.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Baseten faces crowded field but bets developer experience and performance will win enterprise customers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Baseten enters an increasingly crowded market for AI infrastructure. Hyperscalers like &lt;a href="https://aws.amazon.com/"&gt;&lt;u&gt;AWS&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://cloud.google.com/?hl=en"&gt;&lt;u&gt;Google Cloud&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://azure.microsoft.com/en-us/"&gt;&lt;u&gt;Microsoft Azure&lt;/u&gt;&lt;/a&gt; offer GPU compute for training, while specialized providers like Lambda Labs, CoreWeave, and Together AI compete on price, performance, or ease of use. Then there are vertically integrated platforms like &lt;a href="https://huggingface.co/"&gt;&lt;u&gt;Hugging Face&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://replicate.com/"&gt;&lt;u&gt;Replicate&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://modal.com/"&gt;&lt;u&gt;Modal&lt;/u&gt;&lt;/a&gt; that bundle training, inference, and model hosting.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s differentiation rests on three pillars: its MCM system for multi-cloud capacity management, deep performance optimization expertise built from its inference business, and a developer experience tailored for production deployments rather than experimentation.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s recent &lt;a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"&gt;&lt;u&gt;$150 million Series D&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"&gt;&lt;u&gt;$2.15 billion valuation&lt;/u&gt;&lt;/a&gt; provide runway to invest in both products simultaneously. Major customers include &lt;a href="https://www.descript.com/"&gt;&lt;u&gt;Descript&lt;/u&gt;&lt;/a&gt;, which uses Baseten for transcription workloads; &lt;a href="https://decagon.ai/"&gt;&lt;u&gt;Decagon&lt;/u&gt;&lt;/a&gt;, which runs customer service AI; and &lt;a href="https://sourcegraph.com/"&gt;&lt;u&gt;Sourcegraph&lt;/u&gt;&lt;/a&gt;, which powers coding assistants. All three operate in domains where model customization and performance are competitive advantages.&lt;/p&gt;&lt;p&gt;Timing may be Baseten&amp;#x27;s biggest asset. The confluence of improving open-source models, enterprise discomfort with dependence on proprietary AI providers, and growing sophistication around fine-tuning techniques creates what Haghighat sees as a sustainable market shift.&lt;/p&gt;&lt;p&gt;&amp;quot;There is a lot of use cases for which closed models have gotten there and open ones have not,&amp;quot; he said. &amp;quot;Where I&amp;#x27;m seeing in the market is people using different training techniques — more recently, a lot of reinforcement learning and SFT — to be able to get this open model to be as good as the closed model, not at everything, but at this narrow band of capability that they want. That&amp;#x27;s very palpable in the market.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprises navigating the complex transition from closed to open AI models, Baseten&amp;#x27;s positioning offers a clear value proposition: infrastructure that handles the messy middle of fine-tuning while optimizing for the ultimate goal of performant, reliable, cost-effective inference at scale. The company&amp;#x27;s insistence that customers own their model weights — a stark contrast to competitors using training as a lock-in mechanism — reflects confidence that technical excellence, not contractual restrictions, will drive retention.&lt;/p&gt;&lt;p&gt;Whether Baseten can execute on this vision depends on navigating tensions inherent in its strategy: staying at the infrastructure layer without becoming consultants, providing power and flexibility without overwhelming users with complexity, and building abstractions at exactly the right level as the market matures. The company&amp;#x27;s willingness to kill Blueprints when it failed suggests a pragmatism that could prove decisive in a market where many infrastructure providers over-promise and under-deliver.&lt;/p&gt;&lt;p&gt;&amp;quot;Through and through, we&amp;#x27;re an inference company,&amp;quot; Haghighat emphasized. &amp;quot;The reason that we did training is at the service of inference.&amp;quot;&lt;/p&gt;&lt;p&gt;That clarity of purpose — treating training as a means to an end rather than an end in itself—may be Baseten&amp;#x27;s most important strategic asset. As AI deployment matures from experimentation to production, the companies that solve the full stack stand to capture outsized value. But only if they avoid the trap of technology in search of a problem.&lt;/p&gt;&lt;p&gt;At least Baseten&amp;#x27;s customers no longer have to SSH into boxes on Friday and pray their training jobs complete by Monday. In the infrastructure business, sometimes the best innovation is simply making the painful parts disappear.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.baseten.co/"&gt;&lt;u&gt;Baseten&lt;/u&gt;&lt;/a&gt;, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.&lt;/p&gt;&lt;p&gt;The San Francisco-based company announced Thursday the general availability of &lt;a href="https://www.baseten.co/products/training/"&gt;&lt;u&gt;Baseten Training&lt;/u&gt;&lt;/a&gt;, an infrastructure platform designed to help companies fine-tune open-source AI models without the operational headaches of managing GPU clusters, multi-node orchestration, or cloud capacity planning. The move is a calculated expansion beyond Baseten&amp;#x27;s core inference business, driven by what CTO Amir Haghighat describes as relentless customer demand and a strategic imperative to capture the full lifecycle of AI deployment.&lt;/p&gt;&lt;p&gt;&amp;quot;We had a captive audience of customers who kept coming to us saying, &amp;#x27;Hey, I hate this problem,&amp;#x27;&amp;quot; Haghighat said in an interview. &amp;quot;One of them told me, &amp;#x27;Look, I bought a bunch of H100s from a cloud provider. I have to SSH in on Friday, run my fine-tuning job, then check on Monday to see if it worked. Sometimes I realize it just hasn&amp;#x27;t been working all along.&amp;#x27;&amp;quot;&lt;/p&gt;&lt;p&gt;The launch comes at a critical inflection point in enterprise AI adoption. As open-source models from &lt;a href="https://huggingface.co/meta-llama"&gt;&lt;u&gt;Meta&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://huggingface.co/Alibaba-NLP"&gt;&lt;u&gt;Alibaba&lt;/u&gt;&lt;/a&gt;, and others increasingly rival proprietary systems in performance, companies face mounting pressure to reduce their reliance on expensive API calls to services like OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; or Anthropic&amp;#x27;s &lt;a href="https://claude.ai/"&gt;&lt;u&gt;Claude&lt;/u&gt;&lt;/a&gt;. But the path from off-the-shelf open-source model to production-ready custom AI remains treacherous, requiring specialized expertise in machine learning operations, infrastructure management, and performance optimization.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s answer: provide the infrastructure rails while letting companies retain full control over their training code, data, and model weights. It&amp;#x27;s a deliberately low-level approach born from hard-won lessons.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How a failed product taught Baseten what AI training infrastructure really needs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;This isn&amp;#x27;t Baseten&amp;#x27;s first foray into training. The company&amp;#x27;s previous attempt, a product called Blueprints launched roughly two and a half years ago, failed spectacularly — a failure Haghighat now embraces as instructive.&lt;/p&gt;&lt;p&gt;&amp;quot;We had created the abstraction layer a little too high,&amp;quot; he explained. &amp;quot;We were trying to create a magical experience, where as a user, you come in and programmatically choose a base model, choose your data and some hyperparameters, and magically out comes a model.&amp;quot;&lt;/p&gt;&lt;p&gt;The problem? Users didn&amp;#x27;t have the intuition to make the right choices about base models, data quality, or hyperparameters. When their models underperformed, they blamed the product. Baseten found itself in the consulting business rather than the infrastructure business, helping customers debug everything from dataset deduplication to model selection.&lt;/p&gt;&lt;p&gt;&amp;quot;We became consultants,&amp;quot; Haghighat said. &amp;quot;And that&amp;#x27;s not what we had set out to do.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.baseten.co/"&gt;&lt;u&gt;Baseten&lt;/u&gt;&lt;/a&gt; killed Blueprints and refocused entirely on inference, vowing to &amp;quot;earn the right&amp;quot; to expand again. That moment arrived earlier this year, driven by two market realities: the vast majority of Baseten&amp;#x27;s inference revenue comes from custom models that customers train elsewhere, and competing training platforms were using restrictive terms of service to lock customers into their inference products.&lt;/p&gt;&lt;p&gt;&amp;quot;Multiple companies who were building fine-tuning products had in their terms of service that you as a customer cannot take the weights of the fine-tuned model with you somewhere else,&amp;quot; Haghighat said. &amp;quot;I understand why from their perspective — I still don&amp;#x27;t think there is a big company to be made purely on just training or fine-tuning. The sticky part is in inference, the valuable part where value is unlocked is in inference, and ultimately the revenue is in inference.&amp;quot;&lt;/p&gt;&lt;p&gt;Baseten took the opposite approach: customers own their weights and can download them at will. The bet is that superior inference performance will keep them on the platform anyway.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Multi-cloud GPU orchestration and sub-minute scheduling set Baseten apart from hyperscalers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The new &lt;a href="https://www.baseten.co/products/training/"&gt;&lt;u&gt;Baseten Training&lt;/u&gt;&lt;/a&gt; product operates at what Haghighat calls &amp;quot;the infrastructure layer&amp;quot; — lower-level than the failed Blueprints experiment, but with opinionated tooling around reliability, observability, and integration with Baseten&amp;#x27;s inference stack.&lt;/p&gt;&lt;p&gt;Key technical capabilities include multi-node training support across clusters of &lt;a href="https://www.nvidia.com/en-us/data-center/h100/"&gt;&lt;u&gt;NVIDIA H100&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://www.baseten.co/blog/accelerating-inference-nvidia-b200-gpus/?utm_term=b200%20gpu&amp;amp;utm_campaign=Search+-Hosting+Inference&amp;amp;utm_source=adwords&amp;amp;utm_medium=ppc&amp;amp;hsa_acc=9990356727&amp;amp;hsa_cam=21607833837&amp;amp;hsa_grp=179204207220&amp;amp;hsa_ad=747940377782&amp;amp;hsa_src=g&amp;amp;hsa_tgt=kwd-2402895176571&amp;amp;hsa_kw=b200%20gpu&amp;amp;hsa_mt=e&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;gad_source=1&amp;amp;gad_campaignid=21607833837&amp;amp;gbraid=0AAAAAqCKh1tBBfE-A9FM_fms6m5z6IuPk&amp;amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6S_fxqaV-EfbGshbTKib8rt2sQl81yPs6M0y40aoYf1Zy5mV_ECflBoCjPsQAvD_BwE"&gt;&lt;u&gt;B200 GPUs&lt;/u&gt;&lt;/a&gt;, automated checkpointing to protect against node failures, sub-minute job scheduling, and integration with Baseten&amp;#x27;s proprietary &lt;a href="https://www.baseten.co/blog/how-we-built-multi-cloud-capacity-management/"&gt;&lt;u&gt;Multi-Cloud Management (MCM) &lt;/u&gt;&lt;/a&gt;system. That last piece is critical: MCM allows Baseten to dynamically provision GPU capacity across multiple cloud providers and regions, passing cost savings to customers while avoiding the capacity constraints and multi-year contracts typical of hyperscaler deals.&lt;/p&gt;&lt;p&gt;&amp;quot;With hyperscalers, you don&amp;#x27;t get to say, &amp;#x27;Hey, give me three or four B200 nodes while my job is running, and then take it back from me and don&amp;#x27;t charge me for it,&amp;#x27;&amp;quot; Haghighat said. &amp;quot;They say, &amp;#x27;No, you need to sign a three-year contract.&amp;#x27; We don&amp;#x27;t do that.&amp;quot;&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s approach mirrors broader trends in cloud infrastructure, where abstraction layers increasingly allow workloads to move fluidly across providers. When AWS experienced a major outage several weeks ago, Baseten&amp;#x27;s inference services remained operational by automatically routing traffic to other cloud providers — a capability now extended to training workloads.&lt;/p&gt;&lt;p&gt;The technical differentiation extends to Baseten&amp;#x27;s observability tooling, which provides per-GPU metrics for multi-node jobs, granular checkpoint tracking, and a refreshed UI that surfaces infrastructure-level events. The company also introduced an &amp;quot;&lt;a href="https://github.com/basetenlabs/ml-cookbook"&gt;&lt;u&gt;ML Cookbook&lt;/u&gt;&lt;/a&gt;&amp;quot; of open-source training recipes for popular models like Gemma, GPT OSS, and Qwen, designed to help users reach &amp;quot;training success&amp;quot; faster.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Early adopters report 84% cost savings and 50% latency improvements with custom models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Two early customers illustrate the market Baseten is targeting: AI-native companies building specialized vertical solutions that require custom models.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.oxen.ai/"&gt;&lt;u&gt;Oxen AI&lt;/u&gt;&lt;/a&gt;, a platform focused on dataset management and model fine-tuning, exemplifies the partnership model Baseten envisions. CEO Greg Schoeninger articulated a common strategic calculus, telling VentureBeat: &amp;quot;Whenever I&amp;#x27;ve seen a platform try to do both hardware and software, they usually fail at one of them. That&amp;#x27;s why partnering with Baseten to handle infrastructure was the obvious choice.&amp;quot;&lt;/p&gt;&lt;p&gt;Oxen built its customer experience entirely on top of Baseten&amp;#x27;s infrastructure, using the &lt;a href="https://www.baseten.co/resources/changelog/authenticate-from-cli-with-truss-login/"&gt;&lt;u&gt;Baseten CLI&lt;/u&gt;&lt;/a&gt; to programmatically orchestrate training jobs. The system automatically provisions and deprovisions GPUs, fully concealing Baseten&amp;#x27;s interface behind Oxen&amp;#x27;s own. For one Oxen customer, &lt;a href="https://alliumai.com/"&gt;&lt;u&gt;AlliumAI&lt;/u&gt;&lt;/a&gt; — a startup bringing structure to messy retail data — the integration delivered 84% cost savings compared to previous approaches, reducing total inference costs from $46,800 to $7,530.&lt;/p&gt;&lt;p&gt;&amp;quot;Training custom LoRAs has always been one of the most effective ways to leverage open-source models, but it often came with infrastructure headaches,&amp;quot; said Daniel Demillard, CEO of AlliumAI. &amp;quot;With Oxen and Baseten, that complexity disappears. We can train and deploy models at massive scale without ever worrying about CUDA, which GPU to choose, or shutting down servers after training.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://parsed.com/"&gt;&lt;u&gt;Parsed&lt;/u&gt;&lt;/a&gt;, another early customer, tackles a different pain point: helping enterprises reduce dependence on OpenAI by creating specialized models that outperform generalist LLMs on domain-specific tasks. The company works in mission-critical sectors like healthcare, finance, and legal services, where model performance and reliability aren&amp;#x27;t negotiable.&lt;/p&gt;&lt;p&gt;&amp;quot;Prior to switching to Baseten, we were seeing repetitive and degraded performance on our fine-tuned models due to bugs with our previous training provider,&amp;quot; said Charles O&amp;#x27;Neill, Parsed&amp;#x27;s co-founder and chief science officer. &amp;quot;On top of that, we were struggling to easily download and checkpoint weights after training runs.&amp;quot;&lt;/p&gt;&lt;p&gt;With Baseten, Parsed achieved 50% lower end-to-end latency for transcription use cases, spun up HIPAA-compliant EU deployments for testing within 48 hours, and kicked off more than 500 training jobs. The company also leveraged Baseten&amp;#x27;s modified &lt;a href="https://docs.baseten.co/examples/vllm"&gt;&lt;u&gt;vLLM inference framework&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/"&gt;&lt;u&gt;speculative decoding&lt;/u&gt;&lt;/a&gt; — a technique that generates draft tokens to accelerate language model output — to cut latency in half for custom models.&lt;/p&gt;&lt;p&gt;&amp;quot;Fast models matter,&amp;quot; O&amp;#x27;Neill said. &amp;quot;But fast models that get better over time matter more. A model that&amp;#x27;s 2x faster but static loses to one that&amp;#x27;s slightly slower but improving 10% monthly. Baseten gives us both — the performance edge today and the infrastructure for continuous improvement.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why training and inference are more interconnected than the industry realizes&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Parsed example illuminates a deeper strategic rationale for Baseten&amp;#x27;s training expansion: the boundary between training and inference is blurrier than conventional wisdom suggests.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s model performance team uses the training platform extensively to create &amp;quot;draft models&amp;quot; for speculative decoding, a cutting-edge technique that can dramatically accelerate inference. The company recently announced it achieved 650+ tokens per second on OpenAI&amp;#x27;s &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;&lt;u&gt;GPT OSS 120B model&lt;/u&gt;&lt;/a&gt; — a 60% improvement over its launch performance — using &lt;a href="https://arxiv.org/abs/2503.01840"&gt;&lt;u&gt;EAGLE-3&lt;/u&gt;&lt;/a&gt; speculative decoding, which requires training specialized small models to work alongside larger target models.&lt;/p&gt;&lt;p&gt;&amp;quot;Ultimately, inference and training plug in more ways than one might think,&amp;quot; Haghighat said. &amp;quot;When you do speculative decoding in inference, you need to train the draft model. Our model performance team is a big customer of the training product to train these EAGLE heads on a continuous basis.&amp;quot;&lt;/p&gt;&lt;p&gt;This technical interdependence reinforces Baseten&amp;#x27;s thesis that owning both training and inference creates defensible value. The company can optimize the entire lifecycle: a model trained on Baseten can be deployed with a single click to inference endpoints pre-optimized for that architecture, with deployment-from-checkpoint support for chat completion and audio transcription workloads.&lt;/p&gt;&lt;p&gt;The approach contrasts sharply with vertically integrated competitors like &lt;a href="https://replicate.com/"&gt;&lt;u&gt;Replicate&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://modal.com/"&gt;&lt;u&gt;Modal&lt;/u&gt;&lt;/a&gt;, which also offer training and inference but with different architectural tradeoffs. Baseten&amp;#x27;s bet is on lower-level infrastructure flexibility and performance optimization, particularly for companies running custom models at scale.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;As open-source AI models improve, enterprises see fine-tuning as the path away from OpenAI dependency&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Underpinning Baseten&amp;#x27;s entire strategy is a conviction about the trajectory of open-source AI models — namely, that they&amp;#x27;re getting good enough, fast enough, to unlock massive enterprise adoption through fine-tuning.&lt;/p&gt;&lt;p&gt;&amp;quot;Both closed and open-source models are getting better and better in terms of quality,&amp;quot; Haghighat said. &amp;quot;We don&amp;#x27;t even need open source to surpass closed models, because as both of them are getting better, they unlock all these invisible lines of usefulness for different use cases.&amp;quot;&lt;/p&gt;&lt;p&gt;He pointed to the proliferation of reinforcement learning and supervised fine-tuning techniques that allow companies to take an open-source model and make it &amp;quot;as good as the closed model, not at everything, but at this narrow band of capability that they want.&amp;quot;&lt;/p&gt;&lt;p&gt;That trend is already visible in Baseten&amp;#x27;s &lt;a href="https://www.baseten.co/products/model-apis/"&gt;&lt;u&gt;Model APIs business&lt;/u&gt;&lt;/a&gt;, launched alongside Training earlier this year to provide production-grade access to open-source models. The company was the first provider to offer access to &lt;a href="https://api-docs.deepseek.com/news/news1226"&gt;&lt;u&gt;DeepSeek V3&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://api-docs.deepseek.com/news/news250120"&gt;&lt;u&gt;R1&lt;/u&gt;&lt;/a&gt;, and has since added models like &lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;&lt;u&gt;Llama 4&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://qwen.ai/research"&gt;&lt;u&gt;Qwen 3&lt;/u&gt;&lt;/a&gt;, optimized for performance and reliability. Model APIs serves as a top-of-funnel product: companies start with off-the-shelf open-source models, realize they need customization, move to Training for fine-tuning, and ultimately deploy on Baseten&amp;#x27;s &lt;a href="https://www.baseten.co/products/dedicated-deployments/"&gt;&lt;u&gt;Dedicated Deployments&lt;/u&gt;&lt;/a&gt; infrastructure.&lt;/p&gt;&lt;p&gt;Yet Haghighat acknowledged the market remains &amp;quot;fuzzy&amp;quot; around which training techniques will dominate. Baseten is hedging by staying close to the bleeding edge through its &lt;a href="https://www.baseten.co/blog/forward-deployed-engineering/"&gt;&lt;u&gt;Forward Deployed Engineering team&lt;/u&gt;&lt;/a&gt;, which works hands-on with select customers on reinforcement learning, supervised fine-tuning, and other advanced techniques.&lt;/p&gt;&lt;p&gt;&amp;quot;As we do that, we will see patterns emerge about what a productized training product can look like that really addresses the user&amp;#x27;s needs without them having to learn too much about how RL works,&amp;quot; he said. &amp;quot;Are we there as an industry? I would say not quite. I see some attempts at that, but they all seem like almost falling to the same trap that Blueprints fell into—a bit of a walled garden that ties the hands of AI folks behind their back.&amp;quot;&lt;/p&gt;&lt;p&gt;The roadmap ahead includes potential abstractions for common training patterns, expansion into image, audio, and video fine-tuning, and deeper integration of advanced techniques like prefill-decode disaggregation, which separates the initial processing of prompts from token generation to improve efficiency.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Baseten faces crowded field but bets developer experience and performance will win enterprise customers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Baseten enters an increasingly crowded market for AI infrastructure. Hyperscalers like &lt;a href="https://aws.amazon.com/"&gt;&lt;u&gt;AWS&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://cloud.google.com/?hl=en"&gt;&lt;u&gt;Google Cloud&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://azure.microsoft.com/en-us/"&gt;&lt;u&gt;Microsoft Azure&lt;/u&gt;&lt;/a&gt; offer GPU compute for training, while specialized providers like Lambda Labs, CoreWeave, and Together AI compete on price, performance, or ease of use. Then there are vertically integrated platforms like &lt;a href="https://huggingface.co/"&gt;&lt;u&gt;Hugging Face&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://replicate.com/"&gt;&lt;u&gt;Replicate&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://modal.com/"&gt;&lt;u&gt;Modal&lt;/u&gt;&lt;/a&gt; that bundle training, inference, and model hosting.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s differentiation rests on three pillars: its MCM system for multi-cloud capacity management, deep performance optimization expertise built from its inference business, and a developer experience tailored for production deployments rather than experimentation.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s recent &lt;a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"&gt;&lt;u&gt;$150 million Series D&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"&gt;&lt;u&gt;$2.15 billion valuation&lt;/u&gt;&lt;/a&gt; provide runway to invest in both products simultaneously. Major customers include &lt;a href="https://www.descript.com/"&gt;&lt;u&gt;Descript&lt;/u&gt;&lt;/a&gt;, which uses Baseten for transcription workloads; &lt;a href="https://decagon.ai/"&gt;&lt;u&gt;Decagon&lt;/u&gt;&lt;/a&gt;, which runs customer service AI; and &lt;a href="https://sourcegraph.com/"&gt;&lt;u&gt;Sourcegraph&lt;/u&gt;&lt;/a&gt;, which powers coding assistants. All three operate in domains where model customization and performance are competitive advantages.&lt;/p&gt;&lt;p&gt;Timing may be Baseten&amp;#x27;s biggest asset. The confluence of improving open-source models, enterprise discomfort with dependence on proprietary AI providers, and growing sophistication around fine-tuning techniques creates what Haghighat sees as a sustainable market shift.&lt;/p&gt;&lt;p&gt;&amp;quot;There is a lot of use cases for which closed models have gotten there and open ones have not,&amp;quot; he said. &amp;quot;Where I&amp;#x27;m seeing in the market is people using different training techniques — more recently, a lot of reinforcement learning and SFT — to be able to get this open model to be as good as the closed model, not at everything, but at this narrow band of capability that they want. That&amp;#x27;s very palpable in the market.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprises navigating the complex transition from closed to open AI models, Baseten&amp;#x27;s positioning offers a clear value proposition: infrastructure that handles the messy middle of fine-tuning while optimizing for the ultimate goal of performant, reliable, cost-effective inference at scale. The company&amp;#x27;s insistence that customers own their model weights — a stark contrast to competitors using training as a lock-in mechanism — reflects confidence that technical excellence, not contractual restrictions, will drive retention.&lt;/p&gt;&lt;p&gt;Whether Baseten can execute on this vision depends on navigating tensions inherent in its strategy: staying at the infrastructure layer without becoming consultants, providing power and flexibility without overwhelming users with complexity, and building abstractions at exactly the right level as the market matures. The company&amp;#x27;s willingness to kill Blueprints when it failed suggests a pragmatism that could prove decisive in a market where many infrastructure providers over-promise and under-deliver.&lt;/p&gt;&lt;p&gt;&amp;quot;Through and through, we&amp;#x27;re an inference company,&amp;quot; Haghighat emphasized. &amp;quot;The reason that we did training is at the service of inference.&amp;quot;&lt;/p&gt;&lt;p&gt;That clarity of purpose — treating training as a means to an end rather than an end in itself—may be Baseten&amp;#x27;s most important strategic asset. As AI deployment matures from experimentation to production, the companies that solve the full stack stand to capture outsized value. But only if they avoid the trap of technology in search of a problem.&lt;/p&gt;&lt;p&gt;At least Baseten&amp;#x27;s customers no longer have to SSH into boxes on Friday and pray their training jobs complete by Monday. In the infrastructure business, sometimes the best innovation is simply making the painful parts disappear.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/baseten-takes-on-hyperscalers-with-new-ai-training-platform-that-lets-you</guid><pubDate>Mon, 10 Nov 2025 14:00:00 +0000</pubDate></item><item><title>Reimagining cybersecurity in the era of AI and quantum (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/10/1127774/reimagining-cybersecurity-in-the-era-of-ai-and-quantum/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Cisco&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;AI and quantum technologies are dramatically reconfiguring how cybersecurity functions, redefining the speed and scale with which digital defenders and their adversaries can operate.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127802" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/CISCO-iStock-2156608663.jpg" /&gt;&lt;/figure&gt;  &lt;p&gt;The weaponization of AI tools for cyberattacks is already proving a worthy opponent to current defenses. From reconnaissance to ransomware, cybercriminals can automate attacks faster than ever before with AI. This includes using generative AI to create social engineering attacks at scale, churning out tens of thousands of tailored phishing emails in seconds, or accessing widely available voice cloning software capable of bypassing security defenses for as little as a few dollars. And now, agentic AI raises the stakes by introducing autonomous systems that can reason, act, and adapt like human adversaries.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;But AI isn’t the only force shaping the threat landscape. Quantum computing has the potential to seriously undermine current encryption standards if developed unchecked. Quantum algorithms can solve the mathematical problems underlying most modern cryptography, particularly public-key systems like RSA and Elliptic Curve, widely used for secure online communication, digital signatures, and cryptocurrency.&lt;/p&gt;  &lt;p&gt;“We know quantum is coming. Once it does, it will force a change in how we secure data across everything, including governments, telecoms, and financial systems,” says Peter Bailey, senior vice president and general manager of Cisco’s security business.&lt;/p&gt; 
 &lt;p&gt;“Most organizations are understandably focused on the immediacy of AI threats," says Bailey. “Quantum might sound like science fiction, but those scenarios are coming faster than many realize. It’s critical to start investing now in defenses that can withstand both AI and quantum attacks.”&lt;/p&gt;  &lt;p&gt;Critical to this defense is a zero trust approach to cybersecurity, which assumes no user or device can be inherently trusted. By enforcing continuous verification, zero trust enables constant monitoring and ensures that any attempts to exploit vulnerabilities are quickly detected and addressed in real time. This approach is technology-agnostic and creates a resilient framework even in the face of an ever-changing threat landscape.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Putting up AI defenses&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;AI is lowering the barrier to entry for cyberattacks, enabling hackers even with limited skills or resources to infiltrate, manipulate, and exploit the slightest digital vulnerability.&lt;/p&gt;  &lt;p&gt;Nearly three-quarters (74%) of cybersecurity professionals say AI-enabled threats are already having a significant impact on their organization, and 90% anticipate such threats in the next one to two years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“AI-powered adversaries have advanced techniques and operate at machine speed,” says Bailey. “The only way to keep pace is to use AI to automate response and defend at machine speed.”&lt;/p&gt;  &lt;p&gt;To do this, Bailey says, organizations must modernize systems, platforms, and security operations to automate threat detection and response—processes that have previously relied on human rule-writing and reaction times. These systems must adapt dynamically as environments evolve and criminal tactics change.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;At the same time, companies must strengthen the security of their AI models and data to reduce exposure to manipulation from AI-enabled malware. Such risks could include, for instance, prompt injections, where a malicious user crafts a prompt to manipulate an AI model into performing unintended actions, bypassing its original instructions and safeguards.&lt;/p&gt;  &lt;p&gt;Agentic AI further ups the ante, with hackers able to use AI agents to automate attacks and make tactical decisions without constant human oversight. “Agentic AI has the potential to collapse the cost of the kill chain,” says Bailey. “That means everyday cybercriminals could start executing campaigns that today only well-funded espionage operations can afford.”&lt;/p&gt;  &lt;p&gt;Organizations, in turn, are exploring how AI agents can help them stay ahead. Nearly 40% of companies expect agentic AI to augment or assist teams over the next 12 months, especially in cybersecurity, according to Cisco’s 2025 AI Readiness Index. Use cases include AI agents trained on telemetry, which can identify anomalies or signals from machine data too disparate and unstructured to be deciphered by humans.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Calculating the quantum threat&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;As many cybersecurity teams focus on the very real AI-driven threat, quantum is waiting on the sidelines. Almost three-quarters (73%) of US organizations surveyed by KPMG say they believe it is only a matter of time before cybercriminals are using quantum to decrypt and disrupt today’s cybersecurity protocols. And yet, the majority (81%) also admit they could do more to ensure that their data remains secure.&lt;/p&gt; 

 &lt;p&gt;Companies are right to be concerned. Threat actors are already carrying out harvest now, decrypt later attacks, stockpiling sensitive encrypted data to crack once quantum technology matures. Examples include state-sponsored actors intercepting government communications and cybercriminal networks storing encrypted internet traffic or financial records.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Large technology companies are among the first to roll out quantum defenses. For example, Apple is using cryptography protocol PQ3 to defend against harvest now, decrypt later attacks on its iMessage platform. Google is testing post-quantum cryptography (PQC)—which is resistant to attacks from both quantum and classical computers—in its Chrome browser. And Cisco “has made significant investments in quantum-proofing our software and infrastructure,” says Bailey. “You’ll see more enterprises and governments taking similar steps over the next 18 to 24 months,” he adds.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As regulations like the US Quantum Computing Cybersecurity Preparedness Act lay out requirements for mitigating against quantum threats, including standardized PQC algorithms by the National Institute of Standards and Technology, a wider range of organizations will start preparing their own quantum defenses.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For organizations beginning that journey, Bailey outlines two key actions. First, establish visibility. “Understand what data you have and where it lives,” he says. “Take inventory, assess sensitivity, and review your encryption keys, rotating out any that are weak or outdated.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Second, plan for migration. “Next, assess what it will take to support post-quantum algorithms across your infrastructure. That means addressing not just the technology, but also the process and people implications,” Bailey says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Adopting proactive defense&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Ultimately, the foundation for building resilience against both AI and quantum is a zero trust approach, says Bailey. By embedding zero trust access controls across users, devices, business applications, networks, and clouds, this approach grants only the minimum access required to complete a task and enables continuous monitoring. It can also minimize the attack surface by confining a potential threat to an isolated zone, preventing it from accessing other critical systems.&lt;/p&gt;  &lt;p&gt;Into this zero trust architecture, organizations can integrate specific measures to defend against AI and quantum risks. For instance, quantum-immune cryptography and AI-powered analytics and security tools can be used to identify complex attack patterns and automate real-time responses.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Zero trust slows down attacks and builds resilience,” Bailey says. “It ensures that even if a breach occurs, the crown jewels stay protected and operations can recover quickly.”&lt;/p&gt; 
 &lt;p&gt;Ultimately, companies should not wait for threats to emerge and evolve. They must get ahead now. “This isn’t a what-if scenario; it’s a when,” says Bailey. “Organizations that invest early will be the ones setting the pace, not scrambling to catch up.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Cisco&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;AI and quantum technologies are dramatically reconfiguring how cybersecurity functions, redefining the speed and scale with which digital defenders and their adversaries can operate.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127802" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/CISCO-iStock-2156608663.jpg" /&gt;&lt;/figure&gt;  &lt;p&gt;The weaponization of AI tools for cyberattacks is already proving a worthy opponent to current defenses. From reconnaissance to ransomware, cybercriminals can automate attacks faster than ever before with AI. This includes using generative AI to create social engineering attacks at scale, churning out tens of thousands of tailored phishing emails in seconds, or accessing widely available voice cloning software capable of bypassing security defenses for as little as a few dollars. And now, agentic AI raises the stakes by introducing autonomous systems that can reason, act, and adapt like human adversaries.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;But AI isn’t the only force shaping the threat landscape. Quantum computing has the potential to seriously undermine current encryption standards if developed unchecked. Quantum algorithms can solve the mathematical problems underlying most modern cryptography, particularly public-key systems like RSA and Elliptic Curve, widely used for secure online communication, digital signatures, and cryptocurrency.&lt;/p&gt;  &lt;p&gt;“We know quantum is coming. Once it does, it will force a change in how we secure data across everything, including governments, telecoms, and financial systems,” says Peter Bailey, senior vice president and general manager of Cisco’s security business.&lt;/p&gt; 
 &lt;p&gt;“Most organizations are understandably focused on the immediacy of AI threats," says Bailey. “Quantum might sound like science fiction, but those scenarios are coming faster than many realize. It’s critical to start investing now in defenses that can withstand both AI and quantum attacks.”&lt;/p&gt;  &lt;p&gt;Critical to this defense is a zero trust approach to cybersecurity, which assumes no user or device can be inherently trusted. By enforcing continuous verification, zero trust enables constant monitoring and ensures that any attempts to exploit vulnerabilities are quickly detected and addressed in real time. This approach is technology-agnostic and creates a resilient framework even in the face of an ever-changing threat landscape.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Putting up AI defenses&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;AI is lowering the barrier to entry for cyberattacks, enabling hackers even with limited skills or resources to infiltrate, manipulate, and exploit the slightest digital vulnerability.&lt;/p&gt;  &lt;p&gt;Nearly three-quarters (74%) of cybersecurity professionals say AI-enabled threats are already having a significant impact on their organization, and 90% anticipate such threats in the next one to two years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“AI-powered adversaries have advanced techniques and operate at machine speed,” says Bailey. “The only way to keep pace is to use AI to automate response and defend at machine speed.”&lt;/p&gt;  &lt;p&gt;To do this, Bailey says, organizations must modernize systems, platforms, and security operations to automate threat detection and response—processes that have previously relied on human rule-writing and reaction times. These systems must adapt dynamically as environments evolve and criminal tactics change.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;At the same time, companies must strengthen the security of their AI models and data to reduce exposure to manipulation from AI-enabled malware. Such risks could include, for instance, prompt injections, where a malicious user crafts a prompt to manipulate an AI model into performing unintended actions, bypassing its original instructions and safeguards.&lt;/p&gt;  &lt;p&gt;Agentic AI further ups the ante, with hackers able to use AI agents to automate attacks and make tactical decisions without constant human oversight. “Agentic AI has the potential to collapse the cost of the kill chain,” says Bailey. “That means everyday cybercriminals could start executing campaigns that today only well-funded espionage operations can afford.”&lt;/p&gt;  &lt;p&gt;Organizations, in turn, are exploring how AI agents can help them stay ahead. Nearly 40% of companies expect agentic AI to augment or assist teams over the next 12 months, especially in cybersecurity, according to Cisco’s 2025 AI Readiness Index. Use cases include AI agents trained on telemetry, which can identify anomalies or signals from machine data too disparate and unstructured to be deciphered by humans.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Calculating the quantum threat&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;As many cybersecurity teams focus on the very real AI-driven threat, quantum is waiting on the sidelines. Almost three-quarters (73%) of US organizations surveyed by KPMG say they believe it is only a matter of time before cybercriminals are using quantum to decrypt and disrupt today’s cybersecurity protocols. And yet, the majority (81%) also admit they could do more to ensure that their data remains secure.&lt;/p&gt; 

 &lt;p&gt;Companies are right to be concerned. Threat actors are already carrying out harvest now, decrypt later attacks, stockpiling sensitive encrypted data to crack once quantum technology matures. Examples include state-sponsored actors intercepting government communications and cybercriminal networks storing encrypted internet traffic or financial records.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Large technology companies are among the first to roll out quantum defenses. For example, Apple is using cryptography protocol PQ3 to defend against harvest now, decrypt later attacks on its iMessage platform. Google is testing post-quantum cryptography (PQC)—which is resistant to attacks from both quantum and classical computers—in its Chrome browser. And Cisco “has made significant investments in quantum-proofing our software and infrastructure,” says Bailey. “You’ll see more enterprises and governments taking similar steps over the next 18 to 24 months,” he adds.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As regulations like the US Quantum Computing Cybersecurity Preparedness Act lay out requirements for mitigating against quantum threats, including standardized PQC algorithms by the National Institute of Standards and Technology, a wider range of organizations will start preparing their own quantum defenses.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For organizations beginning that journey, Bailey outlines two key actions. First, establish visibility. “Understand what data you have and where it lives,” he says. “Take inventory, assess sensitivity, and review your encryption keys, rotating out any that are weak or outdated.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Second, plan for migration. “Next, assess what it will take to support post-quantum algorithms across your infrastructure. That means addressing not just the technology, but also the process and people implications,” Bailey says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Adopting proactive defense&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Ultimately, the foundation for building resilience against both AI and quantum is a zero trust approach, says Bailey. By embedding zero trust access controls across users, devices, business applications, networks, and clouds, this approach grants only the minimum access required to complete a task and enables continuous monitoring. It can also minimize the attack surface by confining a potential threat to an isolated zone, preventing it from accessing other critical systems.&lt;/p&gt;  &lt;p&gt;Into this zero trust architecture, organizations can integrate specific measures to defend against AI and quantum risks. For instance, quantum-immune cryptography and AI-powered analytics and security tools can be used to identify complex attack patterns and automate real-time responses.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Zero trust slows down attacks and builds resilience,” Bailey says. “It ensures that even if a breach occurs, the crown jewels stay protected and operations can recover quickly.”&lt;/p&gt; 
 &lt;p&gt;Ultimately, companies should not wait for threats to emerge and evolve. They must get ahead now. “This isn’t a what-if scenario; it’s a when,” says Bailey. “Organizations that invest early will be the ones setting the pace, not scrambling to catch up.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/10/1127774/reimagining-cybersecurity-in-the-era-of-ai-and-quantum/</guid><pubDate>Mon, 10 Nov 2025 14:19:28 +0000</pubDate></item><item><title>How context engineering can save your company from AI vibe code overload: lessons from Qodo and Monday.com (AI | VentureBeat)</title><link>https://venturebeat.com/ai/how-context-engineering-can-save-your-company-from-ai-vibe-code-overload</link><description>[unable to retrieve full-text content]&lt;p&gt;As cloud project tracking software &lt;a href="https://monday.com/"&gt;monday.com&lt;/a&gt;’s engineering organization scaled past 500 developers, the team began to feel the strain of its own success. Product lines were multiplying, microservices proliferating, and code was flowing faster than human reviewers could keep up. The company needed a way to review thousands of pull requests each month without drowning developers in tedium — or letting quality slip.&lt;/p&gt;&lt;p&gt;That’s when Guy Regev, VP of R&amp;amp;D and head of the Growth and monday Dev teams, started experimenting with a new AI tool from &lt;a href="https://www.qodo.ai/"&gt;Qodo&lt;/a&gt;, an Israeli startup focused on developer agents. What began as a lightweight test soon became a critical part of monday.com’s software delivery infrastructure, as a &lt;a href="https://www.qodo.ai/blog/monday-com-accelerates-review-cycles-and-improves-code-quality-with-qodo/"&gt;new case study&lt;/a&gt; released by both Qodo and monday.com today reveals. &lt;/p&gt;&lt;p&gt;“Qodo doesn’t feel like just another tool—it’s like adding a new developer to the team who actually learns how we work,&amp;quot; Regev told VentureBeat in a recent video call interview, adding that it has &amp;quot;prevented over 800 issues per month from reaching production—some of them could have caused serious security vulnerabilities.&amp;quot;&lt;/p&gt;&lt;p&gt;Unlike code generation tools like GitHub Copilot or Cursor, Qodo isn’t trying to write new code. Instead, it specializes in reviewing it — using what it calls &lt;b&gt;context engineering&lt;/b&gt; to understand not just what changed in a pull request, but why, how it aligns with business logic, and whether it follows internal best practices. &lt;/p&gt;&lt;p&gt;&amp;quot;You can call Claude Code or Cursor and in five minutes get 1,000 lines of code,&amp;quot; said Itamar Friedman, co-founder and CEO of Qodo, in the same video call interview as with Regev. &amp;quot;You have 40 minutes, and you can&amp;#x27;t review that. So you need Qodo to actually review it.”&lt;/p&gt;&lt;p&gt;For monday.com, this capability wasn’t just helpful — it was transformative.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Code Review, at Scale&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At any given time, monday.com’s developers are shipping updates across hundreds of repositories and services. The engineering org works in tightly coordinated teams, each aligned with specific parts of the product: marketing, CRM, dev tools, internal platforms, and more.&lt;/p&gt;&lt;p&gt;That’s where Qodo came in. The company’s platform uses AI not just to check for obvious bugs or style violations, but to evaluate whether a pull request follows team-specific conventions, architectural guidelines, and historical patterns. &lt;/p&gt;&lt;p&gt;It does this by learning from your own codebase — training on previous PRs, comments, merges, and even Slack threads to understand how your team works.&lt;/p&gt;&lt;p&gt;&amp;quot;The comments Qodo gives aren’t generic—they reflect our values, our libraries, even our standards for things like feature flags and privacy,&amp;quot; Regev said. &amp;quot;It’s context-aware in a way traditional tools aren’t.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What “Context Engineering” Actually Means&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Qodo calls its secret sauce &lt;b&gt;context engineering&lt;/b&gt; — a system-level approach to managing everything the model sees when making a decision.&lt;/p&gt;&lt;p&gt; This includes the PR code diff, of course, but also prior discussions, documentation, relevant files from the repo, even test results and configuration data.&lt;/p&gt;&lt;p&gt;The idea is that language models don’t really “think” — they predict the next token based on the inputs they’re given. So the quality of their output depends almost entirely on the quality and structure of their inputs.&lt;/p&gt;&lt;p&gt;As Dana Fine, Qodo’s community manager, put it in a&lt;a href="https://www.qodo.ai/blog/context-engineering/"&gt; blog post&lt;/a&gt;: “You’re not just writing prompts; you’re designing structured input under a fixed token limit. Every token is a design decision.”&lt;/p&gt;&lt;p&gt;This isn’t just theory. In monday.com’s case, it meant Qodo could catch not only the obvious bugs, but the subtle ones that typically slip past human reviewers — hardcoded variables, missing fallbacks, or violations of cross-team architecture conventions.&lt;/p&gt;&lt;p&gt;One example stood out. In a recent PR, Qodo flagged a line that inadvertently exposed a staging environment variable — something no human reviewer caught. Had it been merged, it might have caused problems in production. &lt;/p&gt;&lt;p&gt;&amp;quot;The hours we would spend on fixing this security leak and the legal issue that it would bring would be much more than the hours that we reduce from a pull-request,&amp;quot; said Regev.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Integration into the Pipeline&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Today, Qodo is deeply integrated into monday.com’s development workflow, analyzing pull requests and surfacing context-aware recommendations based on prior team code reviews. &lt;/p&gt;&lt;p&gt;“It doesn’t feel like just another tool... It feels like another teammate that joined the system — one who learns how we work,&amp;quot; Regev noted. &lt;/p&gt;&lt;p&gt;Developers receive suggestions during the review process and remain in control of final decisions — a human-in-the-loop model that was critical for adoption.&lt;/p&gt;&lt;p&gt;Because Qodo integrated directly into GitHub via pull request actions and comments, Monday.com’s infrastructure team didn’t face a steep learning curve.&lt;/p&gt;&lt;p&gt;“It’s just a GitHub action,” said Regev. “It creates a PR with the tests. It’s not like a separate tool we had to learn.”&lt;/p&gt;&lt;p&gt;“The purpose is to actually help the developer learn the code, take ownership, give feedback to each other, and learn from that and establish the standards,&amp;quot; added Friedman.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The Results: Time Saved, Bugs Prevented&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Since rolling out Qodo more broadly, monday.com has seen measurable improvements across multiple teams.&lt;/p&gt;&lt;p&gt;Internal analysis shows that developers save roughly an hour per pull request on average. Multiply that across thousands of PRs per month, and the savings quickly reach thousands of developer hours annually.&lt;/p&gt;&lt;p&gt;These aren’t just cosmetic issues — many relate to business logic, security, or runtime stability. And because Qodo’s suggestions reflect monday.com’s actual conventions, developers are more likely to act on them.&lt;/p&gt;&lt;p&gt;The system’s accuracy is rooted in its data-first design. Qodo trains on each company’s private codebase and historical data, adapting to different team styles and practices. It doesn’t rely on one-size-fits-all rules or external datasets. Everything is tailored.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From Internal Tool to Product Vision&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Regev’s team was so impressed with Qodo’s impact that they’ve started planning deeper integrations between Qodo and Monday Dev, the developer-focused product line monday.com is building.&lt;/p&gt;&lt;p&gt;The vision is to create a workflow where business context — tasks, tickets, customer feedback — flows directly into the code review layer. That way, reviewers can assess not just whether the code “works,” but whether it solves the right problem.&lt;/p&gt;&lt;p&gt;“Before, we had linters, danger rules, static analysis... rule-based... you need to configure all the rules,&amp;quot; Regev said. &amp;quot;But it doesn’t know what you don’t know... Qodo... feels like it’s learning from our engineers.”&lt;/p&gt;&lt;p&gt;This aligns closely with Qodo’s own roadmap. The company doesn’t just review code. It’s building a full platform of developer agents — including Qodo Gen for context-aware code generation, Qodo Merge for automated PR analysis, and Qodo Cover, a regression-testing agent that uses runtime validation to ensure test coverage.&lt;/p&gt;&lt;p&gt;All of this is powered by Qodo’s own infrastructure, including its new open-source embedding model, Qodo-Embed-1-1.5B, which outperformed offerings from OpenAI and Salesforce on code retrieval benchmarks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What’s Next?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Qodo is now offering its platform under a freemium model — free for individuals, discounted for startups through Google Cloud’s Perks program, and enterprise-grade for companies that need SSO, air-gapped deployment, or advanced controls.&lt;/p&gt;&lt;p&gt;The company is already working with teams at NVIDIA, Intuit, and other Fortune 500 companies. And thanks to a recent partnership with Google Cloud, Qodo’s models are available directly inside Vertex AI’s Model Garden, making it easier to integrate into enterprise pipelines.&lt;/p&gt;&lt;p&gt;&amp;quot;Context engines will be the big story of 2026,&amp;quot; Friedman said. &amp;quot;Every enterprise will need to build their own second brain if they want AI that actually understands and helps them.&amp;quot;&lt;/p&gt;&lt;p&gt;As AI systems become more embedded in software development, tools like Qodo are showing how the right context — delivered at the right moment — can transform how teams build, ship, and scale code across the enterprise.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;As cloud project tracking software &lt;a href="https://monday.com/"&gt;monday.com&lt;/a&gt;’s engineering organization scaled past 500 developers, the team began to feel the strain of its own success. Product lines were multiplying, microservices proliferating, and code was flowing faster than human reviewers could keep up. The company needed a way to review thousands of pull requests each month without drowning developers in tedium — or letting quality slip.&lt;/p&gt;&lt;p&gt;That’s when Guy Regev, VP of R&amp;amp;D and head of the Growth and monday Dev teams, started experimenting with a new AI tool from &lt;a href="https://www.qodo.ai/"&gt;Qodo&lt;/a&gt;, an Israeli startup focused on developer agents. What began as a lightweight test soon became a critical part of monday.com’s software delivery infrastructure, as a &lt;a href="https://www.qodo.ai/blog/monday-com-accelerates-review-cycles-and-improves-code-quality-with-qodo/"&gt;new case study&lt;/a&gt; released by both Qodo and monday.com today reveals. &lt;/p&gt;&lt;p&gt;“Qodo doesn’t feel like just another tool—it’s like adding a new developer to the team who actually learns how we work,&amp;quot; Regev told VentureBeat in a recent video call interview, adding that it has &amp;quot;prevented over 800 issues per month from reaching production—some of them could have caused serious security vulnerabilities.&amp;quot;&lt;/p&gt;&lt;p&gt;Unlike code generation tools like GitHub Copilot or Cursor, Qodo isn’t trying to write new code. Instead, it specializes in reviewing it — using what it calls &lt;b&gt;context engineering&lt;/b&gt; to understand not just what changed in a pull request, but why, how it aligns with business logic, and whether it follows internal best practices. &lt;/p&gt;&lt;p&gt;&amp;quot;You can call Claude Code or Cursor and in five minutes get 1,000 lines of code,&amp;quot; said Itamar Friedman, co-founder and CEO of Qodo, in the same video call interview as with Regev. &amp;quot;You have 40 minutes, and you can&amp;#x27;t review that. So you need Qodo to actually review it.”&lt;/p&gt;&lt;p&gt;For monday.com, this capability wasn’t just helpful — it was transformative.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Code Review, at Scale&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At any given time, monday.com’s developers are shipping updates across hundreds of repositories and services. The engineering org works in tightly coordinated teams, each aligned with specific parts of the product: marketing, CRM, dev tools, internal platforms, and more.&lt;/p&gt;&lt;p&gt;That’s where Qodo came in. The company’s platform uses AI not just to check for obvious bugs or style violations, but to evaluate whether a pull request follows team-specific conventions, architectural guidelines, and historical patterns. &lt;/p&gt;&lt;p&gt;It does this by learning from your own codebase — training on previous PRs, comments, merges, and even Slack threads to understand how your team works.&lt;/p&gt;&lt;p&gt;&amp;quot;The comments Qodo gives aren’t generic—they reflect our values, our libraries, even our standards for things like feature flags and privacy,&amp;quot; Regev said. &amp;quot;It’s context-aware in a way traditional tools aren’t.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What “Context Engineering” Actually Means&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Qodo calls its secret sauce &lt;b&gt;context engineering&lt;/b&gt; — a system-level approach to managing everything the model sees when making a decision.&lt;/p&gt;&lt;p&gt; This includes the PR code diff, of course, but also prior discussions, documentation, relevant files from the repo, even test results and configuration data.&lt;/p&gt;&lt;p&gt;The idea is that language models don’t really “think” — they predict the next token based on the inputs they’re given. So the quality of their output depends almost entirely on the quality and structure of their inputs.&lt;/p&gt;&lt;p&gt;As Dana Fine, Qodo’s community manager, put it in a&lt;a href="https://www.qodo.ai/blog/context-engineering/"&gt; blog post&lt;/a&gt;: “You’re not just writing prompts; you’re designing structured input under a fixed token limit. Every token is a design decision.”&lt;/p&gt;&lt;p&gt;This isn’t just theory. In monday.com’s case, it meant Qodo could catch not only the obvious bugs, but the subtle ones that typically slip past human reviewers — hardcoded variables, missing fallbacks, or violations of cross-team architecture conventions.&lt;/p&gt;&lt;p&gt;One example stood out. In a recent PR, Qodo flagged a line that inadvertently exposed a staging environment variable — something no human reviewer caught. Had it been merged, it might have caused problems in production. &lt;/p&gt;&lt;p&gt;&amp;quot;The hours we would spend on fixing this security leak and the legal issue that it would bring would be much more than the hours that we reduce from a pull-request,&amp;quot; said Regev.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Integration into the Pipeline&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Today, Qodo is deeply integrated into monday.com’s development workflow, analyzing pull requests and surfacing context-aware recommendations based on prior team code reviews. &lt;/p&gt;&lt;p&gt;“It doesn’t feel like just another tool... It feels like another teammate that joined the system — one who learns how we work,&amp;quot; Regev noted. &lt;/p&gt;&lt;p&gt;Developers receive suggestions during the review process and remain in control of final decisions — a human-in-the-loop model that was critical for adoption.&lt;/p&gt;&lt;p&gt;Because Qodo integrated directly into GitHub via pull request actions and comments, Monday.com’s infrastructure team didn’t face a steep learning curve.&lt;/p&gt;&lt;p&gt;“It’s just a GitHub action,” said Regev. “It creates a PR with the tests. It’s not like a separate tool we had to learn.”&lt;/p&gt;&lt;p&gt;“The purpose is to actually help the developer learn the code, take ownership, give feedback to each other, and learn from that and establish the standards,&amp;quot; added Friedman.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The Results: Time Saved, Bugs Prevented&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Since rolling out Qodo more broadly, monday.com has seen measurable improvements across multiple teams.&lt;/p&gt;&lt;p&gt;Internal analysis shows that developers save roughly an hour per pull request on average. Multiply that across thousands of PRs per month, and the savings quickly reach thousands of developer hours annually.&lt;/p&gt;&lt;p&gt;These aren’t just cosmetic issues — many relate to business logic, security, or runtime stability. And because Qodo’s suggestions reflect monday.com’s actual conventions, developers are more likely to act on them.&lt;/p&gt;&lt;p&gt;The system’s accuracy is rooted in its data-first design. Qodo trains on each company’s private codebase and historical data, adapting to different team styles and practices. It doesn’t rely on one-size-fits-all rules or external datasets. Everything is tailored.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From Internal Tool to Product Vision&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Regev’s team was so impressed with Qodo’s impact that they’ve started planning deeper integrations between Qodo and Monday Dev, the developer-focused product line monday.com is building.&lt;/p&gt;&lt;p&gt;The vision is to create a workflow where business context — tasks, tickets, customer feedback — flows directly into the code review layer. That way, reviewers can assess not just whether the code “works,” but whether it solves the right problem.&lt;/p&gt;&lt;p&gt;“Before, we had linters, danger rules, static analysis... rule-based... you need to configure all the rules,&amp;quot; Regev said. &amp;quot;But it doesn’t know what you don’t know... Qodo... feels like it’s learning from our engineers.”&lt;/p&gt;&lt;p&gt;This aligns closely with Qodo’s own roadmap. The company doesn’t just review code. It’s building a full platform of developer agents — including Qodo Gen for context-aware code generation, Qodo Merge for automated PR analysis, and Qodo Cover, a regression-testing agent that uses runtime validation to ensure test coverage.&lt;/p&gt;&lt;p&gt;All of this is powered by Qodo’s own infrastructure, including its new open-source embedding model, Qodo-Embed-1-1.5B, which outperformed offerings from OpenAI and Salesforce on code retrieval benchmarks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What’s Next?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Qodo is now offering its platform under a freemium model — free for individuals, discounted for startups through Google Cloud’s Perks program, and enterprise-grade for companies that need SSO, air-gapped deployment, or advanced controls.&lt;/p&gt;&lt;p&gt;The company is already working with teams at NVIDIA, Intuit, and other Fortune 500 companies. And thanks to a recent partnership with Google Cloud, Qodo’s models are available directly inside Vertex AI’s Model Garden, making it easier to integrate into enterprise pipelines.&lt;/p&gt;&lt;p&gt;&amp;quot;Context engines will be the big story of 2026,&amp;quot; Friedman said. &amp;quot;Every enterprise will need to build their own second brain if they want AI that actually understands and helps them.&amp;quot;&lt;/p&gt;&lt;p&gt;As AI systems become more embedded in software development, tools like Qodo are showing how the right context — delivered at the right moment — can transform how teams build, ship, and scale code across the enterprise.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/how-context-engineering-can-save-your-company-from-ai-vibe-code-overload</guid><pubDate>Mon, 10 Nov 2025 15:00:00 +0000</pubDate></item><item><title>Google Maps releases new AI tools that let you create interactive projects (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/google-maps-releases-new-ai-tools-to-let-you-create-interactive-projects/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google Maps is adding new AI features, including a builder agent and an MCP server — a tool that connects AI assistants to Google Maps’ technical documentation — to help developers and users create interactive projects using Maps data and code. The company said it is using Gemini models across the board to power these features.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among these new tools, the builder agent is a tool that, just like many other coding tools, lets you describe the kind of interactive map-based prototype you want to build in text and creates one for you. For instance, you can type “create a Street View tour of a city,” “create a map visualizing real-time weather in my region,” or “list pet-friendly hotels in the city.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Once the code is generated, you can export it, test the preview project using your own API keys as needed, or modify the project in Firebase Studio.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066365" height="482" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-10-at-10.17.41AM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The same tool also has a styling agent that lets users create a customized map to match a particular style format or theme. This could help brands create maps with specific color coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google already provides map data grounding via the Gemini API. The company is now introducing a similar feature, called Grounding Lite, which allows developers to ground their own AI models using Model Context Protocol (MCP), a standard that lets AI assistants connect to external data sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With this feature, AI assistants can answer questions like, “How far is the nearest grocery store?” The company is also shipping Contextual View, a low-code Google Maps component that can provide visual understanding to users for such questions. The feature can show a list, a map view, or a 3D display as an answer.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066366" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Contextual-view.jpeg?w=541" width="541" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding a code assistant toolkit, the MCP server, which connects with Google Maps’ documentation. Developers can use this connection to get answers about how to use the Google Maps API and data. Last month, the company launched extensions for Gemini’s command line tool to let developers access Maps data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066369" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Code-Assist.jpeg?w=549" width="549" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also trying to add more Gemini-powered features for Maps on the consumer side. Last week, it enabled users to use Gemini hands-free with Maps for navigation. For users in India, Google added incident alerts and speed limit data to the Maps app in select areas.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google Maps is adding new AI features, including a builder agent and an MCP server — a tool that connects AI assistants to Google Maps’ technical documentation — to help developers and users create interactive projects using Maps data and code. The company said it is using Gemini models across the board to power these features.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among these new tools, the builder agent is a tool that, just like many other coding tools, lets you describe the kind of interactive map-based prototype you want to build in text and creates one for you. For instance, you can type “create a Street View tour of a city,” “create a map visualizing real-time weather in my region,” or “list pet-friendly hotels in the city.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Once the code is generated, you can export it, test the preview project using your own API keys as needed, or modify the project in Firebase Studio.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066365" height="482" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-10-at-10.17.41AM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The same tool also has a styling agent that lets users create a customized map to match a particular style format or theme. This could help brands create maps with specific color coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google already provides map data grounding via the Gemini API. The company is now introducing a similar feature, called Grounding Lite, which allows developers to ground their own AI models using Model Context Protocol (MCP), a standard that lets AI assistants connect to external data sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With this feature, AI assistants can answer questions like, “How far is the nearest grocery store?” The company is also shipping Contextual View, a low-code Google Maps component that can provide visual understanding to users for such questions. The feature can show a list, a map view, or a 3D display as an answer.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066366" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Contextual-view.jpeg?w=541" width="541" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding a code assistant toolkit, the MCP server, which connects with Google Maps’ documentation. Developers can use this connection to get answers about how to use the Google Maps API and data. Last month, the company launched extensions for Gemini’s command line tool to let developers access Maps data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066369" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Code-Assist.jpeg?w=549" width="549" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also trying to add more Gemini-powered features for Maps on the consumer side. Last week, it enabled users to use Gemini hands-free with Maps for navigation. For users in India, Google added incident alerts and speed limit data to the Maps app in select areas.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/google-maps-releases-new-ai-tools-to-let-you-create-interactive-projects/</guid><pubDate>Mon, 10 Nov 2025 16:00:00 +0000</pubDate></item><item><title>The State of AI: Energy is king, and the US is falling behind (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/10/1126805/the-state-of-ai-energy-is-king-and-the-us-is-falling-behind/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;Welcome back to&amp;nbsp;The State of AI, a new collaboration between the Financial Times and MIT Technology Review. Every Monday, writers from both publications debate one aspect of the generative AI revolution and how it is reshaping global power.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;This week, Casey Crownhart, senior reporter for energy at MIT Technology Review and Pilita Clark, FT's columnist, consider how China's rapid renewables buildout could help it leapfrog on AI progress.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127502" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Casey Crownhart writes&lt;/strong&gt;:&lt;/p&gt;  &lt;p&gt;In the age of AI, the biggest barrier to progress isn’t money but energy. That should be particularly worrying here in the US, where massive data centers are waiting to come online, and it doesn’t look as if the country will build the steady power supply or infrastructure needed to serve them all.&lt;/p&gt; 
 &lt;p&gt;It wasn’t always like this. For about a decade before 2020, data centers were able to offset increased demand with efficiency improvements. Now, though, electricity demand is ticking up in the US, with billions of queries to popular AI models each day—and efficiency gains aren’t keeping pace. With too little new power capacity coming online, the strain is starting to show: Electricity bills are ballooning for people who live in places where data centers place a growing load on the grid.&lt;/p&gt;  &lt;p&gt;If we want AI to have the chance to deliver on big promises without driving electricity prices sky-high for the rest of us, the US needs to learn some lessons from the rest of the world on energy abundance. Just look at China.&lt;/p&gt; 
 &lt;p&gt;China installed 429 GW of new power generation capacity in 2024, more than six times the net capacity added in the US during that time.&lt;/p&gt;  &lt;p&gt;China still generates much of its electricity with coal, but that makes up a declining share of the mix. Rather, the country is focused on installing solar, wind, nuclear, and gas at record rates.&lt;/p&gt;  &lt;p&gt;The US, meanwhile, is focused on reviving its ailing coal industry. Coal-fired power plants are polluting and, crucially, expensive to run. Aging plants in the US are also less reliable than they used to be, generating electricity just 42% of the time, compared with a 61% capacity factor in 2014.&lt;/p&gt;  &lt;p&gt;It’s not a great situation. And unless the US changes something, we risk becoming consumers as opposed to innovators in both energy and AI tech. Already, China earns more from exporting renewables than the US does from oil and gas exports.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Building and permitting new renewable power plants would certainly help, since they’re currently the cheapest and fastest to bring online. But wind and solar are politically unpopular with the current administration. Natural gas is an obvious candidate, though there are concerns about delays with key equipment.&lt;/p&gt;  &lt;p&gt;One quick fix would be for data centers to be more flexible. If they agreed not to suck electricity from the grid during times of stress, new AI infrastructure might be able to come online without any new energy infrastructure.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;One study from Duke University found that if data centers agree to curtail their consumption just 0.25% of the time (roughly 22 hours over the course of the year), the grid could provide power for about 76 GW of new demand. That’s like adding about 5% of the entire grid’s capacity without needing to build anything new.&lt;/p&gt;  &lt;p&gt;But flexibility wouldn’t be enough to truly meet the swell in AI electricity demand. What do you think, Pilita? What would get the US out of these energy constraints? Is there anything else we should be thinking about when it comes to AI and its energy use?&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;Pilita Clark responds:&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I agree. Data centers that can cut their power use at times of grid stress should be the norm, not the exception. Likewise, we need more deals like those giving cheaper electricity to data centers that let power utilities access their backup generators. Both reduce the need to build more power plants, which makes sense regardless of how much electricity AI ends up using.&lt;/p&gt;  &lt;p&gt;This is a critical point for countries across the world, because we still don’t know exactly how much power AI is going to consume.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Forecasts for what data centers will need in as little as five years’ time vary wildly, from less than twice today’s rates to four times as much.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;This is partly because there’s a lack of public data about AI systems’ energy needs. It’s also because we don’t know how much more efficient these systems will become. The US chip designer Nvidia said last year that its specialized chips had become 45,000 times more energy efficient over the previous eight years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Moreover, we have been very wrong about tech energy needs before. At the height of the dot-com boom in 1999, it was erroneously claimed that the internet would need half the US’s electricity within a decade—necessitating a lot more coal power.&lt;/p&gt;  &lt;p&gt;Still, some countries are clearly feeling the pressure already. In Ireland, data centers chew up so much power that new connections have been restricted around Dublin to avoid straining the grid.&lt;/p&gt;  &lt;p&gt;Some regulators are eyeing new rules forcing tech companies to provide enough power generation to match their demand. I hope such efforts grow. I also hope AI itself helps boost power abundance and, crucially, accelerates the global energy transition needed to combat climate change. OpenAI’s Sam Altman said in 2023 that “once we have a really powerful super intelligence, addressing climate change will not be particularly difficult.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The evidence so far is not promising, especially in the US, where renewable projects are being axed. Still, the US may end up being an outlier in a world where ever cheaper renewables made up more than 90% of new power capacity added globally last year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Europe is aiming to power one of its biggest data centers predominantly with renewables and batteries. But the country leading the green energy expansion is clearly China.&lt;/p&gt; 
 &lt;p&gt;The 20th century was dominated by countries rich in the fossil fuels whose reign the US now wants to prolong. China, in contrast, may become the world’s first green electrostate. If it does this in a way that helps it win an AI race the US has so far controlled, it will mark a striking chapter in economic, technological, and geopolitical history.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Casey Crownhart replies:&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;I share your skepticism of tech executives’ claims that AI will be a groundbreaking help in the race to address climate change. To be fair, AI is progressing rapidly. But we don’t have time to wait for technologies standing on big claims with nothing to back them up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When it comes to the grid, for example, experts say there’s potential for AI to help with planning and even operating, but these efforts are still experimental.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, much of the world is making measurable progress on transitioning to newer, greener forms of energy. How that will affect the AI boom remains to be seen. What is clear is that AI is changing our grid and our world, and we need to be clear-eyed about the consequences.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Further reading&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt; reporters did the math on the energy needs of an AI query.&lt;/p&gt;  &lt;p&gt;There are still a few reasons to be optimistic about AI’s energy demands.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The &lt;em&gt;FT&lt;/em&gt;’s visual data team take a look&lt;strong&gt; &lt;/strong&gt;inside the relentless race for AI capacity.&lt;/p&gt;  &lt;p&gt;And global &lt;em&gt;FT&lt;/em&gt; reporters ask whether data centers can ever truly be green.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;Welcome back to&amp;nbsp;The State of AI, a new collaboration between the Financial Times and MIT Technology Review. Every Monday, writers from both publications debate one aspect of the generative AI revolution and how it is reshaping global power.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;This week, Casey Crownhart, senior reporter for energy at MIT Technology Review and Pilita Clark, FT's columnist, consider how China's rapid renewables buildout could help it leapfrog on AI progress.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127502" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Casey Crownhart writes&lt;/strong&gt;:&lt;/p&gt;  &lt;p&gt;In the age of AI, the biggest barrier to progress isn’t money but energy. That should be particularly worrying here in the US, where massive data centers are waiting to come online, and it doesn’t look as if the country will build the steady power supply or infrastructure needed to serve them all.&lt;/p&gt; 
 &lt;p&gt;It wasn’t always like this. For about a decade before 2020, data centers were able to offset increased demand with efficiency improvements. Now, though, electricity demand is ticking up in the US, with billions of queries to popular AI models each day—and efficiency gains aren’t keeping pace. With too little new power capacity coming online, the strain is starting to show: Electricity bills are ballooning for people who live in places where data centers place a growing load on the grid.&lt;/p&gt;  &lt;p&gt;If we want AI to have the chance to deliver on big promises without driving electricity prices sky-high for the rest of us, the US needs to learn some lessons from the rest of the world on energy abundance. Just look at China.&lt;/p&gt; 
 &lt;p&gt;China installed 429 GW of new power generation capacity in 2024, more than six times the net capacity added in the US during that time.&lt;/p&gt;  &lt;p&gt;China still generates much of its electricity with coal, but that makes up a declining share of the mix. Rather, the country is focused on installing solar, wind, nuclear, and gas at record rates.&lt;/p&gt;  &lt;p&gt;The US, meanwhile, is focused on reviving its ailing coal industry. Coal-fired power plants are polluting and, crucially, expensive to run. Aging plants in the US are also less reliable than they used to be, generating electricity just 42% of the time, compared with a 61% capacity factor in 2014.&lt;/p&gt;  &lt;p&gt;It’s not a great situation. And unless the US changes something, we risk becoming consumers as opposed to innovators in both energy and AI tech. Already, China earns more from exporting renewables than the US does from oil and gas exports.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Building and permitting new renewable power plants would certainly help, since they’re currently the cheapest and fastest to bring online. But wind and solar are politically unpopular with the current administration. Natural gas is an obvious candidate, though there are concerns about delays with key equipment.&lt;/p&gt;  &lt;p&gt;One quick fix would be for data centers to be more flexible. If they agreed not to suck electricity from the grid during times of stress, new AI infrastructure might be able to come online without any new energy infrastructure.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;One study from Duke University found that if data centers agree to curtail their consumption just 0.25% of the time (roughly 22 hours over the course of the year), the grid could provide power for about 76 GW of new demand. That’s like adding about 5% of the entire grid’s capacity without needing to build anything new.&lt;/p&gt;  &lt;p&gt;But flexibility wouldn’t be enough to truly meet the swell in AI electricity demand. What do you think, Pilita? What would get the US out of these energy constraints? Is there anything else we should be thinking about when it comes to AI and its energy use?&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;Pilita Clark responds:&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I agree. Data centers that can cut their power use at times of grid stress should be the norm, not the exception. Likewise, we need more deals like those giving cheaper electricity to data centers that let power utilities access their backup generators. Both reduce the need to build more power plants, which makes sense regardless of how much electricity AI ends up using.&lt;/p&gt;  &lt;p&gt;This is a critical point for countries across the world, because we still don’t know exactly how much power AI is going to consume.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Forecasts for what data centers will need in as little as five years’ time vary wildly, from less than twice today’s rates to four times as much.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;This is partly because there’s a lack of public data about AI systems’ energy needs. It’s also because we don’t know how much more efficient these systems will become. The US chip designer Nvidia said last year that its specialized chips had become 45,000 times more energy efficient over the previous eight years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Moreover, we have been very wrong about tech energy needs before. At the height of the dot-com boom in 1999, it was erroneously claimed that the internet would need half the US’s electricity within a decade—necessitating a lot more coal power.&lt;/p&gt;  &lt;p&gt;Still, some countries are clearly feeling the pressure already. In Ireland, data centers chew up so much power that new connections have been restricted around Dublin to avoid straining the grid.&lt;/p&gt;  &lt;p&gt;Some regulators are eyeing new rules forcing tech companies to provide enough power generation to match their demand. I hope such efforts grow. I also hope AI itself helps boost power abundance and, crucially, accelerates the global energy transition needed to combat climate change. OpenAI’s Sam Altman said in 2023 that “once we have a really powerful super intelligence, addressing climate change will not be particularly difficult.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The evidence so far is not promising, especially in the US, where renewable projects are being axed. Still, the US may end up being an outlier in a world where ever cheaper renewables made up more than 90% of new power capacity added globally last year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Europe is aiming to power one of its biggest data centers predominantly with renewables and batteries. But the country leading the green energy expansion is clearly China.&lt;/p&gt; 
 &lt;p&gt;The 20th century was dominated by countries rich in the fossil fuels whose reign the US now wants to prolong. China, in contrast, may become the world’s first green electrostate. If it does this in a way that helps it win an AI race the US has so far controlled, it will mark a striking chapter in economic, technological, and geopolitical history.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Casey Crownhart replies:&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;I share your skepticism of tech executives’ claims that AI will be a groundbreaking help in the race to address climate change. To be fair, AI is progressing rapidly. But we don’t have time to wait for technologies standing on big claims with nothing to back them up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When it comes to the grid, for example, experts say there’s potential for AI to help with planning and even operating, but these efforts are still experimental.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, much of the world is making measurable progress on transitioning to newer, greener forms of energy. How that will affect the AI boom remains to be seen. What is clear is that AI is changing our grid and our world, and we need to be clear-eyed about the consequences.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Further reading&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt; reporters did the math on the energy needs of an AI query.&lt;/p&gt;  &lt;p&gt;There are still a few reasons to be optimistic about AI’s energy demands.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The &lt;em&gt;FT&lt;/em&gt;’s visual data team take a look&lt;strong&gt; &lt;/strong&gt;inside the relentless race for AI capacity.&lt;/p&gt;  &lt;p&gt;And global &lt;em&gt;FT&lt;/em&gt; reporters ask whether data centers can ever truly be green.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/10/1126805/the-state-of-ai-energy-is-king-and-the-us-is-falling-behind/</guid><pubDate>Mon, 10 Nov 2025 16:45:00 +0000</pubDate></item><item><title>Wikipedia urges AI companies to use its paid API, and stop scraping (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/wikipedia-urges-ai-companies-to-use-its-paid-api-and-stop-scraping/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1873370000.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Wikipedia on Monday laid out a simple plan to ensure its website continues to be supported in the AI era, despite its declining traffic. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post, the Wikimedia Foundation, the organization that runs the popular online encyclopedia, called on AI developers to use its content “responsibly” by ensuring its contributions are properly attributed and that content is accessed through its paid product, the Wikimedia Enterprise platform.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The opt-in, paid product allows companies to use Wikipedia’s content at scale without “severely taxing Wikipedia’s servers,” the Wikimedia Foundation blog post explains. In addition, the product’s paid nature allows AI companies to support the organization’s nonprofit mission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the post doesn’t go so far as to threaten penalties or any sort of legal action for use of its material through scraping, Wikipedia recently noted that AI bots had been scraping its website while trying to appear human. After updating its bot-detection systems, the organization found that its unusually high traffic in May and June had come from AI bots that were trying to “evade detection.” Meanwhile, it said that “human page views” had declined 8% year-over-year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now Wikipedia is laying out its guidelines for AI developers and providers, saying that generative AI developers should provide attribution to give credit to the human contributors whose content it uses to create its outputs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For people to trust information shared on the internet, platforms should make it clear where the information is sourced from and elevate opportunities to visit and participate in those sources,” the post reads. “With fewer visits to Wikipedia, fewer volunteers may grow and enrich the content, and fewer individual donors may support this work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the organization released its AI strategy for editors, which said it would use AI to help editors with workflows around tedious tasks, automating translation, and other tools that help its editors, not replace them.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1873370000.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Wikipedia on Monday laid out a simple plan to ensure its website continues to be supported in the AI era, despite its declining traffic. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post, the Wikimedia Foundation, the organization that runs the popular online encyclopedia, called on AI developers to use its content “responsibly” by ensuring its contributions are properly attributed and that content is accessed through its paid product, the Wikimedia Enterprise platform.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The opt-in, paid product allows companies to use Wikipedia’s content at scale without “severely taxing Wikipedia’s servers,” the Wikimedia Foundation blog post explains. In addition, the product’s paid nature allows AI companies to support the organization’s nonprofit mission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the post doesn’t go so far as to threaten penalties or any sort of legal action for use of its material through scraping, Wikipedia recently noted that AI bots had been scraping its website while trying to appear human. After updating its bot-detection systems, the organization found that its unusually high traffic in May and June had come from AI bots that were trying to “evade detection.” Meanwhile, it said that “human page views” had declined 8% year-over-year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now Wikipedia is laying out its guidelines for AI developers and providers, saying that generative AI developers should provide attribution to give credit to the human contributors whose content it uses to create its outputs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For people to trust information shared on the internet, platforms should make it clear where the information is sourced from and elevate opportunities to visit and participate in those sources,” the post reads. “With fewer visits to Wikipedia, fewer volunteers may grow and enrich the content, and fewer individual donors may support this work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the organization released its AI strategy for editors, which said it would use AI to help editors with workflows around tedious tasks, automating translation, and other tools that help its editors, not replace them.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/wikipedia-urges-ai-companies-to-use-its-paid-api-and-stop-scraping/</guid><pubDate>Mon, 10 Nov 2025 18:30:59 +0000</pubDate></item><item><title>[NEW] Chronosphere takes on Datadog with AI that explains itself, not just outages (AI | VentureBeat)</title><link>https://venturebeat.com/ai/chronosphere-takes-on-datadog-with-ai-that-explains-itself-not-just-outages</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://chronosphere.io/"&gt;&lt;u&gt;Chronosphere&lt;/u&gt;&lt;/a&gt;, a New York-based observability startup &lt;a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"&gt;&lt;u&gt;valued at $1.6 billion&lt;/u&gt;&lt;/a&gt;, announced Monday it will launch &lt;a href="https://chronosphere.io/ai-guided-troubleshooting/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt; capabilities designed to help engineers diagnose and fix production software failures — a problem that has intensified as artificial intelligence tools accelerate code creation while making systems harder to debug.&lt;/p&gt;&lt;p&gt;The new features combine AI-driven analysis with what Chronosphere calls a &lt;a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"&gt;&lt;u&gt;Temporal Knowledge Graph&lt;/u&gt;&lt;/a&gt;, a continuously updated map of an organization&amp;#x27;s services, infrastructure dependencies, and system changes over time. The technology aims to address a mounting challenge in enterprise software: developers are writing code faster than ever with AI assistance, but troubleshooting remains largely manual, creating bottlenecks when applications fail.&lt;/p&gt;&lt;p&gt;&amp;quot;For AI to be effective in observability, it needs more than pattern recognition and summarization,&amp;quot; said Martin Mao, Chronosphere&amp;#x27;s CEO and co-founder, in an exclusive interview with VentureBeat. &amp;quot;Chronosphere has spent years building the data foundation and analytical depth needed for AI to actually help engineers. With our Temporal Knowledge Graph and advanced analytics capabilities, we&amp;#x27;re giving AI the understanding it needs to make observability truly intelligent — and giving engineers the confidence to trust its guidance.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement comes as the observability market — software that monitors complex cloud applications— faces mounting pressure to justify escalating costs. Enterprise log data volumes have grown &lt;a href="https://chronosphere.io/learn/observability-log-data-trends/"&gt;&lt;u&gt;250% year-over-year&lt;/u&gt;&lt;/a&gt;, according to Chronosphere&amp;#x27;s own research, while a study from MIT and the University of Pennsylvania found that generative &lt;a href="https://economics.mit.edu/sites/default/files/inline-files/draft_copilot_experiments.pdf"&gt;&lt;u&gt;AI has spurred a 13.5% increase in weekly code commits&lt;/u&gt;&lt;/a&gt;, signifying faster development velocity but also greater system complexity.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI writes code 13% faster, but debugging stays stubbornly manual&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite advances in automated code generation, debugging production failures remains stubbornly manual. When a major e-commerce site slows during checkout or a banking app fails to process transactions, engineers must sift through millions of data points — server logs, application traces, infrastructure metrics, recent code deployments — to identify root causes.&lt;/p&gt;&lt;p&gt;Chronosphere&amp;#x27;s answer is what it calls &lt;a href="https://chronosphere.io/ai-guided-troubleshooting/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt;, built on four core capabilities: automated &amp;quot;Suggestions&amp;quot; that propose investigation paths backed by data; the Temporal Knowledge Graph that maps system relationships and changes; Investigation Notebooks that document each troubleshooting step for future reference; and natural language query building.&lt;/p&gt;&lt;p&gt;Mao explained the &lt;a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"&gt;&lt;u&gt;Temporal Knowledge Graph&lt;/u&gt;&lt;/a&gt; in practical terms: &amp;quot;It&amp;#x27;s a living, time-aware model of your system. It stitches together telemetry—metrics, traces, logs—infrastructure context, change events like deploys and feature flags, and even human input like notes and runbooks into a single, queryable map that updates as your system evolves.&amp;quot;&lt;/p&gt;&lt;p&gt;This differs fundamentally from the service dependency maps offered by competitors like &lt;a href="https://www.datadoghq.com/dg/monitor/free-trial/?utm_source=google&amp;amp;utm_medium=paid-search&amp;amp;utm_campaign=dg-brand-ww&amp;amp;utm_keyword=datadog&amp;amp;utm_matchtype=b&amp;amp;igaag=95325237782&amp;amp;igaat=&amp;amp;igacm=9551169254&amp;amp;igacr=673270769690&amp;amp;igakw=datadog&amp;amp;igamt=b&amp;amp;igant=g&amp;amp;utm_campaignid=9551169254&amp;amp;utm_adgroupid=95325237782&amp;amp;gad_source=1&amp;amp;gad_campaignid=9551169254&amp;amp;gbraid=0AAAAADFY9NlOpf6xdtUzDLzD2BUR67UTl&amp;amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6T3Vk9lkeno-VLHlyRmDR0PFF8gTUGxe72EBr8QGbSpTNY5qtp63eRoC25kQAvD_BwE"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.dynatrace.com/"&gt;&lt;u&gt;Dynatrace&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.splunk.com/"&gt;&lt;u&gt;Splunk&lt;/u&gt;&lt;/a&gt;, Mao argued. &amp;quot;It adds time, not just topology,&amp;quot; he said. &amp;quot;It tracks how services and dependencies change over time and connects those changes to incidents—what changed and why. Many tools rely on standardized integrations; our graph goes a step further to normalize custom, non-standard telemetry so application-specific signals aren&amp;#x27;t a blind spot.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Chronosphere shows its work instead of making automatic decisions&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Unlike purely automated systems, &lt;a href="https://chronosphere.io/"&gt;&lt;u&gt;Chronosphere&lt;/u&gt;&lt;/a&gt; designed its AI features to keep engineers in the driver&amp;#x27;s seat—a deliberate choice meant to address what Mao calls the &amp;quot;confident-but-wrong guidance&amp;quot; problem plaguing early AI observability tools.&lt;/p&gt;&lt;p&gt;&amp;quot;&amp;#x27;Keeping engineers in control&amp;#x27; means the AI shows its work, proposes next steps, and lets engineers verify or override — never auto-deciding behind the scenes,&amp;quot; Mao explained. &amp;quot;Every Suggestion includes the evidence—timing, dependencies, error patterns — and a &amp;#x27;Why was this suggested?&amp;#x27; view, so they can inspect what was checked and ruled out before acting.&amp;quot;&lt;/p&gt;&lt;p&gt;He walked through a concrete example: &amp;quot;An SLO [service level objective] alert fires on Checkout. Chronosphere immediately surfaces a ranked Suggestion: errors appear to have started in the dependent Payment service. An engineer can click Investigate to see the charts and reasoning and, if it holds up, choose to dig deeper. As they steer into Payment, the system adapts with new Suggestions scoped to that service—all from one view, no tab-hopping.&amp;quot;&lt;/p&gt;&lt;p&gt;In this scenario, the engineer asks &amp;quot;what changed?&amp;quot; and the system pulls in change events. &amp;quot;Our Notebook capability makes the causal chain plain: a feature-flag update preceded pod memory exhaustion in Payment; Checkout&amp;#x27;s spike is a downstream symptom,&amp;quot; Mao said. &amp;quot;They can decide to roll back the flag. That whole path — suggestions followed, evidence viewed, conclusions—is captured automatically in an Investigation Notebook, and the outcome feeds the Temporal Knowledge Graph so similar future incidents are faster to resolve.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How a $1.6 billion startup takes on Datadog, Dynatrace, and Splunk&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere enters an increasingly crowded field. &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt;, the publicly traded observability leader valued at over $40 billion, has introduced its own AI-powered troubleshooting features. So have Dynatrace and Splunk. All three offer comprehensive &amp;quot;all-in-one&amp;quot; platforms that promise single-pane-of-glass visibility.&lt;/p&gt;&lt;p&gt;Mao distinguished Chronosphere&amp;#x27;s approach on technical grounds. &amp;quot;Early &amp;#x27;AI for observability&amp;#x27; leaned heavily on pattern-spotting and summarization, which tends to break down during real incidents,&amp;quot; he said. &amp;quot;These approaches often stop at correlating anomalies or producing fluent explanations without the deeper analysis and causal reasoning observability leaders need. They can feel impressive in demos but disappoint in production—they summarize signals rather than explain cause and effect.&amp;quot;&lt;/p&gt;&lt;p&gt;A specific technical gap, he argued, involves custom application telemetry. &amp;quot;Most platforms reason over standardized integrations—Kubernetes, common cloud services, popular databases—ignoring the most telling clues that live in custom app telemetry,&amp;quot; Mao said. &amp;quot;With an incomplete picture, large language models will &amp;#x27;fill in the gaps,&amp;#x27; producing confident-but-wrong guidance that sends teams down dead ends.&amp;quot;&lt;/p&gt;&lt;p&gt;Chronosphere&amp;#x27;s competitive positioning received validation in July when Gartner named it a Leader in the &lt;a href="https://www.gartner.com/en/documents/6688834"&gt;&lt;u&gt;2025 Magic Quadrant for Observability Platforms&lt;/u&gt;&lt;/a&gt; for the second consecutive year. The firm was recognized based on both &amp;quot;Completeness of Vision&amp;quot; and &amp;quot;Ability to Execute.&amp;quot; In December 2024, Chronosphere also tied for the highest overall rating among recognized vendors in Gartner Peer Insights&amp;#x27; &amp;quot;Voice of the Customer&amp;quot; report, scoring 4.7 out of 5 based on 70 reviews.&lt;/p&gt;&lt;p&gt;Yet the company faces intensifying competition for high-profile customers. UBS analysts noted in July that OpenAI now runs both Datadog and Chronosphere side-by-side to monitor GPU workloads, suggesting the AI leader is evaluating alternatives. While UBS maintained its buy rating on Datadog, the analysts warned that growing Chronosphere usage could pressure Datadog&amp;#x27;s pricing power.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the 84% cost reduction claims—and what CIOs should actually measure&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond technical capabilities, Chronosphere has built its market position on cost control — a critical factor as observability spending spirals. The company claims its platform reduces data volumes and associated costs by 84% on average while cutting critical incidents by up to 75%.&lt;/p&gt;&lt;p&gt;When pressed for specific customer examples with real numbers, Mao pointed to several case studies. &amp;quot;Robinhood has seen a 5x improvement in reliability and a 4x improvement in Mean Time to Detection,&amp;quot; he said. &amp;quot;DoorDash used Chronosphere to improve governance and standardize monitoring practices. Astronomer achieved over 85% cost reduction by shaping data on ingest, and Affirm scaled their load 10x during a Black Friday event with no issues, highlighting the platform&amp;#x27;s reliability under extreme conditions.&amp;quot;&lt;/p&gt;&lt;p&gt;The cost argument matters because, as &lt;a href="https://chronosphere.io/news/chronosphere-logs-raises-bar-in-observability/"&gt;&lt;u&gt;Paul Nashawaty&lt;/u&gt;&lt;/a&gt;, principal analyst at CUBE Research, noted when Chronosphere launched its Logs 2.0 product in June: &amp;quot;Organizations are drowning in telemetry data, with over 70% of observability spend going toward storing logs that are never queried.&amp;quot;&lt;/p&gt;&lt;p&gt;For CIOs fatigued by &amp;quot;AI-powered&amp;quot; announcements, Mao acknowledged skepticism is warranted. &amp;quot;The way to cut through it is to test whether the AI shortens incidents, reduces toil, and builds reusable knowledge in your own environment, not in a demo,&amp;quot; he advised. He recommended CIOs evaluate three factors: transparency and control (does the system show its reasoning?), coverage of custom telemetry (can it handle non-standardized data?), and manual toil avoided (how many ad-hoc queries and tool-switches are eliminated?).&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Chronosphere partners with five vendors instead of building everything itself&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Alongside the AI troubleshooting announcement, Chronosphere revealed a new &lt;a href="https://chronosphere.io/partners/"&gt;&lt;u&gt;Partner Program&lt;/u&gt;&lt;/a&gt; integrating five specialized vendors to fill gaps in its platform: Arize for large language model monitoring, Embrace for real user monitoring, Polar Signals for continuous profiling, Checkly for synthetic monitoring, and Rootly for incident management.&lt;/p&gt;&lt;p&gt;The strategy represents a deliberate bet against the all-in-one platforms dominating the market. &amp;quot;While an all-in-one platform may be sufficient for smaller organizations, global enterprises demand best-in-class depth across each domain,&amp;quot; Mao said. &amp;quot;This is what drove us to build our Partner Program and invest in seamless integrations with leading providers—so our customers can operate with confidence and clarity at every layer of observability.&amp;quot;&lt;/p&gt;&lt;p&gt;Noah Smolen, head of partnerships at Arize, said the collaboration addresses a specific enterprise need. &amp;quot;With a wide array of Fortune 500 customers, we understand the high bar needed to ensure AI agent systems are ready to deploy and stay incident-free, especially given the pace of AI adoption in the enterprise,&amp;quot; Smolen said. &amp;quot;Our partnership with Chronosphere comes at a time when an integrated purpose-built cloud-native and AI-observability suite solves a huge pain point for forward-thinking C-suite leaders who demand the very best across their entire observability stack.&amp;quot;&lt;/p&gt;&lt;p&gt;Similarly, JJ Tang, CEO and founder of Rootly, emphasized the incident resolution benefits. &amp;quot;Incidents hinder innovation and revenue, and the challenge lies in sifting through vast amounts of observability data, mobilizing teams, and resolving issues quickly,&amp;quot; Tang said. &amp;quot;Integrating Chronosphere with Rootly allows engineers to collaborate with context and resolve issues faster within their existing communication channels, drastically reducing time to resolution and ultimately improving reliability—78% plus decreases in repeat Sev0 and Sev1 incidents.&amp;quot;&lt;/p&gt;&lt;p&gt;When asked how total costs compare when customers use multiple partner contracts versus a single platform, Mao acknowledged the current complexity. &amp;quot;At present, mutual customers typically maintain separate contracts unless they engage through a services partner or system integrator,&amp;quot; he said. However, he argued the economics still favor the composable approach: &amp;quot;Our combined technologies deliver exceptional value—in most circumstances at just a fraction of the price of a single-platform solution. Beyond the savings, customers gain a richer, more unified observability experience that unlocks deeper insights and greater efficiency, especially for large-scale environments.&amp;quot;&lt;/p&gt;&lt;p&gt;The company plans to streamline this over time. &amp;quot;As the ISV program matures, we&amp;#x27;re focused on delivering a more streamlined experience by transitioning to a single, unified contract that simplifies procurement and accelerates time to value,&amp;quot; Mao said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How two Uber engineers turned Halloween outages into a billion-dollar startup&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere&amp;#x27;s origins trace to 2019, when Mao and co-founder Rob Skillington left Uber after building the ride-hailing giant&amp;#x27;s internal observability platform. At Uber, Mao&amp;#x27;s team had faced a crisis: the company&amp;#x27;s in-house tools would fail on its two busiest nights — Halloween and New Year&amp;#x27;s Eve — cutting off visibility into whether customers could request rides or drivers could locate passengers.&lt;/p&gt;&lt;p&gt;The solution they built at Uber used open-source software and ultimately allowed the company to operate without outages, even during high-volume events. But the broader market insight came at an industry conference in December 2018, when major cloud providers threw their weight behind Kubernetes, Google&amp;#x27;s container orchestration technology.&lt;/p&gt;&lt;p&gt;&amp;quot;This meant that most technology architectures were eventually going to look like Uber&amp;#x27;s,&amp;quot; Mao recalled in an &lt;a href="https://greylock.com/greymatter/chronosphere-is-making-the-observability-platform-built-for-control/"&gt;&lt;u&gt;August 2024 profile by Greylock Partners&lt;/u&gt;&lt;/a&gt;, Chronosphere&amp;#x27;s lead investor. &amp;quot;And that meant every company, not just a few big tech companies and the Walmarts of the world, would have the exact same problem we had solved at Uber.&amp;quot;&lt;/p&gt;&lt;p&gt;Chronosphere has since raised more than &lt;a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"&gt;&lt;u&gt;$343 million in funding&lt;/u&gt;&lt;/a&gt; across multiple rounds led by Greylock, Lux Capital, General Atlantic, Addition, and Founders Fund. The company operates as a remote-first organization with offices in New York, Austin, Boston, San Francisco, and Seattle, employing approximately 299 people according to LinkedIn data.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s customer base includes &lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.zillow.com"&gt;&lt;u&gt;Zillow&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.snap.com/"&gt;&lt;u&gt;Snap&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://robinhood.com/us/en/"&gt;&lt;u&gt;Robinhood&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.affirm.com/"&gt;&lt;u&gt;Affirm&lt;/u&gt;&lt;/a&gt; — predominantly high-growth technology companies operating cloud-native, Kubernetes-based infrastructures at massive scale.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What&amp;#x27;s available now—and what enterprises can expect in 2026&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere&amp;#x27;s &lt;a href="https://chronosphere.io/news/ai-guided-troubleshooting-redefines-observability/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt; capabilities, including Suggestions and Investigation Notebooks, entered limited availability Monday with select customers. The company plans full general availability in 2026. The &lt;a href="https://docs.chronosphere.io/integrate/mcp-server"&gt;&lt;u&gt;Model Context Protocol (MCP) Server&lt;/u&gt;&lt;/a&gt;, which enables engineers to integrate Chronosphere directly into internal AI workflows and query observability data through AI-enabled development environments, is available immediately for all Chronosphere customers.&lt;/p&gt;&lt;p&gt;The phased rollout reflects the company&amp;#x27;s cautious approach to deploying AI in production environments where mistakes carry real costs. By gathering feedback from early adopters before broad release, Chronosphere aims to refine its guidance algorithms and validate that its suggestions genuinely accelerate troubleshooting rather than simply generating impressive demonstrations.&lt;/p&gt;&lt;p&gt;The longer game, however, extends beyond individual product features. Chronosphere&amp;#x27;s dual bet — on transparent AI that shows its reasoning and on a partner ecosystem rather than all-in-one integration — amounts to a fundamental thesis about how enterprise observability will evolve as systems grow more complex.&lt;/p&gt;&lt;p&gt;If that thesis proves correct, the company that solves observability for the AI age won&amp;#x27;t be the one with the most automated black box. It will be the one that earns engineers&amp;#x27; trust by explaining what it knows, admitting what it doesn&amp;#x27;t, and letting humans make the final call. In an industry drowning in data and promised silver bullets, Chronosphere is wagering that showing your work still matters — even when AI is doing the math.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://chronosphere.io/"&gt;&lt;u&gt;Chronosphere&lt;/u&gt;&lt;/a&gt;, a New York-based observability startup &lt;a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"&gt;&lt;u&gt;valued at $1.6 billion&lt;/u&gt;&lt;/a&gt;, announced Monday it will launch &lt;a href="https://chronosphere.io/ai-guided-troubleshooting/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt; capabilities designed to help engineers diagnose and fix production software failures — a problem that has intensified as artificial intelligence tools accelerate code creation while making systems harder to debug.&lt;/p&gt;&lt;p&gt;The new features combine AI-driven analysis with what Chronosphere calls a &lt;a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"&gt;&lt;u&gt;Temporal Knowledge Graph&lt;/u&gt;&lt;/a&gt;, a continuously updated map of an organization&amp;#x27;s services, infrastructure dependencies, and system changes over time. The technology aims to address a mounting challenge in enterprise software: developers are writing code faster than ever with AI assistance, but troubleshooting remains largely manual, creating bottlenecks when applications fail.&lt;/p&gt;&lt;p&gt;&amp;quot;For AI to be effective in observability, it needs more than pattern recognition and summarization,&amp;quot; said Martin Mao, Chronosphere&amp;#x27;s CEO and co-founder, in an exclusive interview with VentureBeat. &amp;quot;Chronosphere has spent years building the data foundation and analytical depth needed for AI to actually help engineers. With our Temporal Knowledge Graph and advanced analytics capabilities, we&amp;#x27;re giving AI the understanding it needs to make observability truly intelligent — and giving engineers the confidence to trust its guidance.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement comes as the observability market — software that monitors complex cloud applications— faces mounting pressure to justify escalating costs. Enterprise log data volumes have grown &lt;a href="https://chronosphere.io/learn/observability-log-data-trends/"&gt;&lt;u&gt;250% year-over-year&lt;/u&gt;&lt;/a&gt;, according to Chronosphere&amp;#x27;s own research, while a study from MIT and the University of Pennsylvania found that generative &lt;a href="https://economics.mit.edu/sites/default/files/inline-files/draft_copilot_experiments.pdf"&gt;&lt;u&gt;AI has spurred a 13.5% increase in weekly code commits&lt;/u&gt;&lt;/a&gt;, signifying faster development velocity but also greater system complexity.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI writes code 13% faster, but debugging stays stubbornly manual&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite advances in automated code generation, debugging production failures remains stubbornly manual. When a major e-commerce site slows during checkout or a banking app fails to process transactions, engineers must sift through millions of data points — server logs, application traces, infrastructure metrics, recent code deployments — to identify root causes.&lt;/p&gt;&lt;p&gt;Chronosphere&amp;#x27;s answer is what it calls &lt;a href="https://chronosphere.io/ai-guided-troubleshooting/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt;, built on four core capabilities: automated &amp;quot;Suggestions&amp;quot; that propose investigation paths backed by data; the Temporal Knowledge Graph that maps system relationships and changes; Investigation Notebooks that document each troubleshooting step for future reference; and natural language query building.&lt;/p&gt;&lt;p&gt;Mao explained the &lt;a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"&gt;&lt;u&gt;Temporal Knowledge Graph&lt;/u&gt;&lt;/a&gt; in practical terms: &amp;quot;It&amp;#x27;s a living, time-aware model of your system. It stitches together telemetry—metrics, traces, logs—infrastructure context, change events like deploys and feature flags, and even human input like notes and runbooks into a single, queryable map that updates as your system evolves.&amp;quot;&lt;/p&gt;&lt;p&gt;This differs fundamentally from the service dependency maps offered by competitors like &lt;a href="https://www.datadoghq.com/dg/monitor/free-trial/?utm_source=google&amp;amp;utm_medium=paid-search&amp;amp;utm_campaign=dg-brand-ww&amp;amp;utm_keyword=datadog&amp;amp;utm_matchtype=b&amp;amp;igaag=95325237782&amp;amp;igaat=&amp;amp;igacm=9551169254&amp;amp;igacr=673270769690&amp;amp;igakw=datadog&amp;amp;igamt=b&amp;amp;igant=g&amp;amp;utm_campaignid=9551169254&amp;amp;utm_adgroupid=95325237782&amp;amp;gad_source=1&amp;amp;gad_campaignid=9551169254&amp;amp;gbraid=0AAAAADFY9NlOpf6xdtUzDLzD2BUR67UTl&amp;amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6T3Vk9lkeno-VLHlyRmDR0PFF8gTUGxe72EBr8QGbSpTNY5qtp63eRoC25kQAvD_BwE"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.dynatrace.com/"&gt;&lt;u&gt;Dynatrace&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.splunk.com/"&gt;&lt;u&gt;Splunk&lt;/u&gt;&lt;/a&gt;, Mao argued. &amp;quot;It adds time, not just topology,&amp;quot; he said. &amp;quot;It tracks how services and dependencies change over time and connects those changes to incidents—what changed and why. Many tools rely on standardized integrations; our graph goes a step further to normalize custom, non-standard telemetry so application-specific signals aren&amp;#x27;t a blind spot.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Chronosphere shows its work instead of making automatic decisions&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Unlike purely automated systems, &lt;a href="https://chronosphere.io/"&gt;&lt;u&gt;Chronosphere&lt;/u&gt;&lt;/a&gt; designed its AI features to keep engineers in the driver&amp;#x27;s seat—a deliberate choice meant to address what Mao calls the &amp;quot;confident-but-wrong guidance&amp;quot; problem plaguing early AI observability tools.&lt;/p&gt;&lt;p&gt;&amp;quot;&amp;#x27;Keeping engineers in control&amp;#x27; means the AI shows its work, proposes next steps, and lets engineers verify or override — never auto-deciding behind the scenes,&amp;quot; Mao explained. &amp;quot;Every Suggestion includes the evidence—timing, dependencies, error patterns — and a &amp;#x27;Why was this suggested?&amp;#x27; view, so they can inspect what was checked and ruled out before acting.&amp;quot;&lt;/p&gt;&lt;p&gt;He walked through a concrete example: &amp;quot;An SLO [service level objective] alert fires on Checkout. Chronosphere immediately surfaces a ranked Suggestion: errors appear to have started in the dependent Payment service. An engineer can click Investigate to see the charts and reasoning and, if it holds up, choose to dig deeper. As they steer into Payment, the system adapts with new Suggestions scoped to that service—all from one view, no tab-hopping.&amp;quot;&lt;/p&gt;&lt;p&gt;In this scenario, the engineer asks &amp;quot;what changed?&amp;quot; and the system pulls in change events. &amp;quot;Our Notebook capability makes the causal chain plain: a feature-flag update preceded pod memory exhaustion in Payment; Checkout&amp;#x27;s spike is a downstream symptom,&amp;quot; Mao said. &amp;quot;They can decide to roll back the flag. That whole path — suggestions followed, evidence viewed, conclusions—is captured automatically in an Investigation Notebook, and the outcome feeds the Temporal Knowledge Graph so similar future incidents are faster to resolve.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How a $1.6 billion startup takes on Datadog, Dynatrace, and Splunk&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere enters an increasingly crowded field. &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt;, the publicly traded observability leader valued at over $40 billion, has introduced its own AI-powered troubleshooting features. So have Dynatrace and Splunk. All three offer comprehensive &amp;quot;all-in-one&amp;quot; platforms that promise single-pane-of-glass visibility.&lt;/p&gt;&lt;p&gt;Mao distinguished Chronosphere&amp;#x27;s approach on technical grounds. &amp;quot;Early &amp;#x27;AI for observability&amp;#x27; leaned heavily on pattern-spotting and summarization, which tends to break down during real incidents,&amp;quot; he said. &amp;quot;These approaches often stop at correlating anomalies or producing fluent explanations without the deeper analysis and causal reasoning observability leaders need. They can feel impressive in demos but disappoint in production—they summarize signals rather than explain cause and effect.&amp;quot;&lt;/p&gt;&lt;p&gt;A specific technical gap, he argued, involves custom application telemetry. &amp;quot;Most platforms reason over standardized integrations—Kubernetes, common cloud services, popular databases—ignoring the most telling clues that live in custom app telemetry,&amp;quot; Mao said. &amp;quot;With an incomplete picture, large language models will &amp;#x27;fill in the gaps,&amp;#x27; producing confident-but-wrong guidance that sends teams down dead ends.&amp;quot;&lt;/p&gt;&lt;p&gt;Chronosphere&amp;#x27;s competitive positioning received validation in July when Gartner named it a Leader in the &lt;a href="https://www.gartner.com/en/documents/6688834"&gt;&lt;u&gt;2025 Magic Quadrant for Observability Platforms&lt;/u&gt;&lt;/a&gt; for the second consecutive year. The firm was recognized based on both &amp;quot;Completeness of Vision&amp;quot; and &amp;quot;Ability to Execute.&amp;quot; In December 2024, Chronosphere also tied for the highest overall rating among recognized vendors in Gartner Peer Insights&amp;#x27; &amp;quot;Voice of the Customer&amp;quot; report, scoring 4.7 out of 5 based on 70 reviews.&lt;/p&gt;&lt;p&gt;Yet the company faces intensifying competition for high-profile customers. UBS analysts noted in July that OpenAI now runs both Datadog and Chronosphere side-by-side to monitor GPU workloads, suggesting the AI leader is evaluating alternatives. While UBS maintained its buy rating on Datadog, the analysts warned that growing Chronosphere usage could pressure Datadog&amp;#x27;s pricing power.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the 84% cost reduction claims—and what CIOs should actually measure&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond technical capabilities, Chronosphere has built its market position on cost control — a critical factor as observability spending spirals. The company claims its platform reduces data volumes and associated costs by 84% on average while cutting critical incidents by up to 75%.&lt;/p&gt;&lt;p&gt;When pressed for specific customer examples with real numbers, Mao pointed to several case studies. &amp;quot;Robinhood has seen a 5x improvement in reliability and a 4x improvement in Mean Time to Detection,&amp;quot; he said. &amp;quot;DoorDash used Chronosphere to improve governance and standardize monitoring practices. Astronomer achieved over 85% cost reduction by shaping data on ingest, and Affirm scaled their load 10x during a Black Friday event with no issues, highlighting the platform&amp;#x27;s reliability under extreme conditions.&amp;quot;&lt;/p&gt;&lt;p&gt;The cost argument matters because, as &lt;a href="https://chronosphere.io/news/chronosphere-logs-raises-bar-in-observability/"&gt;&lt;u&gt;Paul Nashawaty&lt;/u&gt;&lt;/a&gt;, principal analyst at CUBE Research, noted when Chronosphere launched its Logs 2.0 product in June: &amp;quot;Organizations are drowning in telemetry data, with over 70% of observability spend going toward storing logs that are never queried.&amp;quot;&lt;/p&gt;&lt;p&gt;For CIOs fatigued by &amp;quot;AI-powered&amp;quot; announcements, Mao acknowledged skepticism is warranted. &amp;quot;The way to cut through it is to test whether the AI shortens incidents, reduces toil, and builds reusable knowledge in your own environment, not in a demo,&amp;quot; he advised. He recommended CIOs evaluate three factors: transparency and control (does the system show its reasoning?), coverage of custom telemetry (can it handle non-standardized data?), and manual toil avoided (how many ad-hoc queries and tool-switches are eliminated?).&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Chronosphere partners with five vendors instead of building everything itself&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Alongside the AI troubleshooting announcement, Chronosphere revealed a new &lt;a href="https://chronosphere.io/partners/"&gt;&lt;u&gt;Partner Program&lt;/u&gt;&lt;/a&gt; integrating five specialized vendors to fill gaps in its platform: Arize for large language model monitoring, Embrace for real user monitoring, Polar Signals for continuous profiling, Checkly for synthetic monitoring, and Rootly for incident management.&lt;/p&gt;&lt;p&gt;The strategy represents a deliberate bet against the all-in-one platforms dominating the market. &amp;quot;While an all-in-one platform may be sufficient for smaller organizations, global enterprises demand best-in-class depth across each domain,&amp;quot; Mao said. &amp;quot;This is what drove us to build our Partner Program and invest in seamless integrations with leading providers—so our customers can operate with confidence and clarity at every layer of observability.&amp;quot;&lt;/p&gt;&lt;p&gt;Noah Smolen, head of partnerships at Arize, said the collaboration addresses a specific enterprise need. &amp;quot;With a wide array of Fortune 500 customers, we understand the high bar needed to ensure AI agent systems are ready to deploy and stay incident-free, especially given the pace of AI adoption in the enterprise,&amp;quot; Smolen said. &amp;quot;Our partnership with Chronosphere comes at a time when an integrated purpose-built cloud-native and AI-observability suite solves a huge pain point for forward-thinking C-suite leaders who demand the very best across their entire observability stack.&amp;quot;&lt;/p&gt;&lt;p&gt;Similarly, JJ Tang, CEO and founder of Rootly, emphasized the incident resolution benefits. &amp;quot;Incidents hinder innovation and revenue, and the challenge lies in sifting through vast amounts of observability data, mobilizing teams, and resolving issues quickly,&amp;quot; Tang said. &amp;quot;Integrating Chronosphere with Rootly allows engineers to collaborate with context and resolve issues faster within their existing communication channels, drastically reducing time to resolution and ultimately improving reliability—78% plus decreases in repeat Sev0 and Sev1 incidents.&amp;quot;&lt;/p&gt;&lt;p&gt;When asked how total costs compare when customers use multiple partner contracts versus a single platform, Mao acknowledged the current complexity. &amp;quot;At present, mutual customers typically maintain separate contracts unless they engage through a services partner or system integrator,&amp;quot; he said. However, he argued the economics still favor the composable approach: &amp;quot;Our combined technologies deliver exceptional value—in most circumstances at just a fraction of the price of a single-platform solution. Beyond the savings, customers gain a richer, more unified observability experience that unlocks deeper insights and greater efficiency, especially for large-scale environments.&amp;quot;&lt;/p&gt;&lt;p&gt;The company plans to streamline this over time. &amp;quot;As the ISV program matures, we&amp;#x27;re focused on delivering a more streamlined experience by transitioning to a single, unified contract that simplifies procurement and accelerates time to value,&amp;quot; Mao said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How two Uber engineers turned Halloween outages into a billion-dollar startup&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere&amp;#x27;s origins trace to 2019, when Mao and co-founder Rob Skillington left Uber after building the ride-hailing giant&amp;#x27;s internal observability platform. At Uber, Mao&amp;#x27;s team had faced a crisis: the company&amp;#x27;s in-house tools would fail on its two busiest nights — Halloween and New Year&amp;#x27;s Eve — cutting off visibility into whether customers could request rides or drivers could locate passengers.&lt;/p&gt;&lt;p&gt;The solution they built at Uber used open-source software and ultimately allowed the company to operate without outages, even during high-volume events. But the broader market insight came at an industry conference in December 2018, when major cloud providers threw their weight behind Kubernetes, Google&amp;#x27;s container orchestration technology.&lt;/p&gt;&lt;p&gt;&amp;quot;This meant that most technology architectures were eventually going to look like Uber&amp;#x27;s,&amp;quot; Mao recalled in an &lt;a href="https://greylock.com/greymatter/chronosphere-is-making-the-observability-platform-built-for-control/"&gt;&lt;u&gt;August 2024 profile by Greylock Partners&lt;/u&gt;&lt;/a&gt;, Chronosphere&amp;#x27;s lead investor. &amp;quot;And that meant every company, not just a few big tech companies and the Walmarts of the world, would have the exact same problem we had solved at Uber.&amp;quot;&lt;/p&gt;&lt;p&gt;Chronosphere has since raised more than &lt;a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"&gt;&lt;u&gt;$343 million in funding&lt;/u&gt;&lt;/a&gt; across multiple rounds led by Greylock, Lux Capital, General Atlantic, Addition, and Founders Fund. The company operates as a remote-first organization with offices in New York, Austin, Boston, San Francisco, and Seattle, employing approximately 299 people according to LinkedIn data.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s customer base includes &lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.zillow.com"&gt;&lt;u&gt;Zillow&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.snap.com/"&gt;&lt;u&gt;Snap&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://robinhood.com/us/en/"&gt;&lt;u&gt;Robinhood&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.affirm.com/"&gt;&lt;u&gt;Affirm&lt;/u&gt;&lt;/a&gt; — predominantly high-growth technology companies operating cloud-native, Kubernetes-based infrastructures at massive scale.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What&amp;#x27;s available now—and what enterprises can expect in 2026&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere&amp;#x27;s &lt;a href="https://chronosphere.io/news/ai-guided-troubleshooting-redefines-observability/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt; capabilities, including Suggestions and Investigation Notebooks, entered limited availability Monday with select customers. The company plans full general availability in 2026. The &lt;a href="https://docs.chronosphere.io/integrate/mcp-server"&gt;&lt;u&gt;Model Context Protocol (MCP) Server&lt;/u&gt;&lt;/a&gt;, which enables engineers to integrate Chronosphere directly into internal AI workflows and query observability data through AI-enabled development environments, is available immediately for all Chronosphere customers.&lt;/p&gt;&lt;p&gt;The phased rollout reflects the company&amp;#x27;s cautious approach to deploying AI in production environments where mistakes carry real costs. By gathering feedback from early adopters before broad release, Chronosphere aims to refine its guidance algorithms and validate that its suggestions genuinely accelerate troubleshooting rather than simply generating impressive demonstrations.&lt;/p&gt;&lt;p&gt;The longer game, however, extends beyond individual product features. Chronosphere&amp;#x27;s dual bet — on transparent AI that shows its reasoning and on a partner ecosystem rather than all-in-one integration — amounts to a fundamental thesis about how enterprise observability will evolve as systems grow more complex.&lt;/p&gt;&lt;p&gt;If that thesis proves correct, the company that solves observability for the AI age won&amp;#x27;t be the one with the most automated black box. It will be the one that earns engineers&amp;#x27; trust by explaining what it knows, admitting what it doesn&amp;#x27;t, and letting humans make the final call. In an industry drowning in data and promised silver bullets, Chronosphere is wagering that showing your work still matters — even when AI is doing the math.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/chronosphere-takes-on-datadog-with-ai-that-explains-itself-not-just-outages</guid><pubDate>Mon, 10 Nov 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] Google brings Gemini to the Google TV Streamer (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/google-brings-gemini-to-the-google-tv-streamer/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/Google-TV-Streamer-set-up.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Monday that it’s starting to roll out Gemini to the Google TV Streamer, replacing Google Assistant. The tech giant says the change will enable users to use their voice more naturally to access content and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, users can now ask something like, “I like dramas but my wife likes comedies. What’s a movie we can watch together?” when looking for movie recommendations. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, users could quickly catch up on a show they’re returning to by asking something like, “What happened at the end of Outlander last season?” In another example, Google says users can even ask something like “What’s the new hospital drama everyone’s talking about?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says Gemini for TV goes beyond entertainment, as users can ask the AI assistant any other type of question as well, just like they can with Gemini on their phone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, users could bring learning to their TV by asking Gemini to “Explain why volcanoes erupt to my third grader.” Gemini can also guide users through DIY projects or recipes with YouTube videos, Google says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access Gemini on the Google TV Streamer, you need to press your remote’s microphone button. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the update is rolling out “over the next few weeks” to users aged 18 and older. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as Google announced back in September that it was introducing Gemini for Google TV to select TCL devices. The company said at the time that later in the year, Gemini would be arriving on the 2025 Hisense U7, U8, and UX models, and 2025 TCL QM7K, QM8K, and X11K models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the Google TV Streamer, Gemini is also available on the Walmart Onn 4K Pro streaming device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Monday’s announcement doesn’t come as a surprise, as the change is part of Google’s plans to replace Google Assistant with Gemini across all of its devices and platforms. Plus, the company had announced at CES in January that Gemini would be coming to Google TV this year.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/Google-TV-Streamer-set-up.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Monday that it’s starting to roll out Gemini to the Google TV Streamer, replacing Google Assistant. The tech giant says the change will enable users to use their voice more naturally to access content and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, users can now ask something like, “I like dramas but my wife likes comedies. What’s a movie we can watch together?” when looking for movie recommendations. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, users could quickly catch up on a show they’re returning to by asking something like, “What happened at the end of Outlander last season?” In another example, Google says users can even ask something like “What’s the new hospital drama everyone’s talking about?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says Gemini for TV goes beyond entertainment, as users can ask the AI assistant any other type of question as well, just like they can with Gemini on their phone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, users could bring learning to their TV by asking Gemini to “Explain why volcanoes erupt to my third grader.” Gemini can also guide users through DIY projects or recipes with YouTube videos, Google says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access Gemini on the Google TV Streamer, you need to press your remote’s microphone button. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the update is rolling out “over the next few weeks” to users aged 18 and older. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as Google announced back in September that it was introducing Gemini for Google TV to select TCL devices. The company said at the time that later in the year, Gemini would be arriving on the 2025 Hisense U7, U8, and UX models, and 2025 TCL QM7K, QM8K, and X11K models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the Google TV Streamer, Gemini is also available on the Walmart Onn 4K Pro streaming device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Monday’s announcement doesn’t come as a surprise, as the change is part of Google’s plans to replace Google Assistant with Gemini across all of its devices and platforms. Plus, the company had announced at CES in January that Gemini would be coming to Google TV this year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/google-brings-gemini-to-the-google-tv-streamer/</guid><pubDate>Mon, 10 Nov 2025 19:28:57 +0000</pubDate></item><item><title>[NEW] Meta returns to open source AI with Omnilingual ASR models that can transcribe 1,600+ languages natively (AI | VentureBeat)</title><link>https://venturebeat.com/ai/meta-returns-to-open-source-ai-with-omnilingual-asr-models-that-can</link><description>[unable to retrieve full-text content]&lt;p&gt;Meta has just released a new &lt;a href="https://ai.meta.com/blog/omnilingual-asr"&gt;multilingual automatic speech recognition (ASR) system&lt;/a&gt; supporting 1,600+ languages — dwarfing OpenAI’s open source Whisper model, which supports just 99. &lt;/p&gt;&lt;p&gt;Is architecture also allows developers to extend that support to thousands more. Through a feature called zero-shot in-context learning, users can provide a few paired examples of audio and text in a new language at inference time, enabling the model to transcribe additional utterances in that language without any retraining.&lt;/p&gt;&lt;p&gt;In practice, this expands potential coverage to more than 5,400 languages — roughly every spoken language with a known script.&lt;/p&gt;&lt;p&gt;It’s a shift from static model capabilities to a flexible framework that communities can adapt themselves. So while the 1,600 languages reflect official training coverage, the broader figure represents Omnilingual ASR’s capacity to generalize on demand, making it the most extensible speech recognition system released to date.&lt;/p&gt;&lt;p&gt;Best of all: it&amp;#x27;s been open sourced under&lt;a href="https://github.com/facebookresearch/omnilingual-asr?tab=License-1-ov-file#readme"&gt; a plain Apache 2.0 license&lt;/a&gt; — not a restrictive, quasi open-source Llama license like the company&amp;#x27;s prior releases, which limited use by larger enterprises unless they paid licensing fees — meaning researchers and developers are free to take and implement it right away, for free, without restrictions, even in commercial and enterprise-grade projects!&lt;/p&gt;&lt;p&gt;Released on November 10 on &lt;a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/"&gt;Meta&amp;#x27;s website&lt;/a&gt;, &lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;Github&lt;/a&gt;, along with a &lt;a href="https://huggingface.co/spaces/facebook/omniasr-transcriptions"&gt;demo space on Hugging Face&lt;/a&gt; and &lt;a href="https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/"&gt;technical paper&lt;/a&gt;, Meta’s Omnilingual ASR suite includes a family of speech recognition models, a 7-billion parameter multilingual audio representation model, and a massive speech corpus spanning over 350 previously underserved languages. &lt;/p&gt;&lt;p&gt;All resources are freely available under open licenses, and the models support speech-to-text transcription out of the box.&lt;/p&gt;&lt;p&gt;“By open sourcing these models and dataset, we aim to break down language barriers, expand digital access, and empower communities worldwide,” Meta posted on its &lt;a href="https://x.com/AIatMeta/status/1987957744138416389"&gt;@AIatMeta account on X&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Designed for Speech-to-Text Transcription&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At its core, Omnilingual ASR is a speech-to-text system. &lt;/p&gt;&lt;p&gt;The models are trained to convert spoken language into written text, supporting applications like voice assistants, transcription tools, subtitles, oral archive digitization, and accessibility features for low-resource languages.&lt;/p&gt;&lt;p&gt;Unlike earlier ASR models that required extensive labeled training data, Omnilingual ASR includes a zero-shot variant. &lt;/p&gt;&lt;p&gt;This version can transcribe languages it has never seen before—using just a few paired examples of audio and corresponding text. &lt;/p&gt;&lt;p&gt;This lowers the barrier for adding new or endangered languages dramatically, removing the need for large corpora or retraining.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Model Family and Technical Design&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Omnilingual ASR suite includes multiple model families trained on more than 4.3 million hours of audio from 1,600+ languages:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;wav2vec 2.0 models for self-supervised speech representation learning (300M–7B parameters)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CTC-based ASR models for efficient supervised transcription&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LLM-ASR models combining a speech encoder with a Transformer-based text decoder for state-of-the-art transcription&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LLM-ZeroShot ASR model, enabling inference-time adaptation to unseen languages&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All models follow an encoder–decoder design: raw audio is converted into a language-agnostic representation, then decoded into written text.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why the Scale Matters&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While Whisper and similar models have advanced ASR capabilities for global languages, they fall short on the long tail of human linguistic diversity. Whisper supports 99 languages. Meta’s system:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Directly supports 1,600+ languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Can generalize to 5,400+ languages using in-context learning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Achieves character error rates (CER) under 10% in 78% of supported languages&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Among those supported are more than 500 languages never previously covered by any ASR model, according to Meta’s research paper.&lt;/p&gt;&lt;p&gt;This expansion opens new possibilities for communities whose languages are often excluded from digital tools&lt;/p&gt;&lt;p&gt;Here’s the revised and expanded background section, integrating the broader context of Meta’s 2025 AI strategy, leadership changes, and Llama 4’s reception, complete with in-text citations and links:&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Background: Meta’s AI Overhaul and a Rebound from Llama 4&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of Omnilingual ASR arrives at a pivotal moment in Meta’s AI strategy, following a year marked by organizational turbulence, leadership changes, and uneven product execution. &lt;/p&gt;&lt;p&gt;Omnilingual ASR is the first major open-source model release since the rollout of Llama 4, Meta’s latest large language model, which&lt;a href="https://ftr.bazqux.com/epseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way"&gt; debuted in April 2025&lt;/a&gt; to &lt;a href="https://venturebeat.com/ai/meta-defends-llama-4-release-against-reports-of-mixed-quality-blames-bugs"&gt;mixed and ultimately poor reviews&lt;/a&gt;, with scant enterprise adoption compared to Chinese open source model competitors.&lt;/p&gt;&lt;p&gt;The failure led Meta founder and CEO Mark Zuckerberg to appoint Alexandr Wang, co-founder and prior CEO of AI data supplier Scale AI, &lt;a href="https://www.cnbc.com/2025/06/10/zuckerberg-makes-metas-biggest-bet-on-ai-14-billion-scale-ai-deal.html"&gt;as Chief AI Officer&lt;/a&gt;, and embark on an &lt;a href="https://www.wired.com/story/meta-poaches-openai-researcher-yang-song/"&gt;extensive and costly hiring spree&lt;/a&gt; that shocked the AI and business communities with &lt;a href="https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/"&gt;eye-watering pay packages for top AI researchers&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In contrast, Omnilingual ASR represents a strategic and reputational reset. It returns Meta to a domain where the company has historically led — multilingual AI — and offers a truly extensible, community-oriented stack with minimal barriers to entry. &lt;/p&gt;&lt;p&gt;The system’s support for 1,600+ languages and its extensibility to over 5,000 more via zero-shot in-context learning reassert Meta’s engineering credibility in language technology. &lt;/p&gt;&lt;p&gt;Importantly, it does so through a free and permissively licensed release, under Apache 2.0, with transparent dataset sourcing and reproducible training protocols.&lt;/p&gt;&lt;p&gt;This shift aligns with broader themes in Meta’s 2025 strategy. The company has refocused its narrative around a “personal superintelligence” vision, investing heavily in infrastructure (including a September release of custom AI accelerators and Arm-based inference stacks) &lt;a href="https://engineering.fb.com/2025/09/22/data-infrastructure/meta-custom-accelerators/"&gt;source&lt;/a&gt; while downplaying the metaverse in favor of foundational AI capabilities. The return to public training data in Europe after a regulatory pause also underscores its intention to compete globally, despite privacy scrutiny &lt;a href="https://apnews.com/article/meta-eu-data-training-2025"&gt;source&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Omnilingual ASR, then, is more than a model release — it’s a calculated move to reassert control of the narrative: from the fragmented rollout of Llama 4 to a high-utility, research-grounded contribution that aligns with Meta’s long-term AI platform strategy.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community-Centered Dataset Collection&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To achieve this scale, Meta partnered with researchers and community organizations in Africa, Asia, and elsewhere to create the Omnilingual ASR Corpus, a 3,350-hour dataset across 348 low-resource languages. Contributors were compensated local speakers, and recordings were gathered in collaboration with groups like:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;African Next Voices&lt;/b&gt;: A Gates Foundation–supported consortium including Maseno University (Kenya), University of Pretoria, and Data Science Nigeria&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mozilla Foundation’s Common Voice&lt;/b&gt;, supported through the Open Multilingual Speech Fund&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Lanfrica / NaijaVoices&lt;/b&gt;, which created data for 11 African languages including Igala, Serer, and Urhobo&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The data collection focused on natural, unscripted speech. Prompts were designed to be culturally relevant and open-ended, such as “Is it better to have a few close friends or many casual acquaintances? Why?” Transcriptions used established writing systems, with quality assurance built into every step.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance and Hardware Considerations&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The largest model in the suite, the omniASR_LLM_7B, requires ~17GB of GPU memory for inference, making it suitable for deployment on high-end hardware. Smaller models (300M–1B) can run on lower-power devices and deliver real-time transcription speeds.&lt;/p&gt;&lt;p&gt;Performance benchmarks show strong results even in low-resource scenarios:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;CER &amp;lt;10% in 95% of high-resource and mid-resource languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CER &amp;lt;10% in 36% of low-resource languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Robustness in noisy conditions and unseen domains, especially with fine-tuning&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The zero-shot system, omniASR_LLM_7B_ZS, can transcribe new languages with minimal setup. Users provide a few sample audio–text pairs, and the model generates transcriptions for new utterances in the same language.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Open Access and Developer Tooling&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;All models and the dataset are licensed under permissive terms:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Apache 2.0&lt;/b&gt; for models and code&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;CC-BY 4.0&lt;/b&gt; for the &lt;a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus"&gt;Omnilingual ASR Corpus on HuggingFace&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Installation is supported via PyPI and uv:&lt;/p&gt;&lt;p&gt;&lt;code&gt;pip install omnilingual-asr&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Meta also provides:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A HuggingFace dataset integration&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Pre-built inference pipelines&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Language-code conditioning for improved accuracy&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Developers can view the full list of supported languages using the API:&lt;/p&gt;&lt;p&gt;&lt;code&gt;from omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;print(len(supported_langs))
print(supported_langs)&lt;/code&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Broader Implications&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Omnilingual ASR reframes language coverage in ASR from a fixed list to an &lt;b&gt;extensible framework&lt;/b&gt;. It enables:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Community-driven inclusion of underrepresented languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Digital access for oral and endangered languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Research on speech tech in linguistically diverse contexts&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Crucially, Meta emphasizes ethical considerations throughout—advocating for open-source participation and collaboration with native-speaking communities.&lt;/p&gt;&lt;p&gt;“No model can ever anticipate and include all of the world’s languages in advance,” the Omnilingual ASR paper states, “but Omnilingual ASR makes it possible for communities to extend recognition with their own data.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Access the Tools&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;All resources are now available at:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Code + Models&lt;/b&gt;: &lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;github.com/facebookresearch/omnilingual-asr&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dataset&lt;/b&gt;: &lt;a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus"&gt;huggingface.co/datasets/facebook/omnilingual-asr-corpus&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Blogpost&lt;/b&gt;: &lt;a href="https://ai.meta.com/blog/omnilingual-asr"&gt;ai.meta.com/blog/omnilingual-asr&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;What This Means for Enterprises&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For enterprise developers, especially those operating in multilingual or international markets, Omnilingual ASR significantly lowers the barrier to deploying speech-to-text systems across a broader range of customers and geographies. &lt;/p&gt;&lt;p&gt;Instead of relying on commercial ASR APIs that support only a narrow set of high-resource languages, teams can now integrate an open-source pipeline that covers over 1,600 languages out of the box—with the option to extend it to thousands more via zero-shot learning.&lt;/p&gt;&lt;p&gt;This flexibility is especially valuable for enterprises working in sectors like voice-based customer support, transcription services, accessibility, education, or civic technology, where local language coverage can be a competitive or regulatory necessity. Because the models are released under the permissive Apache 2.0 license, businesses can fine-tune, deploy, or integrate them into proprietary systems without restrictive terms.&lt;/p&gt;&lt;p&gt;It also represents a shift in the ASR landscape—from centralized, cloud-gated offerings to community-extendable infrastructure. By making multilingual speech recognition more accessible, customizable, and cost-effective, Omnilingual ASR opens the door to a new generation of enterprise speech applications built around linguistic inclusion rather than linguistic limitation.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Meta has just released a new &lt;a href="https://ai.meta.com/blog/omnilingual-asr"&gt;multilingual automatic speech recognition (ASR) system&lt;/a&gt; supporting 1,600+ languages — dwarfing OpenAI’s open source Whisper model, which supports just 99. &lt;/p&gt;&lt;p&gt;Is architecture also allows developers to extend that support to thousands more. Through a feature called zero-shot in-context learning, users can provide a few paired examples of audio and text in a new language at inference time, enabling the model to transcribe additional utterances in that language without any retraining.&lt;/p&gt;&lt;p&gt;In practice, this expands potential coverage to more than 5,400 languages — roughly every spoken language with a known script.&lt;/p&gt;&lt;p&gt;It’s a shift from static model capabilities to a flexible framework that communities can adapt themselves. So while the 1,600 languages reflect official training coverage, the broader figure represents Omnilingual ASR’s capacity to generalize on demand, making it the most extensible speech recognition system released to date.&lt;/p&gt;&lt;p&gt;Best of all: it&amp;#x27;s been open sourced under&lt;a href="https://github.com/facebookresearch/omnilingual-asr?tab=License-1-ov-file#readme"&gt; a plain Apache 2.0 license&lt;/a&gt; — not a restrictive, quasi open-source Llama license like the company&amp;#x27;s prior releases, which limited use by larger enterprises unless they paid licensing fees — meaning researchers and developers are free to take and implement it right away, for free, without restrictions, even in commercial and enterprise-grade projects!&lt;/p&gt;&lt;p&gt;Released on November 10 on &lt;a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/"&gt;Meta&amp;#x27;s website&lt;/a&gt;, &lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;Github&lt;/a&gt;, along with a &lt;a href="https://huggingface.co/spaces/facebook/omniasr-transcriptions"&gt;demo space on Hugging Face&lt;/a&gt; and &lt;a href="https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/"&gt;technical paper&lt;/a&gt;, Meta’s Omnilingual ASR suite includes a family of speech recognition models, a 7-billion parameter multilingual audio representation model, and a massive speech corpus spanning over 350 previously underserved languages. &lt;/p&gt;&lt;p&gt;All resources are freely available under open licenses, and the models support speech-to-text transcription out of the box.&lt;/p&gt;&lt;p&gt;“By open sourcing these models and dataset, we aim to break down language barriers, expand digital access, and empower communities worldwide,” Meta posted on its &lt;a href="https://x.com/AIatMeta/status/1987957744138416389"&gt;@AIatMeta account on X&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Designed for Speech-to-Text Transcription&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At its core, Omnilingual ASR is a speech-to-text system. &lt;/p&gt;&lt;p&gt;The models are trained to convert spoken language into written text, supporting applications like voice assistants, transcription tools, subtitles, oral archive digitization, and accessibility features for low-resource languages.&lt;/p&gt;&lt;p&gt;Unlike earlier ASR models that required extensive labeled training data, Omnilingual ASR includes a zero-shot variant. &lt;/p&gt;&lt;p&gt;This version can transcribe languages it has never seen before—using just a few paired examples of audio and corresponding text. &lt;/p&gt;&lt;p&gt;This lowers the barrier for adding new or endangered languages dramatically, removing the need for large corpora or retraining.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Model Family and Technical Design&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Omnilingual ASR suite includes multiple model families trained on more than 4.3 million hours of audio from 1,600+ languages:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;wav2vec 2.0 models for self-supervised speech representation learning (300M–7B parameters)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CTC-based ASR models for efficient supervised transcription&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LLM-ASR models combining a speech encoder with a Transformer-based text decoder for state-of-the-art transcription&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LLM-ZeroShot ASR model, enabling inference-time adaptation to unseen languages&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All models follow an encoder–decoder design: raw audio is converted into a language-agnostic representation, then decoded into written text.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why the Scale Matters&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While Whisper and similar models have advanced ASR capabilities for global languages, they fall short on the long tail of human linguistic diversity. Whisper supports 99 languages. Meta’s system:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Directly supports 1,600+ languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Can generalize to 5,400+ languages using in-context learning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Achieves character error rates (CER) under 10% in 78% of supported languages&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Among those supported are more than 500 languages never previously covered by any ASR model, according to Meta’s research paper.&lt;/p&gt;&lt;p&gt;This expansion opens new possibilities for communities whose languages are often excluded from digital tools&lt;/p&gt;&lt;p&gt;Here’s the revised and expanded background section, integrating the broader context of Meta’s 2025 AI strategy, leadership changes, and Llama 4’s reception, complete with in-text citations and links:&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Background: Meta’s AI Overhaul and a Rebound from Llama 4&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of Omnilingual ASR arrives at a pivotal moment in Meta’s AI strategy, following a year marked by organizational turbulence, leadership changes, and uneven product execution. &lt;/p&gt;&lt;p&gt;Omnilingual ASR is the first major open-source model release since the rollout of Llama 4, Meta’s latest large language model, which&lt;a href="https://ftr.bazqux.com/epseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way"&gt; debuted in April 2025&lt;/a&gt; to &lt;a href="https://venturebeat.com/ai/meta-defends-llama-4-release-against-reports-of-mixed-quality-blames-bugs"&gt;mixed and ultimately poor reviews&lt;/a&gt;, with scant enterprise adoption compared to Chinese open source model competitors.&lt;/p&gt;&lt;p&gt;The failure led Meta founder and CEO Mark Zuckerberg to appoint Alexandr Wang, co-founder and prior CEO of AI data supplier Scale AI, &lt;a href="https://www.cnbc.com/2025/06/10/zuckerberg-makes-metas-biggest-bet-on-ai-14-billion-scale-ai-deal.html"&gt;as Chief AI Officer&lt;/a&gt;, and embark on an &lt;a href="https://www.wired.com/story/meta-poaches-openai-researcher-yang-song/"&gt;extensive and costly hiring spree&lt;/a&gt; that shocked the AI and business communities with &lt;a href="https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/"&gt;eye-watering pay packages for top AI researchers&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In contrast, Omnilingual ASR represents a strategic and reputational reset. It returns Meta to a domain where the company has historically led — multilingual AI — and offers a truly extensible, community-oriented stack with minimal barriers to entry. &lt;/p&gt;&lt;p&gt;The system’s support for 1,600+ languages and its extensibility to over 5,000 more via zero-shot in-context learning reassert Meta’s engineering credibility in language technology. &lt;/p&gt;&lt;p&gt;Importantly, it does so through a free and permissively licensed release, under Apache 2.0, with transparent dataset sourcing and reproducible training protocols.&lt;/p&gt;&lt;p&gt;This shift aligns with broader themes in Meta’s 2025 strategy. The company has refocused its narrative around a “personal superintelligence” vision, investing heavily in infrastructure (including a September release of custom AI accelerators and Arm-based inference stacks) &lt;a href="https://engineering.fb.com/2025/09/22/data-infrastructure/meta-custom-accelerators/"&gt;source&lt;/a&gt; while downplaying the metaverse in favor of foundational AI capabilities. The return to public training data in Europe after a regulatory pause also underscores its intention to compete globally, despite privacy scrutiny &lt;a href="https://apnews.com/article/meta-eu-data-training-2025"&gt;source&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Omnilingual ASR, then, is more than a model release — it’s a calculated move to reassert control of the narrative: from the fragmented rollout of Llama 4 to a high-utility, research-grounded contribution that aligns with Meta’s long-term AI platform strategy.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community-Centered Dataset Collection&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To achieve this scale, Meta partnered with researchers and community organizations in Africa, Asia, and elsewhere to create the Omnilingual ASR Corpus, a 3,350-hour dataset across 348 low-resource languages. Contributors were compensated local speakers, and recordings were gathered in collaboration with groups like:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;African Next Voices&lt;/b&gt;: A Gates Foundation–supported consortium including Maseno University (Kenya), University of Pretoria, and Data Science Nigeria&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mozilla Foundation’s Common Voice&lt;/b&gt;, supported through the Open Multilingual Speech Fund&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Lanfrica / NaijaVoices&lt;/b&gt;, which created data for 11 African languages including Igala, Serer, and Urhobo&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The data collection focused on natural, unscripted speech. Prompts were designed to be culturally relevant and open-ended, such as “Is it better to have a few close friends or many casual acquaintances? Why?” Transcriptions used established writing systems, with quality assurance built into every step.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance and Hardware Considerations&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The largest model in the suite, the omniASR_LLM_7B, requires ~17GB of GPU memory for inference, making it suitable for deployment on high-end hardware. Smaller models (300M–1B) can run on lower-power devices and deliver real-time transcription speeds.&lt;/p&gt;&lt;p&gt;Performance benchmarks show strong results even in low-resource scenarios:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;CER &amp;lt;10% in 95% of high-resource and mid-resource languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CER &amp;lt;10% in 36% of low-resource languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Robustness in noisy conditions and unseen domains, especially with fine-tuning&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The zero-shot system, omniASR_LLM_7B_ZS, can transcribe new languages with minimal setup. Users provide a few sample audio–text pairs, and the model generates transcriptions for new utterances in the same language.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Open Access and Developer Tooling&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;All models and the dataset are licensed under permissive terms:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Apache 2.0&lt;/b&gt; for models and code&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;CC-BY 4.0&lt;/b&gt; for the &lt;a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus"&gt;Omnilingual ASR Corpus on HuggingFace&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Installation is supported via PyPI and uv:&lt;/p&gt;&lt;p&gt;&lt;code&gt;pip install omnilingual-asr&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Meta also provides:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A HuggingFace dataset integration&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Pre-built inference pipelines&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Language-code conditioning for improved accuracy&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Developers can view the full list of supported languages using the API:&lt;/p&gt;&lt;p&gt;&lt;code&gt;from omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;print(len(supported_langs))
print(supported_langs)&lt;/code&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Broader Implications&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Omnilingual ASR reframes language coverage in ASR from a fixed list to an &lt;b&gt;extensible framework&lt;/b&gt;. It enables:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Community-driven inclusion of underrepresented languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Digital access for oral and endangered languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Research on speech tech in linguistically diverse contexts&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Crucially, Meta emphasizes ethical considerations throughout—advocating for open-source participation and collaboration with native-speaking communities.&lt;/p&gt;&lt;p&gt;“No model can ever anticipate and include all of the world’s languages in advance,” the Omnilingual ASR paper states, “but Omnilingual ASR makes it possible for communities to extend recognition with their own data.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Access the Tools&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;All resources are now available at:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Code + Models&lt;/b&gt;: &lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;github.com/facebookresearch/omnilingual-asr&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dataset&lt;/b&gt;: &lt;a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus"&gt;huggingface.co/datasets/facebook/omnilingual-asr-corpus&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Blogpost&lt;/b&gt;: &lt;a href="https://ai.meta.com/blog/omnilingual-asr"&gt;ai.meta.com/blog/omnilingual-asr&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;What This Means for Enterprises&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For enterprise developers, especially those operating in multilingual or international markets, Omnilingual ASR significantly lowers the barrier to deploying speech-to-text systems across a broader range of customers and geographies. &lt;/p&gt;&lt;p&gt;Instead of relying on commercial ASR APIs that support only a narrow set of high-resource languages, teams can now integrate an open-source pipeline that covers over 1,600 languages out of the box—with the option to extend it to thousands more via zero-shot learning.&lt;/p&gt;&lt;p&gt;This flexibility is especially valuable for enterprises working in sectors like voice-based customer support, transcription services, accessibility, education, or civic technology, where local language coverage can be a competitive or regulatory necessity. Because the models are released under the permissive Apache 2.0 license, businesses can fine-tune, deploy, or integrate them into proprietary systems without restrictive terms.&lt;/p&gt;&lt;p&gt;It also represents a shift in the ASR landscape—from centralized, cloud-gated offerings to community-extendable infrastructure. By making multilingual speech recognition more accessible, customizable, and cost-effective, Omnilingual ASR opens the door to a new generation of enterprise speech applications built around linguistic inclusion rather than linguistic limitation.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/meta-returns-to-open-source-ai-with-omnilingual-asr-models-that-can</guid><pubDate>Mon, 10 Nov 2025 20:27:00 +0000</pubDate></item><item><title>[NEW] Kaltura acquires eSelf, founded by creator of Snap’s AI, in $27M deal (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/kaltura-acquires-eself-founded-by-creator-of-snaps-ai-in-27m-deal/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Kaltura-EX-Avatar-Town-Hall-Assistant.png?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kaltura, a New York-headquartered AI video platform company, is acquiring eSelf.ai, an Israel-based startup behind conversational avatars — AI-generated digital humans that can talk with users — for about $27 million. Kaltura announced today that it has signed a definitive agreement to acquire eSelf, a platform supporting more than 30 languages and featuring a user-friendly studio for creating, customizing, and deploying photorealistic digital avatars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Co-founded in 2023 by CEO Alan Bekker, who previously sold his first startup, Voca, to Snap in 2020 — and CTO Eylon Shoshan, eSelf brings deep technical expertise in speech-to-video generation, low-latency speech recognition, and screen understanding, which allows avatars to see and respond to what’s on a user’s screen. The eSelf co-founders will join Kaltura to oversee the integration of eSelf’s technology into the company, with all current eSelf employees coming on board as well.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The two-year-old startup has a small but strong team of around 15 AI experts, Ron Yekutiel, co-founder and CEO of Kaltura, told TechCrunch. He noted that Bekker’s former company specialized in natural language processing, which helps computers understand human speech, and computer vision, saying it was a “very leading company in the area of conversational speech bots. And so he’s an expert [in this field], and that’s what we bought,” Yekutiel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura offers a suite of cloud-based software solutions designed for advanced video applications, including a corporate video portal akin to a private YouTube, tools for webinars and virtual events, and integrations that embed video learning into university learning management systems, or platforms that organize online coursework.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Nasdaq-listed company also delivers virtual classroom products and end-to-end TV streaming solutions. Kaltura’s video platform serves over 800 enterprise customers, helping them engage users across sales, marketing, customer care, education, and entertainment. Its clients include tech giants like Amazon, Oracle, Salesforce, SAP, Adobe, and IBM, as well as leading banks, insurance companies, consulting firms, pharmaceutical companies, and universities in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura plans to integrate eSelf.ai’s virtual agent technology across its video offerings; the integration aims to enable agents that can listen, speak, and interpret user screens in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This acquisition was so strategic. We were actively evaluating multiple companies to find the right fit. We determined that they [eSelf] were best-in-class for real-time, synchronous conversation — not just video-on-demand lip-syncing — and that they had an impressive speech-to-text and text-to-speech technology stack,” Yekutiel said in an interview with TechCrunch. “Beyond the technology, there was also a strong cultural and geographic alignment, which was critical for us.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-why-a-video-company-is-betting-on-conversational-avatars"&gt;&lt;strong&gt;Why a video company is betting on conversational avatars&lt;/strong&gt;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;For the past two decades, businesses have mostly used video for streaming, uploading, and managing content. But that’s changing fast. Thanks to AI, videos can now be generated instantly — hyper-personalized and contextual — giving every viewer their own custom experience, tailored exactly to what they need in that moment, Yekutiel explained.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We started with video, then moved to personalized video, and now, with eSelf’s technology, we’re adding human-like capabilities — faces, eyes, mouths, ears — to make our AI agents conversational and expressive,” Yekutiel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura is evolving from a video platform into a video-based customer and employee experience provider, where video serves as the interface. Unlike most avatar companies that offer only a “face,” it delivers the full workflow — avatar, intelligence, and enterprise-connected knowledge. The focus isn’t just streaming video; it’s driving measurable business results and ROI, the CEO added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company plans to launch standalone, embeddable agents for uses including sales, marketing, customer support, and training. Target sectors include education, media and telecom, e-commerce, financial services, healthcare, and pharmaceuticals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked about media reports saying Kaltura was exploring a sale or merger at a $400 million to $500 million valuation, Yekutiel told TechCrunch that Kaltura has explored opportunities with a range of companies, including potential “acquisitions, mergers with similarly sized firms, and connections with some larger players.” But it never got close to a transaction like the ones being reported, he said. He also pointed to Kaltura’s recent acquisitions, including its fourth company, as evidence of the company’s continued commitment to its current strategy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This marks Kaltura’s fourth acquisition to date. The company acquired cloud TV solution Tvinci in 2014, followed by Rapt Media in 2018, and video conferencing platform Newrow in 2020. eSelf’s most recent funding round was its&amp;nbsp;$4.5 million announced in December 2024.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura, which went public in 2021, reports around $180 million in revenue, is profitable on an adjusted EBITDA and cash flow basis, and has about 600 employees.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Kaltura-EX-Avatar-Town-Hall-Assistant.png?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kaltura, a New York-headquartered AI video platform company, is acquiring eSelf.ai, an Israel-based startup behind conversational avatars — AI-generated digital humans that can talk with users — for about $27 million. Kaltura announced today that it has signed a definitive agreement to acquire eSelf, a platform supporting more than 30 languages and featuring a user-friendly studio for creating, customizing, and deploying photorealistic digital avatars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Co-founded in 2023 by CEO Alan Bekker, who previously sold his first startup, Voca, to Snap in 2020 — and CTO Eylon Shoshan, eSelf brings deep technical expertise in speech-to-video generation, low-latency speech recognition, and screen understanding, which allows avatars to see and respond to what’s on a user’s screen. The eSelf co-founders will join Kaltura to oversee the integration of eSelf’s technology into the company, with all current eSelf employees coming on board as well.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The two-year-old startup has a small but strong team of around 15 AI experts, Ron Yekutiel, co-founder and CEO of Kaltura, told TechCrunch. He noted that Bekker’s former company specialized in natural language processing, which helps computers understand human speech, and computer vision, saying it was a “very leading company in the area of conversational speech bots. And so he’s an expert [in this field], and that’s what we bought,” Yekutiel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura offers a suite of cloud-based software solutions designed for advanced video applications, including a corporate video portal akin to a private YouTube, tools for webinars and virtual events, and integrations that embed video learning into university learning management systems, or platforms that organize online coursework.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Nasdaq-listed company also delivers virtual classroom products and end-to-end TV streaming solutions. Kaltura’s video platform serves over 800 enterprise customers, helping them engage users across sales, marketing, customer care, education, and entertainment. Its clients include tech giants like Amazon, Oracle, Salesforce, SAP, Adobe, and IBM, as well as leading banks, insurance companies, consulting firms, pharmaceutical companies, and universities in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura plans to integrate eSelf.ai’s virtual agent technology across its video offerings; the integration aims to enable agents that can listen, speak, and interpret user screens in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This acquisition was so strategic. We were actively evaluating multiple companies to find the right fit. We determined that they [eSelf] were best-in-class for real-time, synchronous conversation — not just video-on-demand lip-syncing — and that they had an impressive speech-to-text and text-to-speech technology stack,” Yekutiel said in an interview with TechCrunch. “Beyond the technology, there was also a strong cultural and geographic alignment, which was critical for us.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-why-a-video-company-is-betting-on-conversational-avatars"&gt;&lt;strong&gt;Why a video company is betting on conversational avatars&lt;/strong&gt;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;For the past two decades, businesses have mostly used video for streaming, uploading, and managing content. But that’s changing fast. Thanks to AI, videos can now be generated instantly — hyper-personalized and contextual — giving every viewer their own custom experience, tailored exactly to what they need in that moment, Yekutiel explained.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We started with video, then moved to personalized video, and now, with eSelf’s technology, we’re adding human-like capabilities — faces, eyes, mouths, ears — to make our AI agents conversational and expressive,” Yekutiel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura is evolving from a video platform into a video-based customer and employee experience provider, where video serves as the interface. Unlike most avatar companies that offer only a “face,” it delivers the full workflow — avatar, intelligence, and enterprise-connected knowledge. The focus isn’t just streaming video; it’s driving measurable business results and ROI, the CEO added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company plans to launch standalone, embeddable agents for uses including sales, marketing, customer support, and training. Target sectors include education, media and telecom, e-commerce, financial services, healthcare, and pharmaceuticals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked about media reports saying Kaltura was exploring a sale or merger at a $400 million to $500 million valuation, Yekutiel told TechCrunch that Kaltura has explored opportunities with a range of companies, including potential “acquisitions, mergers with similarly sized firms, and connections with some larger players.” But it never got close to a transaction like the ones being reported, he said. He also pointed to Kaltura’s recent acquisitions, including its fourth company, as evidence of the company’s continued commitment to its current strategy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This marks Kaltura’s fourth acquisition to date. The company acquired cloud TV solution Tvinci in 2014, followed by Rapt Media in 2018, and video conferencing platform Newrow in 2020. eSelf’s most recent funding round was its&amp;nbsp;$4.5 million announced in December 2024.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura, which went public in 2021, reports around $180 million in revenue, is profitable on an adjusted EBITDA and cash flow basis, and has about 600 employees.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/kaltura-acquires-eself-founded-by-creator-of-snaps-ai-in-27m-deal/</guid><pubDate>Mon, 10 Nov 2025 21:05:00 +0000</pubDate></item><item><title>[NEW] A better way of thinking about the AI bubble (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/a-better-way-of-thinking-about-the-ai-bubble/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2015/05/bubbles.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People often think about tech bubbles in apocalyptic terms, but it doesn’t have to be as serious as all that. In economic terms, a bubble is a bet that turned out to be too big, leaving you with more supply than demand.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot: It’s not all or nothing, and even good bets can turn sour if you aren’t careful about how you make them.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What makes the question of the AI bubble so tricky to answer is&amp;nbsp;mismatched&amp;nbsp;timelines&amp;nbsp;between the breakneck pace of AI software development and the slow crawl of constructing and powering a data center.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because&amp;nbsp;these data centers take years to build, a lot will inevitably change between now and when they come online.&amp;nbsp;The supply chain that powers AI services is so complex and fluid that&amp;nbsp;it’s&amp;nbsp;hard to have any clarity on how much supply&amp;nbsp;we’ll&amp;nbsp;need a few years from now.&amp;nbsp;It&amp;nbsp;isn’t&amp;nbsp;simply a matter of how much people will be using AI in 2028, but how&amp;nbsp;they’ll&amp;nbsp;be using it, and whether&amp;nbsp;we’ll&amp;nbsp;have any breakthroughs in energy, semiconductor design,&amp;nbsp;or power transmission in the meantime.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When a bet is this big, there are lots of ways&amp;nbsp;it can go wrong — and AI bets are getting&amp;nbsp;very big&amp;nbsp;indeed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, Reuters reported that an Oracle-linked data center campus in New Mexico has drawn as much as&amp;nbsp;$18 billion&amp;nbsp;in credit&amp;nbsp;from a consortium of 20 banks. Oracle has already contracted&amp;nbsp;$300 billion&amp;nbsp;in cloud services to OpenAI, and the companies have&amp;nbsp;joined with&amp;nbsp;SoftBank to build&amp;nbsp;$500 billion&amp;nbsp;in total AI infrastructure as part of the “Stargate” project. Meta, not to be outdone, has&amp;nbsp;pledged to spend&amp;nbsp;$600 billion&amp;nbsp;on infrastructure over the next three years.&amp;nbsp;We’ve&amp;nbsp;been tracking all the major commitments&amp;nbsp;here&amp;nbsp;— and the sheer volume has made it hard to keep up.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, there is real uncertainty about how fast demand for AI services will grow.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A McKinsey survey released last week&amp;nbsp;looked&amp;nbsp;at how top firms are employing AI tools. The results were mixed.&amp;nbsp;Almost all&amp;nbsp;the businesses contacted are using AI in some way,&amp;nbsp;yet&amp;nbsp;few are using it&amp;nbsp;on&amp;nbsp;any real&amp;nbsp;scale. AI has&amp;nbsp;allowed&amp;nbsp;companies to&amp;nbsp;cost-cut in specific use cases, but&amp;nbsp;it’s&amp;nbsp;not making a dent on the overall business. In short, most companies are still in “wait and see” mode. If&amp;nbsp;you’re&amp;nbsp;counting on those companies to buy space in your data center, you may be waiting a long time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But even if AI demand is endless, these projects could run into more straightforward infrastructure problems. Last week, Satya Nadella surprised podcast listeners&amp;nbsp;by saying he was more concerned with&amp;nbsp;running out of data center space&amp;nbsp;than running out of chips. (As he put it, “It’s not a supply issue of chips; it’s the fact that I don’t have warm shells to plug into.”) At the same time, whole data centers are sitting idle because they can’t handle the power demands of the latest generation of chips.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Nvidia and OpenAI have been moving forward as fast as they possibly can, the electrical grid and built environment are still moving at the same pace they always have. That leaves lots of opportunity for expensive bottlenecks, even if everything else goes right.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;We get deeper into the idea in this week’s Equity podcast, which you can listen to below.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2015/05/bubbles.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People often think about tech bubbles in apocalyptic terms, but it doesn’t have to be as serious as all that. In economic terms, a bubble is a bet that turned out to be too big, leaving you with more supply than demand.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot: It’s not all or nothing, and even good bets can turn sour if you aren’t careful about how you make them.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What makes the question of the AI bubble so tricky to answer is&amp;nbsp;mismatched&amp;nbsp;timelines&amp;nbsp;between the breakneck pace of AI software development and the slow crawl of constructing and powering a data center.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because&amp;nbsp;these data centers take years to build, a lot will inevitably change between now and when they come online.&amp;nbsp;The supply chain that powers AI services is so complex and fluid that&amp;nbsp;it’s&amp;nbsp;hard to have any clarity on how much supply&amp;nbsp;we’ll&amp;nbsp;need a few years from now.&amp;nbsp;It&amp;nbsp;isn’t&amp;nbsp;simply a matter of how much people will be using AI in 2028, but how&amp;nbsp;they’ll&amp;nbsp;be using it, and whether&amp;nbsp;we’ll&amp;nbsp;have any breakthroughs in energy, semiconductor design,&amp;nbsp;or power transmission in the meantime.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When a bet is this big, there are lots of ways&amp;nbsp;it can go wrong — and AI bets are getting&amp;nbsp;very big&amp;nbsp;indeed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, Reuters reported that an Oracle-linked data center campus in New Mexico has drawn as much as&amp;nbsp;$18 billion&amp;nbsp;in credit&amp;nbsp;from a consortium of 20 banks. Oracle has already contracted&amp;nbsp;$300 billion&amp;nbsp;in cloud services to OpenAI, and the companies have&amp;nbsp;joined with&amp;nbsp;SoftBank to build&amp;nbsp;$500 billion&amp;nbsp;in total AI infrastructure as part of the “Stargate” project. Meta, not to be outdone, has&amp;nbsp;pledged to spend&amp;nbsp;$600 billion&amp;nbsp;on infrastructure over the next three years.&amp;nbsp;We’ve&amp;nbsp;been tracking all the major commitments&amp;nbsp;here&amp;nbsp;— and the sheer volume has made it hard to keep up.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, there is real uncertainty about how fast demand for AI services will grow.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A McKinsey survey released last week&amp;nbsp;looked&amp;nbsp;at how top firms are employing AI tools. The results were mixed.&amp;nbsp;Almost all&amp;nbsp;the businesses contacted are using AI in some way,&amp;nbsp;yet&amp;nbsp;few are using it&amp;nbsp;on&amp;nbsp;any real&amp;nbsp;scale. AI has&amp;nbsp;allowed&amp;nbsp;companies to&amp;nbsp;cost-cut in specific use cases, but&amp;nbsp;it’s&amp;nbsp;not making a dent on the overall business. In short, most companies are still in “wait and see” mode. If&amp;nbsp;you’re&amp;nbsp;counting on those companies to buy space in your data center, you may be waiting a long time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But even if AI demand is endless, these projects could run into more straightforward infrastructure problems. Last week, Satya Nadella surprised podcast listeners&amp;nbsp;by saying he was more concerned with&amp;nbsp;running out of data center space&amp;nbsp;than running out of chips. (As he put it, “It’s not a supply issue of chips; it’s the fact that I don’t have warm shells to plug into.”) At the same time, whole data centers are sitting idle because they can’t handle the power demands of the latest generation of chips.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Nvidia and OpenAI have been moving forward as fast as they possibly can, the electrical grid and built environment are still moving at the same pace they always have. That leaves lots of opportunity for expensive bottlenecks, even if everything else goes right.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;We get deeper into the idea in this week’s Equity podcast, which you can listen to below.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/a-better-way-of-thinking-about-the-ai-bubble/</guid><pubDate>Mon, 10 Nov 2025 21:16:41 +0000</pubDate></item><item><title>[NEW] Researchers isolate memorization from reasoning in AI neural networks (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/study-finds-ai-models-store-memories-and-logic-in-different-neural-regions/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Basic arithmetic ability lives in the memorization pathways, not logic circuits.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Benj Edwards / Kirillm via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;When engineers build AI language models like GPT-5 from training data, at least two major processing features emerge: memorization (reciting exact text they’ve seen before, like famous quotes or passages from books) and reasoning (solving new problems using general principles). New research from AI startup Goodfire.ai provides the first potentially clear evidence that these different functions actually work through completely separate neural pathways in the model’s architecture.&lt;/p&gt;
&lt;p&gt;The researchers discovered that this separation proves remarkably clean. In a preprint paper released in late October, they described that when they removed the memorization pathways, models lost 97 percent of their ability to recite training data verbatim but kept nearly all their “logical reasoning” ability intact.&lt;/p&gt;
&lt;p&gt;For example, at layer 22 in Allen Institute for AI’s OLMo-7B language model, the bottom 50 percent of weight components showed 23 percent higher activation on memorized data, while the top 10 percent showed 26 percent higher activation on general, non-memorized text. This mechanistic split enabled the researchers to surgically remove memorization while preserving other capabilities.&lt;/p&gt;
&lt;p&gt;Perhaps most surprisingly, the researchers found that arithmetic operations seem to share the same neural pathways as memorization rather than logical reasoning. When they removed memorization circuits, mathematical performance plummeted to 66 percent while logical tasks remained nearly untouched. This discovery may explain why AI language models notoriously struggle with math without the use of external tools. They’re attempting to recall arithmetic from a limited memorization table rather than computing it, like a student who memorized times tables but never learned how multiplication works. The finding suggests that at current scales, language models treat “2+2=4” more like a memorized fact than a logical operation.&lt;/p&gt;
&lt;p&gt;It’s worth noting that “reasoning” in AI research covers a spectrum of abilities that don’t necessarily match what we might call reasoning in humans. The logical reasoning that survived memory removal in this latest research includes tasks like evaluating true/false statements and following if-then rules, which are essentially applying learned patterns to new inputs. This also differs from the deeper “mathematical reasoning” required for proofs or novel problem-solving, which current AI models struggle with even when their pattern-matching abilities remain intact.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Looking ahead, if the information removal techniques receive further development in the future, AI companies could potentially one day remove, say, copyrighted content, private information, or harmful memorized text from a neural network without destroying the model’s ability to perform transformative tasks. However, since neural networks store information in distributed ways that are still not completely understood, for the time being, the researchers say their method “cannot guarantee complete elimination of sensitive information.” These are early steps in a new research direction for AI.&lt;/p&gt;
&lt;h2&gt;Traveling the neural landscape&lt;/h2&gt;
&lt;p&gt;To understand how researchers from Goodfire distinguished memorization from reasoning in these neural networks, it helps to know about a concept in AI called the “loss landscape.” The “loss landscape” is a way of visualizing how wrong or right an AI model’s predictions are as you adjust its internal settings (which are called “weights”).&lt;/p&gt;
&lt;p&gt;Imagine you’re tuning a complex machine with millions of dials. The “loss” measures the number of mistakes the machine makes. High loss means many errors, low loss means few errors. The “landscape” is what you’d see if you could map out the error rate for every possible combination of dial settings.&lt;/p&gt;
&lt;p&gt;During training, AI models essentially “roll downhill” in this landscape (gradient descent), adjusting their weights to find the valleys where they make the fewest mistakes. This process provides AI model outputs, like answers to questions.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126528 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 1: Overview of our approach. We collect activations and gradients from a sample of training data (a), which allows us to approximate loss curvature w.r.t. a weight matrix using K-FAC (b). We decompose these weight matrices into components (each the same size as the matrix), ordered from high to low curvature. In language models, we show that data from different tasks interacts with parts of the spectrum of components differently (c)." class="center large" height="705" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig1_curve-1024x705.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 1 from the paper “From Memorization to Reasoning in the Spectrum of Loss Curvature.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Merullo et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers analyzed the “curvature” of the loss landscapes of particular AI language models, measuring how sensitive the model’s performance is to small changes in different neural network weights. Sharp peaks and valleys represent high curvature (where tiny changes cause big effects), while flat plains represent low curvature (where changes have minimal impact).&lt;/p&gt;
&lt;p&gt;Using a technique called K-FAC (Kronecker-Factored Approximate Curvature), they found that individual memorized facts create sharp spikes in this landscape, but because each memorized item spikes in a different direction, when averaged together they create a flat profile. Meanwhile, reasoning abilities that many different inputs rely on maintain consistent moderate curves across the landscape, like rolling hills that remain roughly the same shape regardless of the direction from which you approach them.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Directions that implement shared mechanisms used by many inputs add coherently and remain high-curvature on average,” the researchers write, describing reasoning pathways. In contrast, memorization uses “idiosyncratic sharp directions associated with specific examples” that appear flat when averaged across data.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Different tasks reveal a spectrum of mechanisms&lt;/h2&gt;
&lt;p&gt;The researchers tested their technique on multiple AI systems to verify the findings held across different architectures. They primarily used Allen Institute’s OLMo-2 family of open language models, specifically the 7-billion and 1-billion parameter versions, chosen because their training data is openly accessible. For vision models, they trained custom 86-million parameter Vision Transformers (ViT-Base models) on ImageNet with intentionally mislabeled data to create controlled memorization. They also validated their findings against existing memorization removal methods like BalancedSubnet to establish performance benchmarks.&lt;/p&gt;
&lt;p&gt;The team tested their discovery by selectively removing low-curvature weight components from these trained models. Memorized content dropped to 3.4 percent recall from nearly 100 percent. Meanwhile, logical reasoning tasks maintained 95 to 106 percent of baseline performance.&lt;/p&gt;
&lt;p&gt;These logical tasks included Boolean expression evaluation, logical deduction puzzles where solvers must track relationships like “if A is taller than B,” object tracking through multiple swaps, and benchmarks like BoolQ for yes/no reasoning, Winogrande for common sense inference, and OpenBookQA for science questions requiring reasoning from provided facts. Some tasks fell between these extremes, revealing a spectrum of mechanisms.&lt;/p&gt;
&lt;p&gt;Mathematical operations and closed-book fact retrieval shared pathways with memorization, dropping to 66 to 86 percent performance after editing. The researchers found arithmetic particularly brittle. Even when models generated identical reasoning chains, they failed at the calculation step after low-curvature components were removed.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126527 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 3: Sensitivity of different kinds of tasks to ablation of flatter eigenvectors. Parametric knowledge retrieval, arithmetic, and memorization are brittle, but openbook fact retrieval and logical reasoning is robust and maintain around 100% of original performance." class="center large" height="574" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig3_curve-1024x574.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 3 from the paper “From Memorization to Reasoning in the Spectrum of Loss Curvature.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Merullo et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;“Arithmetic problems themselves are memorized at the 7B scale, or because they require narrowly used directions to do precise calculations,” the team explains. Open-book question answering, which relies on provided context rather than internal knowledge, proved most robust to the editing procedure, maintaining nearly full performance.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Curiously, the mechanism separation varied by information type. Common facts like country capitals barely changed after editing, while rare facts like company CEOs dropped 78 percent. This suggests models allocate distinct neural resources based on how frequently information appears in training.&lt;/p&gt;
&lt;p&gt;The K-FAC technique outperformed existing memorization removal methods without needing training examples of memorized content. On unseen historical quotes, K-FAC achieved 16.1 percent memorization versus 60 percent for the previous best method, BalancedSubnet.&lt;/p&gt;
&lt;p&gt;Vision transformers showed similar patterns. When trained with intentionally mislabeled images, the models developed distinct pathways for memorizing wrong labels versus learning correct patterns. Removing memorization pathways restored 66.5 percent accuracy on previously mislabeled images.&lt;/p&gt;
&lt;h2&gt;Limits of memory removal&lt;/h2&gt;
&lt;p&gt;However, the researchers acknowledged that their technique isn’t perfect. Once-removed memories might return if the model receives more training, as other research has shown that current unlearning methods only suppress information rather than completely erasing it from the neural network’s weights. That means the “forgotten” content can be reactivated with just a few training steps targeting those suppressed areas.&lt;/p&gt;
&lt;p&gt;The researchers also can’t fully explain why some abilities, like math, break so easily when memorization is removed. It’s unclear whether the model actually memorized all its arithmetic or whether math just happens to use similar neural circuits as memorization. Additionally, some sophisticated capabilities might look like memorization to their detection method, even when they’re actually complex reasoning patterns. Finally, the mathematical tools they use to measure the model’s “landscape” can become unreliable at the extremes, though this doesn’t affect the actual editing process.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Basic arithmetic ability lives in the memorization pathways, not logic circuits.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Benj Edwards / Kirillm via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;When engineers build AI language models like GPT-5 from training data, at least two major processing features emerge: memorization (reciting exact text they’ve seen before, like famous quotes or passages from books) and reasoning (solving new problems using general principles). New research from AI startup Goodfire.ai provides the first potentially clear evidence that these different functions actually work through completely separate neural pathways in the model’s architecture.&lt;/p&gt;
&lt;p&gt;The researchers discovered that this separation proves remarkably clean. In a preprint paper released in late October, they described that when they removed the memorization pathways, models lost 97 percent of their ability to recite training data verbatim but kept nearly all their “logical reasoning” ability intact.&lt;/p&gt;
&lt;p&gt;For example, at layer 22 in Allen Institute for AI’s OLMo-7B language model, the bottom 50 percent of weight components showed 23 percent higher activation on memorized data, while the top 10 percent showed 26 percent higher activation on general, non-memorized text. This mechanistic split enabled the researchers to surgically remove memorization while preserving other capabilities.&lt;/p&gt;
&lt;p&gt;Perhaps most surprisingly, the researchers found that arithmetic operations seem to share the same neural pathways as memorization rather than logical reasoning. When they removed memorization circuits, mathematical performance plummeted to 66 percent while logical tasks remained nearly untouched. This discovery may explain why AI language models notoriously struggle with math without the use of external tools. They’re attempting to recall arithmetic from a limited memorization table rather than computing it, like a student who memorized times tables but never learned how multiplication works. The finding suggests that at current scales, language models treat “2+2=4” more like a memorized fact than a logical operation.&lt;/p&gt;
&lt;p&gt;It’s worth noting that “reasoning” in AI research covers a spectrum of abilities that don’t necessarily match what we might call reasoning in humans. The logical reasoning that survived memory removal in this latest research includes tasks like evaluating true/false statements and following if-then rules, which are essentially applying learned patterns to new inputs. This also differs from the deeper “mathematical reasoning” required for proofs or novel problem-solving, which current AI models struggle with even when their pattern-matching abilities remain intact.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Looking ahead, if the information removal techniques receive further development in the future, AI companies could potentially one day remove, say, copyrighted content, private information, or harmful memorized text from a neural network without destroying the model’s ability to perform transformative tasks. However, since neural networks store information in distributed ways that are still not completely understood, for the time being, the researchers say their method “cannot guarantee complete elimination of sensitive information.” These are early steps in a new research direction for AI.&lt;/p&gt;
&lt;h2&gt;Traveling the neural landscape&lt;/h2&gt;
&lt;p&gt;To understand how researchers from Goodfire distinguished memorization from reasoning in these neural networks, it helps to know about a concept in AI called the “loss landscape.” The “loss landscape” is a way of visualizing how wrong or right an AI model’s predictions are as you adjust its internal settings (which are called “weights”).&lt;/p&gt;
&lt;p&gt;Imagine you’re tuning a complex machine with millions of dials. The “loss” measures the number of mistakes the machine makes. High loss means many errors, low loss means few errors. The “landscape” is what you’d see if you could map out the error rate for every possible combination of dial settings.&lt;/p&gt;
&lt;p&gt;During training, AI models essentially “roll downhill” in this landscape (gradient descent), adjusting their weights to find the valleys where they make the fewest mistakes. This process provides AI model outputs, like answers to questions.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126528 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 1: Overview of our approach. We collect activations and gradients from a sample of training data (a), which allows us to approximate loss curvature w.r.t. a weight matrix using K-FAC (b). We decompose these weight matrices into components (each the same size as the matrix), ordered from high to low curvature. In language models, we show that data from different tasks interacts with parts of the spectrum of components differently (c)." class="center large" height="705" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig1_curve-1024x705.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 1 from the paper “From Memorization to Reasoning in the Spectrum of Loss Curvature.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Merullo et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers analyzed the “curvature” of the loss landscapes of particular AI language models, measuring how sensitive the model’s performance is to small changes in different neural network weights. Sharp peaks and valleys represent high curvature (where tiny changes cause big effects), while flat plains represent low curvature (where changes have minimal impact).&lt;/p&gt;
&lt;p&gt;Using a technique called K-FAC (Kronecker-Factored Approximate Curvature), they found that individual memorized facts create sharp spikes in this landscape, but because each memorized item spikes in a different direction, when averaged together they create a flat profile. Meanwhile, reasoning abilities that many different inputs rely on maintain consistent moderate curves across the landscape, like rolling hills that remain roughly the same shape regardless of the direction from which you approach them.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Directions that implement shared mechanisms used by many inputs add coherently and remain high-curvature on average,” the researchers write, describing reasoning pathways. In contrast, memorization uses “idiosyncratic sharp directions associated with specific examples” that appear flat when averaged across data.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Different tasks reveal a spectrum of mechanisms&lt;/h2&gt;
&lt;p&gt;The researchers tested their technique on multiple AI systems to verify the findings held across different architectures. They primarily used Allen Institute’s OLMo-2 family of open language models, specifically the 7-billion and 1-billion parameter versions, chosen because their training data is openly accessible. For vision models, they trained custom 86-million parameter Vision Transformers (ViT-Base models) on ImageNet with intentionally mislabeled data to create controlled memorization. They also validated their findings against existing memorization removal methods like BalancedSubnet to establish performance benchmarks.&lt;/p&gt;
&lt;p&gt;The team tested their discovery by selectively removing low-curvature weight components from these trained models. Memorized content dropped to 3.4 percent recall from nearly 100 percent. Meanwhile, logical reasoning tasks maintained 95 to 106 percent of baseline performance.&lt;/p&gt;
&lt;p&gt;These logical tasks included Boolean expression evaluation, logical deduction puzzles where solvers must track relationships like “if A is taller than B,” object tracking through multiple swaps, and benchmarks like BoolQ for yes/no reasoning, Winogrande for common sense inference, and OpenBookQA for science questions requiring reasoning from provided facts. Some tasks fell between these extremes, revealing a spectrum of mechanisms.&lt;/p&gt;
&lt;p&gt;Mathematical operations and closed-book fact retrieval shared pathways with memorization, dropping to 66 to 86 percent performance after editing. The researchers found arithmetic particularly brittle. Even when models generated identical reasoning chains, they failed at the calculation step after low-curvature components were removed.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126527 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 3: Sensitivity of different kinds of tasks to ablation of flatter eigenvectors. Parametric knowledge retrieval, arithmetic, and memorization are brittle, but openbook fact retrieval and logical reasoning is robust and maintain around 100% of original performance." class="center large" height="574" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig3_curve-1024x574.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 3 from the paper “From Memorization to Reasoning in the Spectrum of Loss Curvature.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Merullo et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;“Arithmetic problems themselves are memorized at the 7B scale, or because they require narrowly used directions to do precise calculations,” the team explains. Open-book question answering, which relies on provided context rather than internal knowledge, proved most robust to the editing procedure, maintaining nearly full performance.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Curiously, the mechanism separation varied by information type. Common facts like country capitals barely changed after editing, while rare facts like company CEOs dropped 78 percent. This suggests models allocate distinct neural resources based on how frequently information appears in training.&lt;/p&gt;
&lt;p&gt;The K-FAC technique outperformed existing memorization removal methods without needing training examples of memorized content. On unseen historical quotes, K-FAC achieved 16.1 percent memorization versus 60 percent for the previous best method, BalancedSubnet.&lt;/p&gt;
&lt;p&gt;Vision transformers showed similar patterns. When trained with intentionally mislabeled images, the models developed distinct pathways for memorizing wrong labels versus learning correct patterns. Removing memorization pathways restored 66.5 percent accuracy on previously mislabeled images.&lt;/p&gt;
&lt;h2&gt;Limits of memory removal&lt;/h2&gt;
&lt;p&gt;However, the researchers acknowledged that their technique isn’t perfect. Once-removed memories might return if the model receives more training, as other research has shown that current unlearning methods only suppress information rather than completely erasing it from the neural network’s weights. That means the “forgotten” content can be reactivated with just a few training steps targeting those suppressed areas.&lt;/p&gt;
&lt;p&gt;The researchers also can’t fully explain why some abilities, like math, break so easily when memorization is removed. It’s unclear whether the model actually memorized all its arithmetic or whether math just happens to use similar neural circuits as memorization. Additionally, some sophisticated capabilities might look like memorization to their detection method, even when they’re actually complex reasoning patterns. Finally, the mathematical tools they use to measure the model’s “landscape” can become unreliable at the extremes, though this doesn’t affect the actual editing process.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/study-finds-ai-models-store-memories-and-logic-in-different-neural-regions/</guid><pubDate>Mon, 10 Nov 2025 23:06:42 +0000</pubDate></item><item><title>[NEW] The circular money problem at the heart of AI’s biggest deals (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/the-circular-money-problem-at-the-heart-of-ais-biggest-deals/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30660881"&gt;





&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;SoftBank and OpenAI announced&amp;nbsp;a new 50-50 joint venture&amp;nbsp;this week to sell enterprise AI tools in Japan under the brand “Crystal Intelligence.” On paper,&amp;nbsp;it’s&amp;nbsp;a straightforward international expansion deal. But SoftBank’s role as a major investor in OpenAI is raising questions about whether AI’s biggest deals are creating real economic value or just moving money in circles.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Kirsten Korosec, Anthony&amp;nbsp;Ha,&amp;nbsp;and AI editor Russell Brandom break down why this deal has people skeptical, and what it signals about the sustainability of AI’s current investment model, and more.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X and Threads, at @EquityPod.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30660881"&gt;





&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;SoftBank and OpenAI announced&amp;nbsp;a new 50-50 joint venture&amp;nbsp;this week to sell enterprise AI tools in Japan under the brand “Crystal Intelligence.” On paper,&amp;nbsp;it’s&amp;nbsp;a straightforward international expansion deal. But SoftBank’s role as a major investor in OpenAI is raising questions about whether AI’s biggest deals are creating real economic value or just moving money in circles.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Kirsten Korosec, Anthony&amp;nbsp;Ha,&amp;nbsp;and AI editor Russell Brandom break down why this deal has people skeptical, and what it signals about the sustainability of AI’s current investment model, and more.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X and Threads, at @EquityPod.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/the-circular-money-problem-at-the-heart-of-ais-biggest-deals/</guid><pubDate>Mon, 10 Nov 2025 23:32:36 +0000</pubDate></item><item><title>[NEW] Lovable says it’s nearing 8 million users as the year-old AI coding startup eyes more corporate employees (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/lovable-says-its-nearing-8-million-users-as-the-year-old-ai-coding-startup-eyes-more-corporate-employees/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-10-at-11.20.57-PM.png?resize=1200,654" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Lovable, the Stockholm-based AI coding platform, is closing in on 8 million users, CEO Anton Osika told this editor during a sit-down on Monday, a major jump from the 2.3 million active users number the company shared in July. Osika said the company — which was founded almost exactly one year ago — is also seeing “100,000 new products built on Lovable every single day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The metrics suggest rapid growth of the startup, which has raised $228 million in total funding to date, including a $200 million round this summer that valued the company at $1.8 billion. Rumors have swirled in recent weeks — potentially sparked by its own investors — that new backers want to invest at a $5 billion valuation, though Osika said the company isn’t capital constrained and declined to discuss fundraising plans.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speaking to me onstage at the Web Summit event in Lisbon, Osika notably didn’t share another number: Lovable’s current annual recurring revenue. The company, which uses a mix of free and paid tiers, hit $100 million in ARR this June, a milestone it trumpeted publicly. But questions have emerged since about whether the vibe coding boom is sustainable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Research from Barclays this summer, along with Google Trends data, showed that traffic to some of the buzziest services, including Lovable and Vercel’s v0, had declined after peaking earlier this year. (Traffic to Lovable was down 40% as of September, according to the Barclays analysts.) “This waning traffic begs the question on whether app/site vibecoding has peaked out already or has just had a bit of a lull before interest ramps up,” they reportedly wrote in a note to investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Osika insisted retention remains strong, citing more than 100% net dollar retention — meaning users spend more over time. He also said the company has “just passed” the 100-employee mark and is now importing leadership talent from San Francisco to bolster its Stockholm headquarters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable emerged from GPT Engineer, an open source tool Osika built that went viral among developers. But he says he quickly realized the bigger opportunity lay with the 99% of people who don’t know how to code. “I woke up a few days after building GPT Engineer and I realized, look, we’re going to reimagine how you build software,” Osika said. “I biked to my co-founder’s place, and I said, I have this great idea. I woke him up.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The platform has attracted an eclectic user base. More than half of Fortune 500 companies are using Lovable to “supercharge creativity,” according to Osika. At the same time, he said, an 11-year-old in Lisbon built a Facebook clone for his school, while a Swedish duo is making $700,000 annually from a startup they launched seven months ago on the platform. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“What I hear from people trying Lovable is, ‘It just works,’” Osika said, crediting what he described as Swedish design sensibility.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Security remains a thornier issue for the vibe coding sector. When I raised a recent incident in which an app built with vibe coding tools leaked 72,000 images into the wild, including GPS data and user IDs, Osika acknowledged the problem. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The part of the engineering organization where we’re moving the quickest on hiring is security engineers,” he said, adding that his goal is to make building with Lovable “more secure than building with just human-written code.” In fact, he said, before users can deploy, Lovable now runs multiple security checks, though the platform still requires users building sensitive applications — banking apps, for instance — to hire security experts, just as they would with traditional development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Osika was similarly matter-of-fact when I asked about competition from OpenAI and Anthropic, the AI giants whose models power Lovable but that have also released their own coding agents. He sees the market as big enough for multiple winners. “If we can unlock more human creativity and human agency  . . . and just driving the change so that anyone can create if they have good ideas, [and] build businesses on top of that, that should be celebrated, regardless of whoever does that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a decidedly collegial stance in an industry not known for it. (Even Osika has engaged in some light social media sparring with Amjad Masad of competitor Replit.) But he said his focus right now is on building “the most intuitive experience for humans” rather than obsessing over rivals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Osika described Lovable’s mission as building “the last piece of software” — a platform where everything a product organization needs, from understanding users to deploying mission-critical features, can be done through a simple interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Demo, don’t memo,” a popular phrase among product leaders, captures how companies now use Lovable, he said. Employees can now quickly prototype ideas rather than writing long presentations, then test them with early users before committing resources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For all the hypergrowth and investor attention, Osika — dressed simply in a beige T-shirt and matching button-down, floppy hair framing his face — appeared very much at ease. The 30-something former particle physicist, who was the first employee at Sauna Labs before founding Lovable, has gone from open source developer to venture-backed founder to must-have conference guest in rapid succession. Yet he seemed more interested in discussing European work culture than dwelling on his company’s trajectory or the attention suddenly being showered on him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What I care about is that everyone who’s at the company, they’re mission driven, they really care about what they’re doing and how we as a team succeed,” he said, pushing back against Silicon Valley’s intensifying hustle culture. “The best people in my team today, most of them, they have kids, and they really, really care about what we’re doing. They’re not working 12 hours, six days a week.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though he added: “Although it’s a startup, so they’re probably working more than most jobs.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-10-at-11.20.57-PM.png?resize=1200,654" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Lovable, the Stockholm-based AI coding platform, is closing in on 8 million users, CEO Anton Osika told this editor during a sit-down on Monday, a major jump from the 2.3 million active users number the company shared in July. Osika said the company — which was founded almost exactly one year ago — is also seeing “100,000 new products built on Lovable every single day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The metrics suggest rapid growth of the startup, which has raised $228 million in total funding to date, including a $200 million round this summer that valued the company at $1.8 billion. Rumors have swirled in recent weeks — potentially sparked by its own investors — that new backers want to invest at a $5 billion valuation, though Osika said the company isn’t capital constrained and declined to discuss fundraising plans.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speaking to me onstage at the Web Summit event in Lisbon, Osika notably didn’t share another number: Lovable’s current annual recurring revenue. The company, which uses a mix of free and paid tiers, hit $100 million in ARR this June, a milestone it trumpeted publicly. But questions have emerged since about whether the vibe coding boom is sustainable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Research from Barclays this summer, along with Google Trends data, showed that traffic to some of the buzziest services, including Lovable and Vercel’s v0, had declined after peaking earlier this year. (Traffic to Lovable was down 40% as of September, according to the Barclays analysts.) “This waning traffic begs the question on whether app/site vibecoding has peaked out already or has just had a bit of a lull before interest ramps up,” they reportedly wrote in a note to investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Osika insisted retention remains strong, citing more than 100% net dollar retention — meaning users spend more over time. He also said the company has “just passed” the 100-employee mark and is now importing leadership talent from San Francisco to bolster its Stockholm headquarters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable emerged from GPT Engineer, an open source tool Osika built that went viral among developers. But he says he quickly realized the bigger opportunity lay with the 99% of people who don’t know how to code. “I woke up a few days after building GPT Engineer and I realized, look, we’re going to reimagine how you build software,” Osika said. “I biked to my co-founder’s place, and I said, I have this great idea. I woke him up.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The platform has attracted an eclectic user base. More than half of Fortune 500 companies are using Lovable to “supercharge creativity,” according to Osika. At the same time, he said, an 11-year-old in Lisbon built a Facebook clone for his school, while a Swedish duo is making $700,000 annually from a startup they launched seven months ago on the platform. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“What I hear from people trying Lovable is, ‘It just works,’” Osika said, crediting what he described as Swedish design sensibility.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Security remains a thornier issue for the vibe coding sector. When I raised a recent incident in which an app built with vibe coding tools leaked 72,000 images into the wild, including GPS data and user IDs, Osika acknowledged the problem. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The part of the engineering organization where we’re moving the quickest on hiring is security engineers,” he said, adding that his goal is to make building with Lovable “more secure than building with just human-written code.” In fact, he said, before users can deploy, Lovable now runs multiple security checks, though the platform still requires users building sensitive applications — banking apps, for instance — to hire security experts, just as they would with traditional development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Osika was similarly matter-of-fact when I asked about competition from OpenAI and Anthropic, the AI giants whose models power Lovable but that have also released their own coding agents. He sees the market as big enough for multiple winners. “If we can unlock more human creativity and human agency  . . . and just driving the change so that anyone can create if they have good ideas, [and] build businesses on top of that, that should be celebrated, regardless of whoever does that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a decidedly collegial stance in an industry not known for it. (Even Osika has engaged in some light social media sparring with Amjad Masad of competitor Replit.) But he said his focus right now is on building “the most intuitive experience for humans” rather than obsessing over rivals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Osika described Lovable’s mission as building “the last piece of software” — a platform where everything a product organization needs, from understanding users to deploying mission-critical features, can be done through a simple interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Demo, don’t memo,” a popular phrase among product leaders, captures how companies now use Lovable, he said. Employees can now quickly prototype ideas rather than writing long presentations, then test them with early users before committing resources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For all the hypergrowth and investor attention, Osika — dressed simply in a beige T-shirt and matching button-down, floppy hair framing his face — appeared very much at ease. The 30-something former particle physicist, who was the first employee at Sauna Labs before founding Lovable, has gone from open source developer to venture-backed founder to must-have conference guest in rapid succession. Yet he seemed more interested in discussing European work culture than dwelling on his company’s trajectory or the attention suddenly being showered on him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What I care about is that everyone who’s at the company, they’re mission driven, they really care about what they’re doing and how we as a team succeed,” he said, pushing back against Silicon Valley’s intensifying hustle culture. “The best people in my team today, most of them, they have kids, and they really, really care about what we’re doing. They’re not working 12 hours, six days a week.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though he added: “Although it’s a startup, so they’re probably working more than most jobs.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/lovable-says-its-nearing-8-million-users-as-the-year-old-ai-coding-startup-eyes-more-corporate-employees/</guid><pubDate>Mon, 10 Nov 2025 23:53:01 +0000</pubDate></item></channel></rss>