<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 17 Feb 2026 02:26:13 +0000</lastBuildDate><item><title>Tuning into the future of collaboration (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/16/1125881/tuning-into-the-future-of-collaboration/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Sam-Sabet-Brendan-Ittelson.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Shure&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When work went remote, the sound of business changed. What began as a scramble to make home offices functional has evolved into a revolution in how people hear and are heard. From education to enterprises, companies across industries have reimagined what clear, reliable communication can mean in a hybrid world. For major audio and communications enterprises like Shure and Zoom, that transformation has been powered by artificial intelligence, new acoustic technologies, and a shared mission: making connection effortless.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Necessity during the pandemic accelerated years of innovation in months.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;"Audio and video just working is a baseline for collaboration," says chief ecosystem officer at Zoom, Brendan Ittelson. "That expectation has shifted from connecting people to enhancing productivity and creativity across the entire ecosystem."&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Audio is a foundation for trust, understanding, and collaboration. Poor sound quality can distort meaning and fatigue listeners, while crisp audio and intelligent processing can make digital interactions feel nearly as natural as in-person exchanges.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;"If you think about the fundamental need here," adds chief technology officer at Shure, Sam Sabet, "It's the ability to amplify the audio and the information that's really needed, and diminish the unwanted sounds and audio so that we can enhance that experience and make it seamless for people to communicate."&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For both Ittelson and Sabet, AI now sits at the center of this progress. For Shure, machine learning powers real-time noise suppression, adaptive beamforming, and spatial audio that tunes itself to a room’s acoustics. For Zoom, AI underpins every layer of its platform, from dynamic noise reduction to automated meeting summaries and intelligent assistants that anticipate user needs. These tools are transforming communication from reactive to proactive, enabling systems that understand intent, context, and emotion.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;"Even if you're not working from home and coming into the office, the types of spaces and environments you try to collaborate in today are constantly changing because our needs are constantly changing," says Sabet. "Having software and algorithms that adapt seamlessly and self-optimize based on the acoustics of the room, based on the different layouts of the spaces where people collaborate in is instrumental."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The future, they suggest, is one where technology fades into the background. As audio devices and AI companions learn to self-optimize, users won’t think about microphones or meeting links. Instead, they’ll simply connect. Both companies are now exploring agentic AI systems and advanced wireless solutions that promise to make collaboration seamless across spaces, whether in classrooms, conference rooms, or virtual environments yet to come.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"It's about helping people focus on strategy and creativity instead of administrative busy work," says Ittelson.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Shure.&lt;/em&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;Full Transcript&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan Tatum: &lt;/em&gt;From MIT Technology Review, I'm Megan Tatum and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This episode is produced in partnership with Shure.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now as the pandemic ushered in the cultural shift that led to our increasingly virtual world, it also sparked a flurry of innovation in the audio and video industries to keep employees and customers connected and businesses running. Today we're going to talk about the AI technologies behind those innovations, the impact on audio innovation, and the continuing emerging opportunities for further advances in audio capabilities.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Two words for you: elevated audio.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My guests today are Sam Sabet, chief technology officer at Shure, and Brendan Ittelson, chief ecosystem officer at Zoom.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome Sam, welcome Brendan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam Sabet:&lt;/em&gt; Thank you, Megan. It's a pleasure to be here and I'm looking forward to this conversation with both you and Brendan. It should be a very exciting conversation.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Brendan Ittelson:&lt;/em&gt; Thank you so much for having me today. I'm looking forward to the conversation and all the topics we have to dive into on this area.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. Lovely to have you both here. And Sam, just to set some context, I wonder if we could start with the pandemic and the innovation that really was born out of necessity. I mean, when it became clear that we were all going to be virtual for the foreseeable future, I wonder what was the first technological mission for Shure?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah, very good question. The pandemic really accelerated a lot of innovation around virtual communications and fundamentally how we perform our everyday jobs remotely. One of our first technological mission when the pandemic happened and everybody ended up going home and performing their functions remotely was to make sure that people could continue to communicate effectively, whether that's for business meetings, virtual events, or educational purposes. We focused on collaboration and enhancing collaboration tools. And ideally what we were aiming to do, or we focused on, was to basically improve the ease of use and configuration of audio tool sets.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Because unlike the office environment where it might be a lot more controlled, people are working from non-traditional areas like home offices or other makeshift solutions, we needed to make sure that people could still get pristine audio and that studio level audio even in uncontrolled environments that are not really made for that. We expedited development in our software solutions. We created tool sets that allowed for ease of deployment and remote configuration and management so we could enable people to continue doing the things they needed to do without having to worry about the underlying technology.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;And Brendan, during that time, it seemed everyone became a Zoom user of some sort. I mean, what was the first mission at Zoom when virtual connection became this necessity for everyone?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Well, our mission fundamentally didn't change. It's always been about delivering frictionless communications. What shifted was the urgency and the magnitude of what we were doing. Our focus shifted on how we do this reliably, securely, and to scale to ensure these millions of new users could connect instantly without friction. We really shifted our thinking of being just a business continuity tool to becoming a lifeline for so many individuals and industries. The stories that we heard across education, healthcare, and just general human connection, the number of those moments that matter to people that we were able to help facilitate just became so important. We really focused on how can we be there and make it frictionless so folks can focus on that human connection. And that accelerated our thinking in terms of innovation and reinforced the thought that we need to focus on the simplicity, accessibility, and trust in communication technology so that people could focus on that connection and not the technology that makes it possible.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;That's so true. It did really just become an absolute lifeline for people, didn't it? And before we dive into the technologies beyond these emerging capabilities, I wonder if we could first talk about just the importance of clear audio. I mean, Sam, as much as we all worry over how we look on Zoom, is how we sound perhaps as or even more impactful?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah, you're absolutely correct. I mean, clear audio is absolutely critical for effective communications. Video quality is very important absolutely, but poor audio can really hinder understanding and engagement. As a matter of fact, there's studies and research from areas such as Yale University that say that poor audio can make understanding somewhat more challenged and even affect retention of information. Especially in an educational type environment where there's a lot of background noise and very differing types of spaces like auditoriums and lecture halls, it really becomes a high priority that you have great audio quality. And during the pandemic, as you said, and as Brendan rightly said, it became one of our highest priorities to focus on technologies like beamforming mics and ways to focus on the speaker's voice and minimize that unwanted background noise so that we could ensure that the communication was efficient, was well understood, and that it removed the distraction so people could be able to actually communicate and retain the information that was being shared.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;It is incredible just how impactful audio can be, can't it? Brendan, I mean as you said, remote and hybrid collaboration is part of Zoom's DNA. What observations can you share about how users have grown along with the technological advancements and maybe how their expectations have grown as well?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. I mean, users now expect seamless and intelligent experiences. Audio and video just working is a baseline for collaboration. That expectation has shifted from connecting people to enhancing productivity and creativity across the entire ecosystem. When we look at it, we're really looking at these trends in terms of how people want to be better when they're at home. For example, AI-powered tools like Smart Summaries, translation and noise suppression to help people stay productive and connected no matter where they're working. But then this also comes into play at the office. We're starting to see folks that dive into our technology like Intelligent Director and Smart Name Tags that create that meeting equity even when they're in a conference room.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So, the remote experience and the room experience all are similar and create that same ability to be seen, heard, and contribute. And we're now diving further into this that it's beyond just meetings. Zoom is really transforming into an AI-first work platform that's focused on human connection. And so that goes beyond the meetings into things like Chat, Zoom Docs, Zoom Events and Webinars, the Zoom Contact Center and more. And all of this being brought together using our AI Companion at its core to help connect all of those different points of connection for individuals.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;I mean, so Brendan, we know it wasn't only workplaces that were affected by the pandemic, it was also the education sector that had to undergo a huge change. I wondered if you could talk a little bit about how Zoom has operated in that higher education sphere as well.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. Education has always been a focus for Zoom and an area that we've believed in. Because education and learning is something as a company we value and so we have invested in that sector. And personally being the son of academics, it is always an area that I find fascinating. We continue to invest in terms of how do we make the classroom a stronger space? And especially now that the classroom has changed, where it can be in person, it can be virtual, it can be a mix. And using Zoom and its tools, we're able to help bridge all those different scenarios to make learning accessible to students no matter their means.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's what truly excites us, is being able to have that technology that allows people to pursue their desires, their interests, and really up-level their pursuits and inspire more. We're constantly investing in how to allow those messages to get out and to integrate in the flow of communication and collaboration that higher education uses, whether that's being integrated into the classroom, into learning management systems, to make that a seamless flow so that students and their educators can just collaborate seamlessly. And also that we can support all the infrastructure and administration that helps make that possible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. Such an important thing. And Sam, Shure as well, could you talk to us a bit about how you worked in that kind of education space as well from an audio point of view?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Absolutely. Actually, this is a topic that's near and dear to my heart because I'm actually an adjunct professor in my free time.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, wow. Very impressive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;And the challenges of trying to do this sort of a hybrid lecture, if you will. And Shure has been particularly well suited for this environment and we've been focused on it and investing in technologies there for decades. If you think about how a lecture hall is structured, it's a little different than just having a meeting around the conference table. And Shure has focused on creating products that allow this combination of a presenter scenario along with a meeting space plus the far end where users or students are remote, they can hear intelligibly what's happening in the lecture hall, but they can also participate.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Between our products like the Ceiling Mic Arrays and our wireless microphones that are purpose built for presenters and educators like our MXW neXt product line, we've created technologies that allow those two previously separate worlds to integrate together. And then add that onto integrating with Zoom and other products that allow for that collaboration has been very instrumental. And again, being a user and providing those lectures, I can see a night and day difference and just how much more effective my lectures are today from where they were five to six years ago. And that's all just made possible by all the technologies that are purpose built for these scenarios and integrating more with these powerful tools that just make the job so much more seamless.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely fascinating that you got to put the technology to use yourself as well to check that it was all working well. And you mentioned AI there, of course. I mean, Sam, what AI technologies have had the most significant impact on recent audio advancements too?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah. Absolutely. If you think about the fundamental need here, it's the ability to amplify the audio and the information that's really needed and diminish the unwanted sounds and audio so that we can enhance that experience and make it seamless for people to communicate. With our innovations at Shure, we've leveraged the cutting-edge technologies to both enhance communication effectiveness and to align seamlessly with evolving features in unified communications like the ones that Brandon just mentioned in the Zoom platforms.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We partner with industry leaders like Zoom to ensure that we're providing the ability to be able to focus on that needed audio and eliminate all the background distractions. AI has transformed that audio technology with things like machine learning algorithms that enable us to do more real-time audio processing and significantly enhancing things like noise reduction and speech isolation. Just to give you a simple example, our IntelliMix Room audio processing software that we've released as well as part of a complete room solution uses AI to optimize sound in different environments.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And really that's one of the fundamental changes in this period, whether that's pandemic or post-pandemic, is that the key is really flexibility and being able to adapt to changing work environments. Even if you're not working from home and coming into the office, the types of spaces and environments you try to collaborate in today are constantly changing because our needs are constantly changing. And so having software and algorithms that adapt seamlessly and are able to self-optimize based on the acoustics of the room, based on the different layouts of the spaces where people collaborate in is instrumental.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And then last but not least, AI has transformed the way audio and video integrate. For example, we utilize voice recognition systems that integrate with intelligent cameras so that we enable voice tracking technology so that cameras can not only identify who's speaking, but you have the ability to hear and see people clearly. And that in general just enhances the overall communication experience.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Wow. It's just so much innovation in quite a short space of time really. I mean, Brendan, you mentioned AI a little bit there beforehand, but I wonder what other AI technologies have had the biggest impact as Zoom builds out its own emerging capabilities?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. And I couldn't agree more with Sam that, I mean, AI has made such a big shift and it's really across the spectrum. And when I think about it, there's almost three tiers when you look at the stack. You start off at the raw audio where AI is doing those things like noise suppression, echo cancellation, voice enhancements. All of that just makes this amazing audio signal that can then go into the next layer, which is the speech AI and natural language processing. Which starts to open up those items such as the real-time transcription, translation, searchable content to make the communication not just what's heard, but making it more accessible to more individuals and inclusive by providing that content in a format that is best for them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And then you take those two layers and put the generative and agentic AI on top of that, that can start surfacing insights, summarize the conversation, and even take actions on someone's behalf. It really starts to change the way that people work and how they have access and allows them to connect. I think it is a huge shift and I'm very excited by how those three levels start to interact to really enable people to do more and to connect thanks to AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah. Absolutely. So much rich information that can come out from a single call now because of those sorts of tools. And following on from that, Brendan, I mean, you mentioned before the Zoom AI Companion. I wondered if you could talk a bit about what were your top priorities when building that product to ensure it was truly useful for your customers?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. When we developed AI Companion, we had two priority focus areas from day one, trust and security, and then accuracy and relevance. On the trust side, it was a non-negotiable that customer data wouldn't be used to train our models. People need to know that their conversations and content are private and secure.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Of course.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;And then with accuracy, we needed to ensure AI outputs weren't generic but grounded in the actual context of a meeting, a chat or a product. But the real story here when I think about AI Companion is the customer value that it delivers. AI Companion helps people save time with meeting recaps, task generation, and proactive prep for the next session. It reduces that friction in hybrid work, whether you're in a meeting room, a Zoom room, or collaborating across different collaboration tools like Microsoft or Google. And it enables more equitable participation by surfacing the right context for everyone no matter where and how they're working.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;All this leads to a result where it's practical, trustworthy, and embedded where work happens. And it's just not another tool to manage, it's there in someone's flow of work to help them along the way.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah. That trust piece is just so important, isn't it, today? And Sam, as much as AI has impacted audio innovation, audio has also had an impact on AI capabilities. I wondered if you could talk a little bit about audio as a data input and the advancements technologies like large language models, LLMs, are enabling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Absolutely. Audio is really a rich data source that's added a new dimension to AI capabilities. If you think about speech recognition or natural language processing, they've had significant advances due to audio data that's provided for them. And to Brendan's point about trust and accuracy, I like to think of the products that Shure enables customers with as essentially the eyes and ears in the room for leading AI companions just like the Zoom AI Companion. You really need that pristine audio input to be able to trust the accuracy of what the AI generates. These AI Companions have been very instrumental in the way we do business every day. I mean, between transcription, speaker attributions, the ability to add action items within a meeting and be able to track what's happening in our interactions, all of that really has to rely on that accurate and pristine input from audio into the AI. I feel that further improves the trust that our end users have to the results of AI and be able to leverage it more.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If you think about it, if you look at how AI audio inputs enhance that interactive AI system, it enables more natural and intuitive interactions with AI. And it really allows for that seamless integration and the ability for users to use it without having to worry about, is the room set up correctly? Is the audio level proper? And when we talk even about agentic AI, we're working on future developments where systems can self-heal or detect that there are issues in the environment so that they can autocorrect and adapt in all these different environments and further enable the AI to be able to do a much more effective job, if you will.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Sam, you touched on future developments there. I wonder if we could close our conversation today with a bit of a future forward look, if we could. Brendan, can you share innovations that Zoom is working on now and what are you most excited to see come to fruition?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Well, your timing for this question is absolutely perfect because we've just wrapped up Zoomtopia 2025.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, wow.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;And this is where we discussed a lot of the new AI innovations that we have coming to Zoom. Starting off, there's AI Companion 3.0. And we've launched this next generation of agentic AI capabilities in Zoom Workplace. And with 3.0 when it releases, it isn't just about transcribing, it's turned into really a platform that helps you with follow-up task, prep for your next conversation, and even proactively suggest how to free up your time. For example, AI Companion can help you schedule meetings intelligently across time zones, suggest which meetings you can skip, and still stay informed and even prepare you with context and insights before you walk into the conversation. It's about helping people focus on strategy and creativity instead of administrative busy work. And for hybrid work specifically, we introduced Zoomie Group Assistant, which will be a big leap for hybrid collaboration.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Acting as an assistant for a group chat and meetings, you can simply ask, “@Zoomie, what's the latest update on the project?” Or “@Zoomie, what are the team's action items?” And then get instant answers. Or because we're talking about audio here, you can go into a conference room and say, "Hey, Zoomie," and get help with things like checking into a room, adjusting lights, temperature, or even sharing your screen. And while all these are built-in features, we're also expanding the platform to allow custom AI agents through our AI Studio, so organizations can bring their own agents or integrate with third-party ones.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;Zoom has always believed in an open platform and philosophy and that is continuing. Folks using AI Companion 3.0 will be able to use agents across platforms to work with the workflows that they have across all the different SaaS vendors that they might have in their environment, whether that's Google, Microsoft, ServiceNow, Cisco, and so many other tools.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. It certainly sounds like a tool I could use in my work, so I look forward to hearing more about that. And Sam, we've touched on there are so many exciting things happening in audio too. What are you working on at Shure? And what are you most excited to see come to fruition?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;At Shure, our engineering teams are really working on a range of exciting projects, but particularly we're working on developing new collaboration solutions that are integral for IT end users. And these integrate obviously with the leading UC platforms.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We're integrating audio and video technologies that are scalable, reliable solutions. And we want to be able to seamlessly connect these to cloud services so that we can leverage both AI technologies and the tool sets available to optimize every type of workspace essentially. Not just meeting rooms, but lecture halls, work from home scenarios, et cetera.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The other area that we really focus on in terms of our reliability and quality really comes from our DNA in the pro audio world. And that's really all-around wireless audio technologies. We're developing our next-generation wireless systems and these are going to offer even greater reliability and range. And they really become ideal for everything from a large-scale event to personal home use and the gamut across that whole spectrum. And I think all of that in partnership with our partners like Zoom will help just facilitate the modern workspace.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. So much exciting innovation clearly going on behind the scenes. Thank you both so much.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That was Sam Sabet, chief technology officer at Shure, and Brendan Ittelson, chief ecosystem officer at Zoom, whom I spoke with from Brighton in England.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor at Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology and you can find us in print on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. And if you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review and this episode was produced by Giro Studios. Thanks for listening.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Sam-Sabet-Brendan-Ittelson.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Shure&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When work went remote, the sound of business changed. What began as a scramble to make home offices functional has evolved into a revolution in how people hear and are heard. From education to enterprises, companies across industries have reimagined what clear, reliable communication can mean in a hybrid world. For major audio and communications enterprises like Shure and Zoom, that transformation has been powered by artificial intelligence, new acoustic technologies, and a shared mission: making connection effortless.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Necessity during the pandemic accelerated years of innovation in months.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;"Audio and video just working is a baseline for collaboration," says chief ecosystem officer at Zoom, Brendan Ittelson. "That expectation has shifted from connecting people to enhancing productivity and creativity across the entire ecosystem."&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Audio is a foundation for trust, understanding, and collaboration. Poor sound quality can distort meaning and fatigue listeners, while crisp audio and intelligent processing can make digital interactions feel nearly as natural as in-person exchanges.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;"If you think about the fundamental need here," adds chief technology officer at Shure, Sam Sabet, "It's the ability to amplify the audio and the information that's really needed, and diminish the unwanted sounds and audio so that we can enhance that experience and make it seamless for people to communicate."&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For both Ittelson and Sabet, AI now sits at the center of this progress. For Shure, machine learning powers real-time noise suppression, adaptive beamforming, and spatial audio that tunes itself to a room’s acoustics. For Zoom, AI underpins every layer of its platform, from dynamic noise reduction to automated meeting summaries and intelligent assistants that anticipate user needs. These tools are transforming communication from reactive to proactive, enabling systems that understand intent, context, and emotion.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;"Even if you're not working from home and coming into the office, the types of spaces and environments you try to collaborate in today are constantly changing because our needs are constantly changing," says Sabet. "Having software and algorithms that adapt seamlessly and self-optimize based on the acoustics of the room, based on the different layouts of the spaces where people collaborate in is instrumental."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The future, they suggest, is one where technology fades into the background. As audio devices and AI companions learn to self-optimize, users won’t think about microphones or meeting links. Instead, they’ll simply connect. Both companies are now exploring agentic AI systems and advanced wireless solutions that promise to make collaboration seamless across spaces, whether in classrooms, conference rooms, or virtual environments yet to come.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"It's about helping people focus on strategy and creativity instead of administrative busy work," says Ittelson.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Shure.&lt;/em&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;Full Transcript&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan Tatum: &lt;/em&gt;From MIT Technology Review, I'm Megan Tatum and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This episode is produced in partnership with Shure.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now as the pandemic ushered in the cultural shift that led to our increasingly virtual world, it also sparked a flurry of innovation in the audio and video industries to keep employees and customers connected and businesses running. Today we're going to talk about the AI technologies behind those innovations, the impact on audio innovation, and the continuing emerging opportunities for further advances in audio capabilities.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Two words for you: elevated audio.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My guests today are Sam Sabet, chief technology officer at Shure, and Brendan Ittelson, chief ecosystem officer at Zoom.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome Sam, welcome Brendan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam Sabet:&lt;/em&gt; Thank you, Megan. It's a pleasure to be here and I'm looking forward to this conversation with both you and Brendan. It should be a very exciting conversation.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Brendan Ittelson:&lt;/em&gt; Thank you so much for having me today. I'm looking forward to the conversation and all the topics we have to dive into on this area.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. Lovely to have you both here. And Sam, just to set some context, I wonder if we could start with the pandemic and the innovation that really was born out of necessity. I mean, when it became clear that we were all going to be virtual for the foreseeable future, I wonder what was the first technological mission for Shure?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah, very good question. The pandemic really accelerated a lot of innovation around virtual communications and fundamentally how we perform our everyday jobs remotely. One of our first technological mission when the pandemic happened and everybody ended up going home and performing their functions remotely was to make sure that people could continue to communicate effectively, whether that's for business meetings, virtual events, or educational purposes. We focused on collaboration and enhancing collaboration tools. And ideally what we were aiming to do, or we focused on, was to basically improve the ease of use and configuration of audio tool sets.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Because unlike the office environment where it might be a lot more controlled, people are working from non-traditional areas like home offices or other makeshift solutions, we needed to make sure that people could still get pristine audio and that studio level audio even in uncontrolled environments that are not really made for that. We expedited development in our software solutions. We created tool sets that allowed for ease of deployment and remote configuration and management so we could enable people to continue doing the things they needed to do without having to worry about the underlying technology.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;And Brendan, during that time, it seemed everyone became a Zoom user of some sort. I mean, what was the first mission at Zoom when virtual connection became this necessity for everyone?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Well, our mission fundamentally didn't change. It's always been about delivering frictionless communications. What shifted was the urgency and the magnitude of what we were doing. Our focus shifted on how we do this reliably, securely, and to scale to ensure these millions of new users could connect instantly without friction. We really shifted our thinking of being just a business continuity tool to becoming a lifeline for so many individuals and industries. The stories that we heard across education, healthcare, and just general human connection, the number of those moments that matter to people that we were able to help facilitate just became so important. We really focused on how can we be there and make it frictionless so folks can focus on that human connection. And that accelerated our thinking in terms of innovation and reinforced the thought that we need to focus on the simplicity, accessibility, and trust in communication technology so that people could focus on that connection and not the technology that makes it possible.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;That's so true. It did really just become an absolute lifeline for people, didn't it? And before we dive into the technologies beyond these emerging capabilities, I wonder if we could first talk about just the importance of clear audio. I mean, Sam, as much as we all worry over how we look on Zoom, is how we sound perhaps as or even more impactful?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah, you're absolutely correct. I mean, clear audio is absolutely critical for effective communications. Video quality is very important absolutely, but poor audio can really hinder understanding and engagement. As a matter of fact, there's studies and research from areas such as Yale University that say that poor audio can make understanding somewhat more challenged and even affect retention of information. Especially in an educational type environment where there's a lot of background noise and very differing types of spaces like auditoriums and lecture halls, it really becomes a high priority that you have great audio quality. And during the pandemic, as you said, and as Brendan rightly said, it became one of our highest priorities to focus on technologies like beamforming mics and ways to focus on the speaker's voice and minimize that unwanted background noise so that we could ensure that the communication was efficient, was well understood, and that it removed the distraction so people could be able to actually communicate and retain the information that was being shared.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;It is incredible just how impactful audio can be, can't it? Brendan, I mean as you said, remote and hybrid collaboration is part of Zoom's DNA. What observations can you share about how users have grown along with the technological advancements and maybe how their expectations have grown as well?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. I mean, users now expect seamless and intelligent experiences. Audio and video just working is a baseline for collaboration. That expectation has shifted from connecting people to enhancing productivity and creativity across the entire ecosystem. When we look at it, we're really looking at these trends in terms of how people want to be better when they're at home. For example, AI-powered tools like Smart Summaries, translation and noise suppression to help people stay productive and connected no matter where they're working. But then this also comes into play at the office. We're starting to see folks that dive into our technology like Intelligent Director and Smart Name Tags that create that meeting equity even when they're in a conference room.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So, the remote experience and the room experience all are similar and create that same ability to be seen, heard, and contribute. And we're now diving further into this that it's beyond just meetings. Zoom is really transforming into an AI-first work platform that's focused on human connection. And so that goes beyond the meetings into things like Chat, Zoom Docs, Zoom Events and Webinars, the Zoom Contact Center and more. And all of this being brought together using our AI Companion at its core to help connect all of those different points of connection for individuals.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;I mean, so Brendan, we know it wasn't only workplaces that were affected by the pandemic, it was also the education sector that had to undergo a huge change. I wondered if you could talk a little bit about how Zoom has operated in that higher education sphere as well.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. Education has always been a focus for Zoom and an area that we've believed in. Because education and learning is something as a company we value and so we have invested in that sector. And personally being the son of academics, it is always an area that I find fascinating. We continue to invest in terms of how do we make the classroom a stronger space? And especially now that the classroom has changed, where it can be in person, it can be virtual, it can be a mix. And using Zoom and its tools, we're able to help bridge all those different scenarios to make learning accessible to students no matter their means.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's what truly excites us, is being able to have that technology that allows people to pursue their desires, their interests, and really up-level their pursuits and inspire more. We're constantly investing in how to allow those messages to get out and to integrate in the flow of communication and collaboration that higher education uses, whether that's being integrated into the classroom, into learning management systems, to make that a seamless flow so that students and their educators can just collaborate seamlessly. And also that we can support all the infrastructure and administration that helps make that possible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. Such an important thing. And Sam, Shure as well, could you talk to us a bit about how you worked in that kind of education space as well from an audio point of view?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Absolutely. Actually, this is a topic that's near and dear to my heart because I'm actually an adjunct professor in my free time.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, wow. Very impressive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;And the challenges of trying to do this sort of a hybrid lecture, if you will. And Shure has been particularly well suited for this environment and we've been focused on it and investing in technologies there for decades. If you think about how a lecture hall is structured, it's a little different than just having a meeting around the conference table. And Shure has focused on creating products that allow this combination of a presenter scenario along with a meeting space plus the far end where users or students are remote, they can hear intelligibly what's happening in the lecture hall, but they can also participate.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Between our products like the Ceiling Mic Arrays and our wireless microphones that are purpose built for presenters and educators like our MXW neXt product line, we've created technologies that allow those two previously separate worlds to integrate together. And then add that onto integrating with Zoom and other products that allow for that collaboration has been very instrumental. And again, being a user and providing those lectures, I can see a night and day difference and just how much more effective my lectures are today from where they were five to six years ago. And that's all just made possible by all the technologies that are purpose built for these scenarios and integrating more with these powerful tools that just make the job so much more seamless.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely fascinating that you got to put the technology to use yourself as well to check that it was all working well. And you mentioned AI there, of course. I mean, Sam, what AI technologies have had the most significant impact on recent audio advancements too?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Yeah. Absolutely. If you think about the fundamental need here, it's the ability to amplify the audio and the information that's really needed and diminish the unwanted sounds and audio so that we can enhance that experience and make it seamless for people to communicate. With our innovations at Shure, we've leveraged the cutting-edge technologies to both enhance communication effectiveness and to align seamlessly with evolving features in unified communications like the ones that Brandon just mentioned in the Zoom platforms.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We partner with industry leaders like Zoom to ensure that we're providing the ability to be able to focus on that needed audio and eliminate all the background distractions. AI has transformed that audio technology with things like machine learning algorithms that enable us to do more real-time audio processing and significantly enhancing things like noise reduction and speech isolation. Just to give you a simple example, our IntelliMix Room audio processing software that we've released as well as part of a complete room solution uses AI to optimize sound in different environments.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And really that's one of the fundamental changes in this period, whether that's pandemic or post-pandemic, is that the key is really flexibility and being able to adapt to changing work environments. Even if you're not working from home and coming into the office, the types of spaces and environments you try to collaborate in today are constantly changing because our needs are constantly changing. And so having software and algorithms that adapt seamlessly and are able to self-optimize based on the acoustics of the room, based on the different layouts of the spaces where people collaborate in is instrumental.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And then last but not least, AI has transformed the way audio and video integrate. For example, we utilize voice recognition systems that integrate with intelligent cameras so that we enable voice tracking technology so that cameras can not only identify who's speaking, but you have the ability to hear and see people clearly. And that in general just enhances the overall communication experience.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Wow. It's just so much innovation in quite a short space of time really. I mean, Brendan, you mentioned AI a little bit there beforehand, but I wonder what other AI technologies have had the biggest impact as Zoom builds out its own emerging capabilities?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. And I couldn't agree more with Sam that, I mean, AI has made such a big shift and it's really across the spectrum. And when I think about it, there's almost three tiers when you look at the stack. You start off at the raw audio where AI is doing those things like noise suppression, echo cancellation, voice enhancements. All of that just makes this amazing audio signal that can then go into the next layer, which is the speech AI and natural language processing. Which starts to open up those items such as the real-time transcription, translation, searchable content to make the communication not just what's heard, but making it more accessible to more individuals and inclusive by providing that content in a format that is best for them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And then you take those two layers and put the generative and agentic AI on top of that, that can start surfacing insights, summarize the conversation, and even take actions on someone's behalf. It really starts to change the way that people work and how they have access and allows them to connect. I think it is a huge shift and I'm very excited by how those three levels start to interact to really enable people to do more and to connect thanks to AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah. Absolutely. So much rich information that can come out from a single call now because of those sorts of tools. And following on from that, Brendan, I mean, you mentioned before the Zoom AI Companion. I wondered if you could talk a bit about what were your top priorities when building that product to ensure it was truly useful for your customers?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Definitely. When we developed AI Companion, we had two priority focus areas from day one, trust and security, and then accuracy and relevance. On the trust side, it was a non-negotiable that customer data wouldn't be used to train our models. People need to know that their conversations and content are private and secure.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Of course.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;And then with accuracy, we needed to ensure AI outputs weren't generic but grounded in the actual context of a meeting, a chat or a product. But the real story here when I think about AI Companion is the customer value that it delivers. AI Companion helps people save time with meeting recaps, task generation, and proactive prep for the next session. It reduces that friction in hybrid work, whether you're in a meeting room, a Zoom room, or collaborating across different collaboration tools like Microsoft or Google. And it enables more equitable participation by surfacing the right context for everyone no matter where and how they're working.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;All this leads to a result where it's practical, trustworthy, and embedded where work happens. And it's just not another tool to manage, it's there in someone's flow of work to help them along the way.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Yeah. That trust piece is just so important, isn't it, today? And Sam, as much as AI has impacted audio innovation, audio has also had an impact on AI capabilities. I wondered if you could talk a little bit about audio as a data input and the advancements technologies like large language models, LLMs, are enabling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;Absolutely. Audio is really a rich data source that's added a new dimension to AI capabilities. If you think about speech recognition or natural language processing, they've had significant advances due to audio data that's provided for them. And to Brendan's point about trust and accuracy, I like to think of the products that Shure enables customers with as essentially the eyes and ears in the room for leading AI companions just like the Zoom AI Companion. You really need that pristine audio input to be able to trust the accuracy of what the AI generates. These AI Companions have been very instrumental in the way we do business every day. I mean, between transcription, speaker attributions, the ability to add action items within a meeting and be able to track what's happening in our interactions, all of that really has to rely on that accurate and pristine input from audio into the AI. I feel that further improves the trust that our end users have to the results of AI and be able to leverage it more.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If you think about it, if you look at how AI audio inputs enhance that interactive AI system, it enables more natural and intuitive interactions with AI. And it really allows for that seamless integration and the ability for users to use it without having to worry about, is the room set up correctly? Is the audio level proper? And when we talk even about agentic AI, we're working on future developments where systems can self-heal or detect that there are issues in the environment so that they can autocorrect and adapt in all these different environments and further enable the AI to be able to do a much more effective job, if you will.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Sam, you touched on future developments there. I wonder if we could close our conversation today with a bit of a future forward look, if we could. Brendan, can you share innovations that Zoom is working on now and what are you most excited to see come to fruition?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;Well, your timing for this question is absolutely perfect because we've just wrapped up Zoomtopia 2025.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Oh, wow.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Brendan: &lt;/em&gt;And this is where we discussed a lot of the new AI innovations that we have coming to Zoom. Starting off, there's AI Companion 3.0. And we've launched this next generation of agentic AI capabilities in Zoom Workplace. And with 3.0 when it releases, it isn't just about transcribing, it's turned into really a platform that helps you with follow-up task, prep for your next conversation, and even proactively suggest how to free up your time. For example, AI Companion can help you schedule meetings intelligently across time zones, suggest which meetings you can skip, and still stay informed and even prepare you with context and insights before you walk into the conversation. It's about helping people focus on strategy and creativity instead of administrative busy work. And for hybrid work specifically, we introduced Zoomie Group Assistant, which will be a big leap for hybrid collaboration.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Acting as an assistant for a group chat and meetings, you can simply ask, “@Zoomie, what's the latest update on the project?” Or “@Zoomie, what are the team's action items?” And then get instant answers. Or because we're talking about audio here, you can go into a conference room and say, "Hey, Zoomie," and get help with things like checking into a room, adjusting lights, temperature, or even sharing your screen. And while all these are built-in features, we're also expanding the platform to allow custom AI agents through our AI Studio, so organizations can bring their own agents or integrate with third-party ones.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;Zoom has always believed in an open platform and philosophy and that is continuing. Folks using AI Companion 3.0 will be able to use agents across platforms to work with the workflows that they have across all the different SaaS vendors that they might have in their environment, whether that's Google, Microsoft, ServiceNow, Cisco, and so many other tools.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. It certainly sounds like a tool I could use in my work, so I look forward to hearing more about that. And Sam, we've touched on there are so many exciting things happening in audio too. What are you working on at Shure? And what are you most excited to see come to fruition?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Sam: &lt;/em&gt;At Shure, our engineering teams are really working on a range of exciting projects, but particularly we're working on developing new collaboration solutions that are integral for IT end users. And these integrate obviously with the leading UC platforms.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We're integrating audio and video technologies that are scalable, reliable solutions. And we want to be able to seamlessly connect these to cloud services so that we can leverage both AI technologies and the tool sets available to optimize every type of workspace essentially. Not just meeting rooms, but lecture halls, work from home scenarios, et cetera.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The other area that we really focus on in terms of our reliability and quality really comes from our DNA in the pro audio world. And that's really all-around wireless audio technologies. We're developing our next-generation wireless systems and these are going to offer even greater reliability and range. And they really become ideal for everything from a large-scale event to personal home use and the gamut across that whole spectrum. And I think all of that in partnership with our partners like Zoom will help just facilitate the modern workspace.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Absolutely. So much exciting innovation clearly going on behind the scenes. Thank you both so much.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That was Sam Sabet, chief technology officer at Shure, and Brendan Ittelson, chief ecosystem officer at Zoom, whom I spoke with from Brighton in England.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor at Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology and you can find us in print on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. And if you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review and this episode was produced by Giro Studios. Thanks for listening.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/16/1125881/tuning-into-the-future-of-collaboration/</guid><pubDate>Mon, 16 Feb 2026 15:00:00 +0000</pubDate></item><item><title>[NEW] How Ricursive Intelligence raised $335M at a $4B valuation in 4 months (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/16/how-ricursive-intelligence-raised-335m-at-a-4b-valuation-in-4-months/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Ricursive-Intelligence-founders.png?resize=1200,943" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The co-founders of startup Ricursive Intelligence seemed destined to be co-founders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anna Goldie, CEO, and Azalia Mirhoseini, CTO, are so well-known in the AI community that they were among those AI engineers who “got those weird emails from Zuckerberg making crazy offers to us,” Goldie told TechCrunch, chuckling. (They didn’t take the offers.) The pair worked at Google Brain together and were early employees at Anthropic.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They earned acclaim at Google by creating the Alpha Chip — an AI tool that could generate solid chip layouts in hours — a process that normally takes human designers a year or more. The tool helped design three generations of Google’s Tensor Processing Units.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That pedigree explains why, just four months after launching Ricursive, they last month announced a $300 million Series A round at a $4 billion valuation led by Lightspeed, just a couple of months after raising a $35 million seed round led by Sequoia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ricursive is building AI tools that design chips, not the chips themselves. That makes them fundamentally different from nearly every other AI chip startup: they’re not a wannabe Nvidia competitor. In fact, Nvidia is an investor. The GPU giant, along with AMD, Intel, and every other chip maker, are the startup’s target customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to enable any chip, like a custom chip or a more traditional chip, any kind of chip, to be built in an automated and very accelerated way. We’re using AI to do that,” Mirhoseini told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Their paths first crossed at Stanford, where Goldie earned her PhD as Mirhoseini taught computer science classes. Since then, their careers have been in lockstep. “We started at Google Brain on the same day. We left Google Brain on the same day. We joined Anthropic on the same day. We left Anthropic on the same day. We rejoined Google on the same day, and then we left Google again on the same day. Then we started this company together on the same day,” Goldie recounted.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;During their time at Google, the colleagues were so close they even worked out together, both enjoying circuit training. The pun wasn’t lost on Jeff Dean, the famed Google engineer who was their collaborator. He nicknamed their Alpha Chip project “chip circuit training” — a play on their shared workout routine. Internally, the pair also got a nickname: A&amp;amp;A.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Alpha Chip earned them industry notice, but it also attracted controversy. In 2022, one of their colleagues at Google was fired, Wired reported, after he spent years trying to discredit A&amp;amp;A and their chip work, even though that work was used to help produce some of Google’s most important, bet-the-business AI chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Their Alpha Chip project at Google Brain proved the concept that would become Ricursive — using AI to dramatically accelerate chip design.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-designing-chips-is-hard"&gt;Designing chips is hard&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The issue is, computer chips have millions to billions of logic gate components integrated on their silicon wafer. Human designers can spend a year or more placing those components on the chip to ensure performance, good power utilization and any other design needs. Digitally determining the placement of such infinitesimally small components with precision is, as you might expect, hard.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alpha Chip “could generate a very high-quality layout in, like, six hours. And the cool thing about this approach was that it actually learns from experience,” Goldie said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The premise of their AI chip design work is to use “a reward signal” that rates how good the design is. The agent then takes that rating to “update the parameters of its deep neural network to get better,” Goldie said. After completing thousands of designs, the agent got really good. It also got faster as it learned, the founders say.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ricursive’s platform will take the concept further. The AI chip designer they are building will “learn across different chips,” Goldie said. So each chip it designs should help it become a better designer for every next chip. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ricursive’s platform also makes use of LLMs and will handle everything from component placement through design verification. Any company that makes electronics and needs chips is their target customer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If their platform proves itself, as it seems likely to do, Ricursive could play a role in the moonshot goal of achieving artificial general intelligence (AGI). Indeed, their ultimate vision is designing AI chips, meaning the AI will essentially design its own computer brains.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Chips are the fuel for AI,” Goldie said. “I think by building more powerful chips, that’s the best way to advance that frontier.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirhoseini adds that the lengthy chip-design process is constraining how quickly AI can advance. “We think we can also enable this fast co-evolution of the models and the chips that basically power them,” she said. So AI can grow smarter faster.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If the thought of AI designing its own brains at ever increasing speeds brings visions of Skynet and the Terminator to mind, the founders point out that there’s a more positive, immediate and, they think, more likely benefit: hardware efficiency.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;When AI Labs can design far more efficient chips (and, eventually all the underlying hardware), their growth won’t have to consume so much of the world’s resources.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We could design a computer architecture that’s uniquely suited to that model, and we could achieve almost a 10x improvement in performance per total cost of ownership,” Goldie said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the young startup won’t name its early customers, the founders say that they’ve heard from every big chip making name you can imagine. Unsurprisingly, they have their pick of their first development partners, too.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Ricursive-Intelligence-founders.png?resize=1200,943" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The co-founders of startup Ricursive Intelligence seemed destined to be co-founders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anna Goldie, CEO, and Azalia Mirhoseini, CTO, are so well-known in the AI community that they were among those AI engineers who “got those weird emails from Zuckerberg making crazy offers to us,” Goldie told TechCrunch, chuckling. (They didn’t take the offers.) The pair worked at Google Brain together and were early employees at Anthropic.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They earned acclaim at Google by creating the Alpha Chip — an AI tool that could generate solid chip layouts in hours — a process that normally takes human designers a year or more. The tool helped design three generations of Google’s Tensor Processing Units.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That pedigree explains why, just four months after launching Ricursive, they last month announced a $300 million Series A round at a $4 billion valuation led by Lightspeed, just a couple of months after raising a $35 million seed round led by Sequoia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ricursive is building AI tools that design chips, not the chips themselves. That makes them fundamentally different from nearly every other AI chip startup: they’re not a wannabe Nvidia competitor. In fact, Nvidia is an investor. The GPU giant, along with AMD, Intel, and every other chip maker, are the startup’s target customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to enable any chip, like a custom chip or a more traditional chip, any kind of chip, to be built in an automated and very accelerated way. We’re using AI to do that,” Mirhoseini told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Their paths first crossed at Stanford, where Goldie earned her PhD as Mirhoseini taught computer science classes. Since then, their careers have been in lockstep. “We started at Google Brain on the same day. We left Google Brain on the same day. We joined Anthropic on the same day. We left Anthropic on the same day. We rejoined Google on the same day, and then we left Google again on the same day. Then we started this company together on the same day,” Goldie recounted.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;During their time at Google, the colleagues were so close they even worked out together, both enjoying circuit training. The pun wasn’t lost on Jeff Dean, the famed Google engineer who was their collaborator. He nicknamed their Alpha Chip project “chip circuit training” — a play on their shared workout routine. Internally, the pair also got a nickname: A&amp;amp;A.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Alpha Chip earned them industry notice, but it also attracted controversy. In 2022, one of their colleagues at Google was fired, Wired reported, after he spent years trying to discredit A&amp;amp;A and their chip work, even though that work was used to help produce some of Google’s most important, bet-the-business AI chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Their Alpha Chip project at Google Brain proved the concept that would become Ricursive — using AI to dramatically accelerate chip design.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-designing-chips-is-hard"&gt;Designing chips is hard&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The issue is, computer chips have millions to billions of logic gate components integrated on their silicon wafer. Human designers can spend a year or more placing those components on the chip to ensure performance, good power utilization and any other design needs. Digitally determining the placement of such infinitesimally small components with precision is, as you might expect, hard.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alpha Chip “could generate a very high-quality layout in, like, six hours. And the cool thing about this approach was that it actually learns from experience,” Goldie said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The premise of their AI chip design work is to use “a reward signal” that rates how good the design is. The agent then takes that rating to “update the parameters of its deep neural network to get better,” Goldie said. After completing thousands of designs, the agent got really good. It also got faster as it learned, the founders say.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ricursive’s platform will take the concept further. The AI chip designer they are building will “learn across different chips,” Goldie said. So each chip it designs should help it become a better designer for every next chip. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ricursive’s platform also makes use of LLMs and will handle everything from component placement through design verification. Any company that makes electronics and needs chips is their target customer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If their platform proves itself, as it seems likely to do, Ricursive could play a role in the moonshot goal of achieving artificial general intelligence (AGI). Indeed, their ultimate vision is designing AI chips, meaning the AI will essentially design its own computer brains.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Chips are the fuel for AI,” Goldie said. “I think by building more powerful chips, that’s the best way to advance that frontier.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mirhoseini adds that the lengthy chip-design process is constraining how quickly AI can advance. “We think we can also enable this fast co-evolution of the models and the chips that basically power them,” she said. So AI can grow smarter faster.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If the thought of AI designing its own brains at ever increasing speeds brings visions of Skynet and the Terminator to mind, the founders point out that there’s a more positive, immediate and, they think, more likely benefit: hardware efficiency.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;When AI Labs can design far more efficient chips (and, eventually all the underlying hardware), their growth won’t have to consume so much of the world’s resources.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We could design a computer architecture that’s uniquely suited to that model, and we could achieve almost a 10x improvement in performance per total cost of ownership,” Goldie said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the young startup won’t name its early customers, the founders say that they’ve heard from every big chip making name you can imagine. Unsurprisingly, they have their pick of their first development partners, too.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/16/how-ricursive-intelligence-raised-335m-at-a-4b-valuation-in-4-months/</guid><pubDate>Mon, 16 Feb 2026 17:00:00 +0000</pubDate></item><item><title>New SemiAnalysis InferenceX Data Shows NVIDIA Blackwell Ultra Delivers up to 50x Better Performance and 35x Lower Costs for Agentic AI (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/data-blackwell-ultra-performance-lower-cost-agentic-ai/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The NVIDIA Blackwell platform has been widely adopted by leading inference providers such as Baseten, DeepInfra, Fireworks AI and Together AI to reduce cost per token by up to 10x. Now, the NVIDIA Blackwell Ultra platform is taking this momentum further for agentic AI.&lt;/p&gt;
&lt;p&gt;AI agents and coding assistants are driving explosive growth in software-programming-related AI queries: from 11% to about 50% last year, according to OpenRouter’s State of Inference report. These applications require low latency to maintain real-time responsiveness across multistep workflows and long context when reasoning across entire codebases.&lt;/p&gt;
&lt;p&gt;New SemiAnalysis InferenceX performance data shows that the combination of NVIDIA’s software optimizations and the next-generation NVIDIA Blackwell Ultra platform has delivered breakthrough advances on both fronts. NVIDIA GB300 NVL72 systems now deliver up to 50x higher throughput per megawatt, resulting in 35x lower cost per token compared with the NVIDIA Hopper platform.&lt;/p&gt;
&lt;p&gt;By innovating across chips, system architecture and software, NVIDIA’s extreme codesign accelerates performance across AI workloads — from agentic coding to interactive coding assistants — while driving down costs at scale.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-large wp-image-89982" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/semianalysisv5-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;GB300 NVL72 Delivers up to 50x Better Performance for Low-Latency Workloads&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Recent analysis from Signal65 shows that NVIDIA GB200 NVL72 with extreme hardware and software codesign delivers more than 10x more tokens per watt, resulting in one-tenth the cost per token compared with the NVIDIA Hopper platform. These massive performance gains continue to expand as the underlying stack improves.&lt;/p&gt;
&lt;p&gt;Continuous optimizations from the NVIDIA TensorRT-LLM, NVIDIA Dynamo, Mooncake and SGLang teams continue to significantly boost Blackwell NVL72 throughput for mixture-of-experts (MoE) inference across all latency targets. For instance, NVIDIA TensorRT-LLM library improvements have delivered up to 5x better performance on GB200 for low-latency workloads compared with just four months ago.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Higher-performance GPU kernels&lt;/b&gt; optimized for efficiency and low latency help make the most of Blackwell’s immense compute capabilities and boost throughput.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVIDIA NVLink Symmetric Memory&lt;/b&gt; enables direct GPU-to-GPU memory access for more efficient communication.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Programmatic dependent launch&lt;/b&gt; minimizes idle time by launching the next kernel’s setup phase before the previous one completes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Building on these software advances, GB300 NVL72 — which features the Blackwell Ultra GPU — pushes the throughput-per-megawatt frontier to 50x compared with the Hopper platform.&lt;/p&gt;
&lt;p&gt;This performance gain translates into superior economics, with NVIDIA GB300 lowering costs compared with the Hopper platform across the entire latency spectrum. The most dramatic reduction occurs at low latency, where agentic applications operate: up to 35x lower cost per million tokens compared with the Hopper platform.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89954"&gt;&lt;img alt="alt" class="wp-image-89954 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/gb300-nvl72-delivers-35x-reduction-in-token-cost-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89954"&gt;NVIDIA GB300 NVL72 and the codesigned software stack including NVIDIA Dynamo and TensorRT-LLM deliver 35x lower cost per token compared with NVIDIA Hopper platform.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;For agentic coding and interactive assistants workloads where every millisecond compounds across multistep workflows, this combination of relentless software optimization and next-generation hardware enables AI platforms to scale real-time interactive experiences to significantly more users.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;GB300 NVL72 Delivers Superior Economics for Long-Context Workloads&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;While both GB200 NVL72 and GB300 NVL72 efficiently deliver ultralow latency, the distinct advantages of GB300 NVL72 become most apparent in long-context scenarios. For workloads with 128,000-token inputs and 8,000-token outputs — such as AI coding assistants reasoning across codebases — GB300 NVL72 delivers up to 1.5x lower cost per token compared with GB200 NVL72.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89948"&gt;&lt;img alt="alt" class="wp-image-89948 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/gb300-nvl72-delivers-large-leap-for-long-context-ai-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89948"&gt;NVIDIA GB300 NVL72 is ideal for low-latency, long-context workloads.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Context grows as the agent reads in more of the code. This allows it to better understand the code base but also requires much more compete. Blackwell Ultra has 1.5x higher NVFP4 compute performance and 2x faster attention processing, enabling the agent to efficiently understand entire code bases.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Infrastructure for Agentic AI&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Leading cloud providers and AI innovators have already deployed NVIDIA GB200 NVL72 at scale, and are also deploying GB300 NVL72 in production. Microsoft, CoreWeave and OCI are deploying GB300 NVL72 for low-latency and long-context use cases such as agentic coding and coding assistants. By reducing token costs, GB300 NVL72 enables a new class of applications that can reason across massive codebases in real time.&lt;/p&gt;
&lt;p&gt;“As inference moves to the center of AI production, long-context performance and token efficiency become critical,” said Chen Goldberg, senior vice president of engineering at CoreWeave. “Grace Blackwell NVL72 addresses that challenge directly, and CoreWeave’s AI cloud, including CKS and SUNK, is designed to translate GB300 systems’ gains, building on the success of GB200, into predictable performance and cost efficiency. The result is better token economics and more usable inference for customers running workloads at scale.”&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Vera Rubin NVL72 to Bring Next-Generation Performance&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;With NVIDIA Blackwell systems deployed at scale, continuous software optimizations will keep unlocking additional performance and cost improvements across the installed base.&lt;/p&gt;
&lt;p&gt;Looking ahead, the NVIDIA Rubin platform — which combines six new chips to create one AI supercomputer — is set to deliver another round of massive performance leaps. For MoE inference, it delivers up to 10x higher throughput per megawatt compared with Blackwell, translating into one-tenth the cost per million tokens. And for the next wave of frontier AI models, Rubin can train large MoE models using just one-fourth the number of GPUs compared with Blackwell.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the NVIDIA Rubin platform and the &lt;/i&gt;&lt;i&gt;Vera Rubin NVL72 system&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The NVIDIA Blackwell platform has been widely adopted by leading inference providers such as Baseten, DeepInfra, Fireworks AI and Together AI to reduce cost per token by up to 10x. Now, the NVIDIA Blackwell Ultra platform is taking this momentum further for agentic AI.&lt;/p&gt;
&lt;p&gt;AI agents and coding assistants are driving explosive growth in software-programming-related AI queries: from 11% to about 50% last year, according to OpenRouter’s State of Inference report. These applications require low latency to maintain real-time responsiveness across multistep workflows and long context when reasoning across entire codebases.&lt;/p&gt;
&lt;p&gt;New SemiAnalysis InferenceX performance data shows that the combination of NVIDIA’s software optimizations and the next-generation NVIDIA Blackwell Ultra platform has delivered breakthrough advances on both fronts. NVIDIA GB300 NVL72 systems now deliver up to 50x higher throughput per megawatt, resulting in 35x lower cost per token compared with the NVIDIA Hopper platform.&lt;/p&gt;
&lt;p&gt;By innovating across chips, system architecture and software, NVIDIA’s extreme codesign accelerates performance across AI workloads — from agentic coding to interactive coding assistants — while driving down costs at scale.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-large wp-image-89982" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/semianalysisv5-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;GB300 NVL72 Delivers up to 50x Better Performance for Low-Latency Workloads&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Recent analysis from Signal65 shows that NVIDIA GB200 NVL72 with extreme hardware and software codesign delivers more than 10x more tokens per watt, resulting in one-tenth the cost per token compared with the NVIDIA Hopper platform. These massive performance gains continue to expand as the underlying stack improves.&lt;/p&gt;
&lt;p&gt;Continuous optimizations from the NVIDIA TensorRT-LLM, NVIDIA Dynamo, Mooncake and SGLang teams continue to significantly boost Blackwell NVL72 throughput for mixture-of-experts (MoE) inference across all latency targets. For instance, NVIDIA TensorRT-LLM library improvements have delivered up to 5x better performance on GB200 for low-latency workloads compared with just four months ago.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Higher-performance GPU kernels&lt;/b&gt; optimized for efficiency and low latency help make the most of Blackwell’s immense compute capabilities and boost throughput.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVIDIA NVLink Symmetric Memory&lt;/b&gt; enables direct GPU-to-GPU memory access for more efficient communication.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Programmatic dependent launch&lt;/b&gt; minimizes idle time by launching the next kernel’s setup phase before the previous one completes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Building on these software advances, GB300 NVL72 — which features the Blackwell Ultra GPU — pushes the throughput-per-megawatt frontier to 50x compared with the Hopper platform.&lt;/p&gt;
&lt;p&gt;This performance gain translates into superior economics, with NVIDIA GB300 lowering costs compared with the Hopper platform across the entire latency spectrum. The most dramatic reduction occurs at low latency, where agentic applications operate: up to 35x lower cost per million tokens compared with the Hopper platform.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89954"&gt;&lt;img alt="alt" class="wp-image-89954 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/gb300-nvl72-delivers-35x-reduction-in-token-cost-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89954"&gt;NVIDIA GB300 NVL72 and the codesigned software stack including NVIDIA Dynamo and TensorRT-LLM deliver 35x lower cost per token compared with NVIDIA Hopper platform.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;For agentic coding and interactive assistants workloads where every millisecond compounds across multistep workflows, this combination of relentless software optimization and next-generation hardware enables AI platforms to scale real-time interactive experiences to significantly more users.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;GB300 NVL72 Delivers Superior Economics for Long-Context Workloads&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;While both GB200 NVL72 and GB300 NVL72 efficiently deliver ultralow latency, the distinct advantages of GB300 NVL72 become most apparent in long-context scenarios. For workloads with 128,000-token inputs and 8,000-token outputs — such as AI coding assistants reasoning across codebases — GB300 NVL72 delivers up to 1.5x lower cost per token compared with GB200 NVL72.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89948"&gt;&lt;img alt="alt" class="wp-image-89948 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/gb300-nvl72-delivers-large-leap-for-long-context-ai-1680x945.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89948"&gt;NVIDIA GB300 NVL72 is ideal for low-latency, long-context workloads.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Context grows as the agent reads in more of the code. This allows it to better understand the code base but also requires much more compete. Blackwell Ultra has 1.5x higher NVFP4 compute performance and 2x faster attention processing, enabling the agent to efficiently understand entire code bases.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Infrastructure for Agentic AI&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Leading cloud providers and AI innovators have already deployed NVIDIA GB200 NVL72 at scale, and are also deploying GB300 NVL72 in production. Microsoft, CoreWeave and OCI are deploying GB300 NVL72 for low-latency and long-context use cases such as agentic coding and coding assistants. By reducing token costs, GB300 NVL72 enables a new class of applications that can reason across massive codebases in real time.&lt;/p&gt;
&lt;p&gt;“As inference moves to the center of AI production, long-context performance and token efficiency become critical,” said Chen Goldberg, senior vice president of engineering at CoreWeave. “Grace Blackwell NVL72 addresses that challenge directly, and CoreWeave’s AI cloud, including CKS and SUNK, is designed to translate GB300 systems’ gains, building on the success of GB200, into predictable performance and cost efficiency. The result is better token economics and more usable inference for customers running workloads at scale.”&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Vera Rubin NVL72 to Bring Next-Generation Performance&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;With NVIDIA Blackwell systems deployed at scale, continuous software optimizations will keep unlocking additional performance and cost improvements across the installed base.&lt;/p&gt;
&lt;p&gt;Looking ahead, the NVIDIA Rubin platform — which combines six new chips to create one AI supercomputer — is set to deliver another round of massive performance leaps. For MoE inference, it delivers up to 10x higher throughput per megawatt compared with Blackwell, translating into one-tenth the cost per million tokens. And for the next wave of frontier AI models, Rubin can train large MoE models using just one-fourth the number of GPUs compared with Blackwell.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the NVIDIA Rubin platform and the &lt;/i&gt;&lt;i&gt;Vera Rubin NVL72 system&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/data-blackwell-ultra-performance-lower-cost-agentic-ai/</guid><pubDate>Mon, 16 Feb 2026 17:00:40 +0000</pubDate></item><item><title>ByteDance backpedals after Seedance 2.0 turned Hollywood icons into AI “clip art” (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/02/bytedance-backpedals-after-seedance-2-0-turned-hollywood-icons-into-ai-clip-art/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Hollywood backlash puts spotlight on ByteDance’s sketchy launch of Seedance 2.0.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2260459499-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2260459499-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Cheng Xin / Contributor | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;ByteDance says that it’s rushing to add safeguards to block Seedance 2.0 from generating iconic characters and deepfaking celebrities, after substantial Hollywood backlash after launching the latest version of its AI video tool.&lt;/p&gt;
&lt;p&gt;The changes come after Disney and Paramount Skydance sent cease-and-desist letters to ByteDance urging the Chinese company to promptly end the allegedly vast and blatant infringement.&lt;/p&gt;
&lt;p&gt;Studios claimed the infringement was widescale and immediate, with Seedance 2.0 users across social media sharing AI videos featuring copyrighted characters like Spider-Man, Darth Vader, and SpongeBob Square Pants. In its letter, Disney fumed that Seedance was “hijacking” its characters, accusing ByteDance of treating Disney characters like they were “free public domain clip art,” Axios reported.&lt;/p&gt;
&lt;p&gt;“ByteDance’s virtual smash-and-grab of Disney’s IP is willful, pervasive, and totally unacceptable,” Disney’s letter said.&lt;/p&gt;
&lt;p&gt;Defending intellectual property from franchises like &lt;em&gt;Star Trek&lt;/em&gt; and &lt;em&gt;The Godfather&lt;/em&gt;, Paramount Skydance pointed out that Seedance’s outputs are “often indistinguishable, both visually and audibly” from the original characters, Variety reported. Similarly frustrated, Japan’s AI minister Kimi Onoda, sought to protect popular anime and manga characters, officially launching a probe last week into ByteDance over the copyright violations, the South China Morning Post reported.&lt;/p&gt;
&lt;p&gt;“We cannot overlook a situation in which content is being used without the copyright holder’s permission,” Onoda said at a press conference Friday.&lt;/p&gt;
&lt;p&gt;Facing legal threats and Japan’s investigation, ByteDance issued a statement Monday, CNBC reported. In it, the company claimed that it “respects intellectual property rights” and has “heard the concerns regarding Seedance 2.0.”&lt;/p&gt;
&lt;p&gt;“We are taking steps to strengthen current safeguards as we work to prevent the unauthorized use of intellectual property and likeness by users,” ByteDance said.&lt;/p&gt;
&lt;p&gt;However, Disney seems unlikely to accept that ByteDance inadvertently released its tool without implementing such safeguards in advance. In its letter, Disney alleged that “Seedance has infringed on Disney’s copyrighted materials to benefit its commercial service without permission.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;After all, what better way to illustrate Seedance 2.0’s latest features than by generating some of the best-known IP in the world? At least one tech consultant has suggested that ByteDance planned to benefit from inciting Hollywood outrage. The founder of San Francisco-based consultancy Tech Buzz China, Rui Ma, told SCMP that “the controversy surrounding Seedance is likely part of ByteDance’s initial distribution strategy to showcase its underlying technical capabilities.”&lt;/p&gt;
&lt;h2&gt;Seedance 2.0 is an “attack” on creators&lt;/h2&gt;
&lt;p&gt;Studios aren’t the only ones sounding alarms.&lt;/p&gt;
&lt;p&gt;Several industry groups expressed concerns, including the Motion Picture Association, which accused ByteDance of engaging in massive copyright infringement within “a single day,” CNBC reported.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Sean Astin, an actor and president of the actors union, SAG-AFTRA, was directly impacted by the scandal. A video that has since been removed from X showed Astin in the role of Samwise Gamgee from &lt;em&gt;The Lord of the Rings&lt;/em&gt;, delivering a line he never said, Variety reported. Condemning Seedance’s infringement, SAG-AFTRA issued a statement emphasizing that ByteDance did not act responsibly in releasing the model without safeguards:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;“SAG-AFTRA stands with the studios in condemning the blatant infringement enabled by ByteDance’s new AI video model Seedance 2.0. The infringement includes the unauthorized use of our members’ voices and likenesses. This is unacceptable and undercuts the ability of human talent to earn a livelihood. Seedance 2.0 disregards law, ethics, industry standards and basic principles of consent. Responsible AI development demands responsibility, and that is nonexistent here.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Echoing that, a group representing Hollywood creators, the Human Artistry Campaign, declared that “the launch of Seedance 2.0” was “an attack on every creator around the world.”&lt;/p&gt;
&lt;p&gt;“Stealing human creators’ work in an attempt to replace them with AI generated slop is destructive to our culture: stealing isn’t innovation,” the group said. “These unauthorized deepfakes and voice clones of actors violate the most basic aspects of personal autonomy and should be deeply concerning to everyone. Authorities should use every legal tool at their disposal to stop this wholesale theft.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Ars could not immediately reach any of these groups to comment on whether ByteDance’s post-launch efforts to add safeguards addressed industry concerns.&lt;/p&gt;
&lt;p&gt;MPA chairman and CEO Charles Rivkin has previously accused ByteDance of disregarding “well-established copyright law that protects the rights of creators and underpins millions of American jobs.”&lt;/p&gt;
&lt;p&gt;While Disney and other studios are clearly ready to take down any tools that could hurt their revenue or reputation without an agreement in place, they aren’t opposed to all AI uses of their characters. In December, Disney struck a deal with OpenAI, giving Sora access to 200 characters for three years, while investing $1 billion in the technology.&lt;/p&gt;
&lt;p&gt;At that time, Disney CEO Robert A. Iger, said that “the rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI, we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works.”&lt;/p&gt;
&lt;h2&gt;Creators disagree Seedance 2.0 is a game changer&lt;/h2&gt;
&lt;p&gt;In a blog announcing Seedance 2.0, ByteDance boasted that the new model “delivers a substantial leap in generation quality,” particularly in close-up shots and action sequences.&lt;/p&gt;
&lt;p&gt;The company acknowledged that further refinements were needed and the model is “still far from perfect” but hyped that “its generated videos possess a distinct cinematic aesthetic; the textures of objects, lighting, and composition, as well as costume, makeup, and prop designs, all show high degrees of finish.”&lt;/p&gt;
&lt;p&gt;ByteDance likely hoped that the earliest outputs from Seedance 2.0 would produce headlines wowed by the model’s capabilities, and it got what it wanted when a single Hollywood stakeholder’s social media comment went viral.&lt;/p&gt;
&lt;p&gt;Shortly after Seedance 2.0’s rollout, &lt;em&gt;Deadpool&lt;/em&gt; co-writer, Rhett Reese, declared on X that “it’s likely over for us,” The Guardian reported. The screenwriter was impressed by an AI video created by Irish director Ruairi Robinson, which realistically depicted Tom Cruise fighting Brad Pitt. “[I]n next to no time, one person is going to be able to sit at a computer and create a movie indistinguishable from what Hollywood now releases,” Reese opined. “True, if that person is no good, it will suck. But if that person possesses Christopher Nolan’s talent and taste (and someone like that will rapidly come along), it will be tremendous.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;However, some AI critics rejected the notion that Seedance 2.0 is capable of replacing artists in the way that Reese warned. On Bluesky and X, they pushed back on ByteDance claims that this model doomed Hollywood, with some accusing outlets of too quickly ascribing Reese’s reaction to the whole industry.&lt;/p&gt;
&lt;p&gt;Among them was longtime AI critic, Reid Southen, a film concept artist who works on major motion pictures and TV. Responding directly to Reese’s X thread, Southen contradicted the notion that a great filmmaker could be born from fiddling with AI prompts alone.&lt;/p&gt;
&lt;p&gt;“Nolan is capable of doing great work because he’s put in the work,” Southen said. “AI is an automation tool, it’s literally removing key, fundamental work from the process, how does one become good at anything if they insist on using nothing but shortcuts?”&lt;/p&gt;
&lt;p&gt;Perhaps the strongest evidence in Southen’s favor is Darren Aronofsky’s recent AI-generated historical docudrama. Speaking anonymously to Ars following backlash declaring that “AI slop is ruining American history,” one source close to production on that project confirmed that it took “weeks” to produce minutes of usable video using a variety of AI tools.&lt;/p&gt;
&lt;p&gt;That source noted that the creative team went into the project expecting they had a lot to learn but also expecting that tools would continue to evolve, as could audience reactions to AI-assisted movies.&lt;/p&gt;
&lt;p&gt;“It’s a huge experiment, really,” the source told Ars.&lt;/p&gt;
&lt;p&gt;Notably, for both creators and rights-holders concerned about copyright infringement and career threats, questions remain on how Seedance 2.0 was trained. ByteDance has yet to release a technical report for Seedance 2.0 and “has never disclosed the data sets it uses to train its powerful video-generation Seedance models and image-generation Seedream models,” SCMP reported.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Hollywood backlash puts spotlight on ByteDance’s sketchy launch of Seedance 2.0.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2260459499-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2260459499-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Cheng Xin / Contributor | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;ByteDance says that it’s rushing to add safeguards to block Seedance 2.0 from generating iconic characters and deepfaking celebrities, after substantial Hollywood backlash after launching the latest version of its AI video tool.&lt;/p&gt;
&lt;p&gt;The changes come after Disney and Paramount Skydance sent cease-and-desist letters to ByteDance urging the Chinese company to promptly end the allegedly vast and blatant infringement.&lt;/p&gt;
&lt;p&gt;Studios claimed the infringement was widescale and immediate, with Seedance 2.0 users across social media sharing AI videos featuring copyrighted characters like Spider-Man, Darth Vader, and SpongeBob Square Pants. In its letter, Disney fumed that Seedance was “hijacking” its characters, accusing ByteDance of treating Disney characters like they were “free public domain clip art,” Axios reported.&lt;/p&gt;
&lt;p&gt;“ByteDance’s virtual smash-and-grab of Disney’s IP is willful, pervasive, and totally unacceptable,” Disney’s letter said.&lt;/p&gt;
&lt;p&gt;Defending intellectual property from franchises like &lt;em&gt;Star Trek&lt;/em&gt; and &lt;em&gt;The Godfather&lt;/em&gt;, Paramount Skydance pointed out that Seedance’s outputs are “often indistinguishable, both visually and audibly” from the original characters, Variety reported. Similarly frustrated, Japan’s AI minister Kimi Onoda, sought to protect popular anime and manga characters, officially launching a probe last week into ByteDance over the copyright violations, the South China Morning Post reported.&lt;/p&gt;
&lt;p&gt;“We cannot overlook a situation in which content is being used without the copyright holder’s permission,” Onoda said at a press conference Friday.&lt;/p&gt;
&lt;p&gt;Facing legal threats and Japan’s investigation, ByteDance issued a statement Monday, CNBC reported. In it, the company claimed that it “respects intellectual property rights” and has “heard the concerns regarding Seedance 2.0.”&lt;/p&gt;
&lt;p&gt;“We are taking steps to strengthen current safeguards as we work to prevent the unauthorized use of intellectual property and likeness by users,” ByteDance said.&lt;/p&gt;
&lt;p&gt;However, Disney seems unlikely to accept that ByteDance inadvertently released its tool without implementing such safeguards in advance. In its letter, Disney alleged that “Seedance has infringed on Disney’s copyrighted materials to benefit its commercial service without permission.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;After all, what better way to illustrate Seedance 2.0’s latest features than by generating some of the best-known IP in the world? At least one tech consultant has suggested that ByteDance planned to benefit from inciting Hollywood outrage. The founder of San Francisco-based consultancy Tech Buzz China, Rui Ma, told SCMP that “the controversy surrounding Seedance is likely part of ByteDance’s initial distribution strategy to showcase its underlying technical capabilities.”&lt;/p&gt;
&lt;h2&gt;Seedance 2.0 is an “attack” on creators&lt;/h2&gt;
&lt;p&gt;Studios aren’t the only ones sounding alarms.&lt;/p&gt;
&lt;p&gt;Several industry groups expressed concerns, including the Motion Picture Association, which accused ByteDance of engaging in massive copyright infringement within “a single day,” CNBC reported.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Sean Astin, an actor and president of the actors union, SAG-AFTRA, was directly impacted by the scandal. A video that has since been removed from X showed Astin in the role of Samwise Gamgee from &lt;em&gt;The Lord of the Rings&lt;/em&gt;, delivering a line he never said, Variety reported. Condemning Seedance’s infringement, SAG-AFTRA issued a statement emphasizing that ByteDance did not act responsibly in releasing the model without safeguards:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;“SAG-AFTRA stands with the studios in condemning the blatant infringement enabled by ByteDance’s new AI video model Seedance 2.0. The infringement includes the unauthorized use of our members’ voices and likenesses. This is unacceptable and undercuts the ability of human talent to earn a livelihood. Seedance 2.0 disregards law, ethics, industry standards and basic principles of consent. Responsible AI development demands responsibility, and that is nonexistent here.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Echoing that, a group representing Hollywood creators, the Human Artistry Campaign, declared that “the launch of Seedance 2.0” was “an attack on every creator around the world.”&lt;/p&gt;
&lt;p&gt;“Stealing human creators’ work in an attempt to replace them with AI generated slop is destructive to our culture: stealing isn’t innovation,” the group said. “These unauthorized deepfakes and voice clones of actors violate the most basic aspects of personal autonomy and should be deeply concerning to everyone. Authorities should use every legal tool at their disposal to stop this wholesale theft.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Ars could not immediately reach any of these groups to comment on whether ByteDance’s post-launch efforts to add safeguards addressed industry concerns.&lt;/p&gt;
&lt;p&gt;MPA chairman and CEO Charles Rivkin has previously accused ByteDance of disregarding “well-established copyright law that protects the rights of creators and underpins millions of American jobs.”&lt;/p&gt;
&lt;p&gt;While Disney and other studios are clearly ready to take down any tools that could hurt their revenue or reputation without an agreement in place, they aren’t opposed to all AI uses of their characters. In December, Disney struck a deal with OpenAI, giving Sora access to 200 characters for three years, while investing $1 billion in the technology.&lt;/p&gt;
&lt;p&gt;At that time, Disney CEO Robert A. Iger, said that “the rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI, we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works.”&lt;/p&gt;
&lt;h2&gt;Creators disagree Seedance 2.0 is a game changer&lt;/h2&gt;
&lt;p&gt;In a blog announcing Seedance 2.0, ByteDance boasted that the new model “delivers a substantial leap in generation quality,” particularly in close-up shots and action sequences.&lt;/p&gt;
&lt;p&gt;The company acknowledged that further refinements were needed and the model is “still far from perfect” but hyped that “its generated videos possess a distinct cinematic aesthetic; the textures of objects, lighting, and composition, as well as costume, makeup, and prop designs, all show high degrees of finish.”&lt;/p&gt;
&lt;p&gt;ByteDance likely hoped that the earliest outputs from Seedance 2.0 would produce headlines wowed by the model’s capabilities, and it got what it wanted when a single Hollywood stakeholder’s social media comment went viral.&lt;/p&gt;
&lt;p&gt;Shortly after Seedance 2.0’s rollout, &lt;em&gt;Deadpool&lt;/em&gt; co-writer, Rhett Reese, declared on X that “it’s likely over for us,” The Guardian reported. The screenwriter was impressed by an AI video created by Irish director Ruairi Robinson, which realistically depicted Tom Cruise fighting Brad Pitt. “[I]n next to no time, one person is going to be able to sit at a computer and create a movie indistinguishable from what Hollywood now releases,” Reese opined. “True, if that person is no good, it will suck. But if that person possesses Christopher Nolan’s talent and taste (and someone like that will rapidly come along), it will be tremendous.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;However, some AI critics rejected the notion that Seedance 2.0 is capable of replacing artists in the way that Reese warned. On Bluesky and X, they pushed back on ByteDance claims that this model doomed Hollywood, with some accusing outlets of too quickly ascribing Reese’s reaction to the whole industry.&lt;/p&gt;
&lt;p&gt;Among them was longtime AI critic, Reid Southen, a film concept artist who works on major motion pictures and TV. Responding directly to Reese’s X thread, Southen contradicted the notion that a great filmmaker could be born from fiddling with AI prompts alone.&lt;/p&gt;
&lt;p&gt;“Nolan is capable of doing great work because he’s put in the work,” Southen said. “AI is an automation tool, it’s literally removing key, fundamental work from the process, how does one become good at anything if they insist on using nothing but shortcuts?”&lt;/p&gt;
&lt;p&gt;Perhaps the strongest evidence in Southen’s favor is Darren Aronofsky’s recent AI-generated historical docudrama. Speaking anonymously to Ars following backlash declaring that “AI slop is ruining American history,” one source close to production on that project confirmed that it took “weeks” to produce minutes of usable video using a variety of AI tools.&lt;/p&gt;
&lt;p&gt;That source noted that the creative team went into the project expecting they had a lot to learn but also expecting that tools would continue to evolve, as could audience reactions to AI-assisted movies.&lt;/p&gt;
&lt;p&gt;“It’s a huge experiment, really,” the source told Ars.&lt;/p&gt;
&lt;p&gt;Notably, for both creators and rights-holders concerned about copyright infringement and career threats, questions remain on how Seedance 2.0 was trained. ByteDance has yet to release a technical report for Seedance 2.0 and “has never disclosed the data sets it uses to train its powerful video-generation Seedance models and image-generation Seedream models,” SCMP reported.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/02/bytedance-backpedals-after-seedance-2-0-turned-hollywood-icons-into-ai-clip-art/</guid><pubDate>Mon, 16 Feb 2026 17:42:37 +0000</pubDate></item><item><title>[NEW] Have money, will travel: a16z’s hunt for the next European unicorn (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/16/have-money-will-travel-a16zs-hunt-for-the-next-european-unicorn/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-157503102.jpg?resize=1200,776" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Gabriel Vasquez, a partner at Andreessen Horowitz, recently revealed he took nine flights from NYC to Stockholm in one year. While his visits included stops at companies like Lovable — where he posted from its office — the trips were also about finding future Swedish unicorns before they cross the Atlantic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This all came to light when news emerged that a16z had led a $2.3 million pre-seed round into Dentio, a Swedish startup that uses AI to help dentists’ practices with admin work. While this is a small check for a firm that just announced new funds totaling $15 billion, it confirms that U.S. VCs are actively seeking deal flow outside of the U.S., even without local offices.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Stockholm is a natural stop for a16z, which previously achieved significant returns from backing Skype, cofounded by Swedish entrepreneur Niklas Zennström. Since then, a significant number of fast-growing startups have been created in the Swedish capital, and the VC heavyweight tracked down where many of them were coming from.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We spend a lot of time developing a deep understanding of specific markets and knowing where innovation is emerging. In Sweden, that has meant closely tracking ecosystems like [SSE Business Lab] — the startup incubator of the Stockholm School of Economics — and the companies coming out of it,” Vasquez told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like fintech giant Klarna, legal AI startup Legora, and e-scooter company Voi, Dentio is an alum of SSE Business Lab — a startup incubator that has produced several successful Swedish companies. The three former high school classmates Elias Afrasiabi, Anton Li and Lukas Sjögren joined the incubator after reconnecting as students at both the SSE (Stockholm School of Economics) and KTH (Royal Institute of Technology), then joined the incubator with additional backing from KTH’s Innovation Launch program. They tackled a problem close to home: Li’s mom, a dentist, had told them how admin work detracted from clinical care.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The trio intuited that they could leverage LLMs to help people like her — an idea that they also validated with her and her colleagues. This led them to Dentio’s initial product, a recording tool that uses AI to generate clinical notes. But it’s only a matter of time before AI scribes become a commodity product, and Dentio needs to prove its value to dentists so they aren’t tempted to switch providers when that happens, Afrasiabi said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Potential competitors include fellow Swedish startup Tandem Health, which raised a $50 million Series A round last year to support clinicians with AI across multiple medical specialties. Dentio, by contrast, focuses exclusively on dentists, but it believes it can still reach the scale VCs expect through international expansion&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Now we’re a team of seven people, and we think that it’s possible to build a unified way of handling administration all over Europe, and maybe even all over the world,” Afrasiabi said. While Europe’s healthcare systems are fragmented, they share similarities, and Dentio’s assumption is that what works in Sweden could work elsewhere in the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dentio prominently features its “Made in Sweden” branding and emphasizes that “all relevant data is processed in Sweden and Finland in compliance with Swedish and EU law.” It signals data protection to privacy-conscious European customers. But it also signals potential to VCs — a callback to Sweden’s history of producing breakout companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We went to zero meetups. I reached out to zero investors,” Afrasiabi said. While the team was heads down building, the word spread out. “I think it was mostly through referrals and people talking to each other that the news got all the way over to the U.S.,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This wasn’t happenstance: a16z has eyes around the world in order to spot these companies as early as local funds might, Vasquez said. “In Sweden for example, we partnered with top founders abroad like Fredrik Hjelm, founder of Voi, and Johannes Schildt, founder of Kry, by turning them into scouts and mapping the best local talent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Vasquez, who focuses on AI application investments for a16z, this isn’t just about Sweden, but about “a pattern of great global companies being born abroad and scaling quickly,” from Black Forest Labs in Germany to Manus, the Singapore-based AI startup recently acquired by Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Born and raised in El Salvador, he has also been spending time in São Paulo. “I’m really excited about what’s brewing in Brazil and across Latin America in AI,” he wrote on LinkedIn at the time. “I believe AI is the great equalizer,” he added. “Most people now have access to PhD-level intelligence on a phone, and ultimately, Silicon Valley is a state of mind.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Corrections: This story originally stated that a16z is an investor in Lovable owing to an editing error. The name of SSE’s incubator has also been corrected.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-157503102.jpg?resize=1200,776" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Gabriel Vasquez, a partner at Andreessen Horowitz, recently revealed he took nine flights from NYC to Stockholm in one year. While his visits included stops at companies like Lovable — where he posted from its office — the trips were also about finding future Swedish unicorns before they cross the Atlantic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This all came to light when news emerged that a16z had led a $2.3 million pre-seed round into Dentio, a Swedish startup that uses AI to help dentists’ practices with admin work. While this is a small check for a firm that just announced new funds totaling $15 billion, it confirms that U.S. VCs are actively seeking deal flow outside of the U.S., even without local offices.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Stockholm is a natural stop for a16z, which previously achieved significant returns from backing Skype, cofounded by Swedish entrepreneur Niklas Zennström. Since then, a significant number of fast-growing startups have been created in the Swedish capital, and the VC heavyweight tracked down where many of them were coming from.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We spend a lot of time developing a deep understanding of specific markets and knowing where innovation is emerging. In Sweden, that has meant closely tracking ecosystems like [SSE Business Lab] — the startup incubator of the Stockholm School of Economics — and the companies coming out of it,” Vasquez told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like fintech giant Klarna, legal AI startup Legora, and e-scooter company Voi, Dentio is an alum of SSE Business Lab — a startup incubator that has produced several successful Swedish companies. The three former high school classmates Elias Afrasiabi, Anton Li and Lukas Sjögren joined the incubator after reconnecting as students at both the SSE (Stockholm School of Economics) and KTH (Royal Institute of Technology), then joined the incubator with additional backing from KTH’s Innovation Launch program. They tackled a problem close to home: Li’s mom, a dentist, had told them how admin work detracted from clinical care.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The trio intuited that they could leverage LLMs to help people like her — an idea that they also validated with her and her colleagues. This led them to Dentio’s initial product, a recording tool that uses AI to generate clinical notes. But it’s only a matter of time before AI scribes become a commodity product, and Dentio needs to prove its value to dentists so they aren’t tempted to switch providers when that happens, Afrasiabi said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Potential competitors include fellow Swedish startup Tandem Health, which raised a $50 million Series A round last year to support clinicians with AI across multiple medical specialties. Dentio, by contrast, focuses exclusively on dentists, but it believes it can still reach the scale VCs expect through international expansion&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Now we’re a team of seven people, and we think that it’s possible to build a unified way of handling administration all over Europe, and maybe even all over the world,” Afrasiabi said. While Europe’s healthcare systems are fragmented, they share similarities, and Dentio’s assumption is that what works in Sweden could work elsewhere in the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dentio prominently features its “Made in Sweden” branding and emphasizes that “all relevant data is processed in Sweden and Finland in compliance with Swedish and EU law.” It signals data protection to privacy-conscious European customers. But it also signals potential to VCs — a callback to Sweden’s history of producing breakout companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We went to zero meetups. I reached out to zero investors,” Afrasiabi said. While the team was heads down building, the word spread out. “I think it was mostly through referrals and people talking to each other that the news got all the way over to the U.S.,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This wasn’t happenstance: a16z has eyes around the world in order to spot these companies as early as local funds might, Vasquez said. “In Sweden for example, we partnered with top founders abroad like Fredrik Hjelm, founder of Voi, and Johannes Schildt, founder of Kry, by turning them into scouts and mapping the best local talent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Vasquez, who focuses on AI application investments for a16z, this isn’t just about Sweden, but about “a pattern of great global companies being born abroad and scaling quickly,” from Black Forest Labs in Germany to Manus, the Singapore-based AI startup recently acquired by Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Born and raised in El Salvador, he has also been spending time in São Paulo. “I’m really excited about what’s brewing in Brazil and across Latin America in AI,” he wrote on LinkedIn at the time. “I believe AI is the great equalizer,” he added. “Most people now have access to PhD-level intelligence on a phone, and ultimately, Silicon Valley is a state of mind.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Corrections: This story originally stated that a16z is an investor in Lovable owing to an editing error. The name of SSE’s incubator has also been corrected.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/16/have-money-will-travel-a16zs-hunt-for-the-next-european-unicorn/</guid><pubDate>Mon, 16 Feb 2026 18:53:56 +0000</pubDate></item></channel></rss>