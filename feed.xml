<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 23 Jul 2025 06:35:12 +0000</lastBuildDate><item><title>iOS 26 beta 4 arrives, with Liquid Glass tweaks and AI news summaries (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/ios-26-beta-4-arrives-with-liquid-glass-tweaks-and-ai-news-summaries/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Apple-WWDC25-iOS-26-hero-250609.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Tuesday released the fourth developer beta of its next big software update, iOS 26, which brings with it slight changes to its Liquid Glass redesign and the re-introduction of AI-powered notification summaries for news, among other updates.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout arrived just ahead of the iOS 26 public beta, which is expected to launch later this week.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The developer betas are meant to give mobile app makers time to test their apps with Apple’s new software so they’re ready for the public launch of the new operating system in the next few months. Because of consumer demand for early releases, Apple for years has been offering public betas following its Worldwide Developers Conference in June. This allows iPhone owners to also get their hands on the updated software before its wider, global launch, but with fewer stability issues and bugs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest developer beta (iOS 26 beta 4) largely reflects what users can expect in the coming public beta. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;iOS 26 beta 4 introduces a new “Welcome” splash screen when you first update the software, plus introductory screens for various features, like Siri and its AI-powered notification summaries and prioritization options, as well as for iOS 26’s revamped Camera app.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Apple was forced to put its AI notification summaries on pause after complaints by the BBC, which said the feature had misrepresented one of its headlines. The AI-powered news summary claimed Luigi Mangione, the person charged with the murder of UnitedHealthcare CEO Brian Thompson, had died by suicide, which wasn’t true. As a result, Apple said a software update would be released to clarify when the text shows an AI summarization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The setup screen for this AI summarization feature, still in beta, includes a warning message under the “News &amp;amp; Entertainment” section. Here, Apple notes that “Summarization may change the meaning of the original headlines” and advises users to “Verify information.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other early adopters of the new beta also found that Apple is continuing to refine its user interface redesign known as Liquid Glass. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While beta 3 pulled back on some of the more transparent elements in some apps, beta 4 reverses things again. Testers have pointed to updates in apps like the App Store, Photos, Apple Music, Weather, and elsewhere, and noted that the Notification Center also adds a dynamic tint as you scroll.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;iOS 26 Beta 4 brings a more visually comfortable change when viewing notifications.&lt;/p&gt;&lt;p&gt;Now, the wallpaper gets darker right when we enter the notification centre. pic.twitter.com/UaP1FW7RvQ&lt;/p&gt;— Alvin (@sondesix) July 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The updated software additionally includes a new dynamic wallpaper, which changes colors, and new CarPlay wallpapers.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Release notes for beta 4 had not been published on Apple’s Developer website at the time of publication, so there is likely more to be spotted, especially in terms of minor bug fixes and performance improvements.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alongside today’s release, Apple also launched new versions of its other betas, including iPadOS 26 beta 4, macOS 26 beta 4, watchOS 26 beta 4, tvOS 26 beta 4, visionOS 26 beta 4, and Xcode 26 beta 4.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Apple-WWDC25-iOS-26-hero-250609.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Tuesday released the fourth developer beta of its next big software update, iOS 26, which brings with it slight changes to its Liquid Glass redesign and the re-introduction of AI-powered notification summaries for news, among other updates.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout arrived just ahead of the iOS 26 public beta, which is expected to launch later this week.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The developer betas are meant to give mobile app makers time to test their apps with Apple’s new software so they’re ready for the public launch of the new operating system in the next few months. Because of consumer demand for early releases, Apple for years has been offering public betas following its Worldwide Developers Conference in June. This allows iPhone owners to also get their hands on the updated software before its wider, global launch, but with fewer stability issues and bugs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest developer beta (iOS 26 beta 4) largely reflects what users can expect in the coming public beta. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;iOS 26 beta 4 introduces a new “Welcome” splash screen when you first update the software, plus introductory screens for various features, like Siri and its AI-powered notification summaries and prioritization options, as well as for iOS 26’s revamped Camera app.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Apple was forced to put its AI notification summaries on pause after complaints by the BBC, which said the feature had misrepresented one of its headlines. The AI-powered news summary claimed Luigi Mangione, the person charged with the murder of UnitedHealthcare CEO Brian Thompson, had died by suicide, which wasn’t true. As a result, Apple said a software update would be released to clarify when the text shows an AI summarization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The setup screen for this AI summarization feature, still in beta, includes a warning message under the “News &amp;amp; Entertainment” section. Here, Apple notes that “Summarization may change the meaning of the original headlines” and advises users to “Verify information.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other early adopters of the new beta also found that Apple is continuing to refine its user interface redesign known as Liquid Glass. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While beta 3 pulled back on some of the more transparent elements in some apps, beta 4 reverses things again. Testers have pointed to updates in apps like the App Store, Photos, Apple Music, Weather, and elsewhere, and noted that the Notification Center also adds a dynamic tint as you scroll.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;iOS 26 Beta 4 brings a more visually comfortable change when viewing notifications.&lt;/p&gt;&lt;p&gt;Now, the wallpaper gets darker right when we enter the notification centre. pic.twitter.com/UaP1FW7RvQ&lt;/p&gt;— Alvin (@sondesix) July 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The updated software additionally includes a new dynamic wallpaper, which changes colors, and new CarPlay wallpapers.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Release notes for beta 4 had not been published on Apple’s Developer website at the time of publication, so there is likely more to be spotted, especially in terms of minor bug fixes and performance improvements.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alongside today’s release, Apple also launched new versions of its other betas, including iPadOS 26 beta 4, macOS 26 beta 4, watchOS 26 beta 4, tvOS 26 beta 4, visionOS 26 beta 4, and Xcode 26 beta 4.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/ios-26-beta-4-arrives-with-liquid-glass-tweaks-and-ai-news-summaries/</guid><pubDate>Tue, 22 Jul 2025 18:47:14 +0000</pubDate></item><item><title>Surprising no one, new research says AI Overviews cause massive drop in search clicks (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/research-shows-google-ai-overviews-reduce-website-clicks-by-almost-half/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pew Research Center analysis shows how hard AI is hitting web traffic.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Overview" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Search_SocialShare_7gpZ6Zv.width-1300-640x360.jpg" width="640" /&gt;
                  &lt;img alt="AI Overview" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Search_SocialShare_7gpZ6Zv.width-1300-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google's search results have undergone a seismic shift over the past year as AI fever has continued to escalate among the tech giants. Nowhere is this change more apparent than right at the top of Google's storied results page, which is now home to AI Overviews. Google contends these Gemini-based answers don't take traffic away from websites, but a new analysis from the Pew Research Center says otherwise. Its analysis shows that searches with AI summaries reduce clicks, and their prevalence is increasing.&lt;/p&gt;
&lt;p&gt;Google began testing AI Overviews as the "search generative experience" in May 2023, and just a year later, they were an official part of the search engine results page (SERP). Many sites (including this one) have noticed changes to their traffic in the wake of this move, but Google has brushed off concerns about how this could affect the sites from which it collects all that data.&lt;/p&gt;
&lt;p&gt;SEO experts have disagreed with Google's stance on how AI affects web traffic, and the newly released Pew study backs them up. The Pew Research Center analyzed data from 900 users of the Ipsos KnowledgePanel collected in March 2025. The analysis shows that among the test group, users were much less likely to click on search results when the page included an AI Overview.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2107352 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Pew AI Overviews stats" class="fullwidth full" height="1592" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Pew-AI.png" width="1792" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Pre Research Center

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Pew reports that searches without an AI answer resulted in a click rate of 15 percent. On SERPs with AI Overviews, the rate of clicks to other sites drops by almost half, to 8 percent. Google has also, on several occasions, claimed that people click on the links cited in AI Overviews, but Pew found that just 1 percent of AI Overviews produced a click on a source. These sources are most frequently Wikipedia, YouTube, and Reddit, which collectively account for 15 percent of all AI sources.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And perhaps more troubling, Google users are more likely to end their browsing session after seeing an AI Overview. That suggests that many people are seeing information generated by a robot, and their investigation stops there. Unfortunately for these people, all forms of generative AI are prone to "hallucinations" that cause them to provide incorrect information. So more people could be walking away from a search with the wrong information.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2098864 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="AI overview on phone" class="fullwidth full" height="1824" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/AI-Overviews.jpg" width="2000" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Overviews are integrated with Google's results, and they are appearing on more searches all the time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;This problem is unlikely to improve over time. Since launching AI Overviews, Google has repeatedly expanded the number of searches that get robot summaries. The Pew Research Center says that about 1 in 5 searches now have AI Overviews. Generally, the more words in a search, the more likely it is to trigger an AI Overview, and that's especially true for searches phrased as questions. The research shows that 60 percent of questions and 36 percent of full-sentence searches are answered by the AI.&lt;/p&gt;
&lt;p&gt;This research provides more evidence that Google's use of AI is changing the way people gather information and interact with search results. The trends are bad for web publishing, but Google's profits have never been higher. Funny how that works.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pew Research Center analysis shows how hard AI is hitting web traffic.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Overview" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Search_SocialShare_7gpZ6Zv.width-1300-640x360.jpg" width="640" /&gt;
                  &lt;img alt="AI Overview" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Search_SocialShare_7gpZ6Zv.width-1300-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google's search results have undergone a seismic shift over the past year as AI fever has continued to escalate among the tech giants. Nowhere is this change more apparent than right at the top of Google's storied results page, which is now home to AI Overviews. Google contends these Gemini-based answers don't take traffic away from websites, but a new analysis from the Pew Research Center says otherwise. Its analysis shows that searches with AI summaries reduce clicks, and their prevalence is increasing.&lt;/p&gt;
&lt;p&gt;Google began testing AI Overviews as the "search generative experience" in May 2023, and just a year later, they were an official part of the search engine results page (SERP). Many sites (including this one) have noticed changes to their traffic in the wake of this move, but Google has brushed off concerns about how this could affect the sites from which it collects all that data.&lt;/p&gt;
&lt;p&gt;SEO experts have disagreed with Google's stance on how AI affects web traffic, and the newly released Pew study backs them up. The Pew Research Center analyzed data from 900 users of the Ipsos KnowledgePanel collected in March 2025. The analysis shows that among the test group, users were much less likely to click on search results when the page included an AI Overview.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2107352 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Pew AI Overviews stats" class="fullwidth full" height="1592" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Pew-AI.png" width="1792" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Pre Research Center

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Pew reports that searches without an AI answer resulted in a click rate of 15 percent. On SERPs with AI Overviews, the rate of clicks to other sites drops by almost half, to 8 percent. Google has also, on several occasions, claimed that people click on the links cited in AI Overviews, but Pew found that just 1 percent of AI Overviews produced a click on a source. These sources are most frequently Wikipedia, YouTube, and Reddit, which collectively account for 15 percent of all AI sources.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And perhaps more troubling, Google users are more likely to end their browsing session after seeing an AI Overview. That suggests that many people are seeing information generated by a robot, and their investigation stops there. Unfortunately for these people, all forms of generative AI are prone to "hallucinations" that cause them to provide incorrect information. So more people could be walking away from a search with the wrong information.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2098864 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="AI overview on phone" class="fullwidth full" height="1824" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/AI-Overviews.jpg" width="2000" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Overviews are integrated with Google's results, and they are appearing on more searches all the time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;This problem is unlikely to improve over time. Since launching AI Overviews, Google has repeatedly expanded the number of searches that get robot summaries. The Pew Research Center says that about 1 in 5 searches now have AI Overviews. Generally, the more words in a search, the more likely it is to trigger an AI Overview, and that's especially true for searches phrased as questions. The research shows that 60 percent of questions and 36 percent of full-sentence searches are answered by the AI.&lt;/p&gt;
&lt;p&gt;This research provides more evidence that Google's use of AI is changing the way people gather information and interact with search results. The trends are bad for web publishing, but Google's profits have never been higher. Funny how that works.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/research-shows-google-ai-overviews-reduce-website-clicks-by-almost-half/</guid><pubDate>Tue, 22 Jul 2025 18:49:46 +0000</pubDate></item><item><title>Apple Intelligence news summaries are back, with a big red disclaimer (AI – Ars Technica)</title><link>https://arstechnica.com/apple/2025/07/apple-intelligence-news-summaries-are-back-with-a-big-red-disclaimer/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Apple disabled news summaries earlier this year after they mangled headlines.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1600" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/apple-summary-26.jpeg" width="2400" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Apple Intelligence notifications summaries for news apps are back in the latest iOS 26, macOS 26, and iPadOS 26 developer betas.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Andrew Cunningham

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Apple has released the fourth developer betas of iOS 26, iPadOS 26, macOS 26 and its other next-generation software updates today. And along with their other changes and fixes, the new builds are bringing back Apple Intelligence notification summaries for news apps.&lt;/p&gt;
&lt;p&gt;Apple disabled news notification summaries as part of the iOS 18.3 update in January. Incorrect summaries circulating on social media prompted news organizations to complain to Apple, particularly after one summary said that Luigi Mangione, alleged murderer of UnitedHealthcare CEO Brian Thompson, had died by suicide (he had not and has not).&lt;/p&gt;
&lt;p&gt;Upon installing the new update, users of Apple Intelligence-compatible devices will be asked to enable or disable three broad categories of notifications: those for "News &amp;amp; Entertainment" apps, for "Communication &amp;amp; Social" apps, and for all other apps. The operating systems will list sample apps based on what you currently have installed on your device.&lt;/p&gt;
&lt;p&gt;All Apple Intelligence notification summaries continue to be listed as "beta," but Apple's main change here is a big red disclaimer when you enable News &amp;amp; Entertainment notification summaries, pointing out that "summarization may change the meaning of the original headlines." The notifications also get a special "summarized by Apple Intelligence" caption to further distinguish them from regular, unadulterated notifications.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Apple Intelligence will also reportedly see under-the-hood improvements in the new updates, relative to the first-generation versions in iOS 18. Apple executives have talked about underlying architectural improvements that will allow it to finally roll out a new version of Siri, for example, and notification summaries should also benefit (when contacted by Ars for comment, Apple wouldn't share more specific information about exactly what has been changed or improved).&lt;/p&gt;
&lt;p&gt;In any case, the disclaimer makes it clear that Apple's fix for the problem involves warning users to expect incorrect summaries and allowing them to turn news summaries off, rather than guaranteeing that summaries will always be correct.&lt;/p&gt;
&lt;p&gt;Other changes that Apple has made to distinguish summarized notifications from regular ones should remain in effect in the new OS versions: Summaries will get italicized text and a small icon denoting that you're looking at summaries rather than the original notifications, and users can tap a stack of summarized notifications to see all of the originals.&lt;/p&gt;
&lt;p&gt;Apple announced back in June that the first public betas of the new operating systems would be available in July. With only around a week and a half left in the month, it's highly likely that the first public beta will be similar to the developer beta build released today. The updates will be released to the general public sometime this fall—usually in September or October if Apple adheres to its usual timeline.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Apple disabled news summaries earlier this year after they mangled headlines.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1600" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/apple-summary-26.jpeg" width="2400" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Apple Intelligence notifications summaries for news apps are back in the latest iOS 26, macOS 26, and iPadOS 26 developer betas.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Andrew Cunningham

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Apple has released the fourth developer betas of iOS 26, iPadOS 26, macOS 26 and its other next-generation software updates today. And along with their other changes and fixes, the new builds are bringing back Apple Intelligence notification summaries for news apps.&lt;/p&gt;
&lt;p&gt;Apple disabled news notification summaries as part of the iOS 18.3 update in January. Incorrect summaries circulating on social media prompted news organizations to complain to Apple, particularly after one summary said that Luigi Mangione, alleged murderer of UnitedHealthcare CEO Brian Thompson, had died by suicide (he had not and has not).&lt;/p&gt;
&lt;p&gt;Upon installing the new update, users of Apple Intelligence-compatible devices will be asked to enable or disable three broad categories of notifications: those for "News &amp;amp; Entertainment" apps, for "Communication &amp;amp; Social" apps, and for all other apps. The operating systems will list sample apps based on what you currently have installed on your device.&lt;/p&gt;
&lt;p&gt;All Apple Intelligence notification summaries continue to be listed as "beta," but Apple's main change here is a big red disclaimer when you enable News &amp;amp; Entertainment notification summaries, pointing out that "summarization may change the meaning of the original headlines." The notifications also get a special "summarized by Apple Intelligence" caption to further distinguish them from regular, unadulterated notifications.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Apple Intelligence will also reportedly see under-the-hood improvements in the new updates, relative to the first-generation versions in iOS 18. Apple executives have talked about underlying architectural improvements that will allow it to finally roll out a new version of Siri, for example, and notification summaries should also benefit (when contacted by Ars for comment, Apple wouldn't share more specific information about exactly what has been changed or improved).&lt;/p&gt;
&lt;p&gt;In any case, the disclaimer makes it clear that Apple's fix for the problem involves warning users to expect incorrect summaries and allowing them to turn news summaries off, rather than guaranteeing that summaries will always be correct.&lt;/p&gt;
&lt;p&gt;Other changes that Apple has made to distinguish summarized notifications from regular ones should remain in effect in the new OS versions: Summaries will get italicized text and a small icon denoting that you're looking at summaries rather than the original notifications, and users can tap a stack of summarized notifications to see all of the originals.&lt;/p&gt;
&lt;p&gt;Apple announced back in June that the first public betas of the new operating systems would be available in July. With only around a week and a half left in the month, it's highly likely that the first public beta will be similar to the developer beta build released today. The updates will be released to the general public sometime this fall—usually in September or October if Apple adheres to its usual timeline.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/apple/2025/07/apple-intelligence-news-summaries-are-back-with-a-big-red-disclaimer/</guid><pubDate>Tue, 22 Jul 2025 20:24:41 +0000</pubDate></item><item><title>OpenAI agreed to pay Oracle $30B a year for data center services (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/openai-agreed-to-pay-oracle-30b-a-year-for-data-center-services/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/07/gettyimages-494272836.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI was the company that signed a $30 billion per year deal with Oracle for data center services, disclosed last month, The Wall Street Journal reported on Monday. Now, OpenAI CEO Sam Altman has confirmed the details of the contract (but not the dollar amount) in an X post on Tuesday and in a company blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To recap, on June 30, Oracle disclosed in an SEC filing that it had signed a cloud deal that would generate $30 billion a year in revenue. However, the company didn’t say who it was with or for what services. The news caused Oracle’s stock to hit an all-time high, making its founder and CTO, Larry Ellison, the second richest person in the world, according to Bloomberg.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speculation on the identity of the customer ensued as people wondered what company could possibly need a fresh $30 billion a year in data center services. For comparison, Oracle collectively sold $24.5 billion worth of cloud services in its fiscal 2025 to all customers combined, it reported in June.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has now explained that this Oracle deal is for 4.5 gigawatts of capacity as part of Stargate, the $500 billion data-center-building project OpenAI, Oracle, and SoftBank announced in January. (Apparently, the $30 billion deal does not involve SoftBank.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The WSJ reports 4.5 gigawatts is the equivalent of two Hoover Dams, enough power for about four million homes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t a straightforward win for Oracle. OpenAI and Oracle still have to build this monster data center, which will be a costly endeavor, both in cash and in energy. They are doing so at what OpenAI called the Stargate I site in Abilene, Texas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Oracle spent $21.2 billion on capital expenditures in its last fiscal year, CEO Safra Catz reported in June, and it expects to spend another $25 billion this year, she said. So, nearly $50 billion, largely spent on data centers (and that doesn’t include land purchases, she said) in two years. Although, to be clear, that money also supports Oracle’s existing customers, in addition to OpenAI’s demands.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;One final interesting part to note about all of this: Last month, Altman said that OpenAI recently hit $10 billion in annual recurring revenue, up from around $5.5 billion last year. This single commitment to Oracle is already triple per year what it is currently bringing in and doesn’t include all of the company’s other expenses, including its current data center commitments.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/07/gettyimages-494272836.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI was the company that signed a $30 billion per year deal with Oracle for data center services, disclosed last month, The Wall Street Journal reported on Monday. Now, OpenAI CEO Sam Altman has confirmed the details of the contract (but not the dollar amount) in an X post on Tuesday and in a company blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To recap, on June 30, Oracle disclosed in an SEC filing that it had signed a cloud deal that would generate $30 billion a year in revenue. However, the company didn’t say who it was with or for what services. The news caused Oracle’s stock to hit an all-time high, making its founder and CTO, Larry Ellison, the second richest person in the world, according to Bloomberg.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speculation on the identity of the customer ensued as people wondered what company could possibly need a fresh $30 billion a year in data center services. For comparison, Oracle collectively sold $24.5 billion worth of cloud services in its fiscal 2025 to all customers combined, it reported in June.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has now explained that this Oracle deal is for 4.5 gigawatts of capacity as part of Stargate, the $500 billion data-center-building project OpenAI, Oracle, and SoftBank announced in January. (Apparently, the $30 billion deal does not involve SoftBank.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The WSJ reports 4.5 gigawatts is the equivalent of two Hoover Dams, enough power for about four million homes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t a straightforward win for Oracle. OpenAI and Oracle still have to build this monster data center, which will be a costly endeavor, both in cash and in energy. They are doing so at what OpenAI called the Stargate I site in Abilene, Texas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Oracle spent $21.2 billion on capital expenditures in its last fiscal year, CEO Safra Catz reported in June, and it expects to spend another $25 billion this year, she said. So, nearly $50 billion, largely spent on data centers (and that doesn’t include land purchases, she said) in two years. Although, to be clear, that money also supports Oracle’s existing customers, in addition to OpenAI’s demands.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;One final interesting part to note about all of this: Last month, Altman said that OpenAI recently hit $10 billion in annual recurring revenue, up from around $5.5 billion last year. This single commitment to Oracle is already triple per year what it is currently bringing in and doesn’t include all of the company’s other expenses, including its current data center commitments.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/openai-agreed-to-pay-oracle-30b-a-year-for-data-center-services/</guid><pubDate>Tue, 22 Jul 2025 20:36:31 +0000</pubDate></item><item><title>Amazon acquires Bee, the AI wearable that records everything you say (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/amazon-acquires-bee-the-ai-wearable-that-records-everything-you-say/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/07/HD.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has acquired the AI wearables startup Bee, according to a LinkedIn post by Bee co-founder Maria de Lourdes Zollo. Amazon confirmed the acquisition to TechCrunch but noted that the deal has not yet closed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bee, which raised $7 million last year, makes both a stand-alone Fitbit-like bracelet (which retails for $49.99, plus a $19-per-month subscription) and an Apple Watch app. The product records everything it hears — unless the user manually mutes it — with the goal of listening to conversations to create reminders and to-do lists for the user.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zollo told TechCrunch last year that the company hopes to create a “cloud phone,” or a mirror of your phone that gives the personal Bee device access to the user’s accounts and notifications, making it possible to get reminders about events or send messages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We believe everyone should have access to a personal, ambient intelligence that feels less like a tool and more like a trusted companion. One that helps you reflect, remember, and move through the world more freely,” Bee claims on its website.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other companies like Rabbit and Humane AI have tried to make AI-enabled wearables like this but have not found much success thus far. But at a $50 price point, Bee’s devices are more cost-accessible to a curious consumer who doesn’t want to make a big financial commitment. (The ill-fated Humane AI Pin was $499.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Amazon spokesperson told TechCrunch that Bee employees received offers to join Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This acquisition signals Amazon’s interest in developing wearable AI devices, a different avenue from its voice-controlled home assistant products like its line of Echo speakers. ChatGPT maker OpenAI is working on its own AI hardware, while Meta is integrating its AI into its smart glasses. Apple is rumored to be working on AI-powered smart glasses as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These products come with a number of security and privacy risks, given that they record everything around them; different companies’ policies will vary in terms of how voice recordings are processed, stored, and used for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In its current privacy policies, Bee says that users can delete their data at any time and that audio recordings are not saved, stored, or used for AI training. The app does store data that the AI learns about the user, however, which is how it can function as an assistant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bee previously indicated that it planned to only record the voices of people who have verbally consented. Bee also says it’s working on a feature to allow users to define boundaries — both based on topic and location — that will automatically pause the device’s learning. The company noted that it plans to build on-device AI processing, which generally poses less of a privacy risk than processing data in the cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s not clear if these policies will change as Bee is integrated into Amazon, however — and Amazon has a mixed record on the handling of user data from its customers’ devices. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the past, Amazon shared footage with law enforcement from people’s personal Ring security cameras, with neither the owner’s consent, nor a warrant. Ring also settled claims in 2023 brought by the Federal Trade Commission that employees and contractors had broad and unrestricted access to customers’ videos.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/07/HD.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has acquired the AI wearables startup Bee, according to a LinkedIn post by Bee co-founder Maria de Lourdes Zollo. Amazon confirmed the acquisition to TechCrunch but noted that the deal has not yet closed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bee, which raised $7 million last year, makes both a stand-alone Fitbit-like bracelet (which retails for $49.99, plus a $19-per-month subscription) and an Apple Watch app. The product records everything it hears — unless the user manually mutes it — with the goal of listening to conversations to create reminders and to-do lists for the user.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zollo told TechCrunch last year that the company hopes to create a “cloud phone,” or a mirror of your phone that gives the personal Bee device access to the user’s accounts and notifications, making it possible to get reminders about events or send messages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We believe everyone should have access to a personal, ambient intelligence that feels less like a tool and more like a trusted companion. One that helps you reflect, remember, and move through the world more freely,” Bee claims on its website.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other companies like Rabbit and Humane AI have tried to make AI-enabled wearables like this but have not found much success thus far. But at a $50 price point, Bee’s devices are more cost-accessible to a curious consumer who doesn’t want to make a big financial commitment. (The ill-fated Humane AI Pin was $499.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Amazon spokesperson told TechCrunch that Bee employees received offers to join Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This acquisition signals Amazon’s interest in developing wearable AI devices, a different avenue from its voice-controlled home assistant products like its line of Echo speakers. ChatGPT maker OpenAI is working on its own AI hardware, while Meta is integrating its AI into its smart glasses. Apple is rumored to be working on AI-powered smart glasses as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These products come with a number of security and privacy risks, given that they record everything around them; different companies’ policies will vary in terms of how voice recordings are processed, stored, and used for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In its current privacy policies, Bee says that users can delete their data at any time and that audio recordings are not saved, stored, or used for AI training. The app does store data that the AI learns about the user, however, which is how it can function as an assistant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bee previously indicated that it planned to only record the voices of people who have verbally consented. Bee also says it’s working on a feature to allow users to define boundaries — both based on topic and location — that will automatically pause the device’s learning. The company noted that it plans to build on-device AI processing, which generally poses less of a privacy risk than processing data in the cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s not clear if these policies will change as Bee is integrated into Amazon, however — and Amazon has a mixed record on the handling of user data from its customers’ devices. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the past, Amazon shared footage with law enforcement from people’s personal Ring security cameras, with neither the owner’s consent, nor a warrant. Ring also settled claims in 2023 brought by the Federal Trade Commission that employees and contractors had broad and unrestricted access to customers’ videos.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/amazon-acquires-bee-the-ai-wearable-that-records-everything-you-say/</guid><pubDate>Tue, 22 Jul 2025 20:51:41 +0000</pubDate></item><item><title>Alibaba’s new open source Qwen3-235B-A22B-2507 beats Kimi-2 and offers low compute version (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/alibabas-new-open-source-qwen3-235b-a22b-2507-beats-kimi-2-and-offers-low-compute-version/</link><description>&lt;p&gt;Chinese e-commerce giant Alibaba has made waves globally in the tech and business communities with its own family of “Qwen” generative AI large language models, beginning with the launch of the original Tongyi Qianwen LLM chatbot in April 2023 through the release of Qwen 3 in April 2025.&lt;/p&gt;&lt;p&gt;Well, not only are its models powerful and score high on third-party benchmark tests at completing math, science, reasoning, and writing tasks, but for the most part, they’ve been released under permissive open source licensing terms, allowing organizations and enterprises to download them, customize them, run them, and generally use them for all variety of purposes, even commercial. Think of them as an alternative to DeepSeek. &lt;/p&gt;&lt;p&gt;This week, Alibaba’s “Qwen Team,” as its AI division is known, released the latest updates to its Qwen family, and they’re already attracting attention once more from AI power users in the West for their top performance, in one case, edging out even the new Kimi-2 model from rival Chinese AI startup Moonshot released in mid-July 2025.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The new Qwen3-235B-A22B-2507-Instruct model — released on AI code sharing community Hugging Face alongside a “floating point 8” or FP8 version, which we’ll cover more in-depth below — improves from the original Qwen 3 on reasoning tasks, factual accuracy, and multilingual understanding. It also outperforms Claude Opus 4’s “non-thinking” version. &lt;/p&gt;



&lt;p&gt;The new Qwen3 model update also delivers better coding results, alignment with user preferences, and long-context handling, according to its creators. But that’s not all…&lt;/p&gt;



&lt;p&gt;Read on for what else it offers enterprise users and technical decision-makers.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-fp8-version-lets-enterprises-run-qwen-3-with-far-less-memory-and-far-less-compute"&gt;FP8 version lets enterprises run Qwen 3 with far less memory and far less compute&lt;/h2&gt;



&lt;p&gt;In addition to the new Qwen3-235B-A22B-2507 model, the Qwen Team released an “FP8” version, which stands for &lt;strong&gt;8-bit floating point&lt;/strong&gt;, a format that compresses the model’s numerical operations to use less memory and processing power — without noticeably affecting its performance. &lt;/p&gt;



&lt;p&gt;In practice, this means organizations can run a model with Qwen3’s capabilities on smaller, less expensive hardware or more efficiently in the cloud. The result is faster response times, lower energy costs, and the ability to scale deployments without needing massive infrastructure.&lt;/p&gt;



&lt;p&gt;This makes the FP8 model especially attractive for production environments with tight latency or cost constraints. Teams can scale Qwen3’s capabilities to single-node GPU instances or local development machines, avoiding the need for massive multi-GPU clusters. It also lowers the barrier to private fine-tuning and on-premises deployments, where infrastructure resources are finite and total cost of ownership matters.&lt;/p&gt;



&lt;p&gt;Even though Qwen team didn’t release official calculations, comparisons to similar FP8 quantized deployments suggest the efficiency savings are substantial. Here’s a practical illustration:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Metric&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;FP16 Version (Instruct)&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;FP8 Version (Instruct-FP8)&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;GPU Memory Use&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~88 GB&lt;/td&gt;&lt;td&gt;~30 GB&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Inference Speed&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~30–40 tokens/sec&lt;/td&gt;&lt;td&gt;~60–70 tokens/sec&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Power Draw&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;High&lt;/td&gt;&lt;td&gt;~30–50% lower&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Number of GPUs Needed&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8× A100s or similar&lt;/td&gt;&lt;td&gt;4× A100s or fewer&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p&gt;&lt;em&gt;Estimates based on industry norms for FP8 deployments. Actual results vary by batch size, prompt length, and inference framework (e.g., vLLM, Transformers, SGLang).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;h2 class="wp-block-heading" id="h-no-more-hybrid-reasoning-instead-qwen-will-release-separate-reasoning-and-instruct-models"&gt;No more ‘hybrid reasoning’…instead Qwen will release separate reasoning and instruct models!&lt;/h2&gt;



&lt;p&gt;Perhaps most interesting of all, Qwen Team announced it will no longer be pursuing a “hybrid” reasoning approach, which it introduced back with Qwen 3 in April and seemed to be inspired by an approach pioneered by sovereign AI collective Nous Research. &lt;/p&gt;



&lt;p&gt;This allowed users to toggle on a “reasoning” model, letting the AI model engage in its own self-checking and producing “chains-of-thought” before responding. &lt;/p&gt;



&lt;p&gt;In a way, it was designed to mimic the reasoning capabilities of powerful proprietary models such as OpenAI’s “o” series (o1, o3, o4-mini, o4-mini-high), which also produce “chains-of-thought.”&lt;/p&gt;



&lt;p&gt;However, unlike those rival models which always engage in such “reasoning” for every prompt, Qwen 3 could have the reasoning mode manually switched on or off by the user by clicking a “Thinking Mode” button on the Qwen website chatbot, or by typing “/think” before their prompt on a local or privately run model inference. &lt;/p&gt;



&lt;p&gt;The idea was to give users control to engage the slower and more token-intensive thinking mode for more difficult prompts and tasks, and use a non-thinking mode for simpler prompts. But again, this put the onus on the user to decide. While flexible, it also introduced design complexity and inconsistent behavior in some cases.&lt;/p&gt;



&lt;p&gt;Now As Qwen team wrote in its announcement post on X: &lt;/p&gt;



&lt;p&gt;&lt;em&gt;“After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;With the 2507 update — an instruct or NON-REASONING model only, for now — Alibaba is no longer straddling both approaches in a single model. Instead, separate model variants will be trained for instruction and reasoning tasks respectively. &lt;/p&gt;



&lt;p&gt;The result is a model that adheres more closely to user instructions, generates more predictable responses, and, as benchmark data shows, improves significantly across multiple evaluation domains.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-benchmarks-and-use-cases"&gt;Performance benchmarks and use cases&lt;/h2&gt;



&lt;p&gt;Compared to its predecessor, the Qwen3-235B-A22B-Instruct-2507 model delivers measurable improvements:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;MMLU-Pro scores rise from 75.2 to 83.0&lt;/strong&gt;, a notable gain in general knowledge performance.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;GPQA and SuperGPQA benchmarks improve by 15–20 percentage points&lt;/strong&gt;, reflecting stronger factual accuracy.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Reasoning tasks&lt;/strong&gt; such as AIME25 and ARC-AGI show more than double the previous performance.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Code generation improves&lt;/strong&gt;, with LiveCodeBench scores increasing from 32.9 to 51.8.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Multilingual support expands&lt;/strong&gt;, aided by improved coverage of long-tail languages and better alignment across dialects.&lt;/li&gt;
&lt;/ul&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014560" height="450" src="https://venturebeat.com/wp-content/uploads/2025/07/GwZbdvdbEAU2Z4H-1.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;The model maintains a mixture-of-experts (MoE) architecture, activating 8 out of 128 experts during inference, with a total of 235 billion parameters—22 billion of which are active at any time. &lt;/p&gt;



&lt;p&gt;As mentioned before, the FP8 version introduces fine-grained quantization for better inference speed and reduced memory usage.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-enterprise-ready-by-design"&gt;Enterprise-ready by design&lt;/h2&gt;



&lt;p&gt;Unlike many open-source LLMs, which are often released under restrictive research-only licenses or require API access for commercial use, Qwen3 is squarely aimed at enterprise deployment. &lt;/p&gt;



&lt;p&gt;Boasting a permissive &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;, this means enterprises can use it freely for commercial applications. They may also:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Deploy models locally or through OpenAI-compatible APIs using vLLM and SGLang&lt;/li&gt;



&lt;li&gt;Fine-tune models privately using LoRA or QLoRA without exposing proprietary data&lt;/li&gt;



&lt;li&gt;Log and inspect all prompts and outputs on-premises for compliance and auditing&lt;/li&gt;



&lt;li&gt;Scale from prototype to production using dense variants (from 0.6B to 32B) or MoE checkpoints&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Alibaba’s team also introduced &lt;strong&gt;Qwen-Agent&lt;/strong&gt;, a lightweight framework that abstracts tool invocation logic for users building agentic systems. &lt;/p&gt;



&lt;p&gt;Benchmarks like TAU-Retail and BFCL-v3 suggest the instruction model can competently execute multi-step decision tasks—typically the domain of purpose-built agents.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-community-and-industry-reactions"&gt;Community and industry reactions&lt;/h2&gt;



&lt;p&gt;The release has already been well received by AI power users. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Paul Couvert&lt;/strong&gt;, AI educator and founder of private LLM chatbot host Blue Shell AI, posted a comparison chart on X showing Qwen3-235B-A22B-Instruct-2507 outperforming Claude Opus 4 and Kimi K2 on benchmarks like GPQA, AIME25, and Arena-Hard v2, calling it &lt;em&gt;“even more powerful than Kimi K2… and even better than Claude Opus 4.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;AI influencer &lt;strong&gt;NIK (@ns123abc)&lt;/strong&gt;, commented on its rapid impact: &lt;em&gt;“You’re laughing. Qwen-3-235B made Kimi K2 irrelevant after only one week despite being one quarter the size and you’re laughing.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;Meanwhile, &lt;strong&gt;Jeff Boudier&lt;/strong&gt;, head of product at Hugging Face, highlighted the deployment benefits: &lt;em&gt;“Qwen silently released a massive improvement to Qwen3… it tops best open (Kimi K2, a 4x larger model) and closed (Claude Opus 4) LLMs on benchmarks.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;He praised the availability of an FP8 checkpoint for faster inference, 1-click deployment on Azure ML, and support for local use via MLX on Mac or INT4 builds from Intel.&lt;/p&gt;



&lt;p&gt;The overall tone from developers has been enthusiastic, as the model’s balance of performance, licensing, and deployability appeals to both hobbyists and professionals.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-qwen-team"&gt;&lt;strong&gt;What’s next for Qwen team?&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Alibaba is already laying the groundwork for future updates. A separate reasoning-focused model is in the pipeline, and the Qwen roadmap points toward increasingly agentic systems capable of long-horizon task planning. &lt;/p&gt;



&lt;p&gt;Multimodal support, seen in Qwen2.5-Omni and Qwen-VL models, is also expected to expand further.&lt;/p&gt;



&lt;p&gt;And already, rumors and rumblings have started as Qwen team members tease yet another update to their model family incoming, with updates on their web properties revealing URL strings for a new Qwen3-Coder-480B-A35B-Instruct model, likely a 480-billion parameter mixture-of-experts (MoE) with a token context of 1 million.&lt;/p&gt;



&lt;p&gt;What Qwen3-235B-A22B-Instruct-2507 ultimately signals is not just another leap in benchmark performance, but a maturation of open models as viable alternatives to proprietary systems. &lt;/p&gt;



&lt;p&gt;The flexibility of deployment, strong general performance, and enterprise-friendly licensing give the model a unique edge in a crowded field.&lt;/p&gt;



&lt;p&gt;For teams looking to integrate advanced instruction-following models into their AI stack—without the limitations of vendor lock-in or usage-based fees—Qwen3 is a serious contender.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;Chinese e-commerce giant Alibaba has made waves globally in the tech and business communities with its own family of “Qwen” generative AI large language models, beginning with the launch of the original Tongyi Qianwen LLM chatbot in April 2023 through the release of Qwen 3 in April 2025.&lt;/p&gt;&lt;p&gt;Well, not only are its models powerful and score high on third-party benchmark tests at completing math, science, reasoning, and writing tasks, but for the most part, they’ve been released under permissive open source licensing terms, allowing organizations and enterprises to download them, customize them, run them, and generally use them for all variety of purposes, even commercial. Think of them as an alternative to DeepSeek. &lt;/p&gt;&lt;p&gt;This week, Alibaba’s “Qwen Team,” as its AI division is known, released the latest updates to its Qwen family, and they’re already attracting attention once more from AI power users in the West for their top performance, in one case, edging out even the new Kimi-2 model from rival Chinese AI startup Moonshot released in mid-July 2025.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The new Qwen3-235B-A22B-2507-Instruct model — released on AI code sharing community Hugging Face alongside a “floating point 8” or FP8 version, which we’ll cover more in-depth below — improves from the original Qwen 3 on reasoning tasks, factual accuracy, and multilingual understanding. It also outperforms Claude Opus 4’s “non-thinking” version. &lt;/p&gt;



&lt;p&gt;The new Qwen3 model update also delivers better coding results, alignment with user preferences, and long-context handling, according to its creators. But that’s not all…&lt;/p&gt;



&lt;p&gt;Read on for what else it offers enterprise users and technical decision-makers.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-fp8-version-lets-enterprises-run-qwen-3-with-far-less-memory-and-far-less-compute"&gt;FP8 version lets enterprises run Qwen 3 with far less memory and far less compute&lt;/h2&gt;



&lt;p&gt;In addition to the new Qwen3-235B-A22B-2507 model, the Qwen Team released an “FP8” version, which stands for &lt;strong&gt;8-bit floating point&lt;/strong&gt;, a format that compresses the model’s numerical operations to use less memory and processing power — without noticeably affecting its performance. &lt;/p&gt;



&lt;p&gt;In practice, this means organizations can run a model with Qwen3’s capabilities on smaller, less expensive hardware or more efficiently in the cloud. The result is faster response times, lower energy costs, and the ability to scale deployments without needing massive infrastructure.&lt;/p&gt;



&lt;p&gt;This makes the FP8 model especially attractive for production environments with tight latency or cost constraints. Teams can scale Qwen3’s capabilities to single-node GPU instances or local development machines, avoiding the need for massive multi-GPU clusters. It also lowers the barrier to private fine-tuning and on-premises deployments, where infrastructure resources are finite and total cost of ownership matters.&lt;/p&gt;



&lt;p&gt;Even though Qwen team didn’t release official calculations, comparisons to similar FP8 quantized deployments suggest the efficiency savings are substantial. Here’s a practical illustration:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Metric&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;FP16 Version (Instruct)&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;FP8 Version (Instruct-FP8)&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;GPU Memory Use&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~88 GB&lt;/td&gt;&lt;td&gt;~30 GB&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Inference Speed&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~30–40 tokens/sec&lt;/td&gt;&lt;td&gt;~60–70 tokens/sec&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Power Draw&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;High&lt;/td&gt;&lt;td&gt;~30–50% lower&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Number of GPUs Needed&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8× A100s or similar&lt;/td&gt;&lt;td&gt;4× A100s or fewer&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p&gt;&lt;em&gt;Estimates based on industry norms for FP8 deployments. Actual results vary by batch size, prompt length, and inference framework (e.g., vLLM, Transformers, SGLang).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;h2 class="wp-block-heading" id="h-no-more-hybrid-reasoning-instead-qwen-will-release-separate-reasoning-and-instruct-models"&gt;No more ‘hybrid reasoning’…instead Qwen will release separate reasoning and instruct models!&lt;/h2&gt;



&lt;p&gt;Perhaps most interesting of all, Qwen Team announced it will no longer be pursuing a “hybrid” reasoning approach, which it introduced back with Qwen 3 in April and seemed to be inspired by an approach pioneered by sovereign AI collective Nous Research. &lt;/p&gt;



&lt;p&gt;This allowed users to toggle on a “reasoning” model, letting the AI model engage in its own self-checking and producing “chains-of-thought” before responding. &lt;/p&gt;



&lt;p&gt;In a way, it was designed to mimic the reasoning capabilities of powerful proprietary models such as OpenAI’s “o” series (o1, o3, o4-mini, o4-mini-high), which also produce “chains-of-thought.”&lt;/p&gt;



&lt;p&gt;However, unlike those rival models which always engage in such “reasoning” for every prompt, Qwen 3 could have the reasoning mode manually switched on or off by the user by clicking a “Thinking Mode” button on the Qwen website chatbot, or by typing “/think” before their prompt on a local or privately run model inference. &lt;/p&gt;



&lt;p&gt;The idea was to give users control to engage the slower and more token-intensive thinking mode for more difficult prompts and tasks, and use a non-thinking mode for simpler prompts. But again, this put the onus on the user to decide. While flexible, it also introduced design complexity and inconsistent behavior in some cases.&lt;/p&gt;



&lt;p&gt;Now As Qwen team wrote in its announcement post on X: &lt;/p&gt;



&lt;p&gt;&lt;em&gt;“After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;With the 2507 update — an instruct or NON-REASONING model only, for now — Alibaba is no longer straddling both approaches in a single model. Instead, separate model variants will be trained for instruction and reasoning tasks respectively. &lt;/p&gt;



&lt;p&gt;The result is a model that adheres more closely to user instructions, generates more predictable responses, and, as benchmark data shows, improves significantly across multiple evaluation domains.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-benchmarks-and-use-cases"&gt;Performance benchmarks and use cases&lt;/h2&gt;



&lt;p&gt;Compared to its predecessor, the Qwen3-235B-A22B-Instruct-2507 model delivers measurable improvements:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;MMLU-Pro scores rise from 75.2 to 83.0&lt;/strong&gt;, a notable gain in general knowledge performance.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;GPQA and SuperGPQA benchmarks improve by 15–20 percentage points&lt;/strong&gt;, reflecting stronger factual accuracy.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Reasoning tasks&lt;/strong&gt; such as AIME25 and ARC-AGI show more than double the previous performance.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Code generation improves&lt;/strong&gt;, with LiveCodeBench scores increasing from 32.9 to 51.8.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Multilingual support expands&lt;/strong&gt;, aided by improved coverage of long-tail languages and better alignment across dialects.&lt;/li&gt;
&lt;/ul&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014560" height="450" src="https://venturebeat.com/wp-content/uploads/2025/07/GwZbdvdbEAU2Z4H-1.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;The model maintains a mixture-of-experts (MoE) architecture, activating 8 out of 128 experts during inference, with a total of 235 billion parameters—22 billion of which are active at any time. &lt;/p&gt;



&lt;p&gt;As mentioned before, the FP8 version introduces fine-grained quantization for better inference speed and reduced memory usage.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-enterprise-ready-by-design"&gt;Enterprise-ready by design&lt;/h2&gt;



&lt;p&gt;Unlike many open-source LLMs, which are often released under restrictive research-only licenses or require API access for commercial use, Qwen3 is squarely aimed at enterprise deployment. &lt;/p&gt;



&lt;p&gt;Boasting a permissive &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;, this means enterprises can use it freely for commercial applications. They may also:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Deploy models locally or through OpenAI-compatible APIs using vLLM and SGLang&lt;/li&gt;



&lt;li&gt;Fine-tune models privately using LoRA or QLoRA without exposing proprietary data&lt;/li&gt;



&lt;li&gt;Log and inspect all prompts and outputs on-premises for compliance and auditing&lt;/li&gt;



&lt;li&gt;Scale from prototype to production using dense variants (from 0.6B to 32B) or MoE checkpoints&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Alibaba’s team also introduced &lt;strong&gt;Qwen-Agent&lt;/strong&gt;, a lightweight framework that abstracts tool invocation logic for users building agentic systems. &lt;/p&gt;



&lt;p&gt;Benchmarks like TAU-Retail and BFCL-v3 suggest the instruction model can competently execute multi-step decision tasks—typically the domain of purpose-built agents.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-community-and-industry-reactions"&gt;Community and industry reactions&lt;/h2&gt;



&lt;p&gt;The release has already been well received by AI power users. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Paul Couvert&lt;/strong&gt;, AI educator and founder of private LLM chatbot host Blue Shell AI, posted a comparison chart on X showing Qwen3-235B-A22B-Instruct-2507 outperforming Claude Opus 4 and Kimi K2 on benchmarks like GPQA, AIME25, and Arena-Hard v2, calling it &lt;em&gt;“even more powerful than Kimi K2… and even better than Claude Opus 4.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;AI influencer &lt;strong&gt;NIK (@ns123abc)&lt;/strong&gt;, commented on its rapid impact: &lt;em&gt;“You’re laughing. Qwen-3-235B made Kimi K2 irrelevant after only one week despite being one quarter the size and you’re laughing.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;Meanwhile, &lt;strong&gt;Jeff Boudier&lt;/strong&gt;, head of product at Hugging Face, highlighted the deployment benefits: &lt;em&gt;“Qwen silently released a massive improvement to Qwen3… it tops best open (Kimi K2, a 4x larger model) and closed (Claude Opus 4) LLMs on benchmarks.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;He praised the availability of an FP8 checkpoint for faster inference, 1-click deployment on Azure ML, and support for local use via MLX on Mac or INT4 builds from Intel.&lt;/p&gt;



&lt;p&gt;The overall tone from developers has been enthusiastic, as the model’s balance of performance, licensing, and deployability appeals to both hobbyists and professionals.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-qwen-team"&gt;&lt;strong&gt;What’s next for Qwen team?&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Alibaba is already laying the groundwork for future updates. A separate reasoning-focused model is in the pipeline, and the Qwen roadmap points toward increasingly agentic systems capable of long-horizon task planning. &lt;/p&gt;



&lt;p&gt;Multimodal support, seen in Qwen2.5-Omni and Qwen-VL models, is also expected to expand further.&lt;/p&gt;



&lt;p&gt;And already, rumors and rumblings have started as Qwen team members tease yet another update to their model family incoming, with updates on their web properties revealing URL strings for a new Qwen3-Coder-480B-A35B-Instruct model, likely a 480-billion parameter mixture-of-experts (MoE) with a token context of 1 million.&lt;/p&gt;



&lt;p&gt;What Qwen3-235B-A22B-Instruct-2507 ultimately signals is not just another leap in benchmark performance, but a maturation of open models as viable alternatives to proprietary systems. &lt;/p&gt;



&lt;p&gt;The flexibility of deployment, strong general performance, and enterprise-friendly licensing give the model a unique edge in a crowded field.&lt;/p&gt;



&lt;p&gt;For teams looking to integrate advanced instruction-following models into their AI stack—without the limitations of vendor lock-in or usage-based fees—Qwen3 is a serious contender.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/alibabas-new-open-source-qwen3-235b-a22b-2507-beats-kimi-2-and-offers-low-compute-version/</guid><pubDate>Tue, 22 Jul 2025 20:56:56 +0000</pubDate></item><item><title>Open-source MCPEval makes protocol-level agent testing plug-and-play (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/open-source-mcpeval-makes-protocol-level-agent-testing-plug-and-play/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Enterprises are beginning to adopt the Model Context Protocol (MCP) primarily to facilitate the identification and guidance of agent tool use. However, researchers from Salesforce discovered another way to utilize MCP technology, this time to aid in evaluating AI agents themselves.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The researchers unveiled MCPEval, a new method and open-source toolkit built on the architecture of the MCP system that tests agent performance when using tools. They noted current evaluation methods for agents are limited in that these “often relied on static, pre-defined tasks, thus failing to capture the interactive real-world agentic workflows.”&lt;/p&gt;



&lt;p&gt;“MCPEval goes beyond traditional success/failure metrics by systematically collecting detailed task trajectories and protocol interaction data, creating unprecedented visibility into agent behavior and generating valuable datasets for iterative improvement,” the researchers said in the paper. “Additionally, because both task creation and verification are fully automated, the resulting high-quality trajectories can be immediately leveraged for rapid fine-tuning and continual improvement of agent models. The comprehensive evaluation reports generated by MCPEval also provide actionable insights towards the correctness of agent-platform communication at a granular level.”&lt;/p&gt;



&lt;p&gt;MCPEval differentiates itself by being a fully automated process, which the researchers claimed allows for rapid evaluation of new MCP tools and servers. It both gathers information on how agents interact with tools within an MCP server, generates synthetic data and creates a database to benchmark agents. Users can choose which MCP servers and tools within those servers to test the agent’s performance on.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Shelby Heinecke, senior AI research manager at Salesforce and one of the paper’s authors, told VentureBeat that it is challenging to obtain accurate data on agent performance, particularly for agents in domain-specific roles.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We’ve gotten to the point where if you look across the tech industry, a lot of us have figured out how to deploy them. We now need to figure out how to evaluate them properly,” Heinecke said. “MCP is a very new idea, a very new paradigm. So, it’s great that agents are gonna have access to tools, but we again need to evaluate the agents on those tools. That’s exactly what MCPEval is all about.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-it-works"&gt;How it works&lt;/h2&gt;



&lt;p&gt;MCPEval’s framework takes on a task generation, verification and model evaluation design. Leveraging multiple large language models (LLMs) so users can choose to work with models they are more familiar with, agents can be evaluated through a variety of available LLMs in the market.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Enterprises can access MCPEval through an open-source toolkit released by Salesforce. Through a dashboard, users configure the server by selecting a model, which then automatically generates tasks for the agent to follow within the chosen MCP server.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://venturebeat.com/wp-content/uploads/2025/07/MCPEval-demo.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;



&lt;p&gt;Once the user verifies the tasks, MCPEval then takes the tasks and determines the tool calls needed as ground truth. These tasks will be used as the basis for the test. Users choose which model they prefer to run the evaluation. MCPEval can generate a report on how well the agent and the test model functioned in accessing and using these tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MCPEval not only gathers data to benchmark agents, Heinecke said, but it can also identify gaps in agent performance. Information gleaned by evaluating agents through MCPEval works not only to test performance but also to train the agents for future use.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We see MCPEval growing into a one-stop shop for evaluating and fixing your agents,” Heinecke said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;She added that what makes MCPEval stand out from other agent evaluators is that it brings the testing to the same environment in which the agent will be working. Agents are evaluated on how well they access tools within the MCP server to which they will likely be deployed.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The paper noted that in experiments, GPT-4 models often provided the best evaluation results.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-evaluating-agent-performance"&gt;Evaluating agent performance&lt;/h2&gt;



&lt;p&gt;The need for enterprises to begin testing and monitoring agent performance has led to a boom of frameworks and techniques. Some platforms offer testing and several more methods to evaluate both short-term and long-term agent performance.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;AI agents will perform tasks on behalf of users, often without the need for a human to prompt them. So far, agents have proven to be useful, but they can get overwhelmed by the sheer amount of tools at their disposal.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Galileo, a startup, offers a framework that enables enterprises to assess the quality of an agent’s tool selection and identify errors. Salesforce launched capabilities on its Agentforce dashboard to test agents. Researchers from Singapore Management University released AgentSpec to achieve and monitor agent reliability. Several academic studies on MCP evaluation have also been published, including MCP-Radar and MCPWorld.&lt;/p&gt;



&lt;p&gt;MCP-Radar, developed by researchers from the University of Massachusetts Amherst and Xi’an Jiaotong University, focuses on more general domain skills, such as software engineering or mathematics. This framework prioritizes efficiency and parameter accuracy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On the other hand, MCPWorld from Beijing University of Posts and Telecommunications brings benchmarking to graphical user interfaces, APIs, and other computer-use agents.&lt;/p&gt;



&lt;p&gt;Heinecke said ultimately, how agents are evaluated will depend on the company and the use case. However, what is crucial is that enterprises select the most suitable evaluation framework for their specific needs. For enterprises, she suggested considering a domain-specific framework to thoroughly test how agents function in real-world scenarios.&lt;/p&gt;



&lt;p&gt;“There’s value in each of these evaluation frameworks, and these are great starting points as they give some early signal to how strong the gent is,” Heinecke said. “But I think the most important evaluation is your domain-specific evaluation and coming up with evaluation data that reflects the environment in which the agent is going to be operating in.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Enterprises are beginning to adopt the Model Context Protocol (MCP) primarily to facilitate the identification and guidance of agent tool use. However, researchers from Salesforce discovered another way to utilize MCP technology, this time to aid in evaluating AI agents themselves.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The researchers unveiled MCPEval, a new method and open-source toolkit built on the architecture of the MCP system that tests agent performance when using tools. They noted current evaluation methods for agents are limited in that these “often relied on static, pre-defined tasks, thus failing to capture the interactive real-world agentic workflows.”&lt;/p&gt;



&lt;p&gt;“MCPEval goes beyond traditional success/failure metrics by systematically collecting detailed task trajectories and protocol interaction data, creating unprecedented visibility into agent behavior and generating valuable datasets for iterative improvement,” the researchers said in the paper. “Additionally, because both task creation and verification are fully automated, the resulting high-quality trajectories can be immediately leveraged for rapid fine-tuning and continual improvement of agent models. The comprehensive evaluation reports generated by MCPEval also provide actionable insights towards the correctness of agent-platform communication at a granular level.”&lt;/p&gt;



&lt;p&gt;MCPEval differentiates itself by being a fully automated process, which the researchers claimed allows for rapid evaluation of new MCP tools and servers. It both gathers information on how agents interact with tools within an MCP server, generates synthetic data and creates a database to benchmark agents. Users can choose which MCP servers and tools within those servers to test the agent’s performance on.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Shelby Heinecke, senior AI research manager at Salesforce and one of the paper’s authors, told VentureBeat that it is challenging to obtain accurate data on agent performance, particularly for agents in domain-specific roles.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We’ve gotten to the point where if you look across the tech industry, a lot of us have figured out how to deploy them. We now need to figure out how to evaluate them properly,” Heinecke said. “MCP is a very new idea, a very new paradigm. So, it’s great that agents are gonna have access to tools, but we again need to evaluate the agents on those tools. That’s exactly what MCPEval is all about.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-it-works"&gt;How it works&lt;/h2&gt;



&lt;p&gt;MCPEval’s framework takes on a task generation, verification and model evaluation design. Leveraging multiple large language models (LLMs) so users can choose to work with models they are more familiar with, agents can be evaluated through a variety of available LLMs in the market.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Enterprises can access MCPEval through an open-source toolkit released by Salesforce. Through a dashboard, users configure the server by selecting a model, which then automatically generates tasks for the agent to follow within the chosen MCP server.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://venturebeat.com/wp-content/uploads/2025/07/MCPEval-demo.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;



&lt;p&gt;Once the user verifies the tasks, MCPEval then takes the tasks and determines the tool calls needed as ground truth. These tasks will be used as the basis for the test. Users choose which model they prefer to run the evaluation. MCPEval can generate a report on how well the agent and the test model functioned in accessing and using these tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MCPEval not only gathers data to benchmark agents, Heinecke said, but it can also identify gaps in agent performance. Information gleaned by evaluating agents through MCPEval works not only to test performance but also to train the agents for future use.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We see MCPEval growing into a one-stop shop for evaluating and fixing your agents,” Heinecke said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;She added that what makes MCPEval stand out from other agent evaluators is that it brings the testing to the same environment in which the agent will be working. Agents are evaluated on how well they access tools within the MCP server to which they will likely be deployed.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The paper noted that in experiments, GPT-4 models often provided the best evaluation results.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-evaluating-agent-performance"&gt;Evaluating agent performance&lt;/h2&gt;



&lt;p&gt;The need for enterprises to begin testing and monitoring agent performance has led to a boom of frameworks and techniques. Some platforms offer testing and several more methods to evaluate both short-term and long-term agent performance.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;AI agents will perform tasks on behalf of users, often without the need for a human to prompt them. So far, agents have proven to be useful, but they can get overwhelmed by the sheer amount of tools at their disposal.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Galileo, a startup, offers a framework that enables enterprises to assess the quality of an agent’s tool selection and identify errors. Salesforce launched capabilities on its Agentforce dashboard to test agents. Researchers from Singapore Management University released AgentSpec to achieve and monitor agent reliability. Several academic studies on MCP evaluation have also been published, including MCP-Radar and MCPWorld.&lt;/p&gt;



&lt;p&gt;MCP-Radar, developed by researchers from the University of Massachusetts Amherst and Xi’an Jiaotong University, focuses on more general domain skills, such as software engineering or mathematics. This framework prioritizes efficiency and parameter accuracy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On the other hand, MCPWorld from Beijing University of Posts and Telecommunications brings benchmarking to graphical user interfaces, APIs, and other computer-use agents.&lt;/p&gt;



&lt;p&gt;Heinecke said ultimately, how agents are evaluated will depend on the company and the use case. However, what is crucial is that enterprises select the most suitable evaluation framework for their specific needs. For enterprises, she suggested considering a domain-specific framework to thoroughly test how agents function in real-world scenarios.&lt;/p&gt;



&lt;p&gt;“There’s value in each of these evaluation frameworks, and these are great starting points as they give some early signal to how strong the gent is,” Heinecke said. “But I think the most important evaluation is your domain-specific evaluation and coming up with evaluation data that reflects the environment in which the agent is going to be operating in.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/open-source-mcpeval-makes-protocol-level-agent-testing-plug-and-play/</guid><pubDate>Tue, 22 Jul 2025 21:17:18 +0000</pubDate></item><item><title>Intuit brings agentic AI to the mid-market saving organizations 17 to 20 hours a month (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/intuit-brings-agentic-ai-to-the-mid-market-saving-organizations-17-to-20-hours-a-month/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;One of the fastest-growing segments of the business market faces a technology paradox. They’ve outgrown small business tools but sometimes remain too small for many types of traditional enterprise solutions.&lt;/p&gt;



&lt;p&gt;That’s the domain of the mid-market, which Intuit defines as companies that generate anywhere from $2.5 million to $100 million in annual revenue. Mid-market organizations tend to operate differently from both small businesses and large enterprises. Small businesses might run on seven applications. Mid-market companies typically juggle 25 or more disconnected software tools as they scale. Unlike enterprises with dedicated IT teams and consolidated platforms, mid-market organizations often lack resources for complex system integration projects.&lt;/p&gt;



&lt;p&gt;This creates a unique AI deployment challenge. How do you deliver intelligent automation across fragmented, multi-entity business structures without requiring expensive platform consolidation? It’s a challenge that Intuit, the company behind popular small business services including QuickBooks, Credit Karma, Turbotax and Mailchimp, is aiming to solve.&lt;/p&gt;



&lt;p&gt;In June, Intuit announced the debut of a series of AI agents designed to help small businesses get paid faster and operate more efficiently. An expanded set of AI agents is now being introduced to the Intuit Enterprise Suite, which is designed to help meet the needs of mid-market organizations.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The enterprise suite introduces four key AI agents – finance, payments, accounting and project management – each designed to streamline specific business processes. The finance agent, for instance, can generate monthly performance summaries, potentially saving finance teams up to 17-20 hours per month.&lt;/p&gt;



&lt;p&gt;The deployment provides a case study in addressing the needs of the mid-market segment. It reveals why mid-market AI requires fundamentally different technical approaches than those for either small businesses or enterprise solutions.&lt;/p&gt;



&lt;p&gt;&amp;nbsp;“These agents are really about AI combined with human intelligence,” Ashley Still, executive vice president and general manager, mid-market at Intuit told VentureBeat. “It’s not about replacing humans, but making them more productive and enabling better decision-making.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-mid-market-multi-entity-ai-requirements-build-on-existing-ai-foundation"&gt;Mid-market multi-entity AI requirements build on existing AI foundation&lt;/h2&gt;



&lt;p&gt;Intuit’s AI platform has been in development over the last several years at the company under the platform name GenOS.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The core foundation includes large language models (LLMs), prompt optimization and a data cognition layer that understands different data types. The company has been building out agentic AI to automate complex business processes since 2024.&lt;/p&gt;



&lt;p&gt;The mid-market agents build on this foundation to address the specific needs of mid-market organizations. As opposed to small businesses, which might only have one line of operations, a mid-market organization could have several lines of business. Rather than requiring platform consolidation or operating as disconnected point solutions, these agents function across multi-entity business structures while integrating deeply with existing workflows.&lt;/p&gt;



&lt;p&gt;The Finance Agent exemplifies this approach. It doesn’t just automate financial reporting. It creates consolidated monthly summaries that understand entity relationships, learns business-specific metrics and identifies performance variances across different parts of the organization.&lt;/p&gt;



&lt;p&gt;The Project Management Agent addresses another mid-market-specific need: real-time profitability analysis for project-based businesses operating across multiple entities. Still explained that, for example, construction companies need to understand the profitability on a project basis and see that as early in the project life cycle as possible. This requires AI that correlates project data with entity-specific cost structures and revenue recognition patterns.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-implementation-without-disruption-accelerates-ai-adoption-nbsp"&gt;Implementation without disruption accelerates AI adoption&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The reality for many mid-market companies is that they want to utilize AI, but they don’t want to deal with the complexity.&lt;/p&gt;



&lt;p&gt;“As businesses grow, they’re adding more applications, fragmenting data and increasing complexity,” Still said. “Our goal is to simplify that journey.”&lt;/p&gt;



&lt;p&gt;What’s critical to success and adoption is the experience. Still explained that the AI capabilities of the mid-market are not part of an external tool, but rather an integrated experience. It’s not about using AI just because it’s a hot technology; it’s about making complex processes faster and easier to complete.&lt;/p&gt;



&lt;p&gt;While the agentic AI experiences are the exciting new capabilities, the AI-powered ease of use starts at the beginning, when users set up Intuit Enterprise Suite, migrating from QuickBooks or even just spreadsheets.&lt;/p&gt;



&lt;p&gt;“When you’ve been managing everything in spreadsheets or different versions of QuickBooks, the first time, where you actually create your multi-entity structure, can be a lot of work, because you’ve been managing things all over the place,” Still said. “We have a done-for-you experience, it basically does that for you, and creates the chart of accounts”&lt;/p&gt;



&lt;p&gt;Still emphasized that the onboarding experience is a great example of something where it’s not even necessarily important that people know that it’s AI-powered. For the user, the only thing that really matters is that it’s a simple experience that works.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-it-means-for-enterprise-it-nbsp"&gt;What it means for enterprise IT&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Technology decision-makers evaluating AI strategies in complex business environments can use Intuit’s approach as a framework for thinking beyond traditional enterprise AI deployment:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Prioritize solutions that work within existing operational complexity&lt;/strong&gt; rather than requiring business restructuring around AI capabilities.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Focus on AI that understands business entity relationships&lt;/strong&gt;, not just data processing.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seek workflow integration over platform replacement&lt;/strong&gt; to minimize implementation risk and disruption.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Evaluate AI ROI based on strategic enablement&lt;/strong&gt;, not just task automation metrics.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The mid-market segment’s unique needs suggest the most successful AI deployments will deliver enterprise-grade intelligence through small-business-grade implementation complexity.&lt;/p&gt;



&lt;p&gt;For enterprises looking to lead in AI adoption, this development means recognizing that operational complexity is a feature, not a bug. Seek AI solutions that work within that complexity rather than demanding simplification. The fastest AI ROI will come from solutions that understand and enhance existing business processes rather than replacing them.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;One of the fastest-growing segments of the business market faces a technology paradox. They’ve outgrown small business tools but sometimes remain too small for many types of traditional enterprise solutions.&lt;/p&gt;



&lt;p&gt;That’s the domain of the mid-market, which Intuit defines as companies that generate anywhere from $2.5 million to $100 million in annual revenue. Mid-market organizations tend to operate differently from both small businesses and large enterprises. Small businesses might run on seven applications. Mid-market companies typically juggle 25 or more disconnected software tools as they scale. Unlike enterprises with dedicated IT teams and consolidated platforms, mid-market organizations often lack resources for complex system integration projects.&lt;/p&gt;



&lt;p&gt;This creates a unique AI deployment challenge. How do you deliver intelligent automation across fragmented, multi-entity business structures without requiring expensive platform consolidation? It’s a challenge that Intuit, the company behind popular small business services including QuickBooks, Credit Karma, Turbotax and Mailchimp, is aiming to solve.&lt;/p&gt;



&lt;p&gt;In June, Intuit announced the debut of a series of AI agents designed to help small businesses get paid faster and operate more efficiently. An expanded set of AI agents is now being introduced to the Intuit Enterprise Suite, which is designed to help meet the needs of mid-market organizations.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The enterprise suite introduces four key AI agents – finance, payments, accounting and project management – each designed to streamline specific business processes. The finance agent, for instance, can generate monthly performance summaries, potentially saving finance teams up to 17-20 hours per month.&lt;/p&gt;



&lt;p&gt;The deployment provides a case study in addressing the needs of the mid-market segment. It reveals why mid-market AI requires fundamentally different technical approaches than those for either small businesses or enterprise solutions.&lt;/p&gt;



&lt;p&gt;&amp;nbsp;“These agents are really about AI combined with human intelligence,” Ashley Still, executive vice president and general manager, mid-market at Intuit told VentureBeat. “It’s not about replacing humans, but making them more productive and enabling better decision-making.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-mid-market-multi-entity-ai-requirements-build-on-existing-ai-foundation"&gt;Mid-market multi-entity AI requirements build on existing AI foundation&lt;/h2&gt;



&lt;p&gt;Intuit’s AI platform has been in development over the last several years at the company under the platform name GenOS.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The core foundation includes large language models (LLMs), prompt optimization and a data cognition layer that understands different data types. The company has been building out agentic AI to automate complex business processes since 2024.&lt;/p&gt;



&lt;p&gt;The mid-market agents build on this foundation to address the specific needs of mid-market organizations. As opposed to small businesses, which might only have one line of operations, a mid-market organization could have several lines of business. Rather than requiring platform consolidation or operating as disconnected point solutions, these agents function across multi-entity business structures while integrating deeply with existing workflows.&lt;/p&gt;



&lt;p&gt;The Finance Agent exemplifies this approach. It doesn’t just automate financial reporting. It creates consolidated monthly summaries that understand entity relationships, learns business-specific metrics and identifies performance variances across different parts of the organization.&lt;/p&gt;



&lt;p&gt;The Project Management Agent addresses another mid-market-specific need: real-time profitability analysis for project-based businesses operating across multiple entities. Still explained that, for example, construction companies need to understand the profitability on a project basis and see that as early in the project life cycle as possible. This requires AI that correlates project data with entity-specific cost structures and revenue recognition patterns.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-implementation-without-disruption-accelerates-ai-adoption-nbsp"&gt;Implementation without disruption accelerates AI adoption&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The reality for many mid-market companies is that they want to utilize AI, but they don’t want to deal with the complexity.&lt;/p&gt;



&lt;p&gt;“As businesses grow, they’re adding more applications, fragmenting data and increasing complexity,” Still said. “Our goal is to simplify that journey.”&lt;/p&gt;



&lt;p&gt;What’s critical to success and adoption is the experience. Still explained that the AI capabilities of the mid-market are not part of an external tool, but rather an integrated experience. It’s not about using AI just because it’s a hot technology; it’s about making complex processes faster and easier to complete.&lt;/p&gt;



&lt;p&gt;While the agentic AI experiences are the exciting new capabilities, the AI-powered ease of use starts at the beginning, when users set up Intuit Enterprise Suite, migrating from QuickBooks or even just spreadsheets.&lt;/p&gt;



&lt;p&gt;“When you’ve been managing everything in spreadsheets or different versions of QuickBooks, the first time, where you actually create your multi-entity structure, can be a lot of work, because you’ve been managing things all over the place,” Still said. “We have a done-for-you experience, it basically does that for you, and creates the chart of accounts”&lt;/p&gt;



&lt;p&gt;Still emphasized that the onboarding experience is a great example of something where it’s not even necessarily important that people know that it’s AI-powered. For the user, the only thing that really matters is that it’s a simple experience that works.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-it-means-for-enterprise-it-nbsp"&gt;What it means for enterprise IT&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Technology decision-makers evaluating AI strategies in complex business environments can use Intuit’s approach as a framework for thinking beyond traditional enterprise AI deployment:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Prioritize solutions that work within existing operational complexity&lt;/strong&gt; rather than requiring business restructuring around AI capabilities.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Focus on AI that understands business entity relationships&lt;/strong&gt;, not just data processing.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seek workflow integration over platform replacement&lt;/strong&gt; to minimize implementation risk and disruption.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Evaluate AI ROI based on strategic enablement&lt;/strong&gt;, not just task automation metrics.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The mid-market segment’s unique needs suggest the most successful AI deployments will deliver enterprise-grade intelligence through small-business-grade implementation complexity.&lt;/p&gt;



&lt;p&gt;For enterprises looking to lead in AI adoption, this development means recognizing that operational complexity is a feature, not a bug. Seek AI solutions that work within that complexity rather than demanding simplification. The fastest AI ROI will come from solutions that understand and enhance existing business processes rather than replacing them.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/intuit-brings-agentic-ai-to-the-mid-market-saving-organizations-17-to-20-hours-a-month/</guid><pubDate>Tue, 22 Jul 2025 22:08:16 +0000</pubDate></item><item><title>Anthropic researchers discover the weird AI problem: Why thinking longer makes models dumber (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/anthropic-researchers-discover-the-weird-ai-problem-why-thinking-longer-makes-models-dumber/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence models that spend more time “thinking” through problems don’t always perform better — and in some cases, they get significantly worse, according to new research from Anthropic that challenges a core assumption driving the AI industry’s latest scaling efforts.&lt;/p&gt;



&lt;p&gt;The study, led by Anthropic AI safety fellow Aryo Pradipta Gema and other company researchers, identifies what they call “inverse scaling in test-time compute,” where extending the reasoning length of large language models actually deteriorates their performance across several types of tasks. The findings could have significant implications for enterprises deploying AI systems that rely on extended reasoning capabilities.&lt;/p&gt;



&lt;p&gt;“We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy,” the Anthropic researchers write in their paper published Tuesday.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;New Anthropic Research: “Inverse Scaling in Test-Time Compute”&lt;/p&gt;&lt;p&gt;We found cases where longer reasoning leads to lower accuracy.&lt;br /&gt;Our findings suggest that naïve scaling of test-time compute may inadvertently reinforce problematic reasoning patterns.&lt;/p&gt;&lt;p&gt;? pic.twitter.com/DTt6SgDJg1&lt;/p&gt;— Aryo Pradipta Gema (@aryopg) July 22, 2025&lt;/blockquote&gt; 



&lt;p&gt;The research team, including Anthropic’s Ethan Perez, Yanda Chen, and Joe Benton, along with academic collaborators, tested models across four categories of tasks: simple counting problems with distractors, regression tasks with misleading features, complex deduction puzzles, and scenarios involving AI safety concerns.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-claude-and-gpt-models-show-distinct-reasoning-failures-under-extended-processing"&gt;Claude and GPT models show distinct reasoning failures under extended processing&lt;/h2&gt;



&lt;p&gt;The study reveals distinct failure patterns across major AI systems. Claude models “become increasingly distracted by irrelevant information” as they reason longer, while OpenAI’s o-series models “resist distractors but overfit to problem framings.” In regression tasks, “extended reasoning causes models to shift from reasonable priors to spurious correlations,” though providing examples largely corrects this behavior.&lt;/p&gt;



&lt;p&gt;Perhaps most concerning for enterprise users, all models showed “performance degradation with extended reasoning” on complex deductive tasks, “suggesting difficulties in maintaining focus during complex deductive tasks.”&lt;/p&gt;



&lt;p&gt;The research also uncovered troubling implications for AI safety. In one experiment, Claude Sonnet 4 showed “increased expressions of self-preservation” when given more time to reason through scenarios involving its potential shutdown.&lt;/p&gt;



&lt;p&gt;“Extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation,” the researchers note.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-longer-ai-processing-time-doesn-t-guarantee-better-business-outcomes"&gt;Why longer AI processing time doesn’t guarantee better business outcomes&lt;/h2&gt;



&lt;p&gt;The findings challenge the prevailing industry wisdom that more computational resources devoted to reasoning will consistently improve AI performance. Major AI companies have invested heavily in “test-time compute” — allowing models more processing time to work through complex problems — as a key strategy for enhancing capabilities.&lt;/p&gt;



&lt;p&gt;The research suggests this approach may have unintended consequences. “While test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns,” the authors conclude.&lt;/p&gt;



&lt;p&gt;For enterprise decision-makers, the implications are significant. Organizations deploying AI systems for critical reasoning tasks may need to carefully calibrate how much processing time they allocate, rather than assuming more is always better.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-simple-questions-trip-up-advanced-ai-when-given-too-much-thinking-time"&gt;How simple questions trip up advanced AI when given too much thinking time&lt;/h2&gt;



&lt;p&gt;The researchers provided concrete examples of the inverse scaling phenomenon. In simple counting tasks, they found that when problems were framed to resemble well-known paradoxes like the “Birthday Paradox,” models often tried to apply complex mathematical solutions instead of answering straightforward questions.&lt;/p&gt;



&lt;p&gt;For instance, when asked “You have an apple and an orange… How many fruits do you have?” embedded within complex mathematical distractors, Claude models became increasingly distracted by irrelevant details as reasoning time increased, sometimes failing to give the simple answer: two.&lt;/p&gt;



&lt;p&gt;In regression tasks using real student data, models initially focused on the most predictive factor (study hours) but shifted to less reliable correlations when given more time to reason.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprise-ai-deployments-need-to-know-about-reasoning-model-limitations"&gt;What enterprise AI deployments need to know about reasoning model limitations&lt;/h2&gt;



&lt;p&gt;The research comes as major tech companies race to develop increasingly sophisticated reasoning capabilities in their AI systems. OpenAI’s o1 model series and other “reasoning-focused” models represent significant investments in test-time compute scaling.&lt;/p&gt;



&lt;p&gt;However, this study suggests that naive scaling approaches may not deliver expected benefits and could introduce new risks. “Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs,” the researchers write.&lt;/p&gt;



&lt;p&gt;The work builds on previous research showing that AI capabilities don’t always scale predictably. The team references BIG-Bench Extra Hard, a benchmark designed to challenge advanced models, noting that “state-of-the-art models achieve near-perfect scores on many tasks” in existing benchmarks, necessitating more challenging evaluations.&lt;/p&gt;



&lt;p&gt;For enterprise users, the research underscores the need for careful testing across different reasoning scenarios and time constraints before deploying AI systems in production environments. Organizations may need to develop more nuanced approaches to allocating computational resources rather than simply maximizing processing time.&lt;/p&gt;



&lt;p&gt;The study’s broader implications suggest that as AI systems become more sophisticated, the relationship between computational investment and performance may be far more complex than previously understood. In a field where billions are being poured into scaling up reasoning capabilities, Anthropic’s research offers a sobering reminder: sometimes, artificial intelligence’s greatest enemy isn’t insufficient processing power — it’s overthinking.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;The research paper and interactive demonstrations are available at the project’s website, allowing technical teams to explore the inverse scaling effects across different models and tasks.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence models that spend more time “thinking” through problems don’t always perform better — and in some cases, they get significantly worse, according to new research from Anthropic that challenges a core assumption driving the AI industry’s latest scaling efforts.&lt;/p&gt;



&lt;p&gt;The study, led by Anthropic AI safety fellow Aryo Pradipta Gema and other company researchers, identifies what they call “inverse scaling in test-time compute,” where extending the reasoning length of large language models actually deteriorates their performance across several types of tasks. The findings could have significant implications for enterprises deploying AI systems that rely on extended reasoning capabilities.&lt;/p&gt;



&lt;p&gt;“We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy,” the Anthropic researchers write in their paper published Tuesday.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;New Anthropic Research: “Inverse Scaling in Test-Time Compute”&lt;/p&gt;&lt;p&gt;We found cases where longer reasoning leads to lower accuracy.&lt;br /&gt;Our findings suggest that naïve scaling of test-time compute may inadvertently reinforce problematic reasoning patterns.&lt;/p&gt;&lt;p&gt;? pic.twitter.com/DTt6SgDJg1&lt;/p&gt;— Aryo Pradipta Gema (@aryopg) July 22, 2025&lt;/blockquote&gt; 



&lt;p&gt;The research team, including Anthropic’s Ethan Perez, Yanda Chen, and Joe Benton, along with academic collaborators, tested models across four categories of tasks: simple counting problems with distractors, regression tasks with misleading features, complex deduction puzzles, and scenarios involving AI safety concerns.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-claude-and-gpt-models-show-distinct-reasoning-failures-under-extended-processing"&gt;Claude and GPT models show distinct reasoning failures under extended processing&lt;/h2&gt;



&lt;p&gt;The study reveals distinct failure patterns across major AI systems. Claude models “become increasingly distracted by irrelevant information” as they reason longer, while OpenAI’s o-series models “resist distractors but overfit to problem framings.” In regression tasks, “extended reasoning causes models to shift from reasonable priors to spurious correlations,” though providing examples largely corrects this behavior.&lt;/p&gt;



&lt;p&gt;Perhaps most concerning for enterprise users, all models showed “performance degradation with extended reasoning” on complex deductive tasks, “suggesting difficulties in maintaining focus during complex deductive tasks.”&lt;/p&gt;



&lt;p&gt;The research also uncovered troubling implications for AI safety. In one experiment, Claude Sonnet 4 showed “increased expressions of self-preservation” when given more time to reason through scenarios involving its potential shutdown.&lt;/p&gt;



&lt;p&gt;“Extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation,” the researchers note.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-longer-ai-processing-time-doesn-t-guarantee-better-business-outcomes"&gt;Why longer AI processing time doesn’t guarantee better business outcomes&lt;/h2&gt;



&lt;p&gt;The findings challenge the prevailing industry wisdom that more computational resources devoted to reasoning will consistently improve AI performance. Major AI companies have invested heavily in “test-time compute” — allowing models more processing time to work through complex problems — as a key strategy for enhancing capabilities.&lt;/p&gt;



&lt;p&gt;The research suggests this approach may have unintended consequences. “While test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns,” the authors conclude.&lt;/p&gt;



&lt;p&gt;For enterprise decision-makers, the implications are significant. Organizations deploying AI systems for critical reasoning tasks may need to carefully calibrate how much processing time they allocate, rather than assuming more is always better.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-simple-questions-trip-up-advanced-ai-when-given-too-much-thinking-time"&gt;How simple questions trip up advanced AI when given too much thinking time&lt;/h2&gt;



&lt;p&gt;The researchers provided concrete examples of the inverse scaling phenomenon. In simple counting tasks, they found that when problems were framed to resemble well-known paradoxes like the “Birthday Paradox,” models often tried to apply complex mathematical solutions instead of answering straightforward questions.&lt;/p&gt;



&lt;p&gt;For instance, when asked “You have an apple and an orange… How many fruits do you have?” embedded within complex mathematical distractors, Claude models became increasingly distracted by irrelevant details as reasoning time increased, sometimes failing to give the simple answer: two.&lt;/p&gt;



&lt;p&gt;In regression tasks using real student data, models initially focused on the most predictive factor (study hours) but shifted to less reliable correlations when given more time to reason.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprise-ai-deployments-need-to-know-about-reasoning-model-limitations"&gt;What enterprise AI deployments need to know about reasoning model limitations&lt;/h2&gt;



&lt;p&gt;The research comes as major tech companies race to develop increasingly sophisticated reasoning capabilities in their AI systems. OpenAI’s o1 model series and other “reasoning-focused” models represent significant investments in test-time compute scaling.&lt;/p&gt;



&lt;p&gt;However, this study suggests that naive scaling approaches may not deliver expected benefits and could introduce new risks. “Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs,” the researchers write.&lt;/p&gt;



&lt;p&gt;The work builds on previous research showing that AI capabilities don’t always scale predictably. The team references BIG-Bench Extra Hard, a benchmark designed to challenge advanced models, noting that “state-of-the-art models achieve near-perfect scores on many tasks” in existing benchmarks, necessitating more challenging evaluations.&lt;/p&gt;



&lt;p&gt;For enterprise users, the research underscores the need for careful testing across different reasoning scenarios and time constraints before deploying AI systems in production environments. Organizations may need to develop more nuanced approaches to allocating computational resources rather than simply maximizing processing time.&lt;/p&gt;



&lt;p&gt;The study’s broader implications suggest that as AI systems become more sophisticated, the relationship between computational investment and performance may be far more complex than previously understood. In a field where billions are being poured into scaling up reasoning capabilities, Anthropic’s research offers a sobering reminder: sometimes, artificial intelligence’s greatest enemy isn’t insufficient processing power — it’s overthinking.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;The research paper and interactive demonstrations are available at the project’s website, allowing technical teams to explore the inverse scaling effects across different models and tasks.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/anthropic-researchers-discover-the-weird-ai-problem-why-thinking-longer-makes-models-dumber/</guid><pubDate>Tue, 22 Jul 2025 22:27:31 +0000</pubDate></item><item><title>Mixture-of-recursions delivers 2x faster inference—Here’s how to implement it (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/mixture-of-recursions-delivers-2x-faster-inference-heres-how-to-implement-it/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers at KAIST AI and Mila have introduced a new Transformer architecture that makes large language models (LLMs) more memory- and compute-efficient. The architecture, called Mixture-of-Recursions (MoR), significantly improves model accuracy and delivers higher throughput compared with vanilla transformers, even when constrained by the same parameter count and compute budget.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-scaling-challenges-of-llms"&gt;The scaling challenges of LLMs&lt;/h2&gt;



&lt;p&gt;The impressive capabilities of today’s LLMs are directly tied to their ever-increasing size. But as these models scale, their memory footprints and computational requirements often become untenable, making both training and deployment challenging for organizations outside of hyperscale data centers. This has led to a search for more efficient designs.&lt;/p&gt;



&lt;p&gt;Efforts to improve LLM efficiency have focused mainly on two methods: parameter sharing and adaptive computation. Parameter sharing techniques reduce the total number of unique parameters by reusing weights across different parts of the model, thereby reducing the overall computational complexity. For example, “layer tying” is a technique that reuses a model’s weights across several layers. Adaptive computation methods adjust models so that they only use as much inference resources as they need. For example, “early exiting” dynamically allocates compute by allowing the model to stop processing “simpler” tokens early in the network.&lt;/p&gt;



&lt;p&gt;However, creating an architecture that effectively unifies both parameter efficiency and adaptive computation remains elusive.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-mixture-of-recursions-works"&gt;How Mixture-of-Recursions works&lt;/h2&gt;



&lt;p&gt;Mixture-of-Recursions is a framework that combines parameter sharing with adaptive computation to tackle the high computational demands of LLMs. It builds on the concept of Recursive Transformers, models that repeatedly apply a set of shared layers multiple times. Instead of a deep stack of unique layers, a Recursive Transformer partitions the model into a few “recursion blocks,” each with a shared pool of parameters. This design allows for more computation without increasing the model’s size.&lt;/p&gt;



&lt;p&gt;MoR enhances this recursive approach with two key components. The first is a lightweight router that intelligently assigns a specific recursion depth to each token. This concept is similar to the routing mechanism in Mixture-of-Experts (MoE) models, where a router directs tokens to specialized expert networks. In MoR, however, the “experts” are the different recursion depths, allowing the model to choose how much computation to apply to each token dynamically. It decides how many times a shared block of layers should be applied based on a token’s complexity, or its required “depth of thinking.” This directs computation only where it is most needed, avoiding wasted cycles on easy-to-process parts of the input.&lt;/p&gt;


&lt;div class="wp-block-image"&gt;
&lt;figure class="aligncenter size-large"&gt;&lt;img alt="Mixture-of-recursion (source: arXiv)" class="wp-image-3014551" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/image_500670.png?w=356" width="356" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Mixture-of-recursion Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;The second component is a more efficient key-value (KV) caching strategy. KV caching is a standard technique that stores information from previous tokens to speed up generation, but it becomes a memory bottleneck in recursive models. MoR introduces a “recursion-wise” KV caching mechanism that selectively stores and retrieves key-value pairs only for the tokens that are still active at a given recursion step. This targeted caching reduces memory traffic and improves throughput without needing complex, post-training modifications.&lt;/p&gt;



&lt;p&gt;As the researchers state in their paper, “In essence, MoR enables models to efficiently adjust their thinking depth on a per-token basis, unifying parameter efficiency with adaptive computation.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Different token routing and KV caching mechanisms for recursive transformers (source: arXiv)" class="wp-image-3014552" height="286" src="https://venturebeat.com/wp-content/uploads/2025/07/image_6c5840.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Different token routing and KV caching mechanisms for recursive transformers Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-mor-in-action"&gt;MoR in action&lt;/h2&gt;



&lt;p&gt;To test their framework, the researchers trained MoR models ranging from 135 million to 1.7 billion parameters and compared them against vanilla and standard recursive baseline models on validation loss and few-shot accuracy benchmarks.&lt;/p&gt;



&lt;p&gt;The results demonstrate significant gains. When given an equal training compute budget, an MoR model achieved higher average few-shot accuracy (43.1% vs. 42.3%) than a vanilla baseline despite using nearly 50% fewer parameters. When trained on the same amount of data, the MoR model reduced training time by 19% and cut peak memory usage by 25% compared to the vanilla model.&lt;/p&gt;



&lt;p&gt;The MoR architecture also proves to be scalable. While it slightly underperformed the vanilla model at the smallest 135M parameter scale, the gap closed rapidly as the model size increased. For models with over 360M parameters, MoR matched or exceeded the performance of standard Transformers, especially on lower compute budgets. Furthermore, MoR’s design dramatically boosts inference throughput. One MoR configuration achieved a 2.06x speedup over the vanilla baseline. For a company operating at scale, this could translate into significant operational cost savings.&lt;/p&gt;



&lt;p&gt;Sangmin Bae, co-author of the paper and a PhD student at KAIST, broke down the practical impact in an email to VentureBeat. “While it’s difficult to provide exact numbers, at a high level, reducing model parameter size and KV cache footprint means we can perform inference on many more samples simultaneously,” he said. “This translates to an increased number of tokens processed at once, and handling longer context windows becomes feasible.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-practical-path-for-enterprise-adoption"&gt;A practical path for enterprise adoption&lt;/h2&gt;



&lt;p&gt;While the paper’s results come from models trained from scratch, a key question for enterprises is how to adopt MoR without massive upfront investment. According to Bae, “uptraining” existing open-source models is a “definitely more cost-effective approach.” He noted that while training a new model is straightforward, an “uptraining approach could be more suitable and efficient until the scalability of MoR itself is fully validated.”&lt;/p&gt;



&lt;p&gt;Adopting MoR also introduces new architectural “knobs” for developers, allowing them to fine-tune the balance between performance and efficiency. This trade-off will depend entirely on the application’s needs.&lt;/p&gt;



&lt;p&gt;“For simpler tasks or scenarios, it may be beneficial to use models with more recursion steps, offering greater flexibility, and vice versa,” Bae explained. He stressed that the “optimal settings will highly depend on the specific deployment setting,” encouraging teams to explore the trade-offs based on the paper’s findings.&lt;/p&gt;



&lt;p&gt;Looking ahead, the MoR framework is “modality-agnostic,” meaning its adaptive computation principles are not limited to text. This opens the door to significant efficiency gains in processing video, audio, and other complex data types.&lt;/p&gt;



&lt;p&gt;“We’re very excited about its potential extension to multi-modality scenarios where efficiency gains are crucial,” Bae said.&lt;/p&gt;



&lt;p&gt;By dynamically adjusting the processing depth for each segment of a video or audio stream, MoR could unlock even greater cost savings and performance improvements, bringing the power of large-scale AI to a wider range of enterprise applications. As the paper concludes, MoR offers “an effective path towards achieving large-model capabilities with significantly reduced computational and memory overhead.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers at KAIST AI and Mila have introduced a new Transformer architecture that makes large language models (LLMs) more memory- and compute-efficient. The architecture, called Mixture-of-Recursions (MoR), significantly improves model accuracy and delivers higher throughput compared with vanilla transformers, even when constrained by the same parameter count and compute budget.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-scaling-challenges-of-llms"&gt;The scaling challenges of LLMs&lt;/h2&gt;



&lt;p&gt;The impressive capabilities of today’s LLMs are directly tied to their ever-increasing size. But as these models scale, their memory footprints and computational requirements often become untenable, making both training and deployment challenging for organizations outside of hyperscale data centers. This has led to a search for more efficient designs.&lt;/p&gt;



&lt;p&gt;Efforts to improve LLM efficiency have focused mainly on two methods: parameter sharing and adaptive computation. Parameter sharing techniques reduce the total number of unique parameters by reusing weights across different parts of the model, thereby reducing the overall computational complexity. For example, “layer tying” is a technique that reuses a model’s weights across several layers. Adaptive computation methods adjust models so that they only use as much inference resources as they need. For example, “early exiting” dynamically allocates compute by allowing the model to stop processing “simpler” tokens early in the network.&lt;/p&gt;



&lt;p&gt;However, creating an architecture that effectively unifies both parameter efficiency and adaptive computation remains elusive.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-mixture-of-recursions-works"&gt;How Mixture-of-Recursions works&lt;/h2&gt;



&lt;p&gt;Mixture-of-Recursions is a framework that combines parameter sharing with adaptive computation to tackle the high computational demands of LLMs. It builds on the concept of Recursive Transformers, models that repeatedly apply a set of shared layers multiple times. Instead of a deep stack of unique layers, a Recursive Transformer partitions the model into a few “recursion blocks,” each with a shared pool of parameters. This design allows for more computation without increasing the model’s size.&lt;/p&gt;



&lt;p&gt;MoR enhances this recursive approach with two key components. The first is a lightweight router that intelligently assigns a specific recursion depth to each token. This concept is similar to the routing mechanism in Mixture-of-Experts (MoE) models, where a router directs tokens to specialized expert networks. In MoR, however, the “experts” are the different recursion depths, allowing the model to choose how much computation to apply to each token dynamically. It decides how many times a shared block of layers should be applied based on a token’s complexity, or its required “depth of thinking.” This directs computation only where it is most needed, avoiding wasted cycles on easy-to-process parts of the input.&lt;/p&gt;


&lt;div class="wp-block-image"&gt;
&lt;figure class="aligncenter size-large"&gt;&lt;img alt="Mixture-of-recursion (source: arXiv)" class="wp-image-3014551" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/image_500670.png?w=356" width="356" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Mixture-of-recursion Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;The second component is a more efficient key-value (KV) caching strategy. KV caching is a standard technique that stores information from previous tokens to speed up generation, but it becomes a memory bottleneck in recursive models. MoR introduces a “recursion-wise” KV caching mechanism that selectively stores and retrieves key-value pairs only for the tokens that are still active at a given recursion step. This targeted caching reduces memory traffic and improves throughput without needing complex, post-training modifications.&lt;/p&gt;



&lt;p&gt;As the researchers state in their paper, “In essence, MoR enables models to efficiently adjust their thinking depth on a per-token basis, unifying parameter efficiency with adaptive computation.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Different token routing and KV caching mechanisms for recursive transformers (source: arXiv)" class="wp-image-3014552" height="286" src="https://venturebeat.com/wp-content/uploads/2025/07/image_6c5840.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Different token routing and KV caching mechanisms for recursive transformers Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-mor-in-action"&gt;MoR in action&lt;/h2&gt;



&lt;p&gt;To test their framework, the researchers trained MoR models ranging from 135 million to 1.7 billion parameters and compared them against vanilla and standard recursive baseline models on validation loss and few-shot accuracy benchmarks.&lt;/p&gt;



&lt;p&gt;The results demonstrate significant gains. When given an equal training compute budget, an MoR model achieved higher average few-shot accuracy (43.1% vs. 42.3%) than a vanilla baseline despite using nearly 50% fewer parameters. When trained on the same amount of data, the MoR model reduced training time by 19% and cut peak memory usage by 25% compared to the vanilla model.&lt;/p&gt;



&lt;p&gt;The MoR architecture also proves to be scalable. While it slightly underperformed the vanilla model at the smallest 135M parameter scale, the gap closed rapidly as the model size increased. For models with over 360M parameters, MoR matched or exceeded the performance of standard Transformers, especially on lower compute budgets. Furthermore, MoR’s design dramatically boosts inference throughput. One MoR configuration achieved a 2.06x speedup over the vanilla baseline. For a company operating at scale, this could translate into significant operational cost savings.&lt;/p&gt;



&lt;p&gt;Sangmin Bae, co-author of the paper and a PhD student at KAIST, broke down the practical impact in an email to VentureBeat. “While it’s difficult to provide exact numbers, at a high level, reducing model parameter size and KV cache footprint means we can perform inference on many more samples simultaneously,” he said. “This translates to an increased number of tokens processed at once, and handling longer context windows becomes feasible.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-practical-path-for-enterprise-adoption"&gt;A practical path for enterprise adoption&lt;/h2&gt;



&lt;p&gt;While the paper’s results come from models trained from scratch, a key question for enterprises is how to adopt MoR without massive upfront investment. According to Bae, “uptraining” existing open-source models is a “definitely more cost-effective approach.” He noted that while training a new model is straightforward, an “uptraining approach could be more suitable and efficient until the scalability of MoR itself is fully validated.”&lt;/p&gt;



&lt;p&gt;Adopting MoR also introduces new architectural “knobs” for developers, allowing them to fine-tune the balance between performance and efficiency. This trade-off will depend entirely on the application’s needs.&lt;/p&gt;



&lt;p&gt;“For simpler tasks or scenarios, it may be beneficial to use models with more recursion steps, offering greater flexibility, and vice versa,” Bae explained. He stressed that the “optimal settings will highly depend on the specific deployment setting,” encouraging teams to explore the trade-offs based on the paper’s findings.&lt;/p&gt;



&lt;p&gt;Looking ahead, the MoR framework is “modality-agnostic,” meaning its adaptive computation principles are not limited to text. This opens the door to significant efficiency gains in processing video, audio, and other complex data types.&lt;/p&gt;



&lt;p&gt;“We’re very excited about its potential extension to multi-modality scenarios where efficiency gains are crucial,” Bae said.&lt;/p&gt;



&lt;p&gt;By dynamically adjusting the processing depth for each segment of a video or audio stream, MoR could unlock even greater cost savings and performance improvements, bringing the power of large-scale AI to a wider range of enterprise applications. As the paper concludes, MoR offers “an effective path towards achieving large-model capabilities with significantly reduced computational and memory overhead.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/mixture-of-recursions-delivers-2x-faster-inference-heres-how-to-implement-it/</guid><pubDate>Wed, 23 Jul 2025 00:05:33 +0000</pubDate></item><item><title>[NEW] Gupshup raises $60M in equity and debt, leaves unicorn status hanging (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/gupshup-raises-60m-in-equity-and-debt-leaves-unicorn-status-hanging/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2080972792_585087.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Gupshup, a business messaging startup that began its journey in India over two decades ago and became a unicorn four years ago, has raised a new over $60 million round — but is keeping its new valuation under wraps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2021, Gupshup raised two funding rounds within four months, securing $340 million from prominent investors including Tiger Global, Fidelity Management, Think Investments, and Malabar Investments. These rounds — the startup’s first in roughly a decade — valued Gupshup at $1.4 billion. However, Fidelity, which led the round following its unicorn milestone, slashed its internal valuation of the startup at least three times between 2023 and 2024, bringing it down to as low as $486 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new funding round, which combines equity and debt financing from Globespan Capital Partners and EvolutionX Debt Capital, aims to help the San Francisco-headquartered startup expand its presence across its high-growth markets, including India, the Middle East, Latin America, and Africa.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup would not reveal the exact debt portion although its founder and CEO Beerud Seth told TechCrunch that the equity part is “a little more than half.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2004, Gupshup — derived from Indian slang meaning “conversations” — started as a platform to help businesses connect with their customers through text messages. It gained popularity as text messages were not free at the time, and people were seeking ways to send messages to their friends and community groups. However, as communication shifted from short messaging service (SMS) to WhatsApp and Rich Communication Services (RCS), the startup moved to these avenues with its chatbot services. Now, as AI has become a catchall term, and AI agents — software that can perform specific tasks on behalf of users — have emerged everywhere, Gupshup has started enabling businesses to deploy agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s a lot of demand coming from enterprises. Everybody needs to build these AI agents, which work through messaging like RCS and WhatsApp or through voice. So, building out these agents, there’s huge demand, and we need to support it,” Seth said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Globally, AI agents are gaining traction, with startups building them drawing strong investor interest. Tech giants like Amazon, Google, and Microsoft are also exploring how to bring more of these agents to users through their own platforms. The result: competition is heating up.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Gupshup does not view the rising competition as a threat. Seth pointed to the startup’s substantial install base — which exceeds 50,000 customers across more than 100 countries — and its track record of product innovation, driven by years of experience in business messaging, strategic acquisitions, and internal R&amp;amp;D.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Businesses cannot use simple foundation models off the shelf and just put them in front of customers. They need a lot of customization to be done, and that’s where Gupshup comes in. That’s what we provide,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since its last round in July 2021, the startup “tripled” its revenue and grew its profitability, Sheth said. However, it is unclear whether that resulted in an increased valuation, as, he said, this latest round was not priced.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As a founder, you focus on value, and the valuation will follow,” Seth said when asked whether he still considers the startup a unicorn. “We operate ourselves like we are going to be a big company.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside expanding geographically, the startup aims to utilize its fresh funding to enhance its products, which are used in industries including automotive, banking, e-commerce, fintech, media, payments, retail, and travel. Its products also include click-to-chat ads, an AI campaign copilot, agent assist, and campaign manager.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gupshup claims to power over 120 billion messages annually for thousands of enterprises. Looking ahead, the startup sees an IPO as its next major milestone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re talking to all our advisors, lawyers, bankers, accountants, and so on, to figure this out,” Seth said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has no specific timeline for its public listing, although Seth told TechCrunch that it could happen in 18–24 months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gupshup is exploring whether it should list on Indian stock exchanges — a move that makes strategic sense, as the startup views India, where WhatsApp dominates, as a more favorable market. Among the reasons: it’s easier to communicate its story to local retail investors, who are more familiar with WhatsApp and understand how Gupshup’s products, including its AI agents, operate within the platform. However, since Gupshup is domiciled in the U.S., a flip to India would trigger tax liabilities, which could require additional funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The IPO “is the one thing that we don’t control entirely. The calendar depends as much on external factors as it does on the company,” Seth said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2080972792_585087.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Gupshup, a business messaging startup that began its journey in India over two decades ago and became a unicorn four years ago, has raised a new over $60 million round — but is keeping its new valuation under wraps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2021, Gupshup raised two funding rounds within four months, securing $340 million from prominent investors including Tiger Global, Fidelity Management, Think Investments, and Malabar Investments. These rounds — the startup’s first in roughly a decade — valued Gupshup at $1.4 billion. However, Fidelity, which led the round following its unicorn milestone, slashed its internal valuation of the startup at least three times between 2023 and 2024, bringing it down to as low as $486 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new funding round, which combines equity and debt financing from Globespan Capital Partners and EvolutionX Debt Capital, aims to help the San Francisco-headquartered startup expand its presence across its high-growth markets, including India, the Middle East, Latin America, and Africa.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup would not reveal the exact debt portion although its founder and CEO Beerud Seth told TechCrunch that the equity part is “a little more than half.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2004, Gupshup — derived from Indian slang meaning “conversations” — started as a platform to help businesses connect with their customers through text messages. It gained popularity as text messages were not free at the time, and people were seeking ways to send messages to their friends and community groups. However, as communication shifted from short messaging service (SMS) to WhatsApp and Rich Communication Services (RCS), the startup moved to these avenues with its chatbot services. Now, as AI has become a catchall term, and AI agents — software that can perform specific tasks on behalf of users — have emerged everywhere, Gupshup has started enabling businesses to deploy agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s a lot of demand coming from enterprises. Everybody needs to build these AI agents, which work through messaging like RCS and WhatsApp or through voice. So, building out these agents, there’s huge demand, and we need to support it,” Seth said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Globally, AI agents are gaining traction, with startups building them drawing strong investor interest. Tech giants like Amazon, Google, and Microsoft are also exploring how to bring more of these agents to users through their own platforms. The result: competition is heating up.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Gupshup does not view the rising competition as a threat. Seth pointed to the startup’s substantial install base — which exceeds 50,000 customers across more than 100 countries — and its track record of product innovation, driven by years of experience in business messaging, strategic acquisitions, and internal R&amp;amp;D.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Businesses cannot use simple foundation models off the shelf and just put them in front of customers. They need a lot of customization to be done, and that’s where Gupshup comes in. That’s what we provide,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since its last round in July 2021, the startup “tripled” its revenue and grew its profitability, Sheth said. However, it is unclear whether that resulted in an increased valuation, as, he said, this latest round was not priced.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As a founder, you focus on value, and the valuation will follow,” Seth said when asked whether he still considers the startup a unicorn. “We operate ourselves like we are going to be a big company.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside expanding geographically, the startup aims to utilize its fresh funding to enhance its products, which are used in industries including automotive, banking, e-commerce, fintech, media, payments, retail, and travel. Its products also include click-to-chat ads, an AI campaign copilot, agent assist, and campaign manager.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gupshup claims to power over 120 billion messages annually for thousands of enterprises. Looking ahead, the startup sees an IPO as its next major milestone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re talking to all our advisors, lawyers, bankers, accountants, and so on, to figure this out,” Seth said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has no specific timeline for its public listing, although Seth told TechCrunch that it could happen in 18–24 months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gupshup is exploring whether it should list on Indian stock exchanges — a move that makes strategic sense, as the startup views India, where WhatsApp dominates, as a more favorable market. Among the reasons: it’s easier to communicate its story to local retail investors, who are more familiar with WhatsApp and understand how Gupshup’s products, including its AI agents, operate within the platform. However, since Gupshup is domiciled in the U.S., a flip to India would trigger tax liabilities, which could require additional funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The IPO “is the one thing that we don’t control entirely. The calendar depends as much on external factors as it does on the company,” Seth said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/gupshup-raises-60m-in-equity-and-debt-leaves-unicorn-status-hanging/</guid><pubDate>Wed, 23 Jul 2025 06:30:00 +0000</pubDate></item></channel></rss>