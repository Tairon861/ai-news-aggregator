<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 15 Aug 2025 06:34:07 +0000</lastBuildDate><item><title>Beyond billion-parameter burdens: Unlocking data synthesis with a conditional generator (The latest research from Google)</title><link>https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Experiments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;We conducted experiments on four datasets, where three datasets correspond with downstream generative tasks and one dataset with a classification task. Generative tasks are typically more challenging than classification tasks. This is because the generative tasks are evaluated by the next-token prediction accuracy, which requires the synthetic data to preserve fine-grained textual information from the private data. In contrast, the classification tasks only require maintaining the co-occurrence patterns between labels and words in the private data.&lt;/p&gt;&lt;p&gt;The three generative tasks are chosen to cover a diverse set of practical scenarios: PubMed (medical paper abstracts), Chatbot Arena (human-to-machine interactions), and Multi-Session Chat (human-to-human daily dialogues). To evaluate the quality of the generated synthetic data, we followed the setup of Aug-PE to train a small downstream language model on the synthetic data and then compute the next-token prediction accuracy on the real test data.&lt;/p&gt;&lt;p&gt;The classification task is performed on the OpenReview (academic paper reviews) dataset. To evaluate the quality of the generated synthetic data, we train a downstream classifier on the synthetic data, and compute the classification accuracy on the real test data.&lt;/p&gt;&lt;p&gt;To mitigate concerns regarding data contamination, we carefully analyzed our selected datasets. Our analysis showed no overlap between our pre-training data and the downstream datasets.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Experiments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;We conducted experiments on four datasets, where three datasets correspond with downstream generative tasks and one dataset with a classification task. Generative tasks are typically more challenging than classification tasks. This is because the generative tasks are evaluated by the next-token prediction accuracy, which requires the synthetic data to preserve fine-grained textual information from the private data. In contrast, the classification tasks only require maintaining the co-occurrence patterns between labels and words in the private data.&lt;/p&gt;&lt;p&gt;The three generative tasks are chosen to cover a diverse set of practical scenarios: PubMed (medical paper abstracts), Chatbot Arena (human-to-machine interactions), and Multi-Session Chat (human-to-human daily dialogues). To evaluate the quality of the generated synthetic data, we followed the setup of Aug-PE to train a small downstream language model on the synthetic data and then compute the next-token prediction accuracy on the real test data.&lt;/p&gt;&lt;p&gt;The classification task is performed on the OpenReview (academic paper reviews) dataset. To evaluate the quality of the generated synthetic data, we train a downstream classifier on the synthetic data, and compute the classification accuracy on the real test data.&lt;/p&gt;&lt;p&gt;To mitigate concerns regarding data contamination, we carefully analyzed our selected datasets. Our analysis showed no overlap between our pre-training data and the downstream datasets.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/</guid><pubDate>Thu, 14 Aug 2025 19:06:20 +0000</pubDate></item><item><title>Google releases pint-size Gemma open AI model (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/08/google-releases-pint-size-gemma-open-ai-model/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The new Gemma model is a fraction of the size of most new models.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="191" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Gemma3-270M_Wagtail_RD2-V02.original-640x191.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Gemma3-270M_Wagtail_RD2-V02.original-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Big tech has spent the last few years creating ever-larger AI models, leveraging rack after rack of expensive GPUs to provide generative AI as a cloud service. But tiny AI matters, too. Google has announced a tiny version of its Gemma open model designed to run on local devices. Google says the new Gemma 3 270M can be tuned in a snap and maintains robust performance despite its small footprint.&lt;/p&gt;
&lt;p&gt;Google released its first Gemma 3 open models earlier this year, featuring between 1 billion and 27 billion parameters. In generative AI, the parameters are the learned variables that control how the model processes inputs to estimate output tokens. Generally, the more parameters in a model, the better it performs. With just 270 million parameters, the new Gemma 3 can run on devices like smartphones or even entirely inside a web browser.&lt;/p&gt;
&lt;p&gt;Running an AI model locally has numerous benefits, including enhanced privacy and lower latency. Gemma 3 270M was designed with these kinds of use cases in mind. In testing with a Pixel 9 Pro, the new Gemma was able to run 25 conversations on the Tensor G4 chip and use just 0.75 percent of the device's battery. That makes it by far the most efficient Gemma model.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2112077 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Small Gemma benchmark" class="fullwidth full" height="2251" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Gemma3-270M_Chart01_RD3-V01.original.jpg" width="4001" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Gemma 3 270M shows strong instruction-following for its small size.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Developers shouldn't expect the same performance level of a multi-billion-parameter model, but Gemma 3 270M has its uses. Google used the IFEval benchmark, which tests a model's ability to follow instructions, to show that its new model punches above its weight. Gemma 3 270M hits a score of 51.2 percent in this test, which is higher than other lightweight models that have more parameters. The new Gemma falls predictably short of 1 billion-plus models like Llama 3.2, but it gets closer than you might think for having just a fraction of the parameters.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google claims Gemma 3 270M is good at following instructions out of the box, but it expects developers to fine-tune the model for their specific use cases. Due to the small parameter count, that process is fast and low-cost, too. Google sees the new Gemma being used for tasks like text classification and data analysis, which it can accomplish quickly and without heavy computing requirements.&lt;/p&gt;
&lt;h2&gt;Mostly open&lt;/h2&gt;
&lt;p&gt;Google refers to Gemma models as "open," which is not to be confused with "open source." This situation is the same in most ways, though. You can download the new Gemma for free, and the model weights are available. There's no separate commercial licensing agreement, so developers can modify, publish, and deploy Gemma 3 270M derivatives in their tools.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Gemma 270M story generator AI running in a browser.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;However, anyone using Gemma models is bound by the terms of use, which prohibit tuning the models to produce harmful outputs or intentionally violating privacy rules. Developers are also responsible for detailing modifications and providing a copy of the terms of use for all derivative versions, which inherit Google's custom license.&lt;/p&gt;
&lt;p&gt;Gemma 3 270M is available from platforms like Hugging Face and Kaggle in both pre-trained and instruction-tuned versions. It's available in Google's Vertex AI for testing, too. Google has also highlighted the capabilities of the new model with a fully browser-based story generator built on Transformer.js (see above). You can give that a shot even if you're not interested in developing with the new lightweight model.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The new Gemma model is a fraction of the size of most new models.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="191" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Gemma3-270M_Wagtail_RD2-V02.original-640x191.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Gemma3-270M_Wagtail_RD2-V02.original-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Big tech has spent the last few years creating ever-larger AI models, leveraging rack after rack of expensive GPUs to provide generative AI as a cloud service. But tiny AI matters, too. Google has announced a tiny version of its Gemma open model designed to run on local devices. Google says the new Gemma 3 270M can be tuned in a snap and maintains robust performance despite its small footprint.&lt;/p&gt;
&lt;p&gt;Google released its first Gemma 3 open models earlier this year, featuring between 1 billion and 27 billion parameters. In generative AI, the parameters are the learned variables that control how the model processes inputs to estimate output tokens. Generally, the more parameters in a model, the better it performs. With just 270 million parameters, the new Gemma 3 can run on devices like smartphones or even entirely inside a web browser.&lt;/p&gt;
&lt;p&gt;Running an AI model locally has numerous benefits, including enhanced privacy and lower latency. Gemma 3 270M was designed with these kinds of use cases in mind. In testing with a Pixel 9 Pro, the new Gemma was able to run 25 conversations on the Tensor G4 chip and use just 0.75 percent of the device's battery. That makes it by far the most efficient Gemma model.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2112077 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Small Gemma benchmark" class="fullwidth full" height="2251" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Gemma3-270M_Chart01_RD3-V01.original.jpg" width="4001" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Gemma 3 270M shows strong instruction-following for its small size.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Developers shouldn't expect the same performance level of a multi-billion-parameter model, but Gemma 3 270M has its uses. Google used the IFEval benchmark, which tests a model's ability to follow instructions, to show that its new model punches above its weight. Gemma 3 270M hits a score of 51.2 percent in this test, which is higher than other lightweight models that have more parameters. The new Gemma falls predictably short of 1 billion-plus models like Llama 3.2, but it gets closer than you might think for having just a fraction of the parameters.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google claims Gemma 3 270M is good at following instructions out of the box, but it expects developers to fine-tune the model for their specific use cases. Due to the small parameter count, that process is fast and low-cost, too. Google sees the new Gemma being used for tasks like text classification and data analysis, which it can accomplish quickly and without heavy computing requirements.&lt;/p&gt;
&lt;h2&gt;Mostly open&lt;/h2&gt;
&lt;p&gt;Google refers to Gemma models as "open," which is not to be confused with "open source." This situation is the same in most ways, though. You can download the new Gemma for free, and the model weights are available. There's no separate commercial licensing agreement, so developers can modify, publish, and deploy Gemma 3 270M derivatives in their tools.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Gemma 270M story generator AI running in a browser.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;However, anyone using Gemma models is bound by the terms of use, which prohibit tuning the models to produce harmful outputs or intentionally violating privacy rules. Developers are also responsible for detailing modifications and providing a copy of the terms of use for all derivative versions, which inherit Google's custom license.&lt;/p&gt;
&lt;p&gt;Gemma 3 270M is available from platforms like Hugging Face and Kaggle in both pre-trained and instruction-tuned versions. It's available in Google's Vertex AI for testing, too. Google has also highlighted the capabilities of the new model with a fully browser-based story generator built on Transformer.js (see above). You can give that a shot even if you're not interested in developing with the new lightweight model.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/08/google-releases-pint-size-gemma-open-ai-model/</guid><pubDate>Thu, 14 Aug 2025 20:04:22 +0000</pubDate></item><item><title>US government is reportedly in discussions to take stake in Intel (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/14/u-s-government-is-reportedly-in-discussions-to-take-stake-in-intel/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2228936694.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration continues to meddle with semiconductor giant Intel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.S. government is reportedly in discussions to take a stake in Intel, according to reporting from Bloomberg. This deal would be structured to help the company expand its U.S. manufacturing efforts, including its much-delayed Ohio chip factory.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This news comes less than a week after President Donald Trump insisted that Intel CEO Lip-Bu Tan resign because of perceived conflicts of interest. While Trump didn’t provide a reason, this came after Republican U.S. Sen. Tom Cotton wrote to Intel’s board asking about Tan’s alleged ties to China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tan met with the Trump administration on August 11 to quell the administration’s fears and figure out ways for the company to work with the government. This meeting is what sparked discussions of the U.S. government taking a direct stake in the company, according to Bloomberg.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intel declined to comment. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Intel is deeply committed to supporting President Trump’s efforts to strengthen U.S. technology and manufacturing leadership,” an Intel spokesperson said in a statement. “We look forward to continuing our work with the Trump Administration to advance these shared priorities, but we are not going to comment on rumors or speculation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2228936694.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration continues to meddle with semiconductor giant Intel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.S. government is reportedly in discussions to take a stake in Intel, according to reporting from Bloomberg. This deal would be structured to help the company expand its U.S. manufacturing efforts, including its much-delayed Ohio chip factory.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This news comes less than a week after President Donald Trump insisted that Intel CEO Lip-Bu Tan resign because of perceived conflicts of interest. While Trump didn’t provide a reason, this came after Republican U.S. Sen. Tom Cotton wrote to Intel’s board asking about Tan’s alleged ties to China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tan met with the Trump administration on August 11 to quell the administration’s fears and figure out ways for the company to work with the government. This meeting is what sparked discussions of the U.S. government taking a direct stake in the company, according to Bloomberg.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intel declined to comment. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Intel is deeply committed to supporting President Trump’s efforts to strengthen U.S. technology and manufacturing leadership,” an Intel spokesperson said in a statement. “We look forward to continuing our work with the Trump Administration to advance these shared priorities, but we are not going to comment on rumors or speculation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/14/u-s-government-is-reportedly-in-discussions-to-take-stake-in-intel/</guid><pubDate>Thu, 14 Aug 2025 20:38:20 +0000</pubDate></item><item><title>US government agency drops Grok after MechaHitler backlash, report says (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/08/us-government-agency-drops-grok-after-mechahitler-backlash-report-says/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        It appears Grok’s antisemitic rants stopped it from becoming feds’ go-to chatbot.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2224898774-640x426.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2224898774-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anadolu / Contributor | Anadolu

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;xAI apparently lost a government contract after a tweak to Grok's prompting triggered an antisemitic meltdown where the chatbot praised Hitler and declared itself MechaHitler last month.&lt;/p&gt;
&lt;p&gt;Despite the scandal, xAI announced that its products would soon be available for federal workers to purchase through the General Services Administration. At the time, xAI claimed this was an "important milestone" for its government business.&lt;/p&gt;
&lt;p&gt;But Wired reviewed emails and spoke to government insiders, which revealed that GSA leaders abruptly decided to drop xAI's Grok from their contract offering. That decision to pull the plug came after leadership allegedly rushed staff to make Grok available as soon as possible following a persuasive sales meeting with xAI in June.&lt;/p&gt;
&lt;p&gt;It's unclear what exactly caused the GSA to reverse course, but two sources told Wired that they "believe xAI was pulled because of Grok’s antisemitic tirade."&lt;/p&gt;
&lt;p&gt;As of this writing, xAI's "Grok for Government" website has not been updated to reflect GSA's supposed removal of Grok from an offering that xAI noted would have allowed "every federal government department, agency, or office, to access xAI's frontier AI products."&lt;/p&gt;
&lt;p&gt;xAI did not respond to Ars' request to comment and so far has not confirmed that the GSA offering is off the table. If Wired's report is accurate, GSA's decision also seemingly did not influence the military's decision to move forward with a $200 million xAI contract the US Department of Defense granted last month.&lt;/p&gt;
&lt;h2&gt;Government’s go-to tools will come from xAI’s rivals&lt;/h2&gt;
&lt;p&gt;If Grok is cut from the contract, that would suggest that Grok's meltdown came at perhaps the worst possible moment for xAI, which is building the "world's biggest supercomputer" as fast as it can to try to get ahead of its biggest AI rivals.&lt;/p&gt;
&lt;p&gt;Grok seemingly had the potential to become a more widely used tool if federal workers opted for xAI's models. Through Donald Trump's AI Action Plan, the president has similarly emphasized speed, pushing for federal workers to adopt AI as quickly as possible. Although xAI may no longer be involved in that broad push, other AI companies like OpenAI, Anthropic, and Google have partnered with the government to help Trump pull that off and stand to benefit long-term if their tools become entrenched in certain agencies.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Wired's report suggested that some federal workers are uncomfortable not just with the Trump administration's aggressive pace in forcing AI adoption, but also with the price of the products agencies are being required to use. After OpenAI offered to charge federal workers only a nominal $1 fee to access ChatGPT Enterprise, Anthropic quickly set the same price, seemingly hoping to compete.&lt;/p&gt;
&lt;p&gt;Some workers told Wired they felt the nominal fee "amounts to a gift from a tech company and is highly unusual" compared to how the procurement process usually works. Perhaps most glaringly, the partnerships came together so fast, one GSA worker told Wired that "it wasn't even clear who to send the $1 to or how."&lt;/p&gt;
&lt;p&gt;Grok was intended to be pushed through the procurement process at a similar clip, Wired reported, but Grok's antisemitic outputs instead seemingly caused enough internal GSA pushback that the effort was halted.&lt;/p&gt;
&lt;p&gt;For Elon Musk—who spent the past week frustrated by ChatGPT consistently beating out Grok in Apple's app store rankings and locking horns with OpenAI CEO Sam Altman over whose AI model is better for humanity—being cut out of the government's widest AI push at this moment could seemingly risk long-term consequences for Grok's utility not just in federal government but also local governments xAI is targeted with services across the US.&lt;/p&gt;
&lt;p&gt;Notably, Grok's antisemitic outputs came after Musk vowed to make the chatbot less "woke." That seemingly meant adding rules to Grok's prompting, which were later deleted, instructing that the chatbot "should not shy away from making claims which are politically incorrect."&lt;/p&gt;
&lt;p&gt;If OpenAI's ChatGPT wins more government contracts and continues to dominate popular rankings, Musk may be left wondering if his mission to make Grok edgier than other chatbots will ultimately be what prevents Grok from becoming America's go-to chatbot.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        It appears Grok’s antisemitic rants stopped it from becoming feds’ go-to chatbot.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2224898774-640x426.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2224898774-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anadolu / Contributor | Anadolu

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;xAI apparently lost a government contract after a tweak to Grok's prompting triggered an antisemitic meltdown where the chatbot praised Hitler and declared itself MechaHitler last month.&lt;/p&gt;
&lt;p&gt;Despite the scandal, xAI announced that its products would soon be available for federal workers to purchase through the General Services Administration. At the time, xAI claimed this was an "important milestone" for its government business.&lt;/p&gt;
&lt;p&gt;But Wired reviewed emails and spoke to government insiders, which revealed that GSA leaders abruptly decided to drop xAI's Grok from their contract offering. That decision to pull the plug came after leadership allegedly rushed staff to make Grok available as soon as possible following a persuasive sales meeting with xAI in June.&lt;/p&gt;
&lt;p&gt;It's unclear what exactly caused the GSA to reverse course, but two sources told Wired that they "believe xAI was pulled because of Grok’s antisemitic tirade."&lt;/p&gt;
&lt;p&gt;As of this writing, xAI's "Grok for Government" website has not been updated to reflect GSA's supposed removal of Grok from an offering that xAI noted would have allowed "every federal government department, agency, or office, to access xAI's frontier AI products."&lt;/p&gt;
&lt;p&gt;xAI did not respond to Ars' request to comment and so far has not confirmed that the GSA offering is off the table. If Wired's report is accurate, GSA's decision also seemingly did not influence the military's decision to move forward with a $200 million xAI contract the US Department of Defense granted last month.&lt;/p&gt;
&lt;h2&gt;Government’s go-to tools will come from xAI’s rivals&lt;/h2&gt;
&lt;p&gt;If Grok is cut from the contract, that would suggest that Grok's meltdown came at perhaps the worst possible moment for xAI, which is building the "world's biggest supercomputer" as fast as it can to try to get ahead of its biggest AI rivals.&lt;/p&gt;
&lt;p&gt;Grok seemingly had the potential to become a more widely used tool if federal workers opted for xAI's models. Through Donald Trump's AI Action Plan, the president has similarly emphasized speed, pushing for federal workers to adopt AI as quickly as possible. Although xAI may no longer be involved in that broad push, other AI companies like OpenAI, Anthropic, and Google have partnered with the government to help Trump pull that off and stand to benefit long-term if their tools become entrenched in certain agencies.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Wired's report suggested that some federal workers are uncomfortable not just with the Trump administration's aggressive pace in forcing AI adoption, but also with the price of the products agencies are being required to use. After OpenAI offered to charge federal workers only a nominal $1 fee to access ChatGPT Enterprise, Anthropic quickly set the same price, seemingly hoping to compete.&lt;/p&gt;
&lt;p&gt;Some workers told Wired they felt the nominal fee "amounts to a gift from a tech company and is highly unusual" compared to how the procurement process usually works. Perhaps most glaringly, the partnerships came together so fast, one GSA worker told Wired that "it wasn't even clear who to send the $1 to or how."&lt;/p&gt;
&lt;p&gt;Grok was intended to be pushed through the procurement process at a similar clip, Wired reported, but Grok's antisemitic outputs instead seemingly caused enough internal GSA pushback that the effort was halted.&lt;/p&gt;
&lt;p&gt;For Elon Musk—who spent the past week frustrated by ChatGPT consistently beating out Grok in Apple's app store rankings and locking horns with OpenAI CEO Sam Altman over whose AI model is better for humanity—being cut out of the government's widest AI push at this moment could seemingly risk long-term consequences for Grok's utility not just in federal government but also local governments xAI is targeted with services across the US.&lt;/p&gt;
&lt;p&gt;Notably, Grok's antisemitic outputs came after Musk vowed to make the chatbot less "woke." That seemingly meant adding rules to Grok's prompting, which were later deleted, instructing that the chatbot "should not shy away from making claims which are politically incorrect."&lt;/p&gt;
&lt;p&gt;If OpenAI's ChatGPT wins more government contracts and continues to dominate popular rankings, Musk may be left wondering if his mission to make Grok edgier than other chatbots will ultimately be what prevents Grok from becoming America's go-to chatbot.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/08/us-government-agency-drops-grok-after-mechahitler-backlash-report-says/</guid><pubDate>Thu, 14 Aug 2025 21:11:06 +0000</pubDate></item><item><title>Gartner: GPT-5 is here, but the infrastructure to support true agentic AI isn’t (yet) (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/gartner-gpt-5-is-here-but-the-infrastructure-to-support-true-agentic-ai-isnt-yet/</link><description>&lt;p&gt;Here’s an analogy: Freeways didn’t exist in the U.S. until after 1956, when envisioned by President Dwight D. Eisenhower’s administration — yet super fast, powerful cars like Porsche, BMW, Jaguars, Ferrari and others had been around for decades.&amp;nbsp;&lt;/p&gt;&lt;p&gt;You could say AI is at that same pivot point: While models are becoming increasingly more capable, performant and sophisticated, the critical infrastructure they need to bring about true, real-world innovation has yet to be fully built out.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“All we have done is create some very good engines for a car, and we are getting super excited, as if we have this fully functional highway system in place,” Arun Chandrasekaran, Gartner distinguished VP analyst, told VentureBeat.&amp;nbsp;&lt;/p&gt;&lt;p&gt;This is leading to a plateauing, of sorts, in model capabilities such as OpenAI’s GPT-5: While an important step forward, it only features faint glimmers of truly agentic AI. &lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;“It is a very capable model, it is a very versatile model, it has made some very good progress in specific domains,” said Chandrasekaran. “But my view is it’s more of an incremental progress, rather than a radical progress or a radical improvement, given all of the high expectations OpenAI has set in the past.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-gpt-5-improves-in-three-key-areas"&gt;GPT-5 improves in three key areas&lt;/h2&gt;



&lt;p&gt;To be clear, OpenAI has made strides with GPT-5, according to Gartner, including in coding tasks and multi-modal capabilities.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Chandrasekaran pointed out that OpenAI has pivoted to make GPT-5 “very good” at coding, clearly sensing gen AI’s enormous opportunity in enterprise software engineering and taking aim at competitor Anthropic’s leadership in that area.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Meanwhile, GPT-5’s progress in modalities beyond text, particularly in speech and images, provides new integration opportunities for enterprises, Chandrasekaran noted.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;GPT-5 also does, if subtly, advance AI agent and orchestration design, thanks to improved tool use; the model&amp;nbsp;can call third-party APIs and tools and perform parallel tool calling (handle multiple tasks simultaneously). However, this means enterprise systems must have the capacity to handle concurrent API requests in a single session, Chandrasekaran points out. &lt;/p&gt;



&lt;p&gt;Multistep planning in GPT-5 allows more business logic to reside within the model itself, reducing the need for external workflow engines, and its larger context windows (8K for free users, 32K for Plus at $20 per month and 128K for Pro at $200 per month) can “reshape enterprise AI architecture patterns,” he said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This means that applications that previously relied on complex retrieval-augmented generation (RAG) pipelines to work around context limits can now pass much larger datasets directly to the models and simplify some workflows. But this doesn’t mean RAG is irrelevant; “retrieving only the most relevant data is still faster and more cost-effective than always sending massive inputs,” Chandrasekaran pointed out.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Gartner sees a shift to a hybrid approach with less stringent retrieval, with devs using GPT-5 to handle “larger, messier contexts” while improving efficiency.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On the cost front, GPT-5 “significantly” reduces API usage fees; top-level costs are $1.25 per 1 million input tokens and $10 per 1 million output tokens, making it comparable to models like Gemini 2.5, but seriously undercutting Claude Opus. However, GTP-5’s input/output price ratio is higher than earlier models, which AI leaders should take into account when considering GTP-5 for high-token-usage scenarios, Chandrasekaran advised.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-bye-bye-previous-gpt-versions-sorta"&gt;Bye-bye previous GPT versions (sorta)&lt;/h2&gt;



&lt;p&gt;Ultimately, GPT-5 is designed to eventually replace GPT-4o and the o-series (they were initially sunset, then some reintroduced by OpenAI due to user dissent). Three model sizes (pro, mini, nano) will allow architects to tier services based on cost and latency needs; simple queries can be handled by smaller models and complex tasks by the full model, Gartner notes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, differences in output formats, memory and function-calling behaviors may require code review and adjustment, and because GPT-5 may render some previous workarounds obsolete, devs should audit their prompt templates and system instructions. &lt;/p&gt;



&lt;p&gt;By eventually sunsetting previous versions, “I think what OpenAI is trying to do is abstract that level of complexity away from the user,” said Chandrasekaran. “Often we’re not the best people to make those decisions, and sometimes we may even make erroneous decisions, I would argue.”&lt;/p&gt;



&lt;p&gt;Another fact behind the phase-outs: “We all know that OpenAI has a capacity problem,” he said, and thus has forged partnerships with Microsoft, Oracle (Project Stargate), Google and others to provision compute capacity. Running multiple generations of models would require multiple generations of infrastructure, creating new cost implications and physical constraints.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-new-risks-advice-for-adopting-gpt-5"&gt;New risks, advice for adopting GPT-5&lt;/h2&gt;



&lt;p&gt;OpenAI claims it reduced hallucination rates by up to 65% in GPT-5 compared to previous models; this can help reduce compliance risks and make the model more suitable for enterprise use cases, and its chain-of-thought (CoT) explanations support auditability and regulatory alignment, Gartner notes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At the same time, these lower hallucination rates as well as GPT-5’s advanced reasoning and multimodal processing could amplify misuse such as advanced scam and phishing generation. Analysts advise that critical workflows remain under human review, even if with less sampling.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The firm also advises that enterprise leaders:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Pilot and benchmark GPT-5 in mission-critical use cases, running side-by-side evaluations against other models to determine differences in accuracy, speed and user experience.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Monitor practices like vibe coding that risk data exposure (but without being offensive about it or risking defects or guardrail failures).&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Revise governance policies and guidelines to address new model behaviors, expanded context windows and safe completions, and calibrate oversight mechanisms.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Experiment with tool integrations, reasoning parameters, caching and model sizing to optimize performance, and use inbuilt dynamic routing to determine the right model for the right task.&lt;/li&gt;



&lt;li&gt;Audit and upgrade plans for GPT-5’s expanded capabilities. This includes validating API quotas, audit trails and multimodal data pipelines to support new features and increased throughput. Rigorous integration testing is also important.&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-agents-don-t-just-need-more-compute-they-need-infrastructure"&gt;Agents don’t just need more compute; they need infrastructure&lt;/h2&gt;



&lt;p&gt;No doubt, agentic AI is a “super hot topic today,” Chandrasekaran noted, and is one of the top areas for investment in Gartner’s 2025 Hype Cycle for Gen AI. At the same time, the technology has hit Gartner’s “Peak of Inflated Expectations,” meaning it has experienced widespread publicity due to early success stories, in turn building unrealistic expectations.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015582" height="451" src="https://venturebeat.com/wp-content/uploads/2025/08/hype-cycle-for-generative-ai-2025.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;This trend is typically followed by what Gartner calls the “Trough of Disillusionment,” when interest, excitement and investment cool off as experiments and implementations fail to deliver (remember: There have been two notable AI winters since the 1980s).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“A lot of vendors are hyping products beyond what products are capable of,” said Chandrasekaran. “It’s almost like they’re positioning them as being production-ready, enterprise-ready and are going to deliver business value in a really short span of time.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, in reality, the chasm between product quality relative to expectation is wide, he noted. Gartner isn’t seeing enterprise-wide agentic deployments; those they are seeing are in “small, narrow pockets” and specific domains like software engineering or procurement.&lt;/p&gt;



&lt;p&gt;“But even those workflows are not fully autonomous; they are often either human-driven or semi-autonomous in nature,” Chandrasekaran explained.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One of the key culprits is the lack of infrastructure; agents require access to a wide set of enterprise tools and must have the capability to communicate with data stores and SaaS apps. At the same time, there must be adequate identity and access management systems in place to control agent behavior and access, as well as oversight of the types of data they can access (not personally identifiable or sensitive), he noted.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Lastly, enterprises must be confident that the information the agents are producing is trustworthy, meaning it’s free of bias and doesn’t contain hallucinations or false information.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To get there, vendors must collaborate and adopt more open standards for agent-to-enterprise and agent-to-agent tool communication, he advised. &lt;/p&gt;



&lt;p&gt;“While agents or the underlying technologies may be making progress, this orchestration, governance and data layer is still waiting to be built out for agents to thrive,” said Chandrasekaran. “That’s where we see a lot of friction today.”&lt;/p&gt;



&lt;p&gt;Yes, the industry is making progress with AI reasoning, but still struggles to get AI to understand how the physical world works. AI mostly operates in a digital world; it doesn’t have strong interfaces to the physical world, although improvements are being made in spatial robotics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But, “we are very, very, very, very early stage for those kinds of environments,” said Chandrasekaran.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To truly make significant strides requires a “revolution” in model architecture or reasoning. “You cannot be on the current curve and just expect more data, more compute, and hope to get to AGI,” she said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;That’s evident in the much-anticipated GPT-5 rollout: The ultimate goal that OpenAI defined for itself was AGI, but “it’s really apparent that we are nowhere close to that,” said Chandrasekaran. Ultimately, “we’re still very, very far away from AGI.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;Here’s an analogy: Freeways didn’t exist in the U.S. until after 1956, when envisioned by President Dwight D. Eisenhower’s administration — yet super fast, powerful cars like Porsche, BMW, Jaguars, Ferrari and others had been around for decades.&amp;nbsp;&lt;/p&gt;&lt;p&gt;You could say AI is at that same pivot point: While models are becoming increasingly more capable, performant and sophisticated, the critical infrastructure they need to bring about true, real-world innovation has yet to be fully built out.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“All we have done is create some very good engines for a car, and we are getting super excited, as if we have this fully functional highway system in place,” Arun Chandrasekaran, Gartner distinguished VP analyst, told VentureBeat.&amp;nbsp;&lt;/p&gt;&lt;p&gt;This is leading to a plateauing, of sorts, in model capabilities such as OpenAI’s GPT-5: While an important step forward, it only features faint glimmers of truly agentic AI. &lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;“It is a very capable model, it is a very versatile model, it has made some very good progress in specific domains,” said Chandrasekaran. “But my view is it’s more of an incremental progress, rather than a radical progress or a radical improvement, given all of the high expectations OpenAI has set in the past.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-gpt-5-improves-in-three-key-areas"&gt;GPT-5 improves in three key areas&lt;/h2&gt;



&lt;p&gt;To be clear, OpenAI has made strides with GPT-5, according to Gartner, including in coding tasks and multi-modal capabilities.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Chandrasekaran pointed out that OpenAI has pivoted to make GPT-5 “very good” at coding, clearly sensing gen AI’s enormous opportunity in enterprise software engineering and taking aim at competitor Anthropic’s leadership in that area.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Meanwhile, GPT-5’s progress in modalities beyond text, particularly in speech and images, provides new integration opportunities for enterprises, Chandrasekaran noted.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;GPT-5 also does, if subtly, advance AI agent and orchestration design, thanks to improved tool use; the model&amp;nbsp;can call third-party APIs and tools and perform parallel tool calling (handle multiple tasks simultaneously). However, this means enterprise systems must have the capacity to handle concurrent API requests in a single session, Chandrasekaran points out. &lt;/p&gt;



&lt;p&gt;Multistep planning in GPT-5 allows more business logic to reside within the model itself, reducing the need for external workflow engines, and its larger context windows (8K for free users, 32K for Plus at $20 per month and 128K for Pro at $200 per month) can “reshape enterprise AI architecture patterns,” he said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This means that applications that previously relied on complex retrieval-augmented generation (RAG) pipelines to work around context limits can now pass much larger datasets directly to the models and simplify some workflows. But this doesn’t mean RAG is irrelevant; “retrieving only the most relevant data is still faster and more cost-effective than always sending massive inputs,” Chandrasekaran pointed out.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Gartner sees a shift to a hybrid approach with less stringent retrieval, with devs using GPT-5 to handle “larger, messier contexts” while improving efficiency.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On the cost front, GPT-5 “significantly” reduces API usage fees; top-level costs are $1.25 per 1 million input tokens and $10 per 1 million output tokens, making it comparable to models like Gemini 2.5, but seriously undercutting Claude Opus. However, GTP-5’s input/output price ratio is higher than earlier models, which AI leaders should take into account when considering GTP-5 for high-token-usage scenarios, Chandrasekaran advised.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-bye-bye-previous-gpt-versions-sorta"&gt;Bye-bye previous GPT versions (sorta)&lt;/h2&gt;



&lt;p&gt;Ultimately, GPT-5 is designed to eventually replace GPT-4o and the o-series (they were initially sunset, then some reintroduced by OpenAI due to user dissent). Three model sizes (pro, mini, nano) will allow architects to tier services based on cost and latency needs; simple queries can be handled by smaller models and complex tasks by the full model, Gartner notes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, differences in output formats, memory and function-calling behaviors may require code review and adjustment, and because GPT-5 may render some previous workarounds obsolete, devs should audit their prompt templates and system instructions. &lt;/p&gt;



&lt;p&gt;By eventually sunsetting previous versions, “I think what OpenAI is trying to do is abstract that level of complexity away from the user,” said Chandrasekaran. “Often we’re not the best people to make those decisions, and sometimes we may even make erroneous decisions, I would argue.”&lt;/p&gt;



&lt;p&gt;Another fact behind the phase-outs: “We all know that OpenAI has a capacity problem,” he said, and thus has forged partnerships with Microsoft, Oracle (Project Stargate), Google and others to provision compute capacity. Running multiple generations of models would require multiple generations of infrastructure, creating new cost implications and physical constraints.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-new-risks-advice-for-adopting-gpt-5"&gt;New risks, advice for adopting GPT-5&lt;/h2&gt;



&lt;p&gt;OpenAI claims it reduced hallucination rates by up to 65% in GPT-5 compared to previous models; this can help reduce compliance risks and make the model more suitable for enterprise use cases, and its chain-of-thought (CoT) explanations support auditability and regulatory alignment, Gartner notes.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At the same time, these lower hallucination rates as well as GPT-5’s advanced reasoning and multimodal processing could amplify misuse such as advanced scam and phishing generation. Analysts advise that critical workflows remain under human review, even if with less sampling.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The firm also advises that enterprise leaders:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Pilot and benchmark GPT-5 in mission-critical use cases, running side-by-side evaluations against other models to determine differences in accuracy, speed and user experience.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Monitor practices like vibe coding that risk data exposure (but without being offensive about it or risking defects or guardrail failures).&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Revise governance policies and guidelines to address new model behaviors, expanded context windows and safe completions, and calibrate oversight mechanisms.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Experiment with tool integrations, reasoning parameters, caching and model sizing to optimize performance, and use inbuilt dynamic routing to determine the right model for the right task.&lt;/li&gt;



&lt;li&gt;Audit and upgrade plans for GPT-5’s expanded capabilities. This includes validating API quotas, audit trails and multimodal data pipelines to support new features and increased throughput. Rigorous integration testing is also important.&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-agents-don-t-just-need-more-compute-they-need-infrastructure"&gt;Agents don’t just need more compute; they need infrastructure&lt;/h2&gt;



&lt;p&gt;No doubt, agentic AI is a “super hot topic today,” Chandrasekaran noted, and is one of the top areas for investment in Gartner’s 2025 Hype Cycle for Gen AI. At the same time, the technology has hit Gartner’s “Peak of Inflated Expectations,” meaning it has experienced widespread publicity due to early success stories, in turn building unrealistic expectations.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015582" height="451" src="https://venturebeat.com/wp-content/uploads/2025/08/hype-cycle-for-generative-ai-2025.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;This trend is typically followed by what Gartner calls the “Trough of Disillusionment,” when interest, excitement and investment cool off as experiments and implementations fail to deliver (remember: There have been two notable AI winters since the 1980s).&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“A lot of vendors are hyping products beyond what products are capable of,” said Chandrasekaran. “It’s almost like they’re positioning them as being production-ready, enterprise-ready and are going to deliver business value in a really short span of time.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, in reality, the chasm between product quality relative to expectation is wide, he noted. Gartner isn’t seeing enterprise-wide agentic deployments; those they are seeing are in “small, narrow pockets” and specific domains like software engineering or procurement.&lt;/p&gt;



&lt;p&gt;“But even those workflows are not fully autonomous; they are often either human-driven or semi-autonomous in nature,” Chandrasekaran explained.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One of the key culprits is the lack of infrastructure; agents require access to a wide set of enterprise tools and must have the capability to communicate with data stores and SaaS apps. At the same time, there must be adequate identity and access management systems in place to control agent behavior and access, as well as oversight of the types of data they can access (not personally identifiable or sensitive), he noted.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Lastly, enterprises must be confident that the information the agents are producing is trustworthy, meaning it’s free of bias and doesn’t contain hallucinations or false information.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To get there, vendors must collaborate and adopt more open standards for agent-to-enterprise and agent-to-agent tool communication, he advised. &lt;/p&gt;



&lt;p&gt;“While agents or the underlying technologies may be making progress, this orchestration, governance and data layer is still waiting to be built out for agents to thrive,” said Chandrasekaran. “That’s where we see a lot of friction today.”&lt;/p&gt;



&lt;p&gt;Yes, the industry is making progress with AI reasoning, but still struggles to get AI to understand how the physical world works. AI mostly operates in a digital world; it doesn’t have strong interfaces to the physical world, although improvements are being made in spatial robotics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;But, “we are very, very, very, very early stage for those kinds of environments,” said Chandrasekaran.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To truly make significant strides requires a “revolution” in model architecture or reasoning. “You cannot be on the current curve and just expect more data, more compute, and hope to get to AGI,” she said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;That’s evident in the much-anticipated GPT-5 rollout: The ultimate goal that OpenAI defined for itself was AGI, but “it’s really apparent that we are nowhere close to that,” said Chandrasekaran. Ultimately, “we’re still very, very far away from AGI.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/gartner-gpt-5-is-here-but-the-infrastructure-to-support-true-agentic-ai-isnt-yet/</guid><pubDate>Thu, 14 Aug 2025 21:35:28 +0000</pubDate></item><item><title>That ‘cheap’ open-source AI model is actually burning through your compute budget (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/that-cheap-open-source-ai-model-is-actually-burning-through-your-compute-budget/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A comprehensive new study has revealed that open-source artificial intelligence models consume significantly more computing resources than their closed-source competitors when performing identical tasks, potentially undermining their cost advantages and reshaping how enterprises evaluate AI deployment strategies.&lt;/p&gt;



&lt;p&gt;The research, conducted by AI firm Nous Research, found that open-weight models use between 1.5 to 4 times more tokens — the basic units of AI computation — than closed models like those from OpenAI and Anthropic. For simple knowledge questions, the gap widened dramatically, with some open models using up to 10 times more tokens.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmarkhttps://t.co/b1e1rJx6vZ&lt;/p&gt;&lt;p&gt;We measured token usage across reasoning models: open models output 1.5-4x more tokens than closed models on identical tasks, but with huge variance depending on task type (up to… pic.twitter.com/LY1083won8&lt;/p&gt;— Nous Research (@NousResearch) August 14, 2025&lt;/blockquote&gt; 



&lt;p&gt;“Open weight models use 1.5–4× more tokens than closed ones (up to 10× for simple knowledge questions), making them sometimes more expensive per query despite lower per‑token costs,” the researchers wrote in their report published Wednesday.&lt;/p&gt;



&lt;p&gt;The findings challenge a prevailing assumption in the AI industry that open-source models offer clear economic advantages over proprietary alternatives. While open-source models typically cost less per token to run, the study suggests this advantage can be “easily offset if they require more tokens to reason about a given problem.”&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-the-real-cost-of-ai-why-cheaper-models-may-break-your-budget"&gt;The real cost of AI: Why ‘cheaper’ models may break your budget&lt;/h2&gt;



&lt;p&gt;The research examined 19 different AI models across three categories of tasks: basic knowledge questions, mathematical problems, and logic puzzles. The team measured “token efficiency” — how many computational units models use relative to the complexity of their solutions—a metric that has received little systematic study despite its significant cost implications.&lt;/p&gt;



&lt;p&gt;“Token efficiency is a critical metric for several practical reasons,” the researchers noted. “While hosting open weight models may be cheaper, this cost advantage could be easily offset if they require more tokens to reason about a given problem.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015621" height="396" src="https://venturebeat.com/wp-content/uploads/2025/08/image-three.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;Open-source AI models use up to 12 times more computational resources than the most efficient closed models for basic knowledge questions. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The inefficiency is particularly pronounced for Large Reasoning Models (LRMs), which use extended “chains of thought” to solve complex problems. These models, designed to think through problems step-by-step, can consume thousands of tokens pondering simple questions that should require minimal computation.&lt;/p&gt;



&lt;p&gt;For basic knowledge questions like “What is the capital of Australia?” the study found that reasoning models spend “hundreds of tokens pondering simple knowledge questions” that could be answered in a single word.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-which-ai-models-actually-deliver-bang-for-your-buck"&gt;Which AI models actually deliver bang for your buck&lt;/h2&gt;



&lt;p&gt;The research revealed stark differences between model providers. OpenAI’s models, particularly its o4-mini and newly released open-source gpt-oss variants, demonstrated exceptional token efficiency, especially for mathematical problems. The study found OpenAI models “stand out for extreme token efficiency in math problems,” using up to three times fewer tokens than other commercial models.&lt;/p&gt;



&lt;p&gt;Among open-source options, Nvidia’s llama-3.3-nemotron-super-49b-v1 emerged as “the most token efficient open weight model across all domains,” while newer models from companies like Magistral showed “exceptionally high token usage” as outliers.&lt;/p&gt;



&lt;p&gt;The efficiency gap varied significantly by task type. While open models used roughly twice as many tokens for mathematical and logic problems, the difference ballooned for simple knowledge questions where efficient reasoning should be unnecessary.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015623" height="600" src="https://venturebeat.com/wp-content/uploads/2025/08/image-two.png?w=795" width="795" /&gt;&lt;figcaption class="wp-element-caption"&gt;OpenAI’s latest models achieve the lowest costs for simple questions, while some open-source alternatives can cost significantly more despite lower per-token pricing. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprise-leaders-need-to-know-about-ai-computing-costs"&gt;What enterprise leaders need to know about AI computing costs&lt;/h2&gt;



&lt;p&gt;The findings have immediate implications for enterprise AI adoption, where computing costs can scale rapidly with usage. Companies evaluating AI models often focus on accuracy benchmarks and per-token pricing, but may overlook the total computational requirements for real-world tasks.&lt;/p&gt;



&lt;p&gt;“The better token efficiency of closed weight models often compensates for the higher API pricing of those models,” the researchers found when analyzing total inference costs.&lt;/p&gt;



&lt;p&gt;The study also revealed that closed-source model providers appear to be actively optimizing for efficiency. “Closed weight models have been iteratively optimized to use fewer tokens to reduce inference cost,” while open-source models have “increased their token usage for newer versions, possibly reflecting a priority toward better reasoning performance.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015626" height="600" src="https://venturebeat.com/wp-content/uploads/2025/08/image-six.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;The computational overhead varies dramatically between AI providers, with some models using over 1,000 tokens for internal reasoning on simple tasks. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-how-researchers-cracked-the-code-on-ai-efficiency-measurement"&gt;How researchers cracked the code on AI efficiency measurement&lt;/h2&gt;



&lt;p&gt;The research team faced unique challenges in measuring efficiency across different model architectures. Many closed-source models don’t reveal their raw reasoning processes, instead providing compressed summaries of their internal computations to prevent competitors from copying their techniques.&lt;/p&gt;



&lt;p&gt;To address this, researchers used completion tokens — the total computational units billed for each query — as a proxy for reasoning effort. They discovered that “most recent closed source models will not share their raw reasoning traces” and instead “use smaller language models to transcribe the chain of thought into summaries or compressed representations.”&lt;/p&gt;



&lt;p&gt;The study’s methodology included testing with modified versions of well-known problems to minimize the influence of memorized solutions, such as altering variables in mathematical competition problems from the American Invitational Mathematics Examination (AIME).&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015622" height="550" src="https://venturebeat.com/wp-content/uploads/2025/08/Image-one.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;Different AI models show varying relationships between computation and output, with some providers compressing reasoning traces while others provide full details. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-the-future-of-ai-efficiency-what-s-coming-next"&gt;The future of AI efficiency: What’s coming next&lt;/h2&gt;



&lt;p&gt;The researchers suggest that token efficiency should become a primary optimization target alongside accuracy for future model development. “A more densified CoT will also allow for more efficient context usage and may counter context degradation during challenging reasoning tasks,” they wrote.&lt;/p&gt;



&lt;p&gt;The release of OpenAI’s open-source gpt-oss models, which demonstrate state-of-the-art efficiency with “freely accessible CoT,” could serve as a reference point for optimizing other open-source models.&lt;/p&gt;



&lt;p&gt;The complete research dataset and evaluation code are available on GitHub, allowing other researchers to validate and extend the findings. As the AI industry races toward more powerful reasoning capabilities, this study suggests that the real competition may not be about who can build the smartest AI — but who can build the most efficient one.&lt;/p&gt;



&lt;p&gt;After all, in a world where every token counts, the most wasteful models may find themselves priced out of the market, regardless of how well they can think.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A comprehensive new study has revealed that open-source artificial intelligence models consume significantly more computing resources than their closed-source competitors when performing identical tasks, potentially undermining their cost advantages and reshaping how enterprises evaluate AI deployment strategies.&lt;/p&gt;



&lt;p&gt;The research, conducted by AI firm Nous Research, found that open-weight models use between 1.5 to 4 times more tokens — the basic units of AI computation — than closed models like those from OpenAI and Anthropic. For simple knowledge questions, the gap widened dramatically, with some open models using up to 10 times more tokens.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmarkhttps://t.co/b1e1rJx6vZ&lt;/p&gt;&lt;p&gt;We measured token usage across reasoning models: open models output 1.5-4x more tokens than closed models on identical tasks, but with huge variance depending on task type (up to… pic.twitter.com/LY1083won8&lt;/p&gt;— Nous Research (@NousResearch) August 14, 2025&lt;/blockquote&gt; 



&lt;p&gt;“Open weight models use 1.5–4× more tokens than closed ones (up to 10× for simple knowledge questions), making them sometimes more expensive per query despite lower per‑token costs,” the researchers wrote in their report published Wednesday.&lt;/p&gt;



&lt;p&gt;The findings challenge a prevailing assumption in the AI industry that open-source models offer clear economic advantages over proprietary alternatives. While open-source models typically cost less per token to run, the study suggests this advantage can be “easily offset if they require more tokens to reason about a given problem.”&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-the-real-cost-of-ai-why-cheaper-models-may-break-your-budget"&gt;The real cost of AI: Why ‘cheaper’ models may break your budget&lt;/h2&gt;



&lt;p&gt;The research examined 19 different AI models across three categories of tasks: basic knowledge questions, mathematical problems, and logic puzzles. The team measured “token efficiency” — how many computational units models use relative to the complexity of their solutions—a metric that has received little systematic study despite its significant cost implications.&lt;/p&gt;



&lt;p&gt;“Token efficiency is a critical metric for several practical reasons,” the researchers noted. “While hosting open weight models may be cheaper, this cost advantage could be easily offset if they require more tokens to reason about a given problem.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015621" height="396" src="https://venturebeat.com/wp-content/uploads/2025/08/image-three.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;Open-source AI models use up to 12 times more computational resources than the most efficient closed models for basic knowledge questions. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The inefficiency is particularly pronounced for Large Reasoning Models (LRMs), which use extended “chains of thought” to solve complex problems. These models, designed to think through problems step-by-step, can consume thousands of tokens pondering simple questions that should require minimal computation.&lt;/p&gt;



&lt;p&gt;For basic knowledge questions like “What is the capital of Australia?” the study found that reasoning models spend “hundreds of tokens pondering simple knowledge questions” that could be answered in a single word.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-which-ai-models-actually-deliver-bang-for-your-buck"&gt;Which AI models actually deliver bang for your buck&lt;/h2&gt;



&lt;p&gt;The research revealed stark differences between model providers. OpenAI’s models, particularly its o4-mini and newly released open-source gpt-oss variants, demonstrated exceptional token efficiency, especially for mathematical problems. The study found OpenAI models “stand out for extreme token efficiency in math problems,” using up to three times fewer tokens than other commercial models.&lt;/p&gt;



&lt;p&gt;Among open-source options, Nvidia’s llama-3.3-nemotron-super-49b-v1 emerged as “the most token efficient open weight model across all domains,” while newer models from companies like Magistral showed “exceptionally high token usage” as outliers.&lt;/p&gt;



&lt;p&gt;The efficiency gap varied significantly by task type. While open models used roughly twice as many tokens for mathematical and logic problems, the difference ballooned for simple knowledge questions where efficient reasoning should be unnecessary.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015623" height="600" src="https://venturebeat.com/wp-content/uploads/2025/08/image-two.png?w=795" width="795" /&gt;&lt;figcaption class="wp-element-caption"&gt;OpenAI’s latest models achieve the lowest costs for simple questions, while some open-source alternatives can cost significantly more despite lower per-token pricing. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprise-leaders-need-to-know-about-ai-computing-costs"&gt;What enterprise leaders need to know about AI computing costs&lt;/h2&gt;



&lt;p&gt;The findings have immediate implications for enterprise AI adoption, where computing costs can scale rapidly with usage. Companies evaluating AI models often focus on accuracy benchmarks and per-token pricing, but may overlook the total computational requirements for real-world tasks.&lt;/p&gt;



&lt;p&gt;“The better token efficiency of closed weight models often compensates for the higher API pricing of those models,” the researchers found when analyzing total inference costs.&lt;/p&gt;



&lt;p&gt;The study also revealed that closed-source model providers appear to be actively optimizing for efficiency. “Closed weight models have been iteratively optimized to use fewer tokens to reduce inference cost,” while open-source models have “increased their token usage for newer versions, possibly reflecting a priority toward better reasoning performance.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015626" height="600" src="https://venturebeat.com/wp-content/uploads/2025/08/image-six.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;The computational overhead varies dramatically between AI providers, with some models using over 1,000 tokens for internal reasoning on simple tasks. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-how-researchers-cracked-the-code-on-ai-efficiency-measurement"&gt;How researchers cracked the code on AI efficiency measurement&lt;/h2&gt;



&lt;p&gt;The research team faced unique challenges in measuring efficiency across different model architectures. Many closed-source models don’t reveal their raw reasoning processes, instead providing compressed summaries of their internal computations to prevent competitors from copying their techniques.&lt;/p&gt;



&lt;p&gt;To address this, researchers used completion tokens — the total computational units billed for each query — as a proxy for reasoning effort. They discovered that “most recent closed source models will not share their raw reasoning traces” and instead “use smaller language models to transcribe the chain of thought into summaries or compressed representations.”&lt;/p&gt;



&lt;p&gt;The study’s methodology included testing with modified versions of well-known problems to minimize the influence of memorized solutions, such as altering variables in mathematical competition problems from the American Invitational Mathematics Examination (AIME).&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015622" height="550" src="https://venturebeat.com/wp-content/uploads/2025/08/Image-one.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;Different AI models show varying relationships between computation and output, with some providers compressing reasoning traces while others provide full details. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-the-future-of-ai-efficiency-what-s-coming-next"&gt;The future of AI efficiency: What’s coming next&lt;/h2&gt;



&lt;p&gt;The researchers suggest that token efficiency should become a primary optimization target alongside accuracy for future model development. “A more densified CoT will also allow for more efficient context usage and may counter context degradation during challenging reasoning tasks,” they wrote.&lt;/p&gt;



&lt;p&gt;The release of OpenAI’s open-source gpt-oss models, which demonstrate state-of-the-art efficiency with “freely accessible CoT,” could serve as a reference point for optimizing other open-source models.&lt;/p&gt;



&lt;p&gt;The complete research dataset and evaluation code are available on GitHub, allowing other researchers to validate and extend the findings. As the AI industry races toward more powerful reasoning capabilities, this study suggests that the real competition may not be about who can build the smartest AI — but who can build the most efficient one.&lt;/p&gt;



&lt;p&gt;After all, in a world where every token counts, the most wasteful models may find themselves priced out of the market, regardless of how well they can think.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/that-cheap-open-source-ai-model-is-actually-burning-through-your-compute-budget/</guid><pubDate>Fri, 15 Aug 2025 01:24:49 +0000</pubDate></item></channel></rss>