<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 13 Nov 2025 18:31:48 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>[NEW] A new quantum toolkit for optimization (The latest research from Google)</title><link>https://research.google/blog/a-new-quantum-toolkit-for-optimization/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;Where does the quantum advantage come from?&lt;/h2&gt;&lt;p&gt;Taking a step back, we can ask why converting optimization problems into decoding problems should ever be advantageous in the first place? By understanding this more deeply, one could hope to gain intuition to guide the search for additional optimization problems on which quantum computers may provide advantage.&lt;/p&gt;&lt;p&gt;Both the optimization problems that we start with and the decoding problems that we convert them into are something called NP-hard problems. This suggests that it is impossible to efficiently find exact solutions to all instances of these problems, even with the help of quantum computers. By using quantum effects, DQI has converted one hard problem into another hard problem. How does this accomplish anything? The key is that the NP-hardness speaks to the difficulty of the &lt;i&gt;very hardest instances&lt;/i&gt; of a given problem. If the problem instances are restricted to have some additional structure, &lt;i&gt;this can make them easier&lt;/i&gt;. The promise of DQI is that certain kinds of structure may make the decoding problem much easier, without also making the optimization problem easier to solve using conventional computers.&lt;/p&gt;&lt;p&gt;In the OPI problem, the lattice that arises is algebraically structured; the components of the basis vectors, instead of being arbitrary, are obtained by raising a number to successively higher powers. This algebraic structure is reflected in both the original optimization problem (OPI) and the decoding problem that quantum computers can convert it into (Reed-Solomon decoding). This structure makes the decoding problem much easier, but as far as we can tell does not make the optimization problem easier for conventional computers. In this circumstance, the ability to convert the optimization problem into the decoding problem, using the power of quantum computing, provides advantage.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;Where does the quantum advantage come from?&lt;/h2&gt;&lt;p&gt;Taking a step back, we can ask why converting optimization problems into decoding problems should ever be advantageous in the first place? By understanding this more deeply, one could hope to gain intuition to guide the search for additional optimization problems on which quantum computers may provide advantage.&lt;/p&gt;&lt;p&gt;Both the optimization problems that we start with and the decoding problems that we convert them into are something called NP-hard problems. This suggests that it is impossible to efficiently find exact solutions to all instances of these problems, even with the help of quantum computers. By using quantum effects, DQI has converted one hard problem into another hard problem. How does this accomplish anything? The key is that the NP-hardness speaks to the difficulty of the &lt;i&gt;very hardest instances&lt;/i&gt; of a given problem. If the problem instances are restricted to have some additional structure, &lt;i&gt;this can make them easier&lt;/i&gt;. The promise of DQI is that certain kinds of structure may make the decoding problem much easier, without also making the optimization problem easier to solve using conventional computers.&lt;/p&gt;&lt;p&gt;In the OPI problem, the lattice that arises is algebraically structured; the components of the basis vectors, instead of being arbitrary, are obtained by raising a number to successively higher powers. This algebraic structure is reflected in both the original optimization problem (OPI) and the decoding problem that quantum computers can convert it into (Reed-Solomon decoding). This structure makes the decoding problem much easier, but as far as we can tell does not make the optimization problem easier for conventional computers. In this circumstance, the ability to convert the optimization problem into the decoding problem, using the power of quantum computing, provides advantage.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/a-new-quantum-toolkit-for-optimization/</guid><pubDate>Thu, 13 Nov 2025 07:27:00 +0000</pubDate></item><item><title>New data centre projects mark Anthropic’s biggest US expansion yet (AI News)</title><link>https://www.artificialintelligence-news.com/news/new-data-centre-projects-mark-anthropic-biggest-us-expansion/</link><description>&lt;p&gt;New US data centre projects in Texas and New York will receive $50 billion in new funding, part of a plan to grow US computing capacity for advanced AI work. The facilities, built with Fluidstack, are designed for Anthropic’s systems and will focus on power and efficiency needs that come with training and running large models across these data centre sites.&lt;/p&gt;&lt;p&gt;Fluidstack provides large GPU clusters to companies such as Meta, Midjourney, and Mistral. The partnership reflects a wider push across the tech industry this year, as many firms increase spending on US infrastructure while the Trump administration urges companies to build and invest inside the country. These moves show how much demand there is for US data centre capacity as AI workloads grow.&lt;/p&gt;&lt;p&gt;In January, President Donald Trump instructed his administration to craft an AI Action Plan aimed at making “America the world capital in artificial intelligence.” Several firms later outlined major AI and energy spending plans during Trump’s tech and AI summit in July, many of which involved expanding US data centre operations or securing more compute across the country.&lt;/p&gt;&lt;p&gt;The new sites are expected to bring about 800 full-time jobs and 2,400 construction jobs. They are scheduled to come online in phases through 2026 and are meant to support the goals in the AI Action Plan by strengthening domestic compute resources. Company leaders say they want these projects to create stable jobs and improve America’s position in AI research by adding more US data centre capacity.&lt;/p&gt;&lt;p&gt;The investment also comes at a time when lawmakers are paying closer attention to where high-end compute capacity sits and how much of it stays in the US. Anthropic’s growing US data centre footprint places the company among the largest builders of physical AI infrastructure in the country, reinforcing the push to keep more AI development rooted in the US rather than overseas.&lt;/p&gt;&lt;p&gt;“We’re getting closer to AI that can accelerate scientific discovery and help solve complex problems in ways that weren’t possible before. Realising that potential requires infrastructure that can support continued development at the frontier,” said Dario Amodei, CEO and co-founder of Anthropic. “These sites will help us build more capable AI systems that can drive those breakthroughs, while creating American jobs.”&lt;/p&gt;&lt;p&gt;Anthropic’s move comes as OpenAI builds out its own network. The ChatGPT maker has secured more than $1.4 trillion in long-term commitments through partners such as Nvidia, Broadcom, Oracle, and major cloud platforms like Microsoft, Google, and Amazon. The scale of these plans has raised questions about whether the US power grid and related industries can support such rapid expansion, especially as more firms compete for space, energy, and equipment tied to US data centre growth.&lt;/p&gt;&lt;p&gt;Anthropic says its growth has been driven by its technical staff, its focus on safety work, and its research on alignment and interpretability. Claude is now used by more than 300,000 business customers, and the number of large accounts—those producing more than $100,000 in yearly revenue—has grown almost seven times over the past year.&lt;/p&gt;&lt;p&gt;Internal projections reported by &lt;em&gt;The Wall Street Journal&lt;/em&gt; suggest the company expects to break even by 2028. OpenAI, by comparison, is said to be projecting $74 billion in operating losses that same year. To keep up with rising demand, Anthropic chose Fluidstack to build facilities tailored to its hardware needs, pointing to the company’s speed and its ability to deliver large-scale power capacity on tight timelines.&lt;/p&gt;&lt;p&gt;“We selected Fluidstack as our partner for its ability to move with exceptional agility, enabling rapid delivery of gigawatts of power,” an Anthropic leader said. Gary Wu, co-founder and CEO of Fluidstack, added: “Fluidstack was built for this moment. We’re proud to partner with frontier AI leaders like Anthropic to accelerate and deploy the infrastructure necessary to realise their vision.”&lt;/p&gt;&lt;p&gt;Anthropic says this level of spending is needed to support fast-rising usage while keeping its research momentum. The company also plans to focus on cost-efficient ways to scale.&lt;/p&gt;&lt;p&gt;Earlier this fall, the firm was valued at $183 billion. It is backed by Alphabet and Amazon, and a separate 1,200-acre data centre campus built for Anthropic by Amazon in Indiana is already in operation. That $11 billion site is running today, while many others in the sector are still in planning stages. Anthropic has also expanded its compute arrangement with Google by tens of billions of dollars.&lt;/p&gt;&lt;p&gt;These developments come as the federal government’s role in AI infrastructure funding becomes more contested. Last week, OpenAI asked the Trump administration to broaden a CHIPS Act tax credit so it would cover AI data centres and grid equipment such as transformers, according to a letter reported by &lt;em&gt;Bloomberg&lt;/em&gt;. The request followed criticism over earlier comments from CFO Sarah Friar, who mentioned the possibility of a government “backstop” for the company’s compute deals. OpenAI has since stepped away from the idea, but the episode highlighted ongoing uncertainty over how America’s AI infrastructure will be financed — and who will pay for it.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Scott Blake)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Google reveals its own version of Apple’s AI cloud&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110530" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-4.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;New US data centre projects in Texas and New York will receive $50 billion in new funding, part of a plan to grow US computing capacity for advanced AI work. The facilities, built with Fluidstack, are designed for Anthropic’s systems and will focus on power and efficiency needs that come with training and running large models across these data centre sites.&lt;/p&gt;&lt;p&gt;Fluidstack provides large GPU clusters to companies such as Meta, Midjourney, and Mistral. The partnership reflects a wider push across the tech industry this year, as many firms increase spending on US infrastructure while the Trump administration urges companies to build and invest inside the country. These moves show how much demand there is for US data centre capacity as AI workloads grow.&lt;/p&gt;&lt;p&gt;In January, President Donald Trump instructed his administration to craft an AI Action Plan aimed at making “America the world capital in artificial intelligence.” Several firms later outlined major AI and energy spending plans during Trump’s tech and AI summit in July, many of which involved expanding US data centre operations or securing more compute across the country.&lt;/p&gt;&lt;p&gt;The new sites are expected to bring about 800 full-time jobs and 2,400 construction jobs. They are scheduled to come online in phases through 2026 and are meant to support the goals in the AI Action Plan by strengthening domestic compute resources. Company leaders say they want these projects to create stable jobs and improve America’s position in AI research by adding more US data centre capacity.&lt;/p&gt;&lt;p&gt;The investment also comes at a time when lawmakers are paying closer attention to where high-end compute capacity sits and how much of it stays in the US. Anthropic’s growing US data centre footprint places the company among the largest builders of physical AI infrastructure in the country, reinforcing the push to keep more AI development rooted in the US rather than overseas.&lt;/p&gt;&lt;p&gt;“We’re getting closer to AI that can accelerate scientific discovery and help solve complex problems in ways that weren’t possible before. Realising that potential requires infrastructure that can support continued development at the frontier,” said Dario Amodei, CEO and co-founder of Anthropic. “These sites will help us build more capable AI systems that can drive those breakthroughs, while creating American jobs.”&lt;/p&gt;&lt;p&gt;Anthropic’s move comes as OpenAI builds out its own network. The ChatGPT maker has secured more than $1.4 trillion in long-term commitments through partners such as Nvidia, Broadcom, Oracle, and major cloud platforms like Microsoft, Google, and Amazon. The scale of these plans has raised questions about whether the US power grid and related industries can support such rapid expansion, especially as more firms compete for space, energy, and equipment tied to US data centre growth.&lt;/p&gt;&lt;p&gt;Anthropic says its growth has been driven by its technical staff, its focus on safety work, and its research on alignment and interpretability. Claude is now used by more than 300,000 business customers, and the number of large accounts—those producing more than $100,000 in yearly revenue—has grown almost seven times over the past year.&lt;/p&gt;&lt;p&gt;Internal projections reported by &lt;em&gt;The Wall Street Journal&lt;/em&gt; suggest the company expects to break even by 2028. OpenAI, by comparison, is said to be projecting $74 billion in operating losses that same year. To keep up with rising demand, Anthropic chose Fluidstack to build facilities tailored to its hardware needs, pointing to the company’s speed and its ability to deliver large-scale power capacity on tight timelines.&lt;/p&gt;&lt;p&gt;“We selected Fluidstack as our partner for its ability to move with exceptional agility, enabling rapid delivery of gigawatts of power,” an Anthropic leader said. Gary Wu, co-founder and CEO of Fluidstack, added: “Fluidstack was built for this moment. We’re proud to partner with frontier AI leaders like Anthropic to accelerate and deploy the infrastructure necessary to realise their vision.”&lt;/p&gt;&lt;p&gt;Anthropic says this level of spending is needed to support fast-rising usage while keeping its research momentum. The company also plans to focus on cost-efficient ways to scale.&lt;/p&gt;&lt;p&gt;Earlier this fall, the firm was valued at $183 billion. It is backed by Alphabet and Amazon, and a separate 1,200-acre data centre campus built for Anthropic by Amazon in Indiana is already in operation. That $11 billion site is running today, while many others in the sector are still in planning stages. Anthropic has also expanded its compute arrangement with Google by tens of billions of dollars.&lt;/p&gt;&lt;p&gt;These developments come as the federal government’s role in AI infrastructure funding becomes more contested. Last week, OpenAI asked the Trump administration to broaden a CHIPS Act tax credit so it would cover AI data centres and grid equipment such as transformers, according to a letter reported by &lt;em&gt;Bloomberg&lt;/em&gt;. The request followed criticism over earlier comments from CFO Sarah Friar, who mentioned the possibility of a government “backstop” for the company’s compute deals. OpenAI has since stepped away from the idea, but the episode highlighted ongoing uncertainty over how America’s AI infrastructure will be financed — and who will pay for it.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Scott Blake)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Google reveals its own version of Apple’s AI cloud&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110530" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-4.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/new-data-centre-projects-mark-anthropic-biggest-us-expansion/</guid><pubDate>Thu, 13 Nov 2025 10:00:00 +0000</pubDate></item><item><title>Google is still aiming for its “moonshot” 2030 energy goals (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/13/1127896/google-energy-goals/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/EmTech_110525_001-crop.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Last week, we hosted EmTech MIT, MIT Technology Review’s annual flagship conference in Cambridge, Massachusetts. Over the course of three days of main-stage sessions, I learned about innovations in AI, biotech, and robotics.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But as you might imagine, some of this climate reporter’s favorite moments came in the climate sessions. I was listening especially closely to my colleague James Temple’s discussion with Lucia Tian, head of advanced energy technologies at Google.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;They spoke about the tech giant’s growing energy demand and what sort of technologies the company is looking to to help meet it. In case you weren’t able to join us, let’s dig into that session and consider how the company is thinking about energy in the face of AI’s rapid rise.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I’ve been closely following Google’s work in energy this year. Like the rest of the tech industry, the company is seeing ballooning electricity demand in its data centers. That could get in the way of a major goal that Google has been talking about for years.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;See, back in 2020, the company announced an ambitious target: by 2030, it aimed to run on carbon-free energy 24-7. Basically, that means Google would purchase enough renewable energy on the grids where it operates to meet its entire electricity demand, and the purchases would match up so the electricity would have to be generated when the company was actually using energy. (For more on the nuances of Big Tech’s renewable-energy pledges, check out James’s piece from last year.)&lt;/p&gt;  &lt;p&gt;Google’s is an ambitious goal, and on stage, Tian said that the company is still aiming for it but acknowledged that it’s looking tough with the rise of AI.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“It was always a moonshot,” she said. “It’s something very, very hard to achieve, and it’s only harder in the face of this growth. But our perspective is, if we don’t move in that direction, we’ll never get there.”&lt;/p&gt;  &lt;p&gt;Google’s total electricity demand more than doubled from 2020 to 2024, according to its latest Environmental Report. As for that goal of 24-7 carbon-free energy? The company is basically treading water. While it was at 67% for its data centers in 2020, last year it came in at 66%.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Not going backwards is something of an accomplishment, given the rapid growth in electricity demand. But it still leaves the company some distance away from its finish line.&lt;/p&gt;  &lt;p&gt;To close the gap, Google has been signing what feels like constant deals in the energy space. Two recent announcements that Tian talked about on stage were a project involving carbon capture and storage at a natural-gas plant in Illinois and plans to reopen a shuttered nuclear power plant in Iowa.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Let’s start with carbon capture. Google signed an agreement to purchase most of the electricity from a new natural-gas plant, which will capture and store about 90% of its carbon dioxide emissions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That announcement was controversial, with critics arguing that carbon capture keeps fossil-fuel infrastructure online longer and still releases greenhouse gases and other pollutants into the atmosphere.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;One question that James raised on stage: Why build a new natural-gas plant rather than add equipment to an already existing facility? Tacking on equipment to an operational plant would mean cutting emissions from the status quo, rather than adding entirely new fossil-fuel infrastructure.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The company did consider many existing plants, Tian said. But, as she put it, “Retrofits aren’t going to make sense everywhere.” Space can be limited at existing plants, for example, and many may not have the right geology to store carbon dioxide underground.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;“We wanted to lead with a project that could prove this technology at scale,” Tian said. This site has an operational Class VI well, the type used for permanent sequestration, she added, and it also doesn’t require a big pipeline buildout.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;Tian also touched on the company’s recent announcement that it’s collaborating with NextEra Energy to reopen Duane Arnold Energy Center, a nuclear power plant in Iowa. The company will purchase electricity from that plant, which is scheduled to reopen in 2029.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As I covered in a story earlier this year, Duane Arnold was basically the final option in the US for companies looking to reopen shuttered nuclear power plants. “Just a few years back, we were still closing down nuclear plants in this country,” Tian said on stage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While each reopening will look a little different, Tian highlighted the groups working to restart the Palisades plant in Michigan, which was the first reopening to be announced, last spring. “They’re the real heroes of the story,” she said.&lt;/p&gt;  &lt;p&gt;I’m always interested to get a peek behind the curtain at how Big Tech is thinking about energy. I’m skeptical but certainly interested to see how Google’s, and the rest of the industry’s, goals shape up over the next few years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/EmTech_110525_001-crop.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Last week, we hosted EmTech MIT, MIT Technology Review’s annual flagship conference in Cambridge, Massachusetts. Over the course of three days of main-stage sessions, I learned about innovations in AI, biotech, and robotics.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But as you might imagine, some of this climate reporter’s favorite moments came in the climate sessions. I was listening especially closely to my colleague James Temple’s discussion with Lucia Tian, head of advanced energy technologies at Google.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;They spoke about the tech giant’s growing energy demand and what sort of technologies the company is looking to to help meet it. In case you weren’t able to join us, let’s dig into that session and consider how the company is thinking about energy in the face of AI’s rapid rise.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I’ve been closely following Google’s work in energy this year. Like the rest of the tech industry, the company is seeing ballooning electricity demand in its data centers. That could get in the way of a major goal that Google has been talking about for years.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;See, back in 2020, the company announced an ambitious target: by 2030, it aimed to run on carbon-free energy 24-7. Basically, that means Google would purchase enough renewable energy on the grids where it operates to meet its entire electricity demand, and the purchases would match up so the electricity would have to be generated when the company was actually using energy. (For more on the nuances of Big Tech’s renewable-energy pledges, check out James’s piece from last year.)&lt;/p&gt;  &lt;p&gt;Google’s is an ambitious goal, and on stage, Tian said that the company is still aiming for it but acknowledged that it’s looking tough with the rise of AI.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“It was always a moonshot,” she said. “It’s something very, very hard to achieve, and it’s only harder in the face of this growth. But our perspective is, if we don’t move in that direction, we’ll never get there.”&lt;/p&gt;  &lt;p&gt;Google’s total electricity demand more than doubled from 2020 to 2024, according to its latest Environmental Report. As for that goal of 24-7 carbon-free energy? The company is basically treading water. While it was at 67% for its data centers in 2020, last year it came in at 66%.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Not going backwards is something of an accomplishment, given the rapid growth in electricity demand. But it still leaves the company some distance away from its finish line.&lt;/p&gt;  &lt;p&gt;To close the gap, Google has been signing what feels like constant deals in the energy space. Two recent announcements that Tian talked about on stage were a project involving carbon capture and storage at a natural-gas plant in Illinois and plans to reopen a shuttered nuclear power plant in Iowa.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Let’s start with carbon capture. Google signed an agreement to purchase most of the electricity from a new natural-gas plant, which will capture and store about 90% of its carbon dioxide emissions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That announcement was controversial, with critics arguing that carbon capture keeps fossil-fuel infrastructure online longer and still releases greenhouse gases and other pollutants into the atmosphere.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;One question that James raised on stage: Why build a new natural-gas plant rather than add equipment to an already existing facility? Tacking on equipment to an operational plant would mean cutting emissions from the status quo, rather than adding entirely new fossil-fuel infrastructure.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The company did consider many existing plants, Tian said. But, as she put it, “Retrofits aren’t going to make sense everywhere.” Space can be limited at existing plants, for example, and many may not have the right geology to store carbon dioxide underground.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;“We wanted to lead with a project that could prove this technology at scale,” Tian said. This site has an operational Class VI well, the type used for permanent sequestration, she added, and it also doesn’t require a big pipeline buildout.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;Tian also touched on the company’s recent announcement that it’s collaborating with NextEra Energy to reopen Duane Arnold Energy Center, a nuclear power plant in Iowa. The company will purchase electricity from that plant, which is scheduled to reopen in 2029.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As I covered in a story earlier this year, Duane Arnold was basically the final option in the US for companies looking to reopen shuttered nuclear power plants. “Just a few years back, we were still closing down nuclear plants in this country,” Tian said on stage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While each reopening will look a little different, Tian highlighted the groups working to restart the Palisades plant in Michigan, which was the first reopening to be announced, last spring. “They’re the real heroes of the story,” she said.&lt;/p&gt;  &lt;p&gt;I’m always interested to get a peek behind the curtain at how Big Tech is thinking about energy. I’m skeptical but certainly interested to see how Google’s, and the rest of the industry’s, goals shape up over the next few years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/13/1127896/google-energy-goals/</guid><pubDate>Thu, 13 Nov 2025 11:00:00 +0000</pubDate></item><item><title>Milestone raises $10M to make sure AI rhymes with ROI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/milestone-raises-10m-to-make-sure-ai-rhymes-with-roi/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Milestone-Group.png?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Generative AI has become a key part of coding workflows, but most companies struggle to track its use, let alone its return on investment. Israeli startup Milestone hopes to help with a platform designed to correlate AI tool usage with engineering metrics, including code quality.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The catch is that these companies have to give Milestone access to their codebases, a bet that investors initially questioned, CEO and co-founder Liad Elidan told TechCrunch. But with customers, including Kayak, Monday, and Sapiens, the startup has now raised a $10 million seed funding round led by San Francisco-based venture firm Heavybit and Israeli fund Hanaco Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In an unusual arrangement, Elidan and Milestone’s CTO, Professor Stephen Barrett, had gone years without meeting in person by the time they started fundraising. Unlike most of Milestone’s Israel-based team members, Barrett lives in Ireland and teaches computer science at Trinity College Dublin, where Elidan was once his student, and the two bonded through software projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the distance, the duo kept in touch over the years and eventually decided to found a startup focused on engineering efficiency, just as coding assistants and other code-generation tools were taking off. GitHub Copilot has since crossed the bar of 20 million users, but companies still lack visibility into how these tools are used and are impacting productivity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Elidan, Milestone answers these questions by relying on four pillars — codebases, project management platforms, team structure, and the codegen tools themselves — to create what he describes as “a GenAI data lake.” In practice, this gives organizations actionable data on which teams use AI and to what effect — thanks to their own information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Armed with this data, managers under constant pressure to leverage AI to push productivity are, for instance, able to measure feature delivery speed, Elidan said, and can find out whether recent bugs were caused by AI-generated code and can make informed decisions on where to implement these tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This also gives Milestone a front-row seat on ROI — the “holy grail question” that it aims to answer granularly for its customers. But on a high level, he said, “We don’t have a customer that used Milestone and said, ‘Okay, GenAI doesn’t help me, I’m going to revoke all my licenses.’ It’s actually the opposite. They want to try more Gen AI tools.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This fast-paced adoption also means that Milestone has to keep up with a rapidly evolving landscape. “It used to be auto-completes, then it was chat, then it was agentic-based chat, and it keeps going,” Elidan said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s also where Barrett’s academic background helps the team understand the wave of transformation its customers are going through. “A lot of the ways we used to think about engineering are going to have to change,” the professor told TechCrunch. “I think in some sense, AI is filling out the team, and engineers are now becoming managers.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To keep up with the tools that power this wave, Milestone says it has partnered with many vendors, such as GitHub, Augment Code, Qodo, Continue, and Atlassian — the company that powers Jira and whose venture arm Atlassian Ventures also participated in this seed round.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The round was also supported by angel investors, including GitHub co-founder Tom Preston-Werner, former AT&amp;amp;T CEO John Donovan, Accenture’s senior tech advisor Paul Daugherty, and Datadog’s ex-president Amit Agrawal — who all understand that what Milestone is building is relevant for the enterprise market, Elidan said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That enterprise focus was deliberate from day one, with Milestone even saying no to prospective clients that were too small — “a very hard thing to do,” Elidan said, but one that gave the startup clarity around a roadmap requiring enterprise credentials and features. Focus would be his main advice to other founders, and Milestone is taking it: The startup won’t expand into measuring GenAI’s impact on marketing or other functions, even as it grows.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Milestone-Group.png?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Generative AI has become a key part of coding workflows, but most companies struggle to track its use, let alone its return on investment. Israeli startup Milestone hopes to help with a platform designed to correlate AI tool usage with engineering metrics, including code quality.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The catch is that these companies have to give Milestone access to their codebases, a bet that investors initially questioned, CEO and co-founder Liad Elidan told TechCrunch. But with customers, including Kayak, Monday, and Sapiens, the startup has now raised a $10 million seed funding round led by San Francisco-based venture firm Heavybit and Israeli fund Hanaco Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In an unusual arrangement, Elidan and Milestone’s CTO, Professor Stephen Barrett, had gone years without meeting in person by the time they started fundraising. Unlike most of Milestone’s Israel-based team members, Barrett lives in Ireland and teaches computer science at Trinity College Dublin, where Elidan was once his student, and the two bonded through software projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the distance, the duo kept in touch over the years and eventually decided to found a startup focused on engineering efficiency, just as coding assistants and other code-generation tools were taking off. GitHub Copilot has since crossed the bar of 20 million users, but companies still lack visibility into how these tools are used and are impacting productivity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Elidan, Milestone answers these questions by relying on four pillars — codebases, project management platforms, team structure, and the codegen tools themselves — to create what he describes as “a GenAI data lake.” In practice, this gives organizations actionable data on which teams use AI and to what effect — thanks to their own information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Armed with this data, managers under constant pressure to leverage AI to push productivity are, for instance, able to measure feature delivery speed, Elidan said, and can find out whether recent bugs were caused by AI-generated code and can make informed decisions on where to implement these tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This also gives Milestone a front-row seat on ROI — the “holy grail question” that it aims to answer granularly for its customers. But on a high level, he said, “We don’t have a customer that used Milestone and said, ‘Okay, GenAI doesn’t help me, I’m going to revoke all my licenses.’ It’s actually the opposite. They want to try more Gen AI tools.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This fast-paced adoption also means that Milestone has to keep up with a rapidly evolving landscape. “It used to be auto-completes, then it was chat, then it was agentic-based chat, and it keeps going,” Elidan said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s also where Barrett’s academic background helps the team understand the wave of transformation its customers are going through. “A lot of the ways we used to think about engineering are going to have to change,” the professor told TechCrunch. “I think in some sense, AI is filling out the team, and engineers are now becoming managers.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To keep up with the tools that power this wave, Milestone says it has partnered with many vendors, such as GitHub, Augment Code, Qodo, Continue, and Atlassian — the company that powers Jira and whose venture arm Atlassian Ventures also participated in this seed round.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The round was also supported by angel investors, including GitHub co-founder Tom Preston-Werner, former AT&amp;amp;T CEO John Donovan, Accenture’s senior tech advisor Paul Daugherty, and Datadog’s ex-president Amit Agrawal — who all understand that what Milestone is building is relevant for the enterprise market, Elidan said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That enterprise focus was deliberate from day one, with Milestone even saying no to prospective clients that were too small — “a very hard thing to do,” Elidan said, but one that gave the startup clarity around a roadmap requiring enterprise credentials and features. Focus would be his main advice to other founders, and Milestone is taking it: The startup won’t expand into measuring GenAI’s impact on marketing or other functions, even as it grows.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/milestone-raises-10m-to-make-sure-ai-rhymes-with-roi/</guid><pubDate>Thu, 13 Nov 2025 12:00:00 +0000</pubDate></item><item><title>Microsoft’s plan to fix its chip problem is, partly, to let OpenAI do the heavy lifting (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/microsofts-plan-to-fix-its-chip-problem-is-partly-to-let-openai-do-the-heavy-lifting/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2017/07/gettyimages-675949754.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft is taking a page from OpenAI’s playbook, literally. Bloomberg first reported that the tech giant plans to leverage its partner’s custom chip development to bolster its own struggling semiconductor efforts, a move that looks increasingly pragmatic given Microsoft’s lackluster performance compared to rivals like Google and Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The arrangement is straightforward: OpenAI is designing AI chips with Broadcom, and Microsoft gets full access to the innovations. “As they innovate even at the system level, we get access to all of it,” CEO Satya Nadella explained on a newly released interview with podcaster Dwarkesh Patel, describing plans to adopt OpenAI’s designs and then extend them for Microsoft’s own purposes.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Under a revised partnership agreement, Microsoft secured intellectual property rights to OpenAI’s chip designs while maintaining access to the company’s AI models through 2032. The only carve-out? OpenAI’s consumer hardware, which the ChatGPT maker presumably wants to develop and sell independently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The collaboration underscores a broader reality in tech: Building cutting-edge AI chips is brutally difficult and expensive. Rather than continuing to struggle alone, Microsoft is betting that OpenAI’s expertise — plus a smartly structured contract — can accelerate its own ambitions.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2017/07/gettyimages-675949754.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft is taking a page from OpenAI’s playbook, literally. Bloomberg first reported that the tech giant plans to leverage its partner’s custom chip development to bolster its own struggling semiconductor efforts, a move that looks increasingly pragmatic given Microsoft’s lackluster performance compared to rivals like Google and Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The arrangement is straightforward: OpenAI is designing AI chips with Broadcom, and Microsoft gets full access to the innovations. “As they innovate even at the system level, we get access to all of it,” CEO Satya Nadella explained on a newly released interview with podcaster Dwarkesh Patel, describing plans to adopt OpenAI’s designs and then extend them for Microsoft’s own purposes.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Under a revised partnership agreement, Microsoft secured intellectual property rights to OpenAI’s chip designs while maintaining access to the company’s AI models through 2032. The only carve-out? OpenAI’s consumer hardware, which the ChatGPT maker presumably wants to develop and sell independently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The collaboration underscores a broader reality in tech: Building cutting-edge AI chips is brutally difficult and expensive. Rather than continuing to struggle alone, Microsoft is betting that OpenAI’s expertise — plus a smartly structured contract — can accelerate its own ambitions.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/microsofts-plan-to-fix-its-chip-problem-is-partly-to-let-openai-do-the-heavy-lifting/</guid><pubDate>Thu, 13 Nov 2025 12:01:20 +0000</pubDate></item><item><title>[NEW] The Download: AI to measure pain, and how to deal with conspiracy theorists (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/13/1127911/the-download-ai-to-measure-pain-and-how-to-deal-with-conspiracy-theorists/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI is changing how we quantify pain&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Researchers around the world are racing to turn pain—medicine’s most subjective vital sign—into something a camera or sensor can score as reliably as blood pressure.&lt;/p&gt;&lt;p&gt;The push has already produced PainChek—a smartphone app that scans people's faces for tiny muscle movements and uses artificial intelligence to output a pain score—which has been cleared by regulators on three continents and has logged more than 10 million pain assessments. Other startups are beginning to make similar inroads.&lt;/p&gt;  &lt;p&gt;The way we assess pain may finally be shifting, but when algorithms measure our suffering, does that change the way we treat it? Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Deena Mousa&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the latest print issue of MIT Technology Review magazine, which is full of fascinating stories about our bodies. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How to help friends and family dig out of a conspiracy theory black hole&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Niall Firth&lt;/em&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Someone I know became a conspiracy theorist seemingly overnight.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;It was during the pandemic. They suddenly started posting daily on Facebook about the dangers of covid vaccines and masks, warning of an attempt to control us.&lt;/p&gt;&lt;p&gt;As a science and technology journalist, I felt that my duty was to respond. I tried, but all I got was derision. Even now I still wonder: Are there things I could have done differently to talk them back down and help them see sense?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I gave Sander van der Linden, professor of social psychology in society at the University of Cambridge, a call to ask: What would he advise if family members or friends show signs of having fallen down the rabbit hole? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is part of MIT Technology Review’s series “The New Conspiracy Age,” on how the present boom in conspiracy theories is reshaping science and technology. Check out &lt;/strong&gt;&lt;strong&gt;the rest of the series here&lt;/strong&gt;&lt;strong&gt;. It’s also part of our &lt;/strong&gt;&lt;strong&gt;How To series&lt;/strong&gt;&lt;strong&gt;, giving you practical advice to help you get things done.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you’re interested in hearing more about how to survive in the age of conspiracies, join our features editor Amanda Silverman and executive editor Niall Firth for a subscriber-exclusive Roundtable conversation with conspiracy expert Mike Rothschild. It’s at 1pm ET on Thursday November 20—&lt;/strong&gt;&lt;strong&gt;register now to join us&lt;/strong&gt;&lt;strong&gt;!&lt;/strong&gt;&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Google is still aiming for its “moonshot” 2030 energy goals&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Last week, we hosted EmTech MIT, MIT Technology Review’s annual flagship conference in Cambridge, Massachusetts. As you might imagine, some of this climate reporter’s favorite moments came in the climate sessions. I was listening especially closely to my colleague James Temple’s discussion with Lucia Tian, head of advanced energy technologies at Google.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;They spoke about the tech giant’s growing energy demand and what sort of technologies the company is looking to to help meet it. In case you weren’t able to join us, let’s dig into that session and consider how the company is thinking about energy in the face of AI’s rapid rise. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 ChatGPT is now “warmer and more conversational”&lt;/strong&gt;&lt;br /&gt;But it’s also slightly more willing to discuss sexual and violent content. (The Register)&lt;br /&gt;+ &lt;em&gt;ChatGPT has a very specific writing style. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;The looming crackdown on AI companionship. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2 The US could deny visas to visitors with obesity, cancer or diabetes&lt;br /&gt;&lt;/strong&gt;As part of its ongoing efforts to stem the flow of people trying to enter the country. (WP $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Microsoft is planning to create its own AI chip&lt;/strong&gt;&lt;br /&gt;And it’s going to use OpenAI’s internal chip-building plans to do it. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The company is working on a colossal data center in Atlanta. &lt;/em&gt;(WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Early AI agent adopters are convinced they’ll see a return on their investment soon&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Mind you, they &lt;em&gt;would &lt;/em&gt;say that. (WSJ $)&lt;br /&gt;+ &lt;em&gt;An AI adoption riddle. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Waymo’s robotaxis are hitting American highways&lt;br /&gt;Until now, they’ve typically gone out of their way to avoid them. (The Verge)&lt;br /&gt;+ &lt;em&gt;Its vehicles will now reach speeds of up to 65 miles per hour. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;Waymo is proving long-time detractor Elon Musk wrong. &lt;/em&gt;(Insider $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;6 A new Russian unit is hunting down Ukraine’s drone operators&lt;br /&gt;&lt;/strong&gt;It’s tasked with killing the pilots behind Ukraine’s successful attacks. (FT $)&lt;br /&gt;+ &lt;em&gt;US startup Anduril wants to build drones in the UAE. &lt;/em&gt;(Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Meet the radio-obsessed civilian shaping Ukraine’s drone defense. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Anthropic’s Claude successfully controlled a robot dog&lt;br /&gt;&lt;/strong&gt;It’s important to know what AI models may do when given access to physical systems. (Wired $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Grok briefly claimed Donald Trump won the 2020 US election&lt;br /&gt;&lt;/strong&gt;As reliable as ever, I see. (The Guardian)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 The Northern Lights are playing havoc with satellites&lt;/strong&gt;&lt;br /&gt;Solar storms may look spectacular, but they make it harder to keep tabs on space. (NYT $)&lt;br /&gt;+ &lt;em&gt;Seriously though, they look amazing. &lt;/em&gt;(The Atlantic $)&lt;br /&gt;+ &lt;em&gt;NASA’s new AI model can predict when a solar storm may strike. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 Apple users can now use digital versions of their passports&lt;/strong&gt;&lt;br /&gt;But it’s strictly for internal flights within the US only. (TechCrunch)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“I hope this mistake will turn into an experience.”&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;—Vladimir Vitukhin, chief executive of the company behind Russia’s first anthropomorphic robot AIDOL, offers a philosophical response to the machine falling flat on its face during a reveal event, the New York Times reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127915" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/image_337440.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Welcome to the oldest part of the metaverse&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Headlines treat the metaverse as a hazy dream yet to be built. But if it’s defined as a network of virtual worlds we can inhabit, its oldest corner has been already running for 25 years.&lt;/p&gt;&lt;p&gt;It’s a medieval fantasy kingdom created for the online role-playing game Ultima Online. It was the first to simulate an entire world: a vast, dynamic realm where players could interact with almost anything, from fruit on trees to books on shelves.&lt;/p&gt;&lt;p&gt;Ultima Online has already endured a quarter-century of market competition, economic turmoil, and political strife. So what can this game and its players tell us about creating the virtual worlds of the future? Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—John-Clark Levin&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Unlikely duo Sting and Shaggy are starring together in a New York musical.&lt;br /&gt;+ Barry Manilow was almost in &lt;em&gt;Airplane!&lt;/em&gt;&lt;em&gt;?&lt;/em&gt; That would be an entirely different kind of flying, altogether ✈️&lt;br /&gt;+ What makes someone sexy? Well, that depends.&lt;br /&gt;+ Keep an eye on those pink dolphins, they’re notorious thieves.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI is changing how we quantify pain&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Researchers around the world are racing to turn pain—medicine’s most subjective vital sign—into something a camera or sensor can score as reliably as blood pressure.&lt;/p&gt;&lt;p&gt;The push has already produced PainChek—a smartphone app that scans people's faces for tiny muscle movements and uses artificial intelligence to output a pain score—which has been cleared by regulators on three continents and has logged more than 10 million pain assessments. Other startups are beginning to make similar inroads.&lt;/p&gt;  &lt;p&gt;The way we assess pain may finally be shifting, but when algorithms measure our suffering, does that change the way we treat it? Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Deena Mousa&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the latest print issue of MIT Technology Review magazine, which is full of fascinating stories about our bodies. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How to help friends and family dig out of a conspiracy theory black hole&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Niall Firth&lt;/em&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Someone I know became a conspiracy theorist seemingly overnight.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;It was during the pandemic. They suddenly started posting daily on Facebook about the dangers of covid vaccines and masks, warning of an attempt to control us.&lt;/p&gt;&lt;p&gt;As a science and technology journalist, I felt that my duty was to respond. I tried, but all I got was derision. Even now I still wonder: Are there things I could have done differently to talk them back down and help them see sense?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I gave Sander van der Linden, professor of social psychology in society at the University of Cambridge, a call to ask: What would he advise if family members or friends show signs of having fallen down the rabbit hole? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is part of MIT Technology Review’s series “The New Conspiracy Age,” on how the present boom in conspiracy theories is reshaping science and technology. Check out &lt;/strong&gt;&lt;strong&gt;the rest of the series here&lt;/strong&gt;&lt;strong&gt;. It’s also part of our &lt;/strong&gt;&lt;strong&gt;How To series&lt;/strong&gt;&lt;strong&gt;, giving you practical advice to help you get things done.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you’re interested in hearing more about how to survive in the age of conspiracies, join our features editor Amanda Silverman and executive editor Niall Firth for a subscriber-exclusive Roundtable conversation with conspiracy expert Mike Rothschild. It’s at 1pm ET on Thursday November 20—&lt;/strong&gt;&lt;strong&gt;register now to join us&lt;/strong&gt;&lt;strong&gt;!&lt;/strong&gt;&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Google is still aiming for its “moonshot” 2030 energy goals&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Last week, we hosted EmTech MIT, MIT Technology Review’s annual flagship conference in Cambridge, Massachusetts. As you might imagine, some of this climate reporter’s favorite moments came in the climate sessions. I was listening especially closely to my colleague James Temple’s discussion with Lucia Tian, head of advanced energy technologies at Google.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;They spoke about the tech giant’s growing energy demand and what sort of technologies the company is looking to to help meet it. In case you weren’t able to join us, let’s dig into that session and consider how the company is thinking about energy in the face of AI’s rapid rise. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, MIT Technology Review’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 ChatGPT is now “warmer and more conversational”&lt;/strong&gt;&lt;br /&gt;But it’s also slightly more willing to discuss sexual and violent content. (The Register)&lt;br /&gt;+ &lt;em&gt;ChatGPT has a very specific writing style. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;The looming crackdown on AI companionship. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2 The US could deny visas to visitors with obesity, cancer or diabetes&lt;br /&gt;&lt;/strong&gt;As part of its ongoing efforts to stem the flow of people trying to enter the country. (WP $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Microsoft is planning to create its own AI chip&lt;/strong&gt;&lt;br /&gt;And it’s going to use OpenAI’s internal chip-building plans to do it. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The company is working on a colossal data center in Atlanta. &lt;/em&gt;(WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Early AI agent adopters are convinced they’ll see a return on their investment soon&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Mind you, they &lt;em&gt;would &lt;/em&gt;say that. (WSJ $)&lt;br /&gt;+ &lt;em&gt;An AI adoption riddle. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Waymo’s robotaxis are hitting American highways&lt;br /&gt;Until now, they’ve typically gone out of their way to avoid them. (The Verge)&lt;br /&gt;+ &lt;em&gt;Its vehicles will now reach speeds of up to 65 miles per hour. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;Waymo is proving long-time detractor Elon Musk wrong. &lt;/em&gt;(Insider $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;6 A new Russian unit is hunting down Ukraine’s drone operators&lt;br /&gt;&lt;/strong&gt;It’s tasked with killing the pilots behind Ukraine’s successful attacks. (FT $)&lt;br /&gt;+ &lt;em&gt;US startup Anduril wants to build drones in the UAE. &lt;/em&gt;(Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Meet the radio-obsessed civilian shaping Ukraine’s drone defense. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Anthropic’s Claude successfully controlled a robot dog&lt;br /&gt;&lt;/strong&gt;It’s important to know what AI models may do when given access to physical systems. (Wired $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Grok briefly claimed Donald Trump won the 2020 US election&lt;br /&gt;&lt;/strong&gt;As reliable as ever, I see. (The Guardian)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 The Northern Lights are playing havoc with satellites&lt;/strong&gt;&lt;br /&gt;Solar storms may look spectacular, but they make it harder to keep tabs on space. (NYT $)&lt;br /&gt;+ &lt;em&gt;Seriously though, they look amazing. &lt;/em&gt;(The Atlantic $)&lt;br /&gt;+ &lt;em&gt;NASA’s new AI model can predict when a solar storm may strike. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 Apple users can now use digital versions of their passports&lt;/strong&gt;&lt;br /&gt;But it’s strictly for internal flights within the US only. (TechCrunch)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“I hope this mistake will turn into an experience.”&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;—Vladimir Vitukhin, chief executive of the company behind Russia’s first anthropomorphic robot AIDOL, offers a philosophical response to the machine falling flat on its face during a reveal event, the New York Times reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127915" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/image_337440.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Welcome to the oldest part of the metaverse&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Headlines treat the metaverse as a hazy dream yet to be built. But if it’s defined as a network of virtual worlds we can inhabit, its oldest corner has been already running for 25 years.&lt;/p&gt;&lt;p&gt;It’s a medieval fantasy kingdom created for the online role-playing game Ultima Online. It was the first to simulate an entire world: a vast, dynamic realm where players could interact with almost anything, from fruit on trees to books on shelves.&lt;/p&gt;&lt;p&gt;Ultima Online has already endured a quarter-century of market competition, economic turmoil, and political strife. So what can this game and its players tell us about creating the virtual worlds of the future? Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—John-Clark Levin&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Unlikely duo Sting and Shaggy are starring together in a New York musical.&lt;br /&gt;+ Barry Manilow was almost in &lt;em&gt;Airplane!&lt;/em&gt;&lt;em&gt;?&lt;/em&gt; That would be an entirely different kind of flying, altogether ✈️&lt;br /&gt;+ What makes someone sexy? Well, that depends.&lt;br /&gt;+ Keep an eye on those pink dolphins, they’re notorious thieves.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/13/1127911/the-download-ai-to-measure-pain-and-how-to-deal-with-conspiracy-theorists/</guid><pubDate>Thu, 13 Nov 2025 13:10:00 +0000</pubDate></item><item><title>[NEW] IBM: Data silos are holding back enterprise AI (AI News)</title><link>https://www.artificialintelligence-news.com/news/ibm-data-silos-are-holding-back-enterprise-ai/</link><description>&lt;p&gt;According to IBM, the primary barrier holding back enterprise AI isn’t the technology itself but the persistent issue of data silos.&lt;/p&gt;&lt;p&gt;Ed Lovely, VP and Chief Data Officer at IBM, describes data silos as the “Achilles’ heel” of modern data strategy. Lovely made the comments following the release of a new study from the IBM Institute for Business Value that found AI is ready to scale, but enterprise data is not.&lt;/p&gt;&lt;p&gt;The report, which surveyed 1,700 senior data leaders, found that functional data remains stubbornly isolated. Finance, HR, marketing, and supply chain data all operate in isolation, with no common taxonomy or shared standards.&lt;/p&gt;&lt;p&gt;This fragmentation is having a direct, negative impact on AI projects. “When data lives in disconnected silos, every AI initiative becomes a drawn-out, six-to-twelve-month data cleansing project,” said Ed Lovely, VP and Chief Data Officer at IBM. “Teams spend more time hunting for and aligning data than generating meaningful insights”.&lt;/p&gt;&lt;p&gt;This is a direct threat to competitive advantage. For CIOs and CDOs, the mission is no longer just to collect and protect data, but to deploy it effectively to power these new AI systems.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-data-janitor-to-value-driver"&gt;From data janitor to value driver&lt;/h3&gt;&lt;p&gt;The consensus from the study is that data leaders must be relentlessly focused on business outcomes, with 92 percent of CDOs agreeing their success depends on this focus.&lt;/p&gt;&lt;p&gt;Herein lies the central tension: while 92 percent are aiming for business value, only 29 percent are confident they have “clear measures to determine the business value of data-driven outcomes.”&lt;/p&gt;&lt;p&gt;This gap between ambition and reality is where AI agents that can learn and act autonomously to achieve goals are expected to help. Leaders are showing a growing confidence in these tools, with 83 percent of CDOs in IBM’s research stating the potential benefits of deploying AI agents outweigh the risks.&lt;/p&gt;&lt;p&gt;At global medical technology company Medtronic, teams were bogged down matching invoices, purchase orders, and proofs of delivery. By deploying an AI solution, the company automated this workflow. The result was a drop in document matching time from 20 minutes per invoice to just eight seconds, with an accuracy rate exceeding 99 percent. This allowed staff to be redeployed from low-value data entry to higher-value work.&lt;/p&gt;&lt;p&gt;Similarly, renewable energy company Matrix Renewables implemented a centralised data platform to monitor its assets. This led to a 75 percent reduction in reporting time and a 10 percent reduction in costly downtime.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ibm-finds-the-ai-hurdles-architecture-governance-and-the-talent-gap"&gt;IBM finds the AI hurdles: Architecture, governance, and the talent gap&lt;/h3&gt;&lt;p&gt;Achieving these results requires a new approach to data architecture while avoiding silos. The old model of costly, slow data relocation into a central lake is being replaced. IBM’s study finds 81 percent of CDOs now practice bringing AI to the data, rather than moving data to AI.&lt;/p&gt;&lt;p&gt;This approach relies on modern architectural patterns like data mesh and data fabric, which provide a virtualised layer to access data where it lives. It also champions the concept of “data products” (packaged, reusable data assets designed for a specific business purpose, such as a “customer 360” view or a financial forecast dataset.)&lt;/p&gt;&lt;p&gt;However, making data more accessible introduces governance challenges. The CDO-CISO alliance is now essential to balance speed with security. Data sovereignty is a particular concern, with 82 percent of CDOs viewing it as a core part of their risk management strategy.&lt;/p&gt;&lt;p&gt;The biggest hurdle, however, may be people. The report reveals a widening talent gap that threatens to stall progress. In 2025, 77 percent&amp;nbsp;of CDOs report difficulty attracting or retaining top data talent, a sharp increase from 62 percent in 2024.&lt;/p&gt;&lt;p&gt;This scarcity is exacerbated by the fact that the required skills are a moving target. IBM found that 82 percent of CDOs are “hiring for data roles that didn’t exist last year related to generative AI”. This cultural and skills challenge is often the hardest part.&lt;/p&gt;&lt;p&gt;Hiroshi Okuyama, Chief Digital Officer at Yanmar Holdings, explained: “Changing culture is hard, but people are becoming more aware that their decisions must be based on data and facts, and that they need to collect evidence when making decisions.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-opening-the-data-silos-to-launch-enterprise-ai"&gt;Opening the data silos to launch enterprise AI&lt;/h3&gt;&lt;p&gt;On the technical front, enterprise leaders must champion the move away from siloed data estates. This means investing in modern, federated data architectures and pushing teams to develop and use “data products” that can be securely shared and reused across the organisation.&lt;/p&gt;&lt;p&gt;Second, on the cultural front, data literacy must become a business-wide priority, not just an IT concern. The 80 percent of CDOs who say data democratisation helps their organisation move faster are correct. This means fostering a data-driven culture and investing in intuitive tools that make it simpler for non-technical employees to interact with data.&lt;/p&gt;&lt;p&gt;The goal is to elevate the organisation from running isolated AI experiments to scaling intelligent automation across core business processes. The companies that succeed will be those that treat their data not as an application byproduct, but as their most valuable asset.&lt;/p&gt;&lt;p&gt;Ed Lovely, VP and Chief Data Officer at IBM, said: “Enterprise AI at scale is within reach, but success depends on organisations powering it with the right data. For CDOs, this means establishing a seamlessly integrated enterprise data architecture that fuels innovation and unlocks business value.&lt;/p&gt;&lt;p&gt;“Organisations that get this right won’t just improve their AI, they’ll transform how they operate, make faster decisions, adapt to change more quickly, and gain a competitive edge.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;New data centre projects mark Anthropic’s biggest US expansion yet&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;According to IBM, the primary barrier holding back enterprise AI isn’t the technology itself but the persistent issue of data silos.&lt;/p&gt;&lt;p&gt;Ed Lovely, VP and Chief Data Officer at IBM, describes data silos as the “Achilles’ heel” of modern data strategy. Lovely made the comments following the release of a new study from the IBM Institute for Business Value that found AI is ready to scale, but enterprise data is not.&lt;/p&gt;&lt;p&gt;The report, which surveyed 1,700 senior data leaders, found that functional data remains stubbornly isolated. Finance, HR, marketing, and supply chain data all operate in isolation, with no common taxonomy or shared standards.&lt;/p&gt;&lt;p&gt;This fragmentation is having a direct, negative impact on AI projects. “When data lives in disconnected silos, every AI initiative becomes a drawn-out, six-to-twelve-month data cleansing project,” said Ed Lovely, VP and Chief Data Officer at IBM. “Teams spend more time hunting for and aligning data than generating meaningful insights”.&lt;/p&gt;&lt;p&gt;This is a direct threat to competitive advantage. For CIOs and CDOs, the mission is no longer just to collect and protect data, but to deploy it effectively to power these new AI systems.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-data-janitor-to-value-driver"&gt;From data janitor to value driver&lt;/h3&gt;&lt;p&gt;The consensus from the study is that data leaders must be relentlessly focused on business outcomes, with 92 percent of CDOs agreeing their success depends on this focus.&lt;/p&gt;&lt;p&gt;Herein lies the central tension: while 92 percent are aiming for business value, only 29 percent are confident they have “clear measures to determine the business value of data-driven outcomes.”&lt;/p&gt;&lt;p&gt;This gap between ambition and reality is where AI agents that can learn and act autonomously to achieve goals are expected to help. Leaders are showing a growing confidence in these tools, with 83 percent of CDOs in IBM’s research stating the potential benefits of deploying AI agents outweigh the risks.&lt;/p&gt;&lt;p&gt;At global medical technology company Medtronic, teams were bogged down matching invoices, purchase orders, and proofs of delivery. By deploying an AI solution, the company automated this workflow. The result was a drop in document matching time from 20 minutes per invoice to just eight seconds, with an accuracy rate exceeding 99 percent. This allowed staff to be redeployed from low-value data entry to higher-value work.&lt;/p&gt;&lt;p&gt;Similarly, renewable energy company Matrix Renewables implemented a centralised data platform to monitor its assets. This led to a 75 percent reduction in reporting time and a 10 percent reduction in costly downtime.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ibm-finds-the-ai-hurdles-architecture-governance-and-the-talent-gap"&gt;IBM finds the AI hurdles: Architecture, governance, and the talent gap&lt;/h3&gt;&lt;p&gt;Achieving these results requires a new approach to data architecture while avoiding silos. The old model of costly, slow data relocation into a central lake is being replaced. IBM’s study finds 81 percent of CDOs now practice bringing AI to the data, rather than moving data to AI.&lt;/p&gt;&lt;p&gt;This approach relies on modern architectural patterns like data mesh and data fabric, which provide a virtualised layer to access data where it lives. It also champions the concept of “data products” (packaged, reusable data assets designed for a specific business purpose, such as a “customer 360” view or a financial forecast dataset.)&lt;/p&gt;&lt;p&gt;However, making data more accessible introduces governance challenges. The CDO-CISO alliance is now essential to balance speed with security. Data sovereignty is a particular concern, with 82 percent of CDOs viewing it as a core part of their risk management strategy.&lt;/p&gt;&lt;p&gt;The biggest hurdle, however, may be people. The report reveals a widening talent gap that threatens to stall progress. In 2025, 77 percent&amp;nbsp;of CDOs report difficulty attracting or retaining top data talent, a sharp increase from 62 percent in 2024.&lt;/p&gt;&lt;p&gt;This scarcity is exacerbated by the fact that the required skills are a moving target. IBM found that 82 percent of CDOs are “hiring for data roles that didn’t exist last year related to generative AI”. This cultural and skills challenge is often the hardest part.&lt;/p&gt;&lt;p&gt;Hiroshi Okuyama, Chief Digital Officer at Yanmar Holdings, explained: “Changing culture is hard, but people are becoming more aware that their decisions must be based on data and facts, and that they need to collect evidence when making decisions.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-opening-the-data-silos-to-launch-enterprise-ai"&gt;Opening the data silos to launch enterprise AI&lt;/h3&gt;&lt;p&gt;On the technical front, enterprise leaders must champion the move away from siloed data estates. This means investing in modern, federated data architectures and pushing teams to develop and use “data products” that can be securely shared and reused across the organisation.&lt;/p&gt;&lt;p&gt;Second, on the cultural front, data literacy must become a business-wide priority, not just an IT concern. The 80 percent of CDOs who say data democratisation helps their organisation move faster are correct. This means fostering a data-driven culture and investing in intuitive tools that make it simpler for non-technical employees to interact with data.&lt;/p&gt;&lt;p&gt;The goal is to elevate the organisation from running isolated AI experiments to scaling intelligent automation across core business processes. The companies that succeed will be those that treat their data not as an application byproduct, but as their most valuable asset.&lt;/p&gt;&lt;p&gt;Ed Lovely, VP and Chief Data Officer at IBM, said: “Enterprise AI at scale is within reach, but success depends on organisations powering it with the right data. For CDOs, this means establishing a seamlessly integrated enterprise data architecture that fuels innovation and unlocks business value.&lt;/p&gt;&lt;p&gt;“Organisations that get this right won’t just improve their AI, they’ll transform how they operate, make faster decisions, adapt to change more quickly, and gain a competitive edge.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;New data centre projects mark Anthropic’s biggest US expansion yet&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ibm-data-silos-are-holding-back-enterprise-ai/</guid><pubDate>Thu, 13 Nov 2025 13:25:43 +0000</pubDate></item><item><title>[NEW] Coding assistant Cursor raises $2.3bn five months after its previous round (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/coding-assistant-cursor-raises-2-3bn-five-months-after-its-previous-round/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/opengraph-image.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Developer AI coding tool Cursor continues to gobble up venture capital as its valuation keeps climbing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Cursor announced a $2.3 billion funding round that valued the company at $29.3 billion, as originally reported by the Wall Street Journal. This round more than doubles the company’s previous valuation of $9.9 billion, which it achieved in its $900 million Series C round in June.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This most recent fundraise was led by co-led by Accel, an existing investor, and Coatue, which is new to the cap table. Strategic investors including Nvidia (an enterprise customer) and Google (an AI model supplier) also joined the round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Joshua Kushner’s Thrive Capital led the company’s prior two rounds and participated in this round as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor’s co-founder and CEO Michael Truell told the Wall Street Journal that the capital from the round will be put toward developing Composer, an AI model released by Cursor in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor still relies on outside AI models from companies including Google, OpenAI and Anthropic to power its platform, but the plan is for Composer to carry some of that load in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Next year could be a very interesting one for Cursor. While the company is still seeing strong growth, OpenAI and Anthropic are both sharpening their AI coding products as the market for AI development tools continues to get more competitive.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/opengraph-image.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Developer AI coding tool Cursor continues to gobble up venture capital as its valuation keeps climbing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Cursor announced a $2.3 billion funding round that valued the company at $29.3 billion, as originally reported by the Wall Street Journal. This round more than doubles the company’s previous valuation of $9.9 billion, which it achieved in its $900 million Series C round in June.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This most recent fundraise was led by co-led by Accel, an existing investor, and Coatue, which is new to the cap table. Strategic investors including Nvidia (an enterprise customer) and Google (an AI model supplier) also joined the round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Joshua Kushner’s Thrive Capital led the company’s prior two rounds and participated in this round as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor’s co-founder and CEO Michael Truell told the Wall Street Journal that the capital from the round will be put toward developing Composer, an AI model released by Cursor in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor still relies on outside AI models from companies including Google, OpenAI and Anthropic to power its platform, but the plan is for Composer to carry some of that load in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Next year could be a very interesting one for Cursor. While the company is still seeing strong growth, OpenAI and Anthropic are both sharpening their AI coding products as the market for AI development tools continues to get more competitive.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/coding-assistant-cursor-raises-2-3bn-five-months-after-its-previous-round/</guid><pubDate>Thu, 13 Nov 2025 13:58:44 +0000</pubDate></item><item><title>[NEW] Google augments AI shopping with conversational search, agentic checkout and an AI that calls stores for you (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/google-expands-ai-shopping-with-conversational-search-agentic-checkout-and-an-ai-that-calls-stores-for-you/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is rolling out a suite of AI shopping updates just ahead of the holiday season. The company on Thursday unveiled a host of new tools and features, including conversational shopping in Google Search, new shopping features within its Gemini app, agentic checkout, and even an AI tool that can call local stores to find out if a product you want is available.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company believes the additions will help improve the online shopping  experience, which can still today involve a lot of drudgery, explained Vidhya Srinivasan, VP and GM of ads and commerce at Google, in a press briefing ahead of the launch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We feel it really shouldn’t be so tedious, and shopping should feel — and can feel — a lot more natural and easy,” she said. “The idea here is we want to hold on to all the fun parts of shopping, like the browsing, like the serendipitous discovery, and things like that, but then skip all the tedious, hard parts.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067175" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-Mode-in-Search-Best-Product.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;One update will allow consumers to ask shopping questions in AI Mode, Google’s conversational search feature that lets you use natural language queries in a chatbot-style interface. The responses will be tailored to your question, and the chatbot will provide images when you need visual inspiration, alongside other details like price, reviews, and available inventory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So if you were looking for cozy sweaters in autumn colors, you’d see photos of the options available to you. But if you were comparing items, like skin care products, Google may instead return insights in a comparison table.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067176" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-Mode-in-Search-Product-Comparison.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that AI Mode is powered by its Shopping Graph, which includes over 50 billion product listings, 2 billion of which are updated every hour, and said the inventory information you see is usually up-to-date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another shopping update will enable the Gemini app to provide fleshed-out ideas as responses, instead of just text suggestions in response to shopping-related questions. This is only available to users in the U.S. currently.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company confirmed that consumers using AI Mode will see sponsored listings, but as the features are still experimental, these ads won’t appear in the Gemini mobile app just yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Google is rolling out agentic checkout within Google Search in the U.S., including in AI Mode. The feature is currently compatible with merchants like Wayfair, Chewy, Quince, and select Shopify stores.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067179" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Checkout.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;To use agentic checkout, you can begin by tracking an item’s price to be notified if the price drops to fall within your budget. You can then opt to have Google purchase the item for you on the merchant’s website using Google Pay. The company says it will always ask your permission first, and will have you confirm your purchase and shipping details.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is helpful for shoppers, because they don’t have to constantly check to see if the item they want is on sale. And it’s great for retailers because it brings back the customer who might otherwise have moved on,” said Lilian Rincon, VP of product management for Google Shopping, during the briefing. “Agentic checkout is built on Google’s trusted shopping graph and also G Pay, so you can rest assured that you’re seeing accurate results and that your payment information is secure,” she noted.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067177" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Calling-Button-highlighted.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another AI feature can call businesses on your behalf to find out if a store carries a product, how much it costs, and whether there are any promotions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is built on Google’s Duplex technology, introduced back in 2018, as well as its Shopping Graph and payments infrastructure. After you’ve provided information about the product you’re looking for, the AI will call local stores and make inquiries about the product, then come back to you with a summary of its findings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This feature is rolling out now in the U.S. for specific categories like toys, health and beauty products, and electronics. To use this feature, you can search for products “near me,” then use the option “Let Google Call.” The AI will then walk you through questions about the items you’re searching for.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067178" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Calling-Report.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it’s being mindful of how merchants will experience these calls, and it will make sure the chatbot will not call too often and is clear about the questions it asks. Retailers can choose to opt out of receiving such calls as well. Those who don’t will first hear Google disclose that it’s an AI calling on a customer’s behalf and only proceed when the recipient of the call says it’s okay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google execs planned to demo the technology during the press briefing on Wednesday, but Wi-Fi issues on their end led them to abandon the demo before it was completed.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is rolling out a suite of AI shopping updates just ahead of the holiday season. The company on Thursday unveiled a host of new tools and features, including conversational shopping in Google Search, new shopping features within its Gemini app, agentic checkout, and even an AI tool that can call local stores to find out if a product you want is available.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company believes the additions will help improve the online shopping  experience, which can still today involve a lot of drudgery, explained Vidhya Srinivasan, VP and GM of ads and commerce at Google, in a press briefing ahead of the launch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We feel it really shouldn’t be so tedious, and shopping should feel — and can feel — a lot more natural and easy,” she said. “The idea here is we want to hold on to all the fun parts of shopping, like the browsing, like the serendipitous discovery, and things like that, but then skip all the tedious, hard parts.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067175" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-Mode-in-Search-Best-Product.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;One update will allow consumers to ask shopping questions in AI Mode, Google’s conversational search feature that lets you use natural language queries in a chatbot-style interface. The responses will be tailored to your question, and the chatbot will provide images when you need visual inspiration, alongside other details like price, reviews, and available inventory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So if you were looking for cozy sweaters in autumn colors, you’d see photos of the options available to you. But if you were comparing items, like skin care products, Google may instead return insights in a comparison table.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067176" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-Mode-in-Search-Product-Comparison.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that AI Mode is powered by its Shopping Graph, which includes over 50 billion product listings, 2 billion of which are updated every hour, and said the inventory information you see is usually up-to-date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another shopping update will enable the Gemini app to provide fleshed-out ideas as responses, instead of just text suggestions in response to shopping-related questions. This is only available to users in the U.S. currently.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company confirmed that consumers using AI Mode will see sponsored listings, but as the features are still experimental, these ads won’t appear in the Gemini mobile app just yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Google is rolling out agentic checkout within Google Search in the U.S., including in AI Mode. The feature is currently compatible with merchants like Wayfair, Chewy, Quince, and select Shopify stores.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067179" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Checkout.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;To use agentic checkout, you can begin by tracking an item’s price to be notified if the price drops to fall within your budget. You can then opt to have Google purchase the item for you on the merchant’s website using Google Pay. The company says it will always ask your permission first, and will have you confirm your purchase and shipping details.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is helpful for shoppers, because they don’t have to constantly check to see if the item they want is on sale. And it’s great for retailers because it brings back the customer who might otherwise have moved on,” said Lilian Rincon, VP of product management for Google Shopping, during the briefing. “Agentic checkout is built on Google’s trusted shopping graph and also G Pay, so you can rest assured that you’re seeing accurate results and that your payment information is secure,” she noted.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067177" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Calling-Button-highlighted.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another AI feature can call businesses on your behalf to find out if a store carries a product, how much it costs, and whether there are any promotions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is built on Google’s Duplex technology, introduced back in 2018, as well as its Shopping Graph and payments infrastructure. After you’ve provided information about the product you’re looking for, the AI will call local stores and make inquiries about the product, then come back to you with a summary of its findings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This feature is rolling out now in the U.S. for specific categories like toys, health and beauty products, and electronics. To use this feature, you can search for products “near me,” then use the option “Let Google Call.” The AI will then walk you through questions about the items you’re searching for.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067178" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Calling-Report.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it’s being mindful of how merchants will experience these calls, and it will make sure the chatbot will not call too often and is clear about the questions it asks. Retailers can choose to opt out of receiving such calls as well. Those who don’t will first hear Google disclose that it’s an AI calling on a customer’s behalf and only proceed when the recipient of the call says it’s okay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google execs planned to demo the technology during the press briefing on Wednesday, but Wi-Fi issues on their end led them to abandon the demo before it was completed.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/google-expands-ai-shopping-with-conversational-search-agentic-checkout-and-an-ai-that-calls-stores-for-you/</guid><pubDate>Thu, 13 Nov 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] GeForce NOW Enlists ‘Call of Duty: Black Ops 7’ for the Cloud (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-call-of-duty-black-ops-7/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Chaos has entered the chat. It’s GFN Thursday, and things are getting intense with the launch of &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt;, streaming at launch this week on GeForce NOW. Dive straight into the chaos across devices — underpowered laptops, Macs, and Steam Decks.&lt;/p&gt;
&lt;p&gt;Power up the week with 12 new games to stream in the cloud, including the latest GeForce RTX 5080-optimized game, &lt;i&gt;Anno 117: Pax Romana.&lt;/i&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87289"&gt;&lt;img alt="Phoenix now live on GeForce NOW" class="size-large wp-image-87289" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Blackwell_RTX_status-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87289"&gt;&lt;em&gt;Stockholm will be the next region to light up with GeForce RTX 5080-class power.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Phoenix is the latest region to get GeForce RTX 5080-class power, with Stockholm coming up next. Stay tuned to GFN Thursday for updates as more regions upgrade to Blackwell RTX. Follow along with the latest progress on the server rollout page.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The Biggest ‘Black Ops’ Ever&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt;, Treyarch and Raven Software are bringing players the biggest Black Ops ever. The year is 2035, and the world teeters on the edge of collapse after the global events following &lt;i&gt;Black Ops 2&lt;/i&gt; and &lt;i&gt;Black Ops 6&lt;/i&gt;. Wielding cutting-edge technology, the Black Ops team led by David Mason must fight back against a manipulative enemy who weaponizes fear above all else.&lt;/p&gt;
&lt;p&gt;Step into the boots of David Mason and his elite Black Ops team as they fight against unseen forces that bend reality and reason. Squad up in an all-new Co-op Campaign designed for the biggest tactical play, unleash high-tech weaponry across a multiplayer experience brimming with new maps and modes, and descend once again into darkness with the next twisted chapter of Round-Based Zombies.&lt;/p&gt;
&lt;p&gt;Stream &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt; today on GeForce NOW and play seamlessly across devices. The only way to play the game on Steam Deck is through GeForce NOW. Premium members get instant access with no waiting around for downloads, the highest frame rates, lowest latency and longer gaming sessions.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;All Roads Lead to the Cloud&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87292"&gt;&lt;img alt="Anno 117: Pax Romana on GeForce NOW" class="size-large wp-image-87292" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Anno_117_Pax_Romano-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87292"&gt;&lt;em&gt;Build Rome your way — just don’t anger the Senate.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ubisoft’s &lt;i&gt;Anno 117: Pax Romana&lt;/i&gt; builds on the rich legacy of the city‑building and strategy series, inviting players to shape the world at the height of the Roman Empire. Step into the sandals of a Roman leader to expand territories, manage trade and balance prosperity with political intrigue in a meticulously crafted ancient world.&lt;/p&gt;
&lt;p&gt;Players can implement decisive rule, careful diplomacy or a vision for peace as they oversee provinces — from bustling Mediterranean ports to fog‑covered British frontiers — in a game where every decision shapes the empire’s stability and economy, as well as citizens’ happiness levels.&lt;/p&gt;
&lt;p&gt;Launching on GeForce NOW with GeForce RTX 5080-class power,&lt;i&gt; Anno 117&lt;/i&gt; takes advantage of 5K 120 frames-per-second streaming for breathtaking detail and ultrasmooth performance, letting every brick, marketplace and coastline shine in cinematic clarity. Experience the empire come alive instantly in the cloud — no downloads nor lag, just seamless city‑building, even on the go.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Start Your Engines&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87295"&gt;&lt;img alt="Assetto Corsa Rally on GeForce NOW" class="size-large wp-image-87295" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Assetto_Corsa_Rally-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87295"&gt;&lt;em&gt;Every second counts.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Assetto Corsa Rally &lt;/i&gt;by 505 Game captures the pulse-pounding unpredictability and challenge of rally racing with uncompromising enthusiasm. Every stage plunges players into dynamic conditions — from shifting surface grip and evolving weather to crowd reactions — that demand sharp instincts and constant adaptation. With its precisely laser-scanned tracks, realistic environments and authentic co-driver pace notes delivered by real rally professionals, the game radiates motorsport passion and attention to detail.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Surviving Mars: Relaunched &lt;/i&gt;(New release on Steam, Nov. 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Possessor(s) &lt;/i&gt;(New release on Steam, Nov. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Rue Valley &lt;/i&gt;(New release on Steam, Nov. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Anno 117: Pax Romana &lt;/i&gt;(New release on Steam and Ubisoft, Nov. 13, GeForce RTX 5080-ready)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Assetto Corsa Rally &lt;/i&gt;(New release on Steam, Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;INAZUMA ELEVEN: Victory Road &lt;/i&gt;(New release on Steam, Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Songs of Silence&lt;/i&gt; (New release on Epic Games Store, Free on Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Call of Duty: Black Ops 7 &lt;/i&gt;(New release on Steam, Battle.net and Xbox, available on PC Game Pass, Nov. 14)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Where Winds Meet &lt;/i&gt;(New release on Epic Games Store, Nov. 14)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Megabonk &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;R.E.P.O. &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;RV There Yet? &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Members who already own the Install-to-Play titles that are being added as Ready-to-Play titles this week can check out this support article for steps on how to re-add them to their libraries. All Steam Cloud Saves will be carried over.&lt;/p&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;What’s been the oddest or weirdest thing you've seen in the background of a game – screenshots and clips are welcome!&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) November 10, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Chaos has entered the chat. It’s GFN Thursday, and things are getting intense with the launch of &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt;, streaming at launch this week on GeForce NOW. Dive straight into the chaos across devices — underpowered laptops, Macs, and Steam Decks.&lt;/p&gt;
&lt;p&gt;Power up the week with 12 new games to stream in the cloud, including the latest GeForce RTX 5080-optimized game, &lt;i&gt;Anno 117: Pax Romana.&lt;/i&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87289"&gt;&lt;img alt="Phoenix now live on GeForce NOW" class="size-large wp-image-87289" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Blackwell_RTX_status-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87289"&gt;&lt;em&gt;Stockholm will be the next region to light up with GeForce RTX 5080-class power.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Phoenix is the latest region to get GeForce RTX 5080-class power, with Stockholm coming up next. Stay tuned to GFN Thursday for updates as more regions upgrade to Blackwell RTX. Follow along with the latest progress on the server rollout page.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The Biggest ‘Black Ops’ Ever&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt;, Treyarch and Raven Software are bringing players the biggest Black Ops ever. The year is 2035, and the world teeters on the edge of collapse after the global events following &lt;i&gt;Black Ops 2&lt;/i&gt; and &lt;i&gt;Black Ops 6&lt;/i&gt;. Wielding cutting-edge technology, the Black Ops team led by David Mason must fight back against a manipulative enemy who weaponizes fear above all else.&lt;/p&gt;
&lt;p&gt;Step into the boots of David Mason and his elite Black Ops team as they fight against unseen forces that bend reality and reason. Squad up in an all-new Co-op Campaign designed for the biggest tactical play, unleash high-tech weaponry across a multiplayer experience brimming with new maps and modes, and descend once again into darkness with the next twisted chapter of Round-Based Zombies.&lt;/p&gt;
&lt;p&gt;Stream &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt; today on GeForce NOW and play seamlessly across devices. The only way to play the game on Steam Deck is through GeForce NOW. Premium members get instant access with no waiting around for downloads, the highest frame rates, lowest latency and longer gaming sessions.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;All Roads Lead to the Cloud&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87292"&gt;&lt;img alt="Anno 117: Pax Romana on GeForce NOW" class="size-large wp-image-87292" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Anno_117_Pax_Romano-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87292"&gt;&lt;em&gt;Build Rome your way — just don’t anger the Senate.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ubisoft’s &lt;i&gt;Anno 117: Pax Romana&lt;/i&gt; builds on the rich legacy of the city‑building and strategy series, inviting players to shape the world at the height of the Roman Empire. Step into the sandals of a Roman leader to expand territories, manage trade and balance prosperity with political intrigue in a meticulously crafted ancient world.&lt;/p&gt;
&lt;p&gt;Players can implement decisive rule, careful diplomacy or a vision for peace as they oversee provinces — from bustling Mediterranean ports to fog‑covered British frontiers — in a game where every decision shapes the empire’s stability and economy, as well as citizens’ happiness levels.&lt;/p&gt;
&lt;p&gt;Launching on GeForce NOW with GeForce RTX 5080-class power,&lt;i&gt; Anno 117&lt;/i&gt; takes advantage of 5K 120 frames-per-second streaming for breathtaking detail and ultrasmooth performance, letting every brick, marketplace and coastline shine in cinematic clarity. Experience the empire come alive instantly in the cloud — no downloads nor lag, just seamless city‑building, even on the go.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Start Your Engines&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87295"&gt;&lt;img alt="Assetto Corsa Rally on GeForce NOW" class="size-large wp-image-87295" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Assetto_Corsa_Rally-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87295"&gt;&lt;em&gt;Every second counts.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Assetto Corsa Rally &lt;/i&gt;by 505 Game captures the pulse-pounding unpredictability and challenge of rally racing with uncompromising enthusiasm. Every stage plunges players into dynamic conditions — from shifting surface grip and evolving weather to crowd reactions — that demand sharp instincts and constant adaptation. With its precisely laser-scanned tracks, realistic environments and authentic co-driver pace notes delivered by real rally professionals, the game radiates motorsport passion and attention to detail.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Surviving Mars: Relaunched &lt;/i&gt;(New release on Steam, Nov. 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Possessor(s) &lt;/i&gt;(New release on Steam, Nov. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Rue Valley &lt;/i&gt;(New release on Steam, Nov. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Anno 117: Pax Romana &lt;/i&gt;(New release on Steam and Ubisoft, Nov. 13, GeForce RTX 5080-ready)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Assetto Corsa Rally &lt;/i&gt;(New release on Steam, Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;INAZUMA ELEVEN: Victory Road &lt;/i&gt;(New release on Steam, Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Songs of Silence&lt;/i&gt; (New release on Epic Games Store, Free on Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Call of Duty: Black Ops 7 &lt;/i&gt;(New release on Steam, Battle.net and Xbox, available on PC Game Pass, Nov. 14)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Where Winds Meet &lt;/i&gt;(New release on Epic Games Store, Nov. 14)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Megabonk &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;R.E.P.O. &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;RV There Yet? &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Members who already own the Install-to-Play titles that are being added as Ready-to-Play titles this week can check out this support article for steps on how to re-add them to their libraries. All Steam Cloud Saves will be carried over.&lt;/p&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;What’s been the oddest or weirdest thing you've seen in the background of a game – screenshots and clips are welcome!&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) November 10, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-call-of-duty-black-ops-7/</guid><pubDate>Thu, 13 Nov 2025 14:00:50 +0000</pubDate></item><item><title>[NEW] Google is rolling out conversational shopping—and ads—in AI Mode search (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/11/google-rolling-out-conversational-shopping-and-ads-in-ai-mode-search/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Conversational shopping is Google’s first big swing at monetizing AI Mode search.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Mode banner" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-640x360.png" width="640" /&gt;
                  &lt;img alt="AI Mode banner" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;In recent months, Google has promised to inject generative AI into the online shopping experience, and now it’s following through. The previously announced shopping features of AI Mode search are rolling out, and Gemini will also worm its way into Google’s forgotten Duplex automated phone call tech. It’s all coming in time for the holidays to allegedly make your gifting more convenient and also conveniently ensure that Google gets a piece of the action.&lt;/p&gt;
&lt;p&gt;At Google I/O in May, the company announced its intention to bring conversational shopping to AI Mode. According to Google, its enormous “Shopping Graph” or retailer data means its AI is uniquely positioned to deliver useful suggestions. In the coming weeks, users in the US will be able to ask AI Mode complex questions about what to buy, and it will deliver suggestions, guides, tables, and other generated content to help you decide. And since this is gen AI, it comes with the usual disclaimers about possible mistakes.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2127187-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-in-Search-Product-Visuals.mp4?_=1" type="video/mp4" /&gt;AI Mode shopping features.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Mode shopping features.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;You’re probably wondering where you’ll see sponsored shopping content in these experiences. Google says some of the content that appears in AI Mode will be ads, just like if you look up shopping results in a traditional search. Shopping features are also coming to the Gemini app, but Google says it won’t have sponsored content in the results for the time being.&lt;/p&gt;
&lt;p&gt;Google is also releasing a feature called “agentic checkout,” a term used only in passing when the company announced the feature alongside AI Mode shopping at I/O. Google is really leaning into the agentic angle now, though. The gist is you can set a price threshold for a product in search, and Google will let you know if the item reaches that price. That part isn’t new, but there’s now an AI twist. After getting the alert, you can authorize an automatic purchase with Google Pay. However, it’s currently only supported at a handful of retailers like Chewy, Wayfair, and some Shopify merchants. It’s not clear whether this qualifies as agentic anything, but it might save you some money regardless.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2127215 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="926" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Agentic-Checkout.jpg" width="1643" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;AI Mode shopping and agentic checkout are beginning their rollout now, and Google says they will be available widely in time for the holiday shopping season.&lt;/p&gt;
&lt;h2&gt;Somehow, Duplex returned&lt;/h2&gt;
&lt;p&gt;Before the current AI craze, Google was fond of demoing Duplex, an Assistant-based AI designed to carry out real-world tasks on the phone. Google thought people would be willing to trust the AI to check business hours and make appointments, but it never gained much traction. The Duplex prompts slowly disappeared from Assistant over the years.&lt;/p&gt;
&lt;p&gt;Now, Duplex is back with what Google calls a “big Gemini model upgrade.” It won’t be making appointments for you, but Google does still plan to use the updated Duplex to allow you to call businesses. This time, Duplex is aimed at saving you from calling stores to check on stock availability. Instead, you can tell the robot what you want, and it will check for you.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2127187-2" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Agentic-Calling-Demo.mp4?_=2" type="video/mp4" /&gt;Duplex is back, baby.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Duplex is back, baby.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google says when you search for certain products near you, you’ll see an option to “Let Google Call.” You’ll have to indicate what specific product you want, and the robot will begin calling around. The robot will identify itself as such when it places calls, which will only happen during business hours and after a reasonable cooldown. If businesses get too annoyed, they’re liable to opt out of Duplex calls, which is still an option.&lt;/p&gt;
&lt;p&gt;Eventually, you’ll get an email or text message with AI summaries of the calls that could help you decide where to go. These messages may also include local inventory data from other nearby stores based on Google’s Shopping Graph. That sounds like it could mean more sponsored links, but it’s unclear. This feature is beginning its rollout today in categories like toys, cosmetics, and electronics. Unsurprisingly, this one is also US-only.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Conversational shopping is Google’s first big swing at monetizing AI Mode search.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Mode banner" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-640x360.png" width="640" /&gt;
                  &lt;img alt="AI Mode banner" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;In recent months, Google has promised to inject generative AI into the online shopping experience, and now it’s following through. The previously announced shopping features of AI Mode search are rolling out, and Gemini will also worm its way into Google’s forgotten Duplex automated phone call tech. It’s all coming in time for the holidays to allegedly make your gifting more convenient and also conveniently ensure that Google gets a piece of the action.&lt;/p&gt;
&lt;p&gt;At Google I/O in May, the company announced its intention to bring conversational shopping to AI Mode. According to Google, its enormous “Shopping Graph” or retailer data means its AI is uniquely positioned to deliver useful suggestions. In the coming weeks, users in the US will be able to ask AI Mode complex questions about what to buy, and it will deliver suggestions, guides, tables, and other generated content to help you decide. And since this is gen AI, it comes with the usual disclaimers about possible mistakes.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2127187-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-in-Search-Product-Visuals.mp4?_=1" type="video/mp4" /&gt;AI Mode shopping features.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Mode shopping features.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;You’re probably wondering where you’ll see sponsored shopping content in these experiences. Google says some of the content that appears in AI Mode will be ads, just like if you look up shopping results in a traditional search. Shopping features are also coming to the Gemini app, but Google says it won’t have sponsored content in the results for the time being.&lt;/p&gt;
&lt;p&gt;Google is also releasing a feature called “agentic checkout,” a term used only in passing when the company announced the feature alongside AI Mode shopping at I/O. Google is really leaning into the agentic angle now, though. The gist is you can set a price threshold for a product in search, and Google will let you know if the item reaches that price. That part isn’t new, but there’s now an AI twist. After getting the alert, you can authorize an automatic purchase with Google Pay. However, it’s currently only supported at a handful of retailers like Chewy, Wayfair, and some Shopify merchants. It’s not clear whether this qualifies as agentic anything, but it might save you some money regardless.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2127215 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="926" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Agentic-Checkout.jpg" width="1643" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;AI Mode shopping and agentic checkout are beginning their rollout now, and Google says they will be available widely in time for the holiday shopping season.&lt;/p&gt;
&lt;h2&gt;Somehow, Duplex returned&lt;/h2&gt;
&lt;p&gt;Before the current AI craze, Google was fond of demoing Duplex, an Assistant-based AI designed to carry out real-world tasks on the phone. Google thought people would be willing to trust the AI to check business hours and make appointments, but it never gained much traction. The Duplex prompts slowly disappeared from Assistant over the years.&lt;/p&gt;
&lt;p&gt;Now, Duplex is back with what Google calls a “big Gemini model upgrade.” It won’t be making appointments for you, but Google does still plan to use the updated Duplex to allow you to call businesses. This time, Duplex is aimed at saving you from calling stores to check on stock availability. Instead, you can tell the robot what you want, and it will check for you.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2127187-2" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Agentic-Calling-Demo.mp4?_=2" type="video/mp4" /&gt;Duplex is back, baby.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Duplex is back, baby.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google says when you search for certain products near you, you’ll see an option to “Let Google Call.” You’ll have to indicate what specific product you want, and the robot will begin calling around. The robot will identify itself as such when it places calls, which will only happen during business hours and after a reasonable cooldown. If businesses get too annoyed, they’re liable to opt out of Duplex calls, which is still an option.&lt;/p&gt;
&lt;p&gt;Eventually, you’ll get an email or text message with AI summaries of the calls that could help you decide where to go. These messages may also include local inventory data from other nearby stores based on Google’s Shopping Graph. That sounds like it could mean more sponsored links, but it’s unclear. This feature is beginning its rollout today in categories like toys, cosmetics, and electronics. Unsurprisingly, this one is also US-only.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/11/google-rolling-out-conversational-shopping-and-ads-in-ai-mode-search/</guid><pubDate>Thu, 13 Nov 2025 14:00:57 +0000</pubDate></item><item><title>[NEW] Google DeepMind is using Gemini to train agents inside Goat Simulator 3 (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/13/1127921/google-deepmind-is-using-gemini-to-train-agents-inside-goat-simulator-3/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/Blog-header-sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds@2x.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Google DeepMind has built a new video-game-playing agent called SIMA 2 that can navigate and solve problems in a wide range of 3D virtual worlds. The company claims it’s a big step toward more general-purpose agents and better real-world robots.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google DeepMind first demoed SIMA (which stands for “scalable instructable multiworld agent”) last year. But SIMA 2 has been built on top of Gemini, the firm’s flagship large language model, which gives the agent a huge boost in capability.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The researchers claim that SIMA 2 can carry out a range of more complex tasks inside virtual worlds, figure out how to solve certain challenges by itself, and chat with its users. It can also improve itself by tackling harder tasks multiple times and learning through trial and error.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_InstructionChaining10.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;“Games have been a driving force behind agent research for quite a while,” Joe Marino, a research scientist at Google DeepMind, said in a press conference this week. He noted that even a simple action in a game, such as lighting a lantern, can involve multiple steps: “It’s a really complex set of tasks you need to solve to progress.”&lt;/p&gt; 
 &lt;p&gt;The ultimate aim is to develop next-generation agents that are able to follow instructions and carry out open-ended tasks inside more complex environments than a web browser. In the long run, Google DeepMind wants to use such agents to drive real-world robots. Marino claimed that the skills SIMA 2 has learned, such as navigating an environment, using tools, and collaborating with humans to solve problems, are essential building blocks for future robot companions.&lt;/p&gt;  &lt;p&gt;Unlike previous work on game-playing agents such as AlphaZero, which beat a Go grandmaster in 2016, or AlphaStar, which beat 99.8% of ranked human competition players at the video game StarCraft 2 in 2019, the idea behind SIMA is to train an agent to play an open-ended game without preset goals. Instead, the agent learns to carry out instructions given to it by people.&lt;/p&gt; 
 &lt;p&gt;Humans control SIMA 2 via text chat, by talking to it out loud, or by drawing on the game’s screen. The agent takes in a video game’s pixels frame by frame and figures out what actions it needs to take to carry out its tasks.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_REASONING_11.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;Like its predecessor, SIMA 2 was trained on footage of humans playing eight commercial video games, including No Man’s Sky and Goat Simulator 3, as well as three virtual worlds created by the company. The agent learned to match keyboard and mouse inputs to actions.&lt;/p&gt;  &lt;p&gt;Hooked up to Gemini, the researchers claim, SIMA 2 is far better at following instructions (asking questions and providing updates as it goes) and figuring out for itself how to perform certain more complex tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google DeepMind tested the agent inside environments it had never seen before. In one set of experiments, researchers asked Genie 3, the latest version of the firm’s world model, to produce environments from scratch and dropped SIMA 2 into them. They found that the agent was able to navigate and carry out instructions there.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The researchers also used Gemini to generate new tasks for SIMA 2. If the agent failed, at first Gemini generated tips that SIMA 2 took on board when it tried again. Repeating a task multiple times in this way often allowed SIMA 2 to improve by trial and error until it succeeded, Marino said.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Git gud&lt;/h3&gt;  &lt;p&gt;SIMA 2 is still an experiment. The agent struggles with complex tasks that require multiple steps and more time to complete. It also remembers only its most recent interactions (to make SIMA 2 more responsive, the team cut its long-term memory). It’s also still nowhere near as good as people at using a mouse and keyboard to interact with a virtual world.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_MultimodalPrompting03_v04.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;Julian Togelius, an AI researcher at New York University who works on creativity and video games, thinks it’s an interesting result. Previous attempts at training a single system to play multiple games haven’t gone too well, he says. That’s because training models to control multiple games just by watching the screen isn’t easy: “Playing in real time from visual input only is ‘hard mode,’” he says.&lt;/p&gt;  &lt;p&gt;In particular, Togelius calls out GATO, a previous system from Google DeepMind, which—despite being hyped at the time—could not transfer skills across a significant number of virtual environments.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Still, he is open-minded about whether or not SIMA 2 could lead to better robots. “The real world is both harder and easier than video games,” he says. It’s harder because you can’t just press A to open a door. At the same time, a robot in the real world will know exactly what its body can and can’t do at any time. That’s not the case in video games, where the rules inside each virtual world can differ.&lt;/p&gt;  &lt;p&gt;Others are more skeptical. Matthew Guzdial, an AI researcher at the University of Alberta, isn’t too surprised that SIMA 2 can play many different video games. He notes that most games have very similar keyboard and mouse controls: Learn one and you learn them all. “If you put a game with weird input in front of it, I don’t think it’d be able to perform well,” he says.&lt;/p&gt;  &lt;p&gt;Guzdial also questions how much of what SIMA 2 has learned would really carry over to robots. “It’s much harder to understand visuals from cameras in the real world compared to games, which are designed with easily parsable visuals for human players,” he says.&lt;/p&gt;  &lt;p&gt;Still, Marino and his colleagues hope to continue their work with Genie 3 to allow the agent to improve inside a kind of endless virtual training dojo, where Genie generates worlds for SIMA to learn in via trial and error guided by Gemini’s feedback. “We’ve kind of just scratched the surface of what’s possible,” he said at the press conference. &amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/Blog-header-sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds@2x.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Google DeepMind has built a new video-game-playing agent called SIMA 2 that can navigate and solve problems in a wide range of 3D virtual worlds. The company claims it’s a big step toward more general-purpose agents and better real-world robots.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google DeepMind first demoed SIMA (which stands for “scalable instructable multiworld agent”) last year. But SIMA 2 has been built on top of Gemini, the firm’s flagship large language model, which gives the agent a huge boost in capability.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The researchers claim that SIMA 2 can carry out a range of more complex tasks inside virtual worlds, figure out how to solve certain challenges by itself, and chat with its users. It can also improve itself by tackling harder tasks multiple times and learning through trial and error.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_InstructionChaining10.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;“Games have been a driving force behind agent research for quite a while,” Joe Marino, a research scientist at Google DeepMind, said in a press conference this week. He noted that even a simple action in a game, such as lighting a lantern, can involve multiple steps: “It’s a really complex set of tasks you need to solve to progress.”&lt;/p&gt; 
 &lt;p&gt;The ultimate aim is to develop next-generation agents that are able to follow instructions and carry out open-ended tasks inside more complex environments than a web browser. In the long run, Google DeepMind wants to use such agents to drive real-world robots. Marino claimed that the skills SIMA 2 has learned, such as navigating an environment, using tools, and collaborating with humans to solve problems, are essential building blocks for future robot companions.&lt;/p&gt;  &lt;p&gt;Unlike previous work on game-playing agents such as AlphaZero, which beat a Go grandmaster in 2016, or AlphaStar, which beat 99.8% of ranked human competition players at the video game StarCraft 2 in 2019, the idea behind SIMA is to train an agent to play an open-ended game without preset goals. Instead, the agent learns to carry out instructions given to it by people.&lt;/p&gt; 
 &lt;p&gt;Humans control SIMA 2 via text chat, by talking to it out loud, or by drawing on the game’s screen. The agent takes in a video game’s pixels frame by frame and figures out what actions it needs to take to carry out its tasks.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_REASONING_11.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;Like its predecessor, SIMA 2 was trained on footage of humans playing eight commercial video games, including No Man’s Sky and Goat Simulator 3, as well as three virtual worlds created by the company. The agent learned to match keyboard and mouse inputs to actions.&lt;/p&gt;  &lt;p&gt;Hooked up to Gemini, the researchers claim, SIMA 2 is far better at following instructions (asking questions and providing updates as it goes) and figuring out for itself how to perform certain more complex tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google DeepMind tested the agent inside environments it had never seen before. In one set of experiments, researchers asked Genie 3, the latest version of the firm’s world model, to produce environments from scratch and dropped SIMA 2 into them. They found that the agent was able to navigate and carry out instructions there.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The researchers also used Gemini to generate new tasks for SIMA 2. If the agent failed, at first Gemini generated tips that SIMA 2 took on board when it tried again. Repeating a task multiple times in this way often allowed SIMA 2 to improve by trial and error until it succeeded, Marino said.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Git gud&lt;/h3&gt;  &lt;p&gt;SIMA 2 is still an experiment. The agent struggles with complex tasks that require multiple steps and more time to complete. It also remembers only its most recent interactions (to make SIMA 2 more responsive, the team cut its long-term memory). It’s also still nowhere near as good as people at using a mouse and keyboard to interact with a virtual world.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_MultimodalPrompting03_v04.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;Julian Togelius, an AI researcher at New York University who works on creativity and video games, thinks it’s an interesting result. Previous attempts at training a single system to play multiple games haven’t gone too well, he says. That’s because training models to control multiple games just by watching the screen isn’t easy: “Playing in real time from visual input only is ‘hard mode,’” he says.&lt;/p&gt;  &lt;p&gt;In particular, Togelius calls out GATO, a previous system from Google DeepMind, which—despite being hyped at the time—could not transfer skills across a significant number of virtual environments.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Still, he is open-minded about whether or not SIMA 2 could lead to better robots. “The real world is both harder and easier than video games,” he says. It’s harder because you can’t just press A to open a door. At the same time, a robot in the real world will know exactly what its body can and can’t do at any time. That’s not the case in video games, where the rules inside each virtual world can differ.&lt;/p&gt;  &lt;p&gt;Others are more skeptical. Matthew Guzdial, an AI researcher at the University of Alberta, isn’t too surprised that SIMA 2 can play many different video games. He notes that most games have very similar keyboard and mouse controls: Learn one and you learn them all. “If you put a game with weird input in front of it, I don’t think it’d be able to perform well,” he says.&lt;/p&gt;  &lt;p&gt;Guzdial also questions how much of what SIMA 2 has learned would really carry over to robots. “It’s much harder to understand visuals from cameras in the real world compared to games, which are designed with easily parsable visuals for human players,” he says.&lt;/p&gt;  &lt;p&gt;Still, Marino and his colleagues hope to continue their work with Genie 3 to allow the agent to improve inside a kind of endless virtual training dojo, where Genie generates worlds for SIMA to learn in via trial and error guided by Gemini’s feedback. “We’ve kind of just scratched the surface of what’s possible,” he said at the press conference. &amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/13/1127921/google-deepmind-is-using-gemini-to-train-agents-inside-goat-simulator-3/</guid><pubDate>Thu, 13 Nov 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] Teen founders raise $6M to reinvent pesticides using AI — and convince Paul Graham to join in (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/teen-founders-raise-6m-to-reinvent-pesticides-using-ai-and-convince-paul-graham-to-join-in/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Two teenage founders walked into Y Combinator co-founder Paul Graham’s backyard with an idea no one in agriculture seemed to want — an AI model to help design better pesticides. By the time they left, they had a new business model, a new company, and eventually, Graham’s backing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, that reimagined company — Bindwell — has raised $6 million in a seed round, co-led by General Catalyst and A Capital, with a personal check from Graham himself. Rather than selling AI tools to legacy agrochemical giants, the startup is using its own models to design new pesticide molecules in-house and license the IP directly — a shift in strategy aimed at modernizing a legacy industry still dominated by decades-old chemistry.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pesticide use in agriculture has doubled over the last three decades, yet up to 40% of global crop production is still lost to pests and diseases every year, per the UN Food and Agriculture Organization. As pests evolve and develop resistance, farmers are forced to use increasing amounts of chemicals just to maintain the same yields — a cycle that damages ecosystems and accelerates resistance even further. Regulatory pressure is mounting, but most agrochemical companies still rely on tweaking legacy compounds. Bindwell is betting that AI can break the cycle by discovering entirely new, more targeted molecules — ones designed from scratch for modern challenges.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2024 by Tyler Rose, 18, and Navvye Anand, 19, Bindwell adapts AI-led drug discovery techniques to agriculture, with the goal of speeding up how new pesticide molecules are identified and tested.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell began as a research project in late 2023, when Rose and Anand were students at the Wolfram Summer Research Program. They initially focused on a drug discovery AI model called PLAPT, which involved binding affinity prediction — work that was later cited in a Nature Scientific Reports paper on cancer therapeutics. In 2024, they began exploring how the same approach could be applied to pesticides.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both founders had personal exposure to the problem. Rose learned about the challenges of pest control from his aunt, who farms in China. Anand, who has roots in Punjab, saw firsthand how limited pesticide options affected crop yields.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Agriculture has been in our mind space,” said Rose in an interview. “That led to the realization that we can use the exact same technology that has been successful in drug discovery. We can bring that over to pesticide discovery, because the biochemistry is the same, but pesticides are such a big problem, and I feel like it’s not very focused on by most people.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3067512" height="1280" src="https://techcrunch.com/wp-content/uploads/2025/11/bindwell-co-founders_88d28d.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Bindwell co-founders Tyler Rose (Left) and Navvye Anand (Right)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bindwell&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rose and Anand entered Y Combinator’s Winter 2025 batch with plans to build AI models and sell their access to major agrochemical companies. But they did not find traction — most industry players were reluctant to adopt AI as a core part of pesticide discovery. Midway through the program, they were invited to Paul Graham’s home, where they spoke with him for about 45 minutes on the back patio. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After hearing about their challenges, Graham suggested a different approach: Rather than selling tools, they could use their own models to discover new pesticide molecules themselves. That conversation marked the beginning of Bindwell’s current direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The founders [of Bindwell] will probably do alright,” he later posted on X. “They’re smart and have a good idea.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Bindwell has developed its own AI suite designed to reduce hallucination — a common issue where models produce unreliable or unsupported outputs. The software includes Foldwell, a structure prediction model, which is a custom diffusion system inspired by DeepMind’s AlphaFold, used to identify target protein structures. It also includes PLAPT, an open source protein-ligand interaction model capable of scanning every known synthesized compound in under six hours, and APPT, a protein-protein interaction model for biopesticide screening, reported to outperform existing tools by 1.7× on the Affinity Benchmark v5.5. Moreover, the suite incorporates an uncertainty quantification system that flags when results are trustworthy and when more data is needed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Since we’re not selling AI models, we’re not competing with companies that sell models,” Rose told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, Bindwell’s models can analyze “billions” of molecules, the startup said, and deliver four times faster performance than DeepMind’s AlphaFold 3.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The way most pesticides are discovered right now is not target-based,” said Rose. “Entomologists and chemists suggest different compounds, then test them on insects. You often need to synthesize and test thousands of chemicals, which is expensive just to check for efficacy. With our AI models, you’re able to simplify the problem down to a single protein.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI helps identify proteins that are unique to a specific pest but absent in humans, beneficial insects, or aquatic organisms like water fleas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once you find those proteins, you can design something that binds to them and stops them from working,” Rose said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell is currently testing the efficacy of its AI-generated molecules at its lab in San Carlos. It is also working with a third-party partner to further validate the models, though Rose declined to share details.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rose said the startup is in early discussions with several global agrochemical firms, with its first partnership deal expected to close soon. “A year from now, we want to be entering into our licensing deals with some of these companies,” he said. Bindwell has also begun talks with stakeholders in India and China to conduct field tests.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup currently has a team of four, and also works with external contractors for molecule synthesis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell’s seed round also included participation from SV Angel, alongside Graham. Prior to joining Y Combinator’s Winter 2025 batch, the startup raised a pre-seed round from Character Capital.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Two teenage founders walked into Y Combinator co-founder Paul Graham’s backyard with an idea no one in agriculture seemed to want — an AI model to help design better pesticides. By the time they left, they had a new business model, a new company, and eventually, Graham’s backing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, that reimagined company — Bindwell — has raised $6 million in a seed round, co-led by General Catalyst and A Capital, with a personal check from Graham himself. Rather than selling AI tools to legacy agrochemical giants, the startup is using its own models to design new pesticide molecules in-house and license the IP directly — a shift in strategy aimed at modernizing a legacy industry still dominated by decades-old chemistry.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pesticide use in agriculture has doubled over the last three decades, yet up to 40% of global crop production is still lost to pests and diseases every year, per the UN Food and Agriculture Organization. As pests evolve and develop resistance, farmers are forced to use increasing amounts of chemicals just to maintain the same yields — a cycle that damages ecosystems and accelerates resistance even further. Regulatory pressure is mounting, but most agrochemical companies still rely on tweaking legacy compounds. Bindwell is betting that AI can break the cycle by discovering entirely new, more targeted molecules — ones designed from scratch for modern challenges.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2024 by Tyler Rose, 18, and Navvye Anand, 19, Bindwell adapts AI-led drug discovery techniques to agriculture, with the goal of speeding up how new pesticide molecules are identified and tested.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell began as a research project in late 2023, when Rose and Anand were students at the Wolfram Summer Research Program. They initially focused on a drug discovery AI model called PLAPT, which involved binding affinity prediction — work that was later cited in a Nature Scientific Reports paper on cancer therapeutics. In 2024, they began exploring how the same approach could be applied to pesticides.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both founders had personal exposure to the problem. Rose learned about the challenges of pest control from his aunt, who farms in China. Anand, who has roots in Punjab, saw firsthand how limited pesticide options affected crop yields.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Agriculture has been in our mind space,” said Rose in an interview. “That led to the realization that we can use the exact same technology that has been successful in drug discovery. We can bring that over to pesticide discovery, because the biochemistry is the same, but pesticides are such a big problem, and I feel like it’s not very focused on by most people.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3067512" height="1280" src="https://techcrunch.com/wp-content/uploads/2025/11/bindwell-co-founders_88d28d.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Bindwell co-founders Tyler Rose (Left) and Navvye Anand (Right)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bindwell&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rose and Anand entered Y Combinator’s Winter 2025 batch with plans to build AI models and sell their access to major agrochemical companies. But they did not find traction — most industry players were reluctant to adopt AI as a core part of pesticide discovery. Midway through the program, they were invited to Paul Graham’s home, where they spoke with him for about 45 minutes on the back patio. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After hearing about their challenges, Graham suggested a different approach: Rather than selling tools, they could use their own models to discover new pesticide molecules themselves. That conversation marked the beginning of Bindwell’s current direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The founders [of Bindwell] will probably do alright,” he later posted on X. “They’re smart and have a good idea.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Bindwell has developed its own AI suite designed to reduce hallucination — a common issue where models produce unreliable or unsupported outputs. The software includes Foldwell, a structure prediction model, which is a custom diffusion system inspired by DeepMind’s AlphaFold, used to identify target protein structures. It also includes PLAPT, an open source protein-ligand interaction model capable of scanning every known synthesized compound in under six hours, and APPT, a protein-protein interaction model for biopesticide screening, reported to outperform existing tools by 1.7× on the Affinity Benchmark v5.5. Moreover, the suite incorporates an uncertainty quantification system that flags when results are trustworthy and when more data is needed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Since we’re not selling AI models, we’re not competing with companies that sell models,” Rose told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, Bindwell’s models can analyze “billions” of molecules, the startup said, and deliver four times faster performance than DeepMind’s AlphaFold 3.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The way most pesticides are discovered right now is not target-based,” said Rose. “Entomologists and chemists suggest different compounds, then test them on insects. You often need to synthesize and test thousands of chemicals, which is expensive just to check for efficacy. With our AI models, you’re able to simplify the problem down to a single protein.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI helps identify proteins that are unique to a specific pest but absent in humans, beneficial insects, or aquatic organisms like water fleas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once you find those proteins, you can design something that binds to them and stops them from working,” Rose said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell is currently testing the efficacy of its AI-generated molecules at its lab in San Carlos. It is also working with a third-party partner to further validate the models, though Rose declined to share details.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rose said the startup is in early discussions with several global agrochemical firms, with its first partnership deal expected to close soon. “A year from now, we want to be entering into our licensing deals with some of these companies,” he said. Bindwell has also begun talks with stakeholders in India and China to conduct field tests.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup currently has a team of four, and also works with external contractors for molecule synthesis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell’s seed round also included participation from SV Angel, alongside Graham. Prior to joining Y Combinator’s Winter 2025 batch, the startup raised a pre-seed round from Character Capital.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/teen-founders-raise-6m-to-reinvent-pesticides-using-ai-and-convince-paul-graham-to-join-in/</guid><pubDate>Thu, 13 Nov 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] Inside LinkedIn’s generative AI cookbook: How it scaled people search to 1.3 billion users (AI | VentureBeat)</title><link>https://venturebeat.com/ai/inside-linkedins-generative-ai-cookbook-how-it-scaled-people-search-to-1-3</link><description>[unable to retrieve full-text content]&lt;p&gt;LinkedIn is launching its new AI-powered people search this week, after what seems like a very long wait for what should have been a natural offering for generative AI.&lt;/p&gt;&lt;p&gt;It comes a full three years after the launch of ChatGPT and six months after LinkedIn launched its AI job search offering. For technical leaders, this timeline illustrates a key enterprise lesson: Deploying generative AI in real enterprise settings is challenging, especially at a scale of 1.3 billion users. It’s a slow, brutal process of pragmatic optimization.&lt;/p&gt;&lt;p&gt;The following account is based on several exclusive interviews with the LinkedIn product and engineering team behind the launch.&lt;/p&gt;&lt;p&gt;First, here’s how the product works: A user can now type a natural language query like, &lt;b&gt;&amp;quot;Who is knowledgeable about curing cancer?&amp;quot; &lt;/b&gt;into LinkedIn’s search bar.&lt;/p&gt;&lt;p&gt;LinkedIn&amp;#x27;s old search, based on keywords, would have been stumped. It would have looked only for references to &amp;quot;cancer&amp;quot;. If a user wanted to get sophisticated, they would have had to run separate, rigid keyword searches for &amp;quot;cancer&amp;quot; and then &amp;quot;oncology&amp;quot; and manually try to piece the results together.&lt;/p&gt;&lt;p&gt;The new AI-powered system, however, understands the &lt;i&gt;intent&lt;/i&gt; of the search because the LLM under the hood grasps semantic meaning. It recognizes, for example, that &amp;quot;cancer&amp;quot; is conceptually related to &amp;quot;oncology&amp;quot; and even less directly, to &amp;quot;genomics research.&amp;quot; As a result, it surfaces a far more relevant list of people, including oncology leaders and researchers, even if their profiles don&amp;#x27;t use the exact word &amp;quot;cancer.&amp;quot;&lt;/p&gt;&lt;p&gt;The system also balances this relevance with &lt;i&gt;usefulness&lt;/i&gt;. Instead of just showing the world&amp;#x27;s top oncologist (who might be an unreachable third-degree connection), it will also weigh who in your immediate network — like a first-degree connection — is &amp;quot;pretty relevant&amp;quot; and can serve as a crucial bridge to that expert.&lt;/p&gt;&lt;p&gt;See the video below for an example.&lt;/p&gt;&lt;p&gt;Arguably, though, the more important lesson for enterprise practitioners is the &amp;quot;cookbook&amp;quot; LinkedIn has developed: a replicable, multi-stage pipeline of distillation, co-design, and relentless optimization. LinkedIn had to perfect this on one product before attempting it on another.&lt;/p&gt;&lt;p&gt;&amp;quot;Don&amp;#x27;t try to do too much all at once,&amp;quot; writes Wenjing Zhang, LinkedIn&amp;#x27;s VP of Engineering, in a  post about the product launch, and who also spoke with VentureBeat last week in an interview. She notes that an earlier &amp;quot;sprawling ambition&amp;quot; to build a unified system for all of LinkedIn&amp;#x27;s products &amp;quot;stalled progress.&amp;quot;&lt;/p&gt;&lt;p&gt;Instead, LinkedIn focused on winning one vertical first. The success of its previously launched AI Job Search — which led to job seekers without a four-year degree being &lt;b&gt;10% more likely to get hired&lt;/b&gt;, according to VP of Product Engineering Erran Berger — provided the blueprint.&lt;/p&gt;&lt;p&gt;Now, the company is applying that blueprint to a far larger challenge. &amp;quot;It&amp;#x27;s one thing to be able to do this across tens of millions of jobs,&amp;quot; Berger told VentureBeat. &amp;quot;It&amp;#x27;s another thing to do this across north of a billion members.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprise AI builders, LinkedIn&amp;#x27;s journey provides a technical playbook for what it &lt;i&gt;actually&lt;/i&gt; takes to move from a successful pilot to a billion-user-scale product.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The new challenge: a 1.3 billion-member graph&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The job search product created a robust recipe that the new people search product could build upon, Berger explained. &lt;/p&gt;&lt;p&gt;The recipe started with with a &amp;quot;golden data set&amp;quot; of just a few hundred to a thousand real query-profile pairs, meticulously scored against a detailed 20- to 30-page &amp;quot;product policy&amp;quot; document. To scale this for training, LinkedIn used this small golden set to prompt a large foundation model to generate a massive volume of &lt;i&gt;synthetic&lt;/i&gt; training data. This synthetic data was used to train a &lt;b&gt;7-billion-parameter&lt;/b&gt; &amp;quot;Product Policy&amp;quot; model — a high-fidelity judge of relevance that was too slow for live production but perfect for teaching smaller models.&lt;/p&gt;&lt;p&gt;However, the team hit a wall early on. For six to nine months, they struggled to train a single model that could balance strict policy adherence (relevance) against user engagement signals. The &amp;quot;aha moment&amp;quot; came when they realized they needed to break the problem down. They distilled the 7B policy model into a &lt;b&gt;1.7B teacher model&lt;/b&gt; focused solely on relevance. They then paired it with separate teacher models trained to predict specific member actions, such as job applications for the jobs product, or connecting and following for people search. This &amp;quot;multi-teacher&amp;quot; ensemble produced soft probability scores that the final student model learned to mimic via KL divergence loss.&lt;/p&gt;&lt;p&gt;The resulting architecture operates as a two-stage pipeline. First, a larger &lt;b&gt;8B parameter model&lt;/b&gt; handles broad retrieval, casting a wide net to pull candidates from the graph. Then, the highly distilled student model takes over for fine-grained ranking. While the job search product successfully deployed a &lt;b&gt;0.6B (600-million)&lt;/b&gt; parameter student, the new people search product required even more aggressive compression. As Zhang notes, the team pruned their new student model from 440M down to just &lt;b&gt;220M parameters&lt;/b&gt;, achieving the necessary speed for 1.3 billion users with less than 1% relevance loss.&lt;/p&gt;&lt;p&gt;But applying this to people search broke the old architecture. The new problem included not just &lt;i&gt;ranking&lt;/i&gt; but also &lt;i&gt;retrieval&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;“A billion records,&amp;quot; Berger said, is a &amp;quot;different beast.&amp;quot;&lt;/p&gt;&lt;p&gt;The team’s prior retrieval stack was built on CPUs. To handle the new scale and the latency demands of a &amp;quot;snappy&amp;quot; search experience, the team had to move its indexing to &lt;b&gt;GPU-based infrastructure&lt;/b&gt;. This was a foundational architectural shift that the job search product did not require.&lt;/p&gt;&lt;p&gt;Organizationally, LinkedIn benefited from multiple approaches. For a time, LinkedIn had two separate teams &lt;!-- --&gt;— &lt;!-- --&gt;job search and people search &lt;!-- --&gt;— &lt;!-- --&gt;attempting to solve the problem in parallel. But once the job search team achieved its breakthrough using the policy-driven distillation method, Berger and his leadership team intervened. They brought over the architects of the job search win &lt;!-- --&gt;—  &lt;!-- --&gt;product lead Rohan Rajiv and engineering lead Wenjing Zhang &lt;!-- --&gt;— &lt;!-- --&gt;to transplant their &amp;#x27;cookbook&amp;#x27; directly to the new domain.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Distilling for a 10x throughput gain&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With the retrieval problem solved, the team faced the ranking and efficiency challenge. This is where the cookbook was adapted with new, aggressive optimization techniques.&lt;/p&gt;&lt;p&gt;Zhang’s technical post &lt;b&gt;(I’ll insert the link once it goes live)&lt;/b&gt; provides the specific details our audience of AI engineers will appreciate. One of the more significant optimizations was input size.&lt;/p&gt;&lt;p&gt;To feed the model, the team trained &lt;i&gt;another&lt;/i&gt; LLM with reinforcement learning (RL) for a single purpose: to summarize the input context. This &amp;quot;summarizer&amp;quot; model was able to reduce the model&amp;#x27;s input size by &lt;b&gt;20-fold&lt;/b&gt; with minimal information loss.&lt;/p&gt;&lt;p&gt;The combined result of the 220M-parameter model and the 20x input reduction? A &lt;b&gt;10x increase in ranking throughput&lt;/b&gt;, allowing the team to serve the model efficiently to its massive user base.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pragmatism over hype: building tools, not agents&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Throughout our discussions, Berger was adamant about something else that might catch peoples’ attention: The real value for enterprises today lies in perfecting recommender systems, not in chasing &amp;quot;agentic hype.&amp;quot; He also refused to talk about the specific models that the company used for the searches, suggesting it almost doesn&amp;#x27;t matter. The company selects models based on which one it finds the most efficient for the task.&lt;/p&gt;&lt;p&gt;The new AI-powered people search is a manifestation of Berger’s philosophy that it’s best to optimize the recommender system first. The architecture includes a new &amp;quot;intelligent query routing layer,&amp;quot; as Berger explained, that itself is LLM-powered. This router pragmatically decides if a user&amp;#x27;s query — like &amp;quot;trust expert&amp;quot; — should go to the new semantic, natural-language stack or to the old, reliable lexical search.&lt;/p&gt;&lt;p&gt;This entire, complex system is designed to be a &amp;quot;tool&amp;quot; that a &lt;i&gt;future&lt;/i&gt; agent will use, not the agent itself.&lt;/p&gt;&lt;p&gt;&amp;quot;Agentic products are only as good as the tools that they use to accomplish tasks for people,&amp;quot; Berger said. &amp;quot;You can have the world&amp;#x27;s best reasoning model, and if you&amp;#x27;re trying to use an agent to do people search but the people search engine is not very good, you&amp;#x27;re not going to be able to deliver.&amp;quot; &lt;/p&gt;&lt;p&gt;Now that the people search is available, Berger suggested that one day the company will be offering agents to use it. But he didn’t provide details on timing. He also said the recipe used for job and people search will be spread across the company’s other products.&lt;/p&gt;&lt;p&gt;For enterprises building their own AI roadmaps, LinkedIn&amp;#x27;s playbook is clear:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Be pragmatic:&lt;/b&gt; Don&amp;#x27;t try to boil the ocean. Win one vertical, even if it takes 18 months.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Codify the &amp;quot;cookbook&amp;quot;:&lt;/b&gt; Turn that win into a repeatable process (policy docs, distillation pipelines, co-design).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Optimize relentlessly:&lt;/b&gt; The real 10x gains come &lt;i&gt;after&lt;/i&gt; the initial model, in pruning, distillation, and creative optimizations like an RL-trained summarizer.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;LinkedIn&amp;#x27;s journey shows that for real-world enterprise AI, emphasis on specific models or cool agentic systems should take a back seat. The durable, strategic advantage comes from mastering the &lt;i&gt;pipeline&lt;/i&gt; — the &amp;#x27;AI-native&amp;#x27; cookbook of co-design, distillation, and ruthless optimization.&lt;/p&gt;&lt;p&gt;&lt;i&gt;(Editor&amp;#x27;s note: We will be publishing a full-length podcast with LinkedIn&amp;#x27;s Erran Berger, which will dive deeper into these technical details, on the VentureBeat podcast feed soon.)&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;LinkedIn is launching its new AI-powered people search this week, after what seems like a very long wait for what should have been a natural offering for generative AI.&lt;/p&gt;&lt;p&gt;It comes a full three years after the launch of ChatGPT and six months after LinkedIn launched its AI job search offering. For technical leaders, this timeline illustrates a key enterprise lesson: Deploying generative AI in real enterprise settings is challenging, especially at a scale of 1.3 billion users. It’s a slow, brutal process of pragmatic optimization.&lt;/p&gt;&lt;p&gt;The following account is based on several exclusive interviews with the LinkedIn product and engineering team behind the launch.&lt;/p&gt;&lt;p&gt;First, here’s how the product works: A user can now type a natural language query like, &lt;b&gt;&amp;quot;Who is knowledgeable about curing cancer?&amp;quot; &lt;/b&gt;into LinkedIn’s search bar.&lt;/p&gt;&lt;p&gt;LinkedIn&amp;#x27;s old search, based on keywords, would have been stumped. It would have looked only for references to &amp;quot;cancer&amp;quot;. If a user wanted to get sophisticated, they would have had to run separate, rigid keyword searches for &amp;quot;cancer&amp;quot; and then &amp;quot;oncology&amp;quot; and manually try to piece the results together.&lt;/p&gt;&lt;p&gt;The new AI-powered system, however, understands the &lt;i&gt;intent&lt;/i&gt; of the search because the LLM under the hood grasps semantic meaning. It recognizes, for example, that &amp;quot;cancer&amp;quot; is conceptually related to &amp;quot;oncology&amp;quot; and even less directly, to &amp;quot;genomics research.&amp;quot; As a result, it surfaces a far more relevant list of people, including oncology leaders and researchers, even if their profiles don&amp;#x27;t use the exact word &amp;quot;cancer.&amp;quot;&lt;/p&gt;&lt;p&gt;The system also balances this relevance with &lt;i&gt;usefulness&lt;/i&gt;. Instead of just showing the world&amp;#x27;s top oncologist (who might be an unreachable third-degree connection), it will also weigh who in your immediate network — like a first-degree connection — is &amp;quot;pretty relevant&amp;quot; and can serve as a crucial bridge to that expert.&lt;/p&gt;&lt;p&gt;See the video below for an example.&lt;/p&gt;&lt;p&gt;Arguably, though, the more important lesson for enterprise practitioners is the &amp;quot;cookbook&amp;quot; LinkedIn has developed: a replicable, multi-stage pipeline of distillation, co-design, and relentless optimization. LinkedIn had to perfect this on one product before attempting it on another.&lt;/p&gt;&lt;p&gt;&amp;quot;Don&amp;#x27;t try to do too much all at once,&amp;quot; writes Wenjing Zhang, LinkedIn&amp;#x27;s VP of Engineering, in a  post about the product launch, and who also spoke with VentureBeat last week in an interview. She notes that an earlier &amp;quot;sprawling ambition&amp;quot; to build a unified system for all of LinkedIn&amp;#x27;s products &amp;quot;stalled progress.&amp;quot;&lt;/p&gt;&lt;p&gt;Instead, LinkedIn focused on winning one vertical first. The success of its previously launched AI Job Search — which led to job seekers without a four-year degree being &lt;b&gt;10% more likely to get hired&lt;/b&gt;, according to VP of Product Engineering Erran Berger — provided the blueprint.&lt;/p&gt;&lt;p&gt;Now, the company is applying that blueprint to a far larger challenge. &amp;quot;It&amp;#x27;s one thing to be able to do this across tens of millions of jobs,&amp;quot; Berger told VentureBeat. &amp;quot;It&amp;#x27;s another thing to do this across north of a billion members.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprise AI builders, LinkedIn&amp;#x27;s journey provides a technical playbook for what it &lt;i&gt;actually&lt;/i&gt; takes to move from a successful pilot to a billion-user-scale product.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The new challenge: a 1.3 billion-member graph&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The job search product created a robust recipe that the new people search product could build upon, Berger explained. &lt;/p&gt;&lt;p&gt;The recipe started with with a &amp;quot;golden data set&amp;quot; of just a few hundred to a thousand real query-profile pairs, meticulously scored against a detailed 20- to 30-page &amp;quot;product policy&amp;quot; document. To scale this for training, LinkedIn used this small golden set to prompt a large foundation model to generate a massive volume of &lt;i&gt;synthetic&lt;/i&gt; training data. This synthetic data was used to train a &lt;b&gt;7-billion-parameter&lt;/b&gt; &amp;quot;Product Policy&amp;quot; model — a high-fidelity judge of relevance that was too slow for live production but perfect for teaching smaller models.&lt;/p&gt;&lt;p&gt;However, the team hit a wall early on. For six to nine months, they struggled to train a single model that could balance strict policy adherence (relevance) against user engagement signals. The &amp;quot;aha moment&amp;quot; came when they realized they needed to break the problem down. They distilled the 7B policy model into a &lt;b&gt;1.7B teacher model&lt;/b&gt; focused solely on relevance. They then paired it with separate teacher models trained to predict specific member actions, such as job applications for the jobs product, or connecting and following for people search. This &amp;quot;multi-teacher&amp;quot; ensemble produced soft probability scores that the final student model learned to mimic via KL divergence loss.&lt;/p&gt;&lt;p&gt;The resulting architecture operates as a two-stage pipeline. First, a larger &lt;b&gt;8B parameter model&lt;/b&gt; handles broad retrieval, casting a wide net to pull candidates from the graph. Then, the highly distilled student model takes over for fine-grained ranking. While the job search product successfully deployed a &lt;b&gt;0.6B (600-million)&lt;/b&gt; parameter student, the new people search product required even more aggressive compression. As Zhang notes, the team pruned their new student model from 440M down to just &lt;b&gt;220M parameters&lt;/b&gt;, achieving the necessary speed for 1.3 billion users with less than 1% relevance loss.&lt;/p&gt;&lt;p&gt;But applying this to people search broke the old architecture. The new problem included not just &lt;i&gt;ranking&lt;/i&gt; but also &lt;i&gt;retrieval&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;“A billion records,&amp;quot; Berger said, is a &amp;quot;different beast.&amp;quot;&lt;/p&gt;&lt;p&gt;The team’s prior retrieval stack was built on CPUs. To handle the new scale and the latency demands of a &amp;quot;snappy&amp;quot; search experience, the team had to move its indexing to &lt;b&gt;GPU-based infrastructure&lt;/b&gt;. This was a foundational architectural shift that the job search product did not require.&lt;/p&gt;&lt;p&gt;Organizationally, LinkedIn benefited from multiple approaches. For a time, LinkedIn had two separate teams &lt;!-- --&gt;— &lt;!-- --&gt;job search and people search &lt;!-- --&gt;— &lt;!-- --&gt;attempting to solve the problem in parallel. But once the job search team achieved its breakthrough using the policy-driven distillation method, Berger and his leadership team intervened. They brought over the architects of the job search win &lt;!-- --&gt;—  &lt;!-- --&gt;product lead Rohan Rajiv and engineering lead Wenjing Zhang &lt;!-- --&gt;— &lt;!-- --&gt;to transplant their &amp;#x27;cookbook&amp;#x27; directly to the new domain.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Distilling for a 10x throughput gain&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With the retrieval problem solved, the team faced the ranking and efficiency challenge. This is where the cookbook was adapted with new, aggressive optimization techniques.&lt;/p&gt;&lt;p&gt;Zhang’s technical post &lt;b&gt;(I’ll insert the link once it goes live)&lt;/b&gt; provides the specific details our audience of AI engineers will appreciate. One of the more significant optimizations was input size.&lt;/p&gt;&lt;p&gt;To feed the model, the team trained &lt;i&gt;another&lt;/i&gt; LLM with reinforcement learning (RL) for a single purpose: to summarize the input context. This &amp;quot;summarizer&amp;quot; model was able to reduce the model&amp;#x27;s input size by &lt;b&gt;20-fold&lt;/b&gt; with minimal information loss.&lt;/p&gt;&lt;p&gt;The combined result of the 220M-parameter model and the 20x input reduction? A &lt;b&gt;10x increase in ranking throughput&lt;/b&gt;, allowing the team to serve the model efficiently to its massive user base.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pragmatism over hype: building tools, not agents&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Throughout our discussions, Berger was adamant about something else that might catch peoples’ attention: The real value for enterprises today lies in perfecting recommender systems, not in chasing &amp;quot;agentic hype.&amp;quot; He also refused to talk about the specific models that the company used for the searches, suggesting it almost doesn&amp;#x27;t matter. The company selects models based on which one it finds the most efficient for the task.&lt;/p&gt;&lt;p&gt;The new AI-powered people search is a manifestation of Berger’s philosophy that it’s best to optimize the recommender system first. The architecture includes a new &amp;quot;intelligent query routing layer,&amp;quot; as Berger explained, that itself is LLM-powered. This router pragmatically decides if a user&amp;#x27;s query — like &amp;quot;trust expert&amp;quot; — should go to the new semantic, natural-language stack or to the old, reliable lexical search.&lt;/p&gt;&lt;p&gt;This entire, complex system is designed to be a &amp;quot;tool&amp;quot; that a &lt;i&gt;future&lt;/i&gt; agent will use, not the agent itself.&lt;/p&gt;&lt;p&gt;&amp;quot;Agentic products are only as good as the tools that they use to accomplish tasks for people,&amp;quot; Berger said. &amp;quot;You can have the world&amp;#x27;s best reasoning model, and if you&amp;#x27;re trying to use an agent to do people search but the people search engine is not very good, you&amp;#x27;re not going to be able to deliver.&amp;quot; &lt;/p&gt;&lt;p&gt;Now that the people search is available, Berger suggested that one day the company will be offering agents to use it. But he didn’t provide details on timing. He also said the recipe used for job and people search will be spread across the company’s other products.&lt;/p&gt;&lt;p&gt;For enterprises building their own AI roadmaps, LinkedIn&amp;#x27;s playbook is clear:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Be pragmatic:&lt;/b&gt; Don&amp;#x27;t try to boil the ocean. Win one vertical, even if it takes 18 months.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Codify the &amp;quot;cookbook&amp;quot;:&lt;/b&gt; Turn that win into a repeatable process (policy docs, distillation pipelines, co-design).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Optimize relentlessly:&lt;/b&gt; The real 10x gains come &lt;i&gt;after&lt;/i&gt; the initial model, in pruning, distillation, and creative optimizations like an RL-trained summarizer.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;LinkedIn&amp;#x27;s journey shows that for real-world enterprise AI, emphasis on specific models or cool agentic systems should take a back seat. The durable, strategic advantage comes from mastering the &lt;i&gt;pipeline&lt;/i&gt; — the &amp;#x27;AI-native&amp;#x27; cookbook of co-design, distillation, and ruthless optimization.&lt;/p&gt;&lt;p&gt;&lt;i&gt;(Editor&amp;#x27;s note: We will be publishing a full-length podcast with LinkedIn&amp;#x27;s Erran Berger, which will dive deeper into these technical details, on the VentureBeat podcast feed soon.)&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/inside-linkedins-generative-ai-cookbook-how-it-scaled-people-search-to-1-3</guid><pubDate>Thu, 13 Nov 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] LinkedIn adds AI-powered search to help users find people (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/linkedin-adds-ai-powered-search-to-help-users-find-people/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For the last two years, LinkedIn has tried to infuse AI into different parts of its platforms, including ad copies, content creation, personalized digests, hiring assistance, job hunting advice, and learning. The company is now finally adding AI to one of the most-used parts of the site: search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the company released a job search tool for members in the U.S., allowing them to search for jobs using natural language queries. Now, the company is extending the feature to people search.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users can use queries like “Find me investors in the healthcare sector with FDA experience,” people who “co-founded a productivity company and are based in NYC,” or “Who in my network can help me understand wireless networks.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067452" height="443" src="https://techcrunch.com/wp-content/uploads/2025/11/01D_Investors-with-FDA-experience-for-a-biotech-startup.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LinkedIn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, LinkedIn’s search has been more complicated. You can type in a few words to find the right people or rely on many different LinkedIn filters in the hope of getting the right results. Plus, you also have to think about what kinds of words you might want to use to get the best out of the search system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With lexical search, you have to know the exact title of the person, or you need to wrestle with filters to find the right person, maybe. And if you didn’t know the right combination, the right person remained undiscovered. The new AI-powered people search is designed to be the fastest path to the person who can help you the most,” Rohan Rajiv, senior director of product management at LinkedIn, told TechCrunch over a call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said in its early testing, it has seen people use this to find others who can help them with their next job opportunity, expand their business, or boost their career prospects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Search has been one area where all internet platforms have been rushing to add AI. Seeing people gravitate toward chatbots like ChatGPT and Perplexity for answers, incumbent search engines like Google, Bing, Brave, and DuckDuckGo have added AI-powered answers. There are plenty of startups working on AI-powered people search as well. Reddit has also leaned heavily into AI-powered search and locked down its platform’s data, asking other companies to sign a licensing agreement for AI training and usage.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;LinkedIn is one of the most used sites in AI demos for AI agents, browsers, and assistants. However, the Microsoft-owned company has not put restrictions on its data just yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think we are still early in this age of browsers and how they are working on behalf of people. I think over time, we will have a more sturdy policy [around browsers],” Rajiv said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“On a broader note, I have seen a lot of demos that try to reason over a person’s LinkedIn network. This is sort of an area where I think it is going to be hard to find a substitute for the real thing because this is the worst the search has ever been.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067453" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Screen1.jpeg?w=331" width="331" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LinkedIn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;LinkedIn is rolling out AI-powered people search to premium users in the U.S. with plans to expand it to other geographies in the coming months. People who will have access to this feature will see “I’m looking for…” in the search bar instead of “Search.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The search is not perfect. You will get different results when you use a query like “people who co-founded a YC startup” as compared to using “Y Combinator” in the query. Also, when you search for “people who co-founded a voice AI startup,” some folks who have a LinkedIn top voice badge show up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said it is working on improving the way the search tool understands the query.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For the last two years, LinkedIn has tried to infuse AI into different parts of its platforms, including ad copies, content creation, personalized digests, hiring assistance, job hunting advice, and learning. The company is now finally adding AI to one of the most-used parts of the site: search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the company released a job search tool for members in the U.S., allowing them to search for jobs using natural language queries. Now, the company is extending the feature to people search.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users can use queries like “Find me investors in the healthcare sector with FDA experience,” people who “co-founded a productivity company and are based in NYC,” or “Who in my network can help me understand wireless networks.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067452" height="443" src="https://techcrunch.com/wp-content/uploads/2025/11/01D_Investors-with-FDA-experience-for-a-biotech-startup.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LinkedIn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, LinkedIn’s search has been more complicated. You can type in a few words to find the right people or rely on many different LinkedIn filters in the hope of getting the right results. Plus, you also have to think about what kinds of words you might want to use to get the best out of the search system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With lexical search, you have to know the exact title of the person, or you need to wrestle with filters to find the right person, maybe. And if you didn’t know the right combination, the right person remained undiscovered. The new AI-powered people search is designed to be the fastest path to the person who can help you the most,” Rohan Rajiv, senior director of product management at LinkedIn, told TechCrunch over a call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said in its early testing, it has seen people use this to find others who can help them with their next job opportunity, expand their business, or boost their career prospects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Search has been one area where all internet platforms have been rushing to add AI. Seeing people gravitate toward chatbots like ChatGPT and Perplexity for answers, incumbent search engines like Google, Bing, Brave, and DuckDuckGo have added AI-powered answers. There are plenty of startups working on AI-powered people search as well. Reddit has also leaned heavily into AI-powered search and locked down its platform’s data, asking other companies to sign a licensing agreement for AI training and usage.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;LinkedIn is one of the most used sites in AI demos for AI agents, browsers, and assistants. However, the Microsoft-owned company has not put restrictions on its data just yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think we are still early in this age of browsers and how they are working on behalf of people. I think over time, we will have a more sturdy policy [around browsers],” Rajiv said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“On a broader note, I have seen a lot of demos that try to reason over a person’s LinkedIn network. This is sort of an area where I think it is going to be hard to find a substitute for the real thing because this is the worst the search has ever been.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067453" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Screen1.jpeg?w=331" width="331" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LinkedIn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;LinkedIn is rolling out AI-powered people search to premium users in the U.S. with plans to expand it to other geographies in the coming months. People who will have access to this feature will see “I’m looking for…” in the search bar instead of “Search.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The search is not perfect. You will get different results when you use a query like “people who co-founded a YC startup” as compared to using “Y Combinator” in the query. Also, when you search for “people who co-founded a voice AI startup,” some folks who have a LinkedIn top voice badge show up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said it is working on improving the way the search tool understands the query.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/linkedin-adds-ai-powered-search-to-help-users-find-people/</guid><pubDate>Thu, 13 Nov 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] AWS, Google, Microsoft and OCI Boost AI Inference Performance for Cloud Customers With NVIDIA Dynamo (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/think-smart-dynamo-ai-inference-data-center/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of &lt;/i&gt;&lt;i&gt;Think SMART&lt;/i&gt;&lt;i&gt;, a series focused on how leading AI service providers, developers and enterprises can boost their &lt;/i&gt;&lt;i&gt;inference performance&lt;/i&gt;&lt;i&gt; and return on investment with the latest advancements from NVIDIA’s full-stack &lt;/i&gt;&lt;i&gt;inference platform&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell delivers the highest performance and efficiency, and lowest total cost of ownership across every tested model and use case in the recent independent SemiAnalysis InferenceMAX v1 benchmark.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_87256"&gt;&lt;img alt="alt" class="wp-image-87256 size-large" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/think-smart-codesign-slide-1680x1120.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87256"&gt;NVIDIA CEO Jensen Huang highlighted at NVIDIA GTC Washington, D.C., how Blackwell delivers 10x the performance of NVIDIA Hopper, enabling 10x the revenue.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Achieving this industry-leading performance for today’s most complex AI models, such as large-scale mixture-of-experts (MoE) models, requires distributing (or disaggregating) inference across multiple servers (nodes) to serve millions of concurrent users and deliver faster responses.&lt;/p&gt;
&lt;p&gt;The NVIDIA Dynamo software platform unlocks these powerful multi-node capabilities for production, enabling enterprises to achieve this same benchmark-winning performance and efficiency across their existing cloud environments. Read on to learn how the shift to multi-node inference is driving performance, as well as how cloud platforms are putting this technology to work.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Tapping Disaggregated Inference for Optimized Performance&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For AI models that fit on a single GPU or server, developers often run many identical replicas of&amp;nbsp; the model in parallel across multiple nodes to deliver high throughput. In a recent paper, Russ Fellows, principal analyst at Signal65, showed that this approach achieved an industry-first record aggregate throughput of 1.1 million tokens per second with 72 NVIDIA Blackwell Ultra GPUs.&lt;/p&gt;
&lt;p&gt;When scaling AI models to serve many concurrent users in real time, or when managing demanding workloads with long input sequences, using a technique called disaggregated serving unlocks further performance and efficiency gains.&lt;/p&gt;
&lt;p&gt;Serving AI models involves two phases: processing the input prompt (prefill) and generating the output (decode). Traditionally, both phases run on the same GPUs, which can create inefficiencies and resource bottlenecks.&lt;/p&gt;
&lt;p&gt;Disaggregated serving solves this by intelligently distributing these tasks to independently optimized GPUs. This approach ensures that each part of the workload runs with the optimization techniques best suited for it, maximizing overall performance. For today’s large AI reasoning and MoE models, such as DeepSeek-R1, disaggregated serving is essential.&lt;/p&gt;
&lt;p&gt;NVIDIA Dynamo easily brings features like disaggregated serving to production scale across GPU clusters.&lt;/p&gt;
&lt;p&gt;It’s already delivering value.&lt;/p&gt;
&lt;p&gt;Baseten, for example, used NVIDIA Dynamo to speed up inference serving for long-context code generation by 2x and increase throughput by 1.6x, all without incremental hardware costs. Such software-driven performance boosts enable AI providers to significantly reduce the costs to manufacture intelligence.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Scaling Disaggregated Inference in the Cloud&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Much like it did for large-scale AI training, Kubernetes — the industry standard for containerized application management — is well-positioned to scale disaggregated serving across dozens or even hundreds of nodes for enterprise-scale AI deployments.&lt;/p&gt;
&lt;p&gt;With NVIDIA Dynamo now integrated into managed Kubernetes services from all major cloud providers, customers can scale multi-node inference across NVIDIA Blackwell systems, including GB200 and GB300 NVL72, with the performance, flexibility and reliability that enterprise AI deployments demand.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services is accelerating generative AI inference for its customers with NVIDIA Dynamo and integrated with Amazon EKS.&lt;/li&gt;
&lt;li&gt;Google Cloud is providing&amp;nbsp;&amp;nbsp;Dynamo recipe to optimize large language model (LLM) inference at enterprise scale on its AI Hypercomputer.&lt;/li&gt;
&lt;li&gt;Microsoft Azure is enabling multi-node LLM inference with NVIDIA Dynamo and ND GB200-v6 GPUs on Azure Kubernetes Service.&lt;/li&gt;
&lt;li&gt;Oracle Cloud Infrastructure (OCI) is enabling multi-node LLM inferencing with OCI Superclusters and NVIDIA Dynamo.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The push towards enabling large-scale, multi-node inference extends beyond hyperscalers.&lt;/p&gt;
&lt;p&gt;Nebius, for example, is designing its cloud to serve inference workloads at scale, built on NVIDIA accelerated computing infrastructure and working with NVIDIA Dynamo as an ecosystem partner.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Simplifying Inference on Kubernetes With NVIDIA Grove in NVIDIA Dynamo&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Disaggregated AI inference requires coordinating a team of specialized components — prefill, decode, routing and more — each with different needs. The challenge for Kubernetes is no longer about running more parallel copies of a model, but rather about masterfully conducting these distinct components as one cohesive, high-performance system.&lt;/p&gt;
&lt;p&gt;NVIDIA Grove, an application programming interface now available within NVIDIA Dynamo, allows users to provide a single, high-level specification that describes their entire inference system.&lt;/p&gt;
&lt;p&gt;For example, in that single specification, a user could simply declare their requirements: “I need three GPU nodes for prefill and six GPU nodes for decode, and I require all nodes for a single model replica to be placed on the same high-speed interconnect for the quickest possible response.”&lt;/p&gt;
&lt;p&gt;From that specification, Grove automatically handles all the intricate coordination: scaling related components together while maintaining correct ratios and dependencies, starting them in the right order and placing them strategically across the cluster for fast, efficient communication. Learn more about how to get started with NVIDIA Grove in this technical deep dive.&lt;/p&gt;
&lt;p&gt;As AI inference becomes increasingly distributed, the combination of Kubernetes and NVIDIA Dynamo with NVIDIA Grove simplifies how developers build and scale intelligent applications.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Try &lt;/i&gt;&lt;i&gt;NVIDIA’s AI-at-scale simulation&lt;/i&gt;&lt;i&gt; to see how hardware and deployment choices affect performance, efficiency and user experience. &lt;/i&gt;&lt;i&gt;To dive deeper on disaggregated serving and learn how Dynamo and NVIDIA GB200 NVL72 systems work together to boost inference performance, read this &lt;/i&gt;&lt;i&gt;technical blog&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For monthly updates, sign up for the NVIDIA Think SMART newsletter.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of &lt;/i&gt;&lt;i&gt;Think SMART&lt;/i&gt;&lt;i&gt;, a series focused on how leading AI service providers, developers and enterprises can boost their &lt;/i&gt;&lt;i&gt;inference performance&lt;/i&gt;&lt;i&gt; and return on investment with the latest advancements from NVIDIA’s full-stack &lt;/i&gt;&lt;i&gt;inference platform&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell delivers the highest performance and efficiency, and lowest total cost of ownership across every tested model and use case in the recent independent SemiAnalysis InferenceMAX v1 benchmark.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_87256"&gt;&lt;img alt="alt" class="wp-image-87256 size-large" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/think-smart-codesign-slide-1680x1120.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87256"&gt;NVIDIA CEO Jensen Huang highlighted at NVIDIA GTC Washington, D.C., how Blackwell delivers 10x the performance of NVIDIA Hopper, enabling 10x the revenue.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Achieving this industry-leading performance for today’s most complex AI models, such as large-scale mixture-of-experts (MoE) models, requires distributing (or disaggregating) inference across multiple servers (nodes) to serve millions of concurrent users and deliver faster responses.&lt;/p&gt;
&lt;p&gt;The NVIDIA Dynamo software platform unlocks these powerful multi-node capabilities for production, enabling enterprises to achieve this same benchmark-winning performance and efficiency across their existing cloud environments. Read on to learn how the shift to multi-node inference is driving performance, as well as how cloud platforms are putting this technology to work.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Tapping Disaggregated Inference for Optimized Performance&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For AI models that fit on a single GPU or server, developers often run many identical replicas of&amp;nbsp; the model in parallel across multiple nodes to deliver high throughput. In a recent paper, Russ Fellows, principal analyst at Signal65, showed that this approach achieved an industry-first record aggregate throughput of 1.1 million tokens per second with 72 NVIDIA Blackwell Ultra GPUs.&lt;/p&gt;
&lt;p&gt;When scaling AI models to serve many concurrent users in real time, or when managing demanding workloads with long input sequences, using a technique called disaggregated serving unlocks further performance and efficiency gains.&lt;/p&gt;
&lt;p&gt;Serving AI models involves two phases: processing the input prompt (prefill) and generating the output (decode). Traditionally, both phases run on the same GPUs, which can create inefficiencies and resource bottlenecks.&lt;/p&gt;
&lt;p&gt;Disaggregated serving solves this by intelligently distributing these tasks to independently optimized GPUs. This approach ensures that each part of the workload runs with the optimization techniques best suited for it, maximizing overall performance. For today’s large AI reasoning and MoE models, such as DeepSeek-R1, disaggregated serving is essential.&lt;/p&gt;
&lt;p&gt;NVIDIA Dynamo easily brings features like disaggregated serving to production scale across GPU clusters.&lt;/p&gt;
&lt;p&gt;It’s already delivering value.&lt;/p&gt;
&lt;p&gt;Baseten, for example, used NVIDIA Dynamo to speed up inference serving for long-context code generation by 2x and increase throughput by 1.6x, all without incremental hardware costs. Such software-driven performance boosts enable AI providers to significantly reduce the costs to manufacture intelligence.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Scaling Disaggregated Inference in the Cloud&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Much like it did for large-scale AI training, Kubernetes — the industry standard for containerized application management — is well-positioned to scale disaggregated serving across dozens or even hundreds of nodes for enterprise-scale AI deployments.&lt;/p&gt;
&lt;p&gt;With NVIDIA Dynamo now integrated into managed Kubernetes services from all major cloud providers, customers can scale multi-node inference across NVIDIA Blackwell systems, including GB200 and GB300 NVL72, with the performance, flexibility and reliability that enterprise AI deployments demand.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services is accelerating generative AI inference for its customers with NVIDIA Dynamo and integrated with Amazon EKS.&lt;/li&gt;
&lt;li&gt;Google Cloud is providing&amp;nbsp;&amp;nbsp;Dynamo recipe to optimize large language model (LLM) inference at enterprise scale on its AI Hypercomputer.&lt;/li&gt;
&lt;li&gt;Microsoft Azure is enabling multi-node LLM inference with NVIDIA Dynamo and ND GB200-v6 GPUs on Azure Kubernetes Service.&lt;/li&gt;
&lt;li&gt;Oracle Cloud Infrastructure (OCI) is enabling multi-node LLM inferencing with OCI Superclusters and NVIDIA Dynamo.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The push towards enabling large-scale, multi-node inference extends beyond hyperscalers.&lt;/p&gt;
&lt;p&gt;Nebius, for example, is designing its cloud to serve inference workloads at scale, built on NVIDIA accelerated computing infrastructure and working with NVIDIA Dynamo as an ecosystem partner.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Simplifying Inference on Kubernetes With NVIDIA Grove in NVIDIA Dynamo&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Disaggregated AI inference requires coordinating a team of specialized components — prefill, decode, routing and more — each with different needs. The challenge for Kubernetes is no longer about running more parallel copies of a model, but rather about masterfully conducting these distinct components as one cohesive, high-performance system.&lt;/p&gt;
&lt;p&gt;NVIDIA Grove, an application programming interface now available within NVIDIA Dynamo, allows users to provide a single, high-level specification that describes their entire inference system.&lt;/p&gt;
&lt;p&gt;For example, in that single specification, a user could simply declare their requirements: “I need three GPU nodes for prefill and six GPU nodes for decode, and I require all nodes for a single model replica to be placed on the same high-speed interconnect for the quickest possible response.”&lt;/p&gt;
&lt;p&gt;From that specification, Grove automatically handles all the intricate coordination: scaling related components together while maintaining correct ratios and dependencies, starting them in the right order and placing them strategically across the cluster for fast, efficient communication. Learn more about how to get started with NVIDIA Grove in this technical deep dive.&lt;/p&gt;
&lt;p&gt;As AI inference becomes increasingly distributed, the combination of Kubernetes and NVIDIA Dynamo with NVIDIA Grove simplifies how developers build and scale intelligent applications.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Try &lt;/i&gt;&lt;i&gt;NVIDIA’s AI-at-scale simulation&lt;/i&gt;&lt;i&gt; to see how hardware and deployment choices affect performance, efficiency and user experience. &lt;/i&gt;&lt;i&gt;To dive deeper on disaggregated serving and learn how Dynamo and NVIDIA GB200 NVL72 systems work together to boost inference performance, read this &lt;/i&gt;&lt;i&gt;technical blog&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For monthly updates, sign up for the NVIDIA Think SMART newsletter.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/think-smart-dynamo-ai-inference-data-center/</guid><pubDate>Thu, 13 Nov 2025 16:00:58 +0000</pubDate></item><item><title>[NEW] Google’s SIMA 2 agent uses Gemini to reason and act in virtual worlds (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/googles-sima-2-agent-uses-gemini-to-reason-and-act-in-virtual-worlds/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind shared on Thursday a research preview of SIMA 2, the next generation of its generalist AI agent that integrates the language and reasoning powers of Gemini, Google’s large language model, to move beyond simply following instructions to understanding and interacting with its environment.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like many of DeepMind’s projects, including AlphaFold, the first version of SIMA was trained on hundreds of hours of video game data to learn how to play multiple 3D games like a human, even some games it wasn’t trained on. SIMA 1, unveiled in March 2024, could follow basic instructions across a wide range of virtual environments, but it only had a 31% success rate for completing complex tasks, compared to 71% for humans.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“SIMA 2 is a step change and improvement in capabilities over SIMA 1,” Joe Marino, senior research scientist at DeepMind, said in a press briefing. “It’s a more general agent. It can complete complex tasks in previously unseen environments. And it’s a self-improving agent. So it can actually self-improve based on its own experience, which is a step towards more general-purpose robots and AGI systems more generally.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067551" height="533" src="https://techcrunch.com/wp-content/uploads/2025/11/SIMA-2-blog-figure-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;DeepMind says SIMA 2 doubles the performance of SIMA 1&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;SIMA 2 is powered by the Gemini 2.5 flash-lite model, and AGI refers to artificial general intelligence, which DeepMind defines as a system capable of a wide range of intellectual tasks with the ability to learn new skills and generalize knowledge across different areas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Working with so-called “embodied agents” is crucial to generalized intelligence, DeepMind’s researchers say. Marino explained that an embodied agent interacts with a physical or virtual world via a body — observing inputs and taking actions much like a robot or human would — whereas a non-embodied agent might interact with your calendar, take notes, or execute code.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jane Wang, a senior staff research scientist at DeepMind with a background in neuroscience, told TechCrunch that SIMA 2 goes far beyond gameplay.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re asking it to actually understand what’s happening, understand what the user is asking it to do, and then be able to respond in a common-sense way that’s actually quite difficult,” Wang said.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;By integrating Gemini, SIMA 2 doubled its predecessor’s performance, uniting Gemini’s advanced language and reasoning abilities with the embodied skills developed through training.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3067564" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/deepmind.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Marino demoed SIMA 2 in “No Man’s Sky,” where the agent described its surroundings — a rocky planet surface — and determined its next steps by recognizing and interacting with a distress beacon. SIMA 2 also uses Gemini to reason internally. In another game, when asked to walk to the house that’s the color of a ripe tomato, the agent showed its thinking — ripe tomatoes are red, therefore I should go to the red house — then found and approached it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Being Gemini-powered also means SIMA 2 follows instructions based on emojis: “You instruct it 🪓🌲, and it’ll go chop down a tree,” Marino said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Marino also demonstrated how SIMA 2 can navigate newly generated photorealistic worlds produced by Genie, DeepMind’s world model, correctly identifying and interacting with objects like benches, trees, and butterflies.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067552" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/SIMA-2-blog-figure-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;DeepMind says SIMA 2 is a self-improving agent&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini also enables self-improvement without much human data, Marino added. Where SIMA 1 was trained entirely on human gameplay, SIMA 2 uses it as a baseline to provide a strong initial model. When the team puts the agent into a new environment, it asks another Gemini model to create new tasks and a separate reward model to score the agent’s attempts. Using these self-generated experiences as training data, the agent learns from its own mistakes and gradually performs better, essentially teaching itself new behaviors through trial and error as a human would, guided by AI-based feedback instead of humans.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepMind sees SIMA 2 as a step toward unlocking more general-purpose robots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If we think of what a system needs to do to perform tasks in the real world, like a robot, I think there are two components of it,” Frederic Besse, senior staff research engineer at DeepMind, said during a press briefing. “First, there is a high-level understanding of the real world and what needs to be done, as well as some reasoning.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you ask a humanoid robot in your house to go check how many cans of beans you have in the cupboard, the system needs to understand all of the different concepts — what beans are, what a cupboard is — and navigate to that location. Besse says SIMA 2 touches more on that high-level behavior than it does on lower-level actions, which he refers to as controlling things like physical joints and wheels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The team declined to share a specific timeline for implementing SIMA 2 in physical robotics systems. Besse told TechCrunch that DeepMind’s recently unveiled robotics foundation models — which can also reason about the physical world and create multi-step plans to complete a mission — were trained differently and separately from SIMA.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While there’s also no timeline for releasing more than a preview of SIMA 2, Wang told TechCrunch the goal is to show the world what DeepMind has been working on and see what kinds of collaborations and potential uses are possible.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind shared on Thursday a research preview of SIMA 2, the next generation of its generalist AI agent that integrates the language and reasoning powers of Gemini, Google’s large language model, to move beyond simply following instructions to understanding and interacting with its environment.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like many of DeepMind’s projects, including AlphaFold, the first version of SIMA was trained on hundreds of hours of video game data to learn how to play multiple 3D games like a human, even some games it wasn’t trained on. SIMA 1, unveiled in March 2024, could follow basic instructions across a wide range of virtual environments, but it only had a 31% success rate for completing complex tasks, compared to 71% for humans.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“SIMA 2 is a step change and improvement in capabilities over SIMA 1,” Joe Marino, senior research scientist at DeepMind, said in a press briefing. “It’s a more general agent. It can complete complex tasks in previously unseen environments. And it’s a self-improving agent. So it can actually self-improve based on its own experience, which is a step towards more general-purpose robots and AGI systems more generally.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067551" height="533" src="https://techcrunch.com/wp-content/uploads/2025/11/SIMA-2-blog-figure-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;DeepMind says SIMA 2 doubles the performance of SIMA 1&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;SIMA 2 is powered by the Gemini 2.5 flash-lite model, and AGI refers to artificial general intelligence, which DeepMind defines as a system capable of a wide range of intellectual tasks with the ability to learn new skills and generalize knowledge across different areas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Working with so-called “embodied agents” is crucial to generalized intelligence, DeepMind’s researchers say. Marino explained that an embodied agent interacts with a physical or virtual world via a body — observing inputs and taking actions much like a robot or human would — whereas a non-embodied agent might interact with your calendar, take notes, or execute code.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jane Wang, a senior staff research scientist at DeepMind with a background in neuroscience, told TechCrunch that SIMA 2 goes far beyond gameplay.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re asking it to actually understand what’s happening, understand what the user is asking it to do, and then be able to respond in a common-sense way that’s actually quite difficult,” Wang said.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;By integrating Gemini, SIMA 2 doubled its predecessor’s performance, uniting Gemini’s advanced language and reasoning abilities with the embodied skills developed through training.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3067564" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/deepmind.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Marino demoed SIMA 2 in “No Man’s Sky,” where the agent described its surroundings — a rocky planet surface — and determined its next steps by recognizing and interacting with a distress beacon. SIMA 2 also uses Gemini to reason internally. In another game, when asked to walk to the house that’s the color of a ripe tomato, the agent showed its thinking — ripe tomatoes are red, therefore I should go to the red house — then found and approached it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Being Gemini-powered also means SIMA 2 follows instructions based on emojis: “You instruct it 🪓🌲, and it’ll go chop down a tree,” Marino said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Marino also demonstrated how SIMA 2 can navigate newly generated photorealistic worlds produced by Genie, DeepMind’s world model, correctly identifying and interacting with objects like benches, trees, and butterflies.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067552" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/SIMA-2-blog-figure-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;DeepMind says SIMA 2 is a self-improving agent&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini also enables self-improvement without much human data, Marino added. Where SIMA 1 was trained entirely on human gameplay, SIMA 2 uses it as a baseline to provide a strong initial model. When the team puts the agent into a new environment, it asks another Gemini model to create new tasks and a separate reward model to score the agent’s attempts. Using these self-generated experiences as training data, the agent learns from its own mistakes and gradually performs better, essentially teaching itself new behaviors through trial and error as a human would, guided by AI-based feedback instead of humans.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepMind sees SIMA 2 as a step toward unlocking more general-purpose robots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If we think of what a system needs to do to perform tasks in the real world, like a robot, I think there are two components of it,” Frederic Besse, senior staff research engineer at DeepMind, said during a press briefing. “First, there is a high-level understanding of the real world and what needs to be done, as well as some reasoning.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you ask a humanoid robot in your house to go check how many cans of beans you have in the cupboard, the system needs to understand all of the different concepts — what beans are, what a cupboard is — and navigate to that location. Besse says SIMA 2 touches more on that high-level behavior than it does on lower-level actions, which he refers to as controlling things like physical joints and wheels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The team declined to share a specific timeline for implementing SIMA 2 in physical robotics systems. Besse told TechCrunch that DeepMind’s recently unveiled robotics foundation models — which can also reason about the physical world and create multi-step plans to complete a mission — were trained differently and separately from SIMA.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While there’s also no timeline for releasing more than a preview of SIMA 2, Wang told TechCrunch the goal is to show the world what DeepMind has been working on and see what kinds of collaborations and potential uses are possible.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/googles-sima-2-agent-uses-gemini-to-reason-and-act-in-virtual-worlds/</guid><pubDate>Thu, 13 Nov 2025 16:15:47 +0000</pubDate></item><item><title>[NEW] Google’s NotebookLM adds ‘Deep Research’ tool, support for more file types (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/googles-notebooklm-adds-deep-research-tool-support-for-more-file-types/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is updating NotebookLM, its AI note-taking and research assistant, with a new tool to help users simplify complex research, along with support for additional file types.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The service is rolling out “Deep Research,” a tool that will automate and simplify complex online research. Google says the tool acts like a dedicated researcher, as it can synthesize a detailed report or recommend relevant articles, papers, or websites.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Deep Research can take your question, create a research plan, and then browse websites on your behalf. After a few minutes, it will present you with a source-grounded report that you can add directly into your notebook. While Deep Research runs in the background, you can continue to add other sources. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea behind Deep Research is to help you create a deep, organized knowledge base on a topic without having to leave your workflow. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067541" height="372" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-13-at-10.40.46-AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can access Deep Research by starting a search in the source panel and selecting “Web” as a source. Then, you can choose your research style. You can select “Deep Research” when you’re looking for a full briefing and in-depth analysis. Or, you can choose “Fast Research” if you want a quick search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the additional support for more file types, you can now upload Google Sheets, Drive files as URLs, PDFs from Google Drive, and Microsoft Word Documents. The tech giant says this change will allow users to do things like generate summaries from spreadsheets and quickly copy-paste multiple Drive files as URLs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says these new updates should be available to all users within a week. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Since its launch in late 2023, Google has been building out NotebookLM with additional functionality. Earlier this year, Google introduced Video Overviews to NotebookLM to allow users to&amp;nbsp;turn dense multimedia, such as raw notes, PDFs, and images, into digestible visual presentations. The addition builds on&amp;nbsp;the Audio Overviews feature, which generates an AI podcast based on documents shared with NotebookLM, such as course readings or legal briefs. In May, Google released the NotebookLM apps for&amp;nbsp;Android&amp;nbsp;and&amp;nbsp;iOS, making the service available beyond desktop.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is updating NotebookLM, its AI note-taking and research assistant, with a new tool to help users simplify complex research, along with support for additional file types.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The service is rolling out “Deep Research,” a tool that will automate and simplify complex online research. Google says the tool acts like a dedicated researcher, as it can synthesize a detailed report or recommend relevant articles, papers, or websites.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Deep Research can take your question, create a research plan, and then browse websites on your behalf. After a few minutes, it will present you with a source-grounded report that you can add directly into your notebook. While Deep Research runs in the background, you can continue to add other sources. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea behind Deep Research is to help you create a deep, organized knowledge base on a topic without having to leave your workflow. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067541" height="372" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-13-at-10.40.46-AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can access Deep Research by starting a search in the source panel and selecting “Web” as a source. Then, you can choose your research style. You can select “Deep Research” when you’re looking for a full briefing and in-depth analysis. Or, you can choose “Fast Research” if you want a quick search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the additional support for more file types, you can now upload Google Sheets, Drive files as URLs, PDFs from Google Drive, and Microsoft Word Documents. The tech giant says this change will allow users to do things like generate summaries from spreadsheets and quickly copy-paste multiple Drive files as URLs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says these new updates should be available to all users within a week. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Since its launch in late 2023, Google has been building out NotebookLM with additional functionality. Earlier this year, Google introduced Video Overviews to NotebookLM to allow users to&amp;nbsp;turn dense multimedia, such as raw notes, PDFs, and images, into digestible visual presentations. The addition builds on&amp;nbsp;the Audio Overviews feature, which generates an AI podcast based on documents shared with NotebookLM, such as course readings or legal briefs. In May, Google released the NotebookLM apps for&amp;nbsp;Android&amp;nbsp;and&amp;nbsp;iOS, making the service available beyond desktop.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/googles-notebooklm-adds-deep-research-tool-support-for-more-file-types/</guid><pubDate>Thu, 13 Nov 2025 17:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI’s new LLM exposes the secrets of how AI really works (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/13/1127914/openais-new-llm-exposes-the-secrets-of-how-ai-really-works/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/box-view.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;ChatGPT maker OpenAI has built an experimental large language model that is far easier to understand than typical models.&lt;/p&gt;  &lt;p&gt;That’s a big deal, because today’s LLMs are black boxes: Nobody fully understands how they do what they do. Building a model that is more transparent sheds light on how LLMs work in general, helping researchers figure out why models hallucinate, why they go off the rails, and just how far we should trust them with critical tasks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“As these AI systems get more powerful, they’re going to get integrated more and more into very important domains,” Leo Gao, a research scientist at OpenAI, told &lt;em&gt;MIT Technology Review&lt;/em&gt; in an exclusive preview of the new work. “It’s very important to make sure they’re safe.”&lt;/p&gt;  &lt;p&gt;This is still early research. The new model, called a weight-sparse transformer, is far smaller and far less capable than top-tier mass-market models like the firm’s GPT-5, Anthropic’s Claude, and Google DeepMind’s Gemini. At most it’s as capable as GPT-1, a model that OpenAI developed back in 2018, says Gao (though he and his colleagues haven’t done a direct comparison).&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But the aim isn’t to compete with the best in class (at least, not yet). Instead, by looking at how this experimental model works, OpenAI hopes to learn about the hidden mechanisms inside those bigger and better versions of the technology.&lt;/p&gt;  &lt;p&gt;It’s interesting research, says Elisenda Grigsby, a mathematician at Boston College who studies how LLMs work and who was not involved in the project: “I’m sure the methods it introduces will have a significant impact.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Lee Sharkey, a research scientist at AI startup Goodfire, agrees. “This work aims at the right target and seems well executed,” he says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why models are so hard to understand&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenAI’s work is part of a hot new field of research known as mechanistic interpretability, which is trying to map the internal mechanisms that models use when they carry out different tasks.&lt;/p&gt;  &lt;p&gt;That’s harder than it sounds. LLMs are built from neural networks, which consist of nodes, called neurons, arranged in layers. In most networks, each neuron is connected to every other neuron in its adjacent layers. Such a network is known as a dense network.&lt;/p&gt;  &lt;p&gt;Dense networks are relatively efficient to train and run, but they spread what they learn across a vast knot of connections. The result is that simple concepts or functions can be split up between neurons in different parts of a model. At the same time, specific neurons can also end up representing multiple different features, a phenomenon known as superposition (a term borrowed from quantum physics). The upshot is that you can’t relate specific parts of a model to specific concepts.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;“Neural networks are big and complicated and tangled up and very difficult to understand,” says Dan Mossing, who leads the mechanistic interpretability team at OpenAI. “We’ve sort of said: ‘Okay, what if we tried to make that not the case?’”&lt;/p&gt;  &lt;p&gt;Instead of building a model using a dense network, OpenAI started with a type of neural network known as a weight-sparse transformer, in which each neuron is connected to only a few other neurons. This forced the model to represent features in localized clusters rather than spread them out.&lt;/p&gt;  &lt;p&gt;Their model is far slower than any LLM on the market. But it is easier to relate its neurons or groups of neurons to specific concepts and functions. “There’s a really drastic difference in how interpretable the model is,” says Gao.&lt;/p&gt;  &lt;p&gt;Gao and his colleagues have tested the new model with very simple tasks. For example, they asked it to complete a block of text that opens with quotation marks by adding matching marks at the end.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;It’s a trivial request for an LLM. The point is that figuring out how a model does even a straightforward task like that involves unpicking a complicated tangle of neurons and connections, says Gao. But with the new model, they were able to follow the exact steps the model took.&lt;/p&gt;  &lt;p&gt;“We actually found a circuit that’s exactly the algorithm you would think to implement by hand, but it’s fully learned by the model,” he says. “I think this is really cool and exciting.”&lt;/p&gt;  &lt;p&gt;Where will the research go next? Grigsby is not convinced the technique would scale up to larger models that have to handle a variety of more difficult tasks.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Gao and Mossing acknowledge that this is a big limitation of the model they have built so far and agree that the approach will never lead to models that match the performance of cutting-edge products like GPT-5. And yet OpenAI thinks it might be able to improve the technique enough to build a transparent model on a par with GPT-3, the firm’s breakthrough 2021 LLM.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Maybe within a few years, we could have a fully interpretable GPT-3, so that you could go inside every single part of it and you could understand how it does every single thing,” says Gao. “If we had such a system, we would learn so much.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/box-view.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;ChatGPT maker OpenAI has built an experimental large language model that is far easier to understand than typical models.&lt;/p&gt;  &lt;p&gt;That’s a big deal, because today’s LLMs are black boxes: Nobody fully understands how they do what they do. Building a model that is more transparent sheds light on how LLMs work in general, helping researchers figure out why models hallucinate, why they go off the rails, and just how far we should trust them with critical tasks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“As these AI systems get more powerful, they’re going to get integrated more and more into very important domains,” Leo Gao, a research scientist at OpenAI, told &lt;em&gt;MIT Technology Review&lt;/em&gt; in an exclusive preview of the new work. “It’s very important to make sure they’re safe.”&lt;/p&gt;  &lt;p&gt;This is still early research. The new model, called a weight-sparse transformer, is far smaller and far less capable than top-tier mass-market models like the firm’s GPT-5, Anthropic’s Claude, and Google DeepMind’s Gemini. At most it’s as capable as GPT-1, a model that OpenAI developed back in 2018, says Gao (though he and his colleagues haven’t done a direct comparison).&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But the aim isn’t to compete with the best in class (at least, not yet). Instead, by looking at how this experimental model works, OpenAI hopes to learn about the hidden mechanisms inside those bigger and better versions of the technology.&lt;/p&gt;  &lt;p&gt;It’s interesting research, says Elisenda Grigsby, a mathematician at Boston College who studies how LLMs work and who was not involved in the project: “I’m sure the methods it introduces will have a significant impact.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Lee Sharkey, a research scientist at AI startup Goodfire, agrees. “This work aims at the right target and seems well executed,” he says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why models are so hard to understand&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenAI’s work is part of a hot new field of research known as mechanistic interpretability, which is trying to map the internal mechanisms that models use when they carry out different tasks.&lt;/p&gt;  &lt;p&gt;That’s harder than it sounds. LLMs are built from neural networks, which consist of nodes, called neurons, arranged in layers. In most networks, each neuron is connected to every other neuron in its adjacent layers. Such a network is known as a dense network.&lt;/p&gt;  &lt;p&gt;Dense networks are relatively efficient to train and run, but they spread what they learn across a vast knot of connections. The result is that simple concepts or functions can be split up between neurons in different parts of a model. At the same time, specific neurons can also end up representing multiple different features, a phenomenon known as superposition (a term borrowed from quantum physics). The upshot is that you can’t relate specific parts of a model to specific concepts.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;“Neural networks are big and complicated and tangled up and very difficult to understand,” says Dan Mossing, who leads the mechanistic interpretability team at OpenAI. “We’ve sort of said: ‘Okay, what if we tried to make that not the case?’”&lt;/p&gt;  &lt;p&gt;Instead of building a model using a dense network, OpenAI started with a type of neural network known as a weight-sparse transformer, in which each neuron is connected to only a few other neurons. This forced the model to represent features in localized clusters rather than spread them out.&lt;/p&gt;  &lt;p&gt;Their model is far slower than any LLM on the market. But it is easier to relate its neurons or groups of neurons to specific concepts and functions. “There’s a really drastic difference in how interpretable the model is,” says Gao.&lt;/p&gt;  &lt;p&gt;Gao and his colleagues have tested the new model with very simple tasks. For example, they asked it to complete a block of text that opens with quotation marks by adding matching marks at the end.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;It’s a trivial request for an LLM. The point is that figuring out how a model does even a straightforward task like that involves unpicking a complicated tangle of neurons and connections, says Gao. But with the new model, they were able to follow the exact steps the model took.&lt;/p&gt;  &lt;p&gt;“We actually found a circuit that’s exactly the algorithm you would think to implement by hand, but it’s fully learned by the model,” he says. “I think this is really cool and exciting.”&lt;/p&gt;  &lt;p&gt;Where will the research go next? Grigsby is not convinced the technique would scale up to larger models that have to handle a variety of more difficult tasks.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Gao and Mossing acknowledge that this is a big limitation of the model they have built so far and agree that the approach will never lead to models that match the performance of cutting-edge products like GPT-5. And yet OpenAI thinks it might be able to improve the technique enough to build a transparent model on a par with GPT-3, the firm’s breakthrough 2021 LLM.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Maybe within a few years, we could have a fully interpretable GPT-3, so that you could go inside every single part of it and you could understand how it does every single thing,” says Gao. “If we had such a system, we would learn so much.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/13/1127914/openais-new-llm-exposes-the-secrets-of-how-ai-really-works/</guid><pubDate>Thu, 13 Nov 2025 18:00:00 +0000</pubDate></item><item><title>[NEW] Upwork study shows AI agents excel with human partners but fail independently (AI | VentureBeat)</title><link>https://venturebeat.com/ai/upwork-study-shows-ai-agents-excel-with-human-partners-but-fail</link><description>[unable to retrieve full-text content]&lt;p&gt;Artificial intelligence agents powered by the world&amp;#x27;s most advanced language models routinely fail to complete even straightforward professional tasks on their own, according to &lt;a href="https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf"&gt;&lt;u&gt;groundbreaking research&lt;/u&gt;&lt;/a&gt; released Thursday by &lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt;, the largest online work marketplace.&lt;/p&gt;&lt;p&gt;But the same study reveals a more promising path forward: When AI agents collaborate with human experts, &lt;a href="https://www.globenewswire.com/news-release/2025/11/13/3187462/0/en/Upwork-Human-Agent-Productivity-Index-Reveals-Up-to-70-Boost-in-Work-Completion-from-Human-and-AI-Agent-Collaboration-vs-Agents-Working-Alone.html"&gt;&lt;u&gt;project completion rates surge by up to 70%&lt;/u&gt;&lt;/a&gt;, suggesting the future of work may not pit humans against machines but rather pair them together in powerful new ways.&lt;/p&gt;&lt;p&gt;The findings, drawn from more than 300 real client projects posted to Upwork&amp;#x27;s platform, marking the first systematic evaluation of how human expertise amplifies AI agent performance in actual professional work — not synthetic tests or academic simulations. The research challenges both the hype around fully autonomous AI agents and fears that such technology will imminently replace knowledge workers.&lt;/p&gt;&lt;p&gt;&amp;quot;AI agents aren&amp;#x27;t that agentic, meaning they aren&amp;#x27;t that good,&amp;quot; Andrew Rabinovich, Upwork&amp;#x27;s chief technology officer and head of AI and machine learning, said in an exclusive interview with VentureBeat. &amp;quot;However, when paired with expert human professionals, project completion rates improve dramatically, supporting our firm belief that the future of work will be defined by humans and AI collaborating to get more work done, with human intuition and domain expertise playing a critical role.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How AI agents performed on 300+ real freelance jobs—and why they struggled&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.upwork.com/human-agent-productivity-index"&gt;&lt;u&gt;Upwork&amp;#x27;s Human+Agent Productivity Index (HAPI) &lt;/u&gt;&lt;/a&gt;evaluated how three leading AI systems — &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt;, OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt;, and Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; — performed on actual jobs posted by paying clients across categories including writing, data science, web development, engineering, sales, and translation.&lt;/p&gt;&lt;p&gt;Critically, Upwork deliberately selected simple, well-defined projects where AI agents stood a reasonable chance of success. These jobs, priced under $500, represent less than 6% of Upwork&amp;#x27;s total gross services volume — a tiny fraction of the platform&amp;#x27;s overall business and an acknowledgment of current AI limitations.&lt;/p&gt;&lt;p&gt;&amp;quot;The reality is that although we study AI, and I&amp;#x27;ve been doing this for 25 years, and we see significant breakthroughs, the reality is that these agents aren&amp;#x27;t that agentic,&amp;quot; Rabinovich told VentureBeat. &amp;quot;So if we go up the value chain, the problems become so much more difficult, then we don&amp;#x27;t think they can solve them at all, even to scratch the surface. So we specifically chose simpler tasks that would give an agent some kind of traction.&amp;quot;&lt;/p&gt;&lt;p&gt;Even on these deliberately simplified tasks, AI agents working independently struggled. But when expert freelancers provided feedback — spending an average of just 20 minutes per review cycle — the agents&amp;#x27; performance improved substantially with each iteration.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;20 minutes of human feedback boosted AI completion rates up to 70%&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research reveals stark differences in how AI agents perform with and without human guidance across different types of work. For data science and analytics projects, Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; achieved a 64% completion rate working alone but jumped to 93% after receiving feedback from a human expert. In sales and marketing work, &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt;&amp;#x27;s completion rate rose from 17% independently to 31% with human input. OpenAI&amp;#x27;s &lt;a href="https://openai.com/gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; showed similarly dramatic improvements in engineering and architecture tasks, climbing from 30% to 50% completion.&lt;/p&gt;&lt;p&gt;The pattern held across virtually all categories, with agents responding particularly well to human feedback on qualitative, creative work requiring editorial judgment — areas like writing, translation, and marketing — where completion rates increased by up to 17 percentage points per feedback cycle.&lt;/p&gt;&lt;p&gt;The finding challenges a fundamental assumption in the AI industry: that agent benchmarks conducted in isolation accurately predict real-world performance.&lt;/p&gt;&lt;p&gt;&amp;quot;While we show that in the tasks that we have selected for agents to perform in isolation, they perform similarly to the previous results that we&amp;#x27;ve seen published openly, what we&amp;#x27;ve shown is that in collaboration with humans, the performance of these agents improves surprisingly well,&amp;quot; Rabinovich said. &amp;quot;It&amp;#x27;s not just a one-turn back and forth, but the more feedback the human provides, the better the agent gets at performing.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why ChatGPT can ace the SAT but can&amp;#x27;t count the R&amp;#x27;s in &amp;#x27;strawberry&amp;#x27;&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research arrives as the AI industry grapples with a measurement crisis. Traditional benchmarks — standardized tests that AI models can master, sometimes scoring perfectly on SAT exams or mathematics olympiads — have proven poor predictors of real-world capability.&lt;/p&gt;&lt;p&gt;&amp;quot;With advances of large language models, what we&amp;#x27;re now seeing is that these static, academic datasets are completely saturated,&amp;quot; Rabinovich said. &amp;quot;So you could get a perfect score in the SAT test or LSAT or any of the math olympiads, and then you would ask ChatGPT how many R&amp;#x27;s there are in the word strawberry, and it would get it wrong.&amp;quot;&lt;/p&gt;&lt;p&gt;This phenomenon — where AI systems ace formal tests but stumble on trivial real-world questions — has led to growing skepticism about AI capabilities, even as companies race to deploy autonomous agents. Several recent benchmarks from other firms have tested AI agents on Upwork jobs, but those evaluations measured only isolated performance, not the collaborative potential that Upwork&amp;#x27;s research reveals.&lt;/p&gt;&lt;p&gt;&amp;quot;We wanted to evaluate the quality of these agents on actual real work with economic value associated with it, and not only see how well these agents do, but also see how these agents do in collaboration with humans, because we sort of knew already that in isolation, they&amp;#x27;re not that advanced,&amp;quot; Rabinovich explained.&lt;/p&gt;&lt;p&gt;For &lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt;, which connects roughly 800,000 active clients posting more than 3 million jobs annually to a global pool of freelancers, the research serves a strategic business purpose: establishing quality standards for AI agents before allowing them to compete or collaborate with human workers on its platform.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The economics of human-AI teamwork: Why paying for expert feedback still saves money&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite requiring multiple rounds of human feedback — each lasting about 20 minutes — the time investment remains &amp;quot;orders of magnitude different between a human doing the work alone, versus a human doing the work with an AI agent,&amp;quot; Rabinovich said. Where a project might take a freelancer days to complete independently, the agent-plus-human approach can deliver results in hours through iterative cycles of automated work and expert refinement.&lt;/p&gt;&lt;p&gt;The economic implications extend beyond simple time savings. Upwork recently reported that gross services volume from &lt;a href="https://finance.yahoo.com/news/upworks-stock-soars-q3-blowout-201700500.html"&gt;&lt;u&gt;AI-related work grew 53% year-over-year&lt;/u&gt;&lt;/a&gt; in the third quarter of 2025, one of the strongest growth drivers for the company. But executives have been careful to frame AI not as a replacement for freelancers but as an enhancement to their capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;AI was a huge overhang for our valuation,&amp;quot; Erica Gessert, Upwork&amp;#x27;s CFO, told &lt;a href="https://www.cfobrew.com/stories/2025/10/16/the-cfo-of-upwork-answers-every-cfo-s-most-burning-question"&gt;&lt;u&gt;CFO Brew&lt;/u&gt;&lt;/a&gt; in October. &amp;quot;There was this belief that all work was going to go away. AI was going to take it, and especially work that&amp;#x27;s done by people like freelancers, because they are impermanent. Actually, the opposite is true.&amp;quot;&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s strategy centers on enabling freelancers to handle more complex, higher-value work by offloading routine tasks to AI. &amp;quot;Freelancers actually prefer to have tools that automate the manual labor and repetitive part of their work, and really focus on the creative and conceptual part of the process,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;Rather than replacing jobs, he argues, AI will transform them: &amp;quot;Simpler tasks will be automated by agents, but the jobs will become much more complex in the number of tasks, so the amount of work and therefore earnings for freelancers will actually only go up.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI coding agents excel, but creative writing and translation still need humans&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research reveals a clear pattern in agent capabilities. AI systems perform best on &amp;quot;deterministic and verifiable&amp;quot; tasks with objectively correct answers, like solving math problems or writing basic code. &amp;quot;Most coding tasks are very similar to each other,&amp;quot; Rabinovich noted. &amp;quot;That&amp;#x27;s why coding agents are becoming so good.&amp;quot;&lt;/p&gt;&lt;p&gt;In Upwork&amp;#x27;s tests, web development, mobile app development, and data science projects — especially those involving structured, computational work — saw the highest standalone agent completion rates. Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; completed 68% of web development jobs and 64% of data science projects without human help, while &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; achieved 74% on certain technical tasks.&lt;/p&gt;&lt;p&gt;But qualitative work proved far more challenging. When asked to create website layouts, write marketing copy, or translate content with appropriate cultural nuance, agents floundered without expert guidance. &amp;quot;When you ask it to write you a poem, the quality of the poem is extremely subjective,&amp;quot; Rabinovich said. &amp;quot;Since the rubrics for evaluation were provided by humans, there&amp;#x27;s some level of variability in representation.&amp;quot;&lt;/p&gt;&lt;p&gt;Writing, translation, and sales and marketing projects showed the most dramatic improvements from human feedback. For writing work, completion rates increased by up to 17 percentage points after expert review. Engineering and architecture projects requiring creative problem-solving — like civil engineering or architectural design — improved by as much as 23 percentage points with human oversight.&lt;/p&gt;&lt;p&gt;This pattern suggests AI agents excel at pattern matching and replication but struggle with creativity, judgment, and context — precisely the skills that define higher-value professional work.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the research: How Upwork tested AI agents with peer-reviewed scientific methods&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt; partnered with elite freelancers on its platform to evaluate every deliverable produced by AI agents, both independently and after each cycle of human feedback. These evaluators created detailed rubrics defining whether projects met core requirements specified in job descriptions, then scored outputs across multiple iterations.&lt;/p&gt;&lt;p&gt;Importantly, evaluators focused only on objective completion criteria, excluding subjective factors like stylistic preferences or quality judgments that might emerge in actual client relationships. &amp;quot;Rubric-based completion rates should not be viewed as a measure of whether an agent would be paid in a real marketplace setting,&amp;quot; &lt;a href="https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf"&gt;&lt;u&gt;the research&lt;/u&gt;&lt;/a&gt; notes, &amp;quot;but as an indicator of its ability to fulfill explicitly defined requests.&amp;quot;&lt;/p&gt;&lt;p&gt;This distinction matters: An AI agent might technically complete all specified requirements yet still produce work a client rejects as inadequate. Conversely, subjective client satisfaction — the true measure of marketplace success — remains beyond current measurement capabilities.&lt;/p&gt;&lt;p&gt;The research underwent double-blind peer review and was accepted to &lt;a href="https://neurips.cc/"&gt;&lt;u&gt;NeurIPS&lt;/u&gt;&lt;/a&gt;, the premier academic conference for AI research, where Upwork will present full results in early December. The company plans to publish a complete methodology and make the benchmark available to the research community, updating the task pool regularly to prevent overfitting as agents improve.&lt;/p&gt;&lt;p&gt;&amp;quot;The idea is for this benchmark to be a living and breathing platform where agents can come in and evaluate themselves on all categories of work, and the tasks that will be offered on the platform will always update, so that these agents don&amp;#x27;t overfit and basically memorize the tasks at hand,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Upwork&amp;#x27;s AI strategy: Building Uma, a &amp;#x27;meta-agent&amp;#x27; that manages human and AI workers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research directly informs Upwork&amp;#x27;s product roadmap as the company positions itself for what executives call &amp;quot;the age of AI and beyond.&amp;quot; Rather than building its own AI agents to complete specific tasks, &lt;a href="https://investors.upwork.com/news-releases/news-release-details/upwork-evolves-uma-ai-ai-work-agent-advances-human-ai"&gt;&lt;u&gt;Upwork is developing Uma&lt;/u&gt;&lt;/a&gt;, a &amp;quot;meta orchestration agent&amp;quot; that coordinates between human workers, AI systems, and clients.&lt;/p&gt;&lt;p&gt;&amp;quot;Today, Upwork is a marketplace where clients look for freelancers to get work done, and then talent comes to Upwork to find work,&amp;quot; Rabinovich explained. &amp;quot;This is getting expanded into a domain where clients come to Upwork, communicate with Uma, this meta-orchestration agent, and then Uma identifies the necessary talent to get the job done, gets the tasks outcomes completed, and then delivers that to the client.&amp;quot;&lt;/p&gt;&lt;p&gt;In this vision, clients would interact primarily with Uma rather than directly hiring freelancers. The AI system would analyze project requirements, determine which tasks require human expertise versus AI execution, coordinate the workflow, and ensure quality — acting as an intelligent project manager rather than a replacement worker.&lt;/p&gt;&lt;p&gt;&amp;quot;We don&amp;#x27;t want to build agents that actually complete the tasks, but we are building this meta orchestration agent that figures out what human and agent talent is necessary in order to complete the tasks,&amp;quot; Rabinovich said. &amp;quot;Uma evaluates the work to be delivered to the client, orchestrates the interaction between humans and agents, and is able to learn from all the interactions that happen on the platform how to break jobs into tasks so that they get completed in a timely and effective manner.&amp;quot;&lt;/p&gt;&lt;p&gt;The company recently &lt;a href="https://investors.upwork.com/news-releases/news-release-details/upwork-announces-forthcoming-lisbon-office-scale-ai-innovation"&gt;&lt;u&gt;announced plans to open its first international office&lt;/u&gt;&lt;/a&gt; in Lisbon, Portugal, by the fourth quarter of 2026, with a focus on AI infrastructure development and technical hiring. The expansion follows Upwork&amp;#x27;s record-breaking third quarter, driven partly by AI-powered product innovation and strong demand for workers with AI skills.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;OpenAI, Anthropic, and Google race to build autonomous agents—but reality lags hype&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Upwork&amp;#x27;s findings arrive amid escalating competition in the AI agent space. OpenAI, Anthropic, Google, and numerous startups are racing to develop autonomous agents capable of complex multi-step tasks, from booking travel to analyzing financial data to writing software.&lt;/p&gt;&lt;p&gt;But recent high-profile stumbles have tempered initial enthusiasm. AI agents frequently misunderstand instructions, make logical errors, or produce confidently wrong results — a phenomenon researchers call &amp;quot;hallucination.&amp;quot; The gap between controlled demonstration videos and reliable real-world performance remains vast.&lt;/p&gt;&lt;p&gt;&amp;quot;There have been some evaluations that came from OpenAI and other platforms where real Upwork tasks were considered for completion by agents, and across the board, the reported results were not very optimistic, in the sense that they showed that agents—even the best ones, meaning powered by most advanced LLMs — can&amp;#x27;t really compete with humans that well, because the completion rates are pretty low,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;Rather than waiting for AI to fully mature — a timeline that remains uncertain—Upwork is betting on a hybrid approach that leverages AI&amp;#x27;s strengths (speed, scalability, pattern recognition) while retaining human strengths (judgment, creativity, contextual understanding).&lt;/p&gt;&lt;p&gt;This philosophy extends to learning and improvement. Current AI models train primarily on static datasets scraped from the internet, supplemented by human preference feedback. But most professional work is qualitative, making it difficult for AI systems to know whether their outputs are actually good without expert evaluation.&lt;/p&gt;&lt;p&gt;&amp;quot;Unless you have this collaboration between the human and the machine, where the human is kind of the teacher and the machine is the student trying to discover new solutions, none of this will be possible,&amp;quot; Rabinovich said. &amp;quot;Upwork is very uniquely positioned to create such an environment because if you try to do this with, say, self-driving cars, and you tell Waymo cars to explore new ways of getting to the airport, like avoiding traffic signs, then a bunch of bad things will happen. In doing work on Upwork, if it creates a wrong website, it doesn&amp;#x27;t cost very much, and there&amp;#x27;s no negative side effects. But the opportunity to learn is absolutely tremendous.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Will AI take your job? The evidence suggests a more complicated answer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While much public discourse around AI focuses on job displacement, Rabinovich argues the historical pattern suggests otherwise — though the transition may prove disruptive.&lt;/p&gt;&lt;p&gt;&amp;quot;The narrative in the public is that AI is eliminating jobs, whether it&amp;#x27;s writing, translation, coding or other digital work, but no one really talks about the exponential amount of new types of work that it will create,&amp;quot; he said. &amp;quot;When we invented electricity and steam engines and things like that, they certainly replaced certain jobs, but the amount of new jobs that were introduced is exponentially more, and we think the same is going to happen here.&amp;quot;&lt;/p&gt;&lt;p&gt;The research identifies emerging job categories focused on AI oversight: designing effective human-machine workflows, providing high-quality feedback to improve agent performance, and verifying that AI-generated work meets quality standards. These skills—prompt engineering, agent supervision, output verification—barely existed two years ago but now command premium rates on platforms like Upwork.&lt;/p&gt;&lt;p&gt;&amp;quot;New types of skills from humans are becoming necessary in the form of how to design the interaction between humans and machines, how to guide agents to make them better, and ultimately, how to verify that whatever agentic proposals are being made are actually correct, because that&amp;#x27;s what&amp;#x27;s necessary in order to advance the state of AI,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;The question remains whether this transition—  from doing tasks to overseeing them — will create opportunities as quickly as it disrupts existing roles. For freelancers on Upwork, the answer may already be emerging in their bank accounts: The platform saw AI-related work grow 53% year-over-year, even as fears of AI-driven unemployment dominated headlines.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Artificial intelligence agents powered by the world&amp;#x27;s most advanced language models routinely fail to complete even straightforward professional tasks on their own, according to &lt;a href="https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf"&gt;&lt;u&gt;groundbreaking research&lt;/u&gt;&lt;/a&gt; released Thursday by &lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt;, the largest online work marketplace.&lt;/p&gt;&lt;p&gt;But the same study reveals a more promising path forward: When AI agents collaborate with human experts, &lt;a href="https://www.globenewswire.com/news-release/2025/11/13/3187462/0/en/Upwork-Human-Agent-Productivity-Index-Reveals-Up-to-70-Boost-in-Work-Completion-from-Human-and-AI-Agent-Collaboration-vs-Agents-Working-Alone.html"&gt;&lt;u&gt;project completion rates surge by up to 70%&lt;/u&gt;&lt;/a&gt;, suggesting the future of work may not pit humans against machines but rather pair them together in powerful new ways.&lt;/p&gt;&lt;p&gt;The findings, drawn from more than 300 real client projects posted to Upwork&amp;#x27;s platform, marking the first systematic evaluation of how human expertise amplifies AI agent performance in actual professional work — not synthetic tests or academic simulations. The research challenges both the hype around fully autonomous AI agents and fears that such technology will imminently replace knowledge workers.&lt;/p&gt;&lt;p&gt;&amp;quot;AI agents aren&amp;#x27;t that agentic, meaning they aren&amp;#x27;t that good,&amp;quot; Andrew Rabinovich, Upwork&amp;#x27;s chief technology officer and head of AI and machine learning, said in an exclusive interview with VentureBeat. &amp;quot;However, when paired with expert human professionals, project completion rates improve dramatically, supporting our firm belief that the future of work will be defined by humans and AI collaborating to get more work done, with human intuition and domain expertise playing a critical role.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How AI agents performed on 300+ real freelance jobs—and why they struggled&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.upwork.com/human-agent-productivity-index"&gt;&lt;u&gt;Upwork&amp;#x27;s Human+Agent Productivity Index (HAPI) &lt;/u&gt;&lt;/a&gt;evaluated how three leading AI systems — &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt;, OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt;, and Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; — performed on actual jobs posted by paying clients across categories including writing, data science, web development, engineering, sales, and translation.&lt;/p&gt;&lt;p&gt;Critically, Upwork deliberately selected simple, well-defined projects where AI agents stood a reasonable chance of success. These jobs, priced under $500, represent less than 6% of Upwork&amp;#x27;s total gross services volume — a tiny fraction of the platform&amp;#x27;s overall business and an acknowledgment of current AI limitations.&lt;/p&gt;&lt;p&gt;&amp;quot;The reality is that although we study AI, and I&amp;#x27;ve been doing this for 25 years, and we see significant breakthroughs, the reality is that these agents aren&amp;#x27;t that agentic,&amp;quot; Rabinovich told VentureBeat. &amp;quot;So if we go up the value chain, the problems become so much more difficult, then we don&amp;#x27;t think they can solve them at all, even to scratch the surface. So we specifically chose simpler tasks that would give an agent some kind of traction.&amp;quot;&lt;/p&gt;&lt;p&gt;Even on these deliberately simplified tasks, AI agents working independently struggled. But when expert freelancers provided feedback — spending an average of just 20 minutes per review cycle — the agents&amp;#x27; performance improved substantially with each iteration.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;20 minutes of human feedback boosted AI completion rates up to 70%&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research reveals stark differences in how AI agents perform with and without human guidance across different types of work. For data science and analytics projects, Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; achieved a 64% completion rate working alone but jumped to 93% after receiving feedback from a human expert. In sales and marketing work, &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt;&amp;#x27;s completion rate rose from 17% independently to 31% with human input. OpenAI&amp;#x27;s &lt;a href="https://openai.com/gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; showed similarly dramatic improvements in engineering and architecture tasks, climbing from 30% to 50% completion.&lt;/p&gt;&lt;p&gt;The pattern held across virtually all categories, with agents responding particularly well to human feedback on qualitative, creative work requiring editorial judgment — areas like writing, translation, and marketing — where completion rates increased by up to 17 percentage points per feedback cycle.&lt;/p&gt;&lt;p&gt;The finding challenges a fundamental assumption in the AI industry: that agent benchmarks conducted in isolation accurately predict real-world performance.&lt;/p&gt;&lt;p&gt;&amp;quot;While we show that in the tasks that we have selected for agents to perform in isolation, they perform similarly to the previous results that we&amp;#x27;ve seen published openly, what we&amp;#x27;ve shown is that in collaboration with humans, the performance of these agents improves surprisingly well,&amp;quot; Rabinovich said. &amp;quot;It&amp;#x27;s not just a one-turn back and forth, but the more feedback the human provides, the better the agent gets at performing.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why ChatGPT can ace the SAT but can&amp;#x27;t count the R&amp;#x27;s in &amp;#x27;strawberry&amp;#x27;&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research arrives as the AI industry grapples with a measurement crisis. Traditional benchmarks — standardized tests that AI models can master, sometimes scoring perfectly on SAT exams or mathematics olympiads — have proven poor predictors of real-world capability.&lt;/p&gt;&lt;p&gt;&amp;quot;With advances of large language models, what we&amp;#x27;re now seeing is that these static, academic datasets are completely saturated,&amp;quot; Rabinovich said. &amp;quot;So you could get a perfect score in the SAT test or LSAT or any of the math olympiads, and then you would ask ChatGPT how many R&amp;#x27;s there are in the word strawberry, and it would get it wrong.&amp;quot;&lt;/p&gt;&lt;p&gt;This phenomenon — where AI systems ace formal tests but stumble on trivial real-world questions — has led to growing skepticism about AI capabilities, even as companies race to deploy autonomous agents. Several recent benchmarks from other firms have tested AI agents on Upwork jobs, but those evaluations measured only isolated performance, not the collaborative potential that Upwork&amp;#x27;s research reveals.&lt;/p&gt;&lt;p&gt;&amp;quot;We wanted to evaluate the quality of these agents on actual real work with economic value associated with it, and not only see how well these agents do, but also see how these agents do in collaboration with humans, because we sort of knew already that in isolation, they&amp;#x27;re not that advanced,&amp;quot; Rabinovich explained.&lt;/p&gt;&lt;p&gt;For &lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt;, which connects roughly 800,000 active clients posting more than 3 million jobs annually to a global pool of freelancers, the research serves a strategic business purpose: establishing quality standards for AI agents before allowing them to compete or collaborate with human workers on its platform.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The economics of human-AI teamwork: Why paying for expert feedback still saves money&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite requiring multiple rounds of human feedback — each lasting about 20 minutes — the time investment remains &amp;quot;orders of magnitude different between a human doing the work alone, versus a human doing the work with an AI agent,&amp;quot; Rabinovich said. Where a project might take a freelancer days to complete independently, the agent-plus-human approach can deliver results in hours through iterative cycles of automated work and expert refinement.&lt;/p&gt;&lt;p&gt;The economic implications extend beyond simple time savings. Upwork recently reported that gross services volume from &lt;a href="https://finance.yahoo.com/news/upworks-stock-soars-q3-blowout-201700500.html"&gt;&lt;u&gt;AI-related work grew 53% year-over-year&lt;/u&gt;&lt;/a&gt; in the third quarter of 2025, one of the strongest growth drivers for the company. But executives have been careful to frame AI not as a replacement for freelancers but as an enhancement to their capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;AI was a huge overhang for our valuation,&amp;quot; Erica Gessert, Upwork&amp;#x27;s CFO, told &lt;a href="https://www.cfobrew.com/stories/2025/10/16/the-cfo-of-upwork-answers-every-cfo-s-most-burning-question"&gt;&lt;u&gt;CFO Brew&lt;/u&gt;&lt;/a&gt; in October. &amp;quot;There was this belief that all work was going to go away. AI was going to take it, and especially work that&amp;#x27;s done by people like freelancers, because they are impermanent. Actually, the opposite is true.&amp;quot;&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s strategy centers on enabling freelancers to handle more complex, higher-value work by offloading routine tasks to AI. &amp;quot;Freelancers actually prefer to have tools that automate the manual labor and repetitive part of their work, and really focus on the creative and conceptual part of the process,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;Rather than replacing jobs, he argues, AI will transform them: &amp;quot;Simpler tasks will be automated by agents, but the jobs will become much more complex in the number of tasks, so the amount of work and therefore earnings for freelancers will actually only go up.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI coding agents excel, but creative writing and translation still need humans&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research reveals a clear pattern in agent capabilities. AI systems perform best on &amp;quot;deterministic and verifiable&amp;quot; tasks with objectively correct answers, like solving math problems or writing basic code. &amp;quot;Most coding tasks are very similar to each other,&amp;quot; Rabinovich noted. &amp;quot;That&amp;#x27;s why coding agents are becoming so good.&amp;quot;&lt;/p&gt;&lt;p&gt;In Upwork&amp;#x27;s tests, web development, mobile app development, and data science projects — especially those involving structured, computational work — saw the highest standalone agent completion rates. Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; completed 68% of web development jobs and 64% of data science projects without human help, while &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; achieved 74% on certain technical tasks.&lt;/p&gt;&lt;p&gt;But qualitative work proved far more challenging. When asked to create website layouts, write marketing copy, or translate content with appropriate cultural nuance, agents floundered without expert guidance. &amp;quot;When you ask it to write you a poem, the quality of the poem is extremely subjective,&amp;quot; Rabinovich said. &amp;quot;Since the rubrics for evaluation were provided by humans, there&amp;#x27;s some level of variability in representation.&amp;quot;&lt;/p&gt;&lt;p&gt;Writing, translation, and sales and marketing projects showed the most dramatic improvements from human feedback. For writing work, completion rates increased by up to 17 percentage points after expert review. Engineering and architecture projects requiring creative problem-solving — like civil engineering or architectural design — improved by as much as 23 percentage points with human oversight.&lt;/p&gt;&lt;p&gt;This pattern suggests AI agents excel at pattern matching and replication but struggle with creativity, judgment, and context — precisely the skills that define higher-value professional work.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the research: How Upwork tested AI agents with peer-reviewed scientific methods&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt; partnered with elite freelancers on its platform to evaluate every deliverable produced by AI agents, both independently and after each cycle of human feedback. These evaluators created detailed rubrics defining whether projects met core requirements specified in job descriptions, then scored outputs across multiple iterations.&lt;/p&gt;&lt;p&gt;Importantly, evaluators focused only on objective completion criteria, excluding subjective factors like stylistic preferences or quality judgments that might emerge in actual client relationships. &amp;quot;Rubric-based completion rates should not be viewed as a measure of whether an agent would be paid in a real marketplace setting,&amp;quot; &lt;a href="https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf"&gt;&lt;u&gt;the research&lt;/u&gt;&lt;/a&gt; notes, &amp;quot;but as an indicator of its ability to fulfill explicitly defined requests.&amp;quot;&lt;/p&gt;&lt;p&gt;This distinction matters: An AI agent might technically complete all specified requirements yet still produce work a client rejects as inadequate. Conversely, subjective client satisfaction — the true measure of marketplace success — remains beyond current measurement capabilities.&lt;/p&gt;&lt;p&gt;The research underwent double-blind peer review and was accepted to &lt;a href="https://neurips.cc/"&gt;&lt;u&gt;NeurIPS&lt;/u&gt;&lt;/a&gt;, the premier academic conference for AI research, where Upwork will present full results in early December. The company plans to publish a complete methodology and make the benchmark available to the research community, updating the task pool regularly to prevent overfitting as agents improve.&lt;/p&gt;&lt;p&gt;&amp;quot;The idea is for this benchmark to be a living and breathing platform where agents can come in and evaluate themselves on all categories of work, and the tasks that will be offered on the platform will always update, so that these agents don&amp;#x27;t overfit and basically memorize the tasks at hand,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Upwork&amp;#x27;s AI strategy: Building Uma, a &amp;#x27;meta-agent&amp;#x27; that manages human and AI workers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research directly informs Upwork&amp;#x27;s product roadmap as the company positions itself for what executives call &amp;quot;the age of AI and beyond.&amp;quot; Rather than building its own AI agents to complete specific tasks, &lt;a href="https://investors.upwork.com/news-releases/news-release-details/upwork-evolves-uma-ai-ai-work-agent-advances-human-ai"&gt;&lt;u&gt;Upwork is developing Uma&lt;/u&gt;&lt;/a&gt;, a &amp;quot;meta orchestration agent&amp;quot; that coordinates between human workers, AI systems, and clients.&lt;/p&gt;&lt;p&gt;&amp;quot;Today, Upwork is a marketplace where clients look for freelancers to get work done, and then talent comes to Upwork to find work,&amp;quot; Rabinovich explained. &amp;quot;This is getting expanded into a domain where clients come to Upwork, communicate with Uma, this meta-orchestration agent, and then Uma identifies the necessary talent to get the job done, gets the tasks outcomes completed, and then delivers that to the client.&amp;quot;&lt;/p&gt;&lt;p&gt;In this vision, clients would interact primarily with Uma rather than directly hiring freelancers. The AI system would analyze project requirements, determine which tasks require human expertise versus AI execution, coordinate the workflow, and ensure quality — acting as an intelligent project manager rather than a replacement worker.&lt;/p&gt;&lt;p&gt;&amp;quot;We don&amp;#x27;t want to build agents that actually complete the tasks, but we are building this meta orchestration agent that figures out what human and agent talent is necessary in order to complete the tasks,&amp;quot; Rabinovich said. &amp;quot;Uma evaluates the work to be delivered to the client, orchestrates the interaction between humans and agents, and is able to learn from all the interactions that happen on the platform how to break jobs into tasks so that they get completed in a timely and effective manner.&amp;quot;&lt;/p&gt;&lt;p&gt;The company recently &lt;a href="https://investors.upwork.com/news-releases/news-release-details/upwork-announces-forthcoming-lisbon-office-scale-ai-innovation"&gt;&lt;u&gt;announced plans to open its first international office&lt;/u&gt;&lt;/a&gt; in Lisbon, Portugal, by the fourth quarter of 2026, with a focus on AI infrastructure development and technical hiring. The expansion follows Upwork&amp;#x27;s record-breaking third quarter, driven partly by AI-powered product innovation and strong demand for workers with AI skills.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;OpenAI, Anthropic, and Google race to build autonomous agents—but reality lags hype&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Upwork&amp;#x27;s findings arrive amid escalating competition in the AI agent space. OpenAI, Anthropic, Google, and numerous startups are racing to develop autonomous agents capable of complex multi-step tasks, from booking travel to analyzing financial data to writing software.&lt;/p&gt;&lt;p&gt;But recent high-profile stumbles have tempered initial enthusiasm. AI agents frequently misunderstand instructions, make logical errors, or produce confidently wrong results — a phenomenon researchers call &amp;quot;hallucination.&amp;quot; The gap between controlled demonstration videos and reliable real-world performance remains vast.&lt;/p&gt;&lt;p&gt;&amp;quot;There have been some evaluations that came from OpenAI and other platforms where real Upwork tasks were considered for completion by agents, and across the board, the reported results were not very optimistic, in the sense that they showed that agents—even the best ones, meaning powered by most advanced LLMs — can&amp;#x27;t really compete with humans that well, because the completion rates are pretty low,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;Rather than waiting for AI to fully mature — a timeline that remains uncertain—Upwork is betting on a hybrid approach that leverages AI&amp;#x27;s strengths (speed, scalability, pattern recognition) while retaining human strengths (judgment, creativity, contextual understanding).&lt;/p&gt;&lt;p&gt;This philosophy extends to learning and improvement. Current AI models train primarily on static datasets scraped from the internet, supplemented by human preference feedback. But most professional work is qualitative, making it difficult for AI systems to know whether their outputs are actually good without expert evaluation.&lt;/p&gt;&lt;p&gt;&amp;quot;Unless you have this collaboration between the human and the machine, where the human is kind of the teacher and the machine is the student trying to discover new solutions, none of this will be possible,&amp;quot; Rabinovich said. &amp;quot;Upwork is very uniquely positioned to create such an environment because if you try to do this with, say, self-driving cars, and you tell Waymo cars to explore new ways of getting to the airport, like avoiding traffic signs, then a bunch of bad things will happen. In doing work on Upwork, if it creates a wrong website, it doesn&amp;#x27;t cost very much, and there&amp;#x27;s no negative side effects. But the opportunity to learn is absolutely tremendous.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Will AI take your job? The evidence suggests a more complicated answer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While much public discourse around AI focuses on job displacement, Rabinovich argues the historical pattern suggests otherwise — though the transition may prove disruptive.&lt;/p&gt;&lt;p&gt;&amp;quot;The narrative in the public is that AI is eliminating jobs, whether it&amp;#x27;s writing, translation, coding or other digital work, but no one really talks about the exponential amount of new types of work that it will create,&amp;quot; he said. &amp;quot;When we invented electricity and steam engines and things like that, they certainly replaced certain jobs, but the amount of new jobs that were introduced is exponentially more, and we think the same is going to happen here.&amp;quot;&lt;/p&gt;&lt;p&gt;The research identifies emerging job categories focused on AI oversight: designing effective human-machine workflows, providing high-quality feedback to improve agent performance, and verifying that AI-generated work meets quality standards. These skills—prompt engineering, agent supervision, output verification—barely existed two years ago but now command premium rates on platforms like Upwork.&lt;/p&gt;&lt;p&gt;&amp;quot;New types of skills from humans are becoming necessary in the form of how to design the interaction between humans and machines, how to guide agents to make them better, and ultimately, how to verify that whatever agentic proposals are being made are actually correct, because that&amp;#x27;s what&amp;#x27;s necessary in order to advance the state of AI,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;The question remains whether this transition—  from doing tasks to overseeing them — will create opportunities as quickly as it disrupts existing roles. For freelancers on Upwork, the answer may already be emerging in their bank accounts: The platform saw AI-related work grow 53% year-over-year, even as fears of AI-driven unemployment dominated headlines.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/upwork-study-shows-ai-agents-excel-with-human-partners-but-fail</guid><pubDate>Thu, 13 Nov 2025 18:30:00 +0000</pubDate></item></channel></rss>