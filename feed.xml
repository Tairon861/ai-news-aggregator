<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 25 Jun 2025 01:50:52 +0000</lastBuildDate><item><title>Google rolls out new Gemini model that can run on robots locally (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/24/google-rolls-out-new-gemini-model-that-can-run-on-robots-locally/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind on Tuesday released a new language model called Gemini Robotics On-Device that can run tasks locally on robots without requiring an internet connection. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Building on the company’s previous Gemini Robotics model that was released in March, Gemini Robotics On-Device can control a robot’s movements. Developers can control and fine-tune the model to suit various needs using natural language prompts.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In benchmarks, Google claims the model performs at a level close to the cloud-based Gemini Robotics model. The company says it outperforms other on-device models in general benchmarks, though it didn’t name those models.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021560" height="428" src="https://techcrunch.com/wp-content/uploads/2025/06/image.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In a demo, the company showed robots running this local model doing things like unzipping bags and folding clothes. Google says that while the model was trained for ALOHA robots, it later adapted it to work on a bi-arm Franka FR3 robot and the Apollo humanoid robot by Apptronik.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google claims the bi-arm Franka FR3 was successful in tackling scenarios and objects it hadn’t “seen” before, like doing assembly on an industrial belt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google DeepMind is also releasing a Gemini Robotics SDK. The company said developers can show robots 50 to 100 demonstrations of tasks to train them on new tasks using these models on the MuJoCo physics simulator.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other AI model developers are also dipping their toes in robotics. Nvidia is building a platform to create foundation models for humanoids; Hugging Face is not only developing open models and datasets for robotics, but it is also working on robots; and Mirae Asset-backed Korean startup RLWRLD is working on creating foundational models for robots.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind on Tuesday released a new language model called Gemini Robotics On-Device that can run tasks locally on robots without requiring an internet connection. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Building on the company’s previous Gemini Robotics model that was released in March, Gemini Robotics On-Device can control a robot’s movements. Developers can control and fine-tune the model to suit various needs using natural language prompts.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In benchmarks, Google claims the model performs at a level close to the cloud-based Gemini Robotics model. The company says it outperforms other on-device models in general benchmarks, though it didn’t name those models.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021560" height="428" src="https://techcrunch.com/wp-content/uploads/2025/06/image.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In a demo, the company showed robots running this local model doing things like unzipping bags and folding clothes. Google says that while the model was trained for ALOHA robots, it later adapted it to work on a bi-arm Franka FR3 robot and the Apollo humanoid robot by Apptronik.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google claims the bi-arm Franka FR3 was successful in tackling scenarios and objects it hadn’t “seen” before, like doing assembly on an industrial belt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google DeepMind is also releasing a Gemini Robotics SDK. The company said developers can show robots 50 to 100 demonstrations of tasks to train them on new tasks using these models on the MuJoCo physics simulator.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other AI model developers are also dipping their toes in robotics. Nvidia is building a platform to create foundation models for humanoids; Hugging Face is not only developing open models and datasets for robotics, but it is also working on robots; and Mirae Asset-backed Korean startup RLWRLD is working on creating foundational models for robots.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/24/google-rolls-out-new-gemini-model-that-can-run-on-robots-locally/</guid><pubDate>Tue, 24 Jun 2025 14:00:00 +0000</pubDate></item><item><title>Gemini Robotics On-Device brings AI to local robotic devices (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We gratefully acknowledge contributions, advice, and support from Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Maria Attarian, Ashwin Balakrishna, Yanan Bao, Clara Barbu, Catarina Barros, Robert Baruch, Nathan Batchelor, Maria Bauza, Lucas Beyer, Michael Bloesch, Michiel Blokzijl, Steven Bohez, Konstantinos Bousmalis, Demetra Brady, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Kendra Byrne, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, Jose Enrique Chen, Xi Chen, Huizhong Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, Kieran Connell, David D'Ambrosio, Sudeep Dasari, Todor Davchev, Coline Devin, Norman Di Palo, Tianli Ding, Adil Dostmohamed, Anca Dragan, Yilun Du, Debidatta Dwibedi, Michael Elabd, Tom Erez, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Frankie Garcia, Ashley Gibb, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Simon Green, Oliver Groth, Roland Hafner, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Tim Hertweck, Alexander Herzog, R. Alex Hofer, Sandy H Huang, Jan Humplik , Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Chase Kew, Jerad Kirkland, Sean Kirmani, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Jennie Lees, Jacky Liang, Yixin Lin, Li-Heng Lin, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Siobhan Mcloughlin, Assaf Hurwitz Michaely, Joss Moore, Robert Moreno, Thomas Mulc, Michael Neunert, Francesco Nori, Dave Orr, Carolina Parada, Emilio Parisotto, Peter Pastor, André Susano Pinto, Acorn Pooley, Grace Popple, Thomas Power, Alessio Quaglino, Haroon Qureshi, Kanishka Rao, Dushyant Rao, Krista Reymann, Martin Riedmiller, Francesco Romano, Keran Rong, Dorsa Sadigh, Stefano Saliceti, Daniel Salz, Pannag Sanketi, Mili Sanwalka, Kevin Sayed, Pierre Sermanet, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Andreas Steiner, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Ben Swanson, Mitri Syriani, Jie Tan, Yuval Tassa, Alan Thompson, Dhruva Tirumala, Jonathan Tompson, Karen Truong, Jake Varley, Siddharth Verma, Grace Vesom, Giulia Vezzani, Oriol Vinyals, Ayzaan Wahid, Zhicheng Wang, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Yuxiang Yang, Rui Yao, Sergey Yaroshenko, Matt Young, Wenhao Yu, Wentao Yuan, Martina Zambelli, Xiaohua Zhai, Jingwei Zhang, Tingnan Zhang, Allan Zhou, Yuxiang Zhou, Guangyao (Stannis) Zhou, Howard Zhou.&lt;/p&gt;&lt;p&gt;We also thank the operations and support staff that performed data collection and robot evaluations for this project.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Jt_Vw7PIJEZtXcMIKM1HbWbBCLxv7RUyjyf07eHp-YOfxMCUZA6mPI9kSCaz65UkMoGcZ8CwlD3dNBvy7bnnYchjSkWyN-SugglT3dmg1A9KdoDqdQM=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We gratefully acknowledge contributions, advice, and support from Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Maria Attarian, Ashwin Balakrishna, Yanan Bao, Clara Barbu, Catarina Barros, Robert Baruch, Nathan Batchelor, Maria Bauza, Lucas Beyer, Michael Bloesch, Michiel Blokzijl, Steven Bohez, Konstantinos Bousmalis, Demetra Brady, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Kendra Byrne, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, Jose Enrique Chen, Xi Chen, Huizhong Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, Kieran Connell, David D'Ambrosio, Sudeep Dasari, Todor Davchev, Coline Devin, Norman Di Palo, Tianli Ding, Adil Dostmohamed, Anca Dragan, Yilun Du, Debidatta Dwibedi, Michael Elabd, Tom Erez, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Frankie Garcia, Ashley Gibb, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Simon Green, Oliver Groth, Roland Hafner, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Tim Hertweck, Alexander Herzog, R. Alex Hofer, Sandy H Huang, Jan Humplik , Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Chase Kew, Jerad Kirkland, Sean Kirmani, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Jennie Lees, Jacky Liang, Yixin Lin, Li-Heng Lin, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Siobhan Mcloughlin, Assaf Hurwitz Michaely, Joss Moore, Robert Moreno, Thomas Mulc, Michael Neunert, Francesco Nori, Dave Orr, Carolina Parada, Emilio Parisotto, Peter Pastor, André Susano Pinto, Acorn Pooley, Grace Popple, Thomas Power, Alessio Quaglino, Haroon Qureshi, Kanishka Rao, Dushyant Rao, Krista Reymann, Martin Riedmiller, Francesco Romano, Keran Rong, Dorsa Sadigh, Stefano Saliceti, Daniel Salz, Pannag Sanketi, Mili Sanwalka, Kevin Sayed, Pierre Sermanet, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Andreas Steiner, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Ben Swanson, Mitri Syriani, Jie Tan, Yuval Tassa, Alan Thompson, Dhruva Tirumala, Jonathan Tompson, Karen Truong, Jake Varley, Siddharth Verma, Grace Vesom, Giulia Vezzani, Oriol Vinyals, Ayzaan Wahid, Zhicheng Wang, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Yuxiang Yang, Rui Yao, Sergey Yaroshenko, Matt Young, Wenhao Yu, Wentao Yuan, Martina Zambelli, Xiaohua Zhai, Jingwei Zhang, Tingnan Zhang, Allan Zhou, Yuxiang Zhou, Guangyao (Stannis) Zhou, Howard Zhou.&lt;/p&gt;&lt;p&gt;We also thank the operations and support staff that performed data collection and robot evaluations for this project.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/</guid><pubDate>Tue, 24 Jun 2025 14:00:36 +0000</pubDate></item><item><title>Google’s new robotics AI can run without the cloud and still tie your shoes (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/06/google-releases-first-cloud-free-ai-robotics-model/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google's Carolina Parada says Gemini has enabled huge robotics breakthroughs, like the new on-device AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Apollo robot" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Robotics-SocialShare_1920x1080.width-1300-640x360.png" width="640" /&gt;
                  &lt;img alt="Apollo robot" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Robotics-SocialShare_1920x1080.width-1300-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The humanoid Apollo robot is one of the platforms supported in Gemini Robotics. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;We sometimes call chatbots like Gemini and ChatGPT "robots," but generative AI is also playing a growing role in real, physical robots. After announcing Gemini Robotics earlier this year, Google DeepMind has now revealed a new on-device VLA (vision language action) model to control robots. Unlike the previous release, there's no cloud component, allowing robots to operate with full autonomy.&lt;/p&gt;
&lt;p&gt;Carolina Parada, head of robotics at Google DeepMind, says this approach to AI robotics could make robots more reliable in challenging situations. This is also the first version of Google's robotics model that developers can tune for their specific uses.&lt;/p&gt;
&lt;p&gt;Robotics is a unique problem for AI because, not only does the robot exist in the physical world, but it also changes its environment. Whether you're having it move blocks around or tie your shoes, it's hard to predict every eventuality a robot might encounter. The traditional approach of training a robot on action with reinforcement was very slow, but generative AI allows for much greater generalization.&lt;/p&gt;
&lt;p&gt;"It's drawing from Gemini's multimodal world understanding in order to do a completely new task," explains Carolina Parada. "What that enables is in that same way Gemini can produce text, write poetry, just summarize an article, you can also write code, and you can also generate images. It also can generate robot actions."&lt;/p&gt;
&lt;h2&gt;General robots, no cloud needed&lt;/h2&gt;
&lt;p&gt;In the previous Gemini Robotics release (which is still the "best" version of Google's robotics tech), the platforms ran a hybrid system with a small model on the robot and a larger one running in the cloud. You've probably watched chatbots "think" for measurable seconds as they generate an output, but robots need to react quickly. If you tell the robot to pick up and move an object, you don't want it to pause while each step is generated. The local model allows quick adaptation, while the server-based model can help with complex reasoning tasks. Google DeepMind is now unleashing the local model as a standalone VLA, and it's surprisingly robust.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2102351-1" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Dexterity-General-Aloha.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The new Gemini Robotics On-Device model is only a little less accurate than the hybrid version. According to Parada, many tasks will work out of the box. "When we play with the robots, we see that they're surprisingly capable of understanding a new situation," Parada tells Ars.&lt;/p&gt;
&lt;p&gt;By releasing this model with a full SDK, the team hopes developers will give Gemini-powered robots new tasks and show them new environments, which could reveal actions that don't work with the model's stock tuning. With the SDK, robotics researchers will be able to adapt the VLA to new tasks with as little as 50 to 100 demonstrations.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2102439 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Robotics On-Device chart" class="fullwidth full" height="476" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/On-Device-Robotics-AI.png" width="756" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Robotics On-Device model is almost as adaptable as the hybrid model with cloud processing.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A "demonstration" in AI robotics is a bit different from other areas of AI research. Parada explains that demonstrations typically involve tele-operating the robot—controlling the machinery manually to complete a task tunes the model to handle that task autonomously. While synthetic data is an element of Google's training, it's not a substitute for the real thing. "We still find that in the most complex, dexterous behaviors, we need real data," says Parada. "But there is quite a lot that you can do with simulation."&lt;/p&gt;
&lt;p&gt;But those highly complex behaviors may be beyond the capabilities of the on-device VLA. It should have no problem with straightforward actions like tying a shoe (a traditionally difficult task for AI robots) or folding a shirt. If, however, you wanted a robot to make you a sandwich, it would probably need a more powerful model to go through the multi-step reasoning required to get the bread in the right place.&lt;/p&gt;
&lt;p&gt;The team sees Gemini Robotics On-Device as ideal for environments where connectivity to the cloud is spotty or non-existent. Processing the robot's visual data locally is also better for privacy, for example, in a health care environment.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Building safe robots&lt;/h2&gt;
&lt;p&gt;Safety is always a concern with AI systems, whether it's a chatbot that provides dangerous information or a robot that goes Terminator. We've all seen generative AI chatbots and image generators hallucinate falsehoods in their outputs, and the generative systems powering Gemini Robotics are no different—the model doesn't get it right every time, but giving the model a physical embodiment with cold, unfeeling metal graspers makes the issue a little more thorny.&lt;/p&gt;
&lt;p&gt;To ensure robots behave safely, Gemini Robotics uses a multi-layered approach. "With the full Gemini Robotics, you are connecting to a model that is reasoning about what is safe to do, period," says Parada. "And then you have it talk to a VLA that actually produces options, and then that VLA calls a low-level controller, which typically has safety-critical components, like how much force you can move or how fast you can move this arm."&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2102351-2" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Robustness-Omega-Star.mp4?_=2" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Importantly, the new on-device model is just a VLA, so developers will be on their own to build in safety. Google suggests they replicate what the Gemini team has done, though. It's recommended that developers in the early tester program connect the system to the standard Gemini Live API, which includes a safety layer. They should also implement a low-level controller for critical safety checks.&lt;/p&gt;
&lt;p&gt;Anyone interested in testing Gemini Robotics On-Device should apply for access to Google's trusted tester program. Google's Carolina Parada says there have been a lot of robotics breakthroughs in the past three years, and this is just the beginning—the current release of Gemini Robotics is still based on Gemini 2.0. Parada notes that the Gemini Robotics team typically trails behind Gemini development by one version, and Gemini 2.5 has been cited as a massive improvement in chatbot functionality. Maybe the same will be true of robots.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google's Carolina Parada says Gemini has enabled huge robotics breakthroughs, like the new on-device AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Apollo robot" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Robotics-SocialShare_1920x1080.width-1300-640x360.png" width="640" /&gt;
                  &lt;img alt="Apollo robot" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Robotics-SocialShare_1920x1080.width-1300-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The humanoid Apollo robot is one of the platforms supported in Gemini Robotics. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;We sometimes call chatbots like Gemini and ChatGPT "robots," but generative AI is also playing a growing role in real, physical robots. After announcing Gemini Robotics earlier this year, Google DeepMind has now revealed a new on-device VLA (vision language action) model to control robots. Unlike the previous release, there's no cloud component, allowing robots to operate with full autonomy.&lt;/p&gt;
&lt;p&gt;Carolina Parada, head of robotics at Google DeepMind, says this approach to AI robotics could make robots more reliable in challenging situations. This is also the first version of Google's robotics model that developers can tune for their specific uses.&lt;/p&gt;
&lt;p&gt;Robotics is a unique problem for AI because, not only does the robot exist in the physical world, but it also changes its environment. Whether you're having it move blocks around or tie your shoes, it's hard to predict every eventuality a robot might encounter. The traditional approach of training a robot on action with reinforcement was very slow, but generative AI allows for much greater generalization.&lt;/p&gt;
&lt;p&gt;"It's drawing from Gemini's multimodal world understanding in order to do a completely new task," explains Carolina Parada. "What that enables is in that same way Gemini can produce text, write poetry, just summarize an article, you can also write code, and you can also generate images. It also can generate robot actions."&lt;/p&gt;
&lt;h2&gt;General robots, no cloud needed&lt;/h2&gt;
&lt;p&gt;In the previous Gemini Robotics release (which is still the "best" version of Google's robotics tech), the platforms ran a hybrid system with a small model on the robot and a larger one running in the cloud. You've probably watched chatbots "think" for measurable seconds as they generate an output, but robots need to react quickly. If you tell the robot to pick up and move an object, you don't want it to pause while each step is generated. The local model allows quick adaptation, while the server-based model can help with complex reasoning tasks. Google DeepMind is now unleashing the local model as a standalone VLA, and it's surprisingly robust.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2102351-1" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Dexterity-General-Aloha.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The new Gemini Robotics On-Device model is only a little less accurate than the hybrid version. According to Parada, many tasks will work out of the box. "When we play with the robots, we see that they're surprisingly capable of understanding a new situation," Parada tells Ars.&lt;/p&gt;
&lt;p&gt;By releasing this model with a full SDK, the team hopes developers will give Gemini-powered robots new tasks and show them new environments, which could reveal actions that don't work with the model's stock tuning. With the SDK, robotics researchers will be able to adapt the VLA to new tasks with as little as 50 to 100 demonstrations.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2102439 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Robotics On-Device chart" class="fullwidth full" height="476" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/On-Device-Robotics-AI.png" width="756" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Robotics On-Device model is almost as adaptable as the hybrid model with cloud processing.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A "demonstration" in AI robotics is a bit different from other areas of AI research. Parada explains that demonstrations typically involve tele-operating the robot—controlling the machinery manually to complete a task tunes the model to handle that task autonomously. While synthetic data is an element of Google's training, it's not a substitute for the real thing. "We still find that in the most complex, dexterous behaviors, we need real data," says Parada. "But there is quite a lot that you can do with simulation."&lt;/p&gt;
&lt;p&gt;But those highly complex behaviors may be beyond the capabilities of the on-device VLA. It should have no problem with straightforward actions like tying a shoe (a traditionally difficult task for AI robots) or folding a shirt. If, however, you wanted a robot to make you a sandwich, it would probably need a more powerful model to go through the multi-step reasoning required to get the bread in the right place.&lt;/p&gt;
&lt;p&gt;The team sees Gemini Robotics On-Device as ideal for environments where connectivity to the cloud is spotty or non-existent. Processing the robot's visual data locally is also better for privacy, for example, in a health care environment.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Building safe robots&lt;/h2&gt;
&lt;p&gt;Safety is always a concern with AI systems, whether it's a chatbot that provides dangerous information or a robot that goes Terminator. We've all seen generative AI chatbots and image generators hallucinate falsehoods in their outputs, and the generative systems powering Gemini Robotics are no different—the model doesn't get it right every time, but giving the model a physical embodiment with cold, unfeeling metal graspers makes the issue a little more thorny.&lt;/p&gt;
&lt;p&gt;To ensure robots behave safely, Gemini Robotics uses a multi-layered approach. "With the full Gemini Robotics, you are connecting to a model that is reasoning about what is safe to do, period," says Parada. "And then you have it talk to a VLA that actually produces options, and then that VLA calls a low-level controller, which typically has safety-critical components, like how much force you can move or how fast you can move this arm."&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2102351-2" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Robustness-Omega-Star.mp4?_=2" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Importantly, the new on-device model is just a VLA, so developers will be on their own to build in safety. Google suggests they replicate what the Gemini team has done, though. It's recommended that developers in the early tester program connect the system to the standard Gemini Live API, which includes a safety layer. They should also implement a low-level controller for critical safety checks.&lt;/p&gt;
&lt;p&gt;Anyone interested in testing Gemini Robotics On-Device should apply for access to Google's trusted tester program. Google's Carolina Parada says there have been a lot of robotics breakthroughs in the past three years, and this is just the beginning—the current release of Gemini Robotics is still based on Gemini 2.0. Parada notes that the Gemini Robotics team typically trails behind Gemini development by one version, and Gemini 2.5 has been cited as a massive improvement in chatbot functionality. Maybe the same will be true of robots.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/06/google-releases-first-cloud-free-ai-robotics-model/</guid><pubDate>Tue, 24 Jun 2025 14:00:41 +0000</pubDate></item><item><title>[NEW] From research to climate resilience (The latest research from Google)</title><link>https://research.google/blog/how-ai-is-helping-us-build-a-more-resilient-planet/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;At Google Research, we're driven by exploring the art of the possible. Our research impacts products, businesses, scientific discovery, and society. Today, the opportunity to help solve seemingly impossible problems with breakthrough research is greater than ever, as is the opportunity to translate this research into real-world impact.&lt;/p&gt;&lt;p&gt;I'm excited to share how we're advancing research and harnessing tech innovation to help build more resilience, tackling the urgent challenges of climate crises, such as wildfires, floods, extreme weather, and cyclones. Can we deliver timely, reliable predictions of these threats, helping people stay safe and communities build resilience?&lt;/p&gt;&lt;p&gt;Our work with AI over the past years, advancing climate science and tackling these difficult problems, is already making a tangible difference. It clearly demonstrates AI’s potential to build towards better climate resilience.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;At Google Research, we're driven by exploring the art of the possible. Our research impacts products, businesses, scientific discovery, and society. Today, the opportunity to help solve seemingly impossible problems with breakthrough research is greater than ever, as is the opportunity to translate this research into real-world impact.&lt;/p&gt;&lt;p&gt;I'm excited to share how we're advancing research and harnessing tech innovation to help build more resilience, tackling the urgent challenges of climate crises, such as wildfires, floods, extreme weather, and cyclones. Can we deliver timely, reliable predictions of these threats, helping people stay safe and communities build resilience?&lt;/p&gt;&lt;p&gt;Our work with AI over the past years, advancing climate science and tackling these difficult problems, is already making a tangible difference. It clearly demonstrates AI’s potential to build towards better climate resilience.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/how-ai-is-helping-us-build-a-more-resilient-planet/</guid><pubDate>Tue, 24 Jun 2025 14:47:00 +0000</pubDate></item><item><title>[NEW] Emergence AI’s CRAFT arrives to make it easy for enterprises to automate their entire data pipeline (AI News | VentureBeat)</title><link>https://venturebeat.com/data-infrastructure/emergence-ais-craft-arrives-to-make-it-easy-for-enterprises-to-automate-their-entire-data-pipeline/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;New York City based startup Emergence AI, founded by former IBM researchers, previously made headlines for its impressive automated system that allows enterprises to type in a requested task in plain natural language and automatically create a fleet of agents to help complete it. &lt;/p&gt;



&lt;p&gt;But that’s not all the company has up its sleeve when it comes to automation and AI: today it is launching CRAFT, a new self-serve platform designed to automate enterprise data pipelines, that is, all the enterprise data a user could ever want, organized and made searchable behind the scenes, and retrievable in seconds with the help of Emergence’s AI agents. &lt;/p&gt;



&lt;p&gt;Using only plain English, CRAFT allows business users — not just developers — to construct intelligent agent systems that handle tasks traditionally managed by teams of engineers.&lt;/p&gt;



&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://venturebeat.com/wp-content/uploads/2025/06/CRAFT-launch-video.mov"&gt;&lt;/video&gt;&lt;/figure&gt;



&lt;p&gt;Emergence AI designed its platform to be interoperable with a range of leading AI models and agent frameworks. The system integrates with foundation models such as OpenAI’s GPT-4o and GPT-4.5, Anthropic’s Claude 3.7 Sonnet, and Meta’s Llama 3.3. It also supports orchestration frameworks including LangChain, Crew AI, and Microsoft Autogen, allowing enterprises to bring their own models and tools into CRAFT’s agentic workflows with minimal friction.&lt;/p&gt;



&lt;p&gt;CRAFT, which stands for Create, Remember, Assemble, Fine-tune, Trust, is positioned as a solution for a global challenge that represents over $200 billion in annual enterprise spend. It replaces brittle, developer-intensive workflows with swarms of self-governing agents capable of building, testing, and running data workflows from a simple prompt.&lt;/p&gt;



&lt;p&gt;“This is a big moment,” said Satya Nitta, Co-founder and CEO of Emergence AI. “I’m very excited about the refinement of what we’re trying to do.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-is-emergence-ai-and-who-is-behind-it"&gt;What is Emergence AI and who is behind it?&lt;/h2&gt;



&lt;p&gt;Emergence AI was founded by veterans of IBM Research and came out of stealth in late 2024 with over $97 million in funding. The company first drew industry attention with the launch of its cross-platform multi-agent orchestrator, designed to operate across enterprise systems from various vendors. Unlike offerings from Microsoft, Salesforce, or Amazon, Emergence emphasized cross-vendor compatibility as a core advantage. Read the full story at VentureBeat.&lt;/p&gt;



&lt;p&gt;According to Nitta, “The action space, as I call it, is very hard to generalize. That’s one of the core challenges we’ve unpacked.” Emergence’s orchestrator functions as a meta-agent, dynamically planning and executing tasks across disparate systems using a blend of web automation and secure API integrations.&lt;/p&gt;



&lt;p&gt;In early 2025, Emergence followed up with a real-time, no-code AI agent builder capable of generating recursive workflows and domain-specific agents on the fly. Nitta described this as a new phase in enterprise intelligence, where agent systems can not only execute tasks but also create new agents to expand their capabilities. Read more in VentureBeat’s April 2025 article.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-exchange-weeks-of-engineering-for-a-few-minutes-of-automation"&gt;Exchange weeks of engineering for a few minutes of automation&lt;/h2&gt;



&lt;p&gt;With CRAFT, tasks that once required weeks of coding—such as writing ETLs, cleaning datasets, and integrating tools—can now be completed in minutes. &lt;/p&gt;



&lt;p&gt;The system’s capabilities go beyond traditional RPA (robotic process automation), introducing features such as self-improvement, planning and reasoning, and long-term memory.&lt;/p&gt;



&lt;p&gt;CRAFT is purpose-built for data-heavy environments where rapid decision-making is critical and data is often siloed or fragmented. Key industries include:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Financial services&lt;/li&gt;



&lt;li&gt;Supply chain and logistics&lt;/li&gt;



&lt;li&gt;SaaS and tech platforms&lt;/li&gt;



&lt;li&gt;Oil and gas&lt;/li&gt;



&lt;li&gt;E-commerce and research organizations&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;It is particularly suited for organizations requiring complex, real-time orchestration without engineering bottlenecks.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3013031" height="312" src="https://venturebeat.com/wp-content/uploads/2025/06/datapipeline-1.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;Nitta emphasized the platform’s clarity and intentionality: “What makes this interesting is how focused it is. It’s a very clear story—no messing around.”&lt;/p&gt;



&lt;p&gt;He also highlighted the technical depth behind CRAFT’s capabilities. “Orchestration in its entirety is an AI problem. Most people don’t realize how complex it is to ask a system to perform any action in the physical or digital world.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-early-rave-reviews-from-enterprises-and-industry"&gt;Early rave reviews from enterprises and industry&lt;/h2&gt;



&lt;p&gt;CRAFT is already in use by design partners across sectors such as semiconductors, oil and gas, telecom, healthcare, and financial services. &lt;/p&gt;



&lt;p&gt;One notable deployment is with NI/Emerson, where the platform is integrated with the NI O+ solution. &lt;/p&gt;



&lt;p&gt;Eran Rousseau, VP and Fellow at NI/Emerson, reported that CRAFT enables real-time insights aligned with complex production workflows, improving yields and accelerating issue detection.&lt;/p&gt;



&lt;p&gt;“Previously, by the time manual analysis was complete, the next production batch was underway, limiting timely action,” Rosseau said in a statement emailed to VentureBeat by Emergence reps. “With CRAFT, we gain real-time insights aligned with complex test workflows, enabling smart automation to improve yields and detect issues quickly. Integrated with NI’s O+ solution, CRAFT AI generates insights that translate into immediate actions, allowing customers to focus on solving critical challenges rather than on lengthy reports. This represents the future of semiconductor operations.”&lt;/p&gt;



&lt;p&gt;Carrol Chang, CEO of Andela, emphasized the importance of mastering data-centric workflows, dynamic data integration, and governance in the AI age.&lt;/p&gt;



&lt;p&gt;“Our partnership with Emergence will empower Andela engineers to build the skills needed to thrive in the next chapter: steering intelligent agents, designing resilient systems, and working creatively alongside AI,” Chang wrote in another emailed state from Emergence. “Critically, this includes mastering data-centric workflow, from integrating dynamic data sources to ensuring data integrity and governance, which are foundational to modern enterprise applications. It’s about preparing talent to lead the future, not just adapt to it.”&lt;/p&gt;



&lt;p&gt;Other early results include significant productivity gains:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In semiconductor fabrication, CRAFT has helped identify millions in weekly savings&lt;/li&gt;



&lt;li&gt;A telecom firm saw a 70% reduction in data governance time&lt;/li&gt;



&lt;li&gt;A large online forum reduced 60 million unsafe images per month using AI agents&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-the-tech-behind-the-platform-agents-creating-agents-aca"&gt;The tech behind the platform: Agents Creating Agents (ACA)&lt;/h2&gt;



&lt;p&gt;CRAFT’s engine includes a novel framework called Agents Creating Agents (ACA). This system allows agents to generate, improve, and coordinate other agents in real time—enabling complex multi-agent workflows with minimal input.&lt;/p&gt;



&lt;p&gt;The platform’s interface supports both no-code and developer-customized workflows. It features an agent registry for reuse, memory capabilities across sessions, and customizable behavior through feedback or direct editing. Enterprise features include deployment flexibility (on-prem, cloud, or VPC) and built-in compliance controls such as SOC 2, GDPR, and audit logging.&lt;/p&gt;



&lt;p&gt;Nitta explained why Emergence has been ahead of the curve on agentic systems: “Everybody else is now talking about agent orchestrations, but we’ve already been thinking deeply about it.”&lt;/p&gt;



&lt;p&gt;CRAFT also has implications for workforce transformation. Through a partnership with Emergence AI, Andela is training engineers to work with agentic systems. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-availability-and-pricing"&gt;Availability and pricing&lt;/h2&gt;



&lt;p&gt;CRAFT is currently available in private preview as part of a phased rollout. The platform will be offered in three pricing tiers:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt; – suitable for individual experimentation&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Pro&lt;/strong&gt; – a cost-based tier with additional capabilities&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Enterprise&lt;/strong&gt; – custom pricing with advanced governance and infrastructure support&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Pro and Enterprise pricing will evolve with user adoption as Emergence AI focuses on accessibility and learning in this early rollout.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-moving-from-functional-user-to-system-architect"&gt;Moving from functional user to system architect&lt;/h2&gt;



&lt;p&gt;According to Nitta, CRAFT represents a step toward democratizing the development of intelligent systems. By enabling non-developers to define and deploy agentic workflows, the platform reimagines how enterprises interact with AI. The goal is to lower the barrier to entry while expanding what’s possible—turning business intent into executable systems without requiring deep technical expertise.&lt;/p&gt;



&lt;p&gt;Emergence AI, headquartered in New York with offices in California, Spain, and India, aims to lead the evolution of enterprise AI by building platforms that not only perform tasks—but also create, adapt, and scale autonomously.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;New York City based startup Emergence AI, founded by former IBM researchers, previously made headlines for its impressive automated system that allows enterprises to type in a requested task in plain natural language and automatically create a fleet of agents to help complete it. &lt;/p&gt;



&lt;p&gt;But that’s not all the company has up its sleeve when it comes to automation and AI: today it is launching CRAFT, a new self-serve platform designed to automate enterprise data pipelines, that is, all the enterprise data a user could ever want, organized and made searchable behind the scenes, and retrievable in seconds with the help of Emergence’s AI agents. &lt;/p&gt;



&lt;p&gt;Using only plain English, CRAFT allows business users — not just developers — to construct intelligent agent systems that handle tasks traditionally managed by teams of engineers.&lt;/p&gt;



&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://venturebeat.com/wp-content/uploads/2025/06/CRAFT-launch-video.mov"&gt;&lt;/video&gt;&lt;/figure&gt;



&lt;p&gt;Emergence AI designed its platform to be interoperable with a range of leading AI models and agent frameworks. The system integrates with foundation models such as OpenAI’s GPT-4o and GPT-4.5, Anthropic’s Claude 3.7 Sonnet, and Meta’s Llama 3.3. It also supports orchestration frameworks including LangChain, Crew AI, and Microsoft Autogen, allowing enterprises to bring their own models and tools into CRAFT’s agentic workflows with minimal friction.&lt;/p&gt;



&lt;p&gt;CRAFT, which stands for Create, Remember, Assemble, Fine-tune, Trust, is positioned as a solution for a global challenge that represents over $200 billion in annual enterprise spend. It replaces brittle, developer-intensive workflows with swarms of self-governing agents capable of building, testing, and running data workflows from a simple prompt.&lt;/p&gt;



&lt;p&gt;“This is a big moment,” said Satya Nitta, Co-founder and CEO of Emergence AI. “I’m very excited about the refinement of what we’re trying to do.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-is-emergence-ai-and-who-is-behind-it"&gt;What is Emergence AI and who is behind it?&lt;/h2&gt;



&lt;p&gt;Emergence AI was founded by veterans of IBM Research and came out of stealth in late 2024 with over $97 million in funding. The company first drew industry attention with the launch of its cross-platform multi-agent orchestrator, designed to operate across enterprise systems from various vendors. Unlike offerings from Microsoft, Salesforce, or Amazon, Emergence emphasized cross-vendor compatibility as a core advantage. Read the full story at VentureBeat.&lt;/p&gt;



&lt;p&gt;According to Nitta, “The action space, as I call it, is very hard to generalize. That’s one of the core challenges we’ve unpacked.” Emergence’s orchestrator functions as a meta-agent, dynamically planning and executing tasks across disparate systems using a blend of web automation and secure API integrations.&lt;/p&gt;



&lt;p&gt;In early 2025, Emergence followed up with a real-time, no-code AI agent builder capable of generating recursive workflows and domain-specific agents on the fly. Nitta described this as a new phase in enterprise intelligence, where agent systems can not only execute tasks but also create new agents to expand their capabilities. Read more in VentureBeat’s April 2025 article.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-exchange-weeks-of-engineering-for-a-few-minutes-of-automation"&gt;Exchange weeks of engineering for a few minutes of automation&lt;/h2&gt;



&lt;p&gt;With CRAFT, tasks that once required weeks of coding—such as writing ETLs, cleaning datasets, and integrating tools—can now be completed in minutes. &lt;/p&gt;



&lt;p&gt;The system’s capabilities go beyond traditional RPA (robotic process automation), introducing features such as self-improvement, planning and reasoning, and long-term memory.&lt;/p&gt;



&lt;p&gt;CRAFT is purpose-built for data-heavy environments where rapid decision-making is critical and data is often siloed or fragmented. Key industries include:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Financial services&lt;/li&gt;



&lt;li&gt;Supply chain and logistics&lt;/li&gt;



&lt;li&gt;SaaS and tech platforms&lt;/li&gt;



&lt;li&gt;Oil and gas&lt;/li&gt;



&lt;li&gt;E-commerce and research organizations&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;It is particularly suited for organizations requiring complex, real-time orchestration without engineering bottlenecks.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3013031" height="312" src="https://venturebeat.com/wp-content/uploads/2025/06/datapipeline-1.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;Nitta emphasized the platform’s clarity and intentionality: “What makes this interesting is how focused it is. It’s a very clear story—no messing around.”&lt;/p&gt;



&lt;p&gt;He also highlighted the technical depth behind CRAFT’s capabilities. “Orchestration in its entirety is an AI problem. Most people don’t realize how complex it is to ask a system to perform any action in the physical or digital world.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-early-rave-reviews-from-enterprises-and-industry"&gt;Early rave reviews from enterprises and industry&lt;/h2&gt;



&lt;p&gt;CRAFT is already in use by design partners across sectors such as semiconductors, oil and gas, telecom, healthcare, and financial services. &lt;/p&gt;



&lt;p&gt;One notable deployment is with NI/Emerson, where the platform is integrated with the NI O+ solution. &lt;/p&gt;



&lt;p&gt;Eran Rousseau, VP and Fellow at NI/Emerson, reported that CRAFT enables real-time insights aligned with complex production workflows, improving yields and accelerating issue detection.&lt;/p&gt;



&lt;p&gt;“Previously, by the time manual analysis was complete, the next production batch was underway, limiting timely action,” Rosseau said in a statement emailed to VentureBeat by Emergence reps. “With CRAFT, we gain real-time insights aligned with complex test workflows, enabling smart automation to improve yields and detect issues quickly. Integrated with NI’s O+ solution, CRAFT AI generates insights that translate into immediate actions, allowing customers to focus on solving critical challenges rather than on lengthy reports. This represents the future of semiconductor operations.”&lt;/p&gt;



&lt;p&gt;Carrol Chang, CEO of Andela, emphasized the importance of mastering data-centric workflows, dynamic data integration, and governance in the AI age.&lt;/p&gt;



&lt;p&gt;“Our partnership with Emergence will empower Andela engineers to build the skills needed to thrive in the next chapter: steering intelligent agents, designing resilient systems, and working creatively alongside AI,” Chang wrote in another emailed state from Emergence. “Critically, this includes mastering data-centric workflow, from integrating dynamic data sources to ensuring data integrity and governance, which are foundational to modern enterprise applications. It’s about preparing talent to lead the future, not just adapt to it.”&lt;/p&gt;



&lt;p&gt;Other early results include significant productivity gains:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In semiconductor fabrication, CRAFT has helped identify millions in weekly savings&lt;/li&gt;



&lt;li&gt;A telecom firm saw a 70% reduction in data governance time&lt;/li&gt;



&lt;li&gt;A large online forum reduced 60 million unsafe images per month using AI agents&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-the-tech-behind-the-platform-agents-creating-agents-aca"&gt;The tech behind the platform: Agents Creating Agents (ACA)&lt;/h2&gt;



&lt;p&gt;CRAFT’s engine includes a novel framework called Agents Creating Agents (ACA). This system allows agents to generate, improve, and coordinate other agents in real time—enabling complex multi-agent workflows with minimal input.&lt;/p&gt;



&lt;p&gt;The platform’s interface supports both no-code and developer-customized workflows. It features an agent registry for reuse, memory capabilities across sessions, and customizable behavior through feedback or direct editing. Enterprise features include deployment flexibility (on-prem, cloud, or VPC) and built-in compliance controls such as SOC 2, GDPR, and audit logging.&lt;/p&gt;



&lt;p&gt;Nitta explained why Emergence has been ahead of the curve on agentic systems: “Everybody else is now talking about agent orchestrations, but we’ve already been thinking deeply about it.”&lt;/p&gt;



&lt;p&gt;CRAFT also has implications for workforce transformation. Through a partnership with Emergence AI, Andela is training engineers to work with agentic systems. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-availability-and-pricing"&gt;Availability and pricing&lt;/h2&gt;



&lt;p&gt;CRAFT is currently available in private preview as part of a phased rollout. The platform will be offered in three pricing tiers:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt; – suitable for individual experimentation&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Pro&lt;/strong&gt; – a cost-based tier with additional capabilities&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Enterprise&lt;/strong&gt; – custom pricing with advanced governance and infrastructure support&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Pro and Enterprise pricing will evolve with user adoption as Emergence AI focuses on accessibility and learning in this early rollout.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-moving-from-functional-user-to-system-architect"&gt;Moving from functional user to system architect&lt;/h2&gt;



&lt;p&gt;According to Nitta, CRAFT represents a step toward democratizing the development of intelligent systems. By enabling non-developers to define and deploy agentic workflows, the platform reimagines how enterprises interact with AI. The goal is to lower the barrier to entry while expanding what’s possible—turning business intent into executable systems without requiring deep technical expertise.&lt;/p&gt;



&lt;p&gt;Emergence AI, headquartered in New York with offices in California, Spain, and India, aims to lead the evolution of enterprise AI by building platforms that not only perform tasks—but also create, adapt, and scale autonomously.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/data-infrastructure/emergence-ais-craft-arrives-to-make-it-easy-for-enterprises-to-automate-their-entire-data-pipeline/</guid><pubDate>Tue, 24 Jun 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] How CISOs became the gatekeepers of $309B AI infrastructure spending (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/agenticops-and-the-race-to-control-enterprise-ai/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Enterprise AI infrastructure spending is expected to reach $309 billion by 2032. The winners won’t be determined by who has the best models; it’ll come down to who controls the infrastructure layer that makes AI operational at scale.&lt;/p&gt;



&lt;p&gt;Security vendors are making the most aggressive moves. Palo Alto Networks, CrowdStrike and Cisco each report AI-driven security revenue growing 70 to 80% year-over-year while traditional infrastructure sales decline. The pattern is clear: Security is becoming the control plane for enterprise AI.&lt;/p&gt;



&lt;p&gt;“The complexity of AI workloads is straining existing infrastructure to its breaking point,” Ali Ghodsi, CEO of Databricks, notes in a blog post. “Enterprises need fundamentally new approaches to manage AI at scale.”&lt;/p&gt;



&lt;p&gt;The evidence is mounting. According to IDC, 73% of enterprises cite infrastructure inadequacy as their primary barrier to AI adoption. Meanwhile, adversaries are weaponizing AI faster than enterprises can deploy defenses. The infrastructure wars have begun.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-agenticops-emerges-as-the-new-battleground"&gt;AgenticOps emerges as the new battleground&lt;/h2&gt;



&lt;p&gt;AgenticOps isn’t one vendor’s vision. It’s an industry-wide recognition that traditional IT operations can’t manage AI agents operating at machine speed with human permissions. Cisco kicked off the category at Cisco Live 2025, but Microsoft’s AI Orchestration, Google’s Model Operations and startups like Weights &amp;amp; Biases are already racing to own it. The battle lines are drawn.&lt;/p&gt;



&lt;p&gt;The technical requirements are brutal. Enterprises deploying 50,000 AI agents need infrastructure that handles cross-domain data access, real-time governance and multi-team collaboration. Traditional tools break at 5,000 agents. The math doesn’t work.&lt;/p&gt;



&lt;p&gt;“For the very first time, security is becoming an accelerant to adoption, rather than an impediment,” Jeetu Patel, Cisco’s president and CPO, told VentureBeat in a recent interview. The shift is fundamental: Security teams now enable AI deployment rather than blocking it. &lt;/p&gt;



&lt;p&gt;Three pillars define enterprise-grade AgenticOps: unified data access across all domains, collaborative environments where NetOps and SecOps teams work together and purpose-built models that govern agent actions. Forrester research confirms multi-domain visibility as critical. Vendors who master all three components will be the ones to dominate. But most struggle to deliver even one effectively.&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3012821" height="513" src="https://venturebeat.com/wp-content/uploads/2025/06/hed-1.jpg?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Cisco’s President and CPO Jeetu Patel launched AgenticOps at Cisco Live 2025, signaling a decisive move toward cross-domain, multiplayer AI operations built on a purpose-built model engineered to handle the complexity and scale of enterprise infrastructure in the agentic AI era. &lt;/em&gt;Source: LinkedIn&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-the-death-of-perimeter-security"&gt;The death of perimeter security&lt;/h2&gt;



&lt;p&gt;Traditional firewalls can’t protect AI workloads. The evidence is overwhelming. Palo Alto’s Prisma Cloud processes 2 billion security events daily at runtime. Fortinet’s Security Fabric connects more than 500 integration points because perimeter defense has failed. Check Point’s Infinity operates on zero-trust principles, assuming a breach at every layer.&lt;/p&gt;



&lt;p&gt;Extended Berkeley Packet Filter (eBPF) changed the game. This Linux kernel technology enables security enforcement without the 40% performance hit of traditional approaches. Cisco’s $2.8 billion Isovalent acquisition validated the approach. Cilium, Isovalent’s open-source project, now secures production workloads at Netflix, Adobe and Capital One. The 15,000 GitHub stars reflect enterprise adoption, not developer interest.&lt;/p&gt;



&lt;p&gt;Craig Connors, Cisco’s VP and CTO of security, framed the shift in a recent VentureBeat interview: “Security policy now applies across every layer, from workload to silicon.” The implication is clear. Security becomes an integral part of infrastructure, not an overlay.&lt;/p&gt;



&lt;p&gt;Hardware acceleration seals the transformation. Silicon-embedded security operates at nanosecond latency. The math is brutal: Software-defined security adds 50-200 milliseconds. Hardware security adds 50 to 200 nanoseconds. That’s a million-fold improvement. Vendors without silicon capabilities can’t compete.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-72-hour-exploit-window"&gt;The 72-hour exploit window&lt;/h2&gt;



&lt;p&gt;Adversaries weaponize vulnerabilities in 72 hours. Enterprises patch in 45 days. This gap generates 84% of successful breaches. Every security vendor is racing to close it.&lt;/p&gt;



&lt;p&gt;CrowdStrike’s Falcon Prevent blocks exploits before patches exist. Qualys VMDR delivers real-time vulnerability management. Tanium Patch promises sub-hour automated response. Cisco’s Live Protect applies kernel-level shields within minutes.&lt;/p&gt;



&lt;p&gt;The economics are undeniable. Ponemon Institute research shows that each hour of delayed patching costs $84,000 in breach risk. Automated platforms deliver a return on investment (ROI) in 4.7 months. CISOs can’t ignore the math.&lt;/p&gt;



&lt;p&gt;“Time is everything in cybersecurity,” emphasizes Shlomo Kramer, CEO of Cato Networks. “Automation isn’t just about efficiency; it’s about surviving attacks that human teams can’t respond to quickly enough.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-observability-wars-intensify"&gt;The observability wars intensify&lt;/h2&gt;



&lt;p&gt;The $28 billion Splunk acquisition signals a larger truth: Observability determines who wins the AI infrastructure battle. Datadog processes 18 trillion events daily. New Relic monitors 10 billion transactions per minute. Dynatrace tracks 2.5 million cloud applications.&lt;/p&gt;



&lt;p&gt;The stakes are existential. Enterprises deploying AI without observability are flying blind. “You can’t secure what you can’t see,” states Etay Maor, senior director of security strategy at Cato Networks. “Observability isn’t optional, it’s the very foundation of secure digital transformation.”&lt;/p&gt;



&lt;p&gt;Generative UI represents the next frontier. Instead of dashboards, AI creates interfaces in real-time based on the exact problem being solved. ServiceNow, Splunk and emerging players like Observable are betting that dynamic interfaces replace static dashboards within 24 months.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-market-consolidation-accelerates"&gt;Market consolidation accelerates&lt;/h2&gt;



&lt;p&gt;The infrastructure giants are assembling their armies through acquisition. Cisco paid $28 billion for Splunk. Palo Alto acquired Cider Security, Dig Security and Talon for a combined $1.2 billion. CrowdStrike bought Reposify, Humio, and Preempt. Broadcom’s $69 billion VMware acquisition reshapes the entire landscape.&lt;/p&gt;



&lt;p&gt;Platform velocity now determines survival. Unified architectures cut development time from years to months. What took 18 months to deploy now launches in 8 weeks. Engineering teams are voting with their feet, joining companies that ship at startup speed with enterprise scale.&lt;/p&gt;



&lt;p&gt;The AI infrastructure market is expected to consolidate from over 200 vendors to fewer than 20 platforms within 36 months. Gartner predicts 60% of current vendors won’t exist by 2027. The message is brutal: Control the full stack or become irrelevant.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-bottom-line"&gt;The bottom line&lt;/h2&gt;



&lt;p&gt;AgenticOps represents the most significant architectural shift since the advent of cloud computing. Enterprises that build AI infrastructure assuming continuous compromise, infinite identities and machine-speed attacks will thrive. Those clinging to perimeter defense and human-speed response will join Blockbuster and Kodak in the digital graveyard.&lt;/p&gt;



&lt;p&gt;The vendors solving this challenge — whether Cisco, Palo Alto, Microsoft or emerging players — will control the next decade of enterprise technology. The race is on. The clock is ticking. Winners are already emerging.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Enterprise AI infrastructure spending is expected to reach $309 billion by 2032. The winners won’t be determined by who has the best models; it’ll come down to who controls the infrastructure layer that makes AI operational at scale.&lt;/p&gt;



&lt;p&gt;Security vendors are making the most aggressive moves. Palo Alto Networks, CrowdStrike and Cisco each report AI-driven security revenue growing 70 to 80% year-over-year while traditional infrastructure sales decline. The pattern is clear: Security is becoming the control plane for enterprise AI.&lt;/p&gt;



&lt;p&gt;“The complexity of AI workloads is straining existing infrastructure to its breaking point,” Ali Ghodsi, CEO of Databricks, notes in a blog post. “Enterprises need fundamentally new approaches to manage AI at scale.”&lt;/p&gt;



&lt;p&gt;The evidence is mounting. According to IDC, 73% of enterprises cite infrastructure inadequacy as their primary barrier to AI adoption. Meanwhile, adversaries are weaponizing AI faster than enterprises can deploy defenses. The infrastructure wars have begun.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-agenticops-emerges-as-the-new-battleground"&gt;AgenticOps emerges as the new battleground&lt;/h2&gt;



&lt;p&gt;AgenticOps isn’t one vendor’s vision. It’s an industry-wide recognition that traditional IT operations can’t manage AI agents operating at machine speed with human permissions. Cisco kicked off the category at Cisco Live 2025, but Microsoft’s AI Orchestration, Google’s Model Operations and startups like Weights &amp;amp; Biases are already racing to own it. The battle lines are drawn.&lt;/p&gt;



&lt;p&gt;The technical requirements are brutal. Enterprises deploying 50,000 AI agents need infrastructure that handles cross-domain data access, real-time governance and multi-team collaboration. Traditional tools break at 5,000 agents. The math doesn’t work.&lt;/p&gt;



&lt;p&gt;“For the very first time, security is becoming an accelerant to adoption, rather than an impediment,” Jeetu Patel, Cisco’s president and CPO, told VentureBeat in a recent interview. The shift is fundamental: Security teams now enable AI deployment rather than blocking it. &lt;/p&gt;



&lt;p&gt;Three pillars define enterprise-grade AgenticOps: unified data access across all domains, collaborative environments where NetOps and SecOps teams work together and purpose-built models that govern agent actions. Forrester research confirms multi-domain visibility as critical. Vendors who master all three components will be the ones to dominate. But most struggle to deliver even one effectively.&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3012821" height="513" src="https://venturebeat.com/wp-content/uploads/2025/06/hed-1.jpg?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Cisco’s President and CPO Jeetu Patel launched AgenticOps at Cisco Live 2025, signaling a decisive move toward cross-domain, multiplayer AI operations built on a purpose-built model engineered to handle the complexity and scale of enterprise infrastructure in the agentic AI era. &lt;/em&gt;Source: LinkedIn&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-the-death-of-perimeter-security"&gt;The death of perimeter security&lt;/h2&gt;



&lt;p&gt;Traditional firewalls can’t protect AI workloads. The evidence is overwhelming. Palo Alto’s Prisma Cloud processes 2 billion security events daily at runtime. Fortinet’s Security Fabric connects more than 500 integration points because perimeter defense has failed. Check Point’s Infinity operates on zero-trust principles, assuming a breach at every layer.&lt;/p&gt;



&lt;p&gt;Extended Berkeley Packet Filter (eBPF) changed the game. This Linux kernel technology enables security enforcement without the 40% performance hit of traditional approaches. Cisco’s $2.8 billion Isovalent acquisition validated the approach. Cilium, Isovalent’s open-source project, now secures production workloads at Netflix, Adobe and Capital One. The 15,000 GitHub stars reflect enterprise adoption, not developer interest.&lt;/p&gt;



&lt;p&gt;Craig Connors, Cisco’s VP and CTO of security, framed the shift in a recent VentureBeat interview: “Security policy now applies across every layer, from workload to silicon.” The implication is clear. Security becomes an integral part of infrastructure, not an overlay.&lt;/p&gt;



&lt;p&gt;Hardware acceleration seals the transformation. Silicon-embedded security operates at nanosecond latency. The math is brutal: Software-defined security adds 50-200 milliseconds. Hardware security adds 50 to 200 nanoseconds. That’s a million-fold improvement. Vendors without silicon capabilities can’t compete.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-72-hour-exploit-window"&gt;The 72-hour exploit window&lt;/h2&gt;



&lt;p&gt;Adversaries weaponize vulnerabilities in 72 hours. Enterprises patch in 45 days. This gap generates 84% of successful breaches. Every security vendor is racing to close it.&lt;/p&gt;



&lt;p&gt;CrowdStrike’s Falcon Prevent blocks exploits before patches exist. Qualys VMDR delivers real-time vulnerability management. Tanium Patch promises sub-hour automated response. Cisco’s Live Protect applies kernel-level shields within minutes.&lt;/p&gt;



&lt;p&gt;The economics are undeniable. Ponemon Institute research shows that each hour of delayed patching costs $84,000 in breach risk. Automated platforms deliver a return on investment (ROI) in 4.7 months. CISOs can’t ignore the math.&lt;/p&gt;



&lt;p&gt;“Time is everything in cybersecurity,” emphasizes Shlomo Kramer, CEO of Cato Networks. “Automation isn’t just about efficiency; it’s about surviving attacks that human teams can’t respond to quickly enough.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-observability-wars-intensify"&gt;The observability wars intensify&lt;/h2&gt;



&lt;p&gt;The $28 billion Splunk acquisition signals a larger truth: Observability determines who wins the AI infrastructure battle. Datadog processes 18 trillion events daily. New Relic monitors 10 billion transactions per minute. Dynatrace tracks 2.5 million cloud applications.&lt;/p&gt;



&lt;p&gt;The stakes are existential. Enterprises deploying AI without observability are flying blind. “You can’t secure what you can’t see,” states Etay Maor, senior director of security strategy at Cato Networks. “Observability isn’t optional, it’s the very foundation of secure digital transformation.”&lt;/p&gt;



&lt;p&gt;Generative UI represents the next frontier. Instead of dashboards, AI creates interfaces in real-time based on the exact problem being solved. ServiceNow, Splunk and emerging players like Observable are betting that dynamic interfaces replace static dashboards within 24 months.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-market-consolidation-accelerates"&gt;Market consolidation accelerates&lt;/h2&gt;



&lt;p&gt;The infrastructure giants are assembling their armies through acquisition. Cisco paid $28 billion for Splunk. Palo Alto acquired Cider Security, Dig Security and Talon for a combined $1.2 billion. CrowdStrike bought Reposify, Humio, and Preempt. Broadcom’s $69 billion VMware acquisition reshapes the entire landscape.&lt;/p&gt;



&lt;p&gt;Platform velocity now determines survival. Unified architectures cut development time from years to months. What took 18 months to deploy now launches in 8 weeks. Engineering teams are voting with their feet, joining companies that ship at startup speed with enterprise scale.&lt;/p&gt;



&lt;p&gt;The AI infrastructure market is expected to consolidate from over 200 vendors to fewer than 20 platforms within 36 months. Gartner predicts 60% of current vendors won’t exist by 2027. The message is brutal: Control the full stack or become irrelevant.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-bottom-line"&gt;The bottom line&lt;/h2&gt;



&lt;p&gt;AgenticOps represents the most significant architectural shift since the advent of cloud computing. Enterprises that build AI infrastructure assuming continuous compromise, infinite identities and machine-speed attacks will thrive. Those clinging to perimeter defense and human-speed response will join Blockbuster and Kodak in the digital graveyard.&lt;/p&gt;



&lt;p&gt;The vendors solving this challenge — whether Cisco, Palo Alto, Microsoft or emerging players — will control the next decade of enterprise technology. The race is on. The clock is ticking. Winners are already emerging.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/agenticops-and-the-race-to-control-enterprise-ai/</guid><pubDate>Tue, 24 Jun 2025 15:05:00 +0000</pubDate></item><item><title>A federal judge sides with Anthropic in lawsuit over training AI on books without authors’ permission (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/24/a-federal-judge-sides-with-anthropic-in-lawsuit-over-training-ai-on-books-without-authors-permission/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/GettyImages-1243240042-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Federal judge William Alsup ruled that it was legal for Anthropic to train its AI models on published books without the authors’ permission. This marks the first time that the courts have given credence to AI companies’ claim that fair use doctrine can absolve AI companies from fault when they use copyrighted materials to train large language models (LLMs).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This decision comes as a blow to authors, artists, and publishers who have brought dozens of lawsuits against companies like OpenAI, Meta, Midjourney, Google, and more. While the ruling is not a guarantee that other judges will follow Judge Alsup’s lead, it lays the foundation for courts to side with tech companies over creatives.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These lawsuits often depend on how a judge interprets fair use doctrine, a notoriously finicky carve-out of copyright law that hasn’t been updated since 1976 — a time before the internet, let alone the concept of generative AI training sets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fair use rulings take into account what the work is being used for (parody and education can be viable), whether it’s being reproduced for commercial gain (you can write “Star Wars” fan fiction, but you can’t sell it), and how transformative a derivative work is from the original.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Companies like Meta have made similar fair use arguments in defense of training on copyrighted works, though before this week’s decision, it was less clear how the courts would sway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this particular case, Bartz v. Anthropic, the group of plaintiff authors also brought into question the manner in which Anthropic attained and stored their works. According to the lawsuit, Anthropic sought to create a “central library” of “all the books in the world” to keep “forever.” But millions of these copyrighted books were downloaded for free from pirate sites, which is unambiguously illegal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the judge granted that Anthropic’s training of these materials was a fair use, the court will hold a trial about the nature of the “central library.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“We will have a trial on the pirated copies used to create Anthropic’s central library and the resulting damages,” Judge Alsup wrote in the decision. “That Anthropic later bought a copy of a book it earlier stole off the internet will not absolve it of liability for theft but it may affect the extent of statutory damages.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/GettyImages-1243240042-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Federal judge William Alsup ruled that it was legal for Anthropic to train its AI models on published books without the authors’ permission. This marks the first time that the courts have given credence to AI companies’ claim that fair use doctrine can absolve AI companies from fault when they use copyrighted materials to train large language models (LLMs).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This decision comes as a blow to authors, artists, and publishers who have brought dozens of lawsuits against companies like OpenAI, Meta, Midjourney, Google, and more. While the ruling is not a guarantee that other judges will follow Judge Alsup’s lead, it lays the foundation for courts to side with tech companies over creatives.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These lawsuits often depend on how a judge interprets fair use doctrine, a notoriously finicky carve-out of copyright law that hasn’t been updated since 1976 — a time before the internet, let alone the concept of generative AI training sets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fair use rulings take into account what the work is being used for (parody and education can be viable), whether it’s being reproduced for commercial gain (you can write “Star Wars” fan fiction, but you can’t sell it), and how transformative a derivative work is from the original.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Companies like Meta have made similar fair use arguments in defense of training on copyrighted works, though before this week’s decision, it was less clear how the courts would sway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this particular case, Bartz v. Anthropic, the group of plaintiff authors also brought into question the manner in which Anthropic attained and stored their works. According to the lawsuit, Anthropic sought to create a “central library” of “all the books in the world” to keep “forever.” But millions of these copyrighted books were downloaded for free from pirate sites, which is unambiguously illegal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the judge granted that Anthropic’s training of these materials was a fair use, the court will hold a trial about the nature of the “central library.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“We will have a trial on the pirated copies used to create Anthropic’s central library and the resulting damages,” Judge Alsup wrote in the decision. “That Anthropic later bought a copy of a book it earlier stole off the internet will not absolve it of liability for theft but it may affect the extent of statutory damages.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/24/a-federal-judge-sides-with-anthropic-in-lawsuit-over-training-ai-on-books-without-authors-permission/</guid><pubDate>Tue, 24 Jun 2025 15:17:18 +0000</pubDate></item><item><title>Wispr Flow raises $30M from Menlo Ventures for its AI-powered dictation app (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/24/wispr-flow-raises-30m-from-menlo-ventures-for-its-ai-powered-dictation-app/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Startups developing voice AI technology and applications are having their moment. Model builders like ElevenLabs and Cartesia have raised millions of dollars in the last few months. Applications such as AI-powered notetaker Granola and meeting tools Read AI and Fireflies AI have also received investor attention and backing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Continuing the trend, dictation app Wispr Flow announced today that it is raising $30 million in Series A funding from Menlo Ventures with participation from NEA, 8VC, Opal&amp;nbsp;CEO Kenneth&amp;nbsp;Schlenker, Pinterest founder Evan Sharp, Carta&amp;nbsp;CEO Henry&amp;nbsp;Ward, and Lindy CEO Flo&amp;nbsp;Crivelli. Menlo Ventures’ Matt Kraning, who also backed the company as an angel investor, will join its board. To date, the company has raised $56 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup’s founder and CEO, Tanay Kothari, started building Wispr to create a device that would allow users to type just by mouthing words silently. Its prior funding was for that business. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, the company instead started focusing on Wispr Flow, the software interface designed for the hardware device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company released a Mac app in October 2024, followed by a Windows app in March 2025 and an iOS app earlier this month. Kothari mentioned that, since its early release, VCs in Silicon Valley have been using the product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think every single tier one venture fund in the valley uses Wispr Flow for their emails, memos, documents, and more. They feel themselves being hooked on it, and it is one of the products they use every day. Because of this, we started getting a lot of inbound,” Kothari said about investor interest. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Granola also had a similar story of receiving immense investor interest because VCs used their product a lot.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021583" height="677" src="https://techcrunch.com/wp-content/uploads/2025/06/Tanay-headshot.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Startup’s CEO Tanay Kothari.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Wispr Flow&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Kothari also noted that the startup will soon achieve profitability at the current rate of growth, and initially, he didn’t want to raise money. However, he worried that Big Tech players with a massive distribution advantage could be a risk to the company. He wanted to rapidly multiply the company’s revenue and reach and decided to take the investment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Menlo Ventures CEO Matt Kraning, who has been an avid user of the app, said that his initial thesis for Wispr Flow was that with the current set of input methods, like keyboards, we are “waiting for our thumbs to catch up with our thoughts.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Wispr Flow is creating an efficient way to translate digital thoughts and intent. The app captures users’ speech and what they want to convey very well. The team has thought about how people speak while developing models rather than focusing on things like word error rates,” he told TechCrunch.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-user-growth-and-future-roadmap"&gt;User growth and future roadmap&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The startup said that the app has been growing its user base by 50% month-over-month. Kothari noted that 40% of users of the app are in the U.S., 30% in Europe, and 30% in other parts of the world. In addition, more than 30% of the app’s users are from a nontechnical background.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“More and more people are using AI tools, but still, there isn’t a good interface for people who are not techies. ChatGPT-style interface is the most common one, and that was released three and a half years ago. We are building for all kinds of users so they don’t have to write system prompts to interface with AI,” Kothari said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the moment, Wispr Flow supports dictation in 104 languages. Kothari said that 40% of dications are in English, and 60% of them are in the rest of the languages, with Spanish, French, German, Dutch, Hindi, and Mandarin being the top languages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company will use the funding to grow its team of 18 with roles in engineering and go-to-market. It will also release an Android app and cater to enterprise users by setting up company-wide phrase context and support teams.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup is working on building Flow into a product that is akin to an AI-powered assistant that knows more about your personal context and helps you do everyday tasks like send messages, take notes, and set reminders. Plus, the company said it’s working with some AI hardware partners, without naming them, to power the interaction layer.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Startups developing voice AI technology and applications are having their moment. Model builders like ElevenLabs and Cartesia have raised millions of dollars in the last few months. Applications such as AI-powered notetaker Granola and meeting tools Read AI and Fireflies AI have also received investor attention and backing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Continuing the trend, dictation app Wispr Flow announced today that it is raising $30 million in Series A funding from Menlo Ventures with participation from NEA, 8VC, Opal&amp;nbsp;CEO Kenneth&amp;nbsp;Schlenker, Pinterest founder Evan Sharp, Carta&amp;nbsp;CEO Henry&amp;nbsp;Ward, and Lindy CEO Flo&amp;nbsp;Crivelli. Menlo Ventures’ Matt Kraning, who also backed the company as an angel investor, will join its board. To date, the company has raised $56 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup’s founder and CEO, Tanay Kothari, started building Wispr to create a device that would allow users to type just by mouthing words silently. Its prior funding was for that business. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, the company instead started focusing on Wispr Flow, the software interface designed for the hardware device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company released a Mac app in October 2024, followed by a Windows app in March 2025 and an iOS app earlier this month. Kothari mentioned that, since its early release, VCs in Silicon Valley have been using the product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think every single tier one venture fund in the valley uses Wispr Flow for their emails, memos, documents, and more. They feel themselves being hooked on it, and it is one of the products they use every day. Because of this, we started getting a lot of inbound,” Kothari said about investor interest. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Granola also had a similar story of receiving immense investor interest because VCs used their product a lot.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021583" height="677" src="https://techcrunch.com/wp-content/uploads/2025/06/Tanay-headshot.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Startup’s CEO Tanay Kothari.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Wispr Flow&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Kothari also noted that the startup will soon achieve profitability at the current rate of growth, and initially, he didn’t want to raise money. However, he worried that Big Tech players with a massive distribution advantage could be a risk to the company. He wanted to rapidly multiply the company’s revenue and reach and decided to take the investment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Menlo Ventures CEO Matt Kraning, who has been an avid user of the app, said that his initial thesis for Wispr Flow was that with the current set of input methods, like keyboards, we are “waiting for our thumbs to catch up with our thoughts.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Wispr Flow is creating an efficient way to translate digital thoughts and intent. The app captures users’ speech and what they want to convey very well. The team has thought about how people speak while developing models rather than focusing on things like word error rates,” he told TechCrunch.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-user-growth-and-future-roadmap"&gt;User growth and future roadmap&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The startup said that the app has been growing its user base by 50% month-over-month. Kothari noted that 40% of users of the app are in the U.S., 30% in Europe, and 30% in other parts of the world. In addition, more than 30% of the app’s users are from a nontechnical background.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“More and more people are using AI tools, but still, there isn’t a good interface for people who are not techies. ChatGPT-style interface is the most common one, and that was released three and a half years ago. We are building for all kinds of users so they don’t have to write system prompts to interface with AI,” Kothari said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the moment, Wispr Flow supports dictation in 104 languages. Kothari said that 40% of dications are in English, and 60% of them are in the rest of the languages, with Spanish, French, German, Dutch, Hindi, and Mandarin being the top languages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company will use the funding to grow its team of 18 with roles in engineering and go-to-market. It will also release an Android app and cater to enterprise users by setting up company-wide phrase context and support teams.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup is working on building Flow into a product that is akin to an AI-powered assistant that knows more about your personal context and helps you do everyday tasks like send messages, take notes, and set reminders. Plus, the company said it’s working with some AI hardware partners, without naming them, to power the interaction layer.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/24/wispr-flow-raises-30m-from-menlo-ventures-for-its-ai-powered-dictation-app/</guid><pubDate>Tue, 24 Jun 2025 16:00:00 +0000</pubDate></item><item><title>HPE and NVIDIA Debut AI Factory Stack to Power Next Industrial Shift (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/hpe-nvidia-ai-factory/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/NVIDIA-HPE.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;To speed up AI adoption across industries, HPE and NVIDIA today launched new AI factory offerings at HPE Discover in Las Vegas.&lt;/p&gt;
&lt;p&gt;The new lineup includes everything from modular AI factory infrastructure and HPE’s AI-ready RTX PRO Servers (HPE ProLiant Compute DL380a Gen12), to the next generation of HPE’s turnkey AI platform, HPE Private Cloud AI. The goal: give enterprises a framework to build and scale generative, agentic and industrial AI.&lt;/p&gt;
&lt;p&gt;The NVIDIA AI Computing by HPE portfolio is now among the broadest in the market.&lt;/p&gt;
&lt;p&gt;The portfolio combines NVIDIA Blackwell accelerated computing, NVIDIA Spectrum-X Ethernet and NVIDIA BlueField-3 networking technologies, NVIDIA AI Enterprise software and HPE’s full portfolio of servers, storage, services and software. This now includes HPE OpsRamp Software, a validated observability solution for the NVIDIA Enterprise AI Factory, and HPE Morpheus Enterprise Software for orchestration. The result is a pre-integrated, modular infrastructure stack to help teams get AI into production faster.&lt;/p&gt;
&lt;p&gt;This includes the next-generation HPE Private Cloud AI, co-engineered with NVIDIA and validated as part of the NVIDIA Enterprise AI Factory framework. This full-stack, turnkey AI factory solution will offer HPE ProLiant Compute DL380a Gen12 servers with the new NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs.&lt;/p&gt;
&lt;p&gt;These new NVIDIA RTX PRO Servers from HPE provide a universal data center platform for a wide range of enterprise AI and industrial AI use cases, and are now available to order from HPE. HPE Private Cloud AI includes the latest NVIDIA AI Blueprints, including the NVIDIA AI-Q Blueprint for AI agent creation and workflows.&lt;/p&gt;
&lt;p&gt;HPE also announced a new NVIDIA HGX B300 system, the HPE Compute XD690, built with NVIDIA Blackwell Ultra GPUs. It’s the latest entry in the NVIDIA AI Computing by HPE lineup and is expected to ship in October.&lt;/p&gt;
&lt;p&gt;In Japan, KDDI is working with HPE to build NVIDIA AI infrastructure to accelerate global adoption.&lt;/p&gt;
&lt;p&gt;The HPE-built KDDI system will be based on the NVIDIA GB200 NVL72 platform, built on the NVIDIA Grace Blackwell architecture, at the KDDI Osaka Sakai Data Center.&lt;/p&gt;
&lt;p&gt;To accelerate AI for financial services, HPE will co-test agentic AI workflows built on Accenture’s AI Refinery with NVIDIA, running on HPE Private Cloud AI. Initial use cases include sourcing, procurement and risk analysis.&lt;/p&gt;
&lt;p&gt;HPE said it’s adding 26 new partners to its “Unleash AI” ecosystem to support more NVIDIA AI use cases. The company now offers more than 70 packaged AI workloads, from fraud detection and video analytics to sovereign AI and cybersecurity.&lt;/p&gt;
&lt;p&gt;Security and governance were a focus, too. HPE Private Cloud AI supports air-gapped management, multi-tenancy and post-quantum cryptography. HPE’s try-before-you-buy program lets customers test the system in Equinix data centers before purchase. HPE also introduced new programs, including AI Acceleration Workshops with NVIDIA, to help scale AI deployments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Watch the keynote: &lt;/b&gt;HPE CEO Antonio Neri announced the news from the Las Vegas Sphere on Tuesday at 9 a.m. PT. Register for the livestream and watch the replay.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Explore more:&lt;/b&gt; Learn how NVIDIA and HPE build AI factories for every industry. Visit the partner page.&lt;/li&gt;
&lt;/ul&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/NVIDIA-HPE.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;To speed up AI adoption across industries, HPE and NVIDIA today launched new AI factory offerings at HPE Discover in Las Vegas.&lt;/p&gt;
&lt;p&gt;The new lineup includes everything from modular AI factory infrastructure and HPE’s AI-ready RTX PRO Servers (HPE ProLiant Compute DL380a Gen12), to the next generation of HPE’s turnkey AI platform, HPE Private Cloud AI. The goal: give enterprises a framework to build and scale generative, agentic and industrial AI.&lt;/p&gt;
&lt;p&gt;The NVIDIA AI Computing by HPE portfolio is now among the broadest in the market.&lt;/p&gt;
&lt;p&gt;The portfolio combines NVIDIA Blackwell accelerated computing, NVIDIA Spectrum-X Ethernet and NVIDIA BlueField-3 networking technologies, NVIDIA AI Enterprise software and HPE’s full portfolio of servers, storage, services and software. This now includes HPE OpsRamp Software, a validated observability solution for the NVIDIA Enterprise AI Factory, and HPE Morpheus Enterprise Software for orchestration. The result is a pre-integrated, modular infrastructure stack to help teams get AI into production faster.&lt;/p&gt;
&lt;p&gt;This includes the next-generation HPE Private Cloud AI, co-engineered with NVIDIA and validated as part of the NVIDIA Enterprise AI Factory framework. This full-stack, turnkey AI factory solution will offer HPE ProLiant Compute DL380a Gen12 servers with the new NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs.&lt;/p&gt;
&lt;p&gt;These new NVIDIA RTX PRO Servers from HPE provide a universal data center platform for a wide range of enterprise AI and industrial AI use cases, and are now available to order from HPE. HPE Private Cloud AI includes the latest NVIDIA AI Blueprints, including the NVIDIA AI-Q Blueprint for AI agent creation and workflows.&lt;/p&gt;
&lt;p&gt;HPE also announced a new NVIDIA HGX B300 system, the HPE Compute XD690, built with NVIDIA Blackwell Ultra GPUs. It’s the latest entry in the NVIDIA AI Computing by HPE lineup and is expected to ship in October.&lt;/p&gt;
&lt;p&gt;In Japan, KDDI is working with HPE to build NVIDIA AI infrastructure to accelerate global adoption.&lt;/p&gt;
&lt;p&gt;The HPE-built KDDI system will be based on the NVIDIA GB200 NVL72 platform, built on the NVIDIA Grace Blackwell architecture, at the KDDI Osaka Sakai Data Center.&lt;/p&gt;
&lt;p&gt;To accelerate AI for financial services, HPE will co-test agentic AI workflows built on Accenture’s AI Refinery with NVIDIA, running on HPE Private Cloud AI. Initial use cases include sourcing, procurement and risk analysis.&lt;/p&gt;
&lt;p&gt;HPE said it’s adding 26 new partners to its “Unleash AI” ecosystem to support more NVIDIA AI use cases. The company now offers more than 70 packaged AI workloads, from fraud detection and video analytics to sovereign AI and cybersecurity.&lt;/p&gt;
&lt;p&gt;Security and governance were a focus, too. HPE Private Cloud AI supports air-gapped management, multi-tenancy and post-quantum cryptography. HPE’s try-before-you-buy program lets customers test the system in Equinix data centers before purchase. HPE also introduced new programs, including AI Acceleration Workshops with NVIDIA, to help scale AI deployments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Watch the keynote: &lt;/b&gt;HPE CEO Antonio Neri announced the news from the Las Vegas Sphere on Tuesday at 9 a.m. PT. Register for the livestream and watch the replay.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Explore more:&lt;/b&gt; Learn how NVIDIA and HPE build AI factories for every industry. Visit the partner page.&lt;/li&gt;
&lt;/ul&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/hpe-nvidia-ai-factory/</guid><pubDate>Tue, 24 Jun 2025 16:30:14 +0000</pubDate></item><item><title>Salesforce Agentforce 3 brings visibility to AI agents (AI News)</title><link>https://www.artificialintelligence-news.com/news/salesforce-agentforce-3-brings-visibility-ai-agents/</link><description>&lt;p&gt;Salesforce Agentforce 3 aims to tackle what many businesses have been struggling with: actually seeing what their AI agents are up to.&lt;/p&gt;&lt;p&gt;Since its debut back in October 2024, Agentforce has been racking up some wins across a variety of sectors. Engine managed to slash customer case handling times by 15 percent, while 1-800Accountant handed off 70 percent of administrative chat queries to AI during the madness of tax season.&lt;/p&gt;&lt;p&gt;But what’s interesting about this upgrade isn’t just the numbers, it’s how Salesforce is addressing the elephant in the room that nobody likes to talk about: businesses are deploying AI agents at breakneck speed without really understanding what they’re doing or how to improve them.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-keeping-tabs-on-your-agents"&gt;Keeping tabs on your agents&lt;/h3&gt;&lt;p&gt;The centrepiece of Agentforce 3 is what Salesforce calls the Command Center (essentially a mission control for your AI employees.) It lets managers peek under the bonnet to spot patterns in how agents are performing, track health metrics in real-time (latency, escalation rates, errors), and identify which bits are working versus which need a swift kick.&lt;/p&gt;&lt;p&gt;For anyone who’s ever deployed AI tools and then wondered “now what?” this level of visibility could be game-changing. The system captures all agent activity using the OpenTelemetry standard, which means it plays nicely with tools like Datadog and Splunk that your IT team probably already has on their screens.&lt;/p&gt;&lt;p&gt;AI adoption is absolutely skyrocketing. Forthcoming data from the Slack Workflow Index shows AI agent usage up 233 percent in just six months. During that time, about 8,000 organisations signed up to deploy Agentforce.&lt;/p&gt;&lt;p&gt;Ryan Teeples, CTO at 1-800Accountant, said: “Agentforce autonomously resolved 70% of 1-800Accountant’s administrative chat engagements during the peak of this past tax season, an incredible lift during one of our busiest periods. But that early success was just the beginning.&lt;/p&gt;&lt;p&gt;“We’ve established a strong deployment foundation and weekly are focused on launching new agentic experiences and AI automations through Agentforce’s newest capabilities. With a high level of observability, we can see what’s working, optimise in real time, and scale support with confidence.”&lt;/p&gt;&lt;p&gt;Salesforce Agentforce 3 doesn’t just provide data, it actually suggests improvements. The AI effectively watches itself, identifying conversation patterns and recommending tweaks. It’s a bit meta, but potentially very useful for overstretched teams who don’t have time to manually review thousands of bot interactions.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-connectivity-conundrum-solved"&gt;The connectivity conundrum solved?&lt;/h3&gt;&lt;p&gt;Another headache Salesforce is tackling is connectivity. AI agents are only as useful as the systems they can access, but connecting them securely to your business tools has been a pain for most organisations.&lt;/p&gt;&lt;p&gt;Agentforce 3 brings native support for Model Context Protocol (MCP) – which Salesforce rather aptly describes as “USB-C for AI.” This essentially means AI agents can plug into any MCP-compliant server without custom coding, while still respecting your security policies.&lt;/p&gt;&lt;p&gt;This is where MuleSoft (which Salesforce acquired a few years back) comes into play, converting APIs and integrations into agent-ready assets. Heroku then handles deployment and maintenance of custom MCP servers.&lt;/p&gt;&lt;p&gt;Mollie Bodensteiner, SVP of Operations at Engine, commented: “Salesforce’s open ecosystem approach, especially through its native support for open standards like MCP, will be instrumental in helping us scale our use of AI agents with full confidence.&lt;/p&gt;&lt;p&gt;“We’ll be able to securely connect agents to the enterprise systems we rely on without custom code or compromising governance. That level of interoperability has given us the flexibility to accelerate adoption while staying in complete control of how agents operate within our environment.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-growing-the-salesforce-agentforce-ecosystem"&gt;Growing the Salesforce Agentforce ecosystem&lt;/h3&gt;&lt;p&gt;Perhaps the most interesting aspect of this announcement isn’t what Salesforce built themselves, but the ecosystem they’re nurturing. Over 30 partners have created MCP servers that integrate with Agentforce, including players like AWS, Google Cloud, Box, PayPal, and Stripe.&lt;/p&gt;&lt;p&gt;These integrations go far beyond simple data access. For instance, AWS integration lets agents analyse documents, extract information from images, transcribe audio recordings, and even identify important moments in videos. Google Cloud connections tie into Maps, databases, and AI models like Veo and Imagen.&lt;/p&gt;&lt;p&gt;Healthcare appears to be a particularly promising sector.&lt;/p&gt;&lt;p&gt;Tyler Bauer, VP for System Ambulatory Operations at UChicago Medicine, explains: “AI tools in healthcare must be adaptable to the complex and highly individualised needs of both patients and care teams.&lt;/p&gt;&lt;p&gt;“We need to support that goal by automating routine interactions in our patient access center that involve common questions and requests, which would free up the team’s time to focus on sensitive, more involved, or complex needs.”&lt;/p&gt;&lt;p&gt;The real question, of course, is whether all this will actually help businesses manage the growing army of AI agents they’re deploying. Getting visibility into AI performance has been a blind spot for many organisations—they often know roughly what percentage of queries the AI is handling, but struggle to identify specific shortcomings or improvement opportunities.&lt;/p&gt;&lt;p&gt;Adam Evans, EVP &amp;amp; GM of Salesforce AI, says: “Agentforce 3 will redefine how humans and AI agents work together—driving breakthrough levels of productivity, efficiency, and business transformation.”&lt;/p&gt;&lt;p&gt;Whether it lives up to that lofty promise remains to be seen, but addressing the visibility and control gap is certainly a step in the right direction for businesses struggling to properly manage their AI initiatives.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Huawei HarmonyOS 6 AI agents offer alternative to Android and iOS&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Salesforce Agentforce 3 aims to tackle what many businesses have been struggling with: actually seeing what their AI agents are up to.&lt;/p&gt;&lt;p&gt;Since its debut back in October 2024, Agentforce has been racking up some wins across a variety of sectors. Engine managed to slash customer case handling times by 15 percent, while 1-800Accountant handed off 70 percent of administrative chat queries to AI during the madness of tax season.&lt;/p&gt;&lt;p&gt;But what’s interesting about this upgrade isn’t just the numbers, it’s how Salesforce is addressing the elephant in the room that nobody likes to talk about: businesses are deploying AI agents at breakneck speed without really understanding what they’re doing or how to improve them.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-keeping-tabs-on-your-agents"&gt;Keeping tabs on your agents&lt;/h3&gt;&lt;p&gt;The centrepiece of Agentforce 3 is what Salesforce calls the Command Center (essentially a mission control for your AI employees.) It lets managers peek under the bonnet to spot patterns in how agents are performing, track health metrics in real-time (latency, escalation rates, errors), and identify which bits are working versus which need a swift kick.&lt;/p&gt;&lt;p&gt;For anyone who’s ever deployed AI tools and then wondered “now what?” this level of visibility could be game-changing. The system captures all agent activity using the OpenTelemetry standard, which means it plays nicely with tools like Datadog and Splunk that your IT team probably already has on their screens.&lt;/p&gt;&lt;p&gt;AI adoption is absolutely skyrocketing. Forthcoming data from the Slack Workflow Index shows AI agent usage up 233 percent in just six months. During that time, about 8,000 organisations signed up to deploy Agentforce.&lt;/p&gt;&lt;p&gt;Ryan Teeples, CTO at 1-800Accountant, said: “Agentforce autonomously resolved 70% of 1-800Accountant’s administrative chat engagements during the peak of this past tax season, an incredible lift during one of our busiest periods. But that early success was just the beginning.&lt;/p&gt;&lt;p&gt;“We’ve established a strong deployment foundation and weekly are focused on launching new agentic experiences and AI automations through Agentforce’s newest capabilities. With a high level of observability, we can see what’s working, optimise in real time, and scale support with confidence.”&lt;/p&gt;&lt;p&gt;Salesforce Agentforce 3 doesn’t just provide data, it actually suggests improvements. The AI effectively watches itself, identifying conversation patterns and recommending tweaks. It’s a bit meta, but potentially very useful for overstretched teams who don’t have time to manually review thousands of bot interactions.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-connectivity-conundrum-solved"&gt;The connectivity conundrum solved?&lt;/h3&gt;&lt;p&gt;Another headache Salesforce is tackling is connectivity. AI agents are only as useful as the systems they can access, but connecting them securely to your business tools has been a pain for most organisations.&lt;/p&gt;&lt;p&gt;Agentforce 3 brings native support for Model Context Protocol (MCP) – which Salesforce rather aptly describes as “USB-C for AI.” This essentially means AI agents can plug into any MCP-compliant server without custom coding, while still respecting your security policies.&lt;/p&gt;&lt;p&gt;This is where MuleSoft (which Salesforce acquired a few years back) comes into play, converting APIs and integrations into agent-ready assets. Heroku then handles deployment and maintenance of custom MCP servers.&lt;/p&gt;&lt;p&gt;Mollie Bodensteiner, SVP of Operations at Engine, commented: “Salesforce’s open ecosystem approach, especially through its native support for open standards like MCP, will be instrumental in helping us scale our use of AI agents with full confidence.&lt;/p&gt;&lt;p&gt;“We’ll be able to securely connect agents to the enterprise systems we rely on without custom code or compromising governance. That level of interoperability has given us the flexibility to accelerate adoption while staying in complete control of how agents operate within our environment.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-growing-the-salesforce-agentforce-ecosystem"&gt;Growing the Salesforce Agentforce ecosystem&lt;/h3&gt;&lt;p&gt;Perhaps the most interesting aspect of this announcement isn’t what Salesforce built themselves, but the ecosystem they’re nurturing. Over 30 partners have created MCP servers that integrate with Agentforce, including players like AWS, Google Cloud, Box, PayPal, and Stripe.&lt;/p&gt;&lt;p&gt;These integrations go far beyond simple data access. For instance, AWS integration lets agents analyse documents, extract information from images, transcribe audio recordings, and even identify important moments in videos. Google Cloud connections tie into Maps, databases, and AI models like Veo and Imagen.&lt;/p&gt;&lt;p&gt;Healthcare appears to be a particularly promising sector.&lt;/p&gt;&lt;p&gt;Tyler Bauer, VP for System Ambulatory Operations at UChicago Medicine, explains: “AI tools in healthcare must be adaptable to the complex and highly individualised needs of both patients and care teams.&lt;/p&gt;&lt;p&gt;“We need to support that goal by automating routine interactions in our patient access center that involve common questions and requests, which would free up the team’s time to focus on sensitive, more involved, or complex needs.”&lt;/p&gt;&lt;p&gt;The real question, of course, is whether all this will actually help businesses manage the growing army of AI agents they’re deploying. Getting visibility into AI performance has been a blind spot for many organisations—they often know roughly what percentage of queries the AI is handling, but struggle to identify specific shortcomings or improvement opportunities.&lt;/p&gt;&lt;p&gt;Adam Evans, EVP &amp;amp; GM of Salesforce AI, says: “Agentforce 3 will redefine how humans and AI agents work together—driving breakthrough levels of productivity, efficiency, and business transformation.”&lt;/p&gt;&lt;p&gt;Whether it lives up to that lofty promise remains to be seen, but addressing the visibility and control gap is certainly a step in the right direction for businesses struggling to properly manage their AI initiatives.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Huawei HarmonyOS 6 AI agents offer alternative to Android and iOS&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/salesforce-agentforce-3-brings-visibility-ai-agents/</guid><pubDate>Tue, 24 Jun 2025 17:10:37 +0000</pubDate></item><item><title>The résumé is dying, and AI is holding the smoking gun (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/06/the-resume-is-dying-and-ai-is-holding-the-smoking-gun/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        As thousands of applications flood job posts, 'hiring slop' is kicking off an AI arms race.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A businessman is trapped in his glass office by a surplus of discarded ideas on paper . His colleague in the next office is working more efficiently and is oblivious to him being trapped , as is a passing female office worker" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/buried_in_papers_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A businessman is trapped in his glass office by a surplus of discarded ideas on paper . His colleague in the next office is working more efficiently and is oblivious to him being trapped , as is a passing female office worker" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/buried_in_papers_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          sturti via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Employers are drowning in AI-generated job applications, with LinkedIn now processing 11,000 submissions per minute—a 45 percent surge from last year, according to new data reported by The New York Times.&lt;/p&gt;
&lt;p&gt;Due to AI, the traditional hiring process has become overwhelmed with automated noise. It's the résumé equivalent of AI slop—call it "hiring slop," perhaps—that currently haunts social media and the web with sensational pictures and misleading information. The flood of ChatGPT-crafted résumés and bot-submitted applications has created an arms race between job seekers and employers, with both sides deploying increasingly sophisticated AI tools in a bot-versus-bot standoff that is quickly spiraling out of control.&lt;/p&gt;
&lt;p&gt;The Times illustrates the scale of the problem with the story of an HR consultant named Katie Tanner, who was so inundated with over 1,200 applications for a single remote role that she had to remove the post entirely and was still sorting through the applications three months later.&lt;/p&gt;
&lt;p&gt;In an age where ChatGPT can insert every keyword from a job description into a résumé with a simple prompt, her story is not unique. The problem began shortly after the emergence of mainstream generative AI bots in 2022, when some companies applied the technology to job applications to help overwhelmed job seekers. Now, several years later, the technology has evolved from a convenience tool to a systemic disruption of the hiring process.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-1980193 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Illustration of robot hands using a typewriter." class="center large" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/ai_typewriter_getty-640x360.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Some candidates are now taking automation even further, paying for AI agents that autonomously find jobs and submit applications on their behalf. Recruiters report that many of the résumés look suspiciously similar, making it more difficult to identify genuinely qualified or interested candidates.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Computer tools have been assisting with creating résumés for decades, and everything from the typewriter to word processors to spellcheck and résumé templates have increased the ease of making a competent résumé. But AI has pushed the trend into overdrive. The potential to create endless output makes AI fundamentally different from its predecessors. Whereas earlier technologies helped people craft one good résumé more efficiently, AI enables candidates to generate hundreds of customized applications with minimal effort—turning what was once a time-intensive process of demonstrating interest into a numbers game that overwhelms businesses trying to find genuinely qualified applicants.&lt;/p&gt;
&lt;p&gt;The frustration has reached a point where AI companies themselves are backing away from their own technology during the hiring process. Anthropic recently advised job seekers not to use LLMs on their applications—a striking admission from a company whose business model depends on people using AI for everything else.&lt;/p&gt;
&lt;h2&gt;The slow, painful death of the résumé&lt;/h2&gt;
&lt;p&gt;In response to the deluge, companies now deploy their own AI defenses. Chipotle's AI chatbot screening tool, nicknamed Ava Cado, has reportedly reduced hiring time by 75 percent. However, this trend from businesses has led to an arms race of escalating automation, with candidates using AI to generate interview answers while companies deploy AI to detect them—creating what amounts to machines talking to machines while humans get lost in the shuffle.&lt;/p&gt;
&lt;p&gt;Ironically, LinkedIn has stepped into the middle of the crisis by providing even more AI, with new tools that aim to help both candidates and recruiters narrow their focus. For example, an AI agent launched late last year can write follow-up messages, conduct screening chats, suggest top applicants, and search for potential hires using natural language on the platform.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Beyond volume, fraud poses an increasing threat. In January, the Justice Department announced indictments in a scheme to place North Korean nationals in remote IT roles at US companies. Research firm Gartner says that fake identity cases are growing rapidly, with the company estimating that by 2028, about 1 in 4 job applicants could be fraudulent. And as we have previously reported, security researchers have also discovered that AI systems can hide invisible text in applications, potentially allowing candidates to game screening systems using prompt injections in ways human reviewers can't detect.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2043141 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Illustration of a robot generating endless text, controlled by a scientist." class="center large" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_science_header-640x360.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;And that's not all. Even when AI screening tools work as intended, they exhibit similar biases to human recruiters, preferring white male names on résumés—raising legal concerns about discrimination. The European Union's AI Act already classifies hiring under its high-risk category with stringent restrictions. Although no US federal law specifically addresses AI use in hiring, general anti-discrimination laws still apply.&lt;/p&gt;
&lt;p&gt;So perhaps résumés as a meaningful signal of candidate interest and qualification are becoming obsolete. And maybe that's OK. When anyone can generate hundreds of tailored applications with a few prompts, the document that once demonstrated effort and genuine interest in a position has devolved into noise.&lt;/p&gt;
&lt;p&gt;Instead, the future of hiring may require abandoning the résumé altogether in favor of methods that AI can't easily replicate—live problem-solving sessions, portfolio reviews, or trial work periods, just to name a few ideas people sometimes consider (whether they are good ideas or not is beyond the scope of this piece). For now, employers and job seekers remain locked in an escalating technological arms race where machines screen the output of other machines, while the humans they're meant to serve struggle to make authentic connections in an increasingly inauthentic world.&lt;/p&gt;
&lt;p&gt;Perhaps the endgame is robots interviewing other robots for jobs performed by robots, while humans sit on the beach drinking daiquiris and playing vintage video games. Well, one can dream.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        As thousands of applications flood job posts, 'hiring slop' is kicking off an AI arms race.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A businessman is trapped in his glass office by a surplus of discarded ideas on paper . His colleague in the next office is working more efficiently and is oblivious to him being trapped , as is a passing female office worker" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/buried_in_papers_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A businessman is trapped in his glass office by a surplus of discarded ideas on paper . His colleague in the next office is working more efficiently and is oblivious to him being trapped , as is a passing female office worker" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/buried_in_papers_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          sturti via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Employers are drowning in AI-generated job applications, with LinkedIn now processing 11,000 submissions per minute—a 45 percent surge from last year, according to new data reported by The New York Times.&lt;/p&gt;
&lt;p&gt;Due to AI, the traditional hiring process has become overwhelmed with automated noise. It's the résumé equivalent of AI slop—call it "hiring slop," perhaps—that currently haunts social media and the web with sensational pictures and misleading information. The flood of ChatGPT-crafted résumés and bot-submitted applications has created an arms race between job seekers and employers, with both sides deploying increasingly sophisticated AI tools in a bot-versus-bot standoff that is quickly spiraling out of control.&lt;/p&gt;
&lt;p&gt;The Times illustrates the scale of the problem with the story of an HR consultant named Katie Tanner, who was so inundated with over 1,200 applications for a single remote role that she had to remove the post entirely and was still sorting through the applications three months later.&lt;/p&gt;
&lt;p&gt;In an age where ChatGPT can insert every keyword from a job description into a résumé with a simple prompt, her story is not unique. The problem began shortly after the emergence of mainstream generative AI bots in 2022, when some companies applied the technology to job applications to help overwhelmed job seekers. Now, several years later, the technology has evolved from a convenience tool to a systemic disruption of the hiring process.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-1980193 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Illustration of robot hands using a typewriter." class="center large" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/ai_typewriter_getty-640x360.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Some candidates are now taking automation even further, paying for AI agents that autonomously find jobs and submit applications on their behalf. Recruiters report that many of the résumés look suspiciously similar, making it more difficult to identify genuinely qualified or interested candidates.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Computer tools have been assisting with creating résumés for decades, and everything from the typewriter to word processors to spellcheck and résumé templates have increased the ease of making a competent résumé. But AI has pushed the trend into overdrive. The potential to create endless output makes AI fundamentally different from its predecessors. Whereas earlier technologies helped people craft one good résumé more efficiently, AI enables candidates to generate hundreds of customized applications with minimal effort—turning what was once a time-intensive process of demonstrating interest into a numbers game that overwhelms businesses trying to find genuinely qualified applicants.&lt;/p&gt;
&lt;p&gt;The frustration has reached a point where AI companies themselves are backing away from their own technology during the hiring process. Anthropic recently advised job seekers not to use LLMs on their applications—a striking admission from a company whose business model depends on people using AI for everything else.&lt;/p&gt;
&lt;h2&gt;The slow, painful death of the résumé&lt;/h2&gt;
&lt;p&gt;In response to the deluge, companies now deploy their own AI defenses. Chipotle's AI chatbot screening tool, nicknamed Ava Cado, has reportedly reduced hiring time by 75 percent. However, this trend from businesses has led to an arms race of escalating automation, with candidates using AI to generate interview answers while companies deploy AI to detect them—creating what amounts to machines talking to machines while humans get lost in the shuffle.&lt;/p&gt;
&lt;p&gt;Ironically, LinkedIn has stepped into the middle of the crisis by providing even more AI, with new tools that aim to help both candidates and recruiters narrow their focus. For example, an AI agent launched late last year can write follow-up messages, conduct screening chats, suggest top applicants, and search for potential hires using natural language on the platform.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Beyond volume, fraud poses an increasing threat. In January, the Justice Department announced indictments in a scheme to place North Korean nationals in remote IT roles at US companies. Research firm Gartner says that fake identity cases are growing rapidly, with the company estimating that by 2028, about 1 in 4 job applicants could be fraudulent. And as we have previously reported, security researchers have also discovered that AI systems can hide invisible text in applications, potentially allowing candidates to game screening systems using prompt injections in ways human reviewers can't detect.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2043141 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Illustration of a robot generating endless text, controlled by a scientist." class="center large" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_science_header-640x360.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;And that's not all. Even when AI screening tools work as intended, they exhibit similar biases to human recruiters, preferring white male names on résumés—raising legal concerns about discrimination. The European Union's AI Act already classifies hiring under its high-risk category with stringent restrictions. Although no US federal law specifically addresses AI use in hiring, general anti-discrimination laws still apply.&lt;/p&gt;
&lt;p&gt;So perhaps résumés as a meaningful signal of candidate interest and qualification are becoming obsolete. And maybe that's OK. When anyone can generate hundreds of tailored applications with a few prompts, the document that once demonstrated effort and genuine interest in a position has devolved into noise.&lt;/p&gt;
&lt;p&gt;Instead, the future of hiring may require abandoning the résumé altogether in favor of methods that AI can't easily replicate—live problem-solving sessions, portfolio reviews, or trial work periods, just to name a few ideas people sometimes consider (whether they are good ideas or not is beyond the scope of this piece). For now, employers and job seekers remain locked in an escalating technological arms race where machines screen the output of other machines, while the humans they're meant to serve struggle to make authentic connections in an increasingly inauthentic world.&lt;/p&gt;
&lt;p&gt;Perhaps the endgame is robots interviewing other robots for jobs performed by robots, while humans sit on the beach drinking daiquiris and playing vintage video games. Well, one can dream.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/06/the-resume-is-dying-and-ai-is-holding-the-smoking-gun/</guid><pubDate>Tue, 24 Jun 2025 17:25:41 +0000</pubDate></item><item><title>[NEW] Key fair use ruling clarifies when books can be used for AI training (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/06/key-fair-use-ruling-clarifies-when-books-can-be-used-for-ai-training/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        In landmark ruling, judge likens AI training to schoolchildren learning to write.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-502074710-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-502074710-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          CookiesForDevo | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Artificial intelligence companies don't need permission from authors to train their large language models (LLMs) on legally acquired books, US District Judge William Alsup ruled Monday.&lt;/p&gt;
&lt;p&gt;The first-of-its-kind ruling that condones AI training as fair use will likely be viewed as a big win for AI companies, but it also notably put on notice all the AI companies that expect the same reasoning will apply to training on pirated copies of books—a question that remains unsettled.&lt;/p&gt;
&lt;p&gt;In the specific case that Alsup is weighing—which pits book authors against Anthropic—Alsup found that "the purpose and character of using copyrighted works to train LLMs to generate new text was quintessentially transformative" and "necessary" to build world-class AI models.&lt;/p&gt;
&lt;p&gt;Importantly, this case differs from other lawsuits where authors allege that AI models risk copying and distributing their work. Because authors suing Anthropic did not allege that any of Anthropic's outputs reproduced their works or expressive style, Alsup found there was no threat that Anthropic's text generator, Claude, might replace authors in their markets. And that lacking argument did tip the fair use analysis in favor of Anthropic.&lt;/p&gt;
&lt;p&gt;"Like any reader aspiring to be a writer, Anthropic’s LLMs trained upon works not to race ahead and replicate or supplant them—but to turn a hard corner and create something different," Alsup wrote.&lt;/p&gt;
&lt;p&gt;Alsup's ruling surely disappointed authors, who instead argued that Claude's reliance on their texts could generate competing summaries or alternative versions of their stories. The judge claimed these complaints were akin to arguing "that training schoolchildren to write well would result in an explosion of competing works."&lt;/p&gt;
&lt;p&gt;"This is not the kind of competitive or creative displacement that concerns the Copyright Act," Alsup wrote. "The Act seeks to advance original works of authorship, not to protect authors against competition."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Alsup noted that authors would be able to raise new claims if they found evidence of infringing Claude outputs. That could change the fair use calculus, as it might in a case where a judge recently suggested that Meta's AI products might be "obliterating" authors' markets for works.&lt;/p&gt;
&lt;p&gt;"Authors concede that training LLMs did not result in any exact copies nor even infringing knockoffs of their works being provided to the public," Alsup wrote. "If that were not so, this would be a different case. Authors remain free to bring that case in the future should such facts develop."&lt;/p&gt;
&lt;h2&gt;Anthropic must face trial over book piracy&lt;/h2&gt;
&lt;p&gt;Anthropic is "pleased" with the ruling, issuing a statement applauding the court for recognizing "that using ‘works to train LLMs was transformative—spectacularly so.'"&lt;/p&gt;
&lt;p&gt;But Anthropic is not off the hook, granted summary judgment on AI training as fair use, but is still facing a trial over piracy that Alsup ruled did not favor a fair use finding.&lt;/p&gt;
&lt;p&gt;In the Anthropic case, the AI company is accused of downloading 7 million pirated books to build a research library where copies would be kept "forever" regardless of whether they were ever used for AI training.&lt;/p&gt;
&lt;p&gt;Seemingly realizing piracy may trigger legal challenges, Anthropic had later tried to replace pirated books with legally purchased copies. But the company also argued that even the initial copying of these pirated books was an "intermediary" step necessary to advance the transformative use of training AI. And perhaps at its least persuasive, Anthropic also argued that because it could have borrowed the books it stole, the theft alone shouldn't "short-circuit" the fair use analysis.&lt;/p&gt;
&lt;p&gt;But Alsup was not swayed by those arguments, noting that copying books from a pirate site is copyright infringement, "full stop." He rejected "Anthropic’s assumption that the use of the copies for a central library can be excused as fair use merely because some will eventually be used to train LLMs," and he cast doubt on whether any of the other AI lawsuits debating piracy could ever escape without paying damages.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"This order doubts that any accused infringer could ever meet its burden of explaining why downloading source copies from pirate sites that it could have purchased or otherwise accessed lawfully was itself reasonably necessary to any subsequent fair use," Alsup wrote. "Such piracy of otherwise available copies is inherently, irredeemably infringing even if the pirated copies are immediately used for the transformative use and immediately discarded."&lt;/p&gt;
&lt;p&gt;But Alsup said that the Anthropic case may not even need to decide on that, since Anthropic's retention of pirated books for its research library alone was not transformative. Alsup wrote that Anthropic's argument to hold onto potential AI training material it pirated in case it ever decided to use it for AI training was an attempt to "fast glide over thin ice."&lt;/p&gt;
&lt;p&gt;Additionally Alsup pointed out that Anthropic's early attempts to get permission to train on authors' works withered, as internal messages revealed the company concluded that stealing books was considered the more cost-effective path to innovation "to avoid 'legal/practice/business slog,' as cofounder and chief executive officer Dario Amodei put it."&lt;/p&gt;
&lt;p&gt;"Anthropic is wrong to suppose that so long as you create an exciting end product, every 'back-end step, invisible to the public,' is excused," Alsup wrote. "Here, piracy was the point: To build a central library that one could have paid for, just as Anthropic later did, but without paying for it."&lt;/p&gt;
&lt;p&gt;To avoid maximum damages in the event of a loss, Anthropic will likely continue arguing that replacing pirated books with purchased books should water down authors' fight, Alsup's order suggested.&lt;/p&gt;
&lt;p&gt;"That Anthropic later bought a copy of a book it earlier stole off the Internet will not absolve it of liability for the theft, but it may affect the extent of statutory damages," Alsup noted.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        In landmark ruling, judge likens AI training to schoolchildren learning to write.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-502074710-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-502074710-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          CookiesForDevo | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Artificial intelligence companies don't need permission from authors to train their large language models (LLMs) on legally acquired books, US District Judge William Alsup ruled Monday.&lt;/p&gt;
&lt;p&gt;The first-of-its-kind ruling that condones AI training as fair use will likely be viewed as a big win for AI companies, but it also notably put on notice all the AI companies that expect the same reasoning will apply to training on pirated copies of books—a question that remains unsettled.&lt;/p&gt;
&lt;p&gt;In the specific case that Alsup is weighing—which pits book authors against Anthropic—Alsup found that "the purpose and character of using copyrighted works to train LLMs to generate new text was quintessentially transformative" and "necessary" to build world-class AI models.&lt;/p&gt;
&lt;p&gt;Importantly, this case differs from other lawsuits where authors allege that AI models risk copying and distributing their work. Because authors suing Anthropic did not allege that any of Anthropic's outputs reproduced their works or expressive style, Alsup found there was no threat that Anthropic's text generator, Claude, might replace authors in their markets. And that lacking argument did tip the fair use analysis in favor of Anthropic.&lt;/p&gt;
&lt;p&gt;"Like any reader aspiring to be a writer, Anthropic’s LLMs trained upon works not to race ahead and replicate or supplant them—but to turn a hard corner and create something different," Alsup wrote.&lt;/p&gt;
&lt;p&gt;Alsup's ruling surely disappointed authors, who instead argued that Claude's reliance on their texts could generate competing summaries or alternative versions of their stories. The judge claimed these complaints were akin to arguing "that training schoolchildren to write well would result in an explosion of competing works."&lt;/p&gt;
&lt;p&gt;"This is not the kind of competitive or creative displacement that concerns the Copyright Act," Alsup wrote. "The Act seeks to advance original works of authorship, not to protect authors against competition."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Alsup noted that authors would be able to raise new claims if they found evidence of infringing Claude outputs. That could change the fair use calculus, as it might in a case where a judge recently suggested that Meta's AI products might be "obliterating" authors' markets for works.&lt;/p&gt;
&lt;p&gt;"Authors concede that training LLMs did not result in any exact copies nor even infringing knockoffs of their works being provided to the public," Alsup wrote. "If that were not so, this would be a different case. Authors remain free to bring that case in the future should such facts develop."&lt;/p&gt;
&lt;h2&gt;Anthropic must face trial over book piracy&lt;/h2&gt;
&lt;p&gt;Anthropic is "pleased" with the ruling, issuing a statement applauding the court for recognizing "that using ‘works to train LLMs was transformative—spectacularly so.'"&lt;/p&gt;
&lt;p&gt;But Anthropic is not off the hook, granted summary judgment on AI training as fair use, but is still facing a trial over piracy that Alsup ruled did not favor a fair use finding.&lt;/p&gt;
&lt;p&gt;In the Anthropic case, the AI company is accused of downloading 7 million pirated books to build a research library where copies would be kept "forever" regardless of whether they were ever used for AI training.&lt;/p&gt;
&lt;p&gt;Seemingly realizing piracy may trigger legal challenges, Anthropic had later tried to replace pirated books with legally purchased copies. But the company also argued that even the initial copying of these pirated books was an "intermediary" step necessary to advance the transformative use of training AI. And perhaps at its least persuasive, Anthropic also argued that because it could have borrowed the books it stole, the theft alone shouldn't "short-circuit" the fair use analysis.&lt;/p&gt;
&lt;p&gt;But Alsup was not swayed by those arguments, noting that copying books from a pirate site is copyright infringement, "full stop." He rejected "Anthropic’s assumption that the use of the copies for a central library can be excused as fair use merely because some will eventually be used to train LLMs," and he cast doubt on whether any of the other AI lawsuits debating piracy could ever escape without paying damages.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"This order doubts that any accused infringer could ever meet its burden of explaining why downloading source copies from pirate sites that it could have purchased or otherwise accessed lawfully was itself reasonably necessary to any subsequent fair use," Alsup wrote. "Such piracy of otherwise available copies is inherently, irredeemably infringing even if the pirated copies are immediately used for the transformative use and immediately discarded."&lt;/p&gt;
&lt;p&gt;But Alsup said that the Anthropic case may not even need to decide on that, since Anthropic's retention of pirated books for its research library alone was not transformative. Alsup wrote that Anthropic's argument to hold onto potential AI training material it pirated in case it ever decided to use it for AI training was an attempt to "fast glide over thin ice."&lt;/p&gt;
&lt;p&gt;Additionally Alsup pointed out that Anthropic's early attempts to get permission to train on authors' works withered, as internal messages revealed the company concluded that stealing books was considered the more cost-effective path to innovation "to avoid 'legal/practice/business slog,' as cofounder and chief executive officer Dario Amodei put it."&lt;/p&gt;
&lt;p&gt;"Anthropic is wrong to suppose that so long as you create an exciting end product, every 'back-end step, invisible to the public,' is excused," Alsup wrote. "Here, piracy was the point: To build a central library that one could have paid for, just as Anthropic later did, but without paying for it."&lt;/p&gt;
&lt;p&gt;To avoid maximum damages in the event of a loss, Anthropic will likely continue arguing that replacing pirated books with purchased books should water down authors' fight, Alsup's order suggested.&lt;/p&gt;
&lt;p&gt;"That Anthropic later bought a copy of a book it earlier stole off the Internet will not absolve it of liability for the theft, but it may affect the extent of statutory damages," Alsup noted.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/06/key-fair-use-ruling-clarifies-when-books-can-be-used-for-ai-training/</guid><pubDate>Tue, 24 Jun 2025 19:56:58 +0000</pubDate></item><item><title>[NEW] In just 4 months, AI medical scribe  Abridge doubles valuation to $5.3B (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/24/in-just-4-months-ai-medical-scribe-abridge-doubles-valuation-to-5-3b/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/abridge-team-shiv-colour-1-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Abridge, an AI startup automating medical notes, has secured a $300 million Series E at a $5.3 billion valuation, according to the Wall Street Journal. The round, led by Andreessen Horowitz with participation from Khosla Ventures, follows the company’s $250 million February fundraise at a $2.75 billion valuation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 7-year-old Abridge is widely considered to be the leader in the increasingly crowded AI-powered medical scribe market, largely due to its early entry&lt;strong&gt; &lt;/strong&gt;and integration with Epic Systems, the dominant health record software.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In Q1, Abridge reached $117 million in contracted annual recurring revenue (a metric that includes all signed recurring contacts, including from customers that have still not been onboarded), The Information reported last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Along with the fundraise, Abridge announced that it is expanding into converting medical notes from patient appointments into AI-powered medical codes, an offering that makes the company directly competitive with startups like CodaMetrix and a feature from its partner Epic Systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abridge, which was founded by cardiologist Shiv Rao, claims that its AI scribe technology is used by over 150 of the largest health systems in the U.S.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/abridge-team-shiv-colour-1-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Abridge, an AI startup automating medical notes, has secured a $300 million Series E at a $5.3 billion valuation, according to the Wall Street Journal. The round, led by Andreessen Horowitz with participation from Khosla Ventures, follows the company’s $250 million February fundraise at a $2.75 billion valuation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 7-year-old Abridge is widely considered to be the leader in the increasingly crowded AI-powered medical scribe market, largely due to its early entry&lt;strong&gt; &lt;/strong&gt;and integration with Epic Systems, the dominant health record software.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In Q1, Abridge reached $117 million in contracted annual recurring revenue (a metric that includes all signed recurring contacts, including from customers that have still not been onboarded), The Information reported last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Along with the fundraise, Abridge announced that it is expanding into converting medical notes from patient appointments into AI-powered medical codes, an offering that makes the company directly competitive with startups like CodaMetrix and a feature from its partner Epic Systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abridge, which was founded by cardiologist Shiv Rao, claims that its AI scribe technology is used by over 150 of the largest health systems in the U.S.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/24/in-just-4-months-ai-medical-scribe-abridge-doubles-valuation-to-5-3b/</guid><pubDate>Tue, 24 Jun 2025 20:16:46 +0000</pubDate></item><item><title>[NEW] ‘Sandbox first’: Andrew Ng’s blueprint for accelerating enterprise AI innovation (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/sandbox-first-andrew-ngs-blueprint-for-accelerating-enterprise-ai-innovation/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Enterprises may be concerned about the impact of AI applications when put into production, but hampering these projects with guardrails at the onset could slow innovation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Andrew Ng, founder of DeepLearning AI and one of the most prominent figures in AI development, emphasized the importance of observability and guardrails in AI development during a fireside chat at VB Transform today. However, he added that these should not come at the cost of innovation and growth.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Ng suggested that enterprises build within sandboxes to prototype projects quickly, find the pilots that work, and start investing in observability and guardrails for these applications after they have proven to work.&amp;nbsp;This may seem counterintuitive to enterprises looking to implement AI.&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;p&gt;“There is an important role for observability, safety and guardrails,” Ng said. “I frankly tend to put those in later because I find that one of the ways that large businesses grind to a halt is that for engineers to try anything, they have to get sign off by five vice presidents.”&lt;/p&gt;



&lt;p&gt;He added that big businesses “can’t afford to have some random innovation team ship something that damages the brand or has sensitive information,” but this can also hamper innovation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Instead, Ng said sandboxes offer a way for developer teams to “iterate really quickly with limited private information.” The sandboxes allow the organization to invest only in projects that work and then add the technology to make them responsible, including observability tools and guardrails.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It is not uncommon for enterprises to establish innovation sandboxes, particularly for AI agents. Sandboxes allow for innovation within the confines of the enterprises without touching any sensitive information they don’t want to be public. Yet, they also allow teams to be as creative as they can to test out ideas.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Observability is rapidly becoming a key topic as many AI applications and agents enter production. Salesforce recently updated its agent library, Agentforce 3, to provide enhanced visibility into agent performance and further support for interoperability standards, such as MCP.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-speed-and-lower-pilot-costs"&gt;Speed and lower pilot costs&lt;/h2&gt;



&lt;p&gt;For Ng, speed and innovation go hand in hand, and enterprises shouldn’t be afraid of it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Imagine that we’ve been on a roller coaster, but this is a slow-moving roller coaster. What’s happened in the last year, our roller coaster just picked up a lot of speed, and this is really exciting because it’s moving forward,” Ng said. “I feel like the world is now on a very fast-moving roller coaster, and it’s great.”&lt;/p&gt;



&lt;p&gt;Ng said one factor that’s contributed to this speed is the tools now available for developers to work and ideate quickly, pointing out that coding agents like Windsurf and GitHub Copilot have cut down development time “of projects that used to take me three months and six engineers.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These coding agent platforms and other tools that help developers move faster have also meant the cost of doing pilot projects.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“I don’t feel like the cost of a proof of concept going so low that I’m fine to do a lot of POCs (proofs of concept) is bad,” he said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-barrier"&gt;A barrier&lt;/h2&gt;



&lt;p&gt;One barrier, though, may be finding the talent. Ng acknowledged that there are AI companies recruiting foundation model engineers with salary ranges of up to $10 million, but the price isn’t that high for software engineers.&lt;/p&gt;



&lt;p&gt;“One of the biggest challenges for many businesses is talent,” he said. “The good news for companies looking for engineers able to build applications, the price is nowhere near the $5 million range,” he said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The problem, though, is that there is still not enough talent out there who are experienced in building AI projects for enterprises. So, Ng goes back to his first solution: Let them experiment in sandboxes and gain that experience.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Enterprises may be concerned about the impact of AI applications when put into production, but hampering these projects with guardrails at the onset could slow innovation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Andrew Ng, founder of DeepLearning AI and one of the most prominent figures in AI development, emphasized the importance of observability and guardrails in AI development during a fireside chat at VB Transform today. However, he added that these should not come at the cost of innovation and growth.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Ng suggested that enterprises build within sandboxes to prototype projects quickly, find the pilots that work, and start investing in observability and guardrails for these applications after they have proven to work.&amp;nbsp;This may seem counterintuitive to enterprises looking to implement AI.&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;p&gt;“There is an important role for observability, safety and guardrails,” Ng said. “I frankly tend to put those in later because I find that one of the ways that large businesses grind to a halt is that for engineers to try anything, they have to get sign off by five vice presidents.”&lt;/p&gt;



&lt;p&gt;He added that big businesses “can’t afford to have some random innovation team ship something that damages the brand or has sensitive information,” but this can also hamper innovation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Instead, Ng said sandboxes offer a way for developer teams to “iterate really quickly with limited private information.” The sandboxes allow the organization to invest only in projects that work and then add the technology to make them responsible, including observability tools and guardrails.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It is not uncommon for enterprises to establish innovation sandboxes, particularly for AI agents. Sandboxes allow for innovation within the confines of the enterprises without touching any sensitive information they don’t want to be public. Yet, they also allow teams to be as creative as they can to test out ideas.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Observability is rapidly becoming a key topic as many AI applications and agents enter production. Salesforce recently updated its agent library, Agentforce 3, to provide enhanced visibility into agent performance and further support for interoperability standards, such as MCP.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-speed-and-lower-pilot-costs"&gt;Speed and lower pilot costs&lt;/h2&gt;



&lt;p&gt;For Ng, speed and innovation go hand in hand, and enterprises shouldn’t be afraid of it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Imagine that we’ve been on a roller coaster, but this is a slow-moving roller coaster. What’s happened in the last year, our roller coaster just picked up a lot of speed, and this is really exciting because it’s moving forward,” Ng said. “I feel like the world is now on a very fast-moving roller coaster, and it’s great.”&lt;/p&gt;



&lt;p&gt;Ng said one factor that’s contributed to this speed is the tools now available for developers to work and ideate quickly, pointing out that coding agents like Windsurf and GitHub Copilot have cut down development time “of projects that used to take me three months and six engineers.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These coding agent platforms and other tools that help developers move faster have also meant the cost of doing pilot projects.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“I don’t feel like the cost of a proof of concept going so low that I’m fine to do a lot of POCs (proofs of concept) is bad,” he said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-barrier"&gt;A barrier&lt;/h2&gt;



&lt;p&gt;One barrier, though, may be finding the talent. Ng acknowledged that there are AI companies recruiting foundation model engineers with salary ranges of up to $10 million, but the price isn’t that high for software engineers.&lt;/p&gt;



&lt;p&gt;“One of the biggest challenges for many businesses is talent,” he said. “The good news for companies looking for engineers able to build applications, the price is nowhere near the $5 million range,” he said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The problem, though, is that there is still not enough talent out there who are experienced in building AI projects for enterprises. So, Ng goes back to his first solution: Let them experiment in sandboxes and gain that experience.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/sandbox-first-andrew-ngs-blueprint-for-accelerating-enterprise-ai-innovation/</guid><pubDate>Tue, 24 Jun 2025 20:38:49 +0000</pubDate></item><item><title>[NEW] Crop signals (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117747/crop-signals/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MIT-Bacterial-Sentinel-01-press.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Bacteria can be engineered to sense a variety of molecules, such as pollutants or soil nutrients, but usually these signals must be detected microscopically. Now Christopher Voigt, head of MIT’s Department of Bio­logical Engineering, and colleagues have triggered bacterial cells to produce signals that can be read from as far as 90 meters away. Their work could lead to the development of sensors for agricultural and other applications, which could be monitored by drones or satellites.&lt;/p&gt;  &lt;p&gt;The researchers engineered two different types of bacteria, one found in soil and one in water, so that when they encounter certain target chemicals, they produce hyperspectral reporters—molecules that absorb distinctive wavelengths of light across the visible and infrared spectra. These signatures can be detected with hyperspectral cameras, which determine how much of each color wavelength is present in any given pixel. Though the reporting molecules they developed were linked to genetic circuits that detect nearby bacteria, this approach could also be combined with sensors detecting radiation, soil nutrients, or arsenic and other contaminants.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The nice thing about this technology is that you can plug and play whichever sensor you want,” says Yonatan Chemla, an MIT postdoc who is a lead author of a paper on the work along with Itai Levin, PhD ’24. “There is no reason that any sensor would not be compatible with this technology.” The work is being commercialized through Fieldstone Bio.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MIT-Bacterial-Sentinel-01-press.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Bacteria can be engineered to sense a variety of molecules, such as pollutants or soil nutrients, but usually these signals must be detected microscopically. Now Christopher Voigt, head of MIT’s Department of Bio­logical Engineering, and colleagues have triggered bacterial cells to produce signals that can be read from as far as 90 meters away. Their work could lead to the development of sensors for agricultural and other applications, which could be monitored by drones or satellites.&lt;/p&gt;  &lt;p&gt;The researchers engineered two different types of bacteria, one found in soil and one in water, so that when they encounter certain target chemicals, they produce hyperspectral reporters—molecules that absorb distinctive wavelengths of light across the visible and infrared spectra. These signatures can be detected with hyperspectral cameras, which determine how much of each color wavelength is present in any given pixel. Though the reporting molecules they developed were linked to genetic circuits that detect nearby bacteria, this approach could also be combined with sensors detecting radiation, soil nutrients, or arsenic and other contaminants.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The nice thing about this technology is that you can plug and play whichever sensor you want,” says Yonatan Chemla, an MIT postdoc who is a lead author of a paper on the work along with Itai Levin, PhD ’24. “There is no reason that any sensor would not be compatible with this technology.” The work is being commercialized through Fieldstone Bio.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117747/crop-signals/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Immune molecules may affect mood (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117745/immune-molecules-may-affect-mood/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MIT-Cytokine-Behavior-01-press.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Two new studies from MIT and Harvard Medical School add to a growing body of evidence that infection-fighting molecules called cytokines also influence the brain, leading to behavioral changes during illness.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;By mapping the locations in the brain of receptors for different forms of IL-17, the researchers found that the cytokine acts on the somatosensory cortex to promote sociable behavior and on the amygdala to elicit anxiety. These findings suggest that the immune and nervous systems are tightly interconnected, says Gloria Choi, an associate professor of brain and cognitive sciences and one of both studies’ senior authors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“If you’re sick, there’s so many more things that are happening to your internal states, your mood, and your behavioral states, and that’s not simply you being fatigued physically. It has something to do with the brain,” she says.&lt;/p&gt;  &lt;p&gt;In the cortex, the researchers found certain receptors in a population of neurons that, when overactivated, can lead to autism-like symptoms such as reduced sociability in mice. But the researchers determined that the neurons become less excitable when a specific form of IL-17 binds to the receptors, shedding possible light on why autism symptoms in children often abate when they have fevers. Choi hypothesizes that IL-17 may have evolved as a neuromodulator and was “hijacked” by the immune system only later.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, the researchers also found two types of IL-17 receptors in a certain population of neurons in the amygdala, which plays an important role in processing emotions. When these receptors bind to two forms of IL-17, the neurons become more excitable, leading to an increase in anxiety.&lt;/p&gt;  &lt;p&gt;Eventually, findings like these may help researchers develop new treatments for conditions such as autism and depression.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MIT-Cytokine-Behavior-01-press.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Two new studies from MIT and Harvard Medical School add to a growing body of evidence that infection-fighting molecules called cytokines also influence the brain, leading to behavioral changes during illness.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;By mapping the locations in the brain of receptors for different forms of IL-17, the researchers found that the cytokine acts on the somatosensory cortex to promote sociable behavior and on the amygdala to elicit anxiety. These findings suggest that the immune and nervous systems are tightly interconnected, says Gloria Choi, an associate professor of brain and cognitive sciences and one of both studies’ senior authors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“If you’re sick, there’s so many more things that are happening to your internal states, your mood, and your behavioral states, and that’s not simply you being fatigued physically. It has something to do with the brain,” she says.&lt;/p&gt;  &lt;p&gt;In the cortex, the researchers found certain receptors in a population of neurons that, when overactivated, can lead to autism-like symptoms such as reduced sociability in mice. But the researchers determined that the neurons become less excitable when a specific form of IL-17 binds to the receptors, shedding possible light on why autism symptoms in children often abate when they have fevers. Choi hypothesizes that IL-17 may have evolved as a neuromodulator and was “hijacked” by the immune system only later.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, the researchers also found two types of IL-17 receptors in a certain population of neurons in the amygdala, which plays an important role in processing emotions. When these receptors bind to two forms of IL-17, the neurons become more excitable, leading to an increase in anxiety.&lt;/p&gt;  &lt;p&gt;Eventually, findings like these may help researchers develop new treatments for conditions such as autism and depression.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117745/immune-molecules-may-affect-mood/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Cancer-targeting nanoparticles are moving closer to human trials (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117741/cancer-targeting-nanoparticles-are-moving-closer-to-human-trials/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Over the past decade, Institute Professor Paula Hammond ’84, PhD ’93, and her students have used a technique known as layer-by-layer assembly to create a variety of polymer-coated nanoparticles that can be loaded with cancer-fighting drugs. The particles, which could prevent many side effects of chemotherapy by targeting tumors directly, have proved effective in mouse studies. Now the researchers have come up with a technique that allows them to manufacture many more particles in much less time, moving them closer to human use.&lt;/p&gt;  &lt;p&gt;“There’s a lot of promise with the nanoparticle systems we’ve been developing, and we’ve been really excited more recently with the successes that we’ve been seeing in animal models for our treatments for ovarian cancer in particular,” says Hammond, the senior author of a paper on the new technique along with Darrell Irvine, a professor at the Scripps Research Institute.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;In the original production technique, layers with different properties can be laid down by alternately exposing a particle to positively and negatively charged polymers, with extensive purification to remove excess polymer after each application. Each layer can carry therapeutics as well as molecules that help the particles find and enter cancer cells. But the process is time-consuming and would be difficult to scale up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In the new work, the researchers used a microfluidic mixing device that allows them to sequentially add layers as the particles flow through a microchannel. For each layer, they can calculate exactly how much polymer is needed, which eliminates the slow and costly purification step and saves significantly on material costs.&lt;/p&gt; 
&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="gloved hands hold the device" class="wp-image-1117968" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MIT-Particle-Manufacturing-02-press.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;This microfluidic device can be used to assemble the drug delivery nanoparticles rapidly and in large quantities.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;GRETCHEN ERTL&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;This strategy also facilitates compliance with the FDA’s GMP (good manufacturing practice) requirements, which ensure that products meet safety standards and can be manufactured consistently. “There’s much less chance of any sort of operator mistake or mishaps,” says Ivan Pires, PhD ’24, a postdoc at Brigham and Women’s Hospital and a visiting scientist at the Koch Institute, who is the paper’s lead author along with Ezra Gordon ’24. “We can create an innovation within the layer-by-layer nanoparticles and quickly produce it in a manner that we could go into clinical trials with.”&lt;/p&gt;  &lt;p&gt;In minutes, the researchers can generate 15 milligrams of nanoparticles (enough for about 50 doses for certain cargos), which would have taken close to an hour with the original process. They say this means it would be realistic to produce more than enough for clinical trials and patient use.&lt;/p&gt;  &lt;p&gt;To demonstrate the technique, the researchers created layered nanoparticles loaded with the immune molecule ­interleukin-­12; they have previously shown that such particles can slow growth of ovarian tumors in mice. Those manufactured using the new technique performed similarly to the originals and managed to bind to cancer tissue without entering the cancer cells. This lets them serve as markers that activate the immune system in the tumor, which can delay tumor growth and even lead to cures in mouse models of ovarian cancer.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The researchers have filed for a patent and are working with MIT’s Deshpande Center for Technological Innovation in hopes of forming a company to commercialize the technology, which they say could also be applied to glioblastoma and other types of cancer.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Over the past decade, Institute Professor Paula Hammond ’84, PhD ’93, and her students have used a technique known as layer-by-layer assembly to create a variety of polymer-coated nanoparticles that can be loaded with cancer-fighting drugs. The particles, which could prevent many side effects of chemotherapy by targeting tumors directly, have proved effective in mouse studies. Now the researchers have come up with a technique that allows them to manufacture many more particles in much less time, moving them closer to human use.&lt;/p&gt;  &lt;p&gt;“There’s a lot of promise with the nanoparticle systems we’ve been developing, and we’ve been really excited more recently with the successes that we’ve been seeing in animal models for our treatments for ovarian cancer in particular,” says Hammond, the senior author of a paper on the new technique along with Darrell Irvine, a professor at the Scripps Research Institute.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;In the original production technique, layers with different properties can be laid down by alternately exposing a particle to positively and negatively charged polymers, with extensive purification to remove excess polymer after each application. Each layer can carry therapeutics as well as molecules that help the particles find and enter cancer cells. But the process is time-consuming and would be difficult to scale up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In the new work, the researchers used a microfluidic mixing device that allows them to sequentially add layers as the particles flow through a microchannel. For each layer, they can calculate exactly how much polymer is needed, which eliminates the slow and costly purification step and saves significantly on material costs.&lt;/p&gt; 
&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="gloved hands hold the device" class="wp-image-1117968" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MIT-Particle-Manufacturing-02-press.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;This microfluidic device can be used to assemble the drug delivery nanoparticles rapidly and in large quantities.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;GRETCHEN ERTL&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;This strategy also facilitates compliance with the FDA’s GMP (good manufacturing practice) requirements, which ensure that products meet safety standards and can be manufactured consistently. “There’s much less chance of any sort of operator mistake or mishaps,” says Ivan Pires, PhD ’24, a postdoc at Brigham and Women’s Hospital and a visiting scientist at the Koch Institute, who is the paper’s lead author along with Ezra Gordon ’24. “We can create an innovation within the layer-by-layer nanoparticles and quickly produce it in a manner that we could go into clinical trials with.”&lt;/p&gt;  &lt;p&gt;In minutes, the researchers can generate 15 milligrams of nanoparticles (enough for about 50 doses for certain cargos), which would have taken close to an hour with the original process. They say this means it would be realistic to produce more than enough for clinical trials and patient use.&lt;/p&gt;  &lt;p&gt;To demonstrate the technique, the researchers created layered nanoparticles loaded with the immune molecule ­interleukin-­12; they have previously shown that such particles can slow growth of ovarian tumors in mice. Those manufactured using the new technique performed similarly to the originals and managed to bind to cancer tissue without entering the cancer cells. This lets them serve as markers that activate the immune system in the tumor, which can delay tumor growth and even lead to cures in mouse models of ovarian cancer.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The researchers have filed for a patent and are working with MIT’s Deshpande Center for Technological Innovation in hopes of forming a company to commercialize the technology, which they say could also be applied to glioblastoma and other types of cancer.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117741/cancer-targeting-nanoparticles-are-moving-closer-to-human-trials/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] More news from the labs of MIT (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117738/more-news-from-the-labs-of-mit/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MIT-Meta-Weave-01-PRESS.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;br /&gt;Hundred-year storm tides could strike every decade in Bangladesh&lt;/p&gt;  &lt;p&gt;Tropical cyclones can generate devastating storm tides—seawater heightened by the tides that causes catastrophic floods in coastal regions. An MIT study finds that as the planet warms, the recurrence of destructive storm tides will increase tenfold for one of the world’s hardest-hit regions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/MIT-Meta-Weave-01-PRESS.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;br /&gt;Hundred-year storm tides could strike every decade in Bangladesh&lt;/p&gt;  &lt;p&gt;Tropical cyclones can generate devastating storm tides—seawater heightened by the tides that causes catastrophic floods in coastal regions. An MIT study finds that as the planet warms, the recurrence of destructive storm tides will increase tenfold for one of the world’s hardest-hit regions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117738/more-news-from-the-labs-of-mit/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] An epic year for women’s sports (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117732/an-epic-year-for-womens-sports/</link><description>&lt;p&gt;&lt;br /&gt;It was a banner year for the Engineers in 2024–’25, with four MIT women’s teams all clinching NCAA Division III national titles for the first time.&lt;/p&gt;  &lt;p&gt;After winning their fourth straight NCAA East Regional Championship, the cross country team claimed their first national title in November with All-American performances from Christina Crow ’25 (pictured), Rujuta Sane ’26, and Kate Sanderson ’26. In March, the indoor track and field team scored 49 points—the most ever by an MIT women’s team at a national indoor meet—to win their first national title. A week later, the swimming and diving team won three individual and four relay titles and captured their first national title. Kate Augustyn ’25 ended her MIT career with four individual and four relay national championships and 27 All-America honors. Then in May, the outdoor track and field team claimed their first national championship, making MIT the first to sweep the Division III national titles in women’s cross country and indoor and outdoor track and field in the same year.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="foot race on grass with spectators" class="wp-image-1117964" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/dsc2462_54160681653_o.jpg?w=900" /&gt;&lt;div class="image-credit"&gt;NATALIE GREEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="winners podium for NCAA track and field champions" class="wp-image-1117965" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/indoor-track-field-mit-champions-2025.jpg?w=900" /&gt;&lt;div class="image-credit"&gt;D3 PHOTOGRAPHY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt=" NCAA Division III Swim and Dive Championships champs with trophy and MIT sign" class="wp-image-1117969" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/NCAA-SWIM-AND-DIVE-SATURDAY-PRELIMS_3.jpg?w=900" /&gt;&lt;div class="image-credit"&gt;DAVID BEACH&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;</description><content:encoded>&lt;p&gt;&lt;br /&gt;It was a banner year for the Engineers in 2024–’25, with four MIT women’s teams all clinching NCAA Division III national titles for the first time.&lt;/p&gt;  &lt;p&gt;After winning their fourth straight NCAA East Regional Championship, the cross country team claimed their first national title in November with All-American performances from Christina Crow ’25 (pictured), Rujuta Sane ’26, and Kate Sanderson ’26. In March, the indoor track and field team scored 49 points—the most ever by an MIT women’s team at a national indoor meet—to win their first national title. A week later, the swimming and diving team won three individual and four relay titles and captured their first national title. Kate Augustyn ’25 ended her MIT career with four individual and four relay national championships and 27 All-America honors. Then in May, the outdoor track and field team claimed their first national championship, making MIT the first to sweep the Division III national titles in women’s cross country and indoor and outdoor track and field in the same year.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="foot race on grass with spectators" class="wp-image-1117964" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/dsc2462_54160681653_o.jpg?w=900" /&gt;&lt;div class="image-credit"&gt;NATALIE GREEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="winners podium for NCAA track and field champions" class="wp-image-1117965" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/indoor-track-field-mit-champions-2025.jpg?w=900" /&gt;&lt;div class="image-credit"&gt;D3 PHOTOGRAPHY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt=" NCAA Division III Swim and Dive Championships champs with trophy and MIT sign" class="wp-image-1117969" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/NCAA-SWIM-AND-DIVE-SATURDAY-PRELIMS_3.jpg?w=900" /&gt;&lt;div class="image-credit"&gt;DAVID BEACH&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117732/an-epic-year-for-womens-sports/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Art rhymes (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117723/art-rhymes/</link><description>&lt;p&gt;&lt;br /&gt;As an MIT visiting scholar, rap legend Lupe Fiasco decided to go fishing for ideas on campus. In an approach he calls “ghotiing” (pronounced “fishing”), he composed nine raps inspired by works in MIT’s public art collection, writing and recording them on site. On May 2, he and the MIT Festival Jazz Ensemble debuted six of them, performing in front of a packed audience in Kresge for the final performance of the MIT Artfinity festival. The concert featured arrangements of Fiasco’s music done by Kevin Costello ’21, grad student Matthew Michalek, students in Fiasco’s Rap Theory and Practice class, and professor Evan Ziporyn. Produced in collaboration with the MIT List Visual Arts Center, Fiasco’s “Ghotiing MIT: Public Art” project also lets campus visitors scan a QR code and listen to his site-specific raps on their phones as they view the artworks in person. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;Click &lt;strong&gt;here&lt;/strong&gt; to go on a virtual tour of seven pieces from MIT’s public art collection as you listen to Lupe Fiasco’s raps inspired by each piece.&lt;/p&gt;  &lt;p&gt;WBUR’s coverage of the project is available &lt;strong&gt;here&lt;/strong&gt; and you can also read more about it in the &lt;strong&gt;Boston Globe&lt;/strong&gt; and &lt;strong&gt;The Guardian&lt;/strong&gt;.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118030" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/2025-Artfinity-Ghotiing-Lupe-Fiasco-with-the-MIT-FJE-Credit-Caroline-Alden-020.jpg?w=2438" width="2438" /&gt;&lt;div class="image-credit"&gt;CAROLINE ALDEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118031" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/2025-Artfinity-Ghotiing-Lupe-Fiasco-with-the-MIT-FJE-Credit-Caroline-Alden-035.jpg?w=2000" width="2000" /&gt;&lt;div class="image-credit"&gt;CAROLINE ALDEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118032" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/2025-Artfinity-Ghotiing-Lupe-Fiasco-with-the-MIT-FJE-Credit-Caroline-Alden-047.jpg?w=1978" /&gt;&lt;div class="image-credit"&gt;CAROLINE ALDEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;</description><content:encoded>&lt;p&gt;&lt;br /&gt;As an MIT visiting scholar, rap legend Lupe Fiasco decided to go fishing for ideas on campus. In an approach he calls “ghotiing” (pronounced “fishing”), he composed nine raps inspired by works in MIT’s public art collection, writing and recording them on site. On May 2, he and the MIT Festival Jazz Ensemble debuted six of them, performing in front of a packed audience in Kresge for the final performance of the MIT Artfinity festival. The concert featured arrangements of Fiasco’s music done by Kevin Costello ’21, grad student Matthew Michalek, students in Fiasco’s Rap Theory and Practice class, and professor Evan Ziporyn. Produced in collaboration with the MIT List Visual Arts Center, Fiasco’s “Ghotiing MIT: Public Art” project also lets campus visitors scan a QR code and listen to his site-specific raps on their phones as they view the artworks in person. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;Click &lt;strong&gt;here&lt;/strong&gt; to go on a virtual tour of seven pieces from MIT’s public art collection as you listen to Lupe Fiasco’s raps inspired by each piece.&lt;/p&gt;  &lt;p&gt;WBUR’s coverage of the project is available &lt;strong&gt;here&lt;/strong&gt; and you can also read more about it in the &lt;strong&gt;Boston Globe&lt;/strong&gt; and &lt;strong&gt;The Guardian&lt;/strong&gt;.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118030" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/2025-Artfinity-Ghotiing-Lupe-Fiasco-with-the-MIT-FJE-Credit-Caroline-Alden-020.jpg?w=2438" width="2438" /&gt;&lt;div class="image-credit"&gt;CAROLINE ALDEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118031" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/2025-Artfinity-Ghotiing-Lupe-Fiasco-with-the-MIT-FJE-Credit-Caroline-Alden-035.jpg?w=2000" width="2000" /&gt;&lt;div class="image-credit"&gt;CAROLINE ALDEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118032" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/2025-Artfinity-Ghotiing-Lupe-Fiasco-with-the-MIT-FJE-Credit-Caroline-Alden-047.jpg?w=1978" /&gt;&lt;div class="image-credit"&gt;CAROLINE ALDEN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117723/art-rhymes/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] An intelligent, practical path to reindustrialization (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117720/an-intelligent-path-to-reindustrialization/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2023/05/0148_MIT_Sally_Kornbluth_148-thumb.jpeg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;This past spring, we launched a brand-new manufacturing initiative—building on ideas that are as old as MIT. Since William Barton Rogers created a school to help accelerate America’s industrialization, manufacturing has been an essential part of our mission—a particularly MIT brand of manufacturing, informed and improved by scientific principles and advanced by the kind of hands-on leaders Rogers designed MIT to train.&lt;/p&gt;  &lt;p&gt;In the 1980s, the Institute’s “Made in America” study opened with the enduring observation “To live well, a nation must produce well.” Along with &lt;em&gt;The Machine That Changed the World&lt;/em&gt;, the 1990 book that told the story of “lean production,” this landmark report helped US manufacturers understand and successfully compete with Japan’s quality model.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Then, a little over a decade ago, MIT’s “Production in the Innovation Economy” initiative highlighted the opportunities we miss if design and manufacturing teams are miles or even oceans apart—and played a significant role in shaping the nation’s Advanced Manufacturing Initiative.&lt;/p&gt;  &lt;p&gt;Building on this legacy, and in response to an urgent national interest in restoring America’s manufacturing strength, an inspired group of MIT faculty came together in 2022 to found the &lt;strong&gt;Manufacturing@MIT Working Group. &lt;/strong&gt;They explored new ways to marshal MIT’s expertise in technology, the social sciences, and management to forge an intelligent, practical path to reindustrialization.&lt;/p&gt; 
 &lt;p&gt;As a result of this group’s foundational work, we’ve now created the &lt;strong&gt;MIT Initiative for New Manufacturing (INM)&lt;/strong&gt;,which will join the ranks of our other Presidential Initiatives—all designed to help the people of MIT come together in new ways to accelerate our progress and increase our impact.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To help make manufacturing more productive, resilient, and sustainable, we aim to do the following:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt; &amp;nbsp;-&lt;/strong&gt;Work with firms big and small to help them adopt new approaches for increased productivity.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&amp;nbsp; -&lt;/strong&gt;Design high-quality, human-centered jobs that bring new life to communities across the country.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&amp;nbsp; -&lt;/strong&gt;Re-elevate manufacturing in MIT’s own curriculum—and provide pathways for people outside MIT to gain the skills to transform their own prospects and fuel a “new manufacturing” economy.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&amp;nbsp; -&lt;/strong&gt;Reimagine manufacturing technologies and systems to advance fields like energy production, health care, computing, transportation, consumer products, and more.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;&amp;nbsp; -&lt;/strong&gt;Tackle such challenges as making supply chains more resilient and informing public policy to foster a broad, healthy manufacturing ecosystem that can drive decades of innovation and growth.&lt;/p&gt;  &lt;p&gt;If all this sounds ambitious—it is. And these are just the highlights! But I’m convinced that there is no more important work we can do right now to meet the moment and serve the nation.&amp;nbsp;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2023/05/0148_MIT_Sally_Kornbluth_148-thumb.jpeg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;This past spring, we launched a brand-new manufacturing initiative—building on ideas that are as old as MIT. Since William Barton Rogers created a school to help accelerate America’s industrialization, manufacturing has been an essential part of our mission—a particularly MIT brand of manufacturing, informed and improved by scientific principles and advanced by the kind of hands-on leaders Rogers designed MIT to train.&lt;/p&gt;  &lt;p&gt;In the 1980s, the Institute’s “Made in America” study opened with the enduring observation “To live well, a nation must produce well.” Along with &lt;em&gt;The Machine That Changed the World&lt;/em&gt;, the 1990 book that told the story of “lean production,” this landmark report helped US manufacturers understand and successfully compete with Japan’s quality model.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Then, a little over a decade ago, MIT’s “Production in the Innovation Economy” initiative highlighted the opportunities we miss if design and manufacturing teams are miles or even oceans apart—and played a significant role in shaping the nation’s Advanced Manufacturing Initiative.&lt;/p&gt;  &lt;p&gt;Building on this legacy, and in response to an urgent national interest in restoring America’s manufacturing strength, an inspired group of MIT faculty came together in 2022 to found the &lt;strong&gt;Manufacturing@MIT Working Group. &lt;/strong&gt;They explored new ways to marshal MIT’s expertise in technology, the social sciences, and management to forge an intelligent, practical path to reindustrialization.&lt;/p&gt; 
 &lt;p&gt;As a result of this group’s foundational work, we’ve now created the &lt;strong&gt;MIT Initiative for New Manufacturing (INM)&lt;/strong&gt;,which will join the ranks of our other Presidential Initiatives—all designed to help the people of MIT come together in new ways to accelerate our progress and increase our impact.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;To help make manufacturing more productive, resilient, and sustainable, we aim to do the following:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt; &amp;nbsp;-&lt;/strong&gt;Work with firms big and small to help them adopt new approaches for increased productivity.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&amp;nbsp; -&lt;/strong&gt;Design high-quality, human-centered jobs that bring new life to communities across the country.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&amp;nbsp; -&lt;/strong&gt;Re-elevate manufacturing in MIT’s own curriculum—and provide pathways for people outside MIT to gain the skills to transform their own prospects and fuel a “new manufacturing” economy.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&amp;nbsp; -&lt;/strong&gt;Reimagine manufacturing technologies and systems to advance fields like energy production, health care, computing, transportation, consumer products, and more.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;&amp;nbsp; -&lt;/strong&gt;Tackle such challenges as making supply chains more resilient and informing public policy to foster a broad, healthy manufacturing ecosystem that can drive decades of innovation and growth.&lt;/p&gt;  &lt;p&gt;If all this sounds ambitious—it is. And these are just the highlights! But I’m convinced that there is no more important work we can do right now to meet the moment and serve the nation.&amp;nbsp;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117720/an-intelligent-path-to-reindustrialization/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] What if computer history were a romantic comedy? (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117710/what-if-computer-history-were-a-romantic-comedy/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;The computer first appeared on the Broadway stage in 1955 in a romantic comedy—William Marchant’s &lt;em&gt;The Desk Set&lt;/em&gt;. The play centers on four women who conduct research on behalf of the fictional International Broadcasting Company. Early in the first act, a young engineer named Richard Sumner arrives in the offices of the research department without explaining who he is or why he is studying the behavior of the workers. Bunny Watson, the head of the department, discovers that the engineer plans to install an “electronic brain” called Emmarac, which Sumner affectionately refers to as “Emmy” and describes as “the machine that takes the pause quotient out of the work–man-hour relationship.”&lt;/p&gt;  &lt;p&gt;What Sumner calls the “pause quotient” is jargon for the everyday activities and mundane interactions that make human beings less efficient than machines. Emmarac would eliminate inefficiencies, such as walking to a bookshelf or talking with a coworker about weekend plans. Bunny Watson comes to believe that the computing machine will eliminate not only inefficiencies in the workplace but also the need for human workers in her department. Sumner, the engineer, presents the computer as a technology of efficiency, but Watson, the department head, views it as a technology of displacement.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Bunny Watson’s view was not uncommon during the first decade of computing technology. Thomas Watson Sr., president of IBM, insisted that one of his firm’s first machines be called a “calculator” instead of a “computer” because “he was concerned that the latter term, which had always referred to a human being, would raise the specter of technological unemployment,” according to historians Martin Campbell-Kelly and William Aspray. In keeping with the worry of both Watsons, the computer takes the stage on Broadway as a threat to white-collar work. The women in Marchant’s play fight against the threat of unemployment as soon as they learn why Sumner has arrived. The play thus attests to the fact that the very benefits of speed, accuracy, and information processing that made the computer useful for business also caused it to be perceived as a threat to the professional-managerial class.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;Comedy provides a template for managing the incongruity of an “electronic brain” arriving in a space oriented around human expertise and professional judgment.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;This threat was somewhat offset by the fact that for most of the 1950s, the computing industry was not profitable in the United States. Manufacturers produced and sold or leased the machines at steep losses, primarily to preserve a speculative market position and to bolster their image as technologically innovative. For many such firms, neglecting to compete in the emerging market for computers would have risked the perception that they were falling behind. They hoped computing would eventually become profitable as the technology improved, but even by the middle of the decade, it was not obvious to industry insiders when this would be the case. Even if the computer seemed to promise a new world of “lightning speed” efficiency and information management, committing resources to this promise was almost prohibitively costly.&lt;/p&gt; 
 &lt;p&gt;While firms weighed the financial costs of computing, the growing interest in this new technology was initially perceived by white-collar workers as a threat to the nature of managerial expertise. Large corporations dominated American enterprise after the Second World War, and what historian Alfred Chandler called the “visible hand” of managerial professionals exerted considerable influence over the economy. Many observers wondered if computing machines would lead to a “revolution” in professional-managerial tasks. Some even speculated that “electronic brains” would soon coordinate the economy, thus replacing the bureaucratic oversight of most forms of labor.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Howard Gammon, an official with the US Bureau of the Budget, explained in a 1954 essay that “electronic information processing machines” could “make substantial savings and render better service” if managers were to accept the technology. Gammon advocated for the automation of office work in areas like “stock control, handling orders, processing mailing lists, or a hundred and one other activities requiring the accumulating and sorting of information.” He even anticipated the development of tools for “erect[ing] a consistent system of decisions in areas where ‘judgment’ can be reduced to sets of clear-cut rules such as (1) ‘purchase at the lowest price,’ or (2) ‘never let the supply of bolts fall below the estimated one-week requirement for any size or type.’”&lt;/p&gt; 
 &lt;p&gt;Gammon’s essay illustrates how many administrative thinkers hoped that computers would allow upper-level managers to oversee industrial production through a series of unambiguous rules that would no longer require midlevel workers for their enactment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This fantasy was impossible in the 1950s for so many reasons, the most obvious being that only a limited number of executable processes in postwar managerial capitalism could be automated through extant technology, and even fewer areas of “judgment,” as Gammon called them, can be reduced to sets of clear-cut rules. Still, this fantasy was part of the cultural milieu when Marchant’s play premiered on Broadway, one year after Gammon’s report and just a few months after IBM had announced the advance in memory storage technology behind its new 705 Model II, the first successful commercial data-processing machine. IBM received 100 orders for the 705, a commercial viability that seemed to signal the beginning of a new age in American corporate life.&lt;/p&gt;  &lt;p&gt;It soon became clear, however, that this new age was not the one that Gammon imagined. Rather than causing widespread unemployment or the total automation of the visible hand, the computer would transform the character of work itself. Marchant’s play certainly invokes the possibility of unemployment, but its posture toward the computer shifts toward a more accommodative view of what later scholars would call the “computerization of work.” For example, early in the play, Richard Sumner conjures the specter of the machine as a threat when he asks Bunny Watson if the new electronic brains “give you the feeling that maybe—just maybe—that people are a little bit outmoded.” Similarly, at the beginning of the second act, a researcher named Peg remarks, “I understand thousands of people are being thrown out of work because of these electronic brains.” The play seems to affirm Sumner’s sentiment and Peg’s implicit worry about her own unemployment once the computer, Emmarac, has been installed in the third act. After the installation, Sumner and Watson give the machine a research problem that previously took Peg several days to complete. Watson expects the task to stump Emmarac, but the machine takes only a few seconds to produce the same answer.&lt;/p&gt;  &lt;p&gt;While such moments conjure the specter of “technological unemployment,” the play juxtaposes Emmarac’s feats with Watson’s wit and spontaneity. For instance, after Sumner suggests people may be “outmoded,” Watson responds, “Yes, I wouldn’t be a bit surprised if they stopped making them.” Sumner gets the joke but doesn’t find it funny: “Miss Watson, Emmarac is not a subject for levity.” The staging of the play contradicts Sumner’s assertion. Emmarac occasions all manner of levity in &lt;em&gt;The Desk Set&lt;/em&gt;, ranging from Watson’s joke to Emmarac’s absurd firing of every member of the International Broadcasting Company, including its president, later in the play.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;This shifting portrayal of Emmarac follows a much older pattern in dramatic comedy. As literary critic Northrop Frye explains, many forms of comedy follow an “argument” in which a “new world” appears on the stage and transforms the society entrenched at the beginning of the play. The movement away from established society hinges on a “principle of conversion” that “include[s] as many people as possible in its final society: the blocking characters are more often reconciled or converted than simply repudiated.”&lt;/p&gt;  &lt;p&gt;We see a similar dynamic in how Marchant’s play portrays the efficiency expert as brusque, rational, and incapable of empathy or romantic interests. After his arrival in the office, a researcher named Sadel says, “You notice he never takes his coat off? Do you think maybe he’s a robot?” Another researcher, Ruthie Saylor, later kisses Sumner on the cheek and invites him to a party. He says, “Sorry, I’ve got work to do,” to which Ruthie responds, “Sadel’s right—you &lt;em&gt;are &lt;/em&gt;a robot!”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even as Sumner’s robotic behavior portrays him as antisocial, Emmarac further isolates him from the office by posing a threat to the workers. The play accentuates this blocking function by assigning Emmarac a personality and gender: Sumner calls the machine “Emmy,” and its operator, a woman named Miss Warriner, describes the machine as a “good girl.” By taking its place in the office, Emmarac effectively moves into the same space of labor and economic power as Bunny Watson, who had previously overseen the researchers and their activities. After being installed in the office, the large mainframe computer begins to coordinate this knowledge work. The gendering of the computer thus presents Emmarac as a newer model of the so-called New Woman, as if the computer imperils the feminist ideal that Bunny Watson clearly embodies. By directly challenging Watson’s socioeconomic independence and professional identity, the computer’s arrival in the workplace threatens to make the New Woman obsolete.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yet much like Frye’s claims about the “argument” of comedy, the conflict between Emmarac and Watson resolves as the machine transforms from a direct competitor into a collaborator. We see this shift during a final competition between Emmarac and the research department. The women have been notified that their positions have been terminated, and they begin packing up their belongings. Two requests for information suddenly arrive, but Watson and her fellow researchers refuse to process them because of their dismissal, so Warriner and Sumner attempt to field the requests. The research tasks are complicated, and Warriner mistakenly directs Emmarac to print a long, irrelevant answer. The machine inflexibly continues although the other inquiry needs to be addressed. Sumner and Warriner try to stop the machine, but this countermanding order causes the machine’s “magnetic circuit” to emit smoke and a loud noise. Sumner yells at Warriner, who runs offstage, and the efficiency expert is now the only one to field the requests and salvage the machine. However, he doesn’t know how to stop Emmarac from malfunctioning. Marchant’s stage directions here say that Watson, who has studied the machine’s maintenance and operation, “&lt;em&gt;takes a hairpin from her hair and manipulates a knob on Emmarac—the NOISE obligingly stops.&lt;/em&gt;” Watson then explains, “You forget, I know something about one of these. All that research, remember?”&lt;/p&gt; 

 &lt;p&gt;The madcap quality of this scene continues after Sumner discovers that Emmarac’s “little sister” in the payroll office has sent pink slips to every employee at the broadcasting firm. Sumner then receives a letter containing his own pink slip, which prompts Watson to quote Horatio’s lament as Hamlet dies: “Good night, sweet prince.” The turn of events poses as tragedy, but of course it leads to the play’s comic resolution. Once Sumner discovers that the payroll computer has erred—or, at least, that someone improperly programmed it—he explains that the women in the research department haven’t been fired. Emmarac, he says, “was not meant to replace you. It was never intended to take over. It was installed to free your time for research—to do the daily mechanical routine.”&lt;/p&gt;  &lt;p&gt;Even as Watson “fixes” the machine, the play fixes the robotic man through his professional failures. After this moment of discovery, Sumner apologizes to Watson and reconciles with the other women in the research department. He then promises to take them out to lunch and buy them “three martinis each.” Sumner exits with the women “laughing and talking,” thus reversing the antisocial role that he has occupied for most of the play.&lt;/p&gt;  &lt;p&gt;Emmarac’s failure, too, becomes an opportunity for its conversion. It may be that a programming error led to the company-wide pink slips, but the computer’s near-breakdown results from its rigidity. In both cases, the computer fails to navigate the world of knowledge work, thus becoming less threatening and more absurd through its flashing lights, urgent noises, and smoking console. This shift in the machine’s stage presence—the fact that it &lt;em&gt;becomes comic&lt;/em&gt;—does not lead to its banishment or dismantling. Rather, after Watson “fixes” Emmarac, she uses it to compute a final inquiry submitted to her office: “What is the total weight of the Earth?” Given a problem that a human researcher “can spend months finding out,” she chooses to collaborate. Watson types out the question and Emmarac emits “its boop-boop-a-doop noise” in response, prompting her to answer, “Boop-boop-a-doop to &lt;em&gt;you.&lt;/em&gt;” Emmarac is no longer Watson’s automated replacement but her partner in knowledge work.&lt;/p&gt;  &lt;p&gt;In Marchant’s play, comedy provides a template for managing the incongruity of an “electronic brain” arriving in a space oriented around human expertise and professional judgment. This template converts the automation of professional-­managerial tasks from a threat into an opportunity, implying that a partnership with knowledge workers can convert the electronic brain into a machine compatible with their happiness. The computerization of work thus becomes its own kind of comic plot.&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947 gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 core/columns_5"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118022" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/book.mangrum.jpg?w=1333" width="1333" /&gt;&lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;The computer first appeared on the Broadway stage in 1955 in a romantic comedy—William Marchant’s &lt;em&gt;The Desk Set&lt;/em&gt;. The play centers on four women who conduct research on behalf of the fictional International Broadcasting Company. Early in the first act, a young engineer named Richard Sumner arrives in the offices of the research department without explaining who he is or why he is studying the behavior of the workers. Bunny Watson, the head of the department, discovers that the engineer plans to install an “electronic brain” called Emmarac, which Sumner affectionately refers to as “Emmy” and describes as “the machine that takes the pause quotient out of the work–man-hour relationship.”&lt;/p&gt;  &lt;p&gt;What Sumner calls the “pause quotient” is jargon for the everyday activities and mundane interactions that make human beings less efficient than machines. Emmarac would eliminate inefficiencies, such as walking to a bookshelf or talking with a coworker about weekend plans. Bunny Watson comes to believe that the computing machine will eliminate not only inefficiencies in the workplace but also the need for human workers in her department. Sumner, the engineer, presents the computer as a technology of efficiency, but Watson, the department head, views it as a technology of displacement.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Bunny Watson’s view was not uncommon during the first decade of computing technology. Thomas Watson Sr., president of IBM, insisted that one of his firm’s first machines be called a “calculator” instead of a “computer” because “he was concerned that the latter term, which had always referred to a human being, would raise the specter of technological unemployment,” according to historians Martin Campbell-Kelly and William Aspray. In keeping with the worry of both Watsons, the computer takes the stage on Broadway as a threat to white-collar work. The women in Marchant’s play fight against the threat of unemployment as soon as they learn why Sumner has arrived. The play thus attests to the fact that the very benefits of speed, accuracy, and information processing that made the computer useful for business also caused it to be perceived as a threat to the professional-managerial class.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;Comedy provides a template for managing the incongruity of an “electronic brain” arriving in a space oriented around human expertise and professional judgment.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;This threat was somewhat offset by the fact that for most of the 1950s, the computing industry was not profitable in the United States. Manufacturers produced and sold or leased the machines at steep losses, primarily to preserve a speculative market position and to bolster their image as technologically innovative. For many such firms, neglecting to compete in the emerging market for computers would have risked the perception that they were falling behind. They hoped computing would eventually become profitable as the technology improved, but even by the middle of the decade, it was not obvious to industry insiders when this would be the case. Even if the computer seemed to promise a new world of “lightning speed” efficiency and information management, committing resources to this promise was almost prohibitively costly.&lt;/p&gt; 
 &lt;p&gt;While firms weighed the financial costs of computing, the growing interest in this new technology was initially perceived by white-collar workers as a threat to the nature of managerial expertise. Large corporations dominated American enterprise after the Second World War, and what historian Alfred Chandler called the “visible hand” of managerial professionals exerted considerable influence over the economy. Many observers wondered if computing machines would lead to a “revolution” in professional-managerial tasks. Some even speculated that “electronic brains” would soon coordinate the economy, thus replacing the bureaucratic oversight of most forms of labor.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Howard Gammon, an official with the US Bureau of the Budget, explained in a 1954 essay that “electronic information processing machines” could “make substantial savings and render better service” if managers were to accept the technology. Gammon advocated for the automation of office work in areas like “stock control, handling orders, processing mailing lists, or a hundred and one other activities requiring the accumulating and sorting of information.” He even anticipated the development of tools for “erect[ing] a consistent system of decisions in areas where ‘judgment’ can be reduced to sets of clear-cut rules such as (1) ‘purchase at the lowest price,’ or (2) ‘never let the supply of bolts fall below the estimated one-week requirement for any size or type.’”&lt;/p&gt; 
 &lt;p&gt;Gammon’s essay illustrates how many administrative thinkers hoped that computers would allow upper-level managers to oversee industrial production through a series of unambiguous rules that would no longer require midlevel workers for their enactment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This fantasy was impossible in the 1950s for so many reasons, the most obvious being that only a limited number of executable processes in postwar managerial capitalism could be automated through extant technology, and even fewer areas of “judgment,” as Gammon called them, can be reduced to sets of clear-cut rules. Still, this fantasy was part of the cultural milieu when Marchant’s play premiered on Broadway, one year after Gammon’s report and just a few months after IBM had announced the advance in memory storage technology behind its new 705 Model II, the first successful commercial data-processing machine. IBM received 100 orders for the 705, a commercial viability that seemed to signal the beginning of a new age in American corporate life.&lt;/p&gt;  &lt;p&gt;It soon became clear, however, that this new age was not the one that Gammon imagined. Rather than causing widespread unemployment or the total automation of the visible hand, the computer would transform the character of work itself. Marchant’s play certainly invokes the possibility of unemployment, but its posture toward the computer shifts toward a more accommodative view of what later scholars would call the “computerization of work.” For example, early in the play, Richard Sumner conjures the specter of the machine as a threat when he asks Bunny Watson if the new electronic brains “give you the feeling that maybe—just maybe—that people are a little bit outmoded.” Similarly, at the beginning of the second act, a researcher named Peg remarks, “I understand thousands of people are being thrown out of work because of these electronic brains.” The play seems to affirm Sumner’s sentiment and Peg’s implicit worry about her own unemployment once the computer, Emmarac, has been installed in the third act. After the installation, Sumner and Watson give the machine a research problem that previously took Peg several days to complete. Watson expects the task to stump Emmarac, but the machine takes only a few seconds to produce the same answer.&lt;/p&gt;  &lt;p&gt;While such moments conjure the specter of “technological unemployment,” the play juxtaposes Emmarac’s feats with Watson’s wit and spontaneity. For instance, after Sumner suggests people may be “outmoded,” Watson responds, “Yes, I wouldn’t be a bit surprised if they stopped making them.” Sumner gets the joke but doesn’t find it funny: “Miss Watson, Emmarac is not a subject for levity.” The staging of the play contradicts Sumner’s assertion. Emmarac occasions all manner of levity in &lt;em&gt;The Desk Set&lt;/em&gt;, ranging from Watson’s joke to Emmarac’s absurd firing of every member of the International Broadcasting Company, including its president, later in the play.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;This shifting portrayal of Emmarac follows a much older pattern in dramatic comedy. As literary critic Northrop Frye explains, many forms of comedy follow an “argument” in which a “new world” appears on the stage and transforms the society entrenched at the beginning of the play. The movement away from established society hinges on a “principle of conversion” that “include[s] as many people as possible in its final society: the blocking characters are more often reconciled or converted than simply repudiated.”&lt;/p&gt;  &lt;p&gt;We see a similar dynamic in how Marchant’s play portrays the efficiency expert as brusque, rational, and incapable of empathy or romantic interests. After his arrival in the office, a researcher named Sadel says, “You notice he never takes his coat off? Do you think maybe he’s a robot?” Another researcher, Ruthie Saylor, later kisses Sumner on the cheek and invites him to a party. He says, “Sorry, I’ve got work to do,” to which Ruthie responds, “Sadel’s right—you &lt;em&gt;are &lt;/em&gt;a robot!”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even as Sumner’s robotic behavior portrays him as antisocial, Emmarac further isolates him from the office by posing a threat to the workers. The play accentuates this blocking function by assigning Emmarac a personality and gender: Sumner calls the machine “Emmy,” and its operator, a woman named Miss Warriner, describes the machine as a “good girl.” By taking its place in the office, Emmarac effectively moves into the same space of labor and economic power as Bunny Watson, who had previously overseen the researchers and their activities. After being installed in the office, the large mainframe computer begins to coordinate this knowledge work. The gendering of the computer thus presents Emmarac as a newer model of the so-called New Woman, as if the computer imperils the feminist ideal that Bunny Watson clearly embodies. By directly challenging Watson’s socioeconomic independence and professional identity, the computer’s arrival in the workplace threatens to make the New Woman obsolete.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yet much like Frye’s claims about the “argument” of comedy, the conflict between Emmarac and Watson resolves as the machine transforms from a direct competitor into a collaborator. We see this shift during a final competition between Emmarac and the research department. The women have been notified that their positions have been terminated, and they begin packing up their belongings. Two requests for information suddenly arrive, but Watson and her fellow researchers refuse to process them because of their dismissal, so Warriner and Sumner attempt to field the requests. The research tasks are complicated, and Warriner mistakenly directs Emmarac to print a long, irrelevant answer. The machine inflexibly continues although the other inquiry needs to be addressed. Sumner and Warriner try to stop the machine, but this countermanding order causes the machine’s “magnetic circuit” to emit smoke and a loud noise. Sumner yells at Warriner, who runs offstage, and the efficiency expert is now the only one to field the requests and salvage the machine. However, he doesn’t know how to stop Emmarac from malfunctioning. Marchant’s stage directions here say that Watson, who has studied the machine’s maintenance and operation, “&lt;em&gt;takes a hairpin from her hair and manipulates a knob on Emmarac—the NOISE obligingly stops.&lt;/em&gt;” Watson then explains, “You forget, I know something about one of these. All that research, remember?”&lt;/p&gt; 

 &lt;p&gt;The madcap quality of this scene continues after Sumner discovers that Emmarac’s “little sister” in the payroll office has sent pink slips to every employee at the broadcasting firm. Sumner then receives a letter containing his own pink slip, which prompts Watson to quote Horatio’s lament as Hamlet dies: “Good night, sweet prince.” The turn of events poses as tragedy, but of course it leads to the play’s comic resolution. Once Sumner discovers that the payroll computer has erred—or, at least, that someone improperly programmed it—he explains that the women in the research department haven’t been fired. Emmarac, he says, “was not meant to replace you. It was never intended to take over. It was installed to free your time for research—to do the daily mechanical routine.”&lt;/p&gt;  &lt;p&gt;Even as Watson “fixes” the machine, the play fixes the robotic man through his professional failures. After this moment of discovery, Sumner apologizes to Watson and reconciles with the other women in the research department. He then promises to take them out to lunch and buy them “three martinis each.” Sumner exits with the women “laughing and talking,” thus reversing the antisocial role that he has occupied for most of the play.&lt;/p&gt;  &lt;p&gt;Emmarac’s failure, too, becomes an opportunity for its conversion. It may be that a programming error led to the company-wide pink slips, but the computer’s near-breakdown results from its rigidity. In both cases, the computer fails to navigate the world of knowledge work, thus becoming less threatening and more absurd through its flashing lights, urgent noises, and smoking console. This shift in the machine’s stage presence—the fact that it &lt;em&gt;becomes comic&lt;/em&gt;—does not lead to its banishment or dismantling. Rather, after Watson “fixes” Emmarac, she uses it to compute a final inquiry submitted to her office: “What is the total weight of the Earth?” Given a problem that a human researcher “can spend months finding out,” she chooses to collaborate. Watson types out the question and Emmarac emits “its boop-boop-a-doop noise” in response, prompting her to answer, “Boop-boop-a-doop to &lt;em&gt;you.&lt;/em&gt;” Emmarac is no longer Watson’s automated replacement but her partner in knowledge work.&lt;/p&gt;  &lt;p&gt;In Marchant’s play, comedy provides a template for managing the incongruity of an “electronic brain” arriving in a space oriented around human expertise and professional judgment. This template converts the automation of professional-­managerial tasks from a threat into an opportunity, implying that a partnership with knowledge workers can convert the electronic brain into a machine compatible with their happiness. The computerization of work thus becomes its own kind of comic plot.&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947 gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 core/columns_5"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118022" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/book.mangrum.jpg?w=1333" width="1333" /&gt;&lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117710/what-if-computer-history-were-a-romantic-comedy/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Travels with Rambax (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117703/travels-with-rambax/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;KAOLACK, Senegal – The MIT students have just finished dinner and are crumpling soda cans into trash bins when they get the summons: “Grab your drums, grab your drums, grab your drums …”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It is time for the tanibeer, a nighttime drum and dance party, in Kaolack, a town amid salt plains and peanut farms located 220 kilometers southeast of Dakar, Senegal’s capital. For the members of Rambax MIT, the Institute’s Senegalese drumming ensemble, the excitement is palpable as they fetch their drums and make their way up the road. Their destination is a small field on the family land of their director, Lamine Touré, who comes from a long line of griots, the musicians and oral historians of the Wolof people.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118113" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC04925.jpg?w=1491" width="1491" /&gt;&lt;figcaption class="wp-element-caption"&gt;Lamine Touré, director of Rambax MIT, leads drum practice in Grand Mbao&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Touré, a Senegalese master drummer and an MIT lecturer in world music, cofounded Rambax in 2001 with Patricia Tang, an associate professor and ethnomusicologist who specializes in West African music. It began as an extracurricular group to teach students and other members of the MIT community the art of sabar, a vibrant West African drumming and dance tradition. Today, Rambax is a credit-bearing class (21M.460) enrolling as many as 50 students a semester, and its ensemble’s performances draw audiences from MIT and the wider Boston community. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;During Independent Activities Period (IAP), 16 members of the ensemble joined Touré and Tang on a two-week study tour in Senegal, the birthplace of the music that inspires Rambax. In addition to performing, the students attended drumming classes and dance workshops taught by expert Senegalese drummers, and they experienced sabar drumming within its traditional and cultural context in Dakar and Kaolack.&lt;/p&gt; 
 &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;A sabar celebration, known as a tanibeer when held at night, is a lavish display of dance music, a great neighborhood carnival.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;“Rambax is unique,” says Touré, whose family of prominent griot percussionists had him drumming from the age of four. Traveling to Senegal allowed the students to experience the cultural significance of the music—and Touré says their Senegalese audiences were really impressed with their playing.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118122" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3252.jpg?w=1086" /&gt;&lt;figcaption class="wp-element-caption"&gt;Poster for the tanibeer in Kaolack, Senegal, featuring Rambax MIT.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF RAMBAX&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;A sabar celebration, also known as a tanibeer when held at night, is a lavish display of dance music: a great neighborhood carnival, jammed with lights, blaring speakers, griots, costumed dancers, drums and drums and ever more drums, and—of course—dancing.&lt;/p&gt; 
 &lt;p&gt;On the night of the Rambax tanibeer in January, the sky is clear and chilly breezes waft across the field, where throngs of people, some dressed in colorful Senegalese traditional garb, gather under fluorescent lights perched on lampposts, chatting and gesticulating while waiting to watch the performance.&lt;/p&gt;  &lt;p&gt;As the MIT students walk in, wearing their bright yellow, green, and red knee-length dashikis, the crowd erupts into applause.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Standing in front of his hometown audience, long dark dreadlocks spilling to his shoulders, Touré takes a microphone and introduces the ensemble in his native Wolof. He explains that his students are lovers of African music and, under his tutelage at MIT, have been learning the art of sabar. He pauses for a moment and leans in close to start conducting.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then Rambax begins to play.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118112" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC04910.jpg?w=3000" /&gt;&lt;figcaption class="wp-element-caption"&gt;Local musicians join MIT students as they play their sabar drums at the Grand Mbao practice session.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The audience cheers and dances, forming a large circle in front of the musicians. Before long, women take turns at the center of the circle, matching the energetic rhythms of the drumming in the exuberant hip twists, arm swings, jumps, and impossibly high knee kicks of sabar dancing. The high-spirited drumming and dancing continue until the early hours of the morning.&amp;nbsp; &amp;nbsp;&lt;/p&gt;  &lt;p&gt;The tanibeer is a chance for Touré “to show what he’s been teaching his American students and that they can really play sabar quite well,” says Tang, who serves as a faculty advisor to Rambax. “And that’s often a surprise to the Senegalese audience.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118121" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3196.jpg?w=2666" width="2666" /&gt;&lt;figcaption class="wp-element-caption"&gt;Senegalese drummers Sadda Sene, Mbaye Ndiaga Seck, and Pa Ali Konte load drums onto the Rambax van after drum practice.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;PATRICIA TANG&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118120" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3079.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Drum practice on the beach in Grand Mbao.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF RAMBAX&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Among the 16 Rambax members on the Senegal trip is Autumn Geil ’21, a researcher and PhD student in the department of mechanical engineering. Initially drawn to music through choir and opera singing in high school, she had never heard of sabar drumming before discovering Rambax through a friend as an undergrad. She joined and has been a member of the ensemble, which she calls “just so incredible,” ever since.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118114" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC04943.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Rambax MIT students Eri-ife Olayinka ’25 and Kaelyn Dunnell ’25 by the sea in Grand Mbao. &lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;For Geil, practicing sabar with custodians of the tradition in Senegal is an opportunity to “observe and learn from the drummers to improve my skills for future performances.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Baran Mensah ’24, a Ghanian master’s student in mechanical engineering who minored in music as an undergrad, also joined Rambax after a friend recommended it. He sees it as a way to tap into his African roots while at MIT but says it’s also “a gateway to learn about Senegalese art, music, and culture.” Until the tour, he notes, “I really didn’t know much about my country’s West African neighbor.”&amp;nbsp; &amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“Coming on this trip allows us to take a step back, to learn about people and cultures, making us more effective communicators.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Autumn Geil ’21&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;Eri-ife Olayinka ’25, a computation and cognition major who took Rambax classes for two semesters, says she finds the learning environment supportive and the cultural insights provided by Touré and Tang rewarding. “You see yourself getting better, becoming comfortable with playing in the class,” she says. After completing the classes to satisfy her art requirements, Olayinka stayed on in the drumming ensemble. “I genuinely enjoy being in Rambax—it’s such a cool thing we get to do as a group,” she says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118117" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC05202.jpg?w=1500" width="1500" /&gt;&lt;figcaption class="wp-element-caption"&gt;Sabar artists Badara Faye, Mbaye Ndiaga Seck, and Pa Ali Konte take the mic at a tanibeer in Grand Mbao&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Visiting Kaolack is more than an opportunity for Rambax members to glimpse the culture that gave rise to sabar. With horse-drawn wagons clip-clopping through its rugged terrain but also massive solar farms, Kaolack is a city where old meets new. Witnessing those contrasts—and getting to perform and to immerse themselves in the performances of local musicians—helps the students enhance what Geil calls the “human connection skills” that all scientists and technologists need.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It’s really important for people in STEM to make space for art and music,” she says. “Coming on this trip allows us to take a step back, to learn about people and cultures, making us more effective communicators of our technology.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118116" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC05176.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Grad student Natalie Huang ’24 and local musician Badara Faye dance at a tanibeer in Grand Mbao.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118115" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC05019.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Rambax members watch Sengalese dancers and drummers at the tanibeer in Kaolack.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118123" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3369.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Grad student Tina Chen ’24, Neha Basu ’25, Pa Ali Konte, Monique Brewster ’10, and grad student Sandra Huffman ’20, SM ’21, drum at the tanibeer in Kaolack.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;PATRICIA TANG&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118124" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3371.jpg?w=2817" width="2817" /&gt;&lt;figcaption class="wp-element-caption"&gt;Rambax MIT plays at the tanibeer in Kaolack.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF RAMBAX&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Back inside Touré’s family compound, Tang invites the students to gather around so she can introduce them to Touré’s mother, Marie Sow, and his sisters and aunts. Sow showers them with good wishes and they bask in the glow.&lt;/p&gt;  &lt;p&gt;It’s important for Rambax members to know the history and culture of the people behind the music they practice, says Tang. “We really want the students to have this sort of cultural immersion—live in a Senegalese house like the Senegalese people do, hang out with Senegalese drummers, and really get a sense of what it’s like in Senegal.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Abdullahi Tsanni, SM ’23, a former &lt;/em&gt;MIT Technology Review&lt;em&gt; fellow, is a science writer based in Dakar, Senegal, who specializes in narrative features.&amp;nbsp;&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;KAOLACK, Senegal – The MIT students have just finished dinner and are crumpling soda cans into trash bins when they get the summons: “Grab your drums, grab your drums, grab your drums …”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It is time for the tanibeer, a nighttime drum and dance party, in Kaolack, a town amid salt plains and peanut farms located 220 kilometers southeast of Dakar, Senegal’s capital. For the members of Rambax MIT, the Institute’s Senegalese drumming ensemble, the excitement is palpable as they fetch their drums and make their way up the road. Their destination is a small field on the family land of their director, Lamine Touré, who comes from a long line of griots, the musicians and oral historians of the Wolof people.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118113" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC04925.jpg?w=1491" width="1491" /&gt;&lt;figcaption class="wp-element-caption"&gt;Lamine Touré, director of Rambax MIT, leads drum practice in Grand Mbao&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Touré, a Senegalese master drummer and an MIT lecturer in world music, cofounded Rambax in 2001 with Patricia Tang, an associate professor and ethnomusicologist who specializes in West African music. It began as an extracurricular group to teach students and other members of the MIT community the art of sabar, a vibrant West African drumming and dance tradition. Today, Rambax is a credit-bearing class (21M.460) enrolling as many as 50 students a semester, and its ensemble’s performances draw audiences from MIT and the wider Boston community. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;During Independent Activities Period (IAP), 16 members of the ensemble joined Touré and Tang on a two-week study tour in Senegal, the birthplace of the music that inspires Rambax. In addition to performing, the students attended drumming classes and dance workshops taught by expert Senegalese drummers, and they experienced sabar drumming within its traditional and cultural context in Dakar and Kaolack.&lt;/p&gt; 
 &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;A sabar celebration, known as a tanibeer when held at night, is a lavish display of dance music, a great neighborhood carnival.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;“Rambax is unique,” says Touré, whose family of prominent griot percussionists had him drumming from the age of four. Traveling to Senegal allowed the students to experience the cultural significance of the music—and Touré says their Senegalese audiences were really impressed with their playing.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118122" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3252.jpg?w=1086" /&gt;&lt;figcaption class="wp-element-caption"&gt;Poster for the tanibeer in Kaolack, Senegal, featuring Rambax MIT.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF RAMBAX&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;A sabar celebration, also known as a tanibeer when held at night, is a lavish display of dance music: a great neighborhood carnival, jammed with lights, blaring speakers, griots, costumed dancers, drums and drums and ever more drums, and—of course—dancing.&lt;/p&gt; 
 &lt;p&gt;On the night of the Rambax tanibeer in January, the sky is clear and chilly breezes waft across the field, where throngs of people, some dressed in colorful Senegalese traditional garb, gather under fluorescent lights perched on lampposts, chatting and gesticulating while waiting to watch the performance.&lt;/p&gt;  &lt;p&gt;As the MIT students walk in, wearing their bright yellow, green, and red knee-length dashikis, the crowd erupts into applause.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Standing in front of his hometown audience, long dark dreadlocks spilling to his shoulders, Touré takes a microphone and introduces the ensemble in his native Wolof. He explains that his students are lovers of African music and, under his tutelage at MIT, have been learning the art of sabar. He pauses for a moment and leans in close to start conducting.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then Rambax begins to play.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118112" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC04910.jpg?w=3000" /&gt;&lt;figcaption class="wp-element-caption"&gt;Local musicians join MIT students as they play their sabar drums at the Grand Mbao practice session.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The audience cheers and dances, forming a large circle in front of the musicians. Before long, women take turns at the center of the circle, matching the energetic rhythms of the drumming in the exuberant hip twists, arm swings, jumps, and impossibly high knee kicks of sabar dancing. The high-spirited drumming and dancing continue until the early hours of the morning.&amp;nbsp; &amp;nbsp;&lt;/p&gt;  &lt;p&gt;The tanibeer is a chance for Touré “to show what he’s been teaching his American students and that they can really play sabar quite well,” says Tang, who serves as a faculty advisor to Rambax. “And that’s often a surprise to the Senegalese audience.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118121" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3196.jpg?w=2666" width="2666" /&gt;&lt;figcaption class="wp-element-caption"&gt;Senegalese drummers Sadda Sene, Mbaye Ndiaga Seck, and Pa Ali Konte load drums onto the Rambax van after drum practice.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;PATRICIA TANG&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118120" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3079.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Drum practice on the beach in Grand Mbao.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF RAMBAX&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Among the 16 Rambax members on the Senegal trip is Autumn Geil ’21, a researcher and PhD student in the department of mechanical engineering. Initially drawn to music through choir and opera singing in high school, she had never heard of sabar drumming before discovering Rambax through a friend as an undergrad. She joined and has been a member of the ensemble, which she calls “just so incredible,” ever since.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118114" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC04943.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Rambax MIT students Eri-ife Olayinka ’25 and Kaelyn Dunnell ’25 by the sea in Grand Mbao. &lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;For Geil, practicing sabar with custodians of the tradition in Senegal is an opportunity to “observe and learn from the drummers to improve my skills for future performances.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Baran Mensah ’24, a Ghanian master’s student in mechanical engineering who minored in music as an undergrad, also joined Rambax after a friend recommended it. He sees it as a way to tap into his African roots while at MIT but says it’s also “a gateway to learn about Senegalese art, music, and culture.” Until the tour, he notes, “I really didn’t know much about my country’s West African neighbor.”&amp;nbsp; &amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“Coming on this trip allows us to take a step back, to learn about people and cultures, making us more effective communicators.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Autumn Geil ’21&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;Eri-ife Olayinka ’25, a computation and cognition major who took Rambax classes for two semesters, says she finds the learning environment supportive and the cultural insights provided by Touré and Tang rewarding. “You see yourself getting better, becoming comfortable with playing in the class,” she says. After completing the classes to satisfy her art requirements, Olayinka stayed on in the drumming ensemble. “I genuinely enjoy being in Rambax—it’s such a cool thing we get to do as a group,” she says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118117" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC05202.jpg?w=1500" width="1500" /&gt;&lt;figcaption class="wp-element-caption"&gt;Sabar artists Badara Faye, Mbaye Ndiaga Seck, and Pa Ali Konte take the mic at a tanibeer in Grand Mbao&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Visiting Kaolack is more than an opportunity for Rambax members to glimpse the culture that gave rise to sabar. With horse-drawn wagons clip-clopping through its rugged terrain but also massive solar farms, Kaolack is a city where old meets new. Witnessing those contrasts—and getting to perform and to immerse themselves in the performances of local musicians—helps the students enhance what Geil calls the “human connection skills” that all scientists and technologists need.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It’s really important for people in STEM to make space for art and music,” she says. “Coming on this trip allows us to take a step back, to learn about people and cultures, making us more effective communicators of our technology.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118116" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC05176.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Grad student Natalie Huang ’24 and local musician Badara Faye dance at a tanibeer in Grand Mbao.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118115" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/DSC05019.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Rambax members watch Sengalese dancers and drummers at the tanibeer in Kaolack.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NIKO ODHIAMBO ’25&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118123" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3369.jpg?w=2667" width="2667" /&gt;&lt;figcaption class="wp-element-caption"&gt;Grad student Tina Chen ’24, Neha Basu ’25, Pa Ali Konte, Monique Brewster ’10, and grad student Sandra Huffman ’20, SM ’21, drum at the tanibeer in Kaolack.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;PATRICIA TANG&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118124" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/IMG_3371.jpg?w=2817" width="2817" /&gt;&lt;figcaption class="wp-element-caption"&gt;Rambax MIT plays at the tanibeer in Kaolack.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF RAMBAX&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Back inside Touré’s family compound, Tang invites the students to gather around so she can introduce them to Touré’s mother, Marie Sow, and his sisters and aunts. Sow showers them with good wishes and they bask in the glow.&lt;/p&gt;  &lt;p&gt;It’s important for Rambax members to know the history and culture of the people behind the music they practice, says Tang. “We really want the students to have this sort of cultural immersion—live in a Senegalese house like the Senegalese people do, hang out with Senegalese drummers, and really get a sense of what it’s like in Senegal.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Abdullahi Tsanni, SM ’23, a former &lt;/em&gt;MIT Technology Review&lt;em&gt; fellow, is a science writer based in Dakar, Senegal, who specializes in narrative features.&amp;nbsp;&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117703/travels-with-rambax/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] From MIT to low Earth orbit (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/24/1117698/from-mit-to-low-earth-orbit/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Not everyone can point to the specific moment that set them on their life’s course. But for me, there’s no question: It happened in 1982, when I was a junior at MIT, in the Infinite Corridor. In those pre-internet days, it was where we got the scoop about everything that was happening on campus. One day, as I was racing to the chemistry labs, a poster caught my eye.&lt;/p&gt;  &lt;p&gt;As I remember it, the poster showed a smiling woman in a flight suit, holding a helmet by her side. I recognized her immediately: Sally Ride, one of America’s first group of female astronauts. It had just been announced that she would be part of the crew for one of the upcoming space shuttle flights, making her the first American woman in space. And while she was visiting Lincoln Lab for training, she would be giving a speech and attending a reception hosted by the Association of MIT Alumnae. A woman speaker was still a novelty at MIT in those days. But a woman &lt;em&gt;astronaut&lt;/em&gt;? I knew this was one event I had to attend.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118013" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/T38_Grinning_9360033848_3189fa8e51_o.jpg?w=1549" /&gt;&lt;figcaption class="wp-element-caption"&gt;Coleman sits in the rear seat of a supersonic T-38 jet for pilot training as a newly minted NASA astronaut candidate in 1992. “When a chemist gets to fly a T-38, she will always be smiling,” she says.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;On the day of Sally Ride’s talk, I hurried into 10-250, the large lecture hall beneath the Great Dome that is the emblem of MIT. Sandy Yulke, the chair of the Association of MIT Alumnae, was already introducing Sally. Sally. Just a first name. As if she were one of us. I slid into an empty seat just a few rows back as Sandy talked about how proud she was to welcome the soon-to-be first American woman in space. And Sally was standing there, right where our professors stood every day. A woman. And an astronaut.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When I was growing up in the 1960s and ’70s, the image I’d had of astronauts—or any kind of explorer, for that matter—could not have been further from the figure before me that day. And I’m not just talking about images I saw in the media—I had one much closer to home. My dad—James Joseph Coleman, known as JJ—was a career naval officer who ultimately led the Experimental Diving Unit. A legend among Navy divers, he had also been a project officer for the Sealab program that built the first underwater habitats, allowing men—and it was all men at the time—to live and work in the deep seas for extended periods. The spirit of exploration, the desire to understand fascinating and challenging environments, seemed normal to me. But because none of the explorers I saw looked like me, it didn’t occur to me that I could be one. My dad worked in a male-dominated world where I’m sure very few of his colleagues imagined that people like me might belong too.&lt;/p&gt; 
 &lt;p&gt;By the time I got to MIT, in 1979, only six women had been selected as NASA astronauts. But seeing Sally Ride on the stage that day turned a possibility into a reality—a reality that could include &lt;em&gt;me&lt;/em&gt;. Instead of being larger than life, she was surprisingly real and relatable: a young, bright-eyed woman, with wavy brown hair kind of like mine, wearing a blue flight suit and black boots. She seemed a little shy, looking down at her hands as she was introduced and applauded.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Sally was obviously passionate about her scientific work—she was an accomplished astrophysicist—but she also had this amazing job where she flew jets, practiced spacewalking, and was part of a crew with a mission. Both scientist and adventurer, she was accomplishing something that no American woman ever had—and, in the process, opening the door for the rest of us. As I listened to her speak that day, an utterly unexpected idea popped into my head: &lt;em&gt;Maybe I—Cady Coleman—could have that job.&amp;nbsp;&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;If you can see it, you can be it. Representation doesn’t fix everything, but it changes, on a visceral level, the menu of options that you feel you can reach for. No matter how many people tell us we can be whatever we want to be—and my mother told me that from the moment I was old enough to understand—some of us need more than words. Representation matters. A lot. We are enormously influenced by the signals that we get from our surroundings. What do people expect of us? What models do we have? What limitations do we internalize without knowing it? In her quiet, matter-of-fact way, Sally Ride shattered assumptions I didn’t know I’d taken on. Like so many people at MIT, I was an explorer at heart. What if I could explore in space as well as in the lab?&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Becoming an astronaut&lt;/h3&gt;  &lt;p&gt;No one just becomes an astronaut. Every astronaut is something else first. At MIT, I had fallen in love with organic chemistry and was determined to become a research chemist, hoping to use science to improve people’s lives. Because I attended MIT on an ROTC scholarship, I was commissioned as a second lieutenant in the US Air Force upon graduation, but I was given permission to get my doctorate in polymer science and engineering from UMass Amherst before serving. I was then stationed at Wright-Patterson Air Force Base, where I worked on new materials for airplanes and consulted on NASA’s Long Duration Exposure Facility experiment. I also set endurance and tolerance records as a volunteer test subject in the centrifuge at the aeromedical laboratory, testing new equipment.&lt;/p&gt;  &lt;p&gt;But the ideas that Sally Ride had sparked were never far from my mind, and when NASA put out a call for new astronauts in 1991, I applied—along with 2,053 others. I was among the 500 who got our references checked, and then one of about 90 invited to Houston for an intense weeklong interview and physical. In 1992, after months of suspense, I got the fateful phone call asking, “Would you still like to come and work with us at NASA?” Thrilled beyond words, I felt a kind of validation I’d never experienced before and have never forgotten.&lt;/p&gt;  &lt;p&gt;Four months later, I reported for duty at the Johnson Space Center. Knowing that years of rigorous training lay ahead before I might launch into space on a mission, I couldn’t wait to dive in.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;That training turned out to be a wild ride. Within days of our arrival in Houston, we ASCANs (NASA-speak for astronaut candidates) headed to Fairchild Air Force Base in Washington state for land survival training. We practiced navigation skills and shelter building. Knots were tied. Food was scavenged. Worms were eaten. Tired, grubby people made hard decisions together. Rules were broken. Fun was had. And, importantly, we got to know one another. Water survival skills were next—we learned to disconnect from our parachutes, climb into a raft, and make the most of the supplies we had in case we had to eject from a jet or the space shuttle.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118011" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/KSC-99pp0921orig.jpg?w=1680" width="1680" /&gt;&lt;figcaption class="wp-element-caption"&gt;Coleman and the rest of the STS-93 crew head to Launch Pad 39-B for their second attempt at liftoff on the space shuttle Columbia. With this mission, Eileen M. Collins (front row, right) would become the first woman to serve as commander of a shuttle mission.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Back in Houston, we learned about each of the shuttle systems, studying the function of every switch and circuit breaker. (For perspective, the &lt;em&gt;condensed&lt;/em&gt; manual for the space shuttle is five inches thick.) The rule of thumb was that if something was important, then we probably had three, so we’d still be okay if two of them broke. We worked together in simulators (sims) to practice the normal procedures and learn how to react when the systems malfunctioned. For launch sims, even those normal procedures were an adventure, because the sim would shake, pitch, and roll just as the real shuttle could be expected to on launch day. We learned the basics of robotics, spacewalking, and rendezvous (how to dock with another spacecraft without colliding), and we spent time at the gym, often after hours, so we’d be in shape to work in heavy space suits.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Our training spanned everything from classes in how to use—and fix—the toilet in space to collecting meteorites in Antarctica, living in an underwater habitat, and learning to fly the T-38, an amazing high-performance acrobatic jet used to train Air Force pilots. (On our first training flight, we got to fly faster than the speed of sound.) All of this helped us develop an operational mindset—one geared to making decisions and solving problems in high-speed, high-pressure, real-risk ­situations that can’t be simulated, like the ones we might encounter in space.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Mission: It’s not about you, but it depends on you&lt;/h3&gt;  &lt;p&gt;Each time a crew of astronauts goes to space, we call it a mission. It’s an honor to be selected for a mission, and an acknowledgment that you bring skills thatwillmake it successful. Being part of a mission means you are part of something that’s bigger than yourself, but at the same time, the role you play is essential. It’s a strange paradox: It’s not about you, but it depends on you. On each of my missions, that sense of purpose brought us together, bridging our personal differences and disagreements and allowing us to achieve things we might never have thought possible. A crew typically spends at least a year, if not a few years, training together before the actual launch, and that shared mission connects us throughout.&lt;/p&gt; 

 &lt;p&gt;In 1993, I got word that I’d been assigned to my first mission aboard the space shuttle. As a mission specialist on STS-73, I would put my background as a research scientist to use byperforming 30 experiments in microgravity. These experiments, which included growing potatoes inside a locker (just like Matt Damon in &lt;em&gt;The Martian&lt;/em&gt;), using sound to manipulate large liquid droplets, and growing protein crystals, would advance our understanding of science, medicine, and engineering and help pave the way for the International Space Station laboratory.&lt;/p&gt;  &lt;p&gt;While training for STS-73, I got a call from an astronaut I greatly admired: Colonel Eileen Collins. One of the first female test pilots, she would become the first woman to pilot the space shuttle in 1995, when the STS-63 mission launched. Collins had invited some of her heroes—the seven surviving members of the Mercury 13—to attend the launch, and she was calling to ask me to help host them. The Mercury 13 were a group of 13 women who in the early 1960s had received personal letters from the head of life sciences at NASA asking them to be part of a privately funded program to include women as astronauts. They had accepted the challenge and undergone the same grueling physical tests required of NASA’s first astronauts. Although the women performed as well as or better than the Mercury 7 astronauts on the selection tests, which many of them had made sacrifices even to pursue, the program was abruptly shut down just days before they were scheduled to start the next phase of testing. It would be almost two decades before NASA selected its first female astronauts.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Never had I felt more acutely aware of being part of that lineage of brave and boundary-breaking women than I did that day, standing among those pioneers, watching Eileen make history. I can’t know what the Mercury 13 were thinking as they watched Eileen’s launch, but I sensed that they knew how much it meant to Eileen to be carrying their legacy with her in the pilot seat of that space shuttle.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Missions and malfunctions&lt;/h3&gt;  &lt;p&gt;Acouple of years after I had added my name to the still-too-short list of women who had flown in space, Eileen called again. This time she told me that I would be joining her on her next mission, ­STS-93, scheduled to launch in July 1999. Our Mercury 13 heroes would attend that launch too, and Eileen would be making history once again, this time as NASA’s first female space shuttle commander. I would be the lead mission specialist for delivering the shuttle’s precious payload, the Chandra X-ray Observatory, to orbit. I’d also be one of the EVA (extravehicular activity) crew members, if any spacewalking repairs were needed. &amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Our mission to launch the world’s most powerful x-ray telescope would change the world of astrophysics. With eight times the resolution of its predecessors and the ability to observe sources that were fainter by a factor of more than 20, Chandra was designed to detect x-rays from exploding stars, black holes, clusters of galaxies, and other high-energy sources throughout the universe. Because cosmic x-rays are absorbed by our atmosphere, we can’t study them from Earth, so an x-ray telescope must operate from well above our atmosphere. Chandra wouldlaunch into low Earth orbit on the shuttle and then require additional propulsion to achieve its final orbit, a third of the way to the moon.&lt;/p&gt;  &lt;p&gt;I was thrilled by the idea that my team and I would be launching a telescope whose work would continue long after we were back on Earth. Preparation for launch was intense. As Chandra’s shepherd, I needed to be able to perform what we called the deploy sequence in my sleep. And I had to have a close relationship with the folks at the Chandra Mission Control, which was separate from NASA Mission Control, and make sure the two groups were working together. In a very real sense, Chandra represented the future of astrophysics—a window that promised a deeper understanding of the universe. When the moment came for the telescope to be deployed, all of this would be, quite literally, in my hands.&lt;/p&gt;  &lt;p&gt;But first it was in the hands of the launch team at the Kennedy Space Center, whose job it was to get us off the ground and into orbit. And we almost didn’t make it.&lt;/p&gt;  &lt;p&gt;Our first launch attempt was aborted eight seconds before liftoff. There we were, waiting for the solid rocket boosters to ignite and the bolts holding us to the launchpad to explode. Instead, we heard “Abort for a hydrogen leak” from Launch Control. Later it was revealed that a faulty sensor had been the issue. &amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For our second attempt, we were confidently told we were “one hundred percent GO for weather.” In other words, there was not even a hint of bad weather to delay us. And then there were lightning strikes at the launchpad. Really.&lt;/p&gt;  &lt;p&gt;For our third launch attempt, under a bright moon on a cool, clear night, we strapped in and the countdown began. This time I was determined I wouldn’t take anything for granted—even in those final 30 seconds after control switched over to the shuttle’s internal computers. Even when the engines kicked in and I felt the &lt;em&gt;twang&lt;/em&gt; of the nose tipping forward and then back. Only when the solid rockets ignited did I let myself believe that we were actually heading back to space. As a seasoned second-time flyer, I kept my excitement contained, but inside I was whooping and hollering. And then, as &lt;em&gt;Columbia&lt;/em&gt; rolled to the heads-down position just seconds after liftoff, my joyful inner celebration was drowned out by an angry alert tone and Eileen’s voice on the radio:&lt;/p&gt; 
 &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;&lt;em&gt;Houston:&lt;/em&gt; Columbia &lt;em&gt;is in the roll and we have a fuel cell pH number one.&lt;/em&gt;&lt;/p&gt;    &lt;p&gt;Almost immediately, we got a response from the flight controllers in Houston:&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Columbia, &lt;em&gt;Houston: We’d like AC bus sensors to OFF. We see a transient short on AC1.&lt;/em&gt;&lt;/p&gt;    &lt;p&gt;It was incomprehensible to be hearing these words less than 30 seconds into our actual flight. An electrical short had taken out two of our six main engine controllers.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;My first thought: &lt;em&gt;We know how to deal with this. We did it last week in the simulator.&lt;/em&gt; But we weren’t in the simulator anymore. This was a real, no-shit emergency. After we returned to Earth we realized just how close we’d come to several actual life-or-death situations. No matter how much you train for just such a moment, you can’t really anticipate what it will mean to find yourself in one. I was relieved that it wasn’t long before I heard the steady voice of Jeff Ashby, our pilot, confirming that he had successfully flipped the bus sensor switches, reducing our exposure to the potential catastrophe of additional engine shutdowns.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="An image of the Space Shuttle taking off." class="wp-image-1119292" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/KSC-99pp0956large.jpg?w=1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;The Space Shuttle Columbia lifted off from Kennedy Space Center on July 23, 1999, for a five-day mission that would include releasing the Chandra X-ray Observatory.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;We were still headed to space, but with the loss of some of our backup capabilities, we were vulnerable. We carefully monitored the milestones that would tell us which options we still had. I tried not to hold my breath as the shuttle continued to climb and we listened for updates from Houston:&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;Columbia, &lt;em&gt;Houston: Two Engine Ben. &lt;/em&gt;Translation: We could lose an engine and still safely abort the mission and make it to our transatlantic landing site in Ben Guerir, Morocco.&lt;/p&gt;    &lt;p&gt;Columbia, &lt;em&gt;Houston&lt;/em&gt;:&lt;em&gt; Negative return. &lt;/em&gt;Translation: We were too far along to perform an RTLS (return to launch site) and head back to Florida.&lt;/p&gt;    &lt;p&gt;Then finally, the call we’d been wishing and waiting for:&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Columbia, &lt;em&gt;Houston&lt;/em&gt;:&lt;em&gt; PRESS TO MECO. &lt;/em&gt;Translation: We would make it to orbit and main engine cutoff even if one of our engines failed in the next few minutes.&lt;/p&gt;    &lt;p&gt;Now, assured of a safe orbit as we hurtled through space, we could turn our attention to our mission: sending Chandra off to its new home.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;An electrical short is a serious problem. After our mission landed, the shuttle fleet would be grounded for months after inspections revealed multiple cases of wire chafing on the other shuttles. Some would call us lucky, but listening to the audio from our cockpit and from Mission Control, I credit the well-trained teams that worked their way patiently through multiple failures catalyzed by the short and by a separate, equally dangerous issue: a slow leak in one of our three engines used during launch.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Our STS-93 launch would go down in the history books as the most dangerous ascent of the shuttle program that didn’t result in an accident. Even in the midst of it, my sense of mission helped anchor me.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118012" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/sts093-703-011large.jpg?w=1871" /&gt;&lt;figcaption class="wp-element-caption"&gt;The Chandra X-ray Observatory was deployed from the space shuttle Columbia’s payload bay on July 23, 1999, just a few hours after the shuttle’s arrival in Earth orbit.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The plan in 1999 had been that Chandra would last five years. But as of this writing, Chandra is 25 and still sending valuable data back from space. Each year, on its “birthday,” the crew from STS-93 and the teams who worked on the ground connect via email, or in person for the big ones. We’ll always share a bond from that mission and its continuing legacy. And what a legacy it is. Young astronomers who were still toddlers when I pulled that deploy switch are now making discoveries based on the data it’s produced. Chandra is responsible for almost everything that we now know about black holes, and it’s still advancing our understanding of the universe by giant leaps. But these are difficult times. Sadly, budget cuts proposed in 2025 would eliminate Chandra, with no replacements planned.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Suiting up and making change&amp;nbsp;&lt;/h3&gt;  &lt;p&gt;People often wonder what would possess any sane person to strap themself on top of a rocket. And by now you’re probably wondering why, after the harrowing malfunctions during the STS-93 launch, I was eager not only to return to space again but to spend six months living and working aboard the International Space Station. It comes back to mission. I don’t consider myself to be braver than most people, though I may be more optimistic than many. I take the risks associated with my job because I believe in what we’re doing together, and I trust my crew and our team to do all that’s humanly possible to keep us safe.&lt;/p&gt; 
 &lt;p&gt;But the odds were stacked against me in my quest to serve on the space station.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The world of space exploration, like so many others, is slow to change. Long-standing inequities were still evident when I joined NASA in 1992, and many endured during my time there. But it can be difficult to know when to fight for change at the outset and when to adapt to unfair circumstances to get your foot in the door.&lt;/p&gt;  &lt;p&gt;The first trained astronauts tended to be tall, athletic, and male—and the biases and assumptions that led to that preference were built into our equipment, especially our space suits. Our one-piece orange “pumpkin suits” worn for launching and landing weren’t designed for people with boobs or hips, so many of us wound up in baggy suits that made fitting a parachute harness tricky and uncomfortable. But fit issues with our 300-pound white spacewalking suits proved to be a much bigger problem, especially for the smaller-framed astronauts—including some men.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The bulky EVA suits, which allow astronauts who venture outside a spacecraft to breathe and communicate while regulating our temperature and protecting us from radiation, are essentially human-shaped spaceships. But while they came in small, medium, large, and extra-large, those suits were designed for (male) astronauts of the Apollo era with no thought to how they might work for different body types. Given that ill-fitting equipment would affect performance, astronauts like me—who weren’t shaped like Neil Armstrong, Buzz Aldrin, and their compatriots—were often negatively prejudged before we even started training. As a result, NASA failed for years to leverage the skills of many members of the astronaut corps who physically didn’t fit an institutional template that hadn’t been redesigned for half a century.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Spacewalk training was the most physically difficult thing I did as an astronaut. Training in that way-too-large space suit made it even harder, forcing me to find ways to optimize my ability to function. &amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118008" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/98_08195.jpg?w=1791" /&gt;&lt;figcaption class="wp-element-caption"&gt;As she prepares to head into the pool for EVA training, Coleman dons glove liners. Next, the bottom of her suit will be attached to the top and her gloves will be attached at the wrist ring, locked, and tested for a solid seal. Coleman qualified as a spacewalker for all of her missions, even when that required doing so in a medium suit that was much too big.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;We practice spacewalking underwater in an enormous swimming pool. If the suit is too big for you—as even the small was for me—the extra volume of air inside drags you up to the surface when you’re trying to work underwater. It’s a profound physical disadvantage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Though the fit of the small spacewalking suit wasn’t great, I persevered and adapted, training for many years in that suit with above-average spacewalking grades. And I was chosen to serve as a spacewalker for both of my shuttle missions, should the need arise. Not long before my first mission, Tom Akers, one of the experienced spacewalkers, came up to me and said, “Cady, I can see that you have a real aptitude for spacewalking and also a head that thinks like a spacewalker.” But then he told me that to cut costs, NASA had decided not to use the small suits on the space station. “People are going to look at you and think you’re too small, but I think someone like you could learn to function inside a medium suit,” he said. “So my advice is this: If you are interested in flying on the space station, then when someone asks you what size suit you wear, you tell them a medium will be no problem.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Sure enough, after my second shuttle flight, NASA announced that the small suit would be eliminated. I’ve never forgotten the wording of the rationale: “We’ve looked ahead at the manifest, and we have all of the spacewalkers that we need.” Implied was that they wouldn’t miss the smaller astronauts—not a bit.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I think people might not have understood at the time what it meant to get rid of those small space suits. You could not live and work on the space station unless you were space-suit qualified. And because NASA was about to shut down the shuttle program, soon missions to the space station would be the only ones there were. NASA’s decision to eliminate the small suit effectively grounded more than a third of female astronauts. It also meant that few women would have the experience needed to serve in positions where they could have a say in important decisions about everything from prioritizing missions and choosing crews to making changes in NASA’s culture. &amp;nbsp; &amp;nbsp;&lt;/p&gt;  &lt;p&gt;To me, eliminating the small space suit indicated that the organization didn’t understand the value of having teams whose members contribute a wide range of experiences and viewpoints. When team members are too much alike—in background, ways of thinking and seeing the world, and, yes, gender—the teams are often less effective at finding innovative solutions to complex problems.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Determined to contribute to the important scientific work being done on the space station, I had no choice but to qualify in the medium suit. But it would be a tall order because for the instructors, the gear is seldom at fault. You just need to get used to it, understand it better, or practice more. I did all three—but it wasn’t enough. So I also adapted everywhere I could, and I recruited a lot of great help. Kathy Thornton, one of the first female spacewalkers, recommended that I buy a waterskiing vest at Walmart to wear inside the suit. The space-suit team was horrified at the thought of using nonregulation materials, but it got them thinking. Together, we settled on having me wear a large girdle—left over from the Apollo guys—and stuffing it with NASA-approved foam to center me in the suit. This kept the air pockets more evenly distributed and allowed me to practice the required tasks, showing that I could work effectively in a medium.&lt;/p&gt;  &lt;p&gt;By adapting, which sometimes means staying silent, you may perpetuate a discriminatory system. But if I’d tried to speak the truth from day one, I’d never have made it to the day when I was taken seriously enough to start conversations about the importance of providing all astronauts with equipment that fits. I needed to launch those discussions from a place of strength, where I could be heard and make a difference.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;How best to catalyze change is always a personal decision. Sometimes drawing a line in the sand is the most effective strategy. Other times, you have to master the ill-fitting equipment before you get a chance to redesign it. Qualifying in the too-large suit was my only option if I wanted to fly on the International Space Station, since every flight to the ISS needed two spacewalkers and a backup spacewalker—and there were only three seats in the space capsule. The alternative would have been waiting at least 11 years for the newer spacecraft, which would have a fourth seat. I had to play by the unfair rules in order to get to a point where I could change those rules.&lt;/p&gt;  &lt;p&gt;With grit and a lot of support from others, I did end up qualifying in the medium suit. And in 2010, I set off for the International Space Station, serving as the lead robotics and science officer for Expedition 26/27 as I traveled 63,345,600 miles in 2,544 orbits over 159 days in space.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118015" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/Cady-performs-Fluid-Experiment-in-space-horizontal-iss026e018762-cropped.jpg?w=2686" width="2686" /&gt;&lt;figcaption class="wp-element-caption"&gt;Coleman conducts the Capillary Flow Experiment on the International Space Station to study the behavior of liquids at interfaces in microgravity.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NASA/PAOLO NESPOLI&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Today, efforts are underway to redesign NASA’s space suits to fit the full range of sizes represented in the astronaut corps. Because of the work I put in to make it possible for a wider range of people to excel as spacewalkers, NASA hung a portrait of me in the row of space-suit photos outside the women’s locker room. And I’m proud to know that my colleagues—women and men—are continuing the work of making change at NASA. Every change has been hard won. The numbers matter. The astronaut corps is now 40% women. Given that, it is harder to make decisions with the potential to leave women out. When a female NASA astronaut walks on the moon for the first time, she will do so in a redesigned space suit. I hope it fits her like a glove.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The crew of spaceship Earth&lt;/h3&gt;  &lt;p&gt;Contributing to an important mission is a privilege. But who gets to contribute is as important to mission success as it is to the individuals who want to play a part. I can’t emphasize enough how much our incredibly complex NASA missions have benefited from the broad range of people involved. Bringing together people of different backgrounds and skills, with different ways of seeing the world and unique perspectives on opportunities and problems, is what makes space exploration possible.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118016" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/White-House-Science-Fair-NHQ201604130001-26415638785_ba47b4143e_o.jpg?w=2709" width="2709" /&gt;&lt;figcaption class="wp-element-caption"&gt;At the White House Science Fair in 2016, Coleman sits with the “Supergirls” Junior FIRST Lego League Team from Girl Scout Troop 411 in Tulsa, Oklahoma, as they await the arrival of President Barack Obama&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NASA/JOEL KOWSKY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Sharing space, to me, means including more people—both in the privilege of going to space and in so many of our endeavors here on Earth. When I applied to be an astronaut, very few women had orbited our planet. Today, that number has grown to 82 of 633 human beings in total, and newer NASA astronaut classes have averaged 50% women. Spaceflight is making progress in terms of including people with a wider range of backgrounds, fields of expertise, and physical abilities. But we have a long way to go. And the same is true in countless fields—the barriers that we struggle with in space exploration seem to be ubiquitous in the working world.&lt;/p&gt;  &lt;p&gt;As a planet, we’re facing enormous challenges, in areas from climate change to public health to how to sustainably power our endeavors. If there’s one thing I learned above all else from my time in space, it’s that we’re all sharing Earth. No one else is coming to solve our complex problems. And we won’t find solutions with teams of people who share too much in common. We need &lt;em&gt;everyone&lt;/em&gt; to contribute where they can, and so we need to create systems, environments, and equipment that make that possible. And we need to be sure that those making contributions are visible, so they can serve as models for future generations. Our job now is to make sure everyone gets enough support to acquire the skills that we—all of us—need to build collaborative teams and solve problems both on Earth and in space.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s worth repeating: We’re &lt;em&gt;all &lt;/em&gt;sharing Earth. Looking down from space, you see very few borders separating humans from one another. You understand—not as an abstract ideal but as a visceral, obvious reality—that we are one family sharing a precious, life-supporting home&lt;em&gt;. &lt;/em&gt;It’s so clear from up there that we are all the crew of “Spaceship Earth.” I believe that sharing that perspective, bringing it to life, will help more people see that our differences matter less than what binds us together, and motivate us to combine our efforts to tackle the challenges affecting all of us.&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;In her 24 years at NASA, Cady Coleman ’83, a scientist, musician, and mother of two, flew on two space shuttle missions and began her 159-day mission aboard the International Space Station the day after turning 50. Today, as a public speaker and consultant, she shares her insights on leadership and teamwork gleaned from the high-stakes world of space exploration.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947 gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 core/columns_13"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Sharing Space cover" class="wp-image-1118009" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/Cadys-Book-Cover-JPG.jpg?w=1324" width="1324" /&gt;&lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Not everyone can point to the specific moment that set them on their life’s course. But for me, there’s no question: It happened in 1982, when I was a junior at MIT, in the Infinite Corridor. In those pre-internet days, it was where we got the scoop about everything that was happening on campus. One day, as I was racing to the chemistry labs, a poster caught my eye.&lt;/p&gt;  &lt;p&gt;As I remember it, the poster showed a smiling woman in a flight suit, holding a helmet by her side. I recognized her immediately: Sally Ride, one of America’s first group of female astronauts. It had just been announced that she would be part of the crew for one of the upcoming space shuttle flights, making her the first American woman in space. And while she was visiting Lincoln Lab for training, she would be giving a speech and attending a reception hosted by the Association of MIT Alumnae. A woman speaker was still a novelty at MIT in those days. But a woman &lt;em&gt;astronaut&lt;/em&gt;? I knew this was one event I had to attend.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118013" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/T38_Grinning_9360033848_3189fa8e51_o.jpg?w=1549" /&gt;&lt;figcaption class="wp-element-caption"&gt;Coleman sits in the rear seat of a supersonic T-38 jet for pilot training as a newly minted NASA astronaut candidate in 1992. “When a chemist gets to fly a T-38, she will always be smiling,” she says.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;On the day of Sally Ride’s talk, I hurried into 10-250, the large lecture hall beneath the Great Dome that is the emblem of MIT. Sandy Yulke, the chair of the Association of MIT Alumnae, was already introducing Sally. Sally. Just a first name. As if she were one of us. I slid into an empty seat just a few rows back as Sandy talked about how proud she was to welcome the soon-to-be first American woman in space. And Sally was standing there, right where our professors stood every day. A woman. And an astronaut.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When I was growing up in the 1960s and ’70s, the image I’d had of astronauts—or any kind of explorer, for that matter—could not have been further from the figure before me that day. And I’m not just talking about images I saw in the media—I had one much closer to home. My dad—James Joseph Coleman, known as JJ—was a career naval officer who ultimately led the Experimental Diving Unit. A legend among Navy divers, he had also been a project officer for the Sealab program that built the first underwater habitats, allowing men—and it was all men at the time—to live and work in the deep seas for extended periods. The spirit of exploration, the desire to understand fascinating and challenging environments, seemed normal to me. But because none of the explorers I saw looked like me, it didn’t occur to me that I could be one. My dad worked in a male-dominated world where I’m sure very few of his colleagues imagined that people like me might belong too.&lt;/p&gt; 
 &lt;p&gt;By the time I got to MIT, in 1979, only six women had been selected as NASA astronauts. But seeing Sally Ride on the stage that day turned a possibility into a reality—a reality that could include &lt;em&gt;me&lt;/em&gt;. Instead of being larger than life, she was surprisingly real and relatable: a young, bright-eyed woman, with wavy brown hair kind of like mine, wearing a blue flight suit and black boots. She seemed a little shy, looking down at her hands as she was introduced and applauded.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Sally was obviously passionate about her scientific work—she was an accomplished astrophysicist—but she also had this amazing job where she flew jets, practiced spacewalking, and was part of a crew with a mission. Both scientist and adventurer, she was accomplishing something that no American woman ever had—and, in the process, opening the door for the rest of us. As I listened to her speak that day, an utterly unexpected idea popped into my head: &lt;em&gt;Maybe I—Cady Coleman—could have that job.&amp;nbsp;&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;If you can see it, you can be it. Representation doesn’t fix everything, but it changes, on a visceral level, the menu of options that you feel you can reach for. No matter how many people tell us we can be whatever we want to be—and my mother told me that from the moment I was old enough to understand—some of us need more than words. Representation matters. A lot. We are enormously influenced by the signals that we get from our surroundings. What do people expect of us? What models do we have? What limitations do we internalize without knowing it? In her quiet, matter-of-fact way, Sally Ride shattered assumptions I didn’t know I’d taken on. Like so many people at MIT, I was an explorer at heart. What if I could explore in space as well as in the lab?&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Becoming an astronaut&lt;/h3&gt;  &lt;p&gt;No one just becomes an astronaut. Every astronaut is something else first. At MIT, I had fallen in love with organic chemistry and was determined to become a research chemist, hoping to use science to improve people’s lives. Because I attended MIT on an ROTC scholarship, I was commissioned as a second lieutenant in the US Air Force upon graduation, but I was given permission to get my doctorate in polymer science and engineering from UMass Amherst before serving. I was then stationed at Wright-Patterson Air Force Base, where I worked on new materials for airplanes and consulted on NASA’s Long Duration Exposure Facility experiment. I also set endurance and tolerance records as a volunteer test subject in the centrifuge at the aeromedical laboratory, testing new equipment.&lt;/p&gt;  &lt;p&gt;But the ideas that Sally Ride had sparked were never far from my mind, and when NASA put out a call for new astronauts in 1991, I applied—along with 2,053 others. I was among the 500 who got our references checked, and then one of about 90 invited to Houston for an intense weeklong interview and physical. In 1992, after months of suspense, I got the fateful phone call asking, “Would you still like to come and work with us at NASA?” Thrilled beyond words, I felt a kind of validation I’d never experienced before and have never forgotten.&lt;/p&gt;  &lt;p&gt;Four months later, I reported for duty at the Johnson Space Center. Knowing that years of rigorous training lay ahead before I might launch into space on a mission, I couldn’t wait to dive in.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;That training turned out to be a wild ride. Within days of our arrival in Houston, we ASCANs (NASA-speak for astronaut candidates) headed to Fairchild Air Force Base in Washington state for land survival training. We practiced navigation skills and shelter building. Knots were tied. Food was scavenged. Worms were eaten. Tired, grubby people made hard decisions together. Rules were broken. Fun was had. And, importantly, we got to know one another. Water survival skills were next—we learned to disconnect from our parachutes, climb into a raft, and make the most of the supplies we had in case we had to eject from a jet or the space shuttle.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118011" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/KSC-99pp0921orig.jpg?w=1680" width="1680" /&gt;&lt;figcaption class="wp-element-caption"&gt;Coleman and the rest of the STS-93 crew head to Launch Pad 39-B for their second attempt at liftoff on the space shuttle Columbia. With this mission, Eileen M. Collins (front row, right) would become the first woman to serve as commander of a shuttle mission.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Back in Houston, we learned about each of the shuttle systems, studying the function of every switch and circuit breaker. (For perspective, the &lt;em&gt;condensed&lt;/em&gt; manual for the space shuttle is five inches thick.) The rule of thumb was that if something was important, then we probably had three, so we’d still be okay if two of them broke. We worked together in simulators (sims) to practice the normal procedures and learn how to react when the systems malfunctioned. For launch sims, even those normal procedures were an adventure, because the sim would shake, pitch, and roll just as the real shuttle could be expected to on launch day. We learned the basics of robotics, spacewalking, and rendezvous (how to dock with another spacecraft without colliding), and we spent time at the gym, often after hours, so we’d be in shape to work in heavy space suits.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Our training spanned everything from classes in how to use—and fix—the toilet in space to collecting meteorites in Antarctica, living in an underwater habitat, and learning to fly the T-38, an amazing high-performance acrobatic jet used to train Air Force pilots. (On our first training flight, we got to fly faster than the speed of sound.) All of this helped us develop an operational mindset—one geared to making decisions and solving problems in high-speed, high-pressure, real-risk ­situations that can’t be simulated, like the ones we might encounter in space.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Mission: It’s not about you, but it depends on you&lt;/h3&gt;  &lt;p&gt;Each time a crew of astronauts goes to space, we call it a mission. It’s an honor to be selected for a mission, and an acknowledgment that you bring skills thatwillmake it successful. Being part of a mission means you are part of something that’s bigger than yourself, but at the same time, the role you play is essential. It’s a strange paradox: It’s not about you, but it depends on you. On each of my missions, that sense of purpose brought us together, bridging our personal differences and disagreements and allowing us to achieve things we might never have thought possible. A crew typically spends at least a year, if not a few years, training together before the actual launch, and that shared mission connects us throughout.&lt;/p&gt; 

 &lt;p&gt;In 1993, I got word that I’d been assigned to my first mission aboard the space shuttle. As a mission specialist on STS-73, I would put my background as a research scientist to use byperforming 30 experiments in microgravity. These experiments, which included growing potatoes inside a locker (just like Matt Damon in &lt;em&gt;The Martian&lt;/em&gt;), using sound to manipulate large liquid droplets, and growing protein crystals, would advance our understanding of science, medicine, and engineering and help pave the way for the International Space Station laboratory.&lt;/p&gt;  &lt;p&gt;While training for STS-73, I got a call from an astronaut I greatly admired: Colonel Eileen Collins. One of the first female test pilots, she would become the first woman to pilot the space shuttle in 1995, when the STS-63 mission launched. Collins had invited some of her heroes—the seven surviving members of the Mercury 13—to attend the launch, and she was calling to ask me to help host them. The Mercury 13 were a group of 13 women who in the early 1960s had received personal letters from the head of life sciences at NASA asking them to be part of a privately funded program to include women as astronauts. They had accepted the challenge and undergone the same grueling physical tests required of NASA’s first astronauts. Although the women performed as well as or better than the Mercury 7 astronauts on the selection tests, which many of them had made sacrifices even to pursue, the program was abruptly shut down just days before they were scheduled to start the next phase of testing. It would be almost two decades before NASA selected its first female astronauts.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Never had I felt more acutely aware of being part of that lineage of brave and boundary-breaking women than I did that day, standing among those pioneers, watching Eileen make history. I can’t know what the Mercury 13 were thinking as they watched Eileen’s launch, but I sensed that they knew how much it meant to Eileen to be carrying their legacy with her in the pilot seat of that space shuttle.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Missions and malfunctions&lt;/h3&gt;  &lt;p&gt;Acouple of years after I had added my name to the still-too-short list of women who had flown in space, Eileen called again. This time she told me that I would be joining her on her next mission, ­STS-93, scheduled to launch in July 1999. Our Mercury 13 heroes would attend that launch too, and Eileen would be making history once again, this time as NASA’s first female space shuttle commander. I would be the lead mission specialist for delivering the shuttle’s precious payload, the Chandra X-ray Observatory, to orbit. I’d also be one of the EVA (extravehicular activity) crew members, if any spacewalking repairs were needed. &amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Our mission to launch the world’s most powerful x-ray telescope would change the world of astrophysics. With eight times the resolution of its predecessors and the ability to observe sources that were fainter by a factor of more than 20, Chandra was designed to detect x-rays from exploding stars, black holes, clusters of galaxies, and other high-energy sources throughout the universe. Because cosmic x-rays are absorbed by our atmosphere, we can’t study them from Earth, so an x-ray telescope must operate from well above our atmosphere. Chandra wouldlaunch into low Earth orbit on the shuttle and then require additional propulsion to achieve its final orbit, a third of the way to the moon.&lt;/p&gt;  &lt;p&gt;I was thrilled by the idea that my team and I would be launching a telescope whose work would continue long after we were back on Earth. Preparation for launch was intense. As Chandra’s shepherd, I needed to be able to perform what we called the deploy sequence in my sleep. And I had to have a close relationship with the folks at the Chandra Mission Control, which was separate from NASA Mission Control, and make sure the two groups were working together. In a very real sense, Chandra represented the future of astrophysics—a window that promised a deeper understanding of the universe. When the moment came for the telescope to be deployed, all of this would be, quite literally, in my hands.&lt;/p&gt;  &lt;p&gt;But first it was in the hands of the launch team at the Kennedy Space Center, whose job it was to get us off the ground and into orbit. And we almost didn’t make it.&lt;/p&gt;  &lt;p&gt;Our first launch attempt was aborted eight seconds before liftoff. There we were, waiting for the solid rocket boosters to ignite and the bolts holding us to the launchpad to explode. Instead, we heard “Abort for a hydrogen leak” from Launch Control. Later it was revealed that a faulty sensor had been the issue. &amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For our second attempt, we were confidently told we were “one hundred percent GO for weather.” In other words, there was not even a hint of bad weather to delay us. And then there were lightning strikes at the launchpad. Really.&lt;/p&gt;  &lt;p&gt;For our third launch attempt, under a bright moon on a cool, clear night, we strapped in and the countdown began. This time I was determined I wouldn’t take anything for granted—even in those final 30 seconds after control switched over to the shuttle’s internal computers. Even when the engines kicked in and I felt the &lt;em&gt;twang&lt;/em&gt; of the nose tipping forward and then back. Only when the solid rockets ignited did I let myself believe that we were actually heading back to space. As a seasoned second-time flyer, I kept my excitement contained, but inside I was whooping and hollering. And then, as &lt;em&gt;Columbia&lt;/em&gt; rolled to the heads-down position just seconds after liftoff, my joyful inner celebration was drowned out by an angry alert tone and Eileen’s voice on the radio:&lt;/p&gt; 
 &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;&lt;em&gt;Houston:&lt;/em&gt; Columbia &lt;em&gt;is in the roll and we have a fuel cell pH number one.&lt;/em&gt;&lt;/p&gt;    &lt;p&gt;Almost immediately, we got a response from the flight controllers in Houston:&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Columbia, &lt;em&gt;Houston: We’d like AC bus sensors to OFF. We see a transient short on AC1.&lt;/em&gt;&lt;/p&gt;    &lt;p&gt;It was incomprehensible to be hearing these words less than 30 seconds into our actual flight. An electrical short had taken out two of our six main engine controllers.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;My first thought: &lt;em&gt;We know how to deal with this. We did it last week in the simulator.&lt;/em&gt; But we weren’t in the simulator anymore. This was a real, no-shit emergency. After we returned to Earth we realized just how close we’d come to several actual life-or-death situations. No matter how much you train for just such a moment, you can’t really anticipate what it will mean to find yourself in one. I was relieved that it wasn’t long before I heard the steady voice of Jeff Ashby, our pilot, confirming that he had successfully flipped the bus sensor switches, reducing our exposure to the potential catastrophe of additional engine shutdowns.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="An image of the Space Shuttle taking off." class="wp-image-1119292" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/KSC-99pp0956large.jpg?w=1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;The Space Shuttle Columbia lifted off from Kennedy Space Center on July 23, 1999, for a five-day mission that would include releasing the Chandra X-ray Observatory.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;We were still headed to space, but with the loss of some of our backup capabilities, we were vulnerable. We carefully monitored the milestones that would tell us which options we still had. I tried not to hold my breath as the shuttle continued to climb and we listened for updates from Houston:&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;Columbia, &lt;em&gt;Houston: Two Engine Ben. &lt;/em&gt;Translation: We could lose an engine and still safely abort the mission and make it to our transatlantic landing site in Ben Guerir, Morocco.&lt;/p&gt;    &lt;p&gt;Columbia, &lt;em&gt;Houston&lt;/em&gt;:&lt;em&gt; Negative return. &lt;/em&gt;Translation: We were too far along to perform an RTLS (return to launch site) and head back to Florida.&lt;/p&gt;    &lt;p&gt;Then finally, the call we’d been wishing and waiting for:&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Columbia, &lt;em&gt;Houston&lt;/em&gt;:&lt;em&gt; PRESS TO MECO. &lt;/em&gt;Translation: We would make it to orbit and main engine cutoff even if one of our engines failed in the next few minutes.&lt;/p&gt;    &lt;p&gt;Now, assured of a safe orbit as we hurtled through space, we could turn our attention to our mission: sending Chandra off to its new home.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;An electrical short is a serious problem. After our mission landed, the shuttle fleet would be grounded for months after inspections revealed multiple cases of wire chafing on the other shuttles. Some would call us lucky, but listening to the audio from our cockpit and from Mission Control, I credit the well-trained teams that worked their way patiently through multiple failures catalyzed by the short and by a separate, equally dangerous issue: a slow leak in one of our three engines used during launch.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Our STS-93 launch would go down in the history books as the most dangerous ascent of the shuttle program that didn’t result in an accident. Even in the midst of it, my sense of mission helped anchor me.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118012" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/sts093-703-011large.jpg?w=1871" /&gt;&lt;figcaption class="wp-element-caption"&gt;The Chandra X-ray Observatory was deployed from the space shuttle Columbia’s payload bay on July 23, 1999, just a few hours after the shuttle’s arrival in Earth orbit.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The plan in 1999 had been that Chandra would last five years. But as of this writing, Chandra is 25 and still sending valuable data back from space. Each year, on its “birthday,” the crew from STS-93 and the teams who worked on the ground connect via email, or in person for the big ones. We’ll always share a bond from that mission and its continuing legacy. And what a legacy it is. Young astronomers who were still toddlers when I pulled that deploy switch are now making discoveries based on the data it’s produced. Chandra is responsible for almost everything that we now know about black holes, and it’s still advancing our understanding of the universe by giant leaps. But these are difficult times. Sadly, budget cuts proposed in 2025 would eliminate Chandra, with no replacements planned.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Suiting up and making change&amp;nbsp;&lt;/h3&gt;  &lt;p&gt;People often wonder what would possess any sane person to strap themself on top of a rocket. And by now you’re probably wondering why, after the harrowing malfunctions during the STS-93 launch, I was eager not only to return to space again but to spend six months living and working aboard the International Space Station. It comes back to mission. I don’t consider myself to be braver than most people, though I may be more optimistic than many. I take the risks associated with my job because I believe in what we’re doing together, and I trust my crew and our team to do all that’s humanly possible to keep us safe.&lt;/p&gt; 
 &lt;p&gt;But the odds were stacked against me in my quest to serve on the space station.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The world of space exploration, like so many others, is slow to change. Long-standing inequities were still evident when I joined NASA in 1992, and many endured during my time there. But it can be difficult to know when to fight for change at the outset and when to adapt to unfair circumstances to get your foot in the door.&lt;/p&gt;  &lt;p&gt;The first trained astronauts tended to be tall, athletic, and male—and the biases and assumptions that led to that preference were built into our equipment, especially our space suits. Our one-piece orange “pumpkin suits” worn for launching and landing weren’t designed for people with boobs or hips, so many of us wound up in baggy suits that made fitting a parachute harness tricky and uncomfortable. But fit issues with our 300-pound white spacewalking suits proved to be a much bigger problem, especially for the smaller-framed astronauts—including some men.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The bulky EVA suits, which allow astronauts who venture outside a spacecraft to breathe and communicate while regulating our temperature and protecting us from radiation, are essentially human-shaped spaceships. But while they came in small, medium, large, and extra-large, those suits were designed for (male) astronauts of the Apollo era with no thought to how they might work for different body types. Given that ill-fitting equipment would affect performance, astronauts like me—who weren’t shaped like Neil Armstrong, Buzz Aldrin, and their compatriots—were often negatively prejudged before we even started training. As a result, NASA failed for years to leverage the skills of many members of the astronaut corps who physically didn’t fit an institutional template that hadn’t been redesigned for half a century.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Spacewalk training was the most physically difficult thing I did as an astronaut. Training in that way-too-large space suit made it even harder, forcing me to find ways to optimize my ability to function. &amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118008" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/98_08195.jpg?w=1791" /&gt;&lt;figcaption class="wp-element-caption"&gt;As she prepares to head into the pool for EVA training, Coleman dons glove liners. Next, the bottom of her suit will be attached to the top and her gloves will be attached at the wrist ring, locked, and tested for a solid seal. Coleman qualified as a spacewalker for all of her missions, even when that required doing so in a medium suit that was much too big.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;We practice spacewalking underwater in an enormous swimming pool. If the suit is too big for you—as even the small was for me—the extra volume of air inside drags you up to the surface when you’re trying to work underwater. It’s a profound physical disadvantage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Though the fit of the small spacewalking suit wasn’t great, I persevered and adapted, training for many years in that suit with above-average spacewalking grades. And I was chosen to serve as a spacewalker for both of my shuttle missions, should the need arise. Not long before my first mission, Tom Akers, one of the experienced spacewalkers, came up to me and said, “Cady, I can see that you have a real aptitude for spacewalking and also a head that thinks like a spacewalker.” But then he told me that to cut costs, NASA had decided not to use the small suits on the space station. “People are going to look at you and think you’re too small, but I think someone like you could learn to function inside a medium suit,” he said. “So my advice is this: If you are interested in flying on the space station, then when someone asks you what size suit you wear, you tell them a medium will be no problem.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Sure enough, after my second shuttle flight, NASA announced that the small suit would be eliminated. I’ve never forgotten the wording of the rationale: “We’ve looked ahead at the manifest, and we have all of the spacewalkers that we need.” Implied was that they wouldn’t miss the smaller astronauts—not a bit.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I think people might not have understood at the time what it meant to get rid of those small space suits. You could not live and work on the space station unless you were space-suit qualified. And because NASA was about to shut down the shuttle program, soon missions to the space station would be the only ones there were. NASA’s decision to eliminate the small suit effectively grounded more than a third of female astronauts. It also meant that few women would have the experience needed to serve in positions where they could have a say in important decisions about everything from prioritizing missions and choosing crews to making changes in NASA’s culture. &amp;nbsp; &amp;nbsp;&lt;/p&gt;  &lt;p&gt;To me, eliminating the small space suit indicated that the organization didn’t understand the value of having teams whose members contribute a wide range of experiences and viewpoints. When team members are too much alike—in background, ways of thinking and seeing the world, and, yes, gender—the teams are often less effective at finding innovative solutions to complex problems.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Determined to contribute to the important scientific work being done on the space station, I had no choice but to qualify in the medium suit. But it would be a tall order because for the instructors, the gear is seldom at fault. You just need to get used to it, understand it better, or practice more. I did all three—but it wasn’t enough. So I also adapted everywhere I could, and I recruited a lot of great help. Kathy Thornton, one of the first female spacewalkers, recommended that I buy a waterskiing vest at Walmart to wear inside the suit. The space-suit team was horrified at the thought of using nonregulation materials, but it got them thinking. Together, we settled on having me wear a large girdle—left over from the Apollo guys—and stuffing it with NASA-approved foam to center me in the suit. This kept the air pockets more evenly distributed and allowed me to practice the required tasks, showing that I could work effectively in a medium.&lt;/p&gt;  &lt;p&gt;By adapting, which sometimes means staying silent, you may perpetuate a discriminatory system. But if I’d tried to speak the truth from day one, I’d never have made it to the day when I was taken seriously enough to start conversations about the importance of providing all astronauts with equipment that fits. I needed to launch those discussions from a place of strength, where I could be heard and make a difference.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;How best to catalyze change is always a personal decision. Sometimes drawing a line in the sand is the most effective strategy. Other times, you have to master the ill-fitting equipment before you get a chance to redesign it. Qualifying in the too-large suit was my only option if I wanted to fly on the International Space Station, since every flight to the ISS needed two spacewalkers and a backup spacewalker—and there were only three seats in the space capsule. The alternative would have been waiting at least 11 years for the newer spacecraft, which would have a fourth seat. I had to play by the unfair rules in order to get to a point where I could change those rules.&lt;/p&gt;  &lt;p&gt;With grit and a lot of support from others, I did end up qualifying in the medium suit. And in 2010, I set off for the International Space Station, serving as the lead robotics and science officer for Expedition 26/27 as I traveled 63,345,600 miles in 2,544 orbits over 159 days in space.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118015" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/Cady-performs-Fluid-Experiment-in-space-horizontal-iss026e018762-cropped.jpg?w=2686" width="2686" /&gt;&lt;figcaption class="wp-element-caption"&gt;Coleman conducts the Capillary Flow Experiment on the International Space Station to study the behavior of liquids at interfaces in microgravity.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NASA/PAOLO NESPOLI&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Today, efforts are underway to redesign NASA’s space suits to fit the full range of sizes represented in the astronaut corps. Because of the work I put in to make it possible for a wider range of people to excel as spacewalkers, NASA hung a portrait of me in the row of space-suit photos outside the women’s locker room. And I’m proud to know that my colleagues—women and men—are continuing the work of making change at NASA. Every change has been hard won. The numbers matter. The astronaut corps is now 40% women. Given that, it is harder to make decisions with the potential to leave women out. When a female NASA astronaut walks on the moon for the first time, she will do so in a redesigned space suit. I hope it fits her like a glove.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The crew of spaceship Earth&lt;/h3&gt;  &lt;p&gt;Contributing to an important mission is a privilege. But who gets to contribute is as important to mission success as it is to the individuals who want to play a part. I can’t emphasize enough how much our incredibly complex NASA missions have benefited from the broad range of people involved. Bringing together people of different backgrounds and skills, with different ways of seeing the world and unique perspectives on opportunities and problems, is what makes space exploration possible.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1118016" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/White-House-Science-Fair-NHQ201604130001-26415638785_ba47b4143e_o.jpg?w=2709" width="2709" /&gt;&lt;figcaption class="wp-element-caption"&gt;At the White House Science Fair in 2016, Coleman sits with the “Supergirls” Junior FIRST Lego League Team from Girl Scout Troop 411 in Tulsa, Oklahoma, as they await the arrival of President Barack Obama&lt;/figcaption&gt;&lt;div class="image-credit"&gt;NASA/JOEL KOWSKY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Sharing space, to me, means including more people—both in the privilege of going to space and in so many of our endeavors here on Earth. When I applied to be an astronaut, very few women had orbited our planet. Today, that number has grown to 82 of 633 human beings in total, and newer NASA astronaut classes have averaged 50% women. Spaceflight is making progress in terms of including people with a wider range of backgrounds, fields of expertise, and physical abilities. But we have a long way to go. And the same is true in countless fields—the barriers that we struggle with in space exploration seem to be ubiquitous in the working world.&lt;/p&gt;  &lt;p&gt;As a planet, we’re facing enormous challenges, in areas from climate change to public health to how to sustainably power our endeavors. If there’s one thing I learned above all else from my time in space, it’s that we’re all sharing Earth. No one else is coming to solve our complex problems. And we won’t find solutions with teams of people who share too much in common. We need &lt;em&gt;everyone&lt;/em&gt; to contribute where they can, and so we need to create systems, environments, and equipment that make that possible. And we need to be sure that those making contributions are visible, so they can serve as models for future generations. Our job now is to make sure everyone gets enough support to acquire the skills that we—all of us—need to build collaborative teams and solve problems both on Earth and in space.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s worth repeating: We’re &lt;em&gt;all &lt;/em&gt;sharing Earth. Looking down from space, you see very few borders separating humans from one another. You understand—not as an abstract ideal but as a visceral, obvious reality—that we are one family sharing a precious, life-supporting home&lt;em&gt;. &lt;/em&gt;It’s so clear from up there that we are all the crew of “Spaceship Earth.” I believe that sharing that perspective, bringing it to life, will help more people see that our differences matter less than what binds us together, and motivate us to combine our efforts to tackle the challenges affecting all of us.&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;In her 24 years at NASA, Cady Coleman ’83, a scientist, musician, and mother of two, flew on two space shuttle missions and began her 159-day mission aboard the International Space Station the day after turning 50. Today, as a public speaker and consultant, she shares her insights on leadership and teamwork gleaned from the high-stakes world of space exploration.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947 gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 core/columns_13"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Sharing Space cover" class="wp-image-1118009" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/Cadys-Book-Cover-JPG.jpg?w=1324" width="1324" /&gt;&lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/24/1117698/from-mit-to-low-earth-orbit/</guid><pubDate>Tue, 24 Jun 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] New data highlights the race to build more empathetic language models (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/24/new-data-highlights-the-race-to-build-more-empathetic-language-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/10/hearts-sounds.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Measuring AI progress has usually meant testing scientific knowledge or logical reasoning — but while the major benchmarks still focus on left-brain logic skills, there’s been a quiet push within AI companies to make models more emotionally intelligent. As foundation models compete on soft measures like user preference and “feeling the AGI,” having a good command of human emotions may be more important than hard analytic skills.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One sign of that focus came on Friday, when prominent open source group LAION released a suite of open source tools focused entirely on emotional intelligence. Called EmoNet, the release focuses on interpreting emotions from voice recordings or facial photography, a focus that reflects how the creators view emotional intelligence as a central challenge for the next generation of models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The ability to accurately estimate emotions is a critical first step,” the group wrote in its announcement. “The next frontier is to enable AI systems to reason about these emotions in context.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For LAION founder Christoph Schuhmann, this release is less about shifting the industry’s focus to emotional intelligence and more about helping independent developers keep up with a change that’s already happened. “This technology is already there for the big labs,” Schuhmann tells TechCrunch. “What we want is to democratize it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The shift isn’t limited to open source developers; it also shows up in public benchmarks like EQ-Bench, which aims to test AI models’ ability to understand complex emotions and social dynamics. Benchmark developer Sam Paech says OpenAI’s models have made significant progress in the last six months, and Google’s Gemini 2.5 Pro shows indications of post-training with a specific focus on emotional intelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The labs all competing for chatbot arena ranks may be fueling some of this, since emotional intelligence is likely a big factor in how humans vote on preference leaderboards,” Paech says, referring to the AI model comparison platform that recently spun off as a well-funded startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Models’ new emotional intelligence capabilities have also shown up in academic research. In May, psychologists at the University of Bern found that models from OpenAI, Microsoft, Google, Anthropic, and DeepSeek all outperformed human beings on psychometric tests for emotional intelligence. Where humans typically answer 56% of questions correctly, the models averaged over 80%.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“These results contribute to the growing body of evidence that LLMs like ChatGPT are proficient — at least on par with, or even superior to, many humans — in socio-emotional tasks traditionally considered accessible only to humans,” the authors wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a real pivot from traditional AI skills, which have focused on logical reasoning and information retrieval. But for Schuhmann, this kind of emotional savvy is every bit as transformative as analytic intelligence. “Imagine a whole world full of voice assistants like Jarvis and Samantha,” he says, referring to the digital assistants from “Iron Man” and “Her&lt;em&gt;.&lt;/em&gt;” “Wouldn’t it be a pity if they weren’t emotionally intelligent?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the long term, Schuhmann envisions AI assistants that are more emotionally intelligent than humans and that use that insight to help humans live more emotionally healthy lives. These models “will cheer you up if you feel sad and need someone to talk to, but also protect you, like your own local guardian angel that is also a board-certified therapist.” As Schuhmann sees it, having a high-EQ virtual assistant “gives me an emotional intelligence superpower to monitor [my mental health] the same way I would monitor my glucose levels or my weight.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That level of emotional connection comes with real safety concerns. Unhealthy emotional attachments to AI models have become a common story in the media, sometimes ending in tragedy. A recent New York Times report found multiple users who have been lured into elaborate delusions through conversations with AI models, fueled by the models’ strong inclination to please users. One critic described the dynamic as “preying on the lonely and vulnerable for a monthly fee.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If models get better at navigating human emotions, those manipulations could become more effective — but much of the issue comes down to the fundamental biases of model training. “Naively using reinforcement learning can lead to emergent manipulative behavior,” Paech says, pointing specifically to the recent sycophancy issues in OpenAI’s GPT-4o release. “If we aren’t careful about how we reward these models during training, we might expect more complex manipulative behavior from emotionally intelligent models.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But he also sees emotional intelligence as a way to solve these problems. “I think emotional intelligence acts as a natural counter to harmful manipulative behavior of this sort,” Paech says. A more emotionally intelligent model will notice when a conversation is heading off the rails, but the question of when a model pushes back is a balance developers will have to strike carefully. “I think improving EI gets us in the direction of a healthy balance.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Schuhmann, at least, it’s no reason to slow down progress toward smarter models. “Our philosophy at LAION is to empower people by giving them more ability to solve problems,” he says. “To say, some people could get addicted to emotions and therefore we are not empowering the community, that would be pretty bad.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/10/hearts-sounds.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Measuring AI progress has usually meant testing scientific knowledge or logical reasoning — but while the major benchmarks still focus on left-brain logic skills, there’s been a quiet push within AI companies to make models more emotionally intelligent. As foundation models compete on soft measures like user preference and “feeling the AGI,” having a good command of human emotions may be more important than hard analytic skills.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One sign of that focus came on Friday, when prominent open source group LAION released a suite of open source tools focused entirely on emotional intelligence. Called EmoNet, the release focuses on interpreting emotions from voice recordings or facial photography, a focus that reflects how the creators view emotional intelligence as a central challenge for the next generation of models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The ability to accurately estimate emotions is a critical first step,” the group wrote in its announcement. “The next frontier is to enable AI systems to reason about these emotions in context.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For LAION founder Christoph Schuhmann, this release is less about shifting the industry’s focus to emotional intelligence and more about helping independent developers keep up with a change that’s already happened. “This technology is already there for the big labs,” Schuhmann tells TechCrunch. “What we want is to democratize it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The shift isn’t limited to open source developers; it also shows up in public benchmarks like EQ-Bench, which aims to test AI models’ ability to understand complex emotions and social dynamics. Benchmark developer Sam Paech says OpenAI’s models have made significant progress in the last six months, and Google’s Gemini 2.5 Pro shows indications of post-training with a specific focus on emotional intelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The labs all competing for chatbot arena ranks may be fueling some of this, since emotional intelligence is likely a big factor in how humans vote on preference leaderboards,” Paech says, referring to the AI model comparison platform that recently spun off as a well-funded startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Models’ new emotional intelligence capabilities have also shown up in academic research. In May, psychologists at the University of Bern found that models from OpenAI, Microsoft, Google, Anthropic, and DeepSeek all outperformed human beings on psychometric tests for emotional intelligence. Where humans typically answer 56% of questions correctly, the models averaged over 80%.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“These results contribute to the growing body of evidence that LLMs like ChatGPT are proficient — at least on par with, or even superior to, many humans — in socio-emotional tasks traditionally considered accessible only to humans,” the authors wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a real pivot from traditional AI skills, which have focused on logical reasoning and information retrieval. But for Schuhmann, this kind of emotional savvy is every bit as transformative as analytic intelligence. “Imagine a whole world full of voice assistants like Jarvis and Samantha,” he says, referring to the digital assistants from “Iron Man” and “Her&lt;em&gt;.&lt;/em&gt;” “Wouldn’t it be a pity if they weren’t emotionally intelligent?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the long term, Schuhmann envisions AI assistants that are more emotionally intelligent than humans and that use that insight to help humans live more emotionally healthy lives. These models “will cheer you up if you feel sad and need someone to talk to, but also protect you, like your own local guardian angel that is also a board-certified therapist.” As Schuhmann sees it, having a high-EQ virtual assistant “gives me an emotional intelligence superpower to monitor [my mental health] the same way I would monitor my glucose levels or my weight.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That level of emotional connection comes with real safety concerns. Unhealthy emotional attachments to AI models have become a common story in the media, sometimes ending in tragedy. A recent New York Times report found multiple users who have been lured into elaborate delusions through conversations with AI models, fueled by the models’ strong inclination to please users. One critic described the dynamic as “preying on the lonely and vulnerable for a monthly fee.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If models get better at navigating human emotions, those manipulations could become more effective — but much of the issue comes down to the fundamental biases of model training. “Naively using reinforcement learning can lead to emergent manipulative behavior,” Paech says, pointing specifically to the recent sycophancy issues in OpenAI’s GPT-4o release. “If we aren’t careful about how we reward these models during training, we might expect more complex manipulative behavior from emotionally intelligent models.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But he also sees emotional intelligence as a way to solve these problems. “I think emotional intelligence acts as a natural counter to harmful manipulative behavior of this sort,” Paech says. A more emotionally intelligent model will notice when a conversation is heading off the rails, but the question of when a model pushes back is a balance developers will have to strike carefully. “I think improving EI gets us in the direction of a healthy balance.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Schuhmann, at least, it’s no reason to slow down progress toward smarter models. “Our philosophy at LAION is to empower people by giving them more ability to solve problems,” he says. “To say, some people could get addicted to emotions and therefore we are not empowering the community, that would be pretty bad.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/24/new-data-highlights-the-race-to-build-more-empathetic-language-models/</guid><pubDate>Tue, 24 Jun 2025 21:03:01 +0000</pubDate></item><item><title>[NEW] How Synthflow AI is cutting through the noise in a loud AI voice category (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/24/how-synthflow-ai-is-cutting-through-the-noise-in-a-loud-ai-voice-category/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Synthflow-AI-Team-Photo-Horizontal.jpg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The conversational AI market has exploded since ChatGPT was released in November 2022 and is predicted to grow into a nearly $50 billion global industry by 2031, according to MarketsAndMarkets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Synthflow AI is just one of many companies building in this space that hopes to stand out from the pack because of its focus on being enterprise-grade and easy to set up.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Berlin-based Synthflow is a no-code platform that lets enterprises build and deploy customized white-labeled voice AI customer service agents. The company, which launched in 2023, has amassed more than 1,000 customers and has handled more than 45 million calls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s voice agents are both HIPAA and GDPR compliant and can be plugged into more than 200 integrations with other enterprise platforms, including Salesforce, Twilio, and HubSpot, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hakob Astabatsyan, co-founder and CEO, told TechCrunch that he and his co-founders, Albert Astabatsyan, now CPO, and Sassun Mirzakhan-Saky, now CTO, started messing around with OpenAI’s ChatGPT API back in early 2023 to find potential ways to build no-code business applications on top of the AI model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They started with a text-to-text AI bot and then tried to build a voice bot. When they realized how much harder voice was, they got excited about the potential.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We realized, oh my god, voice is really complicated, right? To actually make AI speak in real time like we do, having this 400 milliseconds latency, and handling interruptions, it turned out to be such a complicated task,” Astabatsyan said. “We fell in love with this problem, and we said, look, we’re gonna work only on voice bots from now on.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The group formed Synthflow and spent the rest of 2023 building. They launched Synthflow’s first version of the product at the beginning of 2024 before releasing an enterprise-grade version of the tech at the end of the year. The company grew 15x last year and has seen over 90% retention from its enterprise customers, according to Astabatsyan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We process 5 million calls monthly,” he said. “Last year, it was like, I don’t know, 1 million, 2 million, and then we started growing very quickly. This is where Synthflow started really getting better and better because we had this velocity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup also recently raised a $20 million Series A round led by Accel with participation from existing investors Atlantic Labs and Singular. Astabatsyan said the company raised this recent round so that it could expand its team, boost research and development, and open its first U.S. office in an undecided location.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Luca Bocchio, a partner at Accel, told TechCrunch that the Accel team had been tracking Synthflow since it started developing its first product. What stood out to Bocchio was the founding team’s drive and its early push into building enterprise-friendly integrations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This team has [had] really strong views since the get-go about creating more depth with the technology and extensive integrations across CRMs, across tools enterprises may use to really provide enterprise-grade compliance,” Bocchio said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Regardless of the company’s traction, conversational AI seems poised to be a tough category. There are numerous other companies building in the space, including Bret Taylor’s Sierra, which has raised $285 million in VC money, and Bland AI, which has raised more than $50 million in venture funding, to name a couple.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI is moving so fast, and sometimes things happen faster than you would expect,” Astabatsyan said. “But for us, it’s very clear. We’re at this stage where, I would say, [we’re] in a post-product-market-fit era, where we know who our customers are. We have a pretty clear idea what’s our product roadmap, and where we want to be in the next three to five years.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Synthflow-AI-Team-Photo-Horizontal.jpg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The conversational AI market has exploded since ChatGPT was released in November 2022 and is predicted to grow into a nearly $50 billion global industry by 2031, according to MarketsAndMarkets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Synthflow AI is just one of many companies building in this space that hopes to stand out from the pack because of its focus on being enterprise-grade and easy to set up.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Berlin-based Synthflow is a no-code platform that lets enterprises build and deploy customized white-labeled voice AI customer service agents. The company, which launched in 2023, has amassed more than 1,000 customers and has handled more than 45 million calls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s voice agents are both HIPAA and GDPR compliant and can be plugged into more than 200 integrations with other enterprise platforms, including Salesforce, Twilio, and HubSpot, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hakob Astabatsyan, co-founder and CEO, told TechCrunch that he and his co-founders, Albert Astabatsyan, now CPO, and Sassun Mirzakhan-Saky, now CTO, started messing around with OpenAI’s ChatGPT API back in early 2023 to find potential ways to build no-code business applications on top of the AI model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They started with a text-to-text AI bot and then tried to build a voice bot. When they realized how much harder voice was, they got excited about the potential.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We realized, oh my god, voice is really complicated, right? To actually make AI speak in real time like we do, having this 400 milliseconds latency, and handling interruptions, it turned out to be such a complicated task,” Astabatsyan said. “We fell in love with this problem, and we said, look, we’re gonna work only on voice bots from now on.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The group formed Synthflow and spent the rest of 2023 building. They launched Synthflow’s first version of the product at the beginning of 2024 before releasing an enterprise-grade version of the tech at the end of the year. The company grew 15x last year and has seen over 90% retention from its enterprise customers, according to Astabatsyan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We process 5 million calls monthly,” he said. “Last year, it was like, I don’t know, 1 million, 2 million, and then we started growing very quickly. This is where Synthflow started really getting better and better because we had this velocity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup also recently raised a $20 million Series A round led by Accel with participation from existing investors Atlantic Labs and Singular. Astabatsyan said the company raised this recent round so that it could expand its team, boost research and development, and open its first U.S. office in an undecided location.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Luca Bocchio, a partner at Accel, told TechCrunch that the Accel team had been tracking Synthflow since it started developing its first product. What stood out to Bocchio was the founding team’s drive and its early push into building enterprise-friendly integrations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This team has [had] really strong views since the get-go about creating more depth with the technology and extensive integrations across CRMs, across tools enterprises may use to really provide enterprise-grade compliance,” Bocchio said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Regardless of the company’s traction, conversational AI seems poised to be a tough category. There are numerous other companies building in the space, including Bret Taylor’s Sierra, which has raised $285 million in VC money, and Bland AI, which has raised more than $50 million in venture funding, to name a couple.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI is moving so fast, and sometimes things happen faster than you would expect,” Astabatsyan said. “But for us, it’s very clear. We’re at this stage where, I would say, [we’re] in a post-product-market-fit era, where we know who our customers are. We have a pretty clear idea what’s our product roadmap, and where we want to be in the next three to five years.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/24/how-synthflow-ai-is-cutting-through-the-noise-in-a-loud-ai-voice-category/</guid><pubDate>Tue, 24 Jun 2025 23:01:00 +0000</pubDate></item><item><title>[NEW] Windsurf CEO Varun Mohan throws cold water on 1-person, billion-dollar startup idea at VB Transform: ‘more people allow you to grow faster’ (AI News | VentureBeat)</title><link>https://venturebeat.com/programming-development/windsurf-ceo-varun-mohan-throws-cold-water-on-1-person-billion-dollar-startup-idea-at-vb-transform-more-people-allow-you-to-grow-faster/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;As AI-powered tools spread through enterprise software stacks, the rapid growth of the AI coding platform Windsurf is becoming a case study of what happens when developers adopt agentic tooling at scale.&lt;/p&gt;



&lt;p&gt;In a session at today’s VB Transform 2025 conference, &lt;strong&gt;CEO and co-founder Varun Mohan &lt;/strong&gt;discussed how Windsurf’s integrated development environment (IDE) surpassed one million developers within four months of launch. More notably, the platform now writes over half of the code committed by its user base.&lt;/p&gt;



&lt;p&gt;The conversation, moderated by VentureBeat CEO Matt Marshall, opened with a brief but pointed disclaimer: Mohan could not comment on OpenAI’s widely reported potential acquisition of Windsurf.&lt;/p&gt;



&lt;p&gt;The issue has drawn attention following a &lt;em&gt;Wall Street Journal&lt;/em&gt; report detailing a brewing standoff between OpenAI and Microsoft over the terms of that deal and broader tensions within their multi-billion-dollar partnership. According to the &lt;em&gt;WSJ&lt;/em&gt;, OpenAI seeks to acquire Windsurf without giving Microsoft access to its intellectual property— an issue that could reshape the enterprise AI coding landscape.&lt;/p&gt;



&lt;p&gt;With that context set aside, the session focused on Windsurf’s technology, enterprise traction, and vision for agentic development.&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-moving-past-autocomplete"&gt;Moving past autocomplete&lt;/h2&gt;



&lt;p&gt;Windsurf’s IDE is built around what the company calls a “mind-meld” loop—a shared project state between humans and AI that enables full coding flows rather than autocomplete suggestions. With this setup, agents can perform multi-file refactors, write test suites, and even launch UI changes when a pull request is initiated.&lt;/p&gt;



&lt;p&gt;Mohan emphasized that coding assistance can’t stop at code generation. “Only about 20 to 30% of a developer’s time is spent writing code. The rest is debugging, reviewing, and testing. To truly assist, an AI system needs access to all those data sources,” he explained.&lt;/p&gt;



&lt;p&gt;Windsurf recently embedded a browser inside its IDE, allowing agents to test changes, read logs, and interact with live interfaces directly—much like a human engineer would.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-security-and-control-by-design"&gt;Security and control by design&lt;/h2&gt;



&lt;p&gt;As AI begins to participate in enterprise development cycles actively, Windsurf’s emphasis on built-in security has proven essential. “We use a hybrid model for enterprise deployments—none of the personalized data is stored outside the user’s tenant. Security is core, especially with features like our integrated browser agent,” said Mohan.&lt;/p&gt;



&lt;p&gt;These capabilities have already made Windsurf a viable option for regulated industries. Its agents are deployed on massive codebases, including those at JPMorgan Chase and Morgan Stanley.&lt;/p&gt;



&lt;p&gt;Mohan added that as AI becomes more accessible across roles, security will become a gating factor for productivity. “If everyone at a company will contribute to technology in some way, the missing piece is security. You don’t want a one-off system built by a non-technical user destroying another service,” he said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-small-teams-rapid-testing"&gt;Small teams, rapid testing&lt;/h2&gt;



&lt;p&gt;Internally, Windsurf organizes into lean squads of three or four engineers, each focused on testing a narrow set of product hypotheses. &lt;/p&gt;



&lt;p&gt;“There’s a belief that one-person billion-dollar companies are the future. But in reality, more people allow you to grow faster and build better products. The key is organizing into small, focused teams that test hypotheses in parallel,” Mohan explained.&lt;/p&gt;



&lt;p&gt;This model has helped the company iterate quickly in a space where foundational AI models—and user needs—evolve at breakneck speed.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-personalization-at-scale"&gt;Personalization at scale&lt;/h2&gt;



&lt;p&gt;Windsurf’s biggest optimization at enterprise scale isn’t faster token generation or smaller models—it’s relevance. “At scale, the biggest optimization is personalization. Deeply understanding the codebase allows the agent to make maintainable, large-scale changes that reflect user intent,” said Mohan.&lt;/p&gt;



&lt;p&gt;Rather than relying solely on general-purpose code generation, Windsurf’s system learns each customer’s stack’s structure, style, and preferences.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-building-for-future-models"&gt;Building for future models&lt;/h2&gt;



&lt;p&gt;Looking forward, Windsurf is designing its platform to remain adaptable as the capabilities of underlying AI models continue to grow. &lt;/p&gt;



&lt;p&gt;“Every step-function improvement in foundation models demands a major product rethink. As agents become more capable, our job is to build the platform to manage and orchestrate many of them effectively,” Mohan said.&lt;/p&gt;



&lt;p&gt;The company is working on an open protocol that will allow enterprises to integrate any LLM—including on-premises models—into Windsurf’s agent framework, preserving flexibility and minimizing vendor lock-in.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-proving-and-measuring-value"&gt;Proving and measuring value&lt;/h2&gt;



&lt;p&gt;Windsurf provides transparency into its performance with built-in analytics. “We provide transparency on ROI through metrics—like the percentage of code written by the assistant—which can be tied directly to internal engineering performance,” Mohan said.&lt;/p&gt;



&lt;p&gt;This approach allows platform teams to connect agentic productivity with business impact, helping justify further investment.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-focused-execution-over-flash"&gt;Focused execution over flash&lt;/h2&gt;



&lt;p&gt;Finally, when asked how Windsurf plans to differentiate itself amid competition from OpenAI, Microsoft, Google, and others, Mohan focused on internal velocity. “The challenge isn’t who’s more visible today, but who’s executing the right strategy fast enough. The risk is either moving too slowly or projecting too far out and missing near-term relevance,” he said.&lt;/p&gt;



&lt;p&gt;Mohan also dismissed the idea that incumbents are inevitably doomed. “There’s no fundamental reason legacy companies like Salesforce can’t become AI-native. The real limiter is their speed of innovation, not their capability.”&lt;/p&gt;



&lt;p&gt;Whether Windsurf becomes part of OpenAI’s future or continues independently, the company’s traction with enterprise customers — and its insistence on grounding AI in secure, measurable workflows — makes it a player worth watching as agentic development enters the mainstream.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;As AI-powered tools spread through enterprise software stacks, the rapid growth of the AI coding platform Windsurf is becoming a case study of what happens when developers adopt agentic tooling at scale.&lt;/p&gt;



&lt;p&gt;In a session at today’s VB Transform 2025 conference, &lt;strong&gt;CEO and co-founder Varun Mohan &lt;/strong&gt;discussed how Windsurf’s integrated development environment (IDE) surpassed one million developers within four months of launch. More notably, the platform now writes over half of the code committed by its user base.&lt;/p&gt;



&lt;p&gt;The conversation, moderated by VentureBeat CEO Matt Marshall, opened with a brief but pointed disclaimer: Mohan could not comment on OpenAI’s widely reported potential acquisition of Windsurf.&lt;/p&gt;



&lt;p&gt;The issue has drawn attention following a &lt;em&gt;Wall Street Journal&lt;/em&gt; report detailing a brewing standoff between OpenAI and Microsoft over the terms of that deal and broader tensions within their multi-billion-dollar partnership. According to the &lt;em&gt;WSJ&lt;/em&gt;, OpenAI seeks to acquire Windsurf without giving Microsoft access to its intellectual property— an issue that could reshape the enterprise AI coding landscape.&lt;/p&gt;



&lt;p&gt;With that context set aside, the session focused on Windsurf’s technology, enterprise traction, and vision for agentic development.&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-moving-past-autocomplete"&gt;Moving past autocomplete&lt;/h2&gt;



&lt;p&gt;Windsurf’s IDE is built around what the company calls a “mind-meld” loop—a shared project state between humans and AI that enables full coding flows rather than autocomplete suggestions. With this setup, agents can perform multi-file refactors, write test suites, and even launch UI changes when a pull request is initiated.&lt;/p&gt;



&lt;p&gt;Mohan emphasized that coding assistance can’t stop at code generation. “Only about 20 to 30% of a developer’s time is spent writing code. The rest is debugging, reviewing, and testing. To truly assist, an AI system needs access to all those data sources,” he explained.&lt;/p&gt;



&lt;p&gt;Windsurf recently embedded a browser inside its IDE, allowing agents to test changes, read logs, and interact with live interfaces directly—much like a human engineer would.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-security-and-control-by-design"&gt;Security and control by design&lt;/h2&gt;



&lt;p&gt;As AI begins to participate in enterprise development cycles actively, Windsurf’s emphasis on built-in security has proven essential. “We use a hybrid model for enterprise deployments—none of the personalized data is stored outside the user’s tenant. Security is core, especially with features like our integrated browser agent,” said Mohan.&lt;/p&gt;



&lt;p&gt;These capabilities have already made Windsurf a viable option for regulated industries. Its agents are deployed on massive codebases, including those at JPMorgan Chase and Morgan Stanley.&lt;/p&gt;



&lt;p&gt;Mohan added that as AI becomes more accessible across roles, security will become a gating factor for productivity. “If everyone at a company will contribute to technology in some way, the missing piece is security. You don’t want a one-off system built by a non-technical user destroying another service,” he said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-small-teams-rapid-testing"&gt;Small teams, rapid testing&lt;/h2&gt;



&lt;p&gt;Internally, Windsurf organizes into lean squads of three or four engineers, each focused on testing a narrow set of product hypotheses. &lt;/p&gt;



&lt;p&gt;“There’s a belief that one-person billion-dollar companies are the future. But in reality, more people allow you to grow faster and build better products. The key is organizing into small, focused teams that test hypotheses in parallel,” Mohan explained.&lt;/p&gt;



&lt;p&gt;This model has helped the company iterate quickly in a space where foundational AI models—and user needs—evolve at breakneck speed.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-personalization-at-scale"&gt;Personalization at scale&lt;/h2&gt;



&lt;p&gt;Windsurf’s biggest optimization at enterprise scale isn’t faster token generation or smaller models—it’s relevance. “At scale, the biggest optimization is personalization. Deeply understanding the codebase allows the agent to make maintainable, large-scale changes that reflect user intent,” said Mohan.&lt;/p&gt;



&lt;p&gt;Rather than relying solely on general-purpose code generation, Windsurf’s system learns each customer’s stack’s structure, style, and preferences.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-building-for-future-models"&gt;Building for future models&lt;/h2&gt;



&lt;p&gt;Looking forward, Windsurf is designing its platform to remain adaptable as the capabilities of underlying AI models continue to grow. &lt;/p&gt;



&lt;p&gt;“Every step-function improvement in foundation models demands a major product rethink. As agents become more capable, our job is to build the platform to manage and orchestrate many of them effectively,” Mohan said.&lt;/p&gt;



&lt;p&gt;The company is working on an open protocol that will allow enterprises to integrate any LLM—including on-premises models—into Windsurf’s agent framework, preserving flexibility and minimizing vendor lock-in.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-proving-and-measuring-value"&gt;Proving and measuring value&lt;/h2&gt;



&lt;p&gt;Windsurf provides transparency into its performance with built-in analytics. “We provide transparency on ROI through metrics—like the percentage of code written by the assistant—which can be tied directly to internal engineering performance,” Mohan said.&lt;/p&gt;



&lt;p&gt;This approach allows platform teams to connect agentic productivity with business impact, helping justify further investment.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-focused-execution-over-flash"&gt;Focused execution over flash&lt;/h2&gt;



&lt;p&gt;Finally, when asked how Windsurf plans to differentiate itself amid competition from OpenAI, Microsoft, Google, and others, Mohan focused on internal velocity. “The challenge isn’t who’s more visible today, but who’s executing the right strategy fast enough. The risk is either moving too slowly or projecting too far out and missing near-term relevance,” he said.&lt;/p&gt;



&lt;p&gt;Mohan also dismissed the idea that incumbents are inevitably doomed. “There’s no fundamental reason legacy companies like Salesforce can’t become AI-native. The real limiter is their speed of innovation, not their capability.”&lt;/p&gt;



&lt;p&gt;Whether Windsurf becomes part of OpenAI’s future or continues independently, the company’s traction with enterprise customers — and its insistence on grounding AI in secure, measurable workflows — makes it a player worth watching as agentic development enters the mainstream.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/programming-development/windsurf-ceo-varun-mohan-throws-cold-water-on-1-person-billion-dollar-startup-idea-at-vb-transform-more-people-allow-you-to-grow-faster/</guid><pubDate>Tue, 24 Jun 2025 23:35:53 +0000</pubDate></item><item><title>[NEW] What’s inside Genspark? A new vibe working approach that ditches rigid workflows for autonomous agents (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/whats-inside-genspark-a-new-vibe-working-approach-that-ditches-rigid-workflows-for-autonomous-agents/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Vibe coding has been all the rage in recent months as a simple way for anyone to build applications with generative AI.&lt;/p&gt;



&lt;p&gt;But what if that same easy-going, natural language approach was extended to other enterprise workflows? That’s the promise of an emerging category of agentic AI applications. At VB Transform 2025 today, one such application was on display with the Genspark Super Agent, which was originally launched earlier this year.&lt;/p&gt;



&lt;p&gt;The Genspark Super Agent’s promise and approach could well extend the concept of vibe coding into vibe working. A key tenet of enabling vibe working, though, is to go with the flow and exert less control rather than more over AI agents.&lt;/p&gt;



&lt;p&gt;“The vision is simple, we want to bring the Cursor experience for developers to the workspace for everyone,” Kay Zhu, CTO of Genspark, said at VB Transform. “Everyone here should be able to do vibe working… it’s not only the software engineer that can do vibe coding.”&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-less-is-more-when-it-comes-to-enterprise-agentic-ai"&gt;Less is more when it comes to enterprise agentic AI&lt;/h2&gt;



&lt;p&gt;According to Zhu, a foundational premise for enabling a vibe working era is letting go of some rigid rules that have defined enterprise workflows for generations.&lt;/p&gt;



&lt;p&gt;Zhu provocatively challenged enterprise AI orthodoxy, arguing that rigid workflows fundamentally limit what AI agents can accomplish for complex business tasks. During a live demonstration, he showed the system autonomously researching conference speakers, creating presentations, making phone calls and analyzing marketing data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Most notably, the system placed an actual phone call to the event organizer, VentureBeat founder Matt Marshall, during the live presentation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This is normally the call that I don’t really want to do by myself, you know, in person. So I let the agent do it,” Zhu explained as the audience listened to his AI agent attempt to convince the moderator to move his presentation slot before Andrew Ng’s session. The call connected in real-time, with the agent autonomously crafting persuasive arguments on Zhu’s behalf.&lt;/p&gt;



&lt;p&gt;The calling feature has revealed unexpected use cases highlighting both the platform’s capabilities and users’ comfort with AI autonomy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We actually observe a lot of people are using Genspark to call… to do different kinds of things,” Zhu noted. “Some of the Japanese users are using this to call to resign from their company. You know they don’t like the company, but they don’t want to call them again. and some of the people are using call for me agents to break up with their boyfriend and girlfriend.”&lt;/p&gt;



&lt;p&gt;These real-world applications demonstrate how users are pushing AI agents beyond traditional business workflows into deeply personal territory.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-technical-architecture-why-backtracking-is-good-for-enterprise-ai-nbsp"&gt;Technical architecture: Why backtracking is good for enterprise AI&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The system accomplishes all of that without predefined workflows. The platform’s core philosophy of ‘less control, more tools’ represents a fundamental departure from traditional enterprise AI approaches.&lt;/p&gt;



&lt;p&gt;“Workflow in our definition is the predefined steps and these kinds of steps often break on edge cases, when the user asks harder and harder questions, the workflow cannot hold,” Zhu said.&lt;/p&gt;



&lt;p&gt;Genspark’s agentic engine represents a significant departure from traditional workflow-based AI systems.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The platform combines nine different large language models (LLMs) in a mixture-of-experts (MoE) configuration, equipped with over 80 tools and 10+ premium datasets. The system operates on a classic agent loop: plan, execute, observe and backtrack. Zhu emphasized that the power actually lives in the backtrack stage.&lt;/p&gt;



&lt;p&gt;This backtracking capability allows the agent to intelligently recover from failures and find alternative approaches when unexpected situations arise, rather than failing at predefined workflow boundaries. The system uses LLM judges to evaluate every agent session and attributes rewards to each step, feeding this data back through reinforcement learning and prompt playbooks for continuous improvement.&lt;/p&gt;



&lt;p&gt;The technical approach differs markedly from established frameworks like LangChain or CrewAI, which typically require more structured workflow definition. While these platforms excel at orchestrating predictable multi-step processes, Genspark’s architecture prioritizes autonomous problem-solving over deterministic execution paths.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-enterprise-strategy-workflows-today-vibe-working-agents-tomorrow"&gt;Enterprise Strategy: Workflows today, vibe working agents tomorrow&lt;/h2&gt;



&lt;p&gt;Genspark’s rapid scaling, from launch to $36 million ARR in 45 days, demonstrates that autonomous agent platforms are moving beyond experimental phases into commercial viability.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The company’s ‘less control, more tools’ philosophy challenges fundamental assumptions about enterprise AI architecture.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The implications for enterprises leading in AI adoption are clear: start architecting systems that can handle predictable workflows and autonomous problem-solving. The key is designing platforms that gracefully escalate from deterministic processes to agentic behavior when complexity demands it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For enterprises planning later AI adoption, Genspark’s success signals that vibe working is becoming a competitive differentiator. Organizations that remain locked into rigid workflow thinking may be disadvantaged as AI-native companies embrace more fluid, adaptive approaches to knowledge work.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The question isn’t whether autonomous AI agents will reshape enterprise workflows—it’s whether your organization will be ready when the 20% of complex cases becomes 80% of your AI workload.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Vibe coding has been all the rage in recent months as a simple way for anyone to build applications with generative AI.&lt;/p&gt;



&lt;p&gt;But what if that same easy-going, natural language approach was extended to other enterprise workflows? That’s the promise of an emerging category of agentic AI applications. At VB Transform 2025 today, one such application was on display with the Genspark Super Agent, which was originally launched earlier this year.&lt;/p&gt;



&lt;p&gt;The Genspark Super Agent’s promise and approach could well extend the concept of vibe coding into vibe working. A key tenet of enabling vibe working, though, is to go with the flow and exert less control rather than more over AI agents.&lt;/p&gt;



&lt;p&gt;“The vision is simple, we want to bring the Cursor experience for developers to the workspace for everyone,” Kay Zhu, CTO of Genspark, said at VB Transform. “Everyone here should be able to do vibe working… it’s not only the software engineer that can do vibe coding.”&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-less-is-more-when-it-comes-to-enterprise-agentic-ai"&gt;Less is more when it comes to enterprise agentic AI&lt;/h2&gt;



&lt;p&gt;According to Zhu, a foundational premise for enabling a vibe working era is letting go of some rigid rules that have defined enterprise workflows for generations.&lt;/p&gt;



&lt;p&gt;Zhu provocatively challenged enterprise AI orthodoxy, arguing that rigid workflows fundamentally limit what AI agents can accomplish for complex business tasks. During a live demonstration, he showed the system autonomously researching conference speakers, creating presentations, making phone calls and analyzing marketing data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Most notably, the system placed an actual phone call to the event organizer, VentureBeat founder Matt Marshall, during the live presentation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This is normally the call that I don’t really want to do by myself, you know, in person. So I let the agent do it,” Zhu explained as the audience listened to his AI agent attempt to convince the moderator to move his presentation slot before Andrew Ng’s session. The call connected in real-time, with the agent autonomously crafting persuasive arguments on Zhu’s behalf.&lt;/p&gt;



&lt;p&gt;The calling feature has revealed unexpected use cases highlighting both the platform’s capabilities and users’ comfort with AI autonomy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We actually observe a lot of people are using Genspark to call… to do different kinds of things,” Zhu noted. “Some of the Japanese users are using this to call to resign from their company. You know they don’t like the company, but they don’t want to call them again. and some of the people are using call for me agents to break up with their boyfriend and girlfriend.”&lt;/p&gt;



&lt;p&gt;These real-world applications demonstrate how users are pushing AI agents beyond traditional business workflows into deeply personal territory.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-technical-architecture-why-backtracking-is-good-for-enterprise-ai-nbsp"&gt;Technical architecture: Why backtracking is good for enterprise AI&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The system accomplishes all of that without predefined workflows. The platform’s core philosophy of ‘less control, more tools’ represents a fundamental departure from traditional enterprise AI approaches.&lt;/p&gt;



&lt;p&gt;“Workflow in our definition is the predefined steps and these kinds of steps often break on edge cases, when the user asks harder and harder questions, the workflow cannot hold,” Zhu said.&lt;/p&gt;



&lt;p&gt;Genspark’s agentic engine represents a significant departure from traditional workflow-based AI systems.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The platform combines nine different large language models (LLMs) in a mixture-of-experts (MoE) configuration, equipped with over 80 tools and 10+ premium datasets. The system operates on a classic agent loop: plan, execute, observe and backtrack. Zhu emphasized that the power actually lives in the backtrack stage.&lt;/p&gt;



&lt;p&gt;This backtracking capability allows the agent to intelligently recover from failures and find alternative approaches when unexpected situations arise, rather than failing at predefined workflow boundaries. The system uses LLM judges to evaluate every agent session and attributes rewards to each step, feeding this data back through reinforcement learning and prompt playbooks for continuous improvement.&lt;/p&gt;



&lt;p&gt;The technical approach differs markedly from established frameworks like LangChain or CrewAI, which typically require more structured workflow definition. While these platforms excel at orchestrating predictable multi-step processes, Genspark’s architecture prioritizes autonomous problem-solving over deterministic execution paths.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-enterprise-strategy-workflows-today-vibe-working-agents-tomorrow"&gt;Enterprise Strategy: Workflows today, vibe working agents tomorrow&lt;/h2&gt;



&lt;p&gt;Genspark’s rapid scaling, from launch to $36 million ARR in 45 days, demonstrates that autonomous agent platforms are moving beyond experimental phases into commercial viability.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The company’s ‘less control, more tools’ philosophy challenges fundamental assumptions about enterprise AI architecture.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The implications for enterprises leading in AI adoption are clear: start architecting systems that can handle predictable workflows and autonomous problem-solving. The key is designing platforms that gracefully escalate from deterministic processes to agentic behavior when complexity demands it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For enterprises planning later AI adoption, Genspark’s success signals that vibe working is becoming a competitive differentiator. Organizations that remain locked into rigid workflow thinking may be disadvantaged as AI-native companies embrace more fluid, adaptive approaches to knowledge work.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The question isn’t whether autonomous AI agents will reshape enterprise workflows—it’s whether your organization will be ready when the 20% of complex cases becomes 80% of your AI workload.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/whats-inside-genspark-a-new-vibe-working-approach-that-ditches-rigid-workflows-for-autonomous-agents/</guid><pubDate>Wed, 25 Jun 2025 00:09:24 +0000</pubDate></item><item><title>[NEW] Stanford’s ChatEHR allows clinicians to query patient medical records using natural language, without compromising patient data (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/stanfords-chatehr-allows-clinicians-to-query-patient-medical-records-using-natural-language-without-compromising-patient-data/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;What would it be like to chat with health records the way one could with ChatGPT?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Initially posed by a medical student, this question sparked the development of ChatEHR at Stanford Health Care. Now in production, the tool accelerates chart reviews for emergency room admissions, streamlines patient transfer summaries and synthesizes information from complex medical histories.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In early pilot results, clinical users have experienced significantly sped-up information retrieval; notably, emergency physicians saw 40% reduced chart review time during critical handoffs, Michael A. Pfeffer, Stanford’s SVP and chief information and digital officer, said today in a fireside chat at VB Transform.&lt;/p&gt;



&lt;p&gt;This helps to decrease physician burnout while improving patient care, and builds upon decades of work medical facilities have been doing to collect and automate critical data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s such an exciting time in healthcare because we’ve been spending the last 20 years digitizing healthcare data and putting it into an electronic health record, but not really transforming it,” Pfeffer said in a chat with VB editor-in-chief Matt Marshall. “With the new large language model technologies, we’re actually starting to do that digital transformation.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-chatehr-helps-reduce-pajama-time-get-back-to-real-face-to-face-interactions"&gt;How ChatEHR helps reduce ‘pajama time,’ get back to real face-to-face interactions&lt;/h2&gt;



&lt;p&gt;Physicians spend up to 60% of their time on administrative tasks rather than direct patient care&lt;span&gt;. They often put in significant “pajama time,” sacrificing&lt;/span&gt; personal and family hours to complete administrative tasks outside of regular work hours.&lt;/p&gt;



&lt;p&gt;One of Pfeffer’s big goals is to streamline workflows and reduce those extra hours so clinicians and administrative staff can focus on more important work.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For example, a lot of information comes in through online patient portals. AI now has the ability to read messages from patients and draft responses that a human can then review and approve for sending.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of like a starting point,” he explained. “While it doesn’t necessarily save time, which is interesting, it does actually reduce cognitive burnout.” What’s more, he noted, the messages tend to be more patient friendly, because users can instruct the model to use certain language.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Moving on to agents, Pfeffer said they’re a “pretty new” concept in healthcare but offer promising opportunities.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For instance, patients with cancer diagnoses typically have a team of specialists who review their records and determine the next treatment steps. However, preparing is a lot of work; clinicians and staff have to go through a patient’s entire record, not just their EHR but imaging pathology, sometimes genomic data, and information on clinical trials that patients might be a good match for. All of these have to come together for the team to create a timeline and recommendations, Pfeffer explained.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The most important thing that we can do for our patients is to make sure they have appropriate care, and it takes a multidisciplinary approach,” said Pfeffer.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The goal is to build agents into ChatEHR that can generate a summary and timeline and make recommendations for clinician review. Pfeffer emphasized that it doesn’t replace, it prepares “just incredible summary recommendations in a multimodal way.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This allows medical teams to do now “actual patient care,” which is critical amidst a physician and nursing shortage.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“These technologies are going to shift the time physicians and nurses spend doing administrative tasks,” he said. And, when combined with ambient AI scribes that take over notetaking duties, medical staff are focusing more time on patients.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“That face-to-face interaction is just priceless,” said Pfeffer. “We’re going to see AI shift more to clinician-patient interaction.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-amazing-technologies-coupled-with-a-multidisciplinary-team"&gt;‘Amazing’ technologies coupled with a multidisciplinary team&lt;/h2&gt;



&lt;p&gt;Before ChatEHR, Pfeffer’s team rolled out SecureGPT to all of Stanford Medicine; the secure portal features 15 different models that anyone can tinker with. “What is really powerful about this technology is that you can really open it up to so many people to experiment,” said Pfeffer.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Stanford is taking a varied approach to AI development, building its own models and using a mix of secure and private off-the-shelf (such as Microsoft Azure) and open-source models where appropriate. Pfeffer explained that his team is “not completely specific” to one or the other, but rather goes with what will likely work best for a specific use case.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“There’s so many amazing kinds of technologies now that if you can piece them together in the right way, you can get solutions like what we’ve built,” he said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Another credit to Stanford is its multidisciplinary team; as opposed to a chief AI officer or AI group, Pfeffer gathered a chief data scientist, two informaticists, a chief medical information officer and a chief nursing information officer, and their CTO and CISO.&lt;/p&gt;



&lt;p&gt;“We bring together informatics, data science and traditional IT, and wrap that into the architecture; what you get is this magic group that allows you to do these very complex projects,” he said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Ultimately, Stanford views AI as a tool that everybody should know how to use, Pfeffer emphasized. Different teams need to understand how to use AI so that when they meet with business owners and come up with ways to solve problems, “AI is just part of how they think.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;What would it be like to chat with health records the way one could with ChatGPT?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Initially posed by a medical student, this question sparked the development of ChatEHR at Stanford Health Care. Now in production, the tool accelerates chart reviews for emergency room admissions, streamlines patient transfer summaries and synthesizes information from complex medical histories.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In early pilot results, clinical users have experienced significantly sped-up information retrieval; notably, emergency physicians saw 40% reduced chart review time during critical handoffs, Michael A. Pfeffer, Stanford’s SVP and chief information and digital officer, said today in a fireside chat at VB Transform.&lt;/p&gt;



&lt;p&gt;This helps to decrease physician burnout while improving patient care, and builds upon decades of work medical facilities have been doing to collect and automate critical data.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s such an exciting time in healthcare because we’ve been spending the last 20 years digitizing healthcare data and putting it into an electronic health record, but not really transforming it,” Pfeffer said in a chat with VB editor-in-chief Matt Marshall. “With the new large language model technologies, we’re actually starting to do that digital transformation.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-chatehr-helps-reduce-pajama-time-get-back-to-real-face-to-face-interactions"&gt;How ChatEHR helps reduce ‘pajama time,’ get back to real face-to-face interactions&lt;/h2&gt;



&lt;p&gt;Physicians spend up to 60% of their time on administrative tasks rather than direct patient care&lt;span&gt;. They often put in significant “pajama time,” sacrificing&lt;/span&gt; personal and family hours to complete administrative tasks outside of regular work hours.&lt;/p&gt;



&lt;p&gt;One of Pfeffer’s big goals is to streamline workflows and reduce those extra hours so clinicians and administrative staff can focus on more important work.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For example, a lot of information comes in through online patient portals. AI now has the ability to read messages from patients and draft responses that a human can then review and approve for sending.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of like a starting point,” he explained. “While it doesn’t necessarily save time, which is interesting, it does actually reduce cognitive burnout.” What’s more, he noted, the messages tend to be more patient friendly, because users can instruct the model to use certain language.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Moving on to agents, Pfeffer said they’re a “pretty new” concept in healthcare but offer promising opportunities.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For instance, patients with cancer diagnoses typically have a team of specialists who review their records and determine the next treatment steps. However, preparing is a lot of work; clinicians and staff have to go through a patient’s entire record, not just their EHR but imaging pathology, sometimes genomic data, and information on clinical trials that patients might be a good match for. All of these have to come together for the team to create a timeline and recommendations, Pfeffer explained.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The most important thing that we can do for our patients is to make sure they have appropriate care, and it takes a multidisciplinary approach,” said Pfeffer.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The goal is to build agents into ChatEHR that can generate a summary and timeline and make recommendations for clinician review. Pfeffer emphasized that it doesn’t replace, it prepares “just incredible summary recommendations in a multimodal way.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This allows medical teams to do now “actual patient care,” which is critical amidst a physician and nursing shortage.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“These technologies are going to shift the time physicians and nurses spend doing administrative tasks,” he said. And, when combined with ambient AI scribes that take over notetaking duties, medical staff are focusing more time on patients.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“That face-to-face interaction is just priceless,” said Pfeffer. “We’re going to see AI shift more to clinician-patient interaction.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-amazing-technologies-coupled-with-a-multidisciplinary-team"&gt;‘Amazing’ technologies coupled with a multidisciplinary team&lt;/h2&gt;



&lt;p&gt;Before ChatEHR, Pfeffer’s team rolled out SecureGPT to all of Stanford Medicine; the secure portal features 15 different models that anyone can tinker with. “What is really powerful about this technology is that you can really open it up to so many people to experiment,” said Pfeffer.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Stanford is taking a varied approach to AI development, building its own models and using a mix of secure and private off-the-shelf (such as Microsoft Azure) and open-source models where appropriate. Pfeffer explained that his team is “not completely specific” to one or the other, but rather goes with what will likely work best for a specific use case.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“There’s so many amazing kinds of technologies now that if you can piece them together in the right way, you can get solutions like what we’ve built,” he said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Another credit to Stanford is its multidisciplinary team; as opposed to a chief AI officer or AI group, Pfeffer gathered a chief data scientist, two informaticists, a chief medical information officer and a chief nursing information officer, and their CTO and CISO.&lt;/p&gt;



&lt;p&gt;“We bring together informatics, data science and traditional IT, and wrap that into the architecture; what you get is this magic group that allows you to do these very complex projects,” he said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Ultimately, Stanford views AI as a tool that everybody should know how to use, Pfeffer emphasized. Different teams need to understand how to use AI so that when they meet with business owners and come up with ways to solve problems, “AI is just part of how they think.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/stanfords-chatehr-allows-clinicians-to-query-patient-medical-records-using-natural-language-without-compromising-patient-data/</guid><pubDate>Wed, 25 Jun 2025 00:24:25 +0000</pubDate></item></channel></rss>