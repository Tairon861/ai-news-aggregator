<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 15 Sep 2025 01:44:32 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>‘Selling coffee beans to Starbucks’ – how the AI boom could leave AI’s biggest companies behind (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/14/selling-coffee-beans-to-starbucks-how-the-ai-boom-could-leave-ais-biggest-companies-behind/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/GettyImages-1157614908.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;How much do foundation models matter?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It might seem like a silly question, but it’s come up a lot in my conversations with AI startups, which are increasingly comfortable with businesses that used to be dismissed as “GPT wrappers,” or companies that build interfaces on top of existing AI models like ChatGPT. These days, startup teams are focused on customizing AI models for specific tasks and interface work, and see the foundation model as a commodity that can be swapped in and out as necessary. That approach was on display especially at last week’s Boxworks conference, which seemed devoted entirely to the user-facing software built on top of AI models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Part of what is driving this is that the scaling benefits of pre-training — that initial process of teaching AI models using massive datasets, which is the sole domain of foundation models — has slowed down. That doesn’t mean AI has stopped making progress, but the early benefits of hyperscaled foundational models have hit diminishing returns, and attention has turned to post-training and reinforcement learning as sources of future progress. If you want to make a better AI coding tool, you’re better off working on fine-tuning and interface design rather than spending another few billion dollars worth in server time on pre-training. As the success of Anthropic’s Claude Code shows, foundation model companies are quite good at these other fields too — but it’s not as durable an advantage as it used to be.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, the competitive landscape of AI is changing in ways that undermine the advantages of the biggest AI labs. Instead of a race for an all-powerful AGI that could match or exceed human abilities across all cognitive tasks, the immediate future looks like a flurry of discrete businesses: software development, enterprise data management, image generation and so on. Aside from a first-mover advantage, it’s not clear that building a foundation model gives you any advantage in those businesses. Worse, the abundance of open-source alternatives means that foundation models may not have any price leverage if they lose the competition at the application layer. This would turn companies like OpenAI and Anthropic into back-end suppliers in a low-margin commodity business – as one founder put it to me, “like selling coffee beans to Starbucks.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s hard to overstate what a dramatic shift this would be for the business of AI. Throughout the contemporary boom, the success of AI has been inextricable from the success of the companies building foundation models — specifically, OpenAI, Anthropic, and Google. Being bullish on AI meant believing that AI’s transformative impact would make these into generationally important companies. We could argue about which company would come out on top, but it was clear that some foundation model company was going to end up with the keys to the kingdom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the time, there were lots of reasons to think this was true. For years, foundation model development was the only AI business there was — and the fast pace of progress made their lead seem insurmountable. And Silicon Valley has always had a deep-rooted love of platform advantage. The assumption was that, however AI models ended up making money, the lion’s share of the benefit would flow back to the foundation model companies, who had done the work that was hardest to replicate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The past year has made that story more complicated. There are lots of successful third-party AI services, but they tend to use foundation models interchangeably. For startups, it no longer matters whether their product sits on top of GPT-5, Claude or Gemini, and they expect to be able to switch models in mid-release without end users noticing the difference. Foundation models continue to make real progress, but it no longer seems plausible for any one company to maintain a large enough advantage to dominate the industry.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;We already have plenty of indication that there is not much of a first-mover advantage. As venture capitalist Martin Casado of a16z pointed out on a recent podcast, OpenAI was the first lab to put out a coding model, as well as generative models for image and video — only to lose all three categories to competitors. “As far as we can tell, there is no inherent moat in the technology stack for AI,” Casado concluded.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, we shouldn’t count foundation model companies out just yet. There are still lots of durable advantages on their side, including brand recognition, infrastructure, and unthinkably vast cash reserves. OpenAI’s consumer business may prove harder to replicate than its coding business, and other advantages may emerge as the sector matures. Given the fast pace of AI development, the current interest in post-training could easily reverse course in the next six months. Most uncertain of all, the race toward general intelligence could pay off with new breakthroughs in pharmaceuticals or materials science, radically shifting our ideas about what makes AI models valuable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But in the meantime, the strategy of building ever-bigger foundation models looks a lot less appealing than it did last year — and Meta’s billion-dollar spending spree is starting to look awfully risky.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/GettyImages-1157614908.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;How much do foundation models matter?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It might seem like a silly question, but it’s come up a lot in my conversations with AI startups, which are increasingly comfortable with businesses that used to be dismissed as “GPT wrappers,” or companies that build interfaces on top of existing AI models like ChatGPT. These days, startup teams are focused on customizing AI models for specific tasks and interface work, and see the foundation model as a commodity that can be swapped in and out as necessary. That approach was on display especially at last week’s Boxworks conference, which seemed devoted entirely to the user-facing software built on top of AI models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Part of what is driving this is that the scaling benefits of pre-training — that initial process of teaching AI models using massive datasets, which is the sole domain of foundation models — has slowed down. That doesn’t mean AI has stopped making progress, but the early benefits of hyperscaled foundational models have hit diminishing returns, and attention has turned to post-training and reinforcement learning as sources of future progress. If you want to make a better AI coding tool, you’re better off working on fine-tuning and interface design rather than spending another few billion dollars worth in server time on pre-training. As the success of Anthropic’s Claude Code shows, foundation model companies are quite good at these other fields too — but it’s not as durable an advantage as it used to be.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, the competitive landscape of AI is changing in ways that undermine the advantages of the biggest AI labs. Instead of a race for an all-powerful AGI that could match or exceed human abilities across all cognitive tasks, the immediate future looks like a flurry of discrete businesses: software development, enterprise data management, image generation and so on. Aside from a first-mover advantage, it’s not clear that building a foundation model gives you any advantage in those businesses. Worse, the abundance of open-source alternatives means that foundation models may not have any price leverage if they lose the competition at the application layer. This would turn companies like OpenAI and Anthropic into back-end suppliers in a low-margin commodity business – as one founder put it to me, “like selling coffee beans to Starbucks.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s hard to overstate what a dramatic shift this would be for the business of AI. Throughout the contemporary boom, the success of AI has been inextricable from the success of the companies building foundation models — specifically, OpenAI, Anthropic, and Google. Being bullish on AI meant believing that AI’s transformative impact would make these into generationally important companies. We could argue about which company would come out on top, but it was clear that some foundation model company was going to end up with the keys to the kingdom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the time, there were lots of reasons to think this was true. For years, foundation model development was the only AI business there was — and the fast pace of progress made their lead seem insurmountable. And Silicon Valley has always had a deep-rooted love of platform advantage. The assumption was that, however AI models ended up making money, the lion’s share of the benefit would flow back to the foundation model companies, who had done the work that was hardest to replicate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The past year has made that story more complicated. There are lots of successful third-party AI services, but they tend to use foundation models interchangeably. For startups, it no longer matters whether their product sits on top of GPT-5, Claude or Gemini, and they expect to be able to switch models in mid-release without end users noticing the difference. Foundation models continue to make real progress, but it no longer seems plausible for any one company to maintain a large enough advantage to dominate the industry.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;We already have plenty of indication that there is not much of a first-mover advantage. As venture capitalist Martin Casado of a16z pointed out on a recent podcast, OpenAI was the first lab to put out a coding model, as well as generative models for image and video — only to lose all three categories to competitors. “As far as we can tell, there is no inherent moat in the technology stack for AI,” Casado concluded.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, we shouldn’t count foundation model companies out just yet. There are still lots of durable advantages on their side, including brand recognition, infrastructure, and unthinkably vast cash reserves. OpenAI’s consumer business may prove harder to replicate than its coding business, and other advantages may emerge as the sector matures. Given the fast pace of AI development, the current interest in post-training could easily reverse course in the next six months. Most uncertain of all, the race toward general intelligence could pay off with new breakthroughs in pharmaceuticals or materials science, radically shifting our ideas about what makes AI models valuable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But in the meantime, the strategy of building ever-bigger foundation models looks a lot less appealing than it did last year — and Meta’s billion-dollar spending spree is starting to look awfully risky.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/14/selling-coffee-beans-to-starbucks-how-the-ai-boom-could-leave-ais-biggest-companies-behind/</guid><pubDate>Sun, 14 Sep 2025 15:00:00 +0000</pubDate></item><item><title>Rolling Stone owner Penske Media sues Google over AI summaries (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/14/rolling-stone-owner-penske-media-sues-google-over-ai-summaries/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2198713751.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google faces a new lawsuit accusing the company of illegally using news publishers’ content to create AI summaries that damage their business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuit comes from Penske Media Corporation (PMC), which owns industry publications such as Rolling Stone, Billboard, Variety, Hollywood Reporter, Deadline, Vibe, and Artforum. While Penske’s suit is the first to target Google and its parent company Alphabet over showing AI-generated summaries in search, both publishers and authors have sued other AI companies over related copyright concerns. Google also is also facing an antitrust complaint over AI Overviews in Europe.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As a leading global publisher, we have a duty to protect PMC’s best-in-class journalists and award-winning journalism as a source of truth,” said Penske Media CEO Jay Penske in a statement. “Furthermore, we have a responsibility to proactively fight for the future of digital media and preserve its integrity — all of which is threatened by Google’s current actions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching its AI Overviews last year, Google has been criticized for threatening the business models of the same publishers it relies on to provide the content needed to create accurate AI summaries and answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new lawsuit goes farther by accusing Google of continuing to “wield its monopoly to coerce PMC into permitting Google to republish PMC’s content in AI Overviews” and to use that content to train its AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google spokesperson José Castañeda said in a statement that AI Overviews make Google search “more helpful” and create “new opportunities for content to be discovered.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Every day, Google sends billions of clicks to sites across the web, and AI Overviews send traffic to a greater diversity of sites,” Castañeda said. “We will defend against these meritless claims.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuit argues that while Penske Media allows Google to crawl its websites in an “exchange of access for traffic” that is “the fundamental bargain that supports the production of content for the open commercial Web,” Google has recently “begun to tie its participation in this bargain to another transaction to which PMC and other publishers do not willingly consent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As a condition of indexing publisher content for search, Google now requires publishers to also supply that content for other uses that cannibalize or preempt search referrals,” the lawsuit claims, adding that the only way for Penske to opt out would be to remove itself from Google search entirely, which would be “devastating.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuit also claims that Penske has seen “significant declines in clicks from Google searches since Google started rolling out AI Overviews.” That means less ad revenue for the publisher, and it also threatens subscription and affiliate revenue, the company says: “These revenue streams rely on people actually &lt;em&gt;visiting&lt;/em&gt; PMC sites.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And while Google has pushed back against complaints that AI Overviews reduce traffic to publishers, the lawsuit says, “Google has offered no credible competing information regarding search referral traffic.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Penske’s suit comes after Google seemingly dodged an antitrust bullet — while a federal judge had ruled the company acted illegally to maintain a monopoly in online search, the judge did not to order the company to break up its businesses (for example by selling Chrome), due in part to an increasing competition in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This post has been updated with a statement from Jay Penske.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2198713751.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google faces a new lawsuit accusing the company of illegally using news publishers’ content to create AI summaries that damage their business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuit comes from Penske Media Corporation (PMC), which owns industry publications such as Rolling Stone, Billboard, Variety, Hollywood Reporter, Deadline, Vibe, and Artforum. While Penske’s suit is the first to target Google and its parent company Alphabet over showing AI-generated summaries in search, both publishers and authors have sued other AI companies over related copyright concerns. Google also is also facing an antitrust complaint over AI Overviews in Europe.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As a leading global publisher, we have a duty to protect PMC’s best-in-class journalists and award-winning journalism as a source of truth,” said Penske Media CEO Jay Penske in a statement. “Furthermore, we have a responsibility to proactively fight for the future of digital media and preserve its integrity — all of which is threatened by Google’s current actions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching its AI Overviews last year, Google has been criticized for threatening the business models of the same publishers it relies on to provide the content needed to create accurate AI summaries and answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new lawsuit goes farther by accusing Google of continuing to “wield its monopoly to coerce PMC into permitting Google to republish PMC’s content in AI Overviews” and to use that content to train its AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google spokesperson José Castañeda said in a statement that AI Overviews make Google search “more helpful” and create “new opportunities for content to be discovered.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Every day, Google sends billions of clicks to sites across the web, and AI Overviews send traffic to a greater diversity of sites,” Castañeda said. “We will defend against these meritless claims.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuit argues that while Penske Media allows Google to crawl its websites in an “exchange of access for traffic” that is “the fundamental bargain that supports the production of content for the open commercial Web,” Google has recently “begun to tie its participation in this bargain to another transaction to which PMC and other publishers do not willingly consent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As a condition of indexing publisher content for search, Google now requires publishers to also supply that content for other uses that cannibalize or preempt search referrals,” the lawsuit claims, adding that the only way for Penske to opt out would be to remove itself from Google search entirely, which would be “devastating.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuit also claims that Penske has seen “significant declines in clicks from Google searches since Google started rolling out AI Overviews.” That means less ad revenue for the publisher, and it also threatens subscription and affiliate revenue, the company says: “These revenue streams rely on people actually &lt;em&gt;visiting&lt;/em&gt; PMC sites.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And while Google has pushed back against complaints that AI Overviews reduce traffic to publishers, the lawsuit says, “Google has offered no credible competing information regarding search referral traffic.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Penske’s suit comes after Google seemingly dodged an antitrust bullet — while a federal judge had ruled the company acted illegally to maintain a monopoly in online search, the judge did not to order the company to break up its businesses (for example by selling Chrome), due in part to an increasing competition in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This post has been updated with a statement from Jay Penske.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/14/rolling-stone-owner-penske-media-sues-google-over-ai-summaries/</guid><pubDate>Sun, 14 Sep 2025 16:20:00 +0000</pubDate></item><item><title>Karen Hao on the Empire of AI, AGI evangelists, and the cost of belief (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/14/karen-hao-on-the-empire-of-ai-agi-evangelists-and-the-cost-of-belief/</link><description>&lt;figure class="wp-block-embed is-type-wp-embed is-provider-megaphone wp-block-embed-megaphone"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At the center of every empire is an ideology, a belief system that propels the system forward and justifies expansion – even if the cost of that expansion directly defies the ideology’s stated mission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For European colonial powers, it was Christianity and the promise of saving souls while extracting resources. For today’s AI empire, it’s artificial general intelligence to “benefit all humanity.” And OpenAI is its chief evangelist, spreading zeal across the industry in a way that has reframed how AI is built.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I was interviewing people whose voices were shaking from the fervor of their beliefs in AGI,” Karen Hao, journalist and bestselling author of “Empire of AI,” told TechCrunch on a recent episode of Equity.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In her book, Hao likens the AI industry in general, and OpenAI in particular, to an empire.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The only way to really understand the scope and scale of OpenAI’s behavior…is actually to recognize that they’ve already grown more powerful than pretty much any nation state in the world, and they’ve consolidated an extraordinary amount of not just economic power, but also political power,” Hao said.&lt;strong&gt; &lt;/strong&gt;“They’re terraforming the Earth. They’re rewiring our geopolitics, all of our lives. And so you can only describe it as an empire.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has described AGI as “a highly autonomous system that outperforms humans at most economically valuable work,” one that will somehow “elevate humanity by increasing abundance, turbocharging the economy, and aiding in the discovery of new scientific knowledge that changes the limits of possibility.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These nebulous promises have fueled the industry’s exponential growth — its massive resource demands, oceans of scraped data, strained energy grids, and willingness to release untested systems into the world. All in service of a future that many experts say may never arrive.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Hao says this path wasn’t inevitable, and that scaling isn’t the only way to get more advances in AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can also develop new techniques in algorithms,” she said. “You can improve the existing algorithms to reduce the amount of data and compute that they need to use.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But that tactic would have meant sacrificing speed.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“When you define the quest to build beneficial AGI as one where the victor takes all — which is what OpenAI did — then the most important thing is speed over anything else,” Hao said. “Speed over efficiency, speed over safety, speed over exploratory research.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Open AI Chief Executive Officer Sam Altman speaks during the Kakao media day in Seoul." class="wp-image-2966984" height="495" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181367.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kim Jae-Hwan/SOPA Images/LightRocket / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI, she said, the best way to guarantee speed was to take existing techniques and “just do the intellectually cheap thing, which is to pump more data, more supercomputers, into those existing techniques.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI set the stage, and rather than fall behind, other tech companies decided to fall in line.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And because the AI industry has successfully captured most of the top AI researchers in the world, and those researchers no longer exist in academia, then you have an entire discipline now being shaped by the agenda of these companies, rather than by real scientific exploration,” Hao said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The spend has been, and will be, astronomical. Last week, OpenAI said it expects to burn through $115 billion in cash by 2029. Meta said in July that it would spend up to $72 billion on building AI infrastructure this year. Google expects to hit up to $85 billion in capital expenditures for 2025, most of which will be spent on expanding AI and cloud infrastructure.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, the goal posts keep moving, and the loftiest “benefits to humanity” haven’t yet materialized, even as the harms mount. Harms like job loss, concentration of wealth, and AI chatbots that fuel delusions and psychosis.&lt;em&gt; &lt;/em&gt;In her book, Hao also documents workers in developing countries like Kenya and Venezuela who were exposed to disturbing content, including child sexual abuse material, and were paid very low wages — around $1 to $2 an hour — in roles like content moderation and data labeling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hao said it’s a false tradeoff to pit AI progress against present harms, especially when other forms of AI offer real benefits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She pointed to Google DeepMind’s Nobel Prize-winning AlphaFold, which is trained on amino acid sequence data and complex protein folding structures, and can now accurately predict the 3D structure of proteins from their amino acids — profoundly useful for drug discovery and understanding disease.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Those are the types of AI systems that we need,” Hao said. “AlphaFold does not create mental health crises in people. AlphaFold does not lead to colossal environmental harms … because it’s trained on substantially less infrastructure. It does not create content moderation harms because [the datasets don’t have] all of the toxic crap that you hoovered up when you were scraping the internet.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the quasi-religious commitment to AGI has been a narrative about the importance of racing to beat China in the AI race, so that Silicon Valley can have a liberalizing effect on the world.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Literally, the opposite has happened,” Hao said. “The gap has continued to close between the U.S. and China, and Silicon Valley has had an illiberalizing effect on the world … and the only actor that has come out of it unscathed, you could argue, is Silicon Valley itself.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, many will argue that OpenAI and other AI companies have benefitted humanity by releasing ChatGPT and other large language models, which promise huge gains in productivity by automating tasks like coding, writing, research, customer support, and other knowledge-work tasks.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the way OpenAI is structured — part non-profit, part for-profit — complicates how it defines and measures its impact on humanity. And that’s further complicated by the news this week that OpenAI reached an agreement with Microsoft that brings it closer to eventually going public.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Two former OpenAI safety researchers told TechCrunch that they fear the AI lab has begun to confuse its for-profit and non-profit missions — that because people enjoy using ChatGPT and other products built on LLMs, this ticks the box of benefiting humanity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hao echoed these concerns, describing the dangers of being so consumed by the mission that reality is ignored. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even as the evidence accumulates that what they’re building is actually harming significant amounts of people, the mission continues to paper all of that over,” Hao said. “There’s something really dangerous and dark about that, of [being] so wrapped up in a belief system you constructed that you lose touch with reality.”&lt;/p&gt;</description><content:encoded>&lt;figure class="wp-block-embed is-type-wp-embed is-provider-megaphone wp-block-embed-megaphone"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At the center of every empire is an ideology, a belief system that propels the system forward and justifies expansion – even if the cost of that expansion directly defies the ideology’s stated mission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For European colonial powers, it was Christianity and the promise of saving souls while extracting resources. For today’s AI empire, it’s artificial general intelligence to “benefit all humanity.” And OpenAI is its chief evangelist, spreading zeal across the industry in a way that has reframed how AI is built.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I was interviewing people whose voices were shaking from the fervor of their beliefs in AGI,” Karen Hao, journalist and bestselling author of “Empire of AI,” told TechCrunch on a recent episode of Equity.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In her book, Hao likens the AI industry in general, and OpenAI in particular, to an empire.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The only way to really understand the scope and scale of OpenAI’s behavior…is actually to recognize that they’ve already grown more powerful than pretty much any nation state in the world, and they’ve consolidated an extraordinary amount of not just economic power, but also political power,” Hao said.&lt;strong&gt; &lt;/strong&gt;“They’re terraforming the Earth. They’re rewiring our geopolitics, all of our lives. And so you can only describe it as an empire.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has described AGI as “a highly autonomous system that outperforms humans at most economically valuable work,” one that will somehow “elevate humanity by increasing abundance, turbocharging the economy, and aiding in the discovery of new scientific knowledge that changes the limits of possibility.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These nebulous promises have fueled the industry’s exponential growth — its massive resource demands, oceans of scraped data, strained energy grids, and willingness to release untested systems into the world. All in service of a future that many experts say may never arrive.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Hao says this path wasn’t inevitable, and that scaling isn’t the only way to get more advances in AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can also develop new techniques in algorithms,” she said. “You can improve the existing algorithms to reduce the amount of data and compute that they need to use.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But that tactic would have meant sacrificing speed.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“When you define the quest to build beneficial AGI as one where the victor takes all — which is what OpenAI did — then the most important thing is speed over anything else,” Hao said. “Speed over efficiency, speed over safety, speed over exploratory research.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Open AI Chief Executive Officer Sam Altman speaks during the Kakao media day in Seoul." class="wp-image-2966984" height="495" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181367.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kim Jae-Hwan/SOPA Images/LightRocket / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI, she said, the best way to guarantee speed was to take existing techniques and “just do the intellectually cheap thing, which is to pump more data, more supercomputers, into those existing techniques.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI set the stage, and rather than fall behind, other tech companies decided to fall in line.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And because the AI industry has successfully captured most of the top AI researchers in the world, and those researchers no longer exist in academia, then you have an entire discipline now being shaped by the agenda of these companies, rather than by real scientific exploration,” Hao said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The spend has been, and will be, astronomical. Last week, OpenAI said it expects to burn through $115 billion in cash by 2029. Meta said in July that it would spend up to $72 billion on building AI infrastructure this year. Google expects to hit up to $85 billion in capital expenditures for 2025, most of which will be spent on expanding AI and cloud infrastructure.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, the goal posts keep moving, and the loftiest “benefits to humanity” haven’t yet materialized, even as the harms mount. Harms like job loss, concentration of wealth, and AI chatbots that fuel delusions and psychosis.&lt;em&gt; &lt;/em&gt;In her book, Hao also documents workers in developing countries like Kenya and Venezuela who were exposed to disturbing content, including child sexual abuse material, and were paid very low wages — around $1 to $2 an hour — in roles like content moderation and data labeling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hao said it’s a false tradeoff to pit AI progress against present harms, especially when other forms of AI offer real benefits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She pointed to Google DeepMind’s Nobel Prize-winning AlphaFold, which is trained on amino acid sequence data and complex protein folding structures, and can now accurately predict the 3D structure of proteins from their amino acids — profoundly useful for drug discovery and understanding disease.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Those are the types of AI systems that we need,” Hao said. “AlphaFold does not create mental health crises in people. AlphaFold does not lead to colossal environmental harms … because it’s trained on substantially less infrastructure. It does not create content moderation harms because [the datasets don’t have] all of the toxic crap that you hoovered up when you were scraping the internet.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the quasi-religious commitment to AGI has been a narrative about the importance of racing to beat China in the AI race, so that Silicon Valley can have a liberalizing effect on the world.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Literally, the opposite has happened,” Hao said. “The gap has continued to close between the U.S. and China, and Silicon Valley has had an illiberalizing effect on the world … and the only actor that has come out of it unscathed, you could argue, is Silicon Valley itself.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, many will argue that OpenAI and other AI companies have benefitted humanity by releasing ChatGPT and other large language models, which promise huge gains in productivity by automating tasks like coding, writing, research, customer support, and other knowledge-work tasks.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the way OpenAI is structured — part non-profit, part for-profit — complicates how it defines and measures its impact on humanity. And that’s further complicated by the news this week that OpenAI reached an agreement with Microsoft that brings it closer to eventually going public.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Two former OpenAI safety researchers told TechCrunch that they fear the AI lab has begun to confuse its for-profit and non-profit missions — that because people enjoy using ChatGPT and other products built on LLMs, this ticks the box of benefiting humanity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hao echoed these concerns, describing the dangers of being so consumed by the mission that reality is ignored. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even as the evidence accumulates that what they’re building is actually harming significant amounts of people, the mission continues to paper all of that over,” Hao said. “There’s something really dangerous and dark about that, of [being] so wrapped up in a belief system you constructed that you lose touch with reality.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/14/karen-hao-on-the-empire-of-ai-agi-evangelists-and-the-cost-of-belief/</guid><pubDate>Sun, 14 Sep 2025 17:00:34 +0000</pubDate></item><item><title>[NEW] Users turn to chatbots for spiritual guidance (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/14/users-turn-to-chatbots-for-spiritual-guidance/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-1533302708.jpg?resize=1200,720" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI-powered chatbots play a growing role in spiritual life, according to a New York Times story that examines the popularity of religious chatbots and apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Times notes that an app called Bible Chat has been downloaded more than 30 million times, while another app, Hallow, reached the number one spot in Apple’s App Store last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For the most part, these apps are supposed to point people to religious doctrine and scripture to answer their questions, although at least one website purports to allow users to chat with God. Rabbi Jonathan Roman suggested chatbots could be a “way into faith” for “a whole generation of people who have never been to a church or synagogue.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, these chatbots are built on top of AI models that are designed to validate users’ opinions, to the point that they can reinforce delusional or conspiratorial thinking. Heidi Campbell, a Texas A&amp;amp;M professor who studies the intersection of digital culture and religion, warned that chatbots “tell us what we want to hear.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s not using spiritual discernment, it is using data and patterns,” Campbell said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-1533302708.jpg?resize=1200,720" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI-powered chatbots play a growing role in spiritual life, according to a New York Times story that examines the popularity of religious chatbots and apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Times notes that an app called Bible Chat has been downloaded more than 30 million times, while another app, Hallow, reached the number one spot in Apple’s App Store last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For the most part, these apps are supposed to point people to religious doctrine and scripture to answer their questions, although at least one website purports to allow users to chat with God. Rabbi Jonathan Roman suggested chatbots could be a “way into faith” for “a whole generation of people who have never been to a church or synagogue.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, these chatbots are built on top of AI models that are designed to validate users’ opinions, to the point that they can reinforce delusional or conspiratorial thinking. Heidi Campbell, a Texas A&amp;amp;M professor who studies the intersection of digital culture and religion, warned that chatbots “tell us what we want to hear.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s not using spiritual discernment, it is using data and patterns,” Campbell said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/14/users-turn-to-chatbots-for-spiritual-guidance/</guid><pubDate>Sun, 14 Sep 2025 18:45:26 +0000</pubDate></item><item><title>[NEW] Vibe coding has turned senior devs into ‘AI babysitters,’ but they say it’s worth it (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/14/vibe-coding-has-turned-senior-devs-into-ai-babysitters-but-they-say-its-worth-it/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Carla Rover once spent 30 minutes sobbing after having to restart a project she vibe coded.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rover has been in the industry for 15 years, mainly working as a web developer. She’s now building a startup, alongside her son, that creates custom machine learning models for marketplaces.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;She called vibe coding a beautiful, endless cocktail napkin on which one can perpetually sketch ideas. But dealing with AI-generated code that one hopes to use in production can be “worse than babysitting,” she said, as these AI models can mess up work in ways that are hard to predict.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She had turned to AI coding in a need for speed with her startup, as is the promise of AI tools.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Because I needed to be quick and impressive, I took a shortcut and did not scan those files after the automated review,” she said. “When I did do it manually, I found so much wrong. When I used a third-party tool, I found more. And I learned my lesson.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She and her son wound up restarting their whole project — hence the tears. “I handed it off like the copilot was an employee,” she said. “It isn’t.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rover is like many experienced programmers turning to AI for coding help. But such programmers are also finding themselves acting like AI babysitters — rewriting and fact-checking the code the AI spits out.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A recent report by content delivery platform company Fastly found that at least 95% of the nearly 800 developers it surveyed said they spend extra time fixing AI-generated code, with the load of such verification falling most heavily on the shoulders of senior developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These experienced coders have discovered issues with AI-generated code ranging from hallucinating package names to deleting important information and security risks. Left unchecked, AI code can leave a product far more buggy than what humans would produce.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Working with AI-generated code has become such a problem that it’s given rise to a new corporate coding job known as “vibe code cleanup specialist.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch spoke to experienced coders about their time using AI-generated code about what they see as the future of vibe coding. Thoughts varied, but one thing remained certain: The technology still has a long way to go.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Using a coding co-pilot is kind of like giving a coffee pot to a smart six-year-old and saying, ‘Please take this into the dining room and pour coffee for the family,’” Rover said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Can they do it? Possibly. Could they fail? Definitely. And most likely, if they do fail, they aren’t going to tell you. “It doesn’t make the kid less clever,” she continued. “It just means you can’t delegate [a task] like that completely.”&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-you-re-absolutely-right-nbsp"&gt;“You’re absolutely right!”&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Feridoon Malekzadeh also compared vibe coding to a child.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’s worked in the industry for more than 20 years, holding various roles in product development, software, and design. He’s building his own startup and heavily using vibe-coding platform Lovable, he said. For fun, he also vibe codes apps like one that generates Gen Alpha slang for Boomers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He likes that he’s able to work alone on projects, saving time and money, but agrees that vibe coding is not like hiring an intern or a junior coder. Instead, vibe coding is akin to “hiring your stubborn, insolent teenager to help you do something,” he told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You have to ask them 15 times to do something,” he said. “In the end, they do some of what you asked, some stuff you didn’t ask for, and they break a bunch of things along the way.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Malekzadeh estimates he spends around 50% of his time writing requirements, 10% to 20% of his time on vibe coding, and 30% to 40% of his time on vibe &lt;em&gt;fixing &lt;/em&gt;— remedying the bugs and “unnecessary script” created by AI-written code.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He also doesn’t think vibe coding is the best at systems thinking — the process of seeing how a complex problem could impact an overall result. AI-generated code, he said, tries to solve more surface-level problems.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you’re creating a feature that should be broadly available in your product, a good engineer would create that once and make it available everywhere that it’s needed,” Malekzadeh said. “Vibe coding will create something five different times, five different ways, if it’s needed in five different places. It leads to a lot of confusion, not only for the user, but for the model.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meanwhile, Rover finds that AI “runs into a wall” when data conflicts with what it was hard-coded to do. “It can offer misleading advice, leave out key elements that are vital, or insert itself into a thought pathway you’re developing,” she said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She also found that rather than admit to making errors, it will manufacture results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She shared another example with TechCrunch, where she questioned the results an AI model initially gave her. The model started to give a detailed explanation pretending it used the data she uploaded. Only when she called it out did the AI model confess. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It freaked me out because it sounded like a toxic co-worker,” she said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3045599" height="224" src="https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3045607" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On top of this, there are the security concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Austin Spires is the senior director of developer enablement at Fastly and has been coding since the early 2000s.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He’s found through his own experience — along with chatting with customers — that vibe code likes to build what is quick rather than what is “right.” This may introduce vulnerabilities to the code of the kind that very new programmers tend to make, he said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What often happens is the engineer needs to review the code, correct the agent, and tell the agent that they made a mistake,” Spires told TechCrunch. “This pattern is why we’ve seen the trope of ‘you’re absolutely right’ appear over social media.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He’s referring to how AI models, like Anthropic Claude, tend to respond “you’re absolutely right” when called out on their mistakes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mike Arrowsmith, the chief technology officer at the IT management software company NinjaOne, has been in software engineering and security for around 20 years. He said that vibe coding is creating a new generation of IT and security blind spots to which young startups in particular are susceptible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Vibe coding often bypasses the rigorous review processes that are foundational to traditional coding and crucial to catching vulnerabilities,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;NinjaOne, he said, counters this by encouraging “safe vibe coding,” where approved AI tools have access controls, along with mandatory peer review and, of course, security scanning.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-new-normal"&gt;The new normal&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While nearly everyone we spoke to agrees that AI-generated code and vibe-coding platforms are useful in many situations — like mocking up ideas — they all agree that human review is essential before building a business on it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That cocktail napkin is not a business model,” Rover said. “You have to balance the ease with insight.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But for all the lamenting on its errors, vibe coding has changed the present and the future of the job.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rover said vibe coding helped her tremendously in crafting a better user interface. Malekzadeh simply said that, despite the time he spends fixing code, he still gets more done with AI coders than without them.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“‘Every technology carries its own negativity, which is invented at the same time as technical progress,” Malekzadeh said, quoting the French theorist Paul Virilio, who spoke about inventing the shipwreck along with the ship.&lt;/p&gt;

&lt;figure class="wp-block-pullquote alignleft"&gt;&lt;blockquote&gt;&lt;p&gt;The pros far outweigh the cons.&lt;/p&gt;&lt;/blockquote&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Fastly survey found that senior developers were twice as likely to put AI-generated code into production compared to junior developers, saying that the technology helped them work faster.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vibe coding is also part of Spires’ coding routine. He uses AI coding agents on several platforms for both front-end and back-end personal projects. He called the technology a mixed experience but said it’s good in helping with prototyping, building out boilerplate, or scaffolding out a test; it removes menial tasks so that engineers can focus on building, shipping, and scaling products.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It seems the extra hours spent combing through the vibe weeds will simply become a tolerated tax on using the innovation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Elvis Kimara, a young engineer, is learning that now. He just graduated with a master’s in AI and is building an AI-powered marketplace.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like many coders, he said vibe coding has made his job harder and has often found vibe coding a joyless experience.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There’s no more dopamine from solving a problem by myself. The AI just figures it out,” he said. At one of his last jobs, he said senior developers didn’t look to help young coders as much&amp;nbsp;— some not understanding new vibe-coding models, while others delegated mentorship tasks to said AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But, he said, “the pros far outweigh the cons,” and he’s prepared to pay the innovation tax.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We won’t just be writing code; we’ll be guiding AI systems, taking accountability when things break, and acting more like consultants to machines,” Kimara said of the new normal for which he’s preparing.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even as I grow into a senior role, I’ll keep using it,” he continued. “It’s been a real accelerator for me. I make sure I review every line of AI-generated code so I learn even faster from it.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Carla Rover once spent 30 minutes sobbing after having to restart a project she vibe coded.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rover has been in the industry for 15 years, mainly working as a web developer. She’s now building a startup, alongside her son, that creates custom machine learning models for marketplaces.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;She called vibe coding a beautiful, endless cocktail napkin on which one can perpetually sketch ideas. But dealing with AI-generated code that one hopes to use in production can be “worse than babysitting,” she said, as these AI models can mess up work in ways that are hard to predict.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She had turned to AI coding in a need for speed with her startup, as is the promise of AI tools.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Because I needed to be quick and impressive, I took a shortcut and did not scan those files after the automated review,” she said. “When I did do it manually, I found so much wrong. When I used a third-party tool, I found more. And I learned my lesson.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She and her son wound up restarting their whole project — hence the tears. “I handed it off like the copilot was an employee,” she said. “It isn’t.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rover is like many experienced programmers turning to AI for coding help. But such programmers are also finding themselves acting like AI babysitters — rewriting and fact-checking the code the AI spits out.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A recent report by content delivery platform company Fastly found that at least 95% of the nearly 800 developers it surveyed said they spend extra time fixing AI-generated code, with the load of such verification falling most heavily on the shoulders of senior developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These experienced coders have discovered issues with AI-generated code ranging from hallucinating package names to deleting important information and security risks. Left unchecked, AI code can leave a product far more buggy than what humans would produce.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Working with AI-generated code has become such a problem that it’s given rise to a new corporate coding job known as “vibe code cleanup specialist.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch spoke to experienced coders about their time using AI-generated code about what they see as the future of vibe coding. Thoughts varied, but one thing remained certain: The technology still has a long way to go.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Using a coding co-pilot is kind of like giving a coffee pot to a smart six-year-old and saying, ‘Please take this into the dining room and pour coffee for the family,’” Rover said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Can they do it? Possibly. Could they fail? Definitely. And most likely, if they do fail, they aren’t going to tell you. “It doesn’t make the kid less clever,” she continued. “It just means you can’t delegate [a task] like that completely.”&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-you-re-absolutely-right-nbsp"&gt;“You’re absolutely right!”&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Feridoon Malekzadeh also compared vibe coding to a child.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’s worked in the industry for more than 20 years, holding various roles in product development, software, and design. He’s building his own startup and heavily using vibe-coding platform Lovable, he said. For fun, he also vibe codes apps like one that generates Gen Alpha slang for Boomers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He likes that he’s able to work alone on projects, saving time and money, but agrees that vibe coding is not like hiring an intern or a junior coder. Instead, vibe coding is akin to “hiring your stubborn, insolent teenager to help you do something,” he told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You have to ask them 15 times to do something,” he said. “In the end, they do some of what you asked, some stuff you didn’t ask for, and they break a bunch of things along the way.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Malekzadeh estimates he spends around 50% of his time writing requirements, 10% to 20% of his time on vibe coding, and 30% to 40% of his time on vibe &lt;em&gt;fixing &lt;/em&gt;— remedying the bugs and “unnecessary script” created by AI-written code.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He also doesn’t think vibe coding is the best at systems thinking — the process of seeing how a complex problem could impact an overall result. AI-generated code, he said, tries to solve more surface-level problems.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you’re creating a feature that should be broadly available in your product, a good engineer would create that once and make it available everywhere that it’s needed,” Malekzadeh said. “Vibe coding will create something five different times, five different ways, if it’s needed in five different places. It leads to a lot of confusion, not only for the user, but for the model.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meanwhile, Rover finds that AI “runs into a wall” when data conflicts with what it was hard-coded to do. “It can offer misleading advice, leave out key elements that are vital, or insert itself into a thought pathway you’re developing,” she said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She also found that rather than admit to making errors, it will manufacture results.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She shared another example with TechCrunch, where she questioned the results an AI model initially gave her. The model started to give a detailed explanation pretending it used the data she uploaded. Only when she called it out did the AI model confess. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It freaked me out because it sounded like a toxic co-worker,” she said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3045599" height="224" src="https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-2.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3045607" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Rover-example-3.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On top of this, there are the security concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Austin Spires is the senior director of developer enablement at Fastly and has been coding since the early 2000s.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He’s found through his own experience — along with chatting with customers — that vibe code likes to build what is quick rather than what is “right.” This may introduce vulnerabilities to the code of the kind that very new programmers tend to make, he said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What often happens is the engineer needs to review the code, correct the agent, and tell the agent that they made a mistake,” Spires told TechCrunch. “This pattern is why we’ve seen the trope of ‘you’re absolutely right’ appear over social media.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He’s referring to how AI models, like Anthropic Claude, tend to respond “you’re absolutely right” when called out on their mistakes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mike Arrowsmith, the chief technology officer at the IT management software company NinjaOne, has been in software engineering and security for around 20 years. He said that vibe coding is creating a new generation of IT and security blind spots to which young startups in particular are susceptible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Vibe coding often bypasses the rigorous review processes that are foundational to traditional coding and crucial to catching vulnerabilities,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;NinjaOne, he said, counters this by encouraging “safe vibe coding,” where approved AI tools have access controls, along with mandatory peer review and, of course, security scanning.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-new-normal"&gt;The new normal&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While nearly everyone we spoke to agrees that AI-generated code and vibe-coding platforms are useful in many situations — like mocking up ideas — they all agree that human review is essential before building a business on it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That cocktail napkin is not a business model,” Rover said. “You have to balance the ease with insight.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But for all the lamenting on its errors, vibe coding has changed the present and the future of the job.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rover said vibe coding helped her tremendously in crafting a better user interface. Malekzadeh simply said that, despite the time he spends fixing code, he still gets more done with AI coders than without them.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“‘Every technology carries its own negativity, which is invented at the same time as technical progress,” Malekzadeh said, quoting the French theorist Paul Virilio, who spoke about inventing the shipwreck along with the ship.&lt;/p&gt;

&lt;figure class="wp-block-pullquote alignleft"&gt;&lt;blockquote&gt;&lt;p&gt;The pros far outweigh the cons.&lt;/p&gt;&lt;/blockquote&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Fastly survey found that senior developers were twice as likely to put AI-generated code into production compared to junior developers, saying that the technology helped them work faster.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vibe coding is also part of Spires’ coding routine. He uses AI coding agents on several platforms for both front-end and back-end personal projects. He called the technology a mixed experience but said it’s good in helping with prototyping, building out boilerplate, or scaffolding out a test; it removes menial tasks so that engineers can focus on building, shipping, and scaling products.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It seems the extra hours spent combing through the vibe weeds will simply become a tolerated tax on using the innovation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Elvis Kimara, a young engineer, is learning that now. He just graduated with a master’s in AI and is building an AI-powered marketplace.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like many coders, he said vibe coding has made his job harder and has often found vibe coding a joyless experience.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There’s no more dopamine from solving a problem by myself. The AI just figures it out,” he said. At one of his last jobs, he said senior developers didn’t look to help young coders as much&amp;nbsp;— some not understanding new vibe-coding models, while others delegated mentorship tasks to said AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But, he said, “the pros far outweigh the cons,” and he’s prepared to pay the innovation tax.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We won’t just be writing code; we’ll be guiding AI systems, taking accountability when things break, and acting more like consultants to machines,” Kimara said of the new normal for which he’s preparing.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even as I grow into a senior role, I’ll keep using it,” he continued. “It’s been a real accelerator for me. I make sure I review every line of AI-generated code so I learn even faster from it.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/14/vibe-coding-has-turned-senior-devs-into-ai-babysitters-but-they-say-its-worth-it/</guid><pubDate>Sun, 14 Sep 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI board chair Bret Taylor says we’re in an AI bubble (but that’s okay) (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/14/openai-board-chair-bret-taylor-says-were-in-an-ai-bubble-but-thats-okay/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/AP22166762614393.jpg?resize=1200,700" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Bret Taylor, board chair at OpenAI and CEO of AI agent startup Sierra, was asked in a recent interview with The Verge whether he agreed with OpenAI CEO Sam Altman’s declaration that “someone is going to lose a phenomenal amount of money in AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor echoed Altman’s sentiments, suggesting that we are indeed in an AI bubble — but like Altman, he didn’t sound too worried about it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think it is both true that AI will transform the economy, and I think it will, like the internet, create huge amounts of economic value in the future,” Taylor said. “I think we’re also in a bubble, and a lot of people will lose a lot of money. I think both are absolutely true at the same time, and there’s a lot of historical precedent for both of those things being true at the same time.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Specifically, Taylor compared today’s AI landscape to the dot-com bubble of the late ‘90s. While many companies failed when the bubble burst, he argued that “all the people in 1999 were kind of right.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/AP22166762614393.jpg?resize=1200,700" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Bret Taylor, board chair at OpenAI and CEO of AI agent startup Sierra, was asked in a recent interview with The Verge whether he agreed with OpenAI CEO Sam Altman’s declaration that “someone is going to lose a phenomenal amount of money in AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor echoed Altman’s sentiments, suggesting that we are indeed in an AI bubble — but like Altman, he didn’t sound too worried about it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think it is both true that AI will transform the economy, and I think it will, like the internet, create huge amounts of economic value in the future,” Taylor said. “I think we’re also in a bubble, and a lot of people will lose a lot of money. I think both are absolutely true at the same time, and there’s a lot of historical precedent for both of those things being true at the same time.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Specifically, Taylor compared today’s AI landscape to the dot-com bubble of the late ‘90s. While many companies failed when the bubble burst, he argued that “all the people in 1999 were kind of right.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/14/openai-board-chair-bret-taylor-says-were-in-an-ai-bubble-but-thats-okay/</guid><pubDate>Sun, 14 Sep 2025 19:33:22 +0000</pubDate></item></channel></rss>