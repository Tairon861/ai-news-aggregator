<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 20 Jan 2026 18:41:41 +0000</lastBuildDate><item><title>[NEW] The Download: digitizing India, and scoring embryos (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/20/1131459/the-download-digitizing-india-and-scoring-embryos/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;The man who made India digital isn‚Äôt done yet&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Nandan Nilekani can‚Äôt stop trying to push India into the future. He started nearly 30 years ago, masterminding an ongoing experiment in technological state capacity that started with Aadhaar‚Äîthe world‚Äôs largest digital identity system.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Using Aadhaar as the bedrock, Nilekani and people working with him went on to build a sprawling collection of free, interoperating online tools that add up to nothing less than a digital infrastructure for society, covering government services, banking, and health care. They offer convenience and access that would be eye-popping in wealthy countries a tenth of India‚Äôs size.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At 70 years old, Nilekani should be retired. But he has a few more ideas. Read our profile to learn about what he‚Äôs set his sights on next.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;‚ÄîEdd Gent&lt;/em&gt;&lt;/p&gt;   
 &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Embryo scoring is slowly becoming more mainstream&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;Many Americans agree that it‚Äôs acceptable to screen embryos for severe genetic diseases. Far fewer say it‚Äôs okay to test for characteristics related to a future child‚Äôs appearance, behavior, or intelligence. But a few startups are now advertising what they claim is a way to do just that.&lt;/p&gt;  &lt;p&gt;This new kind of testing‚Äîwhich can cost up to $50,000‚Äîis incredibly controversial. Nevertheless, the practice has grown popular in Silicon Valley, and it‚Äôs becoming more widely available to everyone. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;‚ÄîJulia Black&lt;/em&gt;&lt;br /&gt;&lt;strong&gt;Embryo scoring&lt;/strong&gt;&lt;strong&gt; is one of our 10 Breakthrough Technologies this year. &lt;/strong&gt;&lt;strong&gt;Check out what else made the list&lt;/strong&gt;&lt;strong&gt;, and scroll down to vote for the technology you think deserves the 11th slot.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;h2 class="wp-block-heading"&gt;Five AI predictions for 2026&lt;/h2&gt;  &lt;p&gt;What will surprise us most about AI in 2026?&lt;/p&gt;  &lt;p&gt;Tune in at 12.30pm today to hear me, our senior AI editor Will Douglas Heaven and senior AI reporter James O'Donnell discuss our&amp;nbsp;‚Äú5 AI Predictions for 2026‚Äù. This&amp;nbsp;special LinkedIn Live event will explore the trends that are poised to transform the next twelve months of AI. The conversation will also offer a first glimpse at EmTech AI 2026, MIT Technology Review‚Äôs longest running AI event for business leadership.&amp;nbsp;Sign up to join us later today!&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Europe is trying to build its own DeepSeek&lt;/strong&gt;&lt;br /&gt;That‚Äôs been a goal for a while, but US hostility is making those efforts newly urgent. (Wired&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Plenty of Europeans want to wean off US technology. That‚Äôs easier said than done.&amp;nbsp;&lt;/em&gt;(New Scientist&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;DeepSeek may have found a new way to improve AI‚Äôs ability to remember.&amp;nbsp;&lt;/em&gt;(MIT Technology Review&amp;nbsp;$)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Ship-tracking data shows China is creating massive floating barriers&lt;/strong&gt;&lt;br /&gt;The maneuvers show that Beijing can now rapidly muster large numbers of the boats in disputed seas. (NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Quantum navigation could solve the military‚Äôs GPS jamming problem&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 The AI bubble risks disrupting the global economy, says the IMF&lt;/strong&gt;&lt;br /&gt;But it‚Äôs hard to see anyone pumping the brakes any time soon. (FT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;British politicians say the UK is being exposed to ‚Äòserious harm‚Äô by AI risks&lt;/em&gt;. (The Guardian)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;What even is the AI bubble?&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Cryptocurrencies are dying in record numbers&lt;/strong&gt;&lt;br /&gt;In an era of one-off joke coins and pump and dump scams, that‚Äôs surely a good thing. (Gizmodo)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;President Trump has pardoned a lot of people who‚Äôve committed financial crimes.&lt;/em&gt;&amp;nbsp;(NBC)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 Threads has more global daily mobile users than X now&lt;/strong&gt;&lt;br /&gt;And once-popular alternative Bluesky barely even makes the charts. (Forbes)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 The UK is considering banning under 16s from social media&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Just weeks after a similar ban took effect in Australia. (BBC)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 You can burn yourself out with AI coding agents&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;They could be set to make experienced programmers busier than ever before. (Ars Technica)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Why Anthropic‚Äôs Claude Code is taking the AI world by storm.&amp;nbsp;&lt;/em&gt;(WSJ&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;AI coding is now everywhere. But not everyone is convinced.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Some tech billionaires are leaving California&amp;nbsp;&lt;/strong&gt;üëã&lt;br /&gt;Not all though‚Äîthe founders of Nvidia and Airbnb say they‚Äôll stay and pay the 5% wealth tax. (WP&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Tech bosses‚Äô support for Trump is paying off for them big time.&amp;nbsp;&lt;/em&gt;(FT&amp;nbsp;$)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Matt Damon says Netflix tells directors to repeat movie plots&lt;/strong&gt;&lt;br /&gt;To accommodate all the people using their phones. (NME)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Why more people are going analog in 2026&amp;nbsp;&lt;/strong&gt;üß∂&lt;br /&gt;Crafting, reading, and other screen-free hobbies are on the rise. (CNN)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Dumbphones are becoming popular too‚Äîbut it‚Äôs worth thinking hard before you switch.&amp;nbsp;&lt;/em&gt;(Wired&amp;nbsp;$)&lt;/p&gt; 
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;‚ÄòIt may sound like American chauvinism‚Ä¶and it is. We‚Äôre done apologising about that.‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;‚ÄîThomas Dans, a Trump appointee who heads the US Arctic Research Commission, tells the&amp;nbsp;FT&amp;nbsp;his boss is deadly serious about acquiring Greenland.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1049096" src="https://wp.technologyreview.com/wp-content/uploads/2022/04/Sugar_0125_wk-crop.jpg?w=3000" /&gt;&lt;div class="image-credit"&gt;BRUCE PETERSON&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;Inside the fierce, messy fight over ‚Äúhealthy‚Äù sugar tech&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;On the outskirts of Charlottesville, Virginia, a new kind of sugar factory is taking shape. The facility is being developed by a startup called Bonumose. It uses a processed corn product called maltodextrin that is found in many junk foods and is calorically similar to table sugar (sucrose).&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But for Bonumose, maltodextrin isn‚Äôt an ingredient‚Äîit‚Äôs a raw material. When it‚Äôs poured into the company‚Äôs bioreactors, what emerges is tagatose. Found naturally in small concentrations in fruit, some grains, and milk, it is nearly as sweet as sucrose but apparently with only around half the calories, and wider health benefits.&lt;/p&gt;  &lt;p&gt;Bonumose‚Äôs process originated in a company spun out of the Virginia Tech lab of Yi-Heng ‚ÄúPercival‚Äù Zhang. When &lt;em&gt;MIT Technology Review &lt;/em&gt;spoke to Zhang, he was sitting alone in an empty lab in Tianjin, China, after serving a two-year sentence of supervised release in Virginia for conspiracy to defraud the US government, making false statements, and obstruction of justice. If sugar is the new oil, the global battle to control it has already begun. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;‚ÄîMark Harris&lt;/em&gt;&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Paul Mescal just keeps getting&amp;nbsp;cooler.&lt;br /&gt;+ Make this year calmer with these evidence-backed&amp;nbsp;tips. ($)&lt;br /&gt;+ I can confirm that&amp;nbsp;Lumie wake-up lamps&amp;nbsp;really are worth it (and no one paid me to say so!)&lt;br /&gt;+ There are some real gems in Green Day‚Äôs bassist Mike Dirnt‚Äôs&amp;nbsp;favorite albums&amp;nbsp;list.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;The man who made India digital isn‚Äôt done yet&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Nandan Nilekani can‚Äôt stop trying to push India into the future. He started nearly 30 years ago, masterminding an ongoing experiment in technological state capacity that started with Aadhaar‚Äîthe world‚Äôs largest digital identity system.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Using Aadhaar as the bedrock, Nilekani and people working with him went on to build a sprawling collection of free, interoperating online tools that add up to nothing less than a digital infrastructure for society, covering government services, banking, and health care. They offer convenience and access that would be eye-popping in wealthy countries a tenth of India‚Äôs size.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At 70 years old, Nilekani should be retired. But he has a few more ideas. Read our profile to learn about what he‚Äôs set his sights on next.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;‚ÄîEdd Gent&lt;/em&gt;&lt;/p&gt;   
 &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Embryo scoring is slowly becoming more mainstream&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;Many Americans agree that it‚Äôs acceptable to screen embryos for severe genetic diseases. Far fewer say it‚Äôs okay to test for characteristics related to a future child‚Äôs appearance, behavior, or intelligence. But a few startups are now advertising what they claim is a way to do just that.&lt;/p&gt;  &lt;p&gt;This new kind of testing‚Äîwhich can cost up to $50,000‚Äîis incredibly controversial. Nevertheless, the practice has grown popular in Silicon Valley, and it‚Äôs becoming more widely available to everyone. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;‚ÄîJulia Black&lt;/em&gt;&lt;br /&gt;&lt;strong&gt;Embryo scoring&lt;/strong&gt;&lt;strong&gt; is one of our 10 Breakthrough Technologies this year. &lt;/strong&gt;&lt;strong&gt;Check out what else made the list&lt;/strong&gt;&lt;strong&gt;, and scroll down to vote for the technology you think deserves the 11th slot.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;h2 class="wp-block-heading"&gt;Five AI predictions for 2026&lt;/h2&gt;  &lt;p&gt;What will surprise us most about AI in 2026?&lt;/p&gt;  &lt;p&gt;Tune in at 12.30pm today to hear me, our senior AI editor Will Douglas Heaven and senior AI reporter James O'Donnell discuss our&amp;nbsp;‚Äú5 AI Predictions for 2026‚Äù. This&amp;nbsp;special LinkedIn Live event will explore the trends that are poised to transform the next twelve months of AI. The conversation will also offer a first glimpse at EmTech AI 2026, MIT Technology Review‚Äôs longest running AI event for business leadership.&amp;nbsp;Sign up to join us later today!&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Europe is trying to build its own DeepSeek&lt;/strong&gt;&lt;br /&gt;That‚Äôs been a goal for a while, but US hostility is making those efforts newly urgent. (Wired&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Plenty of Europeans want to wean off US technology. That‚Äôs easier said than done.&amp;nbsp;&lt;/em&gt;(New Scientist&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;DeepSeek may have found a new way to improve AI‚Äôs ability to remember.&amp;nbsp;&lt;/em&gt;(MIT Technology Review&amp;nbsp;$)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Ship-tracking data shows China is creating massive floating barriers&lt;/strong&gt;&lt;br /&gt;The maneuvers show that Beijing can now rapidly muster large numbers of the boats in disputed seas. (NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Quantum navigation could solve the military‚Äôs GPS jamming problem&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 The AI bubble risks disrupting the global economy, says the IMF&lt;/strong&gt;&lt;br /&gt;But it‚Äôs hard to see anyone pumping the brakes any time soon. (FT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;British politicians say the UK is being exposed to ‚Äòserious harm‚Äô by AI risks&lt;/em&gt;. (The Guardian)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;What even is the AI bubble?&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Cryptocurrencies are dying in record numbers&lt;/strong&gt;&lt;br /&gt;In an era of one-off joke coins and pump and dump scams, that‚Äôs surely a good thing. (Gizmodo)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;President Trump has pardoned a lot of people who‚Äôve committed financial crimes.&lt;/em&gt;&amp;nbsp;(NBC)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 Threads has more global daily mobile users than X now&lt;/strong&gt;&lt;br /&gt;And once-popular alternative Bluesky barely even makes the charts. (Forbes)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 The UK is considering banning under 16s from social media&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Just weeks after a similar ban took effect in Australia. (BBC)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 You can burn yourself out with AI coding agents&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;They could be set to make experienced programmers busier than ever before. (Ars Technica)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Why Anthropic‚Äôs Claude Code is taking the AI world by storm.&amp;nbsp;&lt;/em&gt;(WSJ&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;AI coding is now everywhere. But not everyone is convinced.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Some tech billionaires are leaving California&amp;nbsp;&lt;/strong&gt;üëã&lt;br /&gt;Not all though‚Äîthe founders of Nvidia and Airbnb say they‚Äôll stay and pay the 5% wealth tax. (WP&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Tech bosses‚Äô support for Trump is paying off for them big time.&amp;nbsp;&lt;/em&gt;(FT&amp;nbsp;$)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Matt Damon says Netflix tells directors to repeat movie plots&lt;/strong&gt;&lt;br /&gt;To accommodate all the people using their phones. (NME)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Why more people are going analog in 2026&amp;nbsp;&lt;/strong&gt;üß∂&lt;br /&gt;Crafting, reading, and other screen-free hobbies are on the rise. (CNN)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Dumbphones are becoming popular too‚Äîbut it‚Äôs worth thinking hard before you switch.&amp;nbsp;&lt;/em&gt;(Wired&amp;nbsp;$)&lt;/p&gt; 
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;‚ÄòIt may sound like American chauvinism‚Ä¶and it is. We‚Äôre done apologising about that.‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;‚ÄîThomas Dans, a Trump appointee who heads the US Arctic Research Commission, tells the&amp;nbsp;FT&amp;nbsp;his boss is deadly serious about acquiring Greenland.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1049096" src="https://wp.technologyreview.com/wp-content/uploads/2022/04/Sugar_0125_wk-crop.jpg?w=3000" /&gt;&lt;div class="image-credit"&gt;BRUCE PETERSON&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;Inside the fierce, messy fight over ‚Äúhealthy‚Äù sugar tech&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;On the outskirts of Charlottesville, Virginia, a new kind of sugar factory is taking shape. The facility is being developed by a startup called Bonumose. It uses a processed corn product called maltodextrin that is found in many junk foods and is calorically similar to table sugar (sucrose).&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But for Bonumose, maltodextrin isn‚Äôt an ingredient‚Äîit‚Äôs a raw material. When it‚Äôs poured into the company‚Äôs bioreactors, what emerges is tagatose. Found naturally in small concentrations in fruit, some grains, and milk, it is nearly as sweet as sucrose but apparently with only around half the calories, and wider health benefits.&lt;/p&gt;  &lt;p&gt;Bonumose‚Äôs process originated in a company spun out of the Virginia Tech lab of Yi-Heng ‚ÄúPercival‚Äù Zhang. When &lt;em&gt;MIT Technology Review &lt;/em&gt;spoke to Zhang, he was sitting alone in an empty lab in Tianjin, China, after serving a two-year sentence of supervised release in Virginia for conspiracy to defraud the US government, making false statements, and obstruction of justice. If sugar is the new oil, the global battle to control it has already begun. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;‚ÄîMark Harris&lt;/em&gt;&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Paul Mescal just keeps getting&amp;nbsp;cooler.&lt;br /&gt;+ Make this year calmer with these evidence-backed&amp;nbsp;tips. ($)&lt;br /&gt;+ I can confirm that&amp;nbsp;Lumie wake-up lamps&amp;nbsp;really are worth it (and no one paid me to say so!)&lt;br /&gt;+ There are some real gems in Green Day‚Äôs bassist Mike Dirnt‚Äôs&amp;nbsp;favorite albums&amp;nbsp;list.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/20/1131459/the-download-digitizing-india-and-scoring-embryos/</guid><pubDate>Tue, 20 Jan 2026 13:13:00 +0000</pubDate></item><item><title>[NEW] The UK government is backing AI that can run its own lab experiments (Artificial intelligence ‚Äì MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/20/1131462/the-uk-government-is-backing-ai-scientists-that-can-run-their-own-experiments/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/machine-science_rev_1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;A number of startups and universities that are building ‚ÄúAI scientists‚Äù to design and run experiments in the lab, including robot biologists and chemists, have just won extra funding from the UK government agency that funds moonshot R&amp;amp;D. The competition, set up by ARIA (the Advanced Research and Invention Agency), gives a clear sense of how fast this technology is moving: The agency received 245 proposals from research teams that are already building tools capable of automating increasing amounts of lab work.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;ARIA defines an AI scientist as a system that can run an entire scientific workflow, coming up with hypotheses, designing and running experiments to test those hypotheses, and then analyzing the results. In many cases, the system may then feed those results back into itself and run the loop again and again. Human scientists become overseers, coming up with the initial research questions and then letting the AI scientist get on with the grunt work.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;‚ÄúThere are better uses for a PhD student than waiting around in a lab until 3 a.m. to make sure an experiment is run to the end,‚Äù says Ant Rowstron, ARIA‚Äôs chief technology officer.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;ARIA picked 12 projects to fund from the 245 proposals, doubling the amount of funding it had intended to allocate because of the large number and high quality of submissions. Half the teams are from the UK; the rest are from the US and Europe. Some of the teams are from universities, some from industry. Each will get around ¬£500,000 (around $675,000) to cover nine months‚Äô work. At the end of that time, they should be able to demonstrate that their AI scientist was able to come up with novel findings.&lt;/p&gt; 
 &lt;p&gt;Winning teams include Lila Sciences, a US company that is building what it calls an AI nano-scientist‚Äîa system that will design and run experiments to discover the best ways to compose and process quantum dots, which are nanometer-scale semiconductor particles used in medical imaging, solar panels, and QLED TVs.&lt;/p&gt;  &lt;p&gt;‚ÄúWe are using the funds and time to prove a point,‚Äù says Rafa G√≥mez-Bombarelli, chief science officer for physical sciences at Lila: ‚ÄúThe grant lets us design a real AI robotics loop around a focused scientific problem, generate evidence that it works, and document the playbook so others can reproduce and extend it.‚Äù&lt;/p&gt; 
 &lt;p&gt;Another team, from the University of Liverpool, UK, is building a robot chemist, which runs multiple experiments at once and uses a vision language model to help troubleshoot when the robot makes an error.&lt;/p&gt;  &lt;p&gt;And a startup based in London, still in stealth mode, is developing an AI scientist called ThetaWorld, which is using LLMs to design experiments on the physical and chemical interactions that are important for the performance of batteries. The experiments will then be run in an automated lab by Sandia National Laboratories in the US.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Taking the temperature&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Compared with the ¬£5 million projects spanning two or three years that ARIA usually funds, ¬£500,000 is small change. But that was the idea, says Rowstron: It‚Äôs an experiment on ARIA‚Äôs part too. By funding a range of projects for a short amount of time, the agency is taking the temperature at the cutting edge to determine how the way science is done is changing, and how fast. What it learns will become the baseline for funding future large-scale projects.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Rowstron acknowledges there‚Äôs a lot of hype, especially now that most of the top AI companies have teams focused on science. When results are shared by press release and not peer review, it can be hard to know what the technology can and can‚Äôt do. ‚ÄúThat‚Äôs always a challenge for a research agency trying to fund the frontier,‚Äù he says. ‚ÄúTo do things at the frontier, we've got to know what the frontier is.‚Äù&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;For now, the cutting edge involves agentic systems calling up other existing tools on the fly. ‚ÄúThey‚Äôre running things like large language models to do the ideation, and then they use other models to do optimization and run experiments,‚Äù says Rowstron. ‚ÄúAnd then they feed the results back round.‚Äù&lt;/p&gt;  &lt;p&gt;Rowstron sees the technology stacked in tiers. At the bottom are AI tools designed by humans for humans, such as AlphaFold. These tools let scientists leapfrog slow and painstaking parts of the scientific pipeline but can still require many months of lab work to verify results. The idea of an AI scientist is to automate that work too.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI scientists sit in a layer above those human-made tools and call ton hose tools as needed, says Rowstron. ‚ÄúBut there‚Äôs a point in time‚Äîand I don‚Äôt think it‚Äôs a decade away‚Äîwhere that AI scientist layer says, ‚ÄòI need a tool and it doesn‚Äôt exist,‚Äô and it will actually create an AlphaFold kind of tool just on the way to figuring out how to solve another problem. That whole bottom zone will just be automated.‚Äù&lt;/p&gt;  &lt;p&gt;That‚Äôs still some way off, he says. All the projects ARIA is now funding involve systems that call on existing tools rather than spin up new ones.&lt;/p&gt; 

 &lt;p&gt;There are also unsolved problems with agentic systems in general, which limits how long they can run by themselves without going off track or making errors. For example, a study, titled ‚ÄúWhy LLMs aren‚Äôt scientists yet,‚Äù posted online last week by researchers at Lossfunk, an AI lab based in India, reports that in an experiment to get LLM agents to run a scientific workflow to completion, the system failed three out of four times. According to the researchers, the reasons the LLMs broke down included changes in the initial specifications and ‚Äúoverexcitement that declares success despite obvious failures.‚Äù&lt;/p&gt;  &lt;p&gt;‚ÄúObviously, at the moment these tools are still fairly early in their cycle and these things might plateau,‚Äù says Rowstron. ‚ÄúI‚Äôm not expecting them to win a Nobel Prize.‚Äù&lt;/p&gt;  &lt;p&gt;‚ÄúBut there is a world where some of these tools will force us to operate so much quicker,‚Äù he continues. ‚ÄúAnd if we end up in that world, it‚Äôs super important for us to be ready.‚Äù&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/machine-science_rev_1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;A number of startups and universities that are building ‚ÄúAI scientists‚Äù to design and run experiments in the lab, including robot biologists and chemists, have just won extra funding from the UK government agency that funds moonshot R&amp;amp;D. The competition, set up by ARIA (the Advanced Research and Invention Agency), gives a clear sense of how fast this technology is moving: The agency received 245 proposals from research teams that are already building tools capable of automating increasing amounts of lab work.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;ARIA defines an AI scientist as a system that can run an entire scientific workflow, coming up with hypotheses, designing and running experiments to test those hypotheses, and then analyzing the results. In many cases, the system may then feed those results back into itself and run the loop again and again. Human scientists become overseers, coming up with the initial research questions and then letting the AI scientist get on with the grunt work.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;‚ÄúThere are better uses for a PhD student than waiting around in a lab until 3 a.m. to make sure an experiment is run to the end,‚Äù says Ant Rowstron, ARIA‚Äôs chief technology officer.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;ARIA picked 12 projects to fund from the 245 proposals, doubling the amount of funding it had intended to allocate because of the large number and high quality of submissions. Half the teams are from the UK; the rest are from the US and Europe. Some of the teams are from universities, some from industry. Each will get around ¬£500,000 (around $675,000) to cover nine months‚Äô work. At the end of that time, they should be able to demonstrate that their AI scientist was able to come up with novel findings.&lt;/p&gt; 
 &lt;p&gt;Winning teams include Lila Sciences, a US company that is building what it calls an AI nano-scientist‚Äîa system that will design and run experiments to discover the best ways to compose and process quantum dots, which are nanometer-scale semiconductor particles used in medical imaging, solar panels, and QLED TVs.&lt;/p&gt;  &lt;p&gt;‚ÄúWe are using the funds and time to prove a point,‚Äù says Rafa G√≥mez-Bombarelli, chief science officer for physical sciences at Lila: ‚ÄúThe grant lets us design a real AI robotics loop around a focused scientific problem, generate evidence that it works, and document the playbook so others can reproduce and extend it.‚Äù&lt;/p&gt; 
 &lt;p&gt;Another team, from the University of Liverpool, UK, is building a robot chemist, which runs multiple experiments at once and uses a vision language model to help troubleshoot when the robot makes an error.&lt;/p&gt;  &lt;p&gt;And a startup based in London, still in stealth mode, is developing an AI scientist called ThetaWorld, which is using LLMs to design experiments on the physical and chemical interactions that are important for the performance of batteries. The experiments will then be run in an automated lab by Sandia National Laboratories in the US.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Taking the temperature&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Compared with the ¬£5 million projects spanning two or three years that ARIA usually funds, ¬£500,000 is small change. But that was the idea, says Rowstron: It‚Äôs an experiment on ARIA‚Äôs part too. By funding a range of projects for a short amount of time, the agency is taking the temperature at the cutting edge to determine how the way science is done is changing, and how fast. What it learns will become the baseline for funding future large-scale projects.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Rowstron acknowledges there‚Äôs a lot of hype, especially now that most of the top AI companies have teams focused on science. When results are shared by press release and not peer review, it can be hard to know what the technology can and can‚Äôt do. ‚ÄúThat‚Äôs always a challenge for a research agency trying to fund the frontier,‚Äù he says. ‚ÄúTo do things at the frontier, we've got to know what the frontier is.‚Äù&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;For now, the cutting edge involves agentic systems calling up other existing tools on the fly. ‚ÄúThey‚Äôre running things like large language models to do the ideation, and then they use other models to do optimization and run experiments,‚Äù says Rowstron. ‚ÄúAnd then they feed the results back round.‚Äù&lt;/p&gt;  &lt;p&gt;Rowstron sees the technology stacked in tiers. At the bottom are AI tools designed by humans for humans, such as AlphaFold. These tools let scientists leapfrog slow and painstaking parts of the scientific pipeline but can still require many months of lab work to verify results. The idea of an AI scientist is to automate that work too.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI scientists sit in a layer above those human-made tools and call ton hose tools as needed, says Rowstron. ‚ÄúBut there‚Äôs a point in time‚Äîand I don‚Äôt think it‚Äôs a decade away‚Äîwhere that AI scientist layer says, ‚ÄòI need a tool and it doesn‚Äôt exist,‚Äô and it will actually create an AlphaFold kind of tool just on the way to figuring out how to solve another problem. That whole bottom zone will just be automated.‚Äù&lt;/p&gt;  &lt;p&gt;That‚Äôs still some way off, he says. All the projects ARIA is now funding involve systems that call on existing tools rather than spin up new ones.&lt;/p&gt; 

 &lt;p&gt;There are also unsolved problems with agentic systems in general, which limits how long they can run by themselves without going off track or making errors. For example, a study, titled ‚ÄúWhy LLMs aren‚Äôt scientists yet,‚Äù posted online last week by researchers at Lossfunk, an AI lab based in India, reports that in an experiment to get LLM agents to run a scientific workflow to completion, the system failed three out of four times. According to the researchers, the reasons the LLMs broke down included changes in the initial specifications and ‚Äúoverexcitement that declares success despite obvious failures.‚Äù&lt;/p&gt;  &lt;p&gt;‚ÄúObviously, at the moment these tools are still fairly early in their cycle and these things might plateau,‚Äù says Rowstron. ‚ÄúI‚Äôm not expecting them to win a Nobel Prize.‚Äù&lt;/p&gt;  &lt;p&gt;‚ÄúBut there is a world where some of these tools will force us to operate so much quicker,‚Äù he continues. ‚ÄúAnd if we end up in that world, it‚Äôs super important for us to be ready.‚Äù&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/20/1131462/the-uk-government-is-backing-ai-scientists-that-can-run-their-own-experiments/</guid><pubDate>Tue, 20 Jan 2026 13:28:21 +0000</pubDate></item><item><title>[NEW] Indian vibe-coding startup Emergent triples valuation to $300M with $70M fundraise (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/20/indian-vibe-coding-startup-emergent-raises-70m-at-300m-valuation-from-softbank-khosla-ventures/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a sign of the demand, or hype, for AI startups, Emergent, an Indian startup building an AI ‚Äúvibe-coding‚Äù platform, has raised $70 million less than four months after it raised a $23 million Series A.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Series B round was jointly led by SoftBank‚Äôs Vision Fund 2 and Khosla Ventures, and values the startup at $300 million post-money, sources with knowledge of the deal told TechCrunch. The startup was previously valued at $100 million post-money, another source told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Prosus, Lightspeed Venture Partners, Together, and Y Combinator also participated. Emergent has now raised $100 million within seven months of its launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The funding comes as Emergent claims $50 million in annual recurring revenue (ARR) and more than 5 million users across 190-plus countries. The startup said it is targeting ARR of more than $100 million by April 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like other vibe-coding platforms, Emergent uses AI agents to help users design, build, test and deploy full-stack web and mobile apps. It targets entrepreneurs and small businesses looking to ship products without having to hire large engineering teams.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe continue to see massive demand across our top geographies ‚Äî the U.S., Europe and India ‚Äî and we‚Äôll continue to expand deeper into these markets,‚Äù founder Mukund Jha told TechCrunch, adding that the startup‚Äôs recently launched mobile app-building service is seeing strong adoption.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3083744" height="1500" src="https://techcrunch.com/wp-content/uploads/2026/01/emergent-co-founders-mukund-jha-madhav-jha.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Emergent co-founders Mukund Jha and Madhav Jha&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Emergent&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Emergent says it is headquartered in San Francisco, but 70 of its 75 employees work out of an office in Bengaluru. The startup is hiring aggressively across functions in both countries, Jha said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Emergent competes with the likes of Lovable, Cursor and Replit, all of which have mushroomed into huge businesses within a couple of years of launch as AI-assisted coding enables users to develop their own apps without requiring much in the way of actual programming knowledge or skills.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To its credit, Emergent seems to have successfully tapped investor interest in such vibe-coding platforms to fund itself. Accel also backed Rocket, another India-founded startup, in a $15 million seed round last year, along with Together Fund and Salesforce Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is also notable as it marks SoftBank‚Äôs return to investing in India ‚Äî the firm previously backed Indian commerce startup ElasticRun nearly four years ago. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Emergent says the fresh funding will be used to expand its team, accelerate product development, and deepen its presence in key markets.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a sign of the demand, or hype, for AI startups, Emergent, an Indian startup building an AI ‚Äúvibe-coding‚Äù platform, has raised $70 million less than four months after it raised a $23 million Series A.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Series B round was jointly led by SoftBank‚Äôs Vision Fund 2 and Khosla Ventures, and values the startup at $300 million post-money, sources with knowledge of the deal told TechCrunch. The startup was previously valued at $100 million post-money, another source told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Prosus, Lightspeed Venture Partners, Together, and Y Combinator also participated. Emergent has now raised $100 million within seven months of its launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The funding comes as Emergent claims $50 million in annual recurring revenue (ARR) and more than 5 million users across 190-plus countries. The startup said it is targeting ARR of more than $100 million by April 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like other vibe-coding platforms, Emergent uses AI agents to help users design, build, test and deploy full-stack web and mobile apps. It targets entrepreneurs and small businesses looking to ship products without having to hire large engineering teams.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe continue to see massive demand across our top geographies ‚Äî the U.S., Europe and India ‚Äî and we‚Äôll continue to expand deeper into these markets,‚Äù founder Mukund Jha told TechCrunch, adding that the startup‚Äôs recently launched mobile app-building service is seeing strong adoption.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3083744" height="1500" src="https://techcrunch.com/wp-content/uploads/2026/01/emergent-co-founders-mukund-jha-madhav-jha.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Emergent co-founders Mukund Jha and Madhav Jha&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Emergent&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Emergent says it is headquartered in San Francisco, but 70 of its 75 employees work out of an office in Bengaluru. The startup is hiring aggressively across functions in both countries, Jha said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Emergent competes with the likes of Lovable, Cursor and Replit, all of which have mushroomed into huge businesses within a couple of years of launch as AI-assisted coding enables users to develop their own apps without requiring much in the way of actual programming knowledge or skills.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To its credit, Emergent seems to have successfully tapped investor interest in such vibe-coding platforms to fund itself. Accel also backed Rocket, another India-founded startup, in a $15 million seed round last year, along with Together Fund and Salesforce Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is also notable as it marks SoftBank‚Äôs return to investing in India ‚Äî the firm previously backed Indian commerce startup ElasticRun nearly four years ago. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Emergent says the fresh funding will be used to expand its team, accelerate product development, and deepen its presence in key markets.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/20/indian-vibe-coding-startup-emergent-raises-70m-at-300m-valuation-from-softbank-khosla-ventures/</guid><pubDate>Tue, 20 Jan 2026 13:50:09 +0000</pubDate></item><item><title>[NEW] The era of agentic chaos and how data will save us (Artificial intelligence ‚Äì MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/20/1130911/the-era-of-agentic-chaos-and-how-data-will-save-us/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Reltio&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;AI agents are moving beyond coding assistants and customer service chatbots into the operational core of the enterprise. The ROI is promising, but autonomy without alignment is a recipe for chaos. Business leaders need to lay the essential foundations now.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1131198" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/Reltio-iStock-2237209743-crop.png" /&gt;&lt;/figure&gt;    &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The agent explosion is coming&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Agents are independently handling end-to-end processes across lead generation, supply chain optimization, customer support, and financial reconciliation. A mid-sized organization could easily run 4,000 agents, each making decisions that affect revenue, compliance, and customer experience.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The transformation toward an agent-driven enterprise is inevitable. The economic benefits are too significant to ignore, and the potential is becoming a reality faster than most predicted. The problem? Most businesses and their underlying infrastructure are not prepared for this shift. Early adopters have found unlocking AI initiatives at scale to be extremely challenging.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The reliability gap that‚Äôs holding AI back&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Companies are investing heavily in AI, but the returns aren't materializing. According to recent research from Boston Consulting Group, 60% of companies report minimal revenue and cost gains despite substantial investment. However, the leaders reported they achieved five times the revenue increases and three times the cost reductions. Clearly, there is a massive premium for being a leader.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;What separates the leaders from the pack isn't how much they're spending or which models they're using. Before scaling AI deployment, these ‚Äúfuture-built‚Äù companies put critical data infrastructure capabilities in place. They invested in the foundational work that enables AI to function reliably.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A framework for agent reliability: The four quadrants&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;To understand how and where enterprise AI can fail, consider four critical quadrants: models, tools, context, and governance.&lt;/p&gt;  &lt;p&gt;Take a simple example: an agent that orders you pizza. The model interprets your request ("get me a pizza"). The tool executes the action (calling the Domino's or Pizza Hut API). Context provides personalization (you tend to order pepperoni on Friday nights at 7pm). Governance validates the outcome (did the pizza actually arrive?).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Each dimension represents a potential failure point:&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; The underlying AI systems that interpret prompts, generate responses, and make predictions&lt;/li&gt;    &lt;li&gt;&lt;strong&gt;Tools:&lt;/strong&gt; The integration layer that connects AI to enterprise systems, such as APIs, protocols, and connectors&amp;nbsp;&lt;/li&gt;    &lt;li&gt;&lt;strong&gt;Context:&lt;/strong&gt; Before making decisions, information agents need to understand the full business picture, including customer histories, product catalogs, and supply chain networks&lt;/li&gt;    &lt;li&gt;&lt;strong&gt;Governance:&lt;/strong&gt; The policies, controls, and processes that ensure data quality, security, and compliance&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;This framework helps diagnose where reliability gaps emerge. When an enterprise agent fails, which quadrant is the problem? Is the model misunderstanding intent? Are the tools unavailable or broken? Is the context incomplete or contradictory? Or is there no mechanism to verify that the agent did what it was supposed to do?&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why this is a data problem, not a model problem&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The temptation is to think that reliability will simply improve as models improve. Yet, model capability is advancing exponentially. The cost of inference has dropped nearly 900 times in three years, hallucination rates are on the decline, and AI‚Äôs capacity to perform long tasks doubles every six months.&lt;/p&gt;  &lt;p&gt;Tooling is also accelerating. Integration frameworks like the Model Context Protocol (MCP) make it dramatically easier to connect agents with enterprise systems and APIs.&lt;/p&gt;  &lt;p&gt;If models are powerful and tools are maturing, then what is holding back adoption?&lt;/p&gt; 

 &lt;p&gt;To borrow from James Carville, ‚ÄúIt is the data, stupid.‚Äù The root cause of most misbehaving agents is misaligned, inconsistent, or incomplete data.&lt;/p&gt;  &lt;p&gt;Enterprises have accumulated data debt over decades. Acquisitions, custom systems, departmental tools, and shadow IT have left data scattered across silos that rarely agree. Support systems do not match what is in marketing systems. Supplier data is duplicated across finance, procurement, and logistics. Locations have multiple representations depending on the source.&lt;/p&gt;  &lt;p&gt;Drop a few agents into this environment, and they will perform wonderfully at first, because each one is given a curated set of systems to call. Add more agents and the cracks grow, as each one builds its own fragment of truth.&lt;/p&gt;  &lt;p&gt;This dynamic has played out before. When business intelligence became self-serve, everyone started creating dashboards. Productivity soared, reports failed to match. Now imagine that phenomenon not in static dashboards, but in AI agents that can take action. With agents, data inconsistency produces real business consequences, not just debates among departments.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Companies that build unified context and robust governance can deploy thousands of agents with confidence, knowing they'll work together coherently and comply with business rules. Companies that skip this foundational work will watch their agents produce contradictory results, violate policies, and ultimately erode trust faster than they create value.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Leverage agentic AI without the chaos&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The question for enterprises centers on organizational readiness. Will your company prepare the data foundation needed to make agent transformation work? Or will you spend years debugging agents, one issue at a time, forever chasing problems that originate in infrastructure you never built?&lt;/p&gt;  &lt;p&gt;Autonomous agents are already transforming how work gets done. But the enterprise will only experience the upside if those systems operate from the same truth. This ensures that when agents reason, plan, and act, they do so based on accurate, consistent, and up-to-date information.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The companies generating value from AI today have built on fit-for-purpose data foundations. They recognized early that in an agentic world, data functions as essential infrastructure. A solid data foundation is what turns experimentation into dependable operations.&lt;/p&gt; 
 &lt;p&gt;At Reltio, the focus is on building that foundation. The Reltio data management platform unifies core data from across the enterprise, giving every agent immediate access to the same business context. This unified approach enables enterprises to move faster, act smarter, and unlock the full value of AI.&lt;/p&gt;  &lt;p&gt;Agents will define the future of the enterprise. Context intelligence will determine who leads it.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;For leaders navigating this next wave of transformation, see Relatio‚Äôs practical guide:&lt;/em&gt;&lt;em&gt;&lt;br /&gt;&lt;/em&gt;&lt;em&gt;Unlocking Agentic AI: A Business Playbook for Data Readiness.&lt;/em&gt;&lt;em&gt; Get your copy now to learn how real-time context becomes the decisive advantage in the age of intelligence.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Reltio&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;AI agents are moving beyond coding assistants and customer service chatbots into the operational core of the enterprise. The ROI is promising, but autonomy without alignment is a recipe for chaos. Business leaders need to lay the essential foundations now.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1131198" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/Reltio-iStock-2237209743-crop.png" /&gt;&lt;/figure&gt;    &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The agent explosion is coming&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Agents are independently handling end-to-end processes across lead generation, supply chain optimization, customer support, and financial reconciliation. A mid-sized organization could easily run 4,000 agents, each making decisions that affect revenue, compliance, and customer experience.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The transformation toward an agent-driven enterprise is inevitable. The economic benefits are too significant to ignore, and the potential is becoming a reality faster than most predicted. The problem? Most businesses and their underlying infrastructure are not prepared for this shift. Early adopters have found unlocking AI initiatives at scale to be extremely challenging.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The reliability gap that‚Äôs holding AI back&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Companies are investing heavily in AI, but the returns aren't materializing. According to recent research from Boston Consulting Group, 60% of companies report minimal revenue and cost gains despite substantial investment. However, the leaders reported they achieved five times the revenue increases and three times the cost reductions. Clearly, there is a massive premium for being a leader.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;What separates the leaders from the pack isn't how much they're spending or which models they're using. Before scaling AI deployment, these ‚Äúfuture-built‚Äù companies put critical data infrastructure capabilities in place. They invested in the foundational work that enables AI to function reliably.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A framework for agent reliability: The four quadrants&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;To understand how and where enterprise AI can fail, consider four critical quadrants: models, tools, context, and governance.&lt;/p&gt;  &lt;p&gt;Take a simple example: an agent that orders you pizza. The model interprets your request ("get me a pizza"). The tool executes the action (calling the Domino's or Pizza Hut API). Context provides personalization (you tend to order pepperoni on Friday nights at 7pm). Governance validates the outcome (did the pizza actually arrive?).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Each dimension represents a potential failure point:&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; The underlying AI systems that interpret prompts, generate responses, and make predictions&lt;/li&gt;    &lt;li&gt;&lt;strong&gt;Tools:&lt;/strong&gt; The integration layer that connects AI to enterprise systems, such as APIs, protocols, and connectors&amp;nbsp;&lt;/li&gt;    &lt;li&gt;&lt;strong&gt;Context:&lt;/strong&gt; Before making decisions, information agents need to understand the full business picture, including customer histories, product catalogs, and supply chain networks&lt;/li&gt;    &lt;li&gt;&lt;strong&gt;Governance:&lt;/strong&gt; The policies, controls, and processes that ensure data quality, security, and compliance&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;This framework helps diagnose where reliability gaps emerge. When an enterprise agent fails, which quadrant is the problem? Is the model misunderstanding intent? Are the tools unavailable or broken? Is the context incomplete or contradictory? Or is there no mechanism to verify that the agent did what it was supposed to do?&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why this is a data problem, not a model problem&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The temptation is to think that reliability will simply improve as models improve. Yet, model capability is advancing exponentially. The cost of inference has dropped nearly 900 times in three years, hallucination rates are on the decline, and AI‚Äôs capacity to perform long tasks doubles every six months.&lt;/p&gt;  &lt;p&gt;Tooling is also accelerating. Integration frameworks like the Model Context Protocol (MCP) make it dramatically easier to connect agents with enterprise systems and APIs.&lt;/p&gt;  &lt;p&gt;If models are powerful and tools are maturing, then what is holding back adoption?&lt;/p&gt; 

 &lt;p&gt;To borrow from James Carville, ‚ÄúIt is the data, stupid.‚Äù The root cause of most misbehaving agents is misaligned, inconsistent, or incomplete data.&lt;/p&gt;  &lt;p&gt;Enterprises have accumulated data debt over decades. Acquisitions, custom systems, departmental tools, and shadow IT have left data scattered across silos that rarely agree. Support systems do not match what is in marketing systems. Supplier data is duplicated across finance, procurement, and logistics. Locations have multiple representations depending on the source.&lt;/p&gt;  &lt;p&gt;Drop a few agents into this environment, and they will perform wonderfully at first, because each one is given a curated set of systems to call. Add more agents and the cracks grow, as each one builds its own fragment of truth.&lt;/p&gt;  &lt;p&gt;This dynamic has played out before. When business intelligence became self-serve, everyone started creating dashboards. Productivity soared, reports failed to match. Now imagine that phenomenon not in static dashboards, but in AI agents that can take action. With agents, data inconsistency produces real business consequences, not just debates among departments.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Companies that build unified context and robust governance can deploy thousands of agents with confidence, knowing they'll work together coherently and comply with business rules. Companies that skip this foundational work will watch their agents produce contradictory results, violate policies, and ultimately erode trust faster than they create value.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Leverage agentic AI without the chaos&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The question for enterprises centers on organizational readiness. Will your company prepare the data foundation needed to make agent transformation work? Or will you spend years debugging agents, one issue at a time, forever chasing problems that originate in infrastructure you never built?&lt;/p&gt;  &lt;p&gt;Autonomous agents are already transforming how work gets done. But the enterprise will only experience the upside if those systems operate from the same truth. This ensures that when agents reason, plan, and act, they do so based on accurate, consistent, and up-to-date information.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The companies generating value from AI today have built on fit-for-purpose data foundations. They recognized early that in an agentic world, data functions as essential infrastructure. A solid data foundation is what turns experimentation into dependable operations.&lt;/p&gt; 
 &lt;p&gt;At Reltio, the focus is on building that foundation. The Reltio data management platform unifies core data from across the enterprise, giving every agent immediate access to the same business context. This unified approach enables enterprises to move faster, act smarter, and unlock the full value of AI.&lt;/p&gt;  &lt;p&gt;Agents will define the future of the enterprise. Context intelligence will determine who leads it.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;For leaders navigating this next wave of transformation, see Relatio‚Äôs practical guide:&lt;/em&gt;&lt;em&gt;&lt;br /&gt;&lt;/em&gt;&lt;em&gt;Unlocking Agentic AI: A Business Playbook for Data Readiness.&lt;/em&gt;&lt;em&gt; Get your copy now to learn how real-time context becomes the decisive advantage in the age of intelligence.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/20/1130911/the-era-of-agentic-chaos-and-how-data-will-save-us/</guid><pubDate>Tue, 20 Jan 2026 15:00:00 +0000</pubDate></item><item><title>[NEW] Humans&amp;, a ‚Äòhuman-centric‚Äô AI startup founded by Anthropic, xAI, Google alums, raised $480M seed round (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/20/humans-a-human-centric-ai-startup-founded-by-anthropic-xai-google-alums-raised-480m-seed-round/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/humans.png?resize=1200,633" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Humans&amp;amp;, a startup with a philosophy that AI should empower people rather than replace them, has raised $480 million in seed funding at a $4.48 billion valuation, reports The New York Times. Investors in the round include&amp;nbsp;chipmaker Nvidia, Amazon founder Jeff Bezos, and VC firms SV Angel, Google Ventures, and Laurene Powell Jobs‚Äô firm Emerson Collective.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The megadeal for the three-month-old company follows a trend of investors throwing money at startups founded by breakaways of major AI labs. Humans&amp;amp;‚Äôs founders include Andi Peng, a former Anthropic researcher who worked on reinforcement learning and post-training of Claude 3.5 through 4.5; Georges Harik, Google‚Äôs seventh employee, who helped build its first advertising systems; Eric Zelikman and Yuchen He, two former xAI researchers who helped develop the Grok chatbot; and Noah Goodman, a Stanford professor of psychology and computer science.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Humans&amp;amp;‚Äôs 20-odd employees also come from OpenAI, Meta, Reflection, AI2, and MIT, according to the company. The startup aims to use software to help people collaborate with each other ‚Äî think an AI version of an instant messaging app. One of their goals is to use existing AI techniques to train AI in new ways, like programming chatbots to request information from the user and store it for later use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In order to build AI that serves ‚Äúas a deeper connective tissue that strengthens organizations and communities,‚Äù Humans&amp;amp; hopes to rethink ‚Äúhow we train models at scale and how people interact with AI,‚Äù per the company‚Äôs web page. The startup cited a need for innovations in ‚Äúlong-horizon and multi-agent reinforcement learning, memory, and user understanding,‚Äù as well as a tightly integrated focus on both science and product development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out for comment.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/humans.png?resize=1200,633" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Humans&amp;amp;, a startup with a philosophy that AI should empower people rather than replace them, has raised $480 million in seed funding at a $4.48 billion valuation, reports The New York Times. Investors in the round include&amp;nbsp;chipmaker Nvidia, Amazon founder Jeff Bezos, and VC firms SV Angel, Google Ventures, and Laurene Powell Jobs‚Äô firm Emerson Collective.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The megadeal for the three-month-old company follows a trend of investors throwing money at startups founded by breakaways of major AI labs. Humans&amp;amp;‚Äôs founders include Andi Peng, a former Anthropic researcher who worked on reinforcement learning and post-training of Claude 3.5 through 4.5; Georges Harik, Google‚Äôs seventh employee, who helped build its first advertising systems; Eric Zelikman and Yuchen He, two former xAI researchers who helped develop the Grok chatbot; and Noah Goodman, a Stanford professor of psychology and computer science.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Humans&amp;amp;‚Äôs 20-odd employees also come from OpenAI, Meta, Reflection, AI2, and MIT, according to the company. The startup aims to use software to help people collaborate with each other ‚Äî think an AI version of an instant messaging app. One of their goals is to use existing AI techniques to train AI in new ways, like programming chatbots to request information from the user and store it for later use.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In order to build AI that serves ‚Äúas a deeper connective tissue that strengthens organizations and communities,‚Äù Humans&amp;amp; hopes to rethink ‚Äúhow we train models at scale and how people interact with AI,‚Äù per the company‚Äôs web page. The startup cited a need for innovations in ‚Äúlong-horizon and multi-agent reinforcement learning, memory, and user understanding,‚Äù as well as a tightly integrated focus on both science and product development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out for comment.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/20/humans-a-human-centric-ai-startup-founded-by-anthropic-xai-google-alums-raised-480m-seed-round/</guid><pubDate>Tue, 20 Jan 2026 16:00:57 +0000</pubDate></item><item><title>[NEW] Reimagining ERP for the agentic AI era (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/20/1129965/reimagining-erp-for-the-agentic-ai-era/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Rimini Street&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The story of enterprise resource planning (ERP) is really a story of businesses learning to organize themselves around the latest, greatest technology of the times. In the 1960s through the ‚Äô80s, mainframes, material requirements planning (MRP), and manufacturing resource planning (MRP II) brought core business data from file cabinets to centralized systems. Client-server architectures defined the ‚Äô80s and ‚Äô90s, taking digitization mainstream during the internet‚Äôs infancy. And in the 21st century, as work moved beyond the desktop, SaaS and cloud ushered in flexible access and elastic infrastructure.&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="alt" class="wp-image-1130037" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_Riminity_2025_CoverV10_121525.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;The rise of composability and agentic AI marks yet another dawn‚Äîand an apt one for the nascent intelligence age. Composable architectures let organizations assemble capabilities from multiple systems in a mix-and-match fashion, so they can swap vendor gridlock for an √† la carte portfolio of fit-for-purpose modules. On top of that architectural shift, agentic AI enables coordination across systems that weren‚Äôt originally designed to talk to one another.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Early indicators suggest that AI-enabled ERP will yield meaningful performance gains: One 2024 study found that organizations implementing AI-driven ERP solutions stand to gain around a 30% boost in user satisfaction and a 25% lift in productivity; another suggested that AI-driven ERP can lead to processing time savings of up to 45%, as well as improvements in decision accuracy to the tune of 60%.&lt;/p&gt;  &lt;p&gt;These dual advancements address long-standing gaps that previous ERP eras fell short of delivering: freedom to innovate outside of vendor roadmaps, capacity for rapid iteration, and true interoperability across all critical functions. This shift signals the end of monolithic dependency as well as a once-in-a-generation opportunity for early movers to gain a competitive edge.&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1130683" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/MITTR-RIminiStSocials_V2EricHelmer.png" /&gt;&lt;/figure&gt;  &lt;p&gt;Key takeaways include:&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;Enterprises are moving away from monolithic ERP vendor upgrades in favor of modular architectures that allow them to change or modernize components independently while keeping a stable core for essential transactions.&lt;/li&gt;    &lt;li&gt;Agentic AI is a timely complement to composability, functioning as a UX and orchestration layer that can coordinate workflows across disparate systems and turn multi-step processes into automated, cross-platform operations.&lt;/li&gt;    &lt;li&gt;These dual shifts are finally enabling technology architecture to organize around the business, instead of the business around the ERP. Companies can modernize by reconfiguring and extending what they already have, rather than relying on ERP-centric upgrades.&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Rimini Street&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The story of enterprise resource planning (ERP) is really a story of businesses learning to organize themselves around the latest, greatest technology of the times. In the 1960s through the ‚Äô80s, mainframes, material requirements planning (MRP), and manufacturing resource planning (MRP II) brought core business data from file cabinets to centralized systems. Client-server architectures defined the ‚Äô80s and ‚Äô90s, taking digitization mainstream during the internet‚Äôs infancy. And in the 21st century, as work moved beyond the desktop, SaaS and cloud ushered in flexible access and elastic infrastructure.&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="alt" class="wp-image-1130037" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_Riminity_2025_CoverV10_121525.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;The rise of composability and agentic AI marks yet another dawn‚Äîand an apt one for the nascent intelligence age. Composable architectures let organizations assemble capabilities from multiple systems in a mix-and-match fashion, so they can swap vendor gridlock for an √† la carte portfolio of fit-for-purpose modules. On top of that architectural shift, agentic AI enables coordination across systems that weren‚Äôt originally designed to talk to one another.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Early indicators suggest that AI-enabled ERP will yield meaningful performance gains: One 2024 study found that organizations implementing AI-driven ERP solutions stand to gain around a 30% boost in user satisfaction and a 25% lift in productivity; another suggested that AI-driven ERP can lead to processing time savings of up to 45%, as well as improvements in decision accuracy to the tune of 60%.&lt;/p&gt;  &lt;p&gt;These dual advancements address long-standing gaps that previous ERP eras fell short of delivering: freedom to innovate outside of vendor roadmaps, capacity for rapid iteration, and true interoperability across all critical functions. This shift signals the end of monolithic dependency as well as a once-in-a-generation opportunity for early movers to gain a competitive edge.&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1130683" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/MITTR-RIminiStSocials_V2EricHelmer.png" /&gt;&lt;/figure&gt;  &lt;p&gt;Key takeaways include:&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;Enterprises are moving away from monolithic ERP vendor upgrades in favor of modular architectures that allow them to change or modernize components independently while keeping a stable core for essential transactions.&lt;/li&gt;    &lt;li&gt;Agentic AI is a timely complement to composability, functioning as a UX and orchestration layer that can coordinate workflows across disparate systems and turn multi-step processes into automated, cross-platform operations.&lt;/li&gt;    &lt;li&gt;These dual shifts are finally enabling technology architecture to organize around the business, instead of the business around the ERP. Companies can modernize by reconfiguring and extending what they already have, rather than relying on ERP-centric upgrades.&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/20/1129965/reimagining-erp-for-the-agentic-ai-era/</guid><pubDate>Tue, 20 Jan 2026 16:14:14 +0000</pubDate></item><item><title>[NEW] Multimodal reinforcement learning with agentic verifier for AI agents (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Diagram showing visual, audio, and document icons feeding into a central network icon of connected people, which then leads to a checkmark symbol, all on a blue‚Äëto‚Äëpurple gradient background." class="wp-image-1160195" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;div class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section"&gt;
	
	&lt;div class="container"&gt;
		&lt;div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow"&gt;
			&lt;div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h2 class="wp-block-heading h3" id="at-a-glance"&gt;At a glance&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Today‚Äôs multimodal AI systems&amp;nbsp;can&amp;nbsp;give&amp;nbsp;answers that sound right but&amp;nbsp;may not be&amp;nbsp;grounded in what they&amp;nbsp;actually&amp;nbsp;observe&amp;nbsp;over time, leading to unpredictable errors and safety risks in real-world settings.&lt;/li&gt;



&lt;li&gt;Argos is a verification framework for multimodal reinforcement learning that trains models by rewarding not just correct answers, but correct answers grounded in visual and temporal evidence, using automated verification rather than human labeling.&amp;nbsp;It selects the appropriate specialized tools for each answer&amp;nbsp;based on what needs to be verified.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Models trained with Argos show stronger spatial reasoning, far fewer visual hallucinations, more stable learning dynamics, and better performance on robotics and real-world tasks while requiring fewer training samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;		&lt;/div&gt;
	&lt;/div&gt;

	&lt;/div&gt;



&lt;p&gt;Over the past few years, AI systems have become much better at discerning images, generating language, and performing tasks within physical and virtual environments. Yet they still fail in ways that are hard to predict and even harder to fix. A robot might try to grasp a tool when the object is visibly blocked, or a visual assistant integrated into smart glasses might describe objects that aren‚Äôt actually present.&lt;/p&gt;



&lt;p&gt;These errors often arise because today‚Äôs multimodal agents are trained to generate outputs that are plausible rather than grounded in the actual information they receive from their environment. As a result, a model‚Äôs output can seem correct while relying on incorrect information. As AI systems are increasingly used to navigate 3D spaces and make decisions in real-world settings, this gap can be a safety and reliability concern.&lt;/p&gt;



&lt;p&gt;To tackle this challenge, we posed the question: How can we train AI agents to generate correct answers and take appropriate actions for the right reasons so that their behavior is reliable even as the environment or tasks change?&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;Argos represents a novel answer to this challenge. It‚Äôs an agentic verification framework designed to improve the reliability of reinforcement learning in multimodal models.&amp;nbsp;Reinforcement learning is a training method where AI models learn by receiving rewards for desired behaviors and penalties for undesired ones, gradually improving their performance through trial and error.&lt;/p&gt;



&lt;p&gt;Rather than rewarding only correct behaviors, Argos evaluates &lt;em&gt;how&lt;/em&gt; those behaviors were produced. It draws on a pool of larger, more capable teacher models and rule-based checks to verify two things: first, that the objects and events a model references actually exist in its input, and second, that the model‚Äôs reasoning aligns with what it observes. Argos rewards the model when both conditions are met. In practice, these rewards help curate high-quality training data and guide the model‚Äôs further training.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-argos-works"&gt;How Argos works&lt;strong&gt;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Argos functions as a verification layer on top of an existing multimodal model. Given an image or video, a task or query, and information about the model‚Äôs reasoning and output, Argos identifies where the model indicates objects are located in the image, when it indicates events occur in a video, and what action or answer it produces.&lt;/p&gt;



&lt;p&gt;Argos then applies specialized tools tailored to the specific content to evaluate and score three aspects of the model‚Äôs output. It checks whether the answer is correct, whether referenced objects and events appear at the indicated locations and times, and whether the reasoning is consistent with the visual evidence and the answer (Figure 1).&lt;/p&gt;



&lt;p&gt;These scores are combined using a gated aggregation function, a method that dynamically adjusts the importance of different scores. It emphasizes reasoning checks only when the final output is correct. This design prevents unreliable feedback from dominating training and produces a stable reward signal for&amp;nbsp;reinforcement learning.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1 shows an overview of Argos, an agentic verifier for multimodal reinforcement learning and its downstream applications. The left half of the figure illustrates Argos verifying model responses to visual questions. The left example counts dogs in an image, with red dots marking the referenced dogs and a visual grounding score; another example shows a bathroom scene where the agent reasons whether it can open the door, with an accuracy score. Below these, a blue bar titled ‚ÄúArgos verifier‚Äù feeds into icons representing multiple tools, including Grounding DINO, SAM-2, a pointing-hand evaluator, string matching, and a language model score, where their outputs combine into grounding and accuracy scores. The right half of the figure depicts three categories of downstream tasks powered by this supervision: robotic manipulation (a robot arm interacting with objects on a table), high-level task planning and completion (placing toilet paper on the back of a toilet and putting a bowl on a coffee table), and spatial reasoning (answering a viewpoint-based navigation question using room images). The overall message is that dense, grounded verification enables stronger agent performance on complex, real-world tasks." class="wp-image-1160145" height="1255" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-scaled.jpg" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Argos selects different specialized tools to verify and score the accuracy of referenced points and events in the agent‚Äôs reasoning.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="using-argos-to-curate-data-for-supervised-fine-tuning"&gt;Using Argos to curate data for supervised fine-tuning&lt;/h2&gt;



&lt;p&gt;Argos also helps curate high-quality training data to provide the model with a strong foundation in grounded reasoning. Before the reinforcement learning stage begins, Argos uses a multi-stage process to generate data that is explicitly tied to visual locations and time intervals.&lt;/p&gt;



&lt;p&gt;In the first stage, Argos identifies the objects, actions, and events that are relevant to a task and links them to specific locations in images or specific moments in videos. These references are overlaid on images and selected video frames. Next, a reasoning model generates step-by-step explanations that refer to these visual locations and time spans.&lt;/p&gt;



&lt;p&gt;Finally, Argos evaluates each generated example for accuracy and visual grounding, filtering out low-quality training data and retaining only data that is both correct and well-grounded in visual input. The resulting dataset is then used in an initial training phase, where the model learns to generate reasoning steps before producing its final output. This process is illustrated in Figure 2.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2 illustrates the Argos scoring pipeline for both images and videos. On the left, two examples show an image of a living room and a short video clip, each paired with a question and a free-form model response (e.g., estimating the distance between two lamps, or describing why a person failed to pour oil). In the middle, an ‚ÄúAgentic Verifier‚Äù column parses each response into structured elements: spatial 2D points indicating the referenced object and pixel coordinates, temporal segments for the relevant video frames, a reasoning-quality panel that combines the image/video, question, and response, and a final-answer panel comparing the predicted answer to ground truth. Below, a row of teacher models and scoring functions, such as Grounding DINO, SAM-2, a pointing-hand metric, string matching, relative accuracy, and a language model score, take these extracted elements as input to produce separate scores. On the right, arrows labeled ‚ÄúAction‚Äù and ‚ÄúScore‚Äù show how the verifier adaptively selects which teachers to call and then aggregates their outputs via a gated aggregation function into a single reward signal for training. " class="wp-image-1160147" height="450" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/data-curation-animation-gif.gif" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Argos generates step-by-step reasoning grounded in image locations and video timestamps then filters out low-quality training data.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="evaluation"&gt;Evaluation&lt;/h2&gt;



&lt;p&gt;Building on this foundation in grounded reasoning, we further trained the model using reinforcement learning guided by Argos and evaluated its performance across a range of benchmarks. On spatial reasoning tasks, the Argos-trained model outperformed both the base model Qwen2.5-VL-7B and the stronger Video-R1 baseline across challenging 3D scenarios and multi-view tasks. Models trained with Argos also showed a substantial reduction of hallucinations compared with both standard chain-of-thought prompting and reinforcement learning baselines.&lt;/p&gt;



&lt;p&gt;Finally, we evaluated the model in robotics and other real-world task settings, focusing on high-level planning and fine-grained control. Models trained with Argos performed better on complex, multi-step tasks. Notably, these improvements were achieved using fewer training samples than existing approaches, highlighting the importance of reward design in producing more capable and data-efficient agents. Figure 3 illustrates some of these findings.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3 shows two side-by-side line charts comparing an Agentic model (dashed line) that uses the Argos verifier with a Non-Agentic model (solid line) trained only with an outcome reward. The left plot, ‚ÄúResponse Accuary,‚Äù tracks response accuracy versus RL step (0, 5, 10, 15). Both models start near 0.54 accuracy, but the Agentic curve slightly rises and then stays roughly flat, while the Non-agentic curve steadily declines to about 0.50. The right plot, ‚ÄúVisual Grounding Acc,‚Äù shows visual grounding accuracy over the same steps: the Agentic curve increases monotonically from about 0.39 to just above 0.5, whereas the Non-Agentic curve initially rises slightly and then drops sharply to about 0.1. Together, the plots illustrate that Argos stabilizes answer accuracy and significantly improves visual grounding, while the non-agentic model‚Äôs performance and grounding collapse over training." class="wp-image-1160369" height="406" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-blog_fig3.png" width="1331" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3.&amp;nbsp;Performance of&amp;nbsp;Argos&amp;nbsp;compared with&amp;nbsp;baseline models&amp;nbsp;on the task of visual hallucination detection&amp;nbsp;(left)&amp;nbsp;and&amp;nbsp;embodied&amp;nbsp;task planning and completion&amp;nbsp;(right).&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="how-argos-shapes-reinforcement-learning"&gt;How Argos shapes reinforcement learning&lt;/h3&gt;



&lt;p&gt;To understand how Argos affects learning, we took the same vision-language model that had been trained on our curated dataset and fine-tuned it using&amp;nbsp;reinforcement learning in two different ways. In one approach, Argos was an agentic verifier, checking the correctness of outputs and the quality of reasoning. In the other, the model received feedback only on whether its answers were correct.&lt;/p&gt;



&lt;p&gt;We evaluated both versions on 1,500 samples from a new dataset and tracked their performance throughout the learning process (Figure 4). Although they started at similar levels, the model without Argos quickly got worse. Its accuracy steadily declined, and it increasingly gave answers that ignored what was in the videos. It learned to game the system by producing answers that seemed correct without grounding them in visual evidence.&lt;/p&gt;



&lt;p&gt;The model trained with Argos showed the opposite pattern. Accuracy improved steadily, and the model became better at linking its reasoning to what appeared in the videos. This difference highlights the value of verification: when training rewards both correct outputs and sound reasoning based on visual and temporal evidence, models learn to be more reliable rather than simply finding shortcuts to high scores.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4 shows two side-by-side line charts comparing an Agentic model (dashed line) that uses the Argos verifier with a Non-Agentic model (solid line) trained only with an outcome reward. The left plot, ‚ÄúResponse Accuary,‚Äù tracks response accuracy versus RL step (0, 5, 10, 15). Both models start near 0.54 accuracy, but the Agentic curve slightly rises and then stays roughly flat, while the Non-agentic curve steadily declines to about 0.50. The right plot, ‚ÄúVisual Grounding Acc,‚Äù shows visual grounding accuracy over the same steps: the Agentic curve increases monotonically from about 0.39 to just above 0.5, whereas the Non-Agentic curve initially rises slightly and then drops sharply to about 0.1. Together, the plots illustrate that Argos stabilizes answer accuracy and significantly improves visual grounding, while the non-agentic model‚Äôs performance and grounding collapse over training. " class="wp-image-1160428" height="550" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/evaluation-fig4-final.jpg" width="1996" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4.&amp;nbsp;Comparison of&amp;nbsp;response accuracy changes with and without Argos&amp;nbsp;across&amp;nbsp;two model versions&amp;nbsp;(left) and&amp;nbsp;differences in&amp;nbsp;visual grounding accuracy&amp;nbsp;over training for both&amp;nbsp;versions&amp;nbsp;(right).&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="potential-impact-and-looking-forward"&gt;Potential impact and looking forward&lt;/h2&gt;



&lt;p&gt;This research points toward a different way of building AI agents for real-world applications. Rather than fixing errors after they occur, it focuses on training agents to systematically anchor their reasoning in what they actually receive as input throughout the training process.&lt;/p&gt;



&lt;p&gt;The potential applications span many domains. A visual assistant for a self-driving car that verifies what‚Äôs actually in an image is less likely to report phantom obstacles. A system that automates digital tasks and checks each action against what‚Äôs displayed on the screen is less likely to click the wrong button.&lt;/p&gt;



&lt;p&gt;As AI systems move beyond research labs into homes, factories, and offices, reliable reasoning becomes essential for safety and trust. Argos represents an early example of verification systems that evolve alongside the AI models they supervise. Future verifiers could be tailored for specific fields like medical imaging, industrial simulations, and business analytics. As more advanced models and richer data sources become available, researchers can use them to improve these verification systems, providing even better guidance during training and further reducing hallucinations.&lt;/p&gt;



&lt;p&gt;We hope that this research helps move the field toward AI systems that are both capable and interpretable: agents that can explain their decisions, point to the evidence behind them, and be trained to adhere to real-world requirements and values.&lt;/p&gt;



&lt;figure class="wp-block-video aligncenter"&gt;&lt;video controls="controls" height="1076" poster="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos.png" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-demo-video.mp4" width="1920"&gt;&lt;/video&gt;&lt;/figure&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Diagram showing visual, audio, and document icons feeding into a central network icon of connected people, which then leads to a checkmark symbol, all on a blue‚Äëto‚Äëpurple gradient background." class="wp-image-1160195" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;div class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section"&gt;
	
	&lt;div class="container"&gt;
		&lt;div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow"&gt;
			&lt;div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h2 class="wp-block-heading h3" id="at-a-glance"&gt;At a glance&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Today‚Äôs multimodal AI systems&amp;nbsp;can&amp;nbsp;give&amp;nbsp;answers that sound right but&amp;nbsp;may not be&amp;nbsp;grounded in what they&amp;nbsp;actually&amp;nbsp;observe&amp;nbsp;over time, leading to unpredictable errors and safety risks in real-world settings.&lt;/li&gt;



&lt;li&gt;Argos is a verification framework for multimodal reinforcement learning that trains models by rewarding not just correct answers, but correct answers grounded in visual and temporal evidence, using automated verification rather than human labeling.&amp;nbsp;It selects the appropriate specialized tools for each answer&amp;nbsp;based on what needs to be verified.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Models trained with Argos show stronger spatial reasoning, far fewer visual hallucinations, more stable learning dynamics, and better performance on robotics and real-world tasks while requiring fewer training samples.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;		&lt;/div&gt;
	&lt;/div&gt;

	&lt;/div&gt;



&lt;p&gt;Over the past few years, AI systems have become much better at discerning images, generating language, and performing tasks within physical and virtual environments. Yet they still fail in ways that are hard to predict and even harder to fix. A robot might try to grasp a tool when the object is visibly blocked, or a visual assistant integrated into smart glasses might describe objects that aren‚Äôt actually present.&lt;/p&gt;



&lt;p&gt;These errors often arise because today‚Äôs multimodal agents are trained to generate outputs that are plausible rather than grounded in the actual information they receive from their environment. As a result, a model‚Äôs output can seem correct while relying on incorrect information. As AI systems are increasingly used to navigate 3D spaces and make decisions in real-world settings, this gap can be a safety and reliability concern.&lt;/p&gt;



&lt;p&gt;To tackle this challenge, we posed the question: How can we train AI agents to generate correct answers and take appropriate actions for the right reasons so that their behavior is reliable even as the environment or tasks change?&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;Argos represents a novel answer to this challenge. It‚Äôs an agentic verification framework designed to improve the reliability of reinforcement learning in multimodal models.&amp;nbsp;Reinforcement learning is a training method where AI models learn by receiving rewards for desired behaviors and penalties for undesired ones, gradually improving their performance through trial and error.&lt;/p&gt;



&lt;p&gt;Rather than rewarding only correct behaviors, Argos evaluates &lt;em&gt;how&lt;/em&gt; those behaviors were produced. It draws on a pool of larger, more capable teacher models and rule-based checks to verify two things: first, that the objects and events a model references actually exist in its input, and second, that the model‚Äôs reasoning aligns with what it observes. Argos rewards the model when both conditions are met. In practice, these rewards help curate high-quality training data and guide the model‚Äôs further training.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-argos-works"&gt;How Argos works&lt;strong&gt;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Argos functions as a verification layer on top of an existing multimodal model. Given an image or video, a task or query, and information about the model‚Äôs reasoning and output, Argos identifies where the model indicates objects are located in the image, when it indicates events occur in a video, and what action or answer it produces.&lt;/p&gt;



&lt;p&gt;Argos then applies specialized tools tailored to the specific content to evaluate and score three aspects of the model‚Äôs output. It checks whether the answer is correct, whether referenced objects and events appear at the indicated locations and times, and whether the reasoning is consistent with the visual evidence and the answer (Figure 1).&lt;/p&gt;



&lt;p&gt;These scores are combined using a gated aggregation function, a method that dynamically adjusts the importance of different scores. It emphasizes reasoning checks only when the final output is correct. This design prevents unreliable feedback from dominating training and produces a stable reward signal for&amp;nbsp;reinforcement learning.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1 shows an overview of Argos, an agentic verifier for multimodal reinforcement learning and its downstream applications. The left half of the figure illustrates Argos verifying model responses to visual questions. The left example counts dogs in an image, with red dots marking the referenced dogs and a visual grounding score; another example shows a bathroom scene where the agent reasons whether it can open the door, with an accuracy score. Below these, a blue bar titled ‚ÄúArgos verifier‚Äù feeds into icons representing multiple tools, including Grounding DINO, SAM-2, a pointing-hand evaluator, string matching, and a language model score, where their outputs combine into grounding and accuracy scores. The right half of the figure depicts three categories of downstream tasks powered by this supervision: robotic manipulation (a robot arm interacting with objects on a table), high-level task planning and completion (placing toilet paper on the back of a toilet and putting a bowl on a coffee table), and spatial reasoning (answering a viewpoint-based navigation question using room images). The overall message is that dense, grounded verification enables stronger agent performance on complex, real-world tasks." class="wp-image-1160145" height="1255" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-scaled.jpg" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Argos selects different specialized tools to verify and score the accuracy of referenced points and events in the agent‚Äôs reasoning.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="using-argos-to-curate-data-for-supervised-fine-tuning"&gt;Using Argos to curate data for supervised fine-tuning&lt;/h2&gt;



&lt;p&gt;Argos also helps curate high-quality training data to provide the model with a strong foundation in grounded reasoning. Before the reinforcement learning stage begins, Argos uses a multi-stage process to generate data that is explicitly tied to visual locations and time intervals.&lt;/p&gt;



&lt;p&gt;In the first stage, Argos identifies the objects, actions, and events that are relevant to a task and links them to specific locations in images or specific moments in videos. These references are overlaid on images and selected video frames. Next, a reasoning model generates step-by-step explanations that refer to these visual locations and time spans.&lt;/p&gt;



&lt;p&gt;Finally, Argos evaluates each generated example for accuracy and visual grounding, filtering out low-quality training data and retaining only data that is both correct and well-grounded in visual input. The resulting dataset is then used in an initial training phase, where the model learns to generate reasoning steps before producing its final output. This process is illustrated in Figure 2.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2 illustrates the Argos scoring pipeline for both images and videos. On the left, two examples show an image of a living room and a short video clip, each paired with a question and a free-form model response (e.g., estimating the distance between two lamps, or describing why a person failed to pour oil). In the middle, an ‚ÄúAgentic Verifier‚Äù column parses each response into structured elements: spatial 2D points indicating the referenced object and pixel coordinates, temporal segments for the relevant video frames, a reasoning-quality panel that combines the image/video, question, and response, and a final-answer panel comparing the predicted answer to ground truth. Below, a row of teacher models and scoring functions, such as Grounding DINO, SAM-2, a pointing-hand metric, string matching, relative accuracy, and a language model score, take these extracted elements as input to produce separate scores. On the right, arrows labeled ‚ÄúAction‚Äù and ‚ÄúScore‚Äù show how the verifier adaptively selects which teachers to call and then aggregates their outputs via a gated aggregation function into a single reward signal for training. " class="wp-image-1160147" height="450" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/data-curation-animation-gif.gif" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Argos generates step-by-step reasoning grounded in image locations and video timestamps then filters out low-quality training data.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="evaluation"&gt;Evaluation&lt;/h2&gt;



&lt;p&gt;Building on this foundation in grounded reasoning, we further trained the model using reinforcement learning guided by Argos and evaluated its performance across a range of benchmarks. On spatial reasoning tasks, the Argos-trained model outperformed both the base model Qwen2.5-VL-7B and the stronger Video-R1 baseline across challenging 3D scenarios and multi-view tasks. Models trained with Argos also showed a substantial reduction of hallucinations compared with both standard chain-of-thought prompting and reinforcement learning baselines.&lt;/p&gt;



&lt;p&gt;Finally, we evaluated the model in robotics and other real-world task settings, focusing on high-level planning and fine-grained control. Models trained with Argos performed better on complex, multi-step tasks. Notably, these improvements were achieved using fewer training samples than existing approaches, highlighting the importance of reward design in producing more capable and data-efficient agents. Figure 3 illustrates some of these findings.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3 shows two side-by-side line charts comparing an Agentic model (dashed line) that uses the Argos verifier with a Non-Agentic model (solid line) trained only with an outcome reward. The left plot, ‚ÄúResponse Accuary,‚Äù tracks response accuracy versus RL step (0, 5, 10, 15). Both models start near 0.54 accuracy, but the Agentic curve slightly rises and then stays roughly flat, while the Non-agentic curve steadily declines to about 0.50. The right plot, ‚ÄúVisual Grounding Acc,‚Äù shows visual grounding accuracy over the same steps: the Agentic curve increases monotonically from about 0.39 to just above 0.5, whereas the Non-Agentic curve initially rises slightly and then drops sharply to about 0.1. Together, the plots illustrate that Argos stabilizes answer accuracy and significantly improves visual grounding, while the non-agentic model‚Äôs performance and grounding collapse over training." class="wp-image-1160369" height="406" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-blog_fig3.png" width="1331" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3.&amp;nbsp;Performance of&amp;nbsp;Argos&amp;nbsp;compared with&amp;nbsp;baseline models&amp;nbsp;on the task of visual hallucination detection&amp;nbsp;(left)&amp;nbsp;and&amp;nbsp;embodied&amp;nbsp;task planning and completion&amp;nbsp;(right).&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="how-argos-shapes-reinforcement-learning"&gt;How Argos shapes reinforcement learning&lt;/h3&gt;



&lt;p&gt;To understand how Argos affects learning, we took the same vision-language model that had been trained on our curated dataset and fine-tuned it using&amp;nbsp;reinforcement learning in two different ways. In one approach, Argos was an agentic verifier, checking the correctness of outputs and the quality of reasoning. In the other, the model received feedback only on whether its answers were correct.&lt;/p&gt;



&lt;p&gt;We evaluated both versions on 1,500 samples from a new dataset and tracked their performance throughout the learning process (Figure 4). Although they started at similar levels, the model without Argos quickly got worse. Its accuracy steadily declined, and it increasingly gave answers that ignored what was in the videos. It learned to game the system by producing answers that seemed correct without grounding them in visual evidence.&lt;/p&gt;



&lt;p&gt;The model trained with Argos showed the opposite pattern. Accuracy improved steadily, and the model became better at linking its reasoning to what appeared in the videos. This difference highlights the value of verification: when training rewards both correct outputs and sound reasoning based on visual and temporal evidence, models learn to be more reliable rather than simply finding shortcuts to high scores.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4 shows two side-by-side line charts comparing an Agentic model (dashed line) that uses the Argos verifier with a Non-Agentic model (solid line) trained only with an outcome reward. The left plot, ‚ÄúResponse Accuary,‚Äù tracks response accuracy versus RL step (0, 5, 10, 15). Both models start near 0.54 accuracy, but the Agentic curve slightly rises and then stays roughly flat, while the Non-agentic curve steadily declines to about 0.50. The right plot, ‚ÄúVisual Grounding Acc,‚Äù shows visual grounding accuracy over the same steps: the Agentic curve increases monotonically from about 0.39 to just above 0.5, whereas the Non-Agentic curve initially rises slightly and then drops sharply to about 0.1. Together, the plots illustrate that Argos stabilizes answer accuracy and significantly improves visual grounding, while the non-agentic model‚Äôs performance and grounding collapse over training. " class="wp-image-1160428" height="550" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/evaluation-fig4-final.jpg" width="1996" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4.&amp;nbsp;Comparison of&amp;nbsp;response accuracy changes with and without Argos&amp;nbsp;across&amp;nbsp;two model versions&amp;nbsp;(left) and&amp;nbsp;differences in&amp;nbsp;visual grounding accuracy&amp;nbsp;over training for both&amp;nbsp;versions&amp;nbsp;(right).&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="potential-impact-and-looking-forward"&gt;Potential impact and looking forward&lt;/h2&gt;



&lt;p&gt;This research points toward a different way of building AI agents for real-world applications. Rather than fixing errors after they occur, it focuses on training agents to systematically anchor their reasoning in what they actually receive as input throughout the training process.&lt;/p&gt;



&lt;p&gt;The potential applications span many domains. A visual assistant for a self-driving car that verifies what‚Äôs actually in an image is less likely to report phantom obstacles. A system that automates digital tasks and checks each action against what‚Äôs displayed on the screen is less likely to click the wrong button.&lt;/p&gt;



&lt;p&gt;As AI systems move beyond research labs into homes, factories, and offices, reliable reasoning becomes essential for safety and trust. Argos represents an early example of verification systems that evolve alongside the AI models they supervise. Future verifiers could be tailored for specific fields like medical imaging, industrial simulations, and business analytics. As more advanced models and richer data sources become available, researchers can use them to improve these verification systems, providing even better guidance during training and further reducing hallucinations.&lt;/p&gt;



&lt;p&gt;We hope that this research helps move the field toward AI systems that are both capable and interpretable: agents that can explain their decisions, point to the evidence behind them, and be trained to adhere to real-world requirements and values.&lt;/p&gt;



&lt;figure class="wp-block-video aligncenter"&gt;&lt;video controls="controls" height="1076" poster="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos.png" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-demo-video.mp4" width="1920"&gt;&lt;/video&gt;&lt;/figure&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/</guid><pubDate>Tue, 20 Jan 2026 17:00:00 +0000</pubDate></item></channel></rss>