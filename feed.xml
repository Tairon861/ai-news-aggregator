<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 21 Oct 2025 01:42:30 +0000</lastBuildDate><item><title>Final countdown: Only 7 days until TechCrunch Disrupt 2025 and ticket prices increase (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/final-countdown-only-7-days-until-techcrunch-disrupt-2025-and-ticket-prices-increase/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;T-minus 7 days until &lt;strong&gt;TechCrunch&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;Disrupt&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;2025&lt;/strong&gt; officially kicks off at San Francisco’s Moscone West! One of the year’s biggest tech events is ready to dominate the Bay Area’s thriving tech landscape for almost the entire week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Get ready for one of the biggest tech gatherings on October 27-29. Tech experts from every corner of the globe will come together to engage with the latest innovations, learn trends, and connect through unparalleled networking. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss out on Disrupt ticket savings! &lt;strong&gt;Save up to $444&lt;/strong&gt; on your pass. &lt;strong&gt;Bring a plus-one&lt;/strong&gt; and get 60% off their ticket, or &lt;strong&gt;bring your community&lt;/strong&gt; and save 30%.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 7 days" class="wp-image-3059244" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_7Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-you-nbsp-can-t-nbsp-miss-disrupt-2025"&gt;Why you&amp;nbsp;can’t&amp;nbsp;miss Disrupt 2025&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-10-000-startup-and-vc-leaders"&gt;10,000+ startup and VC leaders&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Forge powerful connections with the 10,000 founders, VCs, operators, and innovators that gather at Disrupt.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-300-nbsp-innovations"&gt;300+&amp;nbsp;innovations&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Discover groundbreaking innovations from 300+ startups from around the world&amp;nbsp;showcased&amp;nbsp;in the Expo Hall&amp;nbsp;and spread throughout the venue. See their latest breakthroughs in action.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-250-nbsp-industry-experts"&gt;250+&amp;nbsp;industry experts&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Gain invaluable insights from some of the top industry heavyweights across&amp;nbsp;&lt;strong&gt;5&amp;nbsp;of the industry stages&lt;/strong&gt;&amp;nbsp;and sessions.&amp;nbsp;Featured speakers include&amp;nbsp;Vinod Khosla,&amp;nbsp;Elizabeth Stone&amp;nbsp;(Netflix), and&amp;nbsp;Kevin Scott&amp;nbsp;(Microsoft).&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Box CEO Aaron Levie on stage at TechCrunch Disrupt in San Francisco in 2019." class="wp-image-2833981" height="454" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-1178603809.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-200-nbsp-hands-on-sessions"&gt;200+&amp;nbsp;hands-on sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Engage in &lt;strong&gt;Q&amp;amp;A breakouts and dynamic roundtable sessions&lt;/strong&gt; led by industry experts, tackling today’s key challenges and tomorrow’s breakthroughs. Topics include the scaling playbook for 2026, exiting strategies, the future of AI, and so much more. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-startup-battlefield-200"&gt;Startup Battlefield 200&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Witness the high-stakes &lt;strong&gt;startup showdown&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;that is a highlight of Disrupt. Twenty handpicked startups, chosen from thousands, will pitch to a panel of leading VC judges, competing for a $100,000 equity-free prize, VC attention, and the iconic Disrupt Cup.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Salva Health Co-Founder &amp;amp; CEO Valentina Agudelo Vargas, winner of the Startup Battlefield 2024, poses onstage during TechCrunch Disrupt 2024 Day 3 at Moscone Center on October 30, 2024 in San Francisco." class="wp-image-2913234" height="453" src="https://techcrunch.com/wp-content/uploads/2024/11/54105085427_2cae9d0502_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-meaningful-connections-maximum-impact"&gt;Meaningful connections, maximum impact&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to impromptu networking across the venue, use Braindate to post and explore topics, then dive into them in 1:1 or small-group sessions in the Networking Lounge. This is where meaningful connections start.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-50-side-events"&gt;50+ Side Events&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Extend the excitement of Disrupt beyond the main event by joining company-hosted &lt;strong&gt;Side Events&lt;/strong&gt; across San Francisco. Immerse yourself in the vibrant tech community in a relaxed setting, with options ranging from workshops and happy hours to cocktail parties, morning runs, and job fairs — there’s something for everyone.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-10-000-tech-leaders-will-be-at-disrupt-will-you-be-nbsp-there"&gt;10,000+ tech leaders will be at Disrupt. Will you be&amp;nbsp;there?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;There are endless reasons to join this epic tech conference, but the best way to understand its value is to experience it yourself. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss out on the last 7 days of ticket savings! &lt;strong&gt;Save up to $444&lt;/strong&gt; and take advantage of &lt;strong&gt;60% savings on a second pass&lt;/strong&gt;. Prices for all tickets will increase when the doors open at Moscone West at Disrupt on October 27.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;T-minus 7 days until &lt;strong&gt;TechCrunch&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;Disrupt&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;2025&lt;/strong&gt; officially kicks off at San Francisco’s Moscone West! One of the year’s biggest tech events is ready to dominate the Bay Area’s thriving tech landscape for almost the entire week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Get ready for one of the biggest tech gatherings on October 27-29. Tech experts from every corner of the globe will come together to engage with the latest innovations, learn trends, and connect through unparalleled networking. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss out on Disrupt ticket savings! &lt;strong&gt;Save up to $444&lt;/strong&gt; on your pass. &lt;strong&gt;Bring a plus-one&lt;/strong&gt; and get 60% off their ticket, or &lt;strong&gt;bring your community&lt;/strong&gt; and save 30%.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 7 days" class="wp-image-3059244" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_7Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-you-nbsp-can-t-nbsp-miss-disrupt-2025"&gt;Why you&amp;nbsp;can’t&amp;nbsp;miss Disrupt 2025&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-10-000-startup-and-vc-leaders"&gt;10,000+ startup and VC leaders&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Forge powerful connections with the 10,000 founders, VCs, operators, and innovators that gather at Disrupt.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-300-nbsp-innovations"&gt;300+&amp;nbsp;innovations&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Discover groundbreaking innovations from 300+ startups from around the world&amp;nbsp;showcased&amp;nbsp;in the Expo Hall&amp;nbsp;and spread throughout the venue. See their latest breakthroughs in action.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-250-nbsp-industry-experts"&gt;250+&amp;nbsp;industry experts&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Gain invaluable insights from some of the top industry heavyweights across&amp;nbsp;&lt;strong&gt;5&amp;nbsp;of the industry stages&lt;/strong&gt;&amp;nbsp;and sessions.&amp;nbsp;Featured speakers include&amp;nbsp;Vinod Khosla,&amp;nbsp;Elizabeth Stone&amp;nbsp;(Netflix), and&amp;nbsp;Kevin Scott&amp;nbsp;(Microsoft).&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Box CEO Aaron Levie on stage at TechCrunch Disrupt in San Francisco in 2019." class="wp-image-2833981" height="454" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-1178603809.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-200-nbsp-hands-on-sessions"&gt;200+&amp;nbsp;hands-on sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Engage in &lt;strong&gt;Q&amp;amp;A breakouts and dynamic roundtable sessions&lt;/strong&gt; led by industry experts, tackling today’s key challenges and tomorrow’s breakthroughs. Topics include the scaling playbook for 2026, exiting strategies, the future of AI, and so much more. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-startup-battlefield-200"&gt;Startup Battlefield 200&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Witness the high-stakes &lt;strong&gt;startup showdown&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;that is a highlight of Disrupt. Twenty handpicked startups, chosen from thousands, will pitch to a panel of leading VC judges, competing for a $100,000 equity-free prize, VC attention, and the iconic Disrupt Cup.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Salva Health Co-Founder &amp;amp; CEO Valentina Agudelo Vargas, winner of the Startup Battlefield 2024, poses onstage during TechCrunch Disrupt 2024 Day 3 at Moscone Center on October 30, 2024 in San Francisco." class="wp-image-2913234" height="453" src="https://techcrunch.com/wp-content/uploads/2024/11/54105085427_2cae9d0502_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-meaningful-connections-maximum-impact"&gt;Meaningful connections, maximum impact&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to impromptu networking across the venue, use Braindate to post and explore topics, then dive into them in 1:1 or small-group sessions in the Networking Lounge. This is where meaningful connections start.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-50-side-events"&gt;50+ Side Events&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Extend the excitement of Disrupt beyond the main event by joining company-hosted &lt;strong&gt;Side Events&lt;/strong&gt; across San Francisco. Immerse yourself in the vibrant tech community in a relaxed setting, with options ranging from workshops and happy hours to cocktail parties, morning runs, and job fairs — there’s something for everyone.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-10-000-tech-leaders-will-be-at-disrupt-will-you-be-nbsp-there"&gt;10,000+ tech leaders will be at Disrupt. Will you be&amp;nbsp;there?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;There are endless reasons to join this epic tech conference, but the best way to understand its value is to experience it yourself. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss out on the last 7 days of ticket savings! &lt;strong&gt;Save up to $444&lt;/strong&gt; and take advantage of &lt;strong&gt;60% savings on a second pass&lt;/strong&gt;. Prices for all tickets will increase when the doors open at Moscone West at Disrupt on October 27.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/final-countdown-only-7-days-until-techcrunch-disrupt-2025-and-ticket-prices-increase/</guid><pubDate>Mon, 20 Oct 2025 14:00:00 +0000</pubDate></item><item><title>Last-minute ticket deal for TechCrunch Disrupt 2025: Save 60% on your plus-one (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/last-minute-ticket-deal-for-techcrunch-disrupt-2025-save-60-on-your-plus-one/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Today begins the 7-day countdown to &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, the flagship tech conference bringing together 10,000+ leaders, VCs, operators, and innovators, taking place at Moscone West in San Francisco, October 27–29.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To celebrate the countdown, we’re offering a special 60% discount for your plus-one. Buy your pass and save up to $444, &lt;strong&gt;plus get 60% off a second ticket&lt;/strong&gt; for your co-founder, partner, friend, or team member.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Get your two discounted passes here&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-disrupt-2025-ticket-type-nbsp-perks"&gt;Disrupt 2025 ticket type&amp;nbsp;perks&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-industry-stage-programming-5-stages"&gt;Industry&amp;nbsp;Stage&amp;nbsp;programming: 5&amp;nbsp;stages&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For&amp;nbsp;Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Five massive stages that are designed to bring insights from the top leaders across the tech ecosystem, such as Vinod Khosla, Aaron Levie (Box), Elizabeth Stone (Netflix), Kevin Scott (Microsoft), and more. &lt;strong&gt;Explore the agenda and meet the speakers&lt;/strong&gt; taking the stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;AI Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Join AI experts as they unpack the latest advancements and possibilities in artificial intelligence. Explore the science driving deep tech innovations, the products it’s shaping, and the ethical, social, and legal questions it raises.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Builders Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— One of the classic founder-focused stages at Disrupt, the Builders Stage delivers the key strategies and lessons from veteran founders and builders to help you achieve your entrepreneurial dreams.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Sit front and center on the main stage to witness the iconic Startup Battlefield 200, where the top 20 handpicked startups pitch to a panel of leading VCs. Learn from these leaders about what it takes to build a viable startup, and gain insights from exclusive discussion sessions with industry heavyweights.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt startup Battlefield presentation" class="wp-image-2964435" height="453" src="https://techcrunch.com/wp-content/uploads/2024/12/Startup-battlefield-disrupt-2024.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Going Public Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— No matter if you’re at day one or planning your IPO, the Going Public Stage delivers lessons, frameworks, and founder stories that resonate across every stage of the scaling journey.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Space Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Emerging companies are disrupting the space industry at a pivotal time when technology and markets are aligning to drive significant breakthroughs in space exploration.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-interactive-roundtable-sessions"&gt;Interactive roundtable sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These &lt;strong&gt;small group dynamic sessions&lt;/strong&gt; let everyone dive deep into a topic, ask questions, share experiences, and truly connect, led by industry leaders from across different sectors in tech.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-q-amp-a-breakout-nbsp-sessions"&gt;Q&amp;amp;A breakout&amp;nbsp;sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Located right next to the Expo Hall, &lt;strong&gt;these sessions&lt;/strong&gt; are led by a panel of industry experts who will cover a topic and then end the session by answering questions from the audience. Bring your pen and notebook; this is where the learning happens.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Breakout session" class="wp-image-2758365" height="340" src="https://techcrunch.com/wp-content/uploads/2024/05/breakout_1200x600.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-expo-hall"&gt;Expo Hall&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meet 300+ startups showcasing their innovations at all stages. The Expo Hall is the heart of the venue, where 10,000+ tech leaders and VCs come to discover what’s next and connect with startups ready to pitch and demo their solutions.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-powerful-networking"&gt;Powerful networking&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is &lt;em&gt;the&lt;/em&gt; launchpad for startup and individual growth. Schedule 1:1 or small group sessions via Braindate, then meet in the Networking Lounge to dive deep into topics, spark insightful conversations, and form impactful connections.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Braindate networking" class="wp-image-2953563" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/disrupt-2024-braindate-networking.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-disrupt-and-the-plus-one-ticket-discount"&gt;Don’t miss Disrupt — and the plus-one ticket discount&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is just around the corner. Don’t miss the final chance to pocket real savings for plus-one and group passes. Bring your crew and experience it together. &lt;strong&gt;Register for you and your plus-one&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;or &lt;strong&gt;grab community passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Today begins the 7-day countdown to &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, the flagship tech conference bringing together 10,000+ leaders, VCs, operators, and innovators, taking place at Moscone West in San Francisco, October 27–29.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To celebrate the countdown, we’re offering a special 60% discount for your plus-one. Buy your pass and save up to $444, &lt;strong&gt;plus get 60% off a second ticket&lt;/strong&gt; for your co-founder, partner, friend, or team member.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Get your two discounted passes here&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-disrupt-2025-ticket-type-nbsp-perks"&gt;Disrupt 2025 ticket type&amp;nbsp;perks&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-industry-stage-programming-5-stages"&gt;Industry&amp;nbsp;Stage&amp;nbsp;programming: 5&amp;nbsp;stages&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For&amp;nbsp;Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Five massive stages that are designed to bring insights from the top leaders across the tech ecosystem, such as Vinod Khosla, Aaron Levie (Box), Elizabeth Stone (Netflix), Kevin Scott (Microsoft), and more. &lt;strong&gt;Explore the agenda and meet the speakers&lt;/strong&gt; taking the stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;AI Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Join AI experts as they unpack the latest advancements and possibilities in artificial intelligence. Explore the science driving deep tech innovations, the products it’s shaping, and the ethical, social, and legal questions it raises.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Builders Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— One of the classic founder-focused stages at Disrupt, the Builders Stage delivers the key strategies and lessons from veteran founders and builders to help you achieve your entrepreneurial dreams.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Sit front and center on the main stage to witness the iconic Startup Battlefield 200, where the top 20 handpicked startups pitch to a panel of leading VCs. Learn from these leaders about what it takes to build a viable startup, and gain insights from exclusive discussion sessions with industry heavyweights.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt startup Battlefield presentation" class="wp-image-2964435" height="453" src="https://techcrunch.com/wp-content/uploads/2024/12/Startup-battlefield-disrupt-2024.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Going Public Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— No matter if you’re at day one or planning your IPO, the Going Public Stage delivers lessons, frameworks, and founder stories that resonate across every stage of the scaling journey.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Space Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Emerging companies are disrupting the space industry at a pivotal time when technology and markets are aligning to drive significant breakthroughs in space exploration.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-interactive-roundtable-sessions"&gt;Interactive roundtable sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These &lt;strong&gt;small group dynamic sessions&lt;/strong&gt; let everyone dive deep into a topic, ask questions, share experiences, and truly connect, led by industry leaders from across different sectors in tech.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-q-amp-a-breakout-nbsp-sessions"&gt;Q&amp;amp;A breakout&amp;nbsp;sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Located right next to the Expo Hall, &lt;strong&gt;these sessions&lt;/strong&gt; are led by a panel of industry experts who will cover a topic and then end the session by answering questions from the audience. Bring your pen and notebook; this is where the learning happens.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Breakout session" class="wp-image-2758365" height="340" src="https://techcrunch.com/wp-content/uploads/2024/05/breakout_1200x600.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-expo-hall"&gt;Expo Hall&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meet 300+ startups showcasing their innovations at all stages. The Expo Hall is the heart of the venue, where 10,000+ tech leaders and VCs come to discover what’s next and connect with startups ready to pitch and demo their solutions.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-powerful-networking"&gt;Powerful networking&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is &lt;em&gt;the&lt;/em&gt; launchpad for startup and individual growth. Schedule 1:1 or small group sessions via Braindate, then meet in the Networking Lounge to dive deep into topics, spark insightful conversations, and form impactful connections.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Braindate networking" class="wp-image-2953563" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/disrupt-2024-braindate-networking.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-disrupt-and-the-plus-one-ticket-discount"&gt;Don’t miss Disrupt — and the plus-one ticket discount&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is just around the corner. Don’t miss the final chance to pocket real savings for plus-one and group passes. Bring your crew and experience it together. &lt;strong&gt;Register for you and your plus-one&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;or &lt;strong&gt;grab community passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/last-minute-ticket-deal-for-techcrunch-disrupt-2025-save-60-on-your-plus-one/</guid><pubDate>Mon, 20 Oct 2025 14:30:00 +0000</pubDate></item><item><title>OpenEvidence, the ChatGPT for doctors, raises $200M at $6B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/openevidence-the-chatgpt-for-doctors-raises-200m-at-6b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/openevidence.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenEvidence, a tool that doctors and nurses have likened to ChatGPT for medicine, plans to announce a $200 million raise at a $6 billion valuation, The New York Times reports.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh funds come three months after the startup raised a $210 million round at a $3.5 billion valuation, a testament to the intense investor interest in industry-specific AI applications.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Trained on medical journals like JAMA and the New England Journal of Medicine, the OpenEvidence platform helps users quickly get answers to existing medical knowledge to help treat patients. Verified medical professionals can access OpenEvidence’s tool, which is supported by advertising, for free. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenEvidence has grown quickly since its founding in 2022. Its number of clinical consultations per month has nearly doubled to 15 million since July, per the Times. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Google Ventures, with participation from Sequoia Capital, Kleiner Perkins, Blackstone, Thrive Capital, Coatue Management, Bond, and Craft. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenEvidence for comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/openevidence.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenEvidence, a tool that doctors and nurses have likened to ChatGPT for medicine, plans to announce a $200 million raise at a $6 billion valuation, The New York Times reports.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh funds come three months after the startup raised a $210 million round at a $3.5 billion valuation, a testament to the intense investor interest in industry-specific AI applications.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Trained on medical journals like JAMA and the New England Journal of Medicine, the OpenEvidence platform helps users quickly get answers to existing medical knowledge to help treat patients. Verified medical professionals can access OpenEvidence’s tool, which is supported by advertising, for free. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenEvidence has grown quickly since its founding in 2022. Its number of clinical consultations per month has nearly doubled to 15 million since July, per the Times. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Google Ventures, with participation from Sequoia Capital, Kleiner Perkins, Blackstone, Thrive Capital, Coatue Management, Bond, and Craft. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenEvidence for comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/openevidence-the-chatgpt-for-doctors-raises-200m-at-6b-valuation/</guid><pubDate>Mon, 20 Oct 2025 14:46:58 +0000</pubDate></item><item><title>How AI Is Unlocking Level 4 Autonomous Driving (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2018/01/automotive-key-visual-corp-blog-level4-av-og-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;When the Society of Automotive Engineers established its framework for vehicle autonomy in 2014, it created the industry-standard roadmap for self-driving technology.&lt;/p&gt;
&lt;p&gt;The levels of automation progress from level 1 (driver assistance) to level 2 (partial automation), level 3 (conditional automation), level 4 (high automation) and level 5 (full automation).&lt;/p&gt;
&lt;p&gt;Predicting when each level would arrive proved more challenging than defining them. This uncertainty created industry-wide anticipation, as breakthroughs seemed perpetually just around the corner.&lt;/p&gt;
&lt;p&gt;That dynamic has shifted dramatically in recent years, with more progress in autonomous driving in the past three to four years than in the previous decade combined. Below, learn about recent advancements that have made such rapid progress possible.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Is Level 4 Autonomous Driving?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Level 4 autonomous driving enables vehicles to handle all driving tasks within specific operating zones — such as certain cities or routes — without the need for human intervention. This high automation level uses AI breakthroughs including foundation models, end-to-end architectures and reasoning models to navigate complex scenarios.&lt;/p&gt;
&lt;p&gt;Today, level 4 “high automation” is bringing the vision of autonomous driving closer to a scalable, commercially viable reality.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Six AI Breakthroughs Advancing Autonomous Vehicles&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Six major AI breakthroughs are converging to accelerate level 4 autonomy:&lt;/p&gt;
&lt;p&gt;&lt;b&gt;1. Foundation Models&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Foundation models can tap internet-scale knowledge, not just proprietary driving fleet data.&lt;/p&gt;
&lt;p&gt;When humans learn to drive at, say, 18 years old, they’re bringing 18 years of world experience to the endeavor. Similarly, foundation models bring a breadth of knowledge — understanding unusual scenarios and predicting outcomes based on general world knowledge.&lt;/p&gt;
&lt;p&gt;With foundation models, a vehicle encountering a mattress in the road or a ball rolling into the street can now reason its way through scenarios it has never seen before, drawing on information learned from vast training datasets.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;2. End-to-End Architectures&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Traditional autonomous driving systems used separate modules for perception, planning and control — losing information at each handoff.&lt;/p&gt;
&lt;p&gt;End-to-end autonomy architectures have the potential to change that. With end-to-end architectures, a single network processes sensor inputs directly into driving decisions, maintaining context throughout. While the concept of end-to-end architectures is not new, architectural advancements and improved training methodologies are finally making this paradigm viable, resulting in better autonomous decision-making with less engineering complexity.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;3. Reasoning Models&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Reasoning vision language action (VLA) models integrate diverse perceptual inputs, language understanding and action generation with step-by-step reasoning. This enables them to break down complex situations, evaluate multiple possible outcomes and decide on the best course of action — much like humans do.&lt;/p&gt;
&lt;p&gt;Systems powered by reasoning models deliver far greater reliability and performance, with explainable, step-by-step decision-making. For autonomous vehicles, this means the ability to flag unusual decision patterns for real-time safety monitoring, as well as post-incident debugging to reveal why a vehicle took a particular action. This improves the performance of autonomous vehicles while building user trust.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;4. Simulation&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;With physical testing alone, it would take decades to test a driving policy in every possible driving scenario, if ever achievable at all. Enter simulation.&lt;/p&gt;
&lt;p&gt;Technologies like neural reconstruction can be used to create interactive simulations from real-world sensor data, while world models like NVIDIA Cosmos Predict and Transfer produce unlimited novel situations for training and testing autonomous vehicles.&lt;/p&gt;
&lt;p&gt;With these technologies, developers can use text prompts to generate new weather and road conditions, or change lighting and introduce obstacles to simulate new scenarios and test driving policies in novel conditions.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;5. Compute Power&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;None of these advances would be possible without sufficient computational power. The NVIDIA DRIVE AGX and NVIDIA DGX platforms have evolved through multiple generations, each designed for today’s AI workloads as well as those anticipated years down the road.&lt;/p&gt;
&lt;p&gt;Co-optimization matters. Technology must be designed anticipating the computational demands of next-generation AI systems.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;6. AI Safety&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Safety is foundational for level 4 autonomy, where reliability is the defining characteristic distinguishing it from lower autonomy levels. Recent advances in physical AI safety enable the trustworthy deployment of AI-based autonomy stacks by introducing safety guardrails at the stages of design, deployment and validation.&lt;/p&gt;
&lt;p&gt;For example, NVIDIA’s safety architecture guardrails the end-to-end driving model with checks supported by a diverse modular stack, and validation is greatly accelerated by the latest advancements in neural reconstruction.&lt;/p&gt;
&lt;p&gt;These and other guardrails are part of NVIDIA Halos, a comprehensive safety system that unifies the NVIDIA DRIVE architecture, the safety-certified NVIDIA DriveOS operating system, and AI models, hardware, software, tools and services to help ensure the safe development and deployment of autonomous vehicles from cloud to car. NVIDIA partners can adopt individual components or the full stack, depending on their needs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Why It Matters: Saving Lives and Resources&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The stakes extend far beyond technological achievement. Improving vehicle safety can help save lives and conserve significant amounts of money and resources. Level 4 autonomy systematically removes human error, the cause of the vast majority of crashes.&lt;/p&gt;
&lt;p&gt;NVIDIA, as a full-stack autonomous vehicle company — from cloud to car — is enabling the broader automotive ecosystem to achieve level 4 autonomy, building on the foundation of its level 2+ stack already in production. In particular, NVIDIA is the only company that offers an end-to-end compute stack for autonomous driving.&lt;/p&gt;
&lt;p&gt;Its three AI compute platforms critical for autonomy are:&lt;/p&gt;

&lt;p&gt;Together, these platforms form a feedback loop for learning, testing and deployment that tightens the cycle of innovation while keeping safety front and center.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;NVIDIA GTC Washington, D.C&lt;/i&gt;&lt;i&gt;., running Oct. 27-29, will feature a wide range of &lt;/i&gt;&lt;i&gt;sessions on autonomous vehicles and safety&lt;/i&gt;&lt;i&gt;, which will also be available on demand.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2018/01/automotive-key-visual-corp-blog-level4-av-og-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;When the Society of Automotive Engineers established its framework for vehicle autonomy in 2014, it created the industry-standard roadmap for self-driving technology.&lt;/p&gt;
&lt;p&gt;The levels of automation progress from level 1 (driver assistance) to level 2 (partial automation), level 3 (conditional automation), level 4 (high automation) and level 5 (full automation).&lt;/p&gt;
&lt;p&gt;Predicting when each level would arrive proved more challenging than defining them. This uncertainty created industry-wide anticipation, as breakthroughs seemed perpetually just around the corner.&lt;/p&gt;
&lt;p&gt;That dynamic has shifted dramatically in recent years, with more progress in autonomous driving in the past three to four years than in the previous decade combined. Below, learn about recent advancements that have made such rapid progress possible.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Is Level 4 Autonomous Driving?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Level 4 autonomous driving enables vehicles to handle all driving tasks within specific operating zones — such as certain cities or routes — without the need for human intervention. This high automation level uses AI breakthroughs including foundation models, end-to-end architectures and reasoning models to navigate complex scenarios.&lt;/p&gt;
&lt;p&gt;Today, level 4 “high automation” is bringing the vision of autonomous driving closer to a scalable, commercially viable reality.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Six AI Breakthroughs Advancing Autonomous Vehicles&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Six major AI breakthroughs are converging to accelerate level 4 autonomy:&lt;/p&gt;
&lt;p&gt;&lt;b&gt;1. Foundation Models&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Foundation models can tap internet-scale knowledge, not just proprietary driving fleet data.&lt;/p&gt;
&lt;p&gt;When humans learn to drive at, say, 18 years old, they’re bringing 18 years of world experience to the endeavor. Similarly, foundation models bring a breadth of knowledge — understanding unusual scenarios and predicting outcomes based on general world knowledge.&lt;/p&gt;
&lt;p&gt;With foundation models, a vehicle encountering a mattress in the road or a ball rolling into the street can now reason its way through scenarios it has never seen before, drawing on information learned from vast training datasets.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;2. End-to-End Architectures&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Traditional autonomous driving systems used separate modules for perception, planning and control — losing information at each handoff.&lt;/p&gt;
&lt;p&gt;End-to-end autonomy architectures have the potential to change that. With end-to-end architectures, a single network processes sensor inputs directly into driving decisions, maintaining context throughout. While the concept of end-to-end architectures is not new, architectural advancements and improved training methodologies are finally making this paradigm viable, resulting in better autonomous decision-making with less engineering complexity.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;3. Reasoning Models&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Reasoning vision language action (VLA) models integrate diverse perceptual inputs, language understanding and action generation with step-by-step reasoning. This enables them to break down complex situations, evaluate multiple possible outcomes and decide on the best course of action — much like humans do.&lt;/p&gt;
&lt;p&gt;Systems powered by reasoning models deliver far greater reliability and performance, with explainable, step-by-step decision-making. For autonomous vehicles, this means the ability to flag unusual decision patterns for real-time safety monitoring, as well as post-incident debugging to reveal why a vehicle took a particular action. This improves the performance of autonomous vehicles while building user trust.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;4. Simulation&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;With physical testing alone, it would take decades to test a driving policy in every possible driving scenario, if ever achievable at all. Enter simulation.&lt;/p&gt;
&lt;p&gt;Technologies like neural reconstruction can be used to create interactive simulations from real-world sensor data, while world models like NVIDIA Cosmos Predict and Transfer produce unlimited novel situations for training and testing autonomous vehicles.&lt;/p&gt;
&lt;p&gt;With these technologies, developers can use text prompts to generate new weather and road conditions, or change lighting and introduce obstacles to simulate new scenarios and test driving policies in novel conditions.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;5. Compute Power&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;None of these advances would be possible without sufficient computational power. The NVIDIA DRIVE AGX and NVIDIA DGX platforms have evolved through multiple generations, each designed for today’s AI workloads as well as those anticipated years down the road.&lt;/p&gt;
&lt;p&gt;Co-optimization matters. Technology must be designed anticipating the computational demands of next-generation AI systems.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;6. AI Safety&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Safety is foundational for level 4 autonomy, where reliability is the defining characteristic distinguishing it from lower autonomy levels. Recent advances in physical AI safety enable the trustworthy deployment of AI-based autonomy stacks by introducing safety guardrails at the stages of design, deployment and validation.&lt;/p&gt;
&lt;p&gt;For example, NVIDIA’s safety architecture guardrails the end-to-end driving model with checks supported by a diverse modular stack, and validation is greatly accelerated by the latest advancements in neural reconstruction.&lt;/p&gt;
&lt;p&gt;These and other guardrails are part of NVIDIA Halos, a comprehensive safety system that unifies the NVIDIA DRIVE architecture, the safety-certified NVIDIA DriveOS operating system, and AI models, hardware, software, tools and services to help ensure the safe development and deployment of autonomous vehicles from cloud to car. NVIDIA partners can adopt individual components or the full stack, depending on their needs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Why It Matters: Saving Lives and Resources&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The stakes extend far beyond technological achievement. Improving vehicle safety can help save lives and conserve significant amounts of money and resources. Level 4 autonomy systematically removes human error, the cause of the vast majority of crashes.&lt;/p&gt;
&lt;p&gt;NVIDIA, as a full-stack autonomous vehicle company — from cloud to car — is enabling the broader automotive ecosystem to achieve level 4 autonomy, building on the foundation of its level 2+ stack already in production. In particular, NVIDIA is the only company that offers an end-to-end compute stack for autonomous driving.&lt;/p&gt;
&lt;p&gt;Its three AI compute platforms critical for autonomy are:&lt;/p&gt;

&lt;p&gt;Together, these platforms form a feedback loop for learning, testing and deployment that tightens the cycle of innovation while keeping safety front and center.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;NVIDIA GTC Washington, D.C&lt;/i&gt;&lt;i&gt;., running Oct. 27-29, will feature a wide range of &lt;/i&gt;&lt;i&gt;sessions on autonomous vehicles and safety&lt;/i&gt;&lt;i&gt;, which will also be available on demand.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/</guid><pubDate>Mon, 20 Oct 2025 15:00:51 +0000</pubDate></item><item><title>Fold your own tessellation (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/20/1125594/technologyreview-com-tessellation/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;strong&gt;Download the pattern&lt;/strong&gt; for Dancing Ribbons here.&lt;/p&gt;  &lt;p&gt;Yoder recommends printing the pattern on paper in between normal printer paper and cardstock in weight, making sure it folds in straight lines (not too thick), folds back and forth easily on the same line (not too thin), and is crisp enough to make a satisfying snapping noise when you shake it. Her favorite paper isSkytone, which is commonly used to print certificates and fancy envelopes.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Watch the &lt;strong&gt;video tutorial&lt;/strong&gt; on folding Dancing Ribbons here.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Yoder’s detailed folding instructions:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Once you have your crease pattern on a sheet of paper, cut out the hexagon that contains the pattern. Yoder recommends using a straightedge and blade on a cutting mat instead of scissors, whether that means an X-Acto knife and a ruler on a sheet of cardboard or a quilting ruler and rotary cutter on a fabric cutting mat.&lt;/p&gt;  &lt;p&gt;The next step is folding the background grid of black lines that the pattern uses as references. Assuming you’ve cut out your hexagon precisely, you can use the edge of the hexagon and the printed lines to make your creases, or you can fold as if there were no lines printed by folding the hexagon in half (edge to opposite edge) and then folding those edges in to the center to make quarter lines, first in one direction and then in the other two. After each set of folds, it’s a good idea to fold the new lines back the other way to make the paper easier to work with later. After folding the quarters, fold the eighths in each direction, and finally the 16ths. Yoder presses the creases with a bone folder to make them easier to work with and to minimize stress on her hands.&lt;/p&gt; 
 &lt;p&gt;You can choose at this point whether to fold the pattern one twist at a time or to precrease the off-grid creases (just crease the short segments that have been printed, folded as mountains on the printed side of the pattern) and collapse everything all at once. Beginning folders may find it helpful to precrease the triangle and rhombus twists, to make the squashing process easier, even if you plan to fold the pattern one twist at a time. Solid red lines in the crease pattern represent mountain folds, and dashed blue lines represent valley folds. The faded lines inside the twists are helper folds used to set up the twists; they will not be used in the final pattern.&lt;/p&gt;  &lt;p&gt;The central closed hexagon twist will be the first twist folded, and it’ll be made on the blank side of the paper. All the mountain folds for this twist (as viewed on the blank side of the paper) will be on grid lines going to the corners of the hexagon, and the valley folds will be one grid spacing above the mountains on the right-hand side of the paper. To fold the twist, set up both the mountain and valley folds of one pleat; then pass that pleat counterclockwise into your other hand before setting up both folds of the next pleat. Keep all pleats folded and the center of the paper elevated as you work your way around the center, eventually folding all six pleats (use your table to keep the pleats folded, or use clips at the edge of the paper) and forming a hexagonal tower in the center of the paper. Make the pleats more flat, working from the edges in, until this hexagon tower is two grid spacings high. Then grab the tower and give it a sharp counterclockwise twist to get it to lie flat. This twist almost never lies down completely flat right away, so lift each pleat slightly to make sure the valley folds have stayed on grid lines to help the central hexagon to smooth out.&lt;/p&gt;  &lt;p&gt;Once the hexagon has been folded, flip the paper over to the printed side. Take the mountain fold of one pleat and split it into a three-way intersection of mountain folds evenly spaced around a point two grid spacings out from the closed hexagon hole. This point is the center of the closed triangle twist, which can be squashed to create the triangle of off-grid creases once the two new pleats are folded over in a clockwise direction (as printed). To squash the triangle twist, press gently on each of the three pleats just outside the point where the valley fold of one pleat contacts the mountain fold of the next pleat. This will start to flatten the central triangle, which can then be pressed from the top to smooth it out and finalize the new creases.&lt;/p&gt;  &lt;p&gt;Fold each of the triangle twists in the same way (causing pleats to overlap with pleats from other triangles), in a counterclockwise order around the central hexagon (this order makes the overlapping pleats easier to work with later).&lt;/p&gt;  &lt;p&gt;Once the triangles have all been folded, find a place where two pleats from triangle twists are overlapping and open up the overlap so you can see all the parts of the paper (leaving the triangle twists folded). Use the printed folds to set up a rhombus twist, and then press the twist flat from the top once all the folds in the pleats are set up.&lt;/p&gt;  &lt;p&gt;Repeat this step with all six of the pleat overlaps (if you followed the recommended sequence for the triangles, only one overlap will be in a different order from the rest) to complete the pattern.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;strong&gt;Download the pattern&lt;/strong&gt; for Dancing Ribbons here.&lt;/p&gt;  &lt;p&gt;Yoder recommends printing the pattern on paper in between normal printer paper and cardstock in weight, making sure it folds in straight lines (not too thick), folds back and forth easily on the same line (not too thin), and is crisp enough to make a satisfying snapping noise when you shake it. Her favorite paper isSkytone, which is commonly used to print certificates and fancy envelopes.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Watch the &lt;strong&gt;video tutorial&lt;/strong&gt; on folding Dancing Ribbons here.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Yoder’s detailed folding instructions:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Once you have your crease pattern on a sheet of paper, cut out the hexagon that contains the pattern. Yoder recommends using a straightedge and blade on a cutting mat instead of scissors, whether that means an X-Acto knife and a ruler on a sheet of cardboard or a quilting ruler and rotary cutter on a fabric cutting mat.&lt;/p&gt;  &lt;p&gt;The next step is folding the background grid of black lines that the pattern uses as references. Assuming you’ve cut out your hexagon precisely, you can use the edge of the hexagon and the printed lines to make your creases, or you can fold as if there were no lines printed by folding the hexagon in half (edge to opposite edge) and then folding those edges in to the center to make quarter lines, first in one direction and then in the other two. After each set of folds, it’s a good idea to fold the new lines back the other way to make the paper easier to work with later. After folding the quarters, fold the eighths in each direction, and finally the 16ths. Yoder presses the creases with a bone folder to make them easier to work with and to minimize stress on her hands.&lt;/p&gt; 
 &lt;p&gt;You can choose at this point whether to fold the pattern one twist at a time or to precrease the off-grid creases (just crease the short segments that have been printed, folded as mountains on the printed side of the pattern) and collapse everything all at once. Beginning folders may find it helpful to precrease the triangle and rhombus twists, to make the squashing process easier, even if you plan to fold the pattern one twist at a time. Solid red lines in the crease pattern represent mountain folds, and dashed blue lines represent valley folds. The faded lines inside the twists are helper folds used to set up the twists; they will not be used in the final pattern.&lt;/p&gt;  &lt;p&gt;The central closed hexagon twist will be the first twist folded, and it’ll be made on the blank side of the paper. All the mountain folds for this twist (as viewed on the blank side of the paper) will be on grid lines going to the corners of the hexagon, and the valley folds will be one grid spacing above the mountains on the right-hand side of the paper. To fold the twist, set up both the mountain and valley folds of one pleat; then pass that pleat counterclockwise into your other hand before setting up both folds of the next pleat. Keep all pleats folded and the center of the paper elevated as you work your way around the center, eventually folding all six pleats (use your table to keep the pleats folded, or use clips at the edge of the paper) and forming a hexagonal tower in the center of the paper. Make the pleats more flat, working from the edges in, until this hexagon tower is two grid spacings high. Then grab the tower and give it a sharp counterclockwise twist to get it to lie flat. This twist almost never lies down completely flat right away, so lift each pleat slightly to make sure the valley folds have stayed on grid lines to help the central hexagon to smooth out.&lt;/p&gt;  &lt;p&gt;Once the hexagon has been folded, flip the paper over to the printed side. Take the mountain fold of one pleat and split it into a three-way intersection of mountain folds evenly spaced around a point two grid spacings out from the closed hexagon hole. This point is the center of the closed triangle twist, which can be squashed to create the triangle of off-grid creases once the two new pleats are folded over in a clockwise direction (as printed). To squash the triangle twist, press gently on each of the three pleats just outside the point where the valley fold of one pleat contacts the mountain fold of the next pleat. This will start to flatten the central triangle, which can then be pressed from the top to smooth it out and finalize the new creases.&lt;/p&gt;  &lt;p&gt;Fold each of the triangle twists in the same way (causing pleats to overlap with pleats from other triangles), in a counterclockwise order around the central hexagon (this order makes the overlapping pleats easier to work with later).&lt;/p&gt;  &lt;p&gt;Once the triangles have all been folded, find a place where two pleats from triangle twists are overlapping and open up the overlap so you can see all the parts of the paper (leaving the triangle twists folded). Use the printed folds to set up a rhombus twist, and then press the twist flat from the top once all the folds in the pleats are set up.&lt;/p&gt;  &lt;p&gt;Repeat this step with all six of the pleat overlaps (if you followed the recommended sequence for the triangles, only one overlap will be in a different order from the rest) to complete the pattern.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/20/1125594/technologyreview-com-tessellation/</guid><pubDate>Mon, 20 Oct 2025 15:46:58 +0000</pubDate></item><item><title>NVIDIA and Google Cloud Accelerate Enterprise AI and Industrial Digitalization (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/partner-promo-google-cloudg4-blog-1920x1080-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA and Google Cloud are expanding access to accelerated computing to transform the full spectrum of enterprise workloads, from visual computing to agentic and physical AI.&lt;/p&gt;
&lt;p&gt;Google Cloud today announced the general availability of G4 VMs, powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. Plus, NVIDIA Omniverse and NVIDIA Isaac Sim are now available as virtual machine images (VMIs) on the Google Cloud Marketplace to unlock physical AI-driven applications for key industries like manufacturing, automotive and logistics.&lt;/p&gt;
&lt;p&gt;This powerful combination creates a versatile, multi-workload platform for enterprises to accelerate their most demanding challenges on Google Cloud.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX PRO 6000 Blackwell GPUs excel at high-performance AI inference for multimodal, generative and agentic AI deployments, while also powering complex visual and simulation workloads ranging from computer-aided engineering and content creation to robotics simulation.&lt;/p&gt;
&lt;p&gt;Customers like WPP are using G4 VMs with NVIDIA Omniverse to instantly generate photorealistic 3D advertising environments at global scale, while Altair is using the platform within Altair One to accelerate demanding simulation and fluid dynamics workloads.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The G4 VM: A Universal Platform for AI and Visual Computing&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At the core of the new G4 VM is the NVIDIA RTX PRO 6000 Blackwell Server Edition GPU, the ultimate data center GPU for AI and visual computing.&lt;/p&gt;
&lt;p&gt;Built on the NVIDIA Blackwell architecture, it serves as a universal platform for a broad range of workloads. Its design uniquely combines two powerful engines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Fifth-Generation Tensor Cores&lt;/b&gt; that deliver a massive leap in AI performance, supporting new data formats like FP4 to enable faster performance with lower memory usage.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fourth-Generation RT Cores&lt;/b&gt; that provide over 2x the real-time ray-tracing performance over the previous generation, enabling cinematic-quality graphics and photorealistic simulations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On Google Cloud, these G4 VMs are built for massive scale, configurable with up to eight RTX PRO 6000 GPUs — totaling 768 GB of GDDR7 memory — and supported by high-throughput local and network storage.&lt;/p&gt;
&lt;p&gt;As part of Google Cloud’s AI Hypercomputer architecture, G4 VMs natively integrate with services like Google Kubernetes Engine and Vertex AI to simplify containerized deployments and streamline machine learning operations for AI workloads. This flexibility extends to accelerating large-scale data analytics on Apache Spark and Hadoop with Dataproc.&lt;/p&gt;
&lt;p&gt;For design and simulation, the VMs also support a broad ecosystem of popular third-party engineering and graphics applications like Autodesk AutoCAD, Blender and Dassault SolidWorks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Industrial Digitalization at Scale With NVIDIA Omniverse&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Google Cloud customers can now tap into NVIDIA Omniverse, a collection of integration-ready libraries and frameworks for developing industrial digitalization applications built on Universal Scene Description (OpenUSD).&lt;/p&gt;
&lt;p&gt;With the availability of the Omniverse VMI and G4 VMs on Google Cloud, enterprises can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Build and Operate Digital Twins&lt;/b&gt;: Easily create physically accurate, real-time virtual replicas of factories and products to simulate and optimize operations. These workflows are powered by the NVIDIA Cosmos world foundation model platform and NVIDIA Omniverse Blueprints for creating digital twins.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Accelerate Robotics Development&lt;/b&gt;: Use NVIDIA Isaac Sim, a reference application built on Omniverse, to train, simulate and validate AI-driven robots in physics-based virtual environments before deployment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA AI Accelerates Every Enterprise Workload&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In addition to Omniverse, Google Cloud customers can use the full NVIDIA software stack to accelerate a range of high-demand workloads, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Agentic AI&lt;/b&gt;: Developers can use the NVIDIA Nemotron family of open reasoning models and NVIDIA Blueprints to get started quickly with building sophisticated AI agents that can reason and act. For deployment, NVIDIA NIM — a set of easy-to-use microservices — provides optimized, high-performance inference for AI models with enterprise-grade security and support.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Scientific and High-Performance Computing&lt;/b&gt;: Solve complex problems in fields like drug discovery and genomics using NVIDIA CUDA-X libraries and microservices. On the RTX PRO 6000 Blackwell GPU, core genomics algorithms used in sequence alignment can see up to 6.8x faster throughput compared with the previous generation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Design and Visual Computing&lt;/b&gt;: Power remote creative and design pipelines with NVIDIA RTX Virtual Workstation software, which delivers high-performance virtual workstation instances from G4 VMs to any device, anywhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These latest announcements establish a complete, end-to-end platform built on the NVIDIA Blackwell platform — from NVIDIA GB200 NVL72 (A4X VMs) and NVIDIA HGX B200 (A4 VMs) for massive-scale AI training and inference, to the RTX PRO 6000 Blackwell for AI inference and visual computing on G4 VMs.&lt;/p&gt;
&lt;p&gt;This unified architecture provides a seamless experience for accelerating every workload, empowering enterprises to tackle complex, multistage pipelines from data analytics to physical AI within a single, consistent cloud ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Learn more about G4 VMs on Google Cloud, and deploy the NVIDIA Omniverse and Isaac Sim VMIs from the Google Cloud Marketplace. Explore NVIDIA Nemotron models and NVIDIA Blueprints&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/partner-promo-google-cloudg4-blog-1920x1080-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA and Google Cloud are expanding access to accelerated computing to transform the full spectrum of enterprise workloads, from visual computing to agentic and physical AI.&lt;/p&gt;
&lt;p&gt;Google Cloud today announced the general availability of G4 VMs, powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. Plus, NVIDIA Omniverse and NVIDIA Isaac Sim are now available as virtual machine images (VMIs) on the Google Cloud Marketplace to unlock physical AI-driven applications for key industries like manufacturing, automotive and logistics.&lt;/p&gt;
&lt;p&gt;This powerful combination creates a versatile, multi-workload platform for enterprises to accelerate their most demanding challenges on Google Cloud.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX PRO 6000 Blackwell GPUs excel at high-performance AI inference for multimodal, generative and agentic AI deployments, while also powering complex visual and simulation workloads ranging from computer-aided engineering and content creation to robotics simulation.&lt;/p&gt;
&lt;p&gt;Customers like WPP are using G4 VMs with NVIDIA Omniverse to instantly generate photorealistic 3D advertising environments at global scale, while Altair is using the platform within Altair One to accelerate demanding simulation and fluid dynamics workloads.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The G4 VM: A Universal Platform for AI and Visual Computing&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At the core of the new G4 VM is the NVIDIA RTX PRO 6000 Blackwell Server Edition GPU, the ultimate data center GPU for AI and visual computing.&lt;/p&gt;
&lt;p&gt;Built on the NVIDIA Blackwell architecture, it serves as a universal platform for a broad range of workloads. Its design uniquely combines two powerful engines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Fifth-Generation Tensor Cores&lt;/b&gt; that deliver a massive leap in AI performance, supporting new data formats like FP4 to enable faster performance with lower memory usage.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fourth-Generation RT Cores&lt;/b&gt; that provide over 2x the real-time ray-tracing performance over the previous generation, enabling cinematic-quality graphics and photorealistic simulations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On Google Cloud, these G4 VMs are built for massive scale, configurable with up to eight RTX PRO 6000 GPUs — totaling 768 GB of GDDR7 memory — and supported by high-throughput local and network storage.&lt;/p&gt;
&lt;p&gt;As part of Google Cloud’s AI Hypercomputer architecture, G4 VMs natively integrate with services like Google Kubernetes Engine and Vertex AI to simplify containerized deployments and streamline machine learning operations for AI workloads. This flexibility extends to accelerating large-scale data analytics on Apache Spark and Hadoop with Dataproc.&lt;/p&gt;
&lt;p&gt;For design and simulation, the VMs also support a broad ecosystem of popular third-party engineering and graphics applications like Autodesk AutoCAD, Blender and Dassault SolidWorks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Industrial Digitalization at Scale With NVIDIA Omniverse&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Google Cloud customers can now tap into NVIDIA Omniverse, a collection of integration-ready libraries and frameworks for developing industrial digitalization applications built on Universal Scene Description (OpenUSD).&lt;/p&gt;
&lt;p&gt;With the availability of the Omniverse VMI and G4 VMs on Google Cloud, enterprises can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Build and Operate Digital Twins&lt;/b&gt;: Easily create physically accurate, real-time virtual replicas of factories and products to simulate and optimize operations. These workflows are powered by the NVIDIA Cosmos world foundation model platform and NVIDIA Omniverse Blueprints for creating digital twins.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Accelerate Robotics Development&lt;/b&gt;: Use NVIDIA Isaac Sim, a reference application built on Omniverse, to train, simulate and validate AI-driven robots in physics-based virtual environments before deployment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA AI Accelerates Every Enterprise Workload&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In addition to Omniverse, Google Cloud customers can use the full NVIDIA software stack to accelerate a range of high-demand workloads, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Agentic AI&lt;/b&gt;: Developers can use the NVIDIA Nemotron family of open reasoning models and NVIDIA Blueprints to get started quickly with building sophisticated AI agents that can reason and act. For deployment, NVIDIA NIM — a set of easy-to-use microservices — provides optimized, high-performance inference for AI models with enterprise-grade security and support.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Scientific and High-Performance Computing&lt;/b&gt;: Solve complex problems in fields like drug discovery and genomics using NVIDIA CUDA-X libraries and microservices. On the RTX PRO 6000 Blackwell GPU, core genomics algorithms used in sequence alignment can see up to 6.8x faster throughput compared with the previous generation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Design and Visual Computing&lt;/b&gt;: Power remote creative and design pipelines with NVIDIA RTX Virtual Workstation software, which delivers high-performance virtual workstation instances from G4 VMs to any device, anywhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These latest announcements establish a complete, end-to-end platform built on the NVIDIA Blackwell platform — from NVIDIA GB200 NVL72 (A4X VMs) and NVIDIA HGX B200 (A4 VMs) for massive-scale AI training and inference, to the RTX PRO 6000 Blackwell for AI inference and visual computing on G4 VMs.&lt;/p&gt;
&lt;p&gt;This unified architecture provides a seamless experience for accelerating every workload, empowering enterprises to tackle complex, multistage pipelines from data analytics to physical AI within a single, consistent cloud ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Learn more about G4 VMs on Google Cloud, and deploy the NVIDIA Omniverse and Isaac Sim VMIs from the Google Cloud Marketplace. Explore NVIDIA Nemotron models and NVIDIA Blueprints&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/</guid><pubDate>Mon, 20 Oct 2025 16:00:53 +0000</pubDate></item><item><title>FTC removes Lina Khan-era posts about AI risks and open source (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/ftc-removes-lina-khan-era-posts-about-ai-risks-and-open-source/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/06/GettyImages-1232440387.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Federal Trade Commission has removed three blog posts from the Lina Khan era that addressed open source AI and risks of AI to consumers, according to a Wired report. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One post, titled “On Open-Weights Foundation Models,” was published July 10, 2024. Another, titled “Consumers Are Voicing Concerns About AI,” came out in October 2023. A third, authored by Khan’s staff, was published on January 3, 2025, with the title “AI and the Risk of Consumer Harm.” That post noted the FTC was “taking note of AI’s potential for real-world instances of harm — from incentivizing commercial surveillance to enabling fraud and impersonation to perpetuating illegal discrimination.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to the FTC to learn why the posts were taken down. Khan declined to comment. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These removals are part of a broader pattern under the Trump administration, which began issuing executive orders to direct federal agencies to remove or modify substantial amounts of government content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After his inauguration, Trump also installed a new head of the FTC and removed several FTC commissioners, installing leadership that focused less on Khan’s aggressive antitrust agenda and more on deregulation for Big Tech.&amp;nbsp;In September, new FTC Chair Andrew Ferguson submitted recommendations for deleting or revising anticompetitive regulations across the entire federal government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The blog posts most recently removed by the FTC, which focused on consumer harm, don’t seem to align with the Trump administration’s AI Action Plan. That plan has reduced its focus on safety and guardrails, instead favoring fast growth and competition with China. However, the Trump administration has been vocal about backing open source initiatives. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Former FTC public affairs director Douglas Farrar told TechCrunch: “I was shocked to see that Andrew Ferguson led FTC be so out of line with the Trump White House on this signal to the market.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This is not the first time this administration’s FTC has removed content. In March, Wired reported that the FTC removed around 300 posts related to AI, consumer protection, and the agency’s lawsuits against tech companies like Amazon and Microsoft. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While hundreds of blog posts from Khan’s tenure and earlier remain on the agency’s Office of Technology Blog, Ferguson’s FTC has yet to publish any posts to the site, despite the feverish pace of the AI race, which has resulted in several business mergers and acquisitions — including acqui-hires — that could be seen as anticompetitive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The FTC blog culling follows the Trump administration’s removal or modification of thousands of government web pages and datasets, particularly content related to diversity, equity, and inclusion; gender identity; public health; and environmental policy. For example, the Centers for Disease Control and Prevention has removed data on topics ranging from chronic medical conditions to HIV/AIDS. The Justice Department has removed studies on hate crimes, and the National Oceanic and Atmospheric Administration has taken down the congressionally mandated National Climate Assessment reports. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The removal of content — including the blog posts from the FTC — could violate the Federal Records Act, which requires federal agencies to preserve records that properly document government activities, and the Open Government Data Act, which requires agencies to publish their data as “open data” by default.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Biden administration’s FTC leadership placed warning labels on content published during previous administrations that it disagreed with, according to Wired.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/06/GettyImages-1232440387.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Federal Trade Commission has removed three blog posts from the Lina Khan era that addressed open source AI and risks of AI to consumers, according to a Wired report. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One post, titled “On Open-Weights Foundation Models,” was published July 10, 2024. Another, titled “Consumers Are Voicing Concerns About AI,” came out in October 2023. A third, authored by Khan’s staff, was published on January 3, 2025, with the title “AI and the Risk of Consumer Harm.” That post noted the FTC was “taking note of AI’s potential for real-world instances of harm — from incentivizing commercial surveillance to enabling fraud and impersonation to perpetuating illegal discrimination.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to the FTC to learn why the posts were taken down. Khan declined to comment. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These removals are part of a broader pattern under the Trump administration, which began issuing executive orders to direct federal agencies to remove or modify substantial amounts of government content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After his inauguration, Trump also installed a new head of the FTC and removed several FTC commissioners, installing leadership that focused less on Khan’s aggressive antitrust agenda and more on deregulation for Big Tech.&amp;nbsp;In September, new FTC Chair Andrew Ferguson submitted recommendations for deleting or revising anticompetitive regulations across the entire federal government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The blog posts most recently removed by the FTC, which focused on consumer harm, don’t seem to align with the Trump administration’s AI Action Plan. That plan has reduced its focus on safety and guardrails, instead favoring fast growth and competition with China. However, the Trump administration has been vocal about backing open source initiatives. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Former FTC public affairs director Douglas Farrar told TechCrunch: “I was shocked to see that Andrew Ferguson led FTC be so out of line with the Trump White House on this signal to the market.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This is not the first time this administration’s FTC has removed content. In March, Wired reported that the FTC removed around 300 posts related to AI, consumer protection, and the agency’s lawsuits against tech companies like Amazon and Microsoft. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While hundreds of blog posts from Khan’s tenure and earlier remain on the agency’s Office of Technology Blog, Ferguson’s FTC has yet to publish any posts to the site, despite the feverish pace of the AI race, which has resulted in several business mergers and acquisitions — including acqui-hires — that could be seen as anticompetitive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The FTC blog culling follows the Trump administration’s removal or modification of thousands of government web pages and datasets, particularly content related to diversity, equity, and inclusion; gender identity; public health; and environmental policy. For example, the Centers for Disease Control and Prevention has removed data on topics ranging from chronic medical conditions to HIV/AIDS. The Justice Department has removed studies on hate crimes, and the National Oceanic and Atmospheric Administration has taken down the congressionally mandated National Climate Assessment reports. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The removal of content — including the blog posts from the FTC — could violate the Federal Records Act, which requires federal agencies to preserve records that properly document government activities, and the Open Government Data Act, which requires agencies to publish their data as “open data” by default.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Biden administration’s FTC leadership placed warning labels on content published during previous administrations that it disagreed with, according to Wired.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/ftc-removes-lina-khan-era-posts-about-ai-risks-and-open-source/</guid><pubDate>Mon, 20 Oct 2025 16:49:58 +0000</pubDate></item><item><title>Open Source AI Week — How Developers and Contributors Are Advancing AI Innovation (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/open-source-ai-week/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;

&lt;p&gt;Computer scientist Andrej Karpathy recently introduced Nanochat, calling it “the best ChatGPT that $100 can buy.” Nanochat is an open-source, full-stack large language model (LLM) implementation built for transparency and experimentation. In about 8,000 lines of minimal, dependency-light code, Nanochat runs the entire LLM pipeline — from tokenization and pretraining to fine-tuning, inference and chat — all through a simple web user interface.&lt;/p&gt;
&lt;p&gt;NVIDIA is supporting Karpathy’s open-source Nanochat project by releasing two NVIDIA Launchables, making it easy to deploy and experiment with Nanochat across various NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;With NVIDIA Launchables, developers can train and interact with their own conversational model in hours with a single click. The Launchables dynamically support different-sized GPUs — including NVIDIA H100 and L40S GPUs — on various clouds without need for modification. They also automatically work on any eight-GPU instance on NVIDIA Brev, so developers can get compute access immediately.&lt;/p&gt;
&lt;p&gt;The &lt;b&gt;first 10 users&lt;/b&gt; to deploy these Launchables will also receive free compute access to NVIDIA H100 or L40S GPUs.&lt;/p&gt;
&lt;p&gt;Start training with Nanochat by deploying a Launchable:&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86022 size-medium" height="527" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/launchable-960x527.png" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Andrej Karpathy’s Next Experiments Begin With NVIDIA DGX Spark&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today, Karpathy received an NVIDIA DGX Spark — the world’s smallest AI supercomputer, designed to bring the power of Blackwell right to a developer’s desktop. With up to a petaflop of AI processing power and 128GB of unified memory in a compact form factor, DGX Spark empowers innovators like Karpathy to experiment, fine-tune and run massive models locally.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;


&lt;p&gt;PyTorch, the fastest-growing AI framework, derives its performance from the NVIDIA CUDA platform and uses the Python programming language to unlock developer productivity. This year, NVIDIA added Python as a first-class language to the CUDA platform, giving the PyTorch developer community greater access to CUDA.&lt;/p&gt;
&lt;p&gt;CUDA Python includes key components that make GPU acceleration in Python easier than ever, with built-in support for kernel fusion, extension module integration and simplified packaging for fast deployment.&lt;/p&gt;
&lt;p&gt;Following PyTorch’s open collaboration model, CUDA Python is available on GitHub and PyPI.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86034"&gt;&lt;img alt="alt" class="wp-image-86034 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/pytorch-infographic-1-1680x672.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86034"&gt;According to PyPI Stats, PyTorch averaged over two million daily downloads, peaking at 2,303,217 on October 14, and had 65 million total downloads last month.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Every month, developers worldwide download hundreds of millions of NVIDIA libraries — including CUDA, cuDNN, cuBLAS and CUTLASS — mostly within Python and PyTorch environments. CUDA Python provides nvmath-python, a new library that acts as the bridge between Python code and these highly optimized GPU libraries.&lt;/p&gt;
&lt;p&gt;Plus, kernel enhancements and support for next-generation frameworks make NVIDIA accelerated computing more efficient, adaptable and widely accessible.&lt;/p&gt;
&lt;p&gt;NVIDIA maintains a long-standing collaboration with the PyTorch community through open-source contributions and technical leadership, as well as by sponsoring and participating in community events and activations.&lt;/p&gt;
&lt;p&gt;At PyTorch Conference 2025 in San Francisco, NVIDIA will host a keynote address, five technical sessions and nine poster presentations.&lt;/p&gt;
&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;

&lt;p&gt;Open Source AI Week kicks off on Monday with a series of hackathons, workshops and meetups spotlighting the latest advances in AI, machine learning and open-source innovation.&lt;/p&gt;
&lt;p&gt;The event brings together leading organizations, researchers and open-source communities to share knowledge, collaborate on tools and explore how openness accelerates AI development.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to expand access to advanced AI innovation by providing open-source tools, models and datasets designed to empower developers. With more than 1,000 open-source tools on NVIDIA GitHub repositories and over 500 models and 100 datasets on the NVIDIA Hugging Face collections, NVIDIA is accelerating the pace of open, collaborative AI development.&lt;/p&gt;
&lt;p&gt;Over the past year, NVIDIA has become the top contributor in Hugging Face repositories, reflecting a deep commitment to sharing models, frameworks and research that empower the community.&lt;/p&gt;

&lt;p&gt;Openly available models, tools and datasets are essential to driving innovation and progress. By empowering anyone to use, modify and share technology, it fosters transparency and accelerates discovery, fueling breakthroughs that benefit both industry and communities alike. That’s why NVIDIA is committed to supporting the open source ecosystem.&lt;/p&gt;
&lt;p&gt;We’re on the ground all week — stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward, with the PyTorch Conference serving as the flagship event.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;

&lt;p&gt;Computer scientist Andrej Karpathy recently introduced Nanochat, calling it “the best ChatGPT that $100 can buy.” Nanochat is an open-source, full-stack large language model (LLM) implementation built for transparency and experimentation. In about 8,000 lines of minimal, dependency-light code, Nanochat runs the entire LLM pipeline — from tokenization and pretraining to fine-tuning, inference and chat — all through a simple web user interface.&lt;/p&gt;
&lt;p&gt;NVIDIA is supporting Karpathy’s open-source Nanochat project by releasing two NVIDIA Launchables, making it easy to deploy and experiment with Nanochat across various NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;With NVIDIA Launchables, developers can train and interact with their own conversational model in hours with a single click. The Launchables dynamically support different-sized GPUs — including NVIDIA H100 and L40S GPUs — on various clouds without need for modification. They also automatically work on any eight-GPU instance on NVIDIA Brev, so developers can get compute access immediately.&lt;/p&gt;
&lt;p&gt;The &lt;b&gt;first 10 users&lt;/b&gt; to deploy these Launchables will also receive free compute access to NVIDIA H100 or L40S GPUs.&lt;/p&gt;
&lt;p&gt;Start training with Nanochat by deploying a Launchable:&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86022 size-medium" height="527" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/launchable-960x527.png" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Andrej Karpathy’s Next Experiments Begin With NVIDIA DGX Spark&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today, Karpathy received an NVIDIA DGX Spark — the world’s smallest AI supercomputer, designed to bring the power of Blackwell right to a developer’s desktop. With up to a petaflop of AI processing power and 128GB of unified memory in a compact form factor, DGX Spark empowers innovators like Karpathy to experiment, fine-tune and run massive models locally.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;


&lt;p&gt;PyTorch, the fastest-growing AI framework, derives its performance from the NVIDIA CUDA platform and uses the Python programming language to unlock developer productivity. This year, NVIDIA added Python as a first-class language to the CUDA platform, giving the PyTorch developer community greater access to CUDA.&lt;/p&gt;
&lt;p&gt;CUDA Python includes key components that make GPU acceleration in Python easier than ever, with built-in support for kernel fusion, extension module integration and simplified packaging for fast deployment.&lt;/p&gt;
&lt;p&gt;Following PyTorch’s open collaboration model, CUDA Python is available on GitHub and PyPI.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86034"&gt;&lt;img alt="alt" class="wp-image-86034 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/pytorch-infographic-1-1680x672.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86034"&gt;According to PyPI Stats, PyTorch averaged over two million daily downloads, peaking at 2,303,217 on October 14, and had 65 million total downloads last month.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Every month, developers worldwide download hundreds of millions of NVIDIA libraries — including CUDA, cuDNN, cuBLAS and CUTLASS — mostly within Python and PyTorch environments. CUDA Python provides nvmath-python, a new library that acts as the bridge between Python code and these highly optimized GPU libraries.&lt;/p&gt;
&lt;p&gt;Plus, kernel enhancements and support for next-generation frameworks make NVIDIA accelerated computing more efficient, adaptable and widely accessible.&lt;/p&gt;
&lt;p&gt;NVIDIA maintains a long-standing collaboration with the PyTorch community through open-source contributions and technical leadership, as well as by sponsoring and participating in community events and activations.&lt;/p&gt;
&lt;p&gt;At PyTorch Conference 2025 in San Francisco, NVIDIA will host a keynote address, five technical sessions and nine poster presentations.&lt;/p&gt;
&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;

&lt;p&gt;Open Source AI Week kicks off on Monday with a series of hackathons, workshops and meetups spotlighting the latest advances in AI, machine learning and open-source innovation.&lt;/p&gt;
&lt;p&gt;The event brings together leading organizations, researchers and open-source communities to share knowledge, collaborate on tools and explore how openness accelerates AI development.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to expand access to advanced AI innovation by providing open-source tools, models and datasets designed to empower developers. With more than 1,000 open-source tools on NVIDIA GitHub repositories and over 500 models and 100 datasets on the NVIDIA Hugging Face collections, NVIDIA is accelerating the pace of open, collaborative AI development.&lt;/p&gt;
&lt;p&gt;Over the past year, NVIDIA has become the top contributor in Hugging Face repositories, reflecting a deep commitment to sharing models, frameworks and research that empower the community.&lt;/p&gt;

&lt;p&gt;Openly available models, tools and datasets are essential to driving innovation and progress. By empowering anyone to use, modify and share technology, it fosters transparency and accelerates discovery, fueling breakthroughs that benefit both industry and communities alike. That’s why NVIDIA is committed to supporting the open source ecosystem.&lt;/p&gt;
&lt;p&gt;We’re on the ground all week — stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward, with the PyTorch Conference serving as the flagship event.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/open-source-ai-week/</guid><pubDate>Mon, 20 Oct 2025 17:29:33 +0000</pubDate></item><item><title>Anthropic brings Claude Code to the web (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/anthropic-brings-claude-code-to-the-web/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/YouTube-Thumb-Text-2-3.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic launched a web app on Monday for its viral AI coding assistant, Claude Code, which lets developers create and manage several AI coding agents from their browser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Claude Code for web is now rolling out to subscribers to Anthropic’s $20-per-month Pro plan, as well as its $100- and $200-per-month Max plans. Pro and Max users can access Claude Code on the web by navigating to claude.ai (the same website for Anthropic’s consumer chatbot) and clicking into the “Code” tab, or through the Claude iOS app.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch marks Anthropic’s latest attempt to evolve Claude Code beyond a command-line interface (CLI) tool that developers access from a terminal. By putting Claude Code on the web, Anthropic hopes developers will spin up AI coding agents in more places.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s increasingly competitive for tech companies trying to make their AI coding tools stand out. While Microsoft’s GitHub Copilot once dominated the space, Cursor, Google, OpenAI, and Anthropic now have highly performant AI coding tools of their own — many of them already available on the web. That said, Claude Code is arguably one of the most popular. Anthropic’s flagship coding tool has grown 10x in users since its broader launch in May, and the product now accounts for more than $500 million of the company’s revenue on an annualized basis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic product manager Cat Wu tells TechCrunch in an interview that she attributes a large part of Claude Code’s success to the company’s AI models, which have become a favorite among developers in recent years. However, Wu also says the Claude Code team deliberately tries to “sprinkle in some fun” to the product wherever they can.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wu said that Anthropic will continue to put Claude Code in more places, but the terminal will likely remain the home base for their AI coding product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we look forward, one of our key focuses is making sure the CLI product is the most intelligent and customizable way for you to use coding agents,” said Wu. “But we’re continuing to put Claude Code everywhere, helping it meet developers wherever they are. Web and mobile are a big step in this direction.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic claims that 90% of the Claude Code product itself is written by the company’s AI models. Wu, who was previously an engineer, says that she rarely ever sits down at a keyboard to write code anymore, and mostly just reviews Claude Code’s outputs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early AI coding tools worked like an autocomplete tool, finishing lines of code as developers wrote them. But the agentic generation of AI coding tools — including Claude Code — allow developers to spin up agents that work autonomously. This shift has made millions of software engineers act more like managers of AI coding assistants in their day-to-day jobs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change has not been welcome to every developer. One recent study found that some engineers were actually slower when using AI coding tools like Cursor. Researchers suggested one factor could be that engineers in the study spent much of their time prompting and waiting for AI tools to finish, rather than working on other problems. AI coding tools also struggle in large, complex code bases, so engineers may have spent a lot of time working through incorrect responses from the AI model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nevertheless, companies like Anthropic are continuing to push ahead making AI coding agents. Anthropic CEO Dario Amodei predicted a few months ago that AI should soon write 90% of code for software engineers. While that may be true inside of Anthropic, the shift may taken longer to pan out in the broader economy.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/YouTube-Thumb-Text-2-3.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic launched a web app on Monday for its viral AI coding assistant, Claude Code, which lets developers create and manage several AI coding agents from their browser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Claude Code for web is now rolling out to subscribers to Anthropic’s $20-per-month Pro plan, as well as its $100- and $200-per-month Max plans. Pro and Max users can access Claude Code on the web by navigating to claude.ai (the same website for Anthropic’s consumer chatbot) and clicking into the “Code” tab, or through the Claude iOS app.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch marks Anthropic’s latest attempt to evolve Claude Code beyond a command-line interface (CLI) tool that developers access from a terminal. By putting Claude Code on the web, Anthropic hopes developers will spin up AI coding agents in more places.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s increasingly competitive for tech companies trying to make their AI coding tools stand out. While Microsoft’s GitHub Copilot once dominated the space, Cursor, Google, OpenAI, and Anthropic now have highly performant AI coding tools of their own — many of them already available on the web. That said, Claude Code is arguably one of the most popular. Anthropic’s flagship coding tool has grown 10x in users since its broader launch in May, and the product now accounts for more than $500 million of the company’s revenue on an annualized basis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic product manager Cat Wu tells TechCrunch in an interview that she attributes a large part of Claude Code’s success to the company’s AI models, which have become a favorite among developers in recent years. However, Wu also says the Claude Code team deliberately tries to “sprinkle in some fun” to the product wherever they can.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wu said that Anthropic will continue to put Claude Code in more places, but the terminal will likely remain the home base for their AI coding product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we look forward, one of our key focuses is making sure the CLI product is the most intelligent and customizable way for you to use coding agents,” said Wu. “But we’re continuing to put Claude Code everywhere, helping it meet developers wherever they are. Web and mobile are a big step in this direction.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic claims that 90% of the Claude Code product itself is written by the company’s AI models. Wu, who was previously an engineer, says that she rarely ever sits down at a keyboard to write code anymore, and mostly just reviews Claude Code’s outputs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early AI coding tools worked like an autocomplete tool, finishing lines of code as developers wrote them. But the agentic generation of AI coding tools — including Claude Code — allow developers to spin up agents that work autonomously. This shift has made millions of software engineers act more like managers of AI coding assistants in their day-to-day jobs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change has not been welcome to every developer. One recent study found that some engineers were actually slower when using AI coding tools like Cursor. Researchers suggested one factor could be that engineers in the study spent much of their time prompting and waiting for AI tools to finish, rather than working on other problems. AI coding tools also struggle in large, complex code bases, so engineers may have spent a lot of time working through incorrect responses from the AI model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nevertheless, companies like Anthropic are continuing to push ahead making AI coding agents. Anthropic CEO Dario Amodei predicted a few months ago that AI should soon write 90% of code for software engineers. While that may be true inside of Anthropic, the shift may taken longer to pan out in the broader economy.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/anthropic-brings-claude-code-to-the-web/</guid><pubDate>Mon, 20 Oct 2025 18:00:00 +0000</pubDate></item><item><title>[NEW] Claude Code comes to web and mobile, letting devs launch parallel jobs on Anthropic’s managed infra (AI | VentureBeat)</title><link>https://venturebeat.com/ai/claude-code-comes-to-web-and-mobile-letting-devs-launch-parallel-jobs-on</link><description>[unable to retrieve full-text content]&lt;p&gt;Vibe coding &lt;a href="https://venturebeat.com/ai/vibe-coding-is-dead-agentic-swarm-coding-is-the-new-enterprise-moat"&gt;&lt;u&gt;is evolving&lt;/u&gt;&lt;/a&gt; and with it are the leading AI-powered coding services and tools, including &lt;a href="https://www.anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt;’s Claude Code. &lt;/p&gt;&lt;p&gt;As of today, the service will be available via the web and, in preview, on the Claude iOS app, giving developers access to additional asynchronous capabilities. Previously, it was available through the terminal on developers&amp;#x27; PCs with support for Git, Docker, Kubernetes, npm, pip, AWS CLI, etc., and as an extension for Microsoft&amp;#x27;s open source &lt;a href="https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code"&gt;VS Code editor&lt;/a&gt; and other JetBrains-powered integrated development environments (IDEs) via &lt;a href="https://blog.jetbrains.com/ai/2025/09/introducing-claude-agent-in-jetbrains-ides/"&gt;Claude Agent&lt;/a&gt;.   &lt;/p&gt;&lt;p&gt;“Claude Code on the web lets you kick off coding sessions without opening your terminal,” Anthropic said in a &lt;a href="https://www.anthropic.com/news/claude-code-on-the-web"&gt;&lt;u&gt;blog post&lt;/u&gt;&lt;/a&gt;. “Connect your GitHub repositories, describe what you need, and Claude handles the implementation. Each session runs in its own isolated environment with real-time progress tracking, and you can actively steer Claude to adjust course as it’s working through tasks.”&lt;/p&gt;&lt;p&gt;This allows users to run coding projects asynchronously, a trend that many enterprises are looking for. &lt;/p&gt;&lt;p&gt;The web version of Claude Code, currently in research preview, will be available to Pro and Max users. However, web Claude Code will be subject to the same rate limits as other versions. Anthropic &lt;a href="https://venturebeat.com/ai/anthropic-throttles-claude-rate-limits-devs-call-foul"&gt;&lt;u&gt;throttled rate limits&lt;/u&gt;&lt;/a&gt; to Claude and Claude Code after the unexpected popularity of the coding tool in July, which enabled some users to run Claude Code overnight. &lt;/p&gt;&lt;p&gt;Anthropic is now ensuring Claude Code comes closer to matching the availability of rival OpenAI&amp;#x27;s Codex AI coding platform, powered by a variant of GPT-5, which launches on mobile and the web back in &lt;a href="https://x.com/OpenAI/status/1967636903165038708"&gt;mid September 2025.&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Parallel usage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Anthropic said running Claude Code in the cloud means teams can “now run multiple tasks in parallel across different repositories from a single interface and ship faster with automatic PR creation and clear change summaries.”&lt;/p&gt;&lt;p&gt;One of the big draws of coding agents is giving developers the ability to run multiple coding projects, such as bugfixes, at the same time. &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;’s &lt;a href="https://venturebeat.com/ai/googles-jules-aims-to-out-code-codex-in-battle-for-the-ai-developer-stack"&gt;&lt;u&gt;two coding agents&lt;/u&gt;&lt;/a&gt;, Jules and Code Assist, both offer asynchronous code generation and checks. &lt;a href="https://venturebeat.com/programming-development/openai-launches-research-preview-of-codex-ai-software-engineering-agent-for-developers-with-parallel-tasking"&gt;&lt;u&gt;Codex&lt;/u&gt;&lt;/a&gt; from &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; also lets people work in parallel. &lt;/p&gt;&lt;p&gt;Anthropic said bringing Claude Code to the web won’t disrupt workflows, but noted running tasks in the cloud work best for tasks such as answering questions around projects and how repositories are mapped, bugfixes and for routine, well-defined tasks, and backend changes to verify any adjustments. &lt;/p&gt;&lt;p&gt;While most developers will likely prefer to use Claude Code on a desktop, Anthropic said the mobile version could encourage more users to “explore coding with Claude on the go.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Isolated environments &lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Anthropic insisted that Claude Code tasks on the cloud will have the same level of security as the earlier version. It runs on an “isolated sandbox environment with network and filesystem restrictions.” &lt;/p&gt;&lt;p&gt;Interactions go through a secure proxy service, which the company said ensures the model only accesses authorized repositories.&lt;/p&gt;&lt;p&gt;Enterprise users can customize which domains Claude Code can connect to. &lt;/p&gt;&lt;p&gt;Claude Code is powered by Claude Sonnet 4.5, which Anthropic claims is the best coding model around. The company recently made Claude Haiku 4.5, a smaller version of Claude that also has strong coding capabilities, &lt;a href="https://venturebeat.com/ai/anthropic-is-giving-away-its-powerful-claude-haiku-4-5-ai-for-free-to-take"&gt;&lt;u&gt;available to all Claude subscribers&lt;/u&gt;&lt;/a&gt;, including free users. &lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Vibe coding &lt;a href="https://venturebeat.com/ai/vibe-coding-is-dead-agentic-swarm-coding-is-the-new-enterprise-moat"&gt;&lt;u&gt;is evolving&lt;/u&gt;&lt;/a&gt; and with it are the leading AI-powered coding services and tools, including &lt;a href="https://www.anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt;’s Claude Code. &lt;/p&gt;&lt;p&gt;As of today, the service will be available via the web and, in preview, on the Claude iOS app, giving developers access to additional asynchronous capabilities. Previously, it was available through the terminal on developers&amp;#x27; PCs with support for Git, Docker, Kubernetes, npm, pip, AWS CLI, etc., and as an extension for Microsoft&amp;#x27;s open source &lt;a href="https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code"&gt;VS Code editor&lt;/a&gt; and other JetBrains-powered integrated development environments (IDEs) via &lt;a href="https://blog.jetbrains.com/ai/2025/09/introducing-claude-agent-in-jetbrains-ides/"&gt;Claude Agent&lt;/a&gt;.   &lt;/p&gt;&lt;p&gt;“Claude Code on the web lets you kick off coding sessions without opening your terminal,” Anthropic said in a &lt;a href="https://www.anthropic.com/news/claude-code-on-the-web"&gt;&lt;u&gt;blog post&lt;/u&gt;&lt;/a&gt;. “Connect your GitHub repositories, describe what you need, and Claude handles the implementation. Each session runs in its own isolated environment with real-time progress tracking, and you can actively steer Claude to adjust course as it’s working through tasks.”&lt;/p&gt;&lt;p&gt;This allows users to run coding projects asynchronously, a trend that many enterprises are looking for. &lt;/p&gt;&lt;p&gt;The web version of Claude Code, currently in research preview, will be available to Pro and Max users. However, web Claude Code will be subject to the same rate limits as other versions. Anthropic &lt;a href="https://venturebeat.com/ai/anthropic-throttles-claude-rate-limits-devs-call-foul"&gt;&lt;u&gt;throttled rate limits&lt;/u&gt;&lt;/a&gt; to Claude and Claude Code after the unexpected popularity of the coding tool in July, which enabled some users to run Claude Code overnight. &lt;/p&gt;&lt;p&gt;Anthropic is now ensuring Claude Code comes closer to matching the availability of rival OpenAI&amp;#x27;s Codex AI coding platform, powered by a variant of GPT-5, which launches on mobile and the web back in &lt;a href="https://x.com/OpenAI/status/1967636903165038708"&gt;mid September 2025.&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Parallel usage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Anthropic said running Claude Code in the cloud means teams can “now run multiple tasks in parallel across different repositories from a single interface and ship faster with automatic PR creation and clear change summaries.”&lt;/p&gt;&lt;p&gt;One of the big draws of coding agents is giving developers the ability to run multiple coding projects, such as bugfixes, at the same time. &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;’s &lt;a href="https://venturebeat.com/ai/googles-jules-aims-to-out-code-codex-in-battle-for-the-ai-developer-stack"&gt;&lt;u&gt;two coding agents&lt;/u&gt;&lt;/a&gt;, Jules and Code Assist, both offer asynchronous code generation and checks. &lt;a href="https://venturebeat.com/programming-development/openai-launches-research-preview-of-codex-ai-software-engineering-agent-for-developers-with-parallel-tasking"&gt;&lt;u&gt;Codex&lt;/u&gt;&lt;/a&gt; from &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; also lets people work in parallel. &lt;/p&gt;&lt;p&gt;Anthropic said bringing Claude Code to the web won’t disrupt workflows, but noted running tasks in the cloud work best for tasks such as answering questions around projects and how repositories are mapped, bugfixes and for routine, well-defined tasks, and backend changes to verify any adjustments. &lt;/p&gt;&lt;p&gt;While most developers will likely prefer to use Claude Code on a desktop, Anthropic said the mobile version could encourage more users to “explore coding with Claude on the go.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Isolated environments &lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Anthropic insisted that Claude Code tasks on the cloud will have the same level of security as the earlier version. It runs on an “isolated sandbox environment with network and filesystem restrictions.” &lt;/p&gt;&lt;p&gt;Interactions go through a secure proxy service, which the company said ensures the model only accesses authorized repositories.&lt;/p&gt;&lt;p&gt;Enterprise users can customize which domains Claude Code can connect to. &lt;/p&gt;&lt;p&gt;Claude Code is powered by Claude Sonnet 4.5, which Anthropic claims is the best coding model around. The company recently made Claude Haiku 4.5, a smaller version of Claude that also has strong coding capabilities, &lt;a href="https://venturebeat.com/ai/anthropic-is-giving-away-its-powerful-claude-haiku-4-5-ai-for-free-to-take"&gt;&lt;u&gt;available to all Claude subscribers&lt;/u&gt;&lt;/a&gt;, including free users. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/claude-code-comes-to-web-and-mobile-letting-devs-launch-parallel-jobs-on</guid><pubDate>Mon, 20 Oct 2025 18:15:00 +0000</pubDate></item><item><title>Meta AI’s app downloads and daily users spiked after launch of ‘Vibes’ AI video feed (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/meta-ais-app-downloads-and-daily-users-spiked-after-launch-of-vibes-ai-video-feed/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;New data indicates that use of Meta AI’s mobile app for iOS and Android has seen a significant increase. According to a new analysis from market intelligence provider Similarweb, the app’s daily active users across both platforms jumped to 2.7 million as of October 17, up from around 775,000 just four weeks ago. In addition, Meta AI’s app installs are also up, reaching 300,000 new downloads per day, compared with under 200,000 daily downloads a few weeks ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For comparison, Meta AI’s app had just 4,000 daily downloads a year ago, on October 17, 2024. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3059538" height="384" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.29PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The firm says it hasn’t seen any meaningful correlation in either search or advertising estimates, but notes Meta could be running Facebook or Instagram promotions that wouldn’t be captured in its model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, there’s also another possible explanation for the sharp rise: the launch of Meta’s new Vibes feed in September, which introduced short-form AI-generated videos to the Meta AI mobile app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta AI introduced the Vibes feed on September 25, which correlates with the sharp increase in the app’s daily active users on iOS and Android, as seen in the chart below. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3059534" height="518" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.45PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Recently, OpenAI’s video generator Sora drew headlines as its app reached the top of the App Store when users rushed to try the new technology. However, Meta AI could have benefited from this launch as well. While Similarweb says its data doesn’t prove cause and effect, it’s possible that the attention to Sora drove some people to try Meta AI, in order to compare the two experiences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another possibility is that Meta could be benefiting from Sora’s invite-only status. That is, those who couldn’t try out the OpenAI app may have looked for an alternative to experiment with. This would be an interesting explanation, too, as it suggests that OpenAI’s decision to gatekeep Sora may have directly boosted its rivals.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Meta AI's Vibes feed, which showcases AI videos, may have driven a spike in app downloads and usage, " class="wp-image-3059536" height="537" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.36PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As of October 17, Meta AI’s app had seen a 15.58% increase in daily active users worldwide, while ChatGPT, Grok, and Perplexity saw declines of 3.51%, 7.35%, and 2.29%, respectively. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;New data indicates that use of Meta AI’s mobile app for iOS and Android has seen a significant increase. According to a new analysis from market intelligence provider Similarweb, the app’s daily active users across both platforms jumped to 2.7 million as of October 17, up from around 775,000 just four weeks ago. In addition, Meta AI’s app installs are also up, reaching 300,000 new downloads per day, compared with under 200,000 daily downloads a few weeks ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For comparison, Meta AI’s app had just 4,000 daily downloads a year ago, on October 17, 2024. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3059538" height="384" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.29PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The firm says it hasn’t seen any meaningful correlation in either search or advertising estimates, but notes Meta could be running Facebook or Instagram promotions that wouldn’t be captured in its model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, there’s also another possible explanation for the sharp rise: the launch of Meta’s new Vibes feed in September, which introduced short-form AI-generated videos to the Meta AI mobile app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta AI introduced the Vibes feed on September 25, which correlates with the sharp increase in the app’s daily active users on iOS and Android, as seen in the chart below. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3059534" height="518" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.45PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Recently, OpenAI’s video generator Sora drew headlines as its app reached the top of the App Store when users rushed to try the new technology. However, Meta AI could have benefited from this launch as well. While Similarweb says its data doesn’t prove cause and effect, it’s possible that the attention to Sora drove some people to try Meta AI, in order to compare the two experiences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another possibility is that Meta could be benefiting from Sora’s invite-only status. That is, those who couldn’t try out the OpenAI app may have looked for an alternative to experiment with. This would be an interesting explanation, too, as it suggests that OpenAI’s decision to gatekeep Sora may have directly boosted its rivals.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Meta AI's Vibes feed, which showcases AI videos, may have driven a spike in app downloads and usage, " class="wp-image-3059536" height="537" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.36PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As of October 17, Meta AI’s app had seen a 15.58% increase in daily active users worldwide, while ChatGPT, Grok, and Perplexity saw declines of 3.51%, 7.35%, and 2.29%, respectively. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/meta-ais-app-downloads-and-daily-users-spiked-after-launch-of-vibes-ai-video-feed/</guid><pubDate>Mon, 20 Oct 2025 18:22:38 +0000</pubDate></item><item><title>[NEW] Claude Code gets a web version—but it’s the new sandboxing that really matters (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/claude-code-gets-a-web-version-but-its-the-new-sandboxing-that-really-matters/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Sandboxing lessens hassle, but fire-and-forget agentic tools still pose risks.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A graphic of a world in brackets" class="absolute inset-0 w-full h-full object-cover hidden" height="355" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-3.36.24-PM-640x355.png" width="640" /&gt;
                  &lt;img alt="A graphic of a world in brackets" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="588" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-3.36.24-PM.png" width="1061" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The vague marketing image that accompanied Anthropic's announcement.

          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Anthropic has added web and mobile interfaces for Claude Code, its immensely popular command-line interface (CLI) agentic AI coding tool.&lt;/p&gt;
&lt;p&gt;The web interface appears to be well-baked at launch, but the mobile version is limited to iOS and is in an earlier stage of development.&lt;/p&gt;
&lt;p&gt;The web version of Claude Code can be given access to a GitHub repository. Once that’s done, developers can give it general marching orders like “add real-time inventory tracking to the dashboard.” As with the CLI version, it gets to work, with updates along the way approximating where it’s at and what it’s doing. The web interface supports the recently implemented Claude Code capability to take suggestions or requested changes while it’s in the middle of working on a task. (Previously, if you saw it doing something wrong or missing something, you often had to cancel and start over.)&lt;/p&gt;
&lt;p&gt;Developers can run multiple sessions at once and switch between them as needed; they’re listed in a left-side panel in the interface.&lt;/p&gt;
&lt;p&gt;Alongside this web and mobile rollout, Anthropic has also introduced a new sandboxing runtime to Claude Code that, along with other things, aims to make the experience both more secure and lower friction.&lt;/p&gt;
&lt;p&gt;In the past, Claude Code worked by asking permission before making most changes and steps along the way.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Now, it can instead be given permissions for specific file system folders and network servers. That means fewer approval steps, but it’s also more secure overall against prompt injection and other risks.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic’s demo video for Claude Code on the web.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;According to Anthropic’s engineering blog, the new network isolation approach only allows Internet access “through a unix domain socket connected to a proxy server running outside the sandbox. … This proxy server enforces restrictions on the domains that a process can connect to, and handles user confirmation for newly requested domains.” Additionally, users can customize the proxy to set their own rules for outgoing traffic.&lt;/p&gt;
&lt;p&gt;This way, the coding agent can do things like fetch npm packages from approved sources, but without carte blanche for communicating with the outside world, and without badgering the user with constant approvals.&lt;/p&gt;
&lt;p&gt;For many developers, these additions are more significant than the availability of web or mobile interfaces. They allow Claude Code agents to operate more independently without as many detailed, line-by-line approvals.&lt;/p&gt;
&lt;p&gt;That’s more convenient, but it’s a double-edged sword, as it will also make code review even more important. One of the strengths of the too-many-approvals approach was that it made sure developers were still looking closely at every little change. Now it might be a little bit easier to miss Claude Code making a bad call.&lt;/p&gt;
&lt;p&gt;The new features are available in beta now as a research preview, and they are available to Claude users with Pro or Max subscriptions.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Sandboxing lessens hassle, but fire-and-forget agentic tools still pose risks.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A graphic of a world in brackets" class="absolute inset-0 w-full h-full object-cover hidden" height="355" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-3.36.24-PM-640x355.png" width="640" /&gt;
                  &lt;img alt="A graphic of a world in brackets" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="588" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-3.36.24-PM.png" width="1061" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The vague marketing image that accompanied Anthropic's announcement.

          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Anthropic has added web and mobile interfaces for Claude Code, its immensely popular command-line interface (CLI) agentic AI coding tool.&lt;/p&gt;
&lt;p&gt;The web interface appears to be well-baked at launch, but the mobile version is limited to iOS and is in an earlier stage of development.&lt;/p&gt;
&lt;p&gt;The web version of Claude Code can be given access to a GitHub repository. Once that’s done, developers can give it general marching orders like “add real-time inventory tracking to the dashboard.” As with the CLI version, it gets to work, with updates along the way approximating where it’s at and what it’s doing. The web interface supports the recently implemented Claude Code capability to take suggestions or requested changes while it’s in the middle of working on a task. (Previously, if you saw it doing something wrong or missing something, you often had to cancel and start over.)&lt;/p&gt;
&lt;p&gt;Developers can run multiple sessions at once and switch between them as needed; they’re listed in a left-side panel in the interface.&lt;/p&gt;
&lt;p&gt;Alongside this web and mobile rollout, Anthropic has also introduced a new sandboxing runtime to Claude Code that, along with other things, aims to make the experience both more secure and lower friction.&lt;/p&gt;
&lt;p&gt;In the past, Claude Code worked by asking permission before making most changes and steps along the way.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Now, it can instead be given permissions for specific file system folders and network servers. That means fewer approval steps, but it’s also more secure overall against prompt injection and other risks.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic’s demo video for Claude Code on the web.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;According to Anthropic’s engineering blog, the new network isolation approach only allows Internet access “through a unix domain socket connected to a proxy server running outside the sandbox. … This proxy server enforces restrictions on the domains that a process can connect to, and handles user confirmation for newly requested domains.” Additionally, users can customize the proxy to set their own rules for outgoing traffic.&lt;/p&gt;
&lt;p&gt;This way, the coding agent can do things like fetch npm packages from approved sources, but without carte blanche for communicating with the outside world, and without badgering the user with constant approvals.&lt;/p&gt;
&lt;p&gt;For many developers, these additions are more significant than the availability of web or mobile interfaces. They allow Claude Code agents to operate more independently without as many detailed, line-by-line approvals.&lt;/p&gt;
&lt;p&gt;That’s more convenient, but it’s a double-edged sword, as it will also make code review even more important. One of the strengths of the too-many-approvals approach was that it made sure developers were still looking closely at every little change. Now it might be a little bit easier to miss Claude Code making a bad call.&lt;/p&gt;
&lt;p&gt;The new features are available in beta now as a research preview, and they are available to Claude users with Pro or Max subscriptions.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/claude-code-gets-a-web-version-but-its-the-new-sandboxing-that-really-matters/</guid><pubDate>Mon, 20 Oct 2025 20:45:26 +0000</pubDate></item><item><title>[NEW] Top OpenAI, Google Brain researchers set off a $300M VC frenzy for their startup Periodic Labs (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/top-openai-google-brain-researchers-set-off-a-300m-vc-frenzy-for-their-startup-periodic-labs/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Periodic-Labs.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Periodic Labs, a new startup by one of OpenAI’s most respected researchers, Liam Fedus, and his former Google Brain colleague, Ekin Dogus Cubuk, came out of stealth last month with an enormous $300 million seed round. It was led by  Felicis and included a who’s who of angels and other top VCs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup began when Fedus had a conversation with&amp;nbsp;Cubuk (whose friends call him “Doge”) about seven months ago. Cubuk was one of Google Brain’s foremost machine learning and material science researchers.&amp;nbsp;After endless Silicon Valley takes on how generative AI would radically change scientific discovery, they decided that the pieces were finally in place to make this a reality. Or at least to found a startup that attempted it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There are a few things that happened in the LLM field, in experimental science and in simulations that kind of made this the right time,” Cubuk told TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For one, he said, robotic arms that could handle powder synthesis — the process of mixing and creating new materials — had recently proved themselves reliable.&amp;nbsp;For another, machine learning simulations had become efficient and accurate enough to model complex physical systems such as those needed to develop new materials.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And, third, LLMs now had powerful reasoning capabilities&amp;nbsp;— in part through the work of Fedus and his team at OpenAI.&amp;nbsp;Fedus was one of the small team that created ChatGPT to begin with and was running OpenAI’s uber-important post-training team, which refines models after their initial development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stitched together, the picture was clear: A simulation could theoretically discover new compounds, a robot could mix the materials, and an LLM could analyze the results and suggest course corrections. AI-automated material science was ready to be built.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In fact, Cubuk was one of the researchers who published a groundbreaking paper in 2023 documenting a precursor Google research project. The team built a fully automated, robotic-powered lab and created 41 novel compounds from recipes suggested by language models.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Equally importantly, the founders realized that even failed experiments would be valuable for their new startup because data is the lifeblood of AI. AI science offered an entirely new source for real-world training and post-training data. This&amp;nbsp;could, the founders believe, turn the existing scientific motivation system on its head, which seeks success, not exploration, rewarded via paper publication and grants.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Making contact with reality, bringing experiments into the [AI] loop — we feel like this is the next frontier,” Fedus told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-felicis-wins-the-deal-openai-does-not-invest-nbsp"&gt;Felicis wins the deal; OpenAI does not invest&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;After that discussion with Cubuk, Fedus went to the powers that be at OpenAI to share his resignation and his plan. He then gleefully tweeted to the world that he was leaving with what appeared to be OpenAI’s blessing and investment.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;This is what I sent to my colleagues at OpenAI:&lt;/p&gt;&lt;p&gt;Hi all, I made the difficult decision to leave OpenAI as an employee, but I’m looking to work closely together as a partner going forward. Contributing to the mission of OpenAI and working with world-class teams to create and…&lt;/p&gt;— William Fedus (@LiamFedus) March 17, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That investment didn’t actually materialize, however. OpenAI is not a backer of Periodic, the founders confirmed to TechCrunch. And while Fedus declined to say why, they actually didn’t need OpenAI’s money.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Fedus’ tweet set off a frenzy of VCs courting the company. “There was almost like a feeling of being reverse pitched. One investor actually wrote a love letter to Periodic Labs,” Fedus laughed, explaining that neither he nor Cubuk “knew what to make of it.” Others sent multi-page documents pitching themselves.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the first call that they actually took was from Peter Deng, a former OpenAI colleague turned investor for top-tier seed firm Felicis. (Deng left OpenAI for Felicis at the start of 2025.)&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Liam is a very big deal within OpenAI, very well loved and an extremely impactful researcher,” Deng told TechCrunch. “When I heard he left, I texted him immediately.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deng met Fedus for coffee in the Noe Valley neighborhood of San Francisco. Hyped on caffeine and enthusiasm, Fedus invited Deng to finish their conversation on a walk over the area’s famously hilly terrain. Pitch walks may be a Silicon Valley trope, but they also really happen.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The chilly day had turned hot. Deng, wearing a sweater, sweated and scrambled to keep up with the fit and friendly Fedus until the founder said something that “literally stopped me in my tracks,” Deng told TechCrunch. He told Deng that “everyone talks about doing science, but in order to do science, you actually have to do science,” Deng recalls.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In other words, they needed to give AI a fully equipped wet lab to try its ideas in a real-world, controlled setting.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The truth about these models is that everything that the models know is within normal distribution. We take a bunch of data, and it can just regurgitate what it knows,” Deng said. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Discovering something new has to involve testing hypotheses.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“And I committed on&amp;nbsp;the spot, in the middle of the hills of Noe Valley, to write the check,” Deng says.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fedus also remembers&amp;nbsp;the moment&amp;nbsp;Deng asked how he could be involved, and Fedus told him the startup needed cash for laptops and a temporary office. And “he’s like, great, I’ll give you money right now. And it was just this huge vote of confidence.” &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Deng didn’t actually whip out his checkbook on the street. He went back to the office elated over the deal only to encounter Felicis’ lawyer, who pointed out that the firm couldn’t promptly sign a contract: The company wasn’t incorporated yet. It didn’t even have a name, much less a bank account to wire funds. “That’s how early we were.” Deng grinned.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Soon they had all of those things and all the term sheets they could handle. With the $300 million war chest, Cubuk and Fedus hired over two dozen of the most prestigious AI and scientific talent like Alexandre Passos (a creator of o1 and o3); Eric Toberer (a materials scientist who has already made key superconductor discoveries); and Matt Horton, a creator of two of Microsoft’s GenAI materials science tools. And the list goes on.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because the team members are all experts in different areas, from AI to physics, each week one of them gives a grad-level lecture to the others. “We do feel like a tight coupling is extremely important,” Cubuk said. He wants everyone to understand all parts of what they are building.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Periodic Labs has already set up its lab, too, and is working with experimental data, simulations and testing some predictions. The main initial mission is to find new superconductor materials — potentially a gold mine discovery. Improved superconductors could power the next era of potent, but lower energy-consuming tech.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the last part — the robots — are not yet up and running. “They will take a bit to train,”&amp;nbsp;Cubuk&amp;nbsp;said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All of this is, of course, a big swing for the fences. AI powered or not, scientific discovery is not typically fast, easy, or predictable. While this team of experts has some indications that they will find what they are looking for — or make other discoveries along the way (or simply generate valuable data on their failures), there’s no guarantees.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And we know that model makers themselves are inching their way toward more AI science. Last month, OpenAI VP Kevin Weil said he was launching an OpenAI for Science unit at the company to “build the next great scientific instrument: an AI-powered platform that accelerates scientific discovery.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the investor who wrote the love letter, he didn’t win the deal (although Fedus did say that the letter was “very flattering.”) The other seed investors include&amp;nbsp;Andreessen Horowitz, DST, Nvidia’s venture capital arm NVentures, Accel, and angel backers like Jeff Bezos, Elad Gil, Eric Schmidt, and Jeff Dean.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Elad Gil will be speaking about how AI has changed the startup landscape at Disrupt in San Francisco on October 29.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Periodic-Labs.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Periodic Labs, a new startup by one of OpenAI’s most respected researchers, Liam Fedus, and his former Google Brain colleague, Ekin Dogus Cubuk, came out of stealth last month with an enormous $300 million seed round. It was led by  Felicis and included a who’s who of angels and other top VCs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup began when Fedus had a conversation with&amp;nbsp;Cubuk (whose friends call him “Doge”) about seven months ago. Cubuk was one of Google Brain’s foremost machine learning and material science researchers.&amp;nbsp;After endless Silicon Valley takes on how generative AI would radically change scientific discovery, they decided that the pieces were finally in place to make this a reality. Or at least to found a startup that attempted it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There are a few things that happened in the LLM field, in experimental science and in simulations that kind of made this the right time,” Cubuk told TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For one, he said, robotic arms that could handle powder synthesis — the process of mixing and creating new materials — had recently proved themselves reliable.&amp;nbsp;For another, machine learning simulations had become efficient and accurate enough to model complex physical systems such as those needed to develop new materials.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And, third, LLMs now had powerful reasoning capabilities&amp;nbsp;— in part through the work of Fedus and his team at OpenAI.&amp;nbsp;Fedus was one of the small team that created ChatGPT to begin with and was running OpenAI’s uber-important post-training team, which refines models after their initial development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stitched together, the picture was clear: A simulation could theoretically discover new compounds, a robot could mix the materials, and an LLM could analyze the results and suggest course corrections. AI-automated material science was ready to be built.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In fact, Cubuk was one of the researchers who published a groundbreaking paper in 2023 documenting a precursor Google research project. The team built a fully automated, robotic-powered lab and created 41 novel compounds from recipes suggested by language models.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Equally importantly, the founders realized that even failed experiments would be valuable for their new startup because data is the lifeblood of AI. AI science offered an entirely new source for real-world training and post-training data. This&amp;nbsp;could, the founders believe, turn the existing scientific motivation system on its head, which seeks success, not exploration, rewarded via paper publication and grants.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Making contact with reality, bringing experiments into the [AI] loop — we feel like this is the next frontier,” Fedus told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-felicis-wins-the-deal-openai-does-not-invest-nbsp"&gt;Felicis wins the deal; OpenAI does not invest&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;After that discussion with Cubuk, Fedus went to the powers that be at OpenAI to share his resignation and his plan. He then gleefully tweeted to the world that he was leaving with what appeared to be OpenAI’s blessing and investment.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;This is what I sent to my colleagues at OpenAI:&lt;/p&gt;&lt;p&gt;Hi all, I made the difficult decision to leave OpenAI as an employee, but I’m looking to work closely together as a partner going forward. Contributing to the mission of OpenAI and working with world-class teams to create and…&lt;/p&gt;— William Fedus (@LiamFedus) March 17, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That investment didn’t actually materialize, however. OpenAI is not a backer of Periodic, the founders confirmed to TechCrunch. And while Fedus declined to say why, they actually didn’t need OpenAI’s money.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Fedus’ tweet set off a frenzy of VCs courting the company. “There was almost like a feeling of being reverse pitched. One investor actually wrote a love letter to Periodic Labs,” Fedus laughed, explaining that neither he nor Cubuk “knew what to make of it.” Others sent multi-page documents pitching themselves.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the first call that they actually took was from Peter Deng, a former OpenAI colleague turned investor for top-tier seed firm Felicis. (Deng left OpenAI for Felicis at the start of 2025.)&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Liam is a very big deal within OpenAI, very well loved and an extremely impactful researcher,” Deng told TechCrunch. “When I heard he left, I texted him immediately.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deng met Fedus for coffee in the Noe Valley neighborhood of San Francisco. Hyped on caffeine and enthusiasm, Fedus invited Deng to finish their conversation on a walk over the area’s famously hilly terrain. Pitch walks may be a Silicon Valley trope, but they also really happen.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The chilly day had turned hot. Deng, wearing a sweater, sweated and scrambled to keep up with the fit and friendly Fedus until the founder said something that “literally stopped me in my tracks,” Deng told TechCrunch. He told Deng that “everyone talks about doing science, but in order to do science, you actually have to do science,” Deng recalls.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In other words, they needed to give AI a fully equipped wet lab to try its ideas in a real-world, controlled setting.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The truth about these models is that everything that the models know is within normal distribution. We take a bunch of data, and it can just regurgitate what it knows,” Deng said. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Discovering something new has to involve testing hypotheses.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“And I committed on&amp;nbsp;the spot, in the middle of the hills of Noe Valley, to write the check,” Deng says.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fedus also remembers&amp;nbsp;the moment&amp;nbsp;Deng asked how he could be involved, and Fedus told him the startup needed cash for laptops and a temporary office. And “he’s like, great, I’ll give you money right now. And it was just this huge vote of confidence.” &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Deng didn’t actually whip out his checkbook on the street. He went back to the office elated over the deal only to encounter Felicis’ lawyer, who pointed out that the firm couldn’t promptly sign a contract: The company wasn’t incorporated yet. It didn’t even have a name, much less a bank account to wire funds. “That’s how early we were.” Deng grinned.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Soon they had all of those things and all the term sheets they could handle. With the $300 million war chest, Cubuk and Fedus hired over two dozen of the most prestigious AI and scientific talent like Alexandre Passos (a creator of o1 and o3); Eric Toberer (a materials scientist who has already made key superconductor discoveries); and Matt Horton, a creator of two of Microsoft’s GenAI materials science tools. And the list goes on.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because the team members are all experts in different areas, from AI to physics, each week one of them gives a grad-level lecture to the others. “We do feel like a tight coupling is extremely important,” Cubuk said. He wants everyone to understand all parts of what they are building.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Periodic Labs has already set up its lab, too, and is working with experimental data, simulations and testing some predictions. The main initial mission is to find new superconductor materials — potentially a gold mine discovery. Improved superconductors could power the next era of potent, but lower energy-consuming tech.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the last part — the robots — are not yet up and running. “They will take a bit to train,”&amp;nbsp;Cubuk&amp;nbsp;said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All of this is, of course, a big swing for the fences. AI powered or not, scientific discovery is not typically fast, easy, or predictable. While this team of experts has some indications that they will find what they are looking for — or make other discoveries along the way (or simply generate valuable data on their failures), there’s no guarantees.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And we know that model makers themselves are inching their way toward more AI science. Last month, OpenAI VP Kevin Weil said he was launching an OpenAI for Science unit at the company to “build the next great scientific instrument: an AI-powered platform that accelerates scientific discovery.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the investor who wrote the love letter, he didn’t win the deal (although Fedus did say that the letter was “very flattering.”) The other seed investors include&amp;nbsp;Andreessen Horowitz, DST, Nvidia’s venture capital arm NVentures, Accel, and angel backers like Jeff Bezos, Elad Gil, Eric Schmidt, and Jeff Dean.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Elad Gil will be speaking about how AI has changed the startup landscape at Disrupt in San Francisco on October 29.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/top-openai-google-brain-researchers-set-off-a-300m-vc-frenzy-for-their-startup-periodic-labs/</guid><pubDate>Mon, 20 Oct 2025 21:15:24 +0000</pubDate></item><item><title>[NEW] A picture's worth a thousand (private) words: Hierarchical generation of coherent synthetic photo albums (The latest research from Google)</title><link>https://research.google/blog/a-pictures-worth-a-thousand-private-words-hierarchical-generation-of-coherent-synthetic-photo-albums/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Differential privacy (DP) provides a powerful, mathematically rigorous assurance that sensitive individual information in a dataset remains protected, even when a dataset is used for analysis. Since DP’s inception nearly two decades ago, researchers have developed differentially private versions of myriad data analysis and machine learning methods, ranging from calculating simple statistics to fine-tuning complex AI models. However, the requirement for organizations to privatize every analytical technique can be complex, burdensome, and error-prone.&lt;/p&gt;&lt;p&gt;Generative AI models like Gemini offer a simpler, more efficient solution. Instead of separately modifying every analysis method, they create a single private synthetic version of the original dataset. This synthetic data is an amalgamation of common data patterns, containing no unique details from any individual user. By using a differentially private training algorithm, such as DP-SGD, to fine-tune the generative model on the original dataset, we ensure the synthetic dataset is both private and highly representative of the real data. Any standard, non-private analytical technique or modeling can then be performed on this safe (and highly representative) substitute dataset, simplifying workflows. DP fine-tuning is a versatile tool that is particularly valuable for generating high-volume, controlled datasets in situations where access to high-quality, representative data is unavailable.&lt;/p&gt;&lt;p&gt;Most published work on private synthetic data generation has focused on simple outputs like short text passages or individual images, but modern applications using multi-modal data (images, video, etc.) rely on modeling complex, real-world systems and behaviors, which simple, unstructured text data cannot adequately capture.&lt;/p&gt;&lt;p&gt;We introduce a new method for privately generating synthetic photo albums as a way to address this need for synthetic versions of rich, structured image-based datasets. This task presents unique challenges beyond generating individual images, specifically the need to maintain thematic coherence and character consistency across multiple photos within a sequential album. Our method is based on translating complex image data to text and back. Our results show that this process, with rigorous DP guarantees enabled, successfully preserves the high-level semantic information and thematic coherence in datasets necessary for effective analysis and modeling applications.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Differential privacy (DP) provides a powerful, mathematically rigorous assurance that sensitive individual information in a dataset remains protected, even when a dataset is used for analysis. Since DP’s inception nearly two decades ago, researchers have developed differentially private versions of myriad data analysis and machine learning methods, ranging from calculating simple statistics to fine-tuning complex AI models. However, the requirement for organizations to privatize every analytical technique can be complex, burdensome, and error-prone.&lt;/p&gt;&lt;p&gt;Generative AI models like Gemini offer a simpler, more efficient solution. Instead of separately modifying every analysis method, they create a single private synthetic version of the original dataset. This synthetic data is an amalgamation of common data patterns, containing no unique details from any individual user. By using a differentially private training algorithm, such as DP-SGD, to fine-tune the generative model on the original dataset, we ensure the synthetic dataset is both private and highly representative of the real data. Any standard, non-private analytical technique or modeling can then be performed on this safe (and highly representative) substitute dataset, simplifying workflows. DP fine-tuning is a versatile tool that is particularly valuable for generating high-volume, controlled datasets in situations where access to high-quality, representative data is unavailable.&lt;/p&gt;&lt;p&gt;Most published work on private synthetic data generation has focused on simple outputs like short text passages or individual images, but modern applications using multi-modal data (images, video, etc.) rely on modeling complex, real-world systems and behaviors, which simple, unstructured text data cannot adequately capture.&lt;/p&gt;&lt;p&gt;We introduce a new method for privately generating synthetic photo albums as a way to address this need for synthetic versions of rich, structured image-based datasets. This task presents unique challenges beyond generating individual images, specifically the need to maintain thematic coherence and character consistency across multiple photos within a sequential album. Our method is based on translating complex image data to text and back. Our results show that this process, with rigorous DP guarantees enabled, successfully preserves the high-level semantic information and thematic coherence in datasets necessary for effective analysis and modeling applications.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/a-pictures-worth-a-thousand-private-words-hierarchical-generation-of-coherent-synthetic-photo-albums/</guid><pubDate>Mon, 20 Oct 2025 21:54:00 +0000</pubDate></item></channel></rss>