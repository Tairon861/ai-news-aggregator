<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 09 Sep 2025 18:27:01 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>AI is changing the grid. Could it help more than it harms? (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/09/1123404/ai-grid-help/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-2181763351.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;The rising popularity of AI is driving an increase in electricity demand so significant it has the potential to reshape our grid. Energy consumption by data centers has gone up by 80% from 2020 to 2025 and is likely to keep growing. Electricity prices are already rising, especially in places where data centers are most concentrated.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yet many people, especially in Big Tech, argue that AI will be, on balance, a positive force for the grid. They claim that the technology could help get more clean power online faster, run our power system more efficiently, and predict and prevent failures that cause blackouts.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is a part of&amp;nbsp;&lt;/em&gt;MIT Technology Review&lt;em&gt;’s series “&lt;/em&gt;Power Hungry: AI and our energy future&lt;em&gt;,” on the energy demands and carbon costs of the artificial-intelligence revolution.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;There are early examples where AI is helping already, including AI tools that utilities are using to help forecast supply and demand. The question is whether these big promises will be realized fast enough to outweigh the negative effects of AI on local grids and communities.&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A delicate balance&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;One area where AI is already being used for the grid is in forecasting, says Utkarsha Agwan, a member of the nonprofit group Climate Change AI.&lt;/p&gt;  &lt;p&gt;Running the grid is a balancing act: Operators have to understand how much electricity demand there is and turn on the right combination of power plants to meet it. They optimize for economics along the way, choosing the sources that will keep prices lowest for the whole system.&lt;/p&gt; 
 &lt;p&gt;That makes it necessary to look ahead hours and in some cases days. Operators consider factors such as historical data (holidays often see higher demand) and the weather (a hot day means more air conditioners sucking up power). These predictions also consider what level of supply is expected from intermittent sources like solar panels.&lt;/p&gt;  &lt;p&gt;There’s little risk in using AI tools in forecasting; it’s often not as time sensitive as other applications, which can require reactions within seconds or even milliseconds. A grid operator might use a forecast to determine which plants will need to turn on. Other groups might run their own forecasts as well, using AI tools to decide how to staff a plant, for example. The tools also can’t physically control anything. Rather, they can be used alongside more conventional methods to provide more data.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Today, grid operators make a lot of approximations to model the grid, because the system is so incredibly complex that it’s impossible to truly know what’s going on in every place at every time. Not only are there a whole host of power plants and consumers to think about, but there are considerations like making sure power lines don’t get overloaded.&lt;/p&gt;  &lt;p&gt;Working with those estimates can lead to some inefficiencies, says Kyri Baker, a professor at the University of Colorado Boulder. Operators tend to generate a bit more electricity than the system uses, for example. Using AI to create a better model could reduce some of those losses and allow operators to make decisions about how to control infrastructure in real time to reach a closer match of supply and demand.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;She gives the example of a trip to the airport. Imagine there’s a route you know will get you there in about 45 minutes. There might be another, more complicated route that &lt;em&gt;could&lt;/em&gt; save you some time in ideal conditions—but you’re not sure whether it’s better on any particular day. What the grid does now is the equivalent of taking the reliable route.&lt;/p&gt;  &lt;p&gt;“So that’s the gap that AI can help close. We can solve this more complex problem, fast enough and reliably enough that we can possibly use it and shave off emissions,” Baker says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In theory, AI could be used to operate the grid entirely without human intervention. But that work is largely still in the research phase. Grid operators are running some of the most critical infrastructure in this country, and the industry is hesitant to mess with something that’s already working, Baker says. If this sort of technology is ever used in grid operations, there will still be humans in the loop to help make decisions, at least when it’s first deployed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Planning ahead&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Another fertile area for AI is planning future updates to the grid. Building a power plant can take a very long time—the typical time from an initial request to commercial operation in the US is roughly four years. One reason for the lengthy wait is that new power plants have to demonstrate how they might affect the rest of the grid before they can connect.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;An interconnection study examines whether adding a new power plant of a particular type in a particular place would require upgrades to the grid to prevent problems. After regulators and utilities determine what upgrades might be needed, they estimate the cost, and the energy developer generally foots the bill.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Today, those studies can take months. They involve trying to understand an incredibly complicated system, and because they rely on estimates of other existing and proposed power plants, only a few can happen in an area at any given time. This has helped create the years-long interconnection queue, a long line of plants waiting for their turn to hook up to the grid in markets like the US and Europe. The vast majority of projects in the queue today are renewables, which means there’s clean power just waiting to come online.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI could help speed this process, producing these reports more quickly. The Midcontinent Independent System Operator, a grid operator that covers 15 states in the central US, is currently working with a company called Pearl Street to help automate these reports.&lt;/p&gt;  &lt;p&gt;AI won’t be a cure-all for grid planning; there are other steps to clearing the interconnection queue, including securing the necessary permits. But the technology could help move things along. “The sooner we can speed up interconnection, the better off we’ll be,” says Rob Gramlich, president of Grid Strategies, a consultancy specializing in transmission and power markets.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;There’s a growing list of other potential uses for AI on the grid and in electricity generation. The technology could monitor and plan ahead for failures in equipment ranging from power lines to gear boxes. Computer vision could help detect everything from wildfires to faulty lines. AI could also help balance supply and demand in virtual power plants, systems of distributed resources like EV chargers or smart water heaters.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While there are early examples of research and pilot programs for AI from grid planning to operation, some experts are skeptical that the technology will deliver at the level some are hoping for. “It’s not that AI has not had some kind of transformation on power systems,” Climate Change AI’s Agwan says. “It’s that the promise has always been bigger, and the hope has always been bigger.”&lt;/p&gt;  &lt;p&gt;Some places are already seeing higher electricity prices because of power needs from data centers. The situation is likely to get worse. Electricity demand from data centers is set to double by the end of the decade, reaching 945 terawatt-hours, roughly the annual demand from the entire country of Japan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The infrastructure growth needed to support AI load growth has outpaced the promises of the technology, “by quite a bit,” says Panayiotis Moutis, an assistant professor of electrical engineering at the City College of New York. Higher bills caused by the increasing energy needs of AI aren’t justified by existing ways of using the technology for the grid, he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“At the moment, I am very hesitant to lean on the side of AI being a silver bullet,” Moutis says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Correction: This story has been updated to correct Moutis's affiliation.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-2181763351.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;The rising popularity of AI is driving an increase in electricity demand so significant it has the potential to reshape our grid. Energy consumption by data centers has gone up by 80% from 2020 to 2025 and is likely to keep growing. Electricity prices are already rising, especially in places where data centers are most concentrated.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yet many people, especially in Big Tech, argue that AI will be, on balance, a positive force for the grid. They claim that the technology could help get more clean power online faster, run our power system more efficiently, and predict and prevent failures that cause blackouts.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is a part of&amp;nbsp;&lt;/em&gt;MIT Technology Review&lt;em&gt;’s series “&lt;/em&gt;Power Hungry: AI and our energy future&lt;em&gt;,” on the energy demands and carbon costs of the artificial-intelligence revolution.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;There are early examples where AI is helping already, including AI tools that utilities are using to help forecast supply and demand. The question is whether these big promises will be realized fast enough to outweigh the negative effects of AI on local grids and communities.&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A delicate balance&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;One area where AI is already being used for the grid is in forecasting, says Utkarsha Agwan, a member of the nonprofit group Climate Change AI.&lt;/p&gt;  &lt;p&gt;Running the grid is a balancing act: Operators have to understand how much electricity demand there is and turn on the right combination of power plants to meet it. They optimize for economics along the way, choosing the sources that will keep prices lowest for the whole system.&lt;/p&gt; 
 &lt;p&gt;That makes it necessary to look ahead hours and in some cases days. Operators consider factors such as historical data (holidays often see higher demand) and the weather (a hot day means more air conditioners sucking up power). These predictions also consider what level of supply is expected from intermittent sources like solar panels.&lt;/p&gt;  &lt;p&gt;There’s little risk in using AI tools in forecasting; it’s often not as time sensitive as other applications, which can require reactions within seconds or even milliseconds. A grid operator might use a forecast to determine which plants will need to turn on. Other groups might run their own forecasts as well, using AI tools to decide how to staff a plant, for example. The tools also can’t physically control anything. Rather, they can be used alongside more conventional methods to provide more data.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Today, grid operators make a lot of approximations to model the grid, because the system is so incredibly complex that it’s impossible to truly know what’s going on in every place at every time. Not only are there a whole host of power plants and consumers to think about, but there are considerations like making sure power lines don’t get overloaded.&lt;/p&gt;  &lt;p&gt;Working with those estimates can lead to some inefficiencies, says Kyri Baker, a professor at the University of Colorado Boulder. Operators tend to generate a bit more electricity than the system uses, for example. Using AI to create a better model could reduce some of those losses and allow operators to make decisions about how to control infrastructure in real time to reach a closer match of supply and demand.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;She gives the example of a trip to the airport. Imagine there’s a route you know will get you there in about 45 minutes. There might be another, more complicated route that &lt;em&gt;could&lt;/em&gt; save you some time in ideal conditions—but you’re not sure whether it’s better on any particular day. What the grid does now is the equivalent of taking the reliable route.&lt;/p&gt;  &lt;p&gt;“So that’s the gap that AI can help close. We can solve this more complex problem, fast enough and reliably enough that we can possibly use it and shave off emissions,” Baker says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In theory, AI could be used to operate the grid entirely without human intervention. But that work is largely still in the research phase. Grid operators are running some of the most critical infrastructure in this country, and the industry is hesitant to mess with something that’s already working, Baker says. If this sort of technology is ever used in grid operations, there will still be humans in the loop to help make decisions, at least when it’s first deployed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Planning ahead&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Another fertile area for AI is planning future updates to the grid. Building a power plant can take a very long time—the typical time from an initial request to commercial operation in the US is roughly four years. One reason for the lengthy wait is that new power plants have to demonstrate how they might affect the rest of the grid before they can connect.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;An interconnection study examines whether adding a new power plant of a particular type in a particular place would require upgrades to the grid to prevent problems. After regulators and utilities determine what upgrades might be needed, they estimate the cost, and the energy developer generally foots the bill.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Today, those studies can take months. They involve trying to understand an incredibly complicated system, and because they rely on estimates of other existing and proposed power plants, only a few can happen in an area at any given time. This has helped create the years-long interconnection queue, a long line of plants waiting for their turn to hook up to the grid in markets like the US and Europe. The vast majority of projects in the queue today are renewables, which means there’s clean power just waiting to come online.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI could help speed this process, producing these reports more quickly. The Midcontinent Independent System Operator, a grid operator that covers 15 states in the central US, is currently working with a company called Pearl Street to help automate these reports.&lt;/p&gt;  &lt;p&gt;AI won’t be a cure-all for grid planning; there are other steps to clearing the interconnection queue, including securing the necessary permits. But the technology could help move things along. “The sooner we can speed up interconnection, the better off we’ll be,” says Rob Gramlich, president of Grid Strategies, a consultancy specializing in transmission and power markets.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;There’s a growing list of other potential uses for AI on the grid and in electricity generation. The technology could monitor and plan ahead for failures in equipment ranging from power lines to gear boxes. Computer vision could help detect everything from wildfires to faulty lines. AI could also help balance supply and demand in virtual power plants, systems of distributed resources like EV chargers or smart water heaters.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;While there are early examples of research and pilot programs for AI from grid planning to operation, some experts are skeptical that the technology will deliver at the level some are hoping for. “It’s not that AI has not had some kind of transformation on power systems,” Climate Change AI’s Agwan says. “It’s that the promise has always been bigger, and the hope has always been bigger.”&lt;/p&gt;  &lt;p&gt;Some places are already seeing higher electricity prices because of power needs from data centers. The situation is likely to get worse. Electricity demand from data centers is set to double by the end of the decade, reaching 945 terawatt-hours, roughly the annual demand from the entire country of Japan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The infrastructure growth needed to support AI load growth has outpaced the promises of the technology, “by quite a bit,” says Panayiotis Moutis, an assistant professor of electrical engineering at the City College of New York. Higher bills caused by the increasing energy needs of AI aren’t justified by existing ways of using the technology for the grid, he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“At the moment, I am very hesitant to lean on the side of AI being a silver bullet,” Moutis says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Correction: This story has been updated to correct Moutis's affiliation.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/09/1123404/ai-grid-help/</guid><pubDate>Tue, 09 Sep 2025 09:00:00 +0000</pubDate></item><item><title>Three big things we still don’t know about AI’s energy burden (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/09/1123408/three-big-things-we-still-dont-know-about-ais-energy-burden/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-517340891.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Earlier this year, when my colleague Casey Crownhart and I spent six months researching the climate and energy burden of AI, we came to see one number in particular as our white whale: how much energy the leading AI models, like ChatGPT or Gemini, use up when generating a single response.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This fundamental number remained elusive even as the scramble to power AI escalated to the White House and the Pentagon, and as projections showed that in three years AI could use as much electricity as 22% of all US households.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The problem with finding that number, as we explain in our piece published in May, was that AI companies are the only ones who have it. We pestered Google, OpenAI, and Microsoft, but each company refused to provide its figure. Researchers we spoke to who study AI’s impact on energy grids compared it to trying to measure the fuel efficiency of a car without ever being able to drive it, making guesses based on rumors of its engine size and what it sounds like going down the highway.&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is a part of&amp;nbsp;&lt;/em&gt;MIT Technology Review&lt;em&gt;’s series “Power Hungry: AI and our energy future,” on the energy demands and carbon costs of the artificial-intelligence revolution.&lt;/em&gt;&lt;/p&gt; 
 &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;But then this summer, after we published, a strange thing started to happen. In June, OpenAI’s Sam Altman wrote that an average ChatGPT query uses 0.34 watt-hours of energy. In July, the French AI startup Mistral didn’t publish a number directly but released an estimate of the emissions generated. In August, Google revealed that answering a question to Gemini uses about 0.24 watt-hours of energy. The figures from Google and OpenAI were similar to what Casey and I estimated for medium-size AI models.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So with this newfound transparency, is our job complete? Did we finally harpoon our white whale, and if so, what happens next for people studying the climate impact of AI? I reached out to some of our old sources, and some new ones, to find out.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;The numbers are vague and chat-only&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The first thing they told me is that there’s a lot missing from the figures tech companies published this summer.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI’s number, for example, did not appear in a detailed technical paper but rather in a blog post by Altman that leaves lots of unanswered questions, such as which model he was referring to, how the energy use was measured, and how much it varies. Google’s figure, as Crownhart points out, refers to the median amount of energy per query, which doesn’t give us a sense of the more energy-demanding Gemini responses, like when it uses a reasoning model to “think” through a hard problem or generates a really long response.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The numbers also refer only to interactions with chatbots, not the other ways that people are becoming increasingly reliant on generative AI.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“As video and image becomes more prominent and used by more and more people, we need the numbers from different modalities and how they measure up,” says Sasha Luccioni, AI and climate lead at the AI platform Hugging Face.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is also important because the figures for asking a question to a chatbot are, as expected, undoubtedly small—the same amount of electricity used by a microwave in just seconds. That’s part of the reason AI and climate researchers don’t suggest that any one individual’s AI use creates a significant climate burden.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;A full accounting of AI’s energy demands—one that goes beyond what’s used to answer an individual query to help us understand its full net impact on the climate—would require application-specific information on how all this AI is being used. Ketan Joshi, an analyst for climate and energy groups, acknowledges that researchers don’t usually get such specific information from other industries but says it might be justified in this case.&lt;/p&gt;  &lt;p&gt;“The rate of data center growth is inarguably unusual,” Joshi says. “Companies should be subject to significantly more scrutiny.”&lt;/p&gt; 

&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;We have questions about energy efficiency&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Companies making billion-dollar investments into AI have struggled to square this growth in energy demand with their sustainability goals. In May, Microsoft said that its emissions have soared by over 23% since 2020, owing largely to AI, while the company has promised to be carbon negative by 2030. “It has become clear that our journey towards being carbon negative is a marathon, not a sprint,” Microsoft wrote.&lt;/p&gt;  &lt;p&gt;Tech companies often justify this emissions burden by arguing that soon enough, AI itself will unlock efficiencies that will make it a net positive for the climate. Perhaps the right AI system, the thinking goes, could design more efficient heating and cooling systems for a building, or help discover the minerals required for electric-vehicle batteries.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But there are no signs that AI has been usefully used to do these things yet. Companies have shared anecdotes about using AI to find methane emission hot spots, for example, but they haven’t been transparent enough to help us know if these successes outweigh the surges in electricity demand and emissions that Big Tech has produced in the AI boom. In the meantime, more data centers are planned, and AI’s energy demand continues to rise and rise.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;&lt;strong&gt;The ‘bubble’ question&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;One of the big unknowns in the AI energy equation is whether society will ever adopt AI at the levels that figure into tech companies’ plans. OpenAI has said that ChatGPT receives 2.5 billion prompts per day. It’s possible that this number, and the equivalent numbers for other AI companies, will continue to soar in the coming years. Projections released last year by the Lawrence Berkeley National Laboratory suggest that if they do, AI alone could consume as much electricity annually as 22% of all US households by 2028.&lt;/p&gt;  &lt;p&gt;But this summer also saw signs of a slowdown that undercut the industry’s optimism. OpenAI’s launch of GPT-5 was largely considered a flop, even by the company itself, and that flop led critics to wonder if AI may be hitting a wall. When a group at MIT found that 95% of businesses are seeing no return on their massive AI investments, stocks floundered. The expansion of AI-specific data centers might be an investment that’s hard to recoup, especially as revenues for AI companies remain elusive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;One of the biggest unknowns about AI’s future energy burden isn’t how much a single query consumes, or any other figure that can be disclosed. It’s whether demand will ever reach the scale companies are building for or whether the technology will collapse under its own hype. The answer will determine whether today’s buildout becomes a lasting shift in our energy system or a short-lived spike.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/Power-Hungry-video.mp4"&gt;&lt;/video&gt; &lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-517340891.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Earlier this year, when my colleague Casey Crownhart and I spent six months researching the climate and energy burden of AI, we came to see one number in particular as our white whale: how much energy the leading AI models, like ChatGPT or Gemini, use up when generating a single response.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This fundamental number remained elusive even as the scramble to power AI escalated to the White House and the Pentagon, and as projections showed that in three years AI could use as much electricity as 22% of all US households.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The problem with finding that number, as we explain in our piece published in May, was that AI companies are the only ones who have it. We pestered Google, OpenAI, and Microsoft, but each company refused to provide its figure. Researchers we spoke to who study AI’s impact on energy grids compared it to trying to measure the fuel efficiency of a car without ever being able to drive it, making guesses based on rumors of its engine size and what it sounds like going down the highway.&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is a part of&amp;nbsp;&lt;/em&gt;MIT Technology Review&lt;em&gt;’s series “Power Hungry: AI and our energy future,” on the energy demands and carbon costs of the artificial-intelligence revolution.&lt;/em&gt;&lt;/p&gt; 
 &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;But then this summer, after we published, a strange thing started to happen. In June, OpenAI’s Sam Altman wrote that an average ChatGPT query uses 0.34 watt-hours of energy. In July, the French AI startup Mistral didn’t publish a number directly but released an estimate of the emissions generated. In August, Google revealed that answering a question to Gemini uses about 0.24 watt-hours of energy. The figures from Google and OpenAI were similar to what Casey and I estimated for medium-size AI models.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So with this newfound transparency, is our job complete? Did we finally harpoon our white whale, and if so, what happens next for people studying the climate impact of AI? I reached out to some of our old sources, and some new ones, to find out.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;The numbers are vague and chat-only&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The first thing they told me is that there’s a lot missing from the figures tech companies published this summer.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI’s number, for example, did not appear in a detailed technical paper but rather in a blog post by Altman that leaves lots of unanswered questions, such as which model he was referring to, how the energy use was measured, and how much it varies. Google’s figure, as Crownhart points out, refers to the median amount of energy per query, which doesn’t give us a sense of the more energy-demanding Gemini responses, like when it uses a reasoning model to “think” through a hard problem or generates a really long response.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The numbers also refer only to interactions with chatbots, not the other ways that people are becoming increasingly reliant on generative AI.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“As video and image becomes more prominent and used by more and more people, we need the numbers from different modalities and how they measure up,” says Sasha Luccioni, AI and climate lead at the AI platform Hugging Face.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is also important because the figures for asking a question to a chatbot are, as expected, undoubtedly small—the same amount of electricity used by a microwave in just seconds. That’s part of the reason AI and climate researchers don’t suggest that any one individual’s AI use creates a significant climate burden.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;A full accounting of AI’s energy demands—one that goes beyond what’s used to answer an individual query to help us understand its full net impact on the climate—would require application-specific information on how all this AI is being used. Ketan Joshi, an analyst for climate and energy groups, acknowledges that researchers don’t usually get such specific information from other industries but says it might be justified in this case.&lt;/p&gt;  &lt;p&gt;“The rate of data center growth is inarguably unusual,” Joshi says. “Companies should be subject to significantly more scrutiny.”&lt;/p&gt; 

&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;We have questions about energy efficiency&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Companies making billion-dollar investments into AI have struggled to square this growth in energy demand with their sustainability goals. In May, Microsoft said that its emissions have soared by over 23% since 2020, owing largely to AI, while the company has promised to be carbon negative by 2030. “It has become clear that our journey towards being carbon negative is a marathon, not a sprint,” Microsoft wrote.&lt;/p&gt;  &lt;p&gt;Tech companies often justify this emissions burden by arguing that soon enough, AI itself will unlock efficiencies that will make it a net positive for the climate. Perhaps the right AI system, the thinking goes, could design more efficient heating and cooling systems for a building, or help discover the minerals required for electric-vehicle batteries.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But there are no signs that AI has been usefully used to do these things yet. Companies have shared anecdotes about using AI to find methane emission hot spots, for example, but they haven’t been transparent enough to help us know if these successes outweigh the surges in electricity demand and emissions that Big Tech has produced in the AI boom. In the meantime, more data centers are planned, and AI’s energy demand continues to rise and rise.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;&lt;strong&gt;The ‘bubble’ question&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;One of the big unknowns in the AI energy equation is whether society will ever adopt AI at the levels that figure into tech companies’ plans. OpenAI has said that ChatGPT receives 2.5 billion prompts per day. It’s possible that this number, and the equivalent numbers for other AI companies, will continue to soar in the coming years. Projections released last year by the Lawrence Berkeley National Laboratory suggest that if they do, AI alone could consume as much electricity annually as 22% of all US households by 2028.&lt;/p&gt;  &lt;p&gt;But this summer also saw signs of a slowdown that undercut the industry’s optimism. OpenAI’s launch of GPT-5 was largely considered a flop, even by the company itself, and that flop led critics to wonder if AI may be hitting a wall. When a group at MIT found that 95% of businesses are seeing no return on their massive AI investments, stocks floundered. The expansion of AI-specific data centers might be an investment that’s hard to recoup, especially as revenues for AI companies remain elusive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;One of the biggest unknowns about AI’s future energy burden isn’t how much a single query consumes, or any other figure that can be disclosed. It’s whether demand will ever reach the scale companies are building for or whether the technology will collapse under its own hype. The answer will determine whether today’s buildout becomes a lasting shift in our energy system or a short-lived spike.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/Power-Hungry-video.mp4"&gt;&lt;/video&gt; &lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/09/1123408/three-big-things-we-still-dont-know-about-ais-energy-burden/</guid><pubDate>Tue, 09 Sep 2025 09:00:00 +0000</pubDate></item><item><title>Help! My therapist is secretly using ChatGPT (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/09/1123386/help-my-therapist-is-secretly-using-chatgpt/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/therapy-ai3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;In Silicon Valley’s imagined future, AI models are so empathetic that we’ll use them as therapists. They’ll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn’t subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time. The model then suggested responses that his therapist parroted.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It’s my favorite AI story as of late, probably because it captures so well the chaos that can unfold when people actually use AI the way tech companies have all but told them to.&lt;/p&gt;  &lt;p&gt;As the writer of the story, Laurie Clarke, points out, it’s not a total pipe dream that AI could be therapeutically useful. Early this year, I wrote about the first clinical trial of an AI bot built specifically for therapy. The results were promising! But the secretive use by therapists of AI models that are not vetted for mental health is something very different. I had a conversation with Clarke to hear more about what she found.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;I have to say, I was really fascinated that people called out their therapists after finding out they were covertly using AI. How did you interpret the reactions of these therapists? Were they trying to hide it?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;In all the cases mentioned in the piece, the therapist hadn’t provided prior disclosure of how they were using AI to their patients. So whether or not they were explicitly trying to conceal it, that’s how it ended up looking when it was discovered. I think for this reason, one of my main takeaways from writing the piece was that therapists should absolutely disclose when they’re going to use AI and how (if they plan to use it). If they don’t, it raises all these really uncomfortable questions for patients when it’s uncovered and risks irrevocably damaging the trust that’s been built.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;In the examples you’ve come across, are therapists turning to AI simply as a time-saver? Or do they think AI models can genuinely give them a new perspective on what’s bothering someone?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Some see AI as a potential time-saver. I heard from a few therapists that notes are the bane of their lives. So I think there is some interest in AI-powered tools that can support this. Most I spoke to were very skeptical about using AI for advice on how to treat a patient. They said it would be better to consult supervisors or colleagues, or case studies in the literature. They were also understandably very wary of inputting sensitive data into these tools.&lt;/p&gt;  &lt;p&gt;There is some evidence AI can deliver more standardized, "manualized" therapies like CBT [cognitive behavioral therapy] reasonably effectively. So it’s possible it could be more useful for that. But that is AI specifically designed for that purpose, not general-purpose tools like ChatGPT.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;What happens if this goes awry? What attention is this getting from ethics groups and lawmakers?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;At present, professional bodies like the American Counseling Association advise against using AI tools to diagnose patients. There could also be more stringent regulations preventing this in future. Nevada and Illinois, for example, have recently passed laws prohibiting the use of AI in therapeutic decision-making. More states could follow.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;OpenAI’s Sam Altman &lt;/strong&gt;&lt;strong&gt;said&lt;/strong&gt;&lt;strong&gt; last month that “a lot of people effectively use ChatGPT as a sort of therapist,” and that to him, that’s a good thing. Do you think tech companies are overpromising on AI’s ability to help us?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I think that tech companies are subtly encouraging this use of AI because clearly it’s a route through which some people are forming an attachment to their products. I think the main issue is that what people are getting from these tools isn’t really “therapy” by any stretch. Good therapy goes far beyond being soothing and validating everything someone says. I’ve never in my life looked forward to a (real, in-person) therapy session. They’re often highly uncomfortable, and even distressing. But that’s part of the point. The therapist should be challenging you and drawing you out and seeking to understand you. ChatGPT doesn’t do any of these things.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Read the full story from Laurie Clarke.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/therapy-ai3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;In Silicon Valley’s imagined future, AI models are so empathetic that we’ll use them as therapists. They’ll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn’t subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time. The model then suggested responses that his therapist parroted.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It’s my favorite AI story as of late, probably because it captures so well the chaos that can unfold when people actually use AI the way tech companies have all but told them to.&lt;/p&gt;  &lt;p&gt;As the writer of the story, Laurie Clarke, points out, it’s not a total pipe dream that AI could be therapeutically useful. Early this year, I wrote about the first clinical trial of an AI bot built specifically for therapy. The results were promising! But the secretive use by therapists of AI models that are not vetted for mental health is something very different. I had a conversation with Clarke to hear more about what she found.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;I have to say, I was really fascinated that people called out their therapists after finding out they were covertly using AI. How did you interpret the reactions of these therapists? Were they trying to hide it?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;In all the cases mentioned in the piece, the therapist hadn’t provided prior disclosure of how they were using AI to their patients. So whether or not they were explicitly trying to conceal it, that’s how it ended up looking when it was discovered. I think for this reason, one of my main takeaways from writing the piece was that therapists should absolutely disclose when they’re going to use AI and how (if they plan to use it). If they don’t, it raises all these really uncomfortable questions for patients when it’s uncovered and risks irrevocably damaging the trust that’s been built.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;In the examples you’ve come across, are therapists turning to AI simply as a time-saver? Or do they think AI models can genuinely give them a new perspective on what’s bothering someone?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Some see AI as a potential time-saver. I heard from a few therapists that notes are the bane of their lives. So I think there is some interest in AI-powered tools that can support this. Most I spoke to were very skeptical about using AI for advice on how to treat a patient. They said it would be better to consult supervisors or colleagues, or case studies in the literature. They were also understandably very wary of inputting sensitive data into these tools.&lt;/p&gt;  &lt;p&gt;There is some evidence AI can deliver more standardized, "manualized" therapies like CBT [cognitive behavioral therapy] reasonably effectively. So it’s possible it could be more useful for that. But that is AI specifically designed for that purpose, not general-purpose tools like ChatGPT.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;What happens if this goes awry? What attention is this getting from ethics groups and lawmakers?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;At present, professional bodies like the American Counseling Association advise against using AI tools to diagnose patients. There could also be more stringent regulations preventing this in future. Nevada and Illinois, for example, have recently passed laws prohibiting the use of AI in therapeutic decision-making. More states could follow.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;OpenAI’s Sam Altman &lt;/strong&gt;&lt;strong&gt;said&lt;/strong&gt;&lt;strong&gt; last month that “a lot of people effectively use ChatGPT as a sort of therapist,” and that to him, that’s a good thing. Do you think tech companies are overpromising on AI’s ability to help us?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I think that tech companies are subtly encouraging this use of AI because clearly it’s a route through which some people are forming an attachment to their products. I think the main issue is that what people are getting from these tools isn’t really “therapy” by any stretch. Good therapy goes far beyond being soothing and validating everything someone says. I’ve never in my life looked forward to a (real, in-person) therapy session. They’re often highly uncomfortable, and even distressing. But that’s part of the point. The therapist should be challenging you and drawing you out and seeking to understand you. ChatGPT doesn’t do any of these things.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Read the full story from Laurie Clarke.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/09/1123386/help-my-therapist-is-secretly-using-chatgpt/</guid><pubDate>Tue, 09 Sep 2025 09:00:00 +0000</pubDate></item><item><title>Thinking Machines becomes OpenAI’s first services partner in APAC (AI News)</title><link>https://www.artificialintelligence-news.com/news/thinking-machines-becomes-openai-first-services-partner-in-apac/</link><description>&lt;p&gt;Thinking Machines Data Science is joining forces with OpenAI to help more businesses across Asia Pacific turn artificial intelligence into measurable results. The collaboration makes Thinking Machines the first official Services Partner for OpenAI in the region.&lt;/p&gt;&lt;p&gt;The partnership comes as AI adoption in APAC continues to rise. An IBM study found that 61% of enterprises already use AI, yet many struggle to move beyond pilot projects and deliver real business impact. Thinking Machines and OpenAI aim to change that by offering executive training on ChatGPT Enterprise, support for building custom AI applications, and guidance on embedding AI into everyday operations.&lt;/p&gt;&lt;p&gt;Stephanie Sy, Founder and CEO of Thinking Machines, framed the partnership around capability building: “We’re not just bringing in new technology but we’re helping organisations build the skills, strategies, and support systems they need to take advantage of AI. For us, it’s about reinventing the future of work through human-AI collaboration and making AI truly work for people across the Asia Pacific region.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-turning-ai-pilots-into-results-with-thinking-machines"&gt;Turning AI pilots into results with Thinking Machines&lt;/h3&gt;&lt;p&gt;In an interview with &lt;em&gt;AI News&lt;/em&gt;, Sy explained that one of the biggest hurdles for enterprises is how they frame AI adoption. Too often, organisations see it as a technology acquisition rather than a business transformation. That approach leads to pilots that stall or fail to scale.&lt;/p&gt;&lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="Stephanie Sy, Founder and CEO of Thinking Machines." class="wp-image-109278" height="1024" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/Stef_1-683x1024.jpg" width="683" /&gt;&lt;figcaption class="wp-element-caption"&gt;Stephanie Sy, Founder and CEO of Thinking Machines.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;“The main challenge is that many organisations approach AI as a technology acquisition rather than a business transformation,” she said. “This leads to pilots that never scale because three fundamentals are missing: clear leadership alignment on the value to create, redesign of workflows to embed AI into how work gets done, and investment in workforce skills to ensure adoption. Get those three right—vision, process, people—and pilots scale into impact.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-leadership-at-the-centre"&gt;Leadership at the centre&lt;/h3&gt;&lt;p&gt;Many executives still treat AI as a technical project rather than a strategic priority. Sy believes that boards and C-suites need to set the tone. Their role is to decide whether AI is a growth driver or just a managed risk.&lt;/p&gt;&lt;p&gt;“Boards and C-suites set the tone: Is AI a strategic growth driver or a managed risk? Their role is to name a few priority outcomes, define risk appetite, and assign clear ownership,” she said. Thinking Machines often begins with executive sessions where leaders can explore where tools like ChatGPT add value, how to govern them, and when to scale. “That top-down clarity is what turns AI from an experiment into an enterprise capability.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-human-ai-collaboration-in-practice"&gt;Human-AI collaboration in practice&lt;/h3&gt;&lt;p&gt;Sy often talks about “reinventing the future of work through human-AI collaboration.” She explained what this looks like in practice: a “human-in-command” approach where people focus on judgment, decision-making, and exceptions, while AI handles routine steps like retrieval, drafting, or summarising.&lt;/p&gt;&lt;p&gt;“Human-in-command means redesigning work so people focus on judgment and exceptions, while AI takes on retrieval, drafting, and routine steps, with transparency through audit trails and source links,” she said. The results are measured in time saved and quality improvements.&lt;/p&gt;&lt;p&gt;In workshops run by Thinking Machines, professionals using ChatGPT often free up one to two hours per day. Research supports these outcomes—Sy pointed to an MIT study showing a 14% productivity boost for contact centre agents, with the biggest gains seen among less-experienced staff. “That’s clear evidence AI can elevate human talent rather than displace it,” she added.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-agentic-ai-with-thinking-machines-guardrails"&gt;&lt;strong&gt;Agentic AI with Thinking Machines’ guardrails&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Another area of focus for Thinking Machines is agentic AI, which goes beyond single queries to handle multi-step processes. Instead of just answering a question, agentic systems can manage research, fill forms, and make API calls, coordinating entire workflows with a human still in charge.&lt;/p&gt;&lt;p&gt;“Agentic systems can take work from ‘ask-and-answer’ to multi-step execution: coordinating research, browsing, form-filling, and API calls so teams ship faster with a human in command,” Sy said. The promise is faster execution and productivity, but the risks are real. “The principles of human-in-command and auditability remain critical; to avoid the lack of proper guardrails. Our approach is to pair enterprise controls and auditability with agent capabilities to ensure actions are traceable, reversible, and policy-aligned before we scale.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-governance-that-builds-trust"&gt;&lt;strong&gt;Governance that builds trust&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;While adoption is accelerating, governance often lags behind. Sy cautioned that governance fails when it’s treated as paperwork instead of part of daily work.&lt;/p&gt;&lt;p&gt;“We keep humans in command and make governance visible in daily work: use approved data sources, enforce role-based access, maintain audit trails, and require human decision points for sensitive actions,” she explained. Thinking Machines also applies what it calls “control + reliability”: restricting retrieval to trusted content and returning answers with citations. Workflows are then adapted to local rules in sectors such as finance, government, and healthcare.&lt;/p&gt;&lt;p&gt;For Sy, success isn’t measured in the volume of policies but in auditability and exception rates. “Good governance accelerates adoption because teams trust what they ship,” she said.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-local-context-regional-scale"&gt;Local context, regional scale&lt;/h3&gt;&lt;p&gt;Asia Pacific’s cultural and linguistic diversity poses unique challenges for scaling AI. A one-size-fits-all model doesn’t work. Sy emphasised that the right playbook is to build locally first and then scale deliberately.&lt;/p&gt;&lt;p&gt;“Global templates fail when they ignore how local teams work. The playbook is build locally, scale deliberately: fit the AI to local language, forms, policies, and escalation paths; then standardise the parts that travel such as your governance pattern, data connectors, and impact metrics,” she said.&lt;/p&gt;&lt;p&gt;That’s the approach Thinking Machines has taken in Singapore, the Philippines, and Thailand—prove value with local teams first, then roll out region by region. The aim is not a uniform chatbot but a reliable pattern that respects local context while maintaining scalability.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-skills-over-tools"&gt;Skills over tools&lt;/h3&gt;&lt;p&gt;When asked what skills will matter most in an AI-enabled workplace, Sy pointed out that scale comes from skills, not just tools. She broke this down into three categories:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Executive literacy&lt;/strong&gt;: the ability for leaders to set outcomes and guardrails, and know when and where to scale AI.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Workflow design&lt;/strong&gt;: the redesign of human-AI handoffs, clarifying who drafts, who approves, and how exceptions escalate.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Hands-on skills&lt;/strong&gt;: prompting, evaluation, and retrieval from trusted sources so answers are verifiable, not just plausible.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;“When leaders and teams share that foundation, adoption moves from experimenting to repeatable, production-level results,” she said. In Thinking Machines’ programs, many professionals report saving one to two hours per day after just a one-day workshop. To date, more than 10,000 people across roles have been trained, and Sy noted the pattern is consistent: “skills + governance unlock scale.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-industry-transformation-ahead"&gt;Industry transformation ahead&lt;/h3&gt;&lt;p&gt;Looking to the next five years, Sy sees AI shifting from drafting to full execution in critical business functions. She expects major gains in software development, marketing, service operations, and supply chain management.&lt;/p&gt;&lt;p&gt;“For the next wave, we see three concrete patterns: policy-aware assistants in finance, supply chain copilots in manufacturing, and personalised yet compliant CX in retail—each built with human checkpoints and verifiable sources so leaders can scale with confidence,” she said.&lt;/p&gt;&lt;p&gt;A practical example is a system Thinking Machines built with the Bank of the Philippine Islands. Called BEAi, it’s a retrieval-augmented generation (RAG) system that supports English, Filipino, and Taglish. It returns answers linked to sources with page numbers and understands policy supersession, turning complex policy documents into everyday guidance for staff. “That’s what ‘AI-native’ looks like in practice,” Sy said.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-thinking-machines-expands-ai-across-apac"&gt;Thinking Machines expands AI across APAC&lt;/h3&gt;&lt;p&gt;The partnership with OpenAI will start with programs in Singapore, the Philippines, and Thailand through Thinking Machines’ regional offices before expanding further across APAC. Future plans include tailoring services to sectors such as finance, retail, and manufacturing, where AI can address specific challenges and open new opportunities.&lt;/p&gt;&lt;p&gt;For Sy, the goal is clear: “AI adoption isn’t just about experimenting with new tools. It’s about building the vision, processes, and skills that let organisations move from pilots to impact. When leaders, teams, and technology come together, that’s when AI delivers lasting value.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: X and xAI sue Apple and OpenAI over AI monopoly claims&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-109277" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-9.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Thinking Machines Data Science is joining forces with OpenAI to help more businesses across Asia Pacific turn artificial intelligence into measurable results. The collaboration makes Thinking Machines the first official Services Partner for OpenAI in the region.&lt;/p&gt;&lt;p&gt;The partnership comes as AI adoption in APAC continues to rise. An IBM study found that 61% of enterprises already use AI, yet many struggle to move beyond pilot projects and deliver real business impact. Thinking Machines and OpenAI aim to change that by offering executive training on ChatGPT Enterprise, support for building custom AI applications, and guidance on embedding AI into everyday operations.&lt;/p&gt;&lt;p&gt;Stephanie Sy, Founder and CEO of Thinking Machines, framed the partnership around capability building: “We’re not just bringing in new technology but we’re helping organisations build the skills, strategies, and support systems they need to take advantage of AI. For us, it’s about reinventing the future of work through human-AI collaboration and making AI truly work for people across the Asia Pacific region.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-turning-ai-pilots-into-results-with-thinking-machines"&gt;Turning AI pilots into results with Thinking Machines&lt;/h3&gt;&lt;p&gt;In an interview with &lt;em&gt;AI News&lt;/em&gt;, Sy explained that one of the biggest hurdles for enterprises is how they frame AI adoption. Too often, organisations see it as a technology acquisition rather than a business transformation. That approach leads to pilots that stall or fail to scale.&lt;/p&gt;&lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="Stephanie Sy, Founder and CEO of Thinking Machines." class="wp-image-109278" height="1024" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/Stef_1-683x1024.jpg" width="683" /&gt;&lt;figcaption class="wp-element-caption"&gt;Stephanie Sy, Founder and CEO of Thinking Machines.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;“The main challenge is that many organisations approach AI as a technology acquisition rather than a business transformation,” she said. “This leads to pilots that never scale because three fundamentals are missing: clear leadership alignment on the value to create, redesign of workflows to embed AI into how work gets done, and investment in workforce skills to ensure adoption. Get those three right—vision, process, people—and pilots scale into impact.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-leadership-at-the-centre"&gt;Leadership at the centre&lt;/h3&gt;&lt;p&gt;Many executives still treat AI as a technical project rather than a strategic priority. Sy believes that boards and C-suites need to set the tone. Their role is to decide whether AI is a growth driver or just a managed risk.&lt;/p&gt;&lt;p&gt;“Boards and C-suites set the tone: Is AI a strategic growth driver or a managed risk? Their role is to name a few priority outcomes, define risk appetite, and assign clear ownership,” she said. Thinking Machines often begins with executive sessions where leaders can explore where tools like ChatGPT add value, how to govern them, and when to scale. “That top-down clarity is what turns AI from an experiment into an enterprise capability.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-human-ai-collaboration-in-practice"&gt;Human-AI collaboration in practice&lt;/h3&gt;&lt;p&gt;Sy often talks about “reinventing the future of work through human-AI collaboration.” She explained what this looks like in practice: a “human-in-command” approach where people focus on judgment, decision-making, and exceptions, while AI handles routine steps like retrieval, drafting, or summarising.&lt;/p&gt;&lt;p&gt;“Human-in-command means redesigning work so people focus on judgment and exceptions, while AI takes on retrieval, drafting, and routine steps, with transparency through audit trails and source links,” she said. The results are measured in time saved and quality improvements.&lt;/p&gt;&lt;p&gt;In workshops run by Thinking Machines, professionals using ChatGPT often free up one to two hours per day. Research supports these outcomes—Sy pointed to an MIT study showing a 14% productivity boost for contact centre agents, with the biggest gains seen among less-experienced staff. “That’s clear evidence AI can elevate human talent rather than displace it,” she added.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-agentic-ai-with-thinking-machines-guardrails"&gt;&lt;strong&gt;Agentic AI with Thinking Machines’ guardrails&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Another area of focus for Thinking Machines is agentic AI, which goes beyond single queries to handle multi-step processes. Instead of just answering a question, agentic systems can manage research, fill forms, and make API calls, coordinating entire workflows with a human still in charge.&lt;/p&gt;&lt;p&gt;“Agentic systems can take work from ‘ask-and-answer’ to multi-step execution: coordinating research, browsing, form-filling, and API calls so teams ship faster with a human in command,” Sy said. The promise is faster execution and productivity, but the risks are real. “The principles of human-in-command and auditability remain critical; to avoid the lack of proper guardrails. Our approach is to pair enterprise controls and auditability with agent capabilities to ensure actions are traceable, reversible, and policy-aligned before we scale.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-governance-that-builds-trust"&gt;&lt;strong&gt;Governance that builds trust&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;While adoption is accelerating, governance often lags behind. Sy cautioned that governance fails when it’s treated as paperwork instead of part of daily work.&lt;/p&gt;&lt;p&gt;“We keep humans in command and make governance visible in daily work: use approved data sources, enforce role-based access, maintain audit trails, and require human decision points for sensitive actions,” she explained. Thinking Machines also applies what it calls “control + reliability”: restricting retrieval to trusted content and returning answers with citations. Workflows are then adapted to local rules in sectors such as finance, government, and healthcare.&lt;/p&gt;&lt;p&gt;For Sy, success isn’t measured in the volume of policies but in auditability and exception rates. “Good governance accelerates adoption because teams trust what they ship,” she said.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-local-context-regional-scale"&gt;Local context, regional scale&lt;/h3&gt;&lt;p&gt;Asia Pacific’s cultural and linguistic diversity poses unique challenges for scaling AI. A one-size-fits-all model doesn’t work. Sy emphasised that the right playbook is to build locally first and then scale deliberately.&lt;/p&gt;&lt;p&gt;“Global templates fail when they ignore how local teams work. The playbook is build locally, scale deliberately: fit the AI to local language, forms, policies, and escalation paths; then standardise the parts that travel such as your governance pattern, data connectors, and impact metrics,” she said.&lt;/p&gt;&lt;p&gt;That’s the approach Thinking Machines has taken in Singapore, the Philippines, and Thailand—prove value with local teams first, then roll out region by region. The aim is not a uniform chatbot but a reliable pattern that respects local context while maintaining scalability.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-skills-over-tools"&gt;Skills over tools&lt;/h3&gt;&lt;p&gt;When asked what skills will matter most in an AI-enabled workplace, Sy pointed out that scale comes from skills, not just tools. She broke this down into three categories:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Executive literacy&lt;/strong&gt;: the ability for leaders to set outcomes and guardrails, and know when and where to scale AI.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Workflow design&lt;/strong&gt;: the redesign of human-AI handoffs, clarifying who drafts, who approves, and how exceptions escalate.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Hands-on skills&lt;/strong&gt;: prompting, evaluation, and retrieval from trusted sources so answers are verifiable, not just plausible.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;“When leaders and teams share that foundation, adoption moves from experimenting to repeatable, production-level results,” she said. In Thinking Machines’ programs, many professionals report saving one to two hours per day after just a one-day workshop. To date, more than 10,000 people across roles have been trained, and Sy noted the pattern is consistent: “skills + governance unlock scale.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-industry-transformation-ahead"&gt;Industry transformation ahead&lt;/h3&gt;&lt;p&gt;Looking to the next five years, Sy sees AI shifting from drafting to full execution in critical business functions. She expects major gains in software development, marketing, service operations, and supply chain management.&lt;/p&gt;&lt;p&gt;“For the next wave, we see three concrete patterns: policy-aware assistants in finance, supply chain copilots in manufacturing, and personalised yet compliant CX in retail—each built with human checkpoints and verifiable sources so leaders can scale with confidence,” she said.&lt;/p&gt;&lt;p&gt;A practical example is a system Thinking Machines built with the Bank of the Philippine Islands. Called BEAi, it’s a retrieval-augmented generation (RAG) system that supports English, Filipino, and Taglish. It returns answers linked to sources with page numbers and understands policy supersession, turning complex policy documents into everyday guidance for staff. “That’s what ‘AI-native’ looks like in practice,” Sy said.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-thinking-machines-expands-ai-across-apac"&gt;Thinking Machines expands AI across APAC&lt;/h3&gt;&lt;p&gt;The partnership with OpenAI will start with programs in Singapore, the Philippines, and Thailand through Thinking Machines’ regional offices before expanding further across APAC. Future plans include tailoring services to sectors such as finance, retail, and manufacturing, where AI can address specific challenges and open new opportunities.&lt;/p&gt;&lt;p&gt;For Sy, the goal is clear: “AI adoption isn’t just about experimenting with new tools. It’s about building the vision, processes, and skills that let organisations move from pilots to impact. When leaders, teams, and technology come together, that’s when AI delivers lasting value.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: X and xAI sue Apple and OpenAI over AI monopoly claims&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-109277" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-9.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/thinking-machines-becomes-openai-first-services-partner-in-apac/</guid><pubDate>Tue, 09 Sep 2025 10:16:22 +0000</pubDate></item><item><title>Why accessibility might be AI’s biggest breakthrough (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/09/study-finds-neurodiverse-workers-more-satisfied-with-ai-assistants/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        UK study findings may challenge assumptions about who benefits most from AI tools.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Group of People with differing personalities" class="absolute inset-0 w-full h-full object-cover hidden" height="409" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-640x409.jpg" width="640" /&gt;
                  &lt;img alt="Group of People with differing personalities" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Group of People with differing personalities

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Chris Madden via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;While tech companies market AI as a productivity tool for everyone, a UK government study reveals an unexpected result: Neurodiverse employees may be benefiting far more from chatbots than their neurotypical colleagues.&lt;/p&gt;
&lt;p&gt;The UK's Department for Business and Trade recently released evaluation results from its Microsoft 365 Copilot trial showing that while overall satisfaction was 72 percent, neurodiverse employees reported statistically higher satisfaction (at a 90 percent confidence level) and were more likely to recommend the tool (at a 95 percent confidence level) than other respondents.&lt;/p&gt;
&lt;p&gt;"It's leveled the playing field," one participant with ADHD told researchers during follow-up interviews. One user with dyslexia said that the tool "empowered" them to perform tasks with confidence they previously lacked, particularly in report writing. Another dyslexic participant drew direct comparisons to existing accessibility software, noting that Copilot "does a hell of a lot more" than traditional assistive technology while being "embedded in your applications" rather than requiring separate programs.&lt;/p&gt;
&lt;p&gt;The reported benefits extended beyond neurodiversity. Users with hearing disabilities reported that AI-powered meeting transcription allowed them to participate more fully in discussions. "I can very quickly recall and be able to share my inputs rather than sit quietly thinking I missed the point," one participant explained, describing how constant focus requirements in meetings left them exhausted.&lt;/p&gt;
&lt;p&gt;The study, titled "The Evaluation of the M365 Copilot Pilot in the Department for Business and Trade," suggests that AI tools might be addressing workplace accessibility gaps that traditional accommodations have missed. The department conducted the study between October 2024 and March 2025 using diary studies, interviews, and observed tasks to measure how the AI assistant affected different user groups.&lt;/p&gt;
&lt;p&gt;The finding emerges from 300 participants who consented to analysis out of 1,000 licenses distributed, though the study doesn't specify how many identified as neurodiverse. While the 90 percent confidence for satisfaction falls below typical academic standards, the stronger finding for likelihood to recommend suggests a meaningful difference.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;These experiences echo some personal accounts online from autistic and ADHD users who describe AI as providing scaffolding for their writing and executive function needs. Some users also find value in having help decoding social subtext in workplace communications or suggesting appropriate professional language.&lt;/p&gt;
&lt;h2&gt;Beyond traditional accommodations&lt;/h2&gt;
&lt;p&gt;Even with what appears to be generally positive reviews of AI assistants for neurodivergent people who responded to the study, there's still plenty of room for nuanced takes on the overall potential of AI language models. The Register reported on the same study Thursday, emphasizing a lack of clear productivity gains and issues with Excel and PowerPoint outputs found by the researchers. The accessibility findings show the impact of AI from a different angle—one that Silicon Valley executives racing for flashy investment-attracting concepts like "superintelligence" might not consider as often.&lt;/p&gt;
&lt;p&gt;The disconnect between AI's promised productivity revolution and its actual impact might reveal a fundamental misunderstanding about where these tools excel. Traditional productivity gains require AI to outperform humans at tasks we're already good at—a high bar that current technology struggles to clear consistently. But for accessibility, AI doesn't need to be perfect; it just needs to bridge gaps that would otherwise exclude people entirely. The difference between writing a report 20 percent faster and being able to write a report at all represents two entirely different value propositions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For people with dyslexia in any setting, AI assistants might serve as writing aids that go beyond traditional spell-checkers, potentially helping with sentence structure and organizing thoughts without requiring specialized software. People with ADHD might be able to use these tools as executive function support, helping break down complex tasks and organize scattered thoughts.&lt;/p&gt;
&lt;p&gt;Some users report using AI to overcome procrastination and create structure as transformative for managing ADHD symptoms. "ChatGPT can help us hash things out so that we feel more prepared, comfortable, and confident in communicating with others," a reader named Lena&amp;nbsp;told ADDitude magazine.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For those with visual impairments, language models can summarize visual content and reformat information. Tools like ChatGPT's voice mode with video and Be My Eyes allow a machine to describe real-world visual scenes in ways that were impossible just a few years ago.&lt;/p&gt;
&lt;p&gt;AI language tools may be providing unofficial stealth accommodations for students—support that doesn't require formal diagnosis, workplace disclosure, or special equipment. Yet this informal support system comes with its own risks. Language models do confabulate—the UK Department for Business and Trade study found 22 percent of users identified false information in AI outputs—which could be particularly harmful for users relying on them for essential support.&lt;/p&gt;
&lt;h2&gt;When AI assistance becomes dependence&lt;/h2&gt;
&lt;p&gt;Beyond the workplace, the drawbacks may have a particular impact on students who use the technology. The authors of a 2025 study on students with disabilities using generative AI cautioned, "Key concerns students with disabilities had included the inaccuracy of AI answers, risks to academic integrity, and subscription cost barriers," they wrote. Students in that study had ADHD, dyslexia, dyspraxia, and autism, with ChatGPT being the most commonly used tool.&lt;/p&gt;
&lt;p&gt;Mistakes in AI outputs are especially pernicious because, due to grandiose visions of near-term AI technology, some people think today's AI assistants can perform tasks that are actually far outside their scope. As research on blind users' experiences suggested, people develop complex (sometimes flawed) mental models of how these tools work, showing the need for higher awareness of AI language model drawbacks among the general public.&lt;/p&gt;
&lt;p&gt;For the UK government employees who participated in the initial study, these questions moved from theoretical to immediate when the pilot ended in December 2024. After that time, many participants reported difficulty readjusting to work without AI assistance—particularly those with disabilities who had come to rely on the accessibility benefits. The department hasn't announced the next steps, leaving users in limbo. When participants report difficulty readjusting to work without AI while productivity gains remain marginal, accessibility emerges as potentially the first AI application with irreplaceable value.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        UK study findings may challenge assumptions about who benefits most from AI tools.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Group of People with differing personalities" class="absolute inset-0 w-full h-full object-cover hidden" height="409" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-640x409.jpg" width="640" /&gt;
                  &lt;img alt="Group of People with differing personalities" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Group of People with differing personalities

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Chris Madden via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;While tech companies market AI as a productivity tool for everyone, a UK government study reveals an unexpected result: Neurodiverse employees may be benefiting far more from chatbots than their neurotypical colleagues.&lt;/p&gt;
&lt;p&gt;The UK's Department for Business and Trade recently released evaluation results from its Microsoft 365 Copilot trial showing that while overall satisfaction was 72 percent, neurodiverse employees reported statistically higher satisfaction (at a 90 percent confidence level) and were more likely to recommend the tool (at a 95 percent confidence level) than other respondents.&lt;/p&gt;
&lt;p&gt;"It's leveled the playing field," one participant with ADHD told researchers during follow-up interviews. One user with dyslexia said that the tool "empowered" them to perform tasks with confidence they previously lacked, particularly in report writing. Another dyslexic participant drew direct comparisons to existing accessibility software, noting that Copilot "does a hell of a lot more" than traditional assistive technology while being "embedded in your applications" rather than requiring separate programs.&lt;/p&gt;
&lt;p&gt;The reported benefits extended beyond neurodiversity. Users with hearing disabilities reported that AI-powered meeting transcription allowed them to participate more fully in discussions. "I can very quickly recall and be able to share my inputs rather than sit quietly thinking I missed the point," one participant explained, describing how constant focus requirements in meetings left them exhausted.&lt;/p&gt;
&lt;p&gt;The study, titled "The Evaluation of the M365 Copilot Pilot in the Department for Business and Trade," suggests that AI tools might be addressing workplace accessibility gaps that traditional accommodations have missed. The department conducted the study between October 2024 and March 2025 using diary studies, interviews, and observed tasks to measure how the AI assistant affected different user groups.&lt;/p&gt;
&lt;p&gt;The finding emerges from 300 participants who consented to analysis out of 1,000 licenses distributed, though the study doesn't specify how many identified as neurodiverse. While the 90 percent confidence for satisfaction falls below typical academic standards, the stronger finding for likelihood to recommend suggests a meaningful difference.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;These experiences echo some personal accounts online from autistic and ADHD users who describe AI as providing scaffolding for their writing and executive function needs. Some users also find value in having help decoding social subtext in workplace communications or suggesting appropriate professional language.&lt;/p&gt;
&lt;h2&gt;Beyond traditional accommodations&lt;/h2&gt;
&lt;p&gt;Even with what appears to be generally positive reviews of AI assistants for neurodivergent people who responded to the study, there's still plenty of room for nuanced takes on the overall potential of AI language models. The Register reported on the same study Thursday, emphasizing a lack of clear productivity gains and issues with Excel and PowerPoint outputs found by the researchers. The accessibility findings show the impact of AI from a different angle—one that Silicon Valley executives racing for flashy investment-attracting concepts like "superintelligence" might not consider as often.&lt;/p&gt;
&lt;p&gt;The disconnect between AI's promised productivity revolution and its actual impact might reveal a fundamental misunderstanding about where these tools excel. Traditional productivity gains require AI to outperform humans at tasks we're already good at—a high bar that current technology struggles to clear consistently. But for accessibility, AI doesn't need to be perfect; it just needs to bridge gaps that would otherwise exclude people entirely. The difference between writing a report 20 percent faster and being able to write a report at all represents two entirely different value propositions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For people with dyslexia in any setting, AI assistants might serve as writing aids that go beyond traditional spell-checkers, potentially helping with sentence structure and organizing thoughts without requiring specialized software. People with ADHD might be able to use these tools as executive function support, helping break down complex tasks and organize scattered thoughts.&lt;/p&gt;
&lt;p&gt;Some users report using AI to overcome procrastination and create structure as transformative for managing ADHD symptoms. "ChatGPT can help us hash things out so that we feel more prepared, comfortable, and confident in communicating with others," a reader named Lena&amp;nbsp;told ADDitude magazine.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For those with visual impairments, language models can summarize visual content and reformat information. Tools like ChatGPT's voice mode with video and Be My Eyes allow a machine to describe real-world visual scenes in ways that were impossible just a few years ago.&lt;/p&gt;
&lt;p&gt;AI language tools may be providing unofficial stealth accommodations for students—support that doesn't require formal diagnosis, workplace disclosure, or special equipment. Yet this informal support system comes with its own risks. Language models do confabulate—the UK Department for Business and Trade study found 22 percent of users identified false information in AI outputs—which could be particularly harmful for users relying on them for essential support.&lt;/p&gt;
&lt;h2&gt;When AI assistance becomes dependence&lt;/h2&gt;
&lt;p&gt;Beyond the workplace, the drawbacks may have a particular impact on students who use the technology. The authors of a 2025 study on students with disabilities using generative AI cautioned, "Key concerns students with disabilities had included the inaccuracy of AI answers, risks to academic integrity, and subscription cost barriers," they wrote. Students in that study had ADHD, dyslexia, dyspraxia, and autism, with ChatGPT being the most commonly used tool.&lt;/p&gt;
&lt;p&gt;Mistakes in AI outputs are especially pernicious because, due to grandiose visions of near-term AI technology, some people think today's AI assistants can perform tasks that are actually far outside their scope. As research on blind users' experiences suggested, people develop complex (sometimes flawed) mental models of how these tools work, showing the need for higher awareness of AI language model drawbacks among the general public.&lt;/p&gt;
&lt;p&gt;For the UK government employees who participated in the initial study, these questions moved from theoretical to immediate when the pilot ended in December 2024. After that time, many participants reported difficulty readjusting to work without AI assistance—particularly those with disabilities who had come to rely on the accessibility benefits. The department hasn't announced the next steps, leaving users in limbo. When participants report difficulty readjusting to work without AI while productivity gains remain marginal, accessibility emerges as potentially the first AI application with irreplaceable value.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/09/study-finds-neurodiverse-workers-more-satisfied-with-ai-assistants/</guid><pubDate>Tue, 09 Sep 2025 11:08:44 +0000</pubDate></item><item><title>The Download: meet our AI innovators, and what happens when therapists use AI covertly (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/09/1123447/the-download-meet-our-ai-innovators-and-what-happens-when-therapists-use-ai-covertly/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Meet the AI honorees on our 35 Innovators Under 35 list for 2025&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Each year, we select 35 outstanding individuals under the age of 35 who are using technology to tackle tough problems in their respective fields.&lt;/p&gt;&lt;p&gt;Our AI honorees include people who steer model development at Silicon Valley’s biggest tech firms and academic researchers who develop new techniques to improve AI’s performance.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Check out all of our AI innovators &lt;strong&gt;here&lt;/strong&gt;&lt;strong&gt;, and the full list—including our innovator of the year—&lt;/strong&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How Yichao “Peak” Ji became a global AI app hitmaker&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;When Yichao Ji—also known as “Peak”—appeared in a launch video for Manus in March, he didn’t expect it to go viral. Speaking in fluent English, the 32-year-old introduced the AI agent built by Chinese startup Butterfly Effect, where he serves as chief scientist.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The video was not an elaborate production but something about Ji’s delivery, and the vision behind the product, cut through the noise. The product, then still an early preview available only through invite codes, spread across the Chinese internet to the world in a matter of days. Within a week of its debut, Manus had attracted a waiting list of around 2 million people.&lt;/p&gt;&lt;p&gt;Despite his relative youth, Ji has over a decade of experience building products that merge technical complexity with real-world usability. That earned him credibility—and put him at the forefront of a rising class of Chinese technologists with global ambitions. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Help! My therapist is secretly using ChatGPT&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;In Silicon Valley’s imagined future, AI models are so empathetic that we’ll use them as therapists. They’ll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn’t subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time.&lt;/p&gt;&lt;p&gt;As the writer of the story, Laurie Clarke, points out, it’s not a total pipe dream that AI could be therapeutically useful. But the secretive use by therapists of AI models that are not vetted for mental health is something very different. James O’Donnell, our senior AI reporter, had a conversation with Clarke to hear more about what she found.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   

 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What’s next in tech: the breakthroughs that matter&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Some technologies reshape industries, whether we’re ready or not.&lt;/p&gt;&lt;p&gt;Join us for our next LinkedIn Live event on September 10 as our editorial team explores the breakthroughs defining this moment and the ones on the horizon that demand our attention.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;From quantum computing to humanoid robotics, AI agents to climate tech, we’ll explore the innovations that excite us, the challenges they may bring, and why they’re worth watching now. It kicks off at 12.30pm ET tomorrow—register here to join us.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The US is abandoning its international push against disinformation&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;The State Department will no longer collaborate with Europe to combat malicious information spread by foreign governments. (FT $)&lt;br /&gt;+ &lt;em&gt;It comes as Russia is increasing its efforts to interfere overseas. &lt;/em&gt;(NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The judge overseeing Anthropic’s copyright case isn’t happy&lt;/strong&gt;&lt;br /&gt;Judge William Alsup says a $1.5 billion out-of-court settlement may not be in the authors' best interests. (Bloomberg $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;3 WhatsApp’s former head of security is suing Meta&lt;/strong&gt;&lt;br /&gt;Attaullah Baig is accusing the company of failing to protect user data. (WP $)&lt;br /&gt;+ &lt;em&gt;He claims he uncovered systemic security failures, but was ignored. &lt;/em&gt;(Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Meta maintains that Baig was dismissed for poor performance, not whistleblowing. &lt;/em&gt;(NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 DOGE’s acting head is urging the US government to start hiring again&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Following months of widespread firings and resignations. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;How DOGE wreaked havoc in Social Security. &lt;/em&gt;(ProPublica)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 OpenAI is weighing up leaving California&lt;/strong&gt;&lt;br /&gt;It’s worried that state regulators could derail its efforts to convert to a for-profit entity. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Rival Anthropic is backing California governor Gavin Newsom’s AI bill. &lt;/em&gt;(Politico)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 ICE spends millions on facial recognition tech&lt;br /&gt;&lt;/strong&gt;In an effort to pinpoint people it suspects have assaulted officers. (404 Media)&lt;br /&gt;+ &lt;em&gt;The Supreme Court has given ICE the go-ahead to target people based on race. &lt;/em&gt;(Vox)&lt;br /&gt;+ &lt;em&gt;ICE directors were told to triple their daily arrests for undocumented immigrants. &lt;/em&gt;(NY Mag $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;7 AI researchers are training AI to replace them&lt;/strong&gt;&lt;br /&gt;They’re recording every detail of their working days to help AI grasp their jobs. (The Information $)&lt;br /&gt;+ &lt;em&gt;People are worried that AI will take everyone’s jobs. We’ve been here before. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 What comes after the smartphone?&lt;br /&gt;&lt;/strong&gt;The rise of AI agents means we may not be staring at glass slabs forever. (NYT $)&lt;br /&gt;+ &lt;em&gt;What’s next for smart glasses. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Social media’s obsession with ‘locking in’ needs to die&lt;/strong&gt;&lt;br /&gt;Hustle culture and maximizing productivity at all costs are the aims of the game. (Insider $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 What it’s like to receive a massage from a robot&lt;/strong&gt;&lt;br /&gt;While it may not be quite as relaxing, it’s relatively cheap. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Will we ever trust robots? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“It was hell on Earth.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Duncan Okindo, who was enslaved in a Myanmar cyberscam compound and beaten for missing his targets, tells the Guardian about his harrowing experience.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123449" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/image_1e647b.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;AI means the end of internet search as we’ve known it&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We all know what it means, colloquially, to google something. You pop a few words in a search box and in return get a list of blue links to the most relevant results. Fundamentally, it’s just fetching information that’s already out there on the internet and showing it to you, in a structured way.&lt;/p&gt;&lt;p&gt;But all that is up for grabs. We are at a new inflection point. The biggest change to the way search engines deliver information to us since the 1990s is happening right now, thanks to generative AI.&lt;/p&gt;&lt;p&gt;Not everyone is excited for the change. Publishers are completely freaked out. And people are also worried about what these new LLM-powered results will mean for our fundamental shared reality. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Mat Honan&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Stephen King’s list of favorite movies doesn’t feature a whole lot of horror.&lt;br /&gt;+ Tune into a breathtaking livestream of Earth, beamed live from the International Space Station.&lt;br /&gt;+ Rodent thumbnails are way more important than I gave them credit for 🐿️&lt;br /&gt;+ Mark our words, actor Wagner Moura is going to be the next big thing.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Meet the AI honorees on our 35 Innovators Under 35 list for 2025&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Each year, we select 35 outstanding individuals under the age of 35 who are using technology to tackle tough problems in their respective fields.&lt;/p&gt;&lt;p&gt;Our AI honorees include people who steer model development at Silicon Valley’s biggest tech firms and academic researchers who develop new techniques to improve AI’s performance.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Check out all of our AI innovators &lt;strong&gt;here&lt;/strong&gt;&lt;strong&gt;, and the full list—including our innovator of the year—&lt;/strong&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How Yichao “Peak” Ji became a global AI app hitmaker&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;When Yichao Ji—also known as “Peak”—appeared in a launch video for Manus in March, he didn’t expect it to go viral. Speaking in fluent English, the 32-year-old introduced the AI agent built by Chinese startup Butterfly Effect, where he serves as chief scientist.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The video was not an elaborate production but something about Ji’s delivery, and the vision behind the product, cut through the noise. The product, then still an early preview available only through invite codes, spread across the Chinese internet to the world in a matter of days. Within a week of its debut, Manus had attracted a waiting list of around 2 million people.&lt;/p&gt;&lt;p&gt;Despite his relative youth, Ji has over a decade of experience building products that merge technical complexity with real-world usability. That earned him credibility—and put him at the forefront of a rising class of Chinese technologists with global ambitions. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Help! My therapist is secretly using ChatGPT&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;In Silicon Valley’s imagined future, AI models are so empathetic that we’ll use them as therapists. They’ll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn’t subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time.&lt;/p&gt;&lt;p&gt;As the writer of the story, Laurie Clarke, points out, it’s not a total pipe dream that AI could be therapeutically useful. But the secretive use by therapists of AI models that are not vetted for mental health is something very different. James O’Donnell, our senior AI reporter, had a conversation with Clarke to hear more about what she found.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   

 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What’s next in tech: the breakthroughs that matter&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Some technologies reshape industries, whether we’re ready or not.&lt;/p&gt;&lt;p&gt;Join us for our next LinkedIn Live event on September 10 as our editorial team explores the breakthroughs defining this moment and the ones on the horizon that demand our attention.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;From quantum computing to humanoid robotics, AI agents to climate tech, we’ll explore the innovations that excite us, the challenges they may bring, and why they’re worth watching now. It kicks off at 12.30pm ET tomorrow—register here to join us.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The US is abandoning its international push against disinformation&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;The State Department will no longer collaborate with Europe to combat malicious information spread by foreign governments. (FT $)&lt;br /&gt;+ &lt;em&gt;It comes as Russia is increasing its efforts to interfere overseas. &lt;/em&gt;(NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The judge overseeing Anthropic’s copyright case isn’t happy&lt;/strong&gt;&lt;br /&gt;Judge William Alsup says a $1.5 billion out-of-court settlement may not be in the authors' best interests. (Bloomberg $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;3 WhatsApp’s former head of security is suing Meta&lt;/strong&gt;&lt;br /&gt;Attaullah Baig is accusing the company of failing to protect user data. (WP $)&lt;br /&gt;+ &lt;em&gt;He claims he uncovered systemic security failures, but was ignored. &lt;/em&gt;(Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Meta maintains that Baig was dismissed for poor performance, not whistleblowing. &lt;/em&gt;(NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 DOGE’s acting head is urging the US government to start hiring again&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Following months of widespread firings and resignations. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;How DOGE wreaked havoc in Social Security. &lt;/em&gt;(ProPublica)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 OpenAI is weighing up leaving California&lt;/strong&gt;&lt;br /&gt;It’s worried that state regulators could derail its efforts to convert to a for-profit entity. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Rival Anthropic is backing California governor Gavin Newsom’s AI bill. &lt;/em&gt;(Politico)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 ICE spends millions on facial recognition tech&lt;br /&gt;&lt;/strong&gt;In an effort to pinpoint people it suspects have assaulted officers. (404 Media)&lt;br /&gt;+ &lt;em&gt;The Supreme Court has given ICE the go-ahead to target people based on race. &lt;/em&gt;(Vox)&lt;br /&gt;+ &lt;em&gt;ICE directors were told to triple their daily arrests for undocumented immigrants. &lt;/em&gt;(NY Mag $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;7 AI researchers are training AI to replace them&lt;/strong&gt;&lt;br /&gt;They’re recording every detail of their working days to help AI grasp their jobs. (The Information $)&lt;br /&gt;+ &lt;em&gt;People are worried that AI will take everyone’s jobs. We’ve been here before. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 What comes after the smartphone?&lt;br /&gt;&lt;/strong&gt;The rise of AI agents means we may not be staring at glass slabs forever. (NYT $)&lt;br /&gt;+ &lt;em&gt;What’s next for smart glasses. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Social media’s obsession with ‘locking in’ needs to die&lt;/strong&gt;&lt;br /&gt;Hustle culture and maximizing productivity at all costs are the aims of the game. (Insider $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 What it’s like to receive a massage from a robot&lt;/strong&gt;&lt;br /&gt;While it may not be quite as relaxing, it’s relatively cheap. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Will we ever trust robots? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“It was hell on Earth.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Duncan Okindo, who was enslaved in a Myanmar cyberscam compound and beaten for missing his targets, tells the Guardian about his harrowing experience.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123449" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/image_1e647b.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;AI means the end of internet search as we’ve known it&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We all know what it means, colloquially, to google something. You pop a few words in a search box and in return get a list of blue links to the most relevant results. Fundamentally, it’s just fetching information that’s already out there on the internet and showing it to you, in a structured way.&lt;/p&gt;&lt;p&gt;But all that is up for grabs. We are at a new inflection point. The biggest change to the way search engines deliver information to us since the 1990s is happening right now, thanks to generative AI.&lt;/p&gt;&lt;p&gt;Not everyone is excited for the change. Publishers are completely freaked out. And people are also worried about what these new LLM-powered results will mean for our fundamental shared reality. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Mat Honan&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Stephen King’s list of favorite movies doesn’t feature a whole lot of horror.&lt;br /&gt;+ Tune into a breathtaking livestream of Earth, beamed live from the International Space Station.&lt;br /&gt;+ Rodent thumbnails are way more important than I gave them credit for 🐿️&lt;br /&gt;+ Mark our words, actor Wagner Moura is going to be the next big thing.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/09/1123447/the-download-meet-our-ai-innovators-and-what-happens-when-therapists-use-ai-covertly/</guid><pubDate>Tue, 09 Sep 2025 12:10:00 +0000</pubDate></item><item><title>What is Mistral AI? Everything to know about the OpenAI competitor (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/what-is-mistral-ai-everything-to-know-about-the-openai-competitor/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219786590.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Mistral AI, the French company that develops the AI chatbot Le Chat and several foundational large language models, is considered one of France’s most promising tech startups and is arguably the only European company that could compete with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Go and download Le Chat, which is made by Mistral, rather than ChatGPT by OpenAI — or something else,” French president Emmanuel Macron said in a TV interview ahead of the AI Action Summit in Paris in February 2025.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a significant step up from its June 2024 valuation of $6 billion, Mistral is now valued at €11.7 billion (approximately $13.8 billion) following a Series C funding round led by Dutch semiconductor company ASML, which invested €1.3 billion (approximately $1.5 billion) in September, alongside signing a new strategic partnership with the AI company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ASML’s interest in having its clients benefit from its collaboration is an important milestone for Mistral. While the French company describes itself as “the world’s greenest and leading independent AI lab,” it is still not as well known as its biggest competitors. &amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-mistral-ai"&gt;What is Mistral AI?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral AI, which offers some open source AI models, has raised significant funding since its creation in 2023, with the ambition to “put frontier AI in the hands of everyone.” While this isn’t a direct jab at OpenAI, the slogan is meant to highlight the company’s openness versus OpenAI’s more recent, closed source take at developing AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral’s chatbot Le Chat is available on iOS and Android, reaching 1 million downloads in the two weeks following its mobile release and grabbing France’s top spot for free downloads on the iOS App Store.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July 2025, Mistral AI updated Le Chat with new features that bring it closer to rival full-stack AI chatbots: a new “deep research” mode, native multilingual reasoning, and advanced image editing. This update also added Projects, which lets users group chats, documents, and ideas into focused spaces.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As of September 2025, Le Chat can remember previous conversations thanks to a feature called Memories.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is in addition to Mistral AI’s suite of models:&amp;nbsp;&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Mistral Large 2, the primary large language model replacing Mistral Large.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Pixtral Large, unveiled in 2024 as a new addition to the Pixtral family of multimodal models.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Magistral, its first family of reasoning models, launched in June 2025.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mistral Medium 3, released in May 2025 with the promise of providing efficiency without compromising performance, meant for coding and STEM tasks.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Voxtral, Mistral’s first open source AI audio model, released in July 2025.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Devstral, an AI model designed for coding and openly available under the Apache 2.0 license, meaning it can be used commercially without restriction.

&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Codestral, an earlier generative AI model for code whose license banned commercial applications.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;“Les Ministraux,” a family of models optimized for edge devices such as phones.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mistral Saba, focused on Arabic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;In March 2025, the company introduced Mistral OCR, an optical character-recognition API that can turn any PDF into a text file to make it easier for AI models to ingest.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In June 2025, Mistral AI also released a vibe-coding client, Mistral Code, to compete with Windsurf, Anysphere’s Cursor, and GitHub Copilot.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-who-are-mistral-ai-s-founders"&gt;Who are Mistral AI’s founders?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral AI’s three founders&amp;nbsp;share a background in AI research at major U.S. tech companies that have operations in Paris. Its CEO Arthur Mensch used to work at Google’s DeepMind; CTO Timothée Lacroix and chief scientist officer Guillaume Lample are former Meta staffers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral’s co-founding advisers include Jean-Charles Samuelian-Werve (also a board member) and Charles Gorintin from health insurance startup Alan. Former digital minister Cédric O is also an adviser to the company, a fact that has caused persistent controversy due to his previous role.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-are-mistral-s-models-open-source"&gt;Are Mistral’s models open source?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Not all of them. Mistral differentiates its premier models, whose weights are not available for commercial purposes, from its free models, for which it provides weights under the Apache 2.0 license.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Free models include research models such as Mistral NeMo, built in collaboration with Nvidia which the startup open sourced in July 2024.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-mistral-make-money"&gt;How does Mistral make money?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While many of Mistral AI’s offerings are free or now have free tiers, Le Chat also has paid tiers. Introduced in February 2025, Le Chat’s Pro subscription costs $14.99 a month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the B2B front, Mistral AI monetizes its premier models through APIs with usage-based pricing. Enterprises can also license these models, and the company likely also generates a significant share of its revenue from its strategic partnerships, some of which it highlighted during the Paris AI Summit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, however, Mistral AI’s revenue is reportedly in the eight-digit range, according to multiple sources.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-partnerships-has-mistral-ai-closed"&gt;What partnerships has Mistral AI closed?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In 2024, Mistral AI signed a deal with Microsoft that included a €15 million investment and a strategic partnership for distributing the French company’s AI models through Microsoft’s Azure platform. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The U.K.’s Competition and Markets Authority (CMA) swiftly concluded that the deal didn’t qualify for investigation due to its small size, though the deal sparked some criticism in the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In January 2025, Mistral signed a deal with press agency Agence France-Presse (AFP) to let Le Chat query the AFP’s entire text archive dating back to 1983.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral AI also secured strategic partnerships with France’s army and job agency, Luxembourg, shipping giant CMA, German defense tech startup Helsing, IBM, Orange, and Stellantis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May 2025, Mistral said it would participate in the creation of an AI Campus in the Paris region, as part of a joint venture with UAE-investment firm MGX, NVIDIA, and France’s state-owned investment bank Bpifrance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In June 2025, Mistral said it would launch a European platform dedicated to AI and powered by Nvidia processors, Mistral Compute, in 2026. The initiative was hailed as “historic” by Macron, who shared the stage with Mensch and Nvidia CEO Jensen Huang at the VivaTech conference shortly after the announcement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July 2025, Mistral launched AI for Citizens, an initiative that the company claimed could “help States and public institutions strategically harness AI for their people by transforming public services.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In September 2025, Mistral and chip company ASML struck a partnership “to explore the use of AI models across ASML’s product portfolio as well as research, development and operations.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-enterprise-features-has-mistral-ai-developed"&gt;&lt;strong&gt;What enterprise features has Mistral AI developed?&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In May 2025, Mistral AI released the Mistral Agents API to “empower enterprises to use AI in more practical and impactful ways,” according to its Head of Developer Relations, Sophia Yang.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In September 2025, the company unveiled a revamped Connectors directory, showcasing Le Chat’s integrations with some 20 enterprise tools, including Asana, Atlassian, Box, Google Drive, Notion, and Zapier, as well as emails and calendars; and soon, Databricks and Snowflake.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-much-funding-has-mistral-ai-raised-to-date"&gt;&lt;strong&gt;How much funding has Mistral AI raised to date?&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;By February 2025, Mistral AI had raised a total of around €1 billion, some of which was debt financing. The money was raised across several equity rounds conducted in close succession.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In June 2023, just one month after being founded, Mistral AI raised a record $113 million seed round led by Lightspeed Venture Partners. Sources at the time said the seed round, Europe’s largest ever, valued the startup at $260 million.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other investors in that round included Bpifrance, Eric Schmidt, Exor Ventures, First Minute Capital, Headline, JCDecaux Holding, La Famiglia, LocalGlobe, Motier Ventures, Rodolphe Saadé, Sofina, and Xavier Niel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Six months later, Mistral closed a €385 million Series A ($415 million at the time), at a reported valuation of $2 billion. The round was led by Andreessen Horowitz and saw participation from Lightspeed, as well as BNP Paribas, CMA-CGM, Conviction, Elad Gil, General Catalyst, and Salesforce.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s $16.3 million convertible investment in Mistral as part of a partnership announced in February 2024 was presented as a Series A extension, implying an unchanged valuation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In June 2024, Mistral raised €600 million (about $640 million) in a mix of equity and debt. The long-rumored round was led by General Catalyst at a $6 billion valuation, with notable investors including Cisco, IBM, Nvidia, and Samsung Venture Investment Corporation participating.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Mistral AI was rumored to be finalizing a €2 billion investment at a post-money valuation of $14 billion. This followed earlier reports that the company was in talks to raise $1 billion in equity from investors, who included Abu Dhabi’s MGX fund, as well as hundreds of millions of euros in debt.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On September 9, 2025, Mistral closed a €1.7 billion (about $2 billion) Series C round  led by ASML at a €11.7 billion (approximately $13.8 billion) valuation. According to the company, the round saw investments from existing backers DST Global, Andreessen Horowitz, Bpifrance, General Catalyst, Index Ventures, Lightspeed, and NVIDIA.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-is-mistral-ai-approaching-ai-regulation"&gt;How is Mistral AI approaching AI regulation?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Mensch was part of a group of European CEOs who signed an open letter in July 2025 urging Brussels to “stop the clock” for two years before key obligations of the EU Artificial Intelligence Act enter into force. The European Commission is sticking to its original timeline.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-could-a-mistral-ai-exit-look-like"&gt;What could a Mistral AI exit look like?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral is “not for sale,” Mensch said in January 2025 at the World Economic Forum in Davos. “Of course, [an IPO is] the plan.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This makes sense, given how much the startup has raised so far: Even a large sale may not provide high enough multiples for its investors, not to mention sovereignty concerns depending on the acquirer.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the only way to definitely squash persistent acquisition rumors — lately naming Apple — is to scale its revenue to levels that could even remotely justify its valuation. Either way, stay tuned.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was originally published on February 28, 2025, and will be regularly updated&lt;/em&gt;.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219786590.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Mistral AI, the French company that develops the AI chatbot Le Chat and several foundational large language models, is considered one of France’s most promising tech startups and is arguably the only European company that could compete with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Go and download Le Chat, which is made by Mistral, rather than ChatGPT by OpenAI — or something else,” French president Emmanuel Macron said in a TV interview ahead of the AI Action Summit in Paris in February 2025.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a significant step up from its June 2024 valuation of $6 billion, Mistral is now valued at €11.7 billion (approximately $13.8 billion) following a Series C funding round led by Dutch semiconductor company ASML, which invested €1.3 billion (approximately $1.5 billion) in September, alongside signing a new strategic partnership with the AI company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ASML’s interest in having its clients benefit from its collaboration is an important milestone for Mistral. While the French company describes itself as “the world’s greenest and leading independent AI lab,” it is still not as well known as its biggest competitors. &amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-mistral-ai"&gt;What is Mistral AI?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral AI, which offers some open source AI models, has raised significant funding since its creation in 2023, with the ambition to “put frontier AI in the hands of everyone.” While this isn’t a direct jab at OpenAI, the slogan is meant to highlight the company’s openness versus OpenAI’s more recent, closed source take at developing AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral’s chatbot Le Chat is available on iOS and Android, reaching 1 million downloads in the two weeks following its mobile release and grabbing France’s top spot for free downloads on the iOS App Store.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July 2025, Mistral AI updated Le Chat with new features that bring it closer to rival full-stack AI chatbots: a new “deep research” mode, native multilingual reasoning, and advanced image editing. This update also added Projects, which lets users group chats, documents, and ideas into focused spaces.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As of September 2025, Le Chat can remember previous conversations thanks to a feature called Memories.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is in addition to Mistral AI’s suite of models:&amp;nbsp;&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Mistral Large 2, the primary large language model replacing Mistral Large.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Pixtral Large, unveiled in 2024 as a new addition to the Pixtral family of multimodal models.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Magistral, its first family of reasoning models, launched in June 2025.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mistral Medium 3, released in May 2025 with the promise of providing efficiency without compromising performance, meant for coding and STEM tasks.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Voxtral, Mistral’s first open source AI audio model, released in July 2025.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Devstral, an AI model designed for coding and openly available under the Apache 2.0 license, meaning it can be used commercially without restriction.

&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Codestral, an earlier generative AI model for code whose license banned commercial applications.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;“Les Ministraux,” a family of models optimized for edge devices such as phones.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mistral Saba, focused on Arabic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;In March 2025, the company introduced Mistral OCR, an optical character-recognition API that can turn any PDF into a text file to make it easier for AI models to ingest.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In June 2025, Mistral AI also released a vibe-coding client, Mistral Code, to compete with Windsurf, Anysphere’s Cursor, and GitHub Copilot.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-who-are-mistral-ai-s-founders"&gt;Who are Mistral AI’s founders?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral AI’s three founders&amp;nbsp;share a background in AI research at major U.S. tech companies that have operations in Paris. Its CEO Arthur Mensch used to work at Google’s DeepMind; CTO Timothée Lacroix and chief scientist officer Guillaume Lample are former Meta staffers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral’s co-founding advisers include Jean-Charles Samuelian-Werve (also a board member) and Charles Gorintin from health insurance startup Alan. Former digital minister Cédric O is also an adviser to the company, a fact that has caused persistent controversy due to his previous role.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-are-mistral-s-models-open-source"&gt;Are Mistral’s models open source?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Not all of them. Mistral differentiates its premier models, whose weights are not available for commercial purposes, from its free models, for which it provides weights under the Apache 2.0 license.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Free models include research models such as Mistral NeMo, built in collaboration with Nvidia which the startup open sourced in July 2024.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-mistral-make-money"&gt;How does Mistral make money?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While many of Mistral AI’s offerings are free or now have free tiers, Le Chat also has paid tiers. Introduced in February 2025, Le Chat’s Pro subscription costs $14.99 a month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the B2B front, Mistral AI monetizes its premier models through APIs with usage-based pricing. Enterprises can also license these models, and the company likely also generates a significant share of its revenue from its strategic partnerships, some of which it highlighted during the Paris AI Summit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, however, Mistral AI’s revenue is reportedly in the eight-digit range, according to multiple sources.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-partnerships-has-mistral-ai-closed"&gt;What partnerships has Mistral AI closed?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In 2024, Mistral AI signed a deal with Microsoft that included a €15 million investment and a strategic partnership for distributing the French company’s AI models through Microsoft’s Azure platform. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The U.K.’s Competition and Markets Authority (CMA) swiftly concluded that the deal didn’t qualify for investigation due to its small size, though the deal sparked some criticism in the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In January 2025, Mistral signed a deal with press agency Agence France-Presse (AFP) to let Le Chat query the AFP’s entire text archive dating back to 1983.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral AI also secured strategic partnerships with France’s army and job agency, Luxembourg, shipping giant CMA, German defense tech startup Helsing, IBM, Orange, and Stellantis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May 2025, Mistral said it would participate in the creation of an AI Campus in the Paris region, as part of a joint venture with UAE-investment firm MGX, NVIDIA, and France’s state-owned investment bank Bpifrance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In June 2025, Mistral said it would launch a European platform dedicated to AI and powered by Nvidia processors, Mistral Compute, in 2026. The initiative was hailed as “historic” by Macron, who shared the stage with Mensch and Nvidia CEO Jensen Huang at the VivaTech conference shortly after the announcement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July 2025, Mistral launched AI for Citizens, an initiative that the company claimed could “help States and public institutions strategically harness AI for their people by transforming public services.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In September 2025, Mistral and chip company ASML struck a partnership “to explore the use of AI models across ASML’s product portfolio as well as research, development and operations.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-enterprise-features-has-mistral-ai-developed"&gt;&lt;strong&gt;What enterprise features has Mistral AI developed?&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In May 2025, Mistral AI released the Mistral Agents API to “empower enterprises to use AI in more practical and impactful ways,” according to its Head of Developer Relations, Sophia Yang.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In September 2025, the company unveiled a revamped Connectors directory, showcasing Le Chat’s integrations with some 20 enterprise tools, including Asana, Atlassian, Box, Google Drive, Notion, and Zapier, as well as emails and calendars; and soon, Databricks and Snowflake.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-much-funding-has-mistral-ai-raised-to-date"&gt;&lt;strong&gt;How much funding has Mistral AI raised to date?&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;By February 2025, Mistral AI had raised a total of around €1 billion, some of which was debt financing. The money was raised across several equity rounds conducted in close succession.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In June 2023, just one month after being founded, Mistral AI raised a record $113 million seed round led by Lightspeed Venture Partners. Sources at the time said the seed round, Europe’s largest ever, valued the startup at $260 million.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other investors in that round included Bpifrance, Eric Schmidt, Exor Ventures, First Minute Capital, Headline, JCDecaux Holding, La Famiglia, LocalGlobe, Motier Ventures, Rodolphe Saadé, Sofina, and Xavier Niel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Six months later, Mistral closed a €385 million Series A ($415 million at the time), at a reported valuation of $2 billion. The round was led by Andreessen Horowitz and saw participation from Lightspeed, as well as BNP Paribas, CMA-CGM, Conviction, Elad Gil, General Catalyst, and Salesforce.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s $16.3 million convertible investment in Mistral as part of a partnership announced in February 2024 was presented as a Series A extension, implying an unchanged valuation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In June 2024, Mistral raised €600 million (about $640 million) in a mix of equity and debt. The long-rumored round was led by General Catalyst at a $6 billion valuation, with notable investors including Cisco, IBM, Nvidia, and Samsung Venture Investment Corporation participating.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Mistral AI was rumored to be finalizing a €2 billion investment at a post-money valuation of $14 billion. This followed earlier reports that the company was in talks to raise $1 billion in equity from investors, who included Abu Dhabi’s MGX fund, as well as hundreds of millions of euros in debt.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On September 9, 2025, Mistral closed a €1.7 billion (about $2 billion) Series C round  led by ASML at a €11.7 billion (approximately $13.8 billion) valuation. According to the company, the round saw investments from existing backers DST Global, Andreessen Horowitz, Bpifrance, General Catalyst, Index Ventures, Lightspeed, and NVIDIA.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-is-mistral-ai-approaching-ai-regulation"&gt;How is Mistral AI approaching AI regulation?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Mensch was part of a group of European CEOs who signed an open letter in July 2025 urging Brussels to “stop the clock” for two years before key obligations of the EU Artificial Intelligence Act enter into force. The European Commission is sticking to its original timeline.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-could-a-mistral-ai-exit-look-like"&gt;What could a Mistral AI exit look like?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Mistral is “not for sale,” Mensch said in January 2025 at the World Economic Forum in Davos. “Of course, [an IPO is] the plan.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This makes sense, given how much the startup has raised so far: Even a large sale may not provide high enough multiples for its investors, not to mention sovereignty concerns depending on the acquirer.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the only way to definitely squash persistent acquisition rumors — lately naming Apple — is to scale its revenue to levels that could even remotely justify its valuation. Either way, stay tuned.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was originally published on February 28, 2025, and will be regularly updated&lt;/em&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/what-is-mistral-ai-everything-to-know-about-the-openai-competitor/</guid><pubDate>Tue, 09 Sep 2025 12:15:00 +0000</pubDate></item><item><title>Nuclearn gets $10.5M to help the nuclear industry embrace AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/nuclearn-gets-10-5m-to-help-the-nuclear-industry-embrace-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-124923144.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Companies that have dug deep into AI have fallen in love with nuclear power for its promise of 24/7 electricity. Meta, Google, and Microsoft have all made deals with startups or reactor operators. But does the nuclear industry love AI back?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yes, with caveats.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;No one is proposing to let an AI run a reactor, but power companies are increasingly interested in the technology’s potential to tighten things up on the business side, Bradley Fox, co-founder and CEO of Nuclearn, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fox and Jerrold Vincent started Nuclearn to capitalize on that interest. The company says its AI tools are being used in more than 65 nuclear reactors around the world.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It recently raised a $10.5 million Series A round led by Blue Bear Capital with participation from AZ-VC, Nucleation Capital, and SJF Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nuclearn got its start when the founders were working at the Palo Verde Nuclear Generating Station just west of Phoenix. They had been experimenting with ways to streamline various repetitive tasks first from a data science perspective, then with more advanced AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Soon, other reactors took note, Fox said. “Can you help us do the same thing you’re doing for Palo Verde but for my plant?” they asked him.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;That interest coincided with the COVID pandemic. “We both were kind of bored after work,” Fox said. “We’re like, hey, let’s work on a startup.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nuclearn has developed models trained on nuclear industry-specific terminology. The startup can train custom models for utilities and power providers that request it, and while its software runs in the cloud, it can also help reactors set up hardware on-site if their security protocols require it.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s software can generate routine documentation that reactor employees then review and sign off on.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Most AI in the industry now, the [Nuclear Regulatory Commission] considers it a tool. It’s the same way as if you’re going to use Excel or Mathematica or some type of engineering software,” Fox said. “Liability always falls with a person.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reactor operators can set thresholds for how much gets automated depending on their level of comfort and their confidence in how well the model can tackle the problem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If the model doesn’t know or if we’re unsure, based on the setting you select, it’ll send it back to the right people and get a double check,” Fox said. “We tell the customers, ‘Think of this as the junior employee.’”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-124923144.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Companies that have dug deep into AI have fallen in love with nuclear power for its promise of 24/7 electricity. Meta, Google, and Microsoft have all made deals with startups or reactor operators. But does the nuclear industry love AI back?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yes, with caveats.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;No one is proposing to let an AI run a reactor, but power companies are increasingly interested in the technology’s potential to tighten things up on the business side, Bradley Fox, co-founder and CEO of Nuclearn, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fox and Jerrold Vincent started Nuclearn to capitalize on that interest. The company says its AI tools are being used in more than 65 nuclear reactors around the world.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It recently raised a $10.5 million Series A round led by Blue Bear Capital with participation from AZ-VC, Nucleation Capital, and SJF Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nuclearn got its start when the founders were working at the Palo Verde Nuclear Generating Station just west of Phoenix. They had been experimenting with ways to streamline various repetitive tasks first from a data science perspective, then with more advanced AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Soon, other reactors took note, Fox said. “Can you help us do the same thing you’re doing for Palo Verde but for my plant?” they asked him.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;That interest coincided with the COVID pandemic. “We both were kind of bored after work,” Fox said. “We’re like, hey, let’s work on a startup.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nuclearn has developed models trained on nuclear industry-specific terminology. The startup can train custom models for utilities and power providers that request it, and while its software runs in the cloud, it can also help reactors set up hardware on-site if their security protocols require it.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s software can generate routine documentation that reactor employees then review and sign off on.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Most AI in the industry now, the [Nuclear Regulatory Commission] considers it a tool. It’s the same way as if you’re going to use Excel or Mathematica or some type of engineering software,” Fox said. “Liability always falls with a person.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reactor operators can set thresholds for how much gets automated depending on their level of comfort and their confidence in how well the model can tackle the problem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If the model doesn’t know or if we’re unsure, based on the setting you select, it’ll send it back to the right people and get a double check,” Fox said. “We tell the customers, ‘Think of this as the junior employee.’”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/nuclearn-gets-10-5m-to-help-the-nuclear-industry-embrace-ai/</guid><pubDate>Tue, 09 Sep 2025 12:45:00 +0000</pubDate></item><item><title>[NEW] Get Started Using Generative AI for Content Creation With ComfyUI and NVIDIA RTX AI PCs (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-wan-qwen-flux-krea-remix/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;ComfyUI — an open-source, node-based graphical interface for running and building generative AI workflows for content creation — published major updates this past month, including up to 40% performance improvements for NVIDIA RTX GPUs, and support for new AI models including Wan 2.2, Qwen-Image, FLUX.1 Krea [dev] and Hunyuan3D 2.1.&lt;/p&gt;
&lt;p&gt;NVIDIA also released NVIDIA TensorRT-optimized versions of popular diffusion models like Stable Diffusion 3.5 and FLUX.1 Kontext as NVIDIA NIM microservices, allowing users to run these models in ComfyUI up to 3x faster and with 50% less VRAM.&lt;/p&gt;
&lt;p&gt;Plus, an update to NVIDIA RTX Remix — a platform that lets modders remaster classic games — launched today, adding an advanced path-traced particle system that delivers stunning visuals to breathe new life into classic titles.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;ComfyUI v3.57 Gets a Performance Boost With RTX&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA has collaborated with ComfyUI to boost AI model performance by up to 40%. To put this in perspective, GPU generation upgrades typically only deliver a 20-30% performance boost.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84634"&gt;&lt;img alt="alt" class="wp-image-84634 size-large" height="792" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ComfyUI-v3.57-gets-a-performance-boost-with-RTX-1680x792.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84634"&gt;Measured on a GeForce RTX 5090 with Intel Core i9 14900K. All models run on ComfyUI using 20 steps at 1024×1024 resolution.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Developers interested in optimizing performance and efficiency of diffusion models on their apps can read more on how NVIDIA helps accelerate these workloads in the developer forum.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;State-of-the-Art AI Models, Accelerated by RTX&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Incredible models for AI content creation have been released in the last weeks, all of which are now available in ComfyUI.&lt;/p&gt;
&lt;p&gt;Wan 2.2 is a new video model that provides incredible quality and control for video generation on PCs. It’s the latest model from Wan AI — a creative AI platform offering an impressive lineup of AI models, including Text to Image, Text to Video, Image to Video and Speech to Video. GeForce RTX and NVIDIA RTX PRO GPUs are the only GPUs capable of running Wan 2.2 14B models in ComfyUI without major delays in output. Check out the example below created with the single prompt: “A robot is cracking an egg, but accidentally hits it outside the bowl.”&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-84648" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/Robot.gif" width="1280" /&gt;&lt;/p&gt;
&lt;p&gt;Qwen-Image is a new image generation foundation model from Alibaba that achieves significant advances in complex text rendering and precise image editing. It excels at rendering complex text, handling intricate editing and maintaining both semantic and visual accuracy in generated images. The model runs 7x faster on a GeForce RTX 5090 vs. Apple M3 Ultra.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84638"&gt;&lt;img alt="alt" class="size-large wp-image-84638" height="1000" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/Qwen-Image-1680x1000.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84638"&gt;Qwen-Image excels in text generation across numerous languages, as well as image generation.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Black Forest Labs’ new FLUX.1 Krea [dev] AI model is the open-weight version of Krea 1, offering strong performance and trained to generate more realistic, diverse images that don’t contain oversaturated textures. Black Forest Labs calls the model “opinionated,” as it offers a wide variety of diverse, visually interesting images. This model runs 8x faster on a GeForce RTX 5090 vs. Apple M3 Ultra.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84641"&gt;&lt;img alt="alt" class="size-large wp-image-84641" height="1400" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/FLUX.1-Krea-an-‘opinionated-text-to-image-model-1680x1400.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84641"&gt;Black Forest Labs’ new FLUX.1 Krea [dev] model offers more realistic, diverse imagery.&lt;/figcaption&gt;&lt;/figure&gt;Hunyuan3D 2.1 is a fully open-source, production-ready 3D generative system that transforms input images or text into high-fidelity 3D assets enriched with physically based rendering materials. Core components include a 3.3-billion-parameter model for shape generation and a 2-billion-parameter model for texture analysis to quickly generate more realistic materials. It all runs faster on Blackwell RTX GPUs.
&lt;figure class="wp-caption aligncenter" id="attachment_84644"&gt;&lt;img alt="alt" class="size-full wp-image-84644" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/Hunyuan3D.gif" width="2439" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84644"&gt;Go from image to 3D model quickly with Hunyuan3D 2.1.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Get Started With Advanced Visual Generative Techniques&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Visual generative AI is a powerful tool, but getting started can be difficult even for technical experts and learning to use more advanced techniques typically takes months.&lt;/p&gt;
&lt;p&gt;ComfyUI makes it easy to get started with advanced workflows by providing templates or preset nodes that achieve a specific task, like keeping a character constant throughout different generations, adjusting the light of an image or loading a fine-tuning. This allows even non-technical artists to easily use advanced AI workflows.&lt;/p&gt;
&lt;p&gt;These are 10 key techniques to get started with generative AI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Guide video generation by defining the start frame and end frame&lt;/b&gt;: Upload a start and end frame, as well as how a video clip should begin and end. Wan 2.2 can then generate a smooth, animated transition, filling the in-between frames to create a coherent animation. It’s ideal for making animations, scene shifts or defining poses.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Edit images with natural language&lt;/b&gt;: Use FLUX.1 [dev] KONTEXT to edit specific text sections of an image.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Upscale images or videos&lt;/b&gt;: Take image or video at a lower resolution and increase its resolution and detail quality by adding realistic, high-frequency details.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Control a&lt;/b&gt;&lt;b&gt;rea composition&lt;/b&gt;: Assume more granular control over image generation by controlling the arrangement and layout of visual elements within specific regions of an image.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Restyle images&lt;/b&gt;: Use FLUX Redux to create different variations of images while preserving core visual elements and details.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Tap&lt;/b&gt; &lt;b&gt;image-to-multiview-to-3D models&lt;/b&gt;: Use multiple images of an object captured from different angles to create a high-fidelity, textured 3D model.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Transform sound to video&lt;/b&gt;: Create video clips or animations directly from audio inputs such as speech, music or environmental sounds.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Control video trajectory&lt;/b&gt;: Automatically guide the motion of objects, camera or scenes within videos.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Edit images with inpainting&lt;/b&gt;: Fill in or alter missing or unwanted parts of a digital image in a visually seamless, contextually consistent way with surrounding areas.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Expand the canvas with outpainting&lt;/b&gt;: Generate new image content to extend the boundaries of an existing image or video footage, add detail to cropped sections or complete a scene.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Follow ComfyUI on X for updates to creative templates and workflows.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Expand Your ComfyUI Zone&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;ComfyUI plug-ins enable users to add generative AI workflows into their existing applications. The ComfyUI community has started building plug-ins for some of the top popular creative applications.&lt;/p&gt;
&lt;p&gt;The Adobe Photoshop plug-in complements Photoshop’s native Firefly models by allowing users to run their own flows and select specialized models for specific tasks. Local inference also enables unlimited generative fill with low latency.&lt;/p&gt;
&lt;p&gt;The Blender plug-in — featured in the NVIDIA AI Blueprint for 3D-guided generative AI — allows users to connect 2D and 3D workflows. Artists can use 3D scenes to control image generations or create textures in ComfyUI and apply them to separate 3D assets.&lt;/p&gt;
&lt;p&gt;The Foundry Nuke plug-in — similar to the Blender — enables a connection between 2D and 3D workflows so users don’t have to alt-tab and swap between applications.&lt;/p&gt;
&lt;p&gt;The Unreal Engine plug-in enables ComfyUI nodes directly in the Unreal Engine user interface to quickly create and refine textures for scenes using generative diffusion models. See the example below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-84654" height="1440" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/Unreal-Engine.gif" width="2560" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Run Hyper-Optimized Models for NVIDIA RTX GPUs in ComfyUI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The best way to use NVIDIA RTX GPUs is with the TensorRT library — a high-performance deep learning inference engine designed to squeeze maximum speed out of the Tensor Cores in NVIDIA RTX GPUs.&lt;/p&gt;
&lt;p&gt;NVIDIA has collaborated with the top AI labs to integrate TensorRT in their models, such as Black Forest Labs’ models and Stability AI’s models. These models are also available quantized — a compressed version of the network that uses 50-70% less VRAM and offers up to 2x faster inference while maintaining similar quality.&lt;/p&gt;
&lt;p&gt;TensorRT-optimized models can be run directly in ComfyUI through the TensorRT node, which currently supports SDXL, SD3 and SD3.5, as well as FLUX.1-dev and FLUX.1-schnell models. The node converts the AI model into a TensorRT-optimized model, and then generates a TensorRT-optimized engine for the user’s GPU — a map of how to execute that model with optimal efficiency for their particular hardware — providing significant speedups.&lt;/p&gt;
&lt;p&gt;Quantizing models, however, takes a bit more work. For users interested in running quantized and TensorRT-optimized models, NVIDIA offers preconfigured files in a simple container called a NIM microservice. Users can use the NIM node in ComfyUI to load these containers and use quantized versions of models like FLUX.1-dev, FLUX.1-schnell, FLUX.1 Kontext, SD3.5 Large and Microsoft TRELLIS.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Remix Update Adds a Path-Traced Particle System&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;A new RTX Remix update released today through the NVIDIA app, adding an advanced particle system that enables modders to enhance traditional fire and smoke effects, as well as more fantastical effects, like those in the video game &lt;i&gt;Portal&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;With RTX Remix, legacy particles from classic games could be interpreted as path-traced, enabling them to cast realistic light to enhance the appearance of many scenes. But ultimately, these particles were still over 20 years old, lacking detail, flair and fluid animations.&lt;/p&gt;
&lt;p&gt;RTX Remix’s new particles have physically accurate properties and can interact with a game’s lighting and other effects. This enables particles to collide, accurately move in response to wind and other forces, be reflected on surfaces, cast shadows and assign their own shadows.&lt;/p&gt;
&lt;p&gt;For a complete breakdown of the new particle system, read the GeForce article.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Join NVIDIA’s &lt;/i&gt;&lt;i&gt;Discord server&lt;/i&gt;&lt;i&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;ComfyUI — an open-source, node-based graphical interface for running and building generative AI workflows for content creation — published major updates this past month, including up to 40% performance improvements for NVIDIA RTX GPUs, and support for new AI models including Wan 2.2, Qwen-Image, FLUX.1 Krea [dev] and Hunyuan3D 2.1.&lt;/p&gt;
&lt;p&gt;NVIDIA also released NVIDIA TensorRT-optimized versions of popular diffusion models like Stable Diffusion 3.5 and FLUX.1 Kontext as NVIDIA NIM microservices, allowing users to run these models in ComfyUI up to 3x faster and with 50% less VRAM.&lt;/p&gt;
&lt;p&gt;Plus, an update to NVIDIA RTX Remix — a platform that lets modders remaster classic games — launched today, adding an advanced path-traced particle system that delivers stunning visuals to breathe new life into classic titles.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;ComfyUI v3.57 Gets a Performance Boost With RTX&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA has collaborated with ComfyUI to boost AI model performance by up to 40%. To put this in perspective, GPU generation upgrades typically only deliver a 20-30% performance boost.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84634"&gt;&lt;img alt="alt" class="wp-image-84634 size-large" height="792" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ComfyUI-v3.57-gets-a-performance-boost-with-RTX-1680x792.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84634"&gt;Measured on a GeForce RTX 5090 with Intel Core i9 14900K. All models run on ComfyUI using 20 steps at 1024×1024 resolution.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Developers interested in optimizing performance and efficiency of diffusion models on their apps can read more on how NVIDIA helps accelerate these workloads in the developer forum.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;State-of-the-Art AI Models, Accelerated by RTX&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Incredible models for AI content creation have been released in the last weeks, all of which are now available in ComfyUI.&lt;/p&gt;
&lt;p&gt;Wan 2.2 is a new video model that provides incredible quality and control for video generation on PCs. It’s the latest model from Wan AI — a creative AI platform offering an impressive lineup of AI models, including Text to Image, Text to Video, Image to Video and Speech to Video. GeForce RTX and NVIDIA RTX PRO GPUs are the only GPUs capable of running Wan 2.2 14B models in ComfyUI without major delays in output. Check out the example below created with the single prompt: “A robot is cracking an egg, but accidentally hits it outside the bowl.”&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-84648" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/Robot.gif" width="1280" /&gt;&lt;/p&gt;
&lt;p&gt;Qwen-Image is a new image generation foundation model from Alibaba that achieves significant advances in complex text rendering and precise image editing. It excels at rendering complex text, handling intricate editing and maintaining both semantic and visual accuracy in generated images. The model runs 7x faster on a GeForce RTX 5090 vs. Apple M3 Ultra.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84638"&gt;&lt;img alt="alt" class="size-large wp-image-84638" height="1000" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/Qwen-Image-1680x1000.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84638"&gt;Qwen-Image excels in text generation across numerous languages, as well as image generation.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Black Forest Labs’ new FLUX.1 Krea [dev] AI model is the open-weight version of Krea 1, offering strong performance and trained to generate more realistic, diverse images that don’t contain oversaturated textures. Black Forest Labs calls the model “opinionated,” as it offers a wide variety of diverse, visually interesting images. This model runs 8x faster on a GeForce RTX 5090 vs. Apple M3 Ultra.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_84641"&gt;&lt;img alt="alt" class="size-large wp-image-84641" height="1400" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/FLUX.1-Krea-an-‘opinionated-text-to-image-model-1680x1400.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84641"&gt;Black Forest Labs’ new FLUX.1 Krea [dev] model offers more realistic, diverse imagery.&lt;/figcaption&gt;&lt;/figure&gt;Hunyuan3D 2.1 is a fully open-source, production-ready 3D generative system that transforms input images or text into high-fidelity 3D assets enriched with physically based rendering materials. Core components include a 3.3-billion-parameter model for shape generation and a 2-billion-parameter model for texture analysis to quickly generate more realistic materials. It all runs faster on Blackwell RTX GPUs.
&lt;figure class="wp-caption aligncenter" id="attachment_84644"&gt;&lt;img alt="alt" class="size-full wp-image-84644" height="1080" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/Hunyuan3D.gif" width="2439" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-84644"&gt;Go from image to 3D model quickly with Hunyuan3D 2.1.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Get Started With Advanced Visual Generative Techniques&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Visual generative AI is a powerful tool, but getting started can be difficult even for technical experts and learning to use more advanced techniques typically takes months.&lt;/p&gt;
&lt;p&gt;ComfyUI makes it easy to get started with advanced workflows by providing templates or preset nodes that achieve a specific task, like keeping a character constant throughout different generations, adjusting the light of an image or loading a fine-tuning. This allows even non-technical artists to easily use advanced AI workflows.&lt;/p&gt;
&lt;p&gt;These are 10 key techniques to get started with generative AI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Guide video generation by defining the start frame and end frame&lt;/b&gt;: Upload a start and end frame, as well as how a video clip should begin and end. Wan 2.2 can then generate a smooth, animated transition, filling the in-between frames to create a coherent animation. It’s ideal for making animations, scene shifts or defining poses.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Edit images with natural language&lt;/b&gt;: Use FLUX.1 [dev] KONTEXT to edit specific text sections of an image.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Upscale images or videos&lt;/b&gt;: Take image or video at a lower resolution and increase its resolution and detail quality by adding realistic, high-frequency details.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Control a&lt;/b&gt;&lt;b&gt;rea composition&lt;/b&gt;: Assume more granular control over image generation by controlling the arrangement and layout of visual elements within specific regions of an image.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Restyle images&lt;/b&gt;: Use FLUX Redux to create different variations of images while preserving core visual elements and details.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Tap&lt;/b&gt; &lt;b&gt;image-to-multiview-to-3D models&lt;/b&gt;: Use multiple images of an object captured from different angles to create a high-fidelity, textured 3D model.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Transform sound to video&lt;/b&gt;: Create video clips or animations directly from audio inputs such as speech, music or environmental sounds.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Control video trajectory&lt;/b&gt;: Automatically guide the motion of objects, camera or scenes within videos.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Edit images with inpainting&lt;/b&gt;: Fill in or alter missing or unwanted parts of a digital image in a visually seamless, contextually consistent way with surrounding areas.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Expand the canvas with outpainting&lt;/b&gt;: Generate new image content to extend the boundaries of an existing image or video footage, add detail to cropped sections or complete a scene.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Follow ComfyUI on X for updates to creative templates and workflows.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Expand Your ComfyUI Zone&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;ComfyUI plug-ins enable users to add generative AI workflows into their existing applications. The ComfyUI community has started building plug-ins for some of the top popular creative applications.&lt;/p&gt;
&lt;p&gt;The Adobe Photoshop plug-in complements Photoshop’s native Firefly models by allowing users to run their own flows and select specialized models for specific tasks. Local inference also enables unlimited generative fill with low latency.&lt;/p&gt;
&lt;p&gt;The Blender plug-in — featured in the NVIDIA AI Blueprint for 3D-guided generative AI — allows users to connect 2D and 3D workflows. Artists can use 3D scenes to control image generations or create textures in ComfyUI and apply them to separate 3D assets.&lt;/p&gt;
&lt;p&gt;The Foundry Nuke plug-in — similar to the Blender — enables a connection between 2D and 3D workflows so users don’t have to alt-tab and swap between applications.&lt;/p&gt;
&lt;p&gt;The Unreal Engine plug-in enables ComfyUI nodes directly in the Unreal Engine user interface to quickly create and refine textures for scenes using generative diffusion models. See the example below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-84654" height="1440" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/Unreal-Engine.gif" width="2560" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Run Hyper-Optimized Models for NVIDIA RTX GPUs in ComfyUI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The best way to use NVIDIA RTX GPUs is with the TensorRT library — a high-performance deep learning inference engine designed to squeeze maximum speed out of the Tensor Cores in NVIDIA RTX GPUs.&lt;/p&gt;
&lt;p&gt;NVIDIA has collaborated with the top AI labs to integrate TensorRT in their models, such as Black Forest Labs’ models and Stability AI’s models. These models are also available quantized — a compressed version of the network that uses 50-70% less VRAM and offers up to 2x faster inference while maintaining similar quality.&lt;/p&gt;
&lt;p&gt;TensorRT-optimized models can be run directly in ComfyUI through the TensorRT node, which currently supports SDXL, SD3 and SD3.5, as well as FLUX.1-dev and FLUX.1-schnell models. The node converts the AI model into a TensorRT-optimized model, and then generates a TensorRT-optimized engine for the user’s GPU — a map of how to execute that model with optimal efficiency for their particular hardware — providing significant speedups.&lt;/p&gt;
&lt;p&gt;Quantizing models, however, takes a bit more work. For users interested in running quantized and TensorRT-optimized models, NVIDIA offers preconfigured files in a simple container called a NIM microservice. Users can use the NIM node in ComfyUI to load these containers and use quantized versions of models like FLUX.1-dev, FLUX.1-schnell, FLUX.1 Kontext, SD3.5 Large and Microsoft TRELLIS.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Remix Update Adds a Path-Traced Particle System&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;A new RTX Remix update released today through the NVIDIA app, adding an advanced particle system that enables modders to enhance traditional fire and smoke effects, as well as more fantastical effects, like those in the video game &lt;i&gt;Portal&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;With RTX Remix, legacy particles from classic games could be interpreted as path-traced, enabling them to cast realistic light to enhance the appearance of many scenes. But ultimately, these particles were still over 20 years old, lacking detail, flair and fluid animations.&lt;/p&gt;
&lt;p&gt;RTX Remix’s new particles have physically accurate properties and can interact with a game’s lighting and other effects. This enables particles to collide, accurately move in response to wind and other forces, be reflected on surfaces, cast shadows and assign their own shadows.&lt;/p&gt;
&lt;p&gt;For a complete breakdown of the new particle system, read the GeForce article.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Join NVIDIA’s &lt;/i&gt;&lt;i&gt;Discord server&lt;/i&gt;&lt;i&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-wan-qwen-flux-krea-remix/</guid><pubDate>Tue, 09 Sep 2025 13:00:26 +0000</pubDate></item><item><title>[NEW] Breaking the networking wall in AI infrastructure (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Two white line icons on a gradient background transitioning from blue to pink. From left to right: icon representing a set of gears; an icon representing three connected nodes each containing a user icon." class="wp-image-1148762" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&amp;nbsp;utilization&amp;nbsp;and overall efficiency,&amp;nbsp;ultimately preventing&amp;nbsp;infrastructure from reaching its full potential&amp;nbsp;despite enormous investments.&amp;nbsp;At the&amp;nbsp;core&amp;nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.&lt;/p&gt;



&lt;p&gt;Datacenters typically deploy two types of physical cables&amp;nbsp;for&amp;nbsp;communication between&amp;nbsp;GPUs.&amp;nbsp;Traditional copper links&amp;nbsp;are power-efficient and&amp;nbsp;reliable,&amp;nbsp;but&amp;nbsp;limited to&amp;nbsp;very short&amp;nbsp;distances&amp;nbsp;( 2 meters)&amp;nbsp;that&amp;nbsp;restrict their use&amp;nbsp;to within a single&amp;nbsp;GPU&amp;nbsp;rack. Optical&amp;nbsp;fiber&amp;nbsp;links&amp;nbsp;can&amp;nbsp;reach&amp;nbsp;tens of meters,&amp;nbsp;but&amp;nbsp;they&amp;nbsp;consume far more&amp;nbsp;power&amp;nbsp;and fail up to 100 times&amp;nbsp;as often as&amp;nbsp;copper. A&amp;nbsp;team working across&amp;nbsp;Microsoft&amp;nbsp;aims&amp;nbsp;to&amp;nbsp;resolve&amp;nbsp;this trade-off&amp;nbsp;by&amp;nbsp;developing&amp;nbsp;MOSAIC,&amp;nbsp;a novel optical link technology&amp;nbsp;that&amp;nbsp;can provide&amp;nbsp;low power and cost, high reliability, and long reach (up to 50 meters)&amp;nbsp;&lt;em&gt;simultaneously&lt;/em&gt;.&amp;nbsp;This approach leverages a hardware-system co-design and adopts&amp;nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&amp;nbsp;microLEDs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The fundamental trade-off&amp;nbsp;among&amp;nbsp;power, reliability, and reach&amp;nbsp;stems from&amp;nbsp;the&amp;nbsp;&lt;em&gt;narrow-and-fast&lt;/em&gt;&amp;nbsp;architecture&amp;nbsp;deployed&amp;nbsp;in&amp;nbsp;today’s copper and optical links,&amp;nbsp;comprising&amp;nbsp;a few channels&amp;nbsp;operating&amp;nbsp;at&amp;nbsp;very high&amp;nbsp;data rates. For example,&amp;nbsp;an&amp;nbsp;800 Gbps link&amp;nbsp;consists of eight 100 Gbps channels.&amp;nbsp;With&amp;nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&amp;nbsp;With optical&amp;nbsp;links,&amp;nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&amp;nbsp;complex electronics&amp;nbsp;to compensate for transmission impairments. These challenges&amp;nbsp;grow&amp;nbsp;as speeds increase&amp;nbsp;with&amp;nbsp;every&amp;nbsp;generation&amp;nbsp;of networks.&amp;nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&amp;nbsp;systems&amp;nbsp;margins&amp;nbsp;and increasing failure rates.&amp;nbsp;&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: AI-POWERED EXPERIENCE&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft research copilot experience&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-copilot-experience"&gt;Discover more about research at Microsoft through our AI-powered experience&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;These limitations force systems designers to make unpleasant&amp;nbsp;choices,&amp;nbsp;limiting the scalability of AI infrastructure.&amp;nbsp;For example,&amp;nbsp;scale-up networks connecting AI accelerators at&amp;nbsp;multi-Tbps&amp;nbsp;bandwidth&amp;nbsp;typically&amp;nbsp;must&amp;nbsp;rely on&amp;nbsp;copper links&amp;nbsp;to meet&amp;nbsp;the&amp;nbsp;power budget,&amp;nbsp;requiring&amp;nbsp;ultra-dense racks that&amp;nbsp;consume&amp;nbsp;hundreds of kilowatts&amp;nbsp;&lt;em&gt;per rack&lt;/em&gt;. This creates significant challenges in cooling&amp;nbsp;and&amp;nbsp;mechanical design,&amp;nbsp;which constrain&amp;nbsp;the practical scale of these networks and end-to-end performance. This imbalance&amp;nbsp;ultimately&amp;nbsp;erects&amp;nbsp;a&amp;nbsp;&lt;em&gt;networking wall&lt;/em&gt;&amp;nbsp;akin&amp;nbsp;to the&amp;nbsp;&lt;em&gt;memory wall&lt;/em&gt;, in&amp;nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.&lt;/p&gt;



&lt;p class="has-text-align-left"&gt;A technology offering copper-like power efficiency and reliability over long distances can overcome this networking&amp;nbsp;wall,&amp;nbsp;enabling&amp;nbsp;multi-rack&amp;nbsp;scale-up domains and unlocking&amp;nbsp;new architectures. This is a highly active R&amp;amp;D area, with many candidate technologies currently being developed across the industry.&amp;nbsp;In&amp;nbsp;our recent&amp;nbsp;paper,&amp;nbsp;&lt;em&gt;“MOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs”&lt;/em&gt;, which received the Best Paper award at ACM SIGCOMM&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, we present&amp;nbsp;one such promising&amp;nbsp;approach&amp;nbsp;that is&amp;nbsp;the result of a multi-year collaboration between Microsoft Research,&amp;nbsp;Azure, and M365.&amp;nbsp;This&amp;nbsp;work is&amp;nbsp;centered around&amp;nbsp;an optical&amp;nbsp;wide-and-slow architecture, shifting from a small number of high-speed serial channels towards&amp;nbsp;hundreds of parallel low-speed channels.&amp;nbsp;This&amp;nbsp;would be impractical&amp;nbsp;to realize with today’s copper and optical technologies because of&amp;nbsp;i)&amp;nbsp;electromagnetic interference challenges in high-density copper cables and ii) the&amp;nbsp;high cost&amp;nbsp;and power consumption of lasers&amp;nbsp;in optical links,&amp;nbsp;as well as the increase in packaging complexity.&amp;nbsp;MOSAIC overcomes these issues by&amp;nbsp;leveraging&amp;nbsp;directly modulated&amp;nbsp;microLEDs, a technology originally developed for&amp;nbsp;screen&amp;nbsp;displays.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MicroLEDs&amp;nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&amp;nbsp;microns) and, due to their&amp;nbsp;small size,&amp;nbsp;they&amp;nbsp;can be modulated at several Gbps.&amp;nbsp;They&amp;nbsp;are manufactured in large arrays,&amp;nbsp;with over half a million&amp;nbsp;in a small physical footprint for high-resolution displays&amp;nbsp;like&amp;nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&amp;nbsp;microLED&amp;nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20×20&amp;nbsp;microLED&amp;nbsp;array, which can fit in less than 1 mm×1 mm&amp;nbsp;silicon&amp;nbsp;die.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MOSAIC’s&amp;nbsp;wide-and-slow&amp;nbsp;design&amp;nbsp;provides four core benefits.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Operating&amp;nbsp;at low speed improves power efficiency&amp;nbsp;by&amp;nbsp;eliminating&amp;nbsp;the need for&amp;nbsp;complex&amp;nbsp;electronics&amp;nbsp;and&amp;nbsp;reducing optical power requirements.&lt;/li&gt;



&lt;li&gt;By&amp;nbsp;leveraging&amp;nbsp;optical transmission (via&amp;nbsp;microLEDs),&amp;nbsp;MOSAIC&amp;nbsp;sidesteps&amp;nbsp;copper’s reach issues, supporting distances up to 50 meters,&amp;nbsp;or&amp;nbsp;&amp;gt; 10x&amp;nbsp;further&amp;nbsp;than copper.&lt;/li&gt;



&lt;li&gt;MicroLEDs’&amp;nbsp;simpler structure&amp;nbsp;and temperature insensitivity&amp;nbsp;make them more reliable than lasers. The parallel nature of&amp;nbsp;wide-and-slow&amp;nbsp;also&amp;nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;The&amp;nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&amp;nbsp;1.6&amp;nbsp;Tbps&amp;nbsp;or 3.2&amp;nbsp;Tbps) can be achieved by increasing the number of&amp;nbsp;channels and/or raising per-channel speed&amp;nbsp;(e.g., to 4-8 Gbps).&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Further,&amp;nbsp;MOSAIC is fully compatible with today’s pluggable transceivers’ form&amp;nbsp;factor&amp;nbsp;and it provides a drop-in replacement for today’s copper and optical cables, without requiring any changes to existing server and network infrastructure.&amp;nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&amp;nbsp;terminating&amp;nbsp;or inspecting the connection&amp;nbsp;and, hence,&amp;nbsp;it’s&amp;nbsp;fully compatible with today’s protocols (e.g.,&amp;nbsp;Ethernet, PCIe, CXL).&amp;nbsp;We are currently working with our suppliers to&amp;nbsp;productize&amp;nbsp;this technology and&amp;nbsp;scale&amp;nbsp;to mass production.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While&amp;nbsp;conceptually simple, realizing this architecture posed a few key challenges&amp;nbsp;across the stack, which&amp;nbsp;required&amp;nbsp;a multi-disciplinary team with&amp;nbsp;expertise&amp;nbsp;spanning across integrated photonics, lens design, optical transmission, and&amp;nbsp;analog&amp;nbsp;and digital design.&amp;nbsp;For example, using individual&amp;nbsp;fibers&amp;nbsp;per channel would be prohibitively complex and costly due to the&amp;nbsp;large number&amp;nbsp;of channels. We addressed this by employing imaging&amp;nbsp;fibers,&amp;nbsp;which are typically used for medical applications (e.g., endoscopy).&amp;nbsp;They&amp;nbsp;can support thousands of cores&amp;nbsp;per&amp;nbsp;fiber, enabling multiplexing&amp;nbsp;of&amp;nbsp;many channels within a single&amp;nbsp;fiber.&amp;nbsp;Also,&amp;nbsp;microLEDs&amp;nbsp;are a less pure light source&amp;nbsp;than lasers,&amp;nbsp;with&amp;nbsp;a larger beam shape (which complicates&amp;nbsp;fiber&amp;nbsp;coupling) and&amp;nbsp;a broader spectrum (which&amp;nbsp;degrades&amp;nbsp;fiber&amp;nbsp;transmission due to chromatic dispersion).&amp;nbsp;We tackled these issues through&amp;nbsp;a novel&amp;nbsp;microLED and&amp;nbsp;optical lens design,&amp;nbsp;and&amp;nbsp;a power-efficient&amp;nbsp;analog-only electronic back&amp;nbsp;end, which does not require any expensive digital signal processing.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Based on our current estimates, this approach can save&amp;nbsp;up to 68% of power, i.e., more&amp;nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&amp;nbsp;reaching into&amp;nbsp;the tens of millions, this translates to over 100MW of power savings per year,&amp;nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&amp;nbsp;to rethink&amp;nbsp;AI&amp;nbsp;infrastructure from network and cluster architectures to compute and memory designs.&lt;/p&gt;



&lt;p&gt;For example,&amp;nbsp;by&amp;nbsp;supporting&amp;nbsp;low-power,&amp;nbsp;high-bandwidth connectivity at long reach,&amp;nbsp;MOSAIC&amp;nbsp;removes the need for ultra-dense racks and&amp;nbsp;enables&amp;nbsp;novel network topologies, which would be impractical today. The resulting redesign could&amp;nbsp;reduce&amp;nbsp;resource fragmentation and&amp;nbsp;simplify&amp;nbsp;collective optimization.&amp;nbsp;Similarly,&amp;nbsp;on the&amp;nbsp;compute&amp;nbsp;front,&amp;nbsp;the ability&amp;nbsp;to&amp;nbsp;connect&amp;nbsp;silicon&amp;nbsp;dies at low power over long distances&amp;nbsp;could&amp;nbsp;enable&amp;nbsp;resource&amp;nbsp;disaggregation, shifting from today’s&amp;nbsp;large,&amp;nbsp;multi-die packages to&amp;nbsp;smaller, more cost-effective, ones.&amp;nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&amp;nbsp;GPU&amp;nbsp;memory&amp;nbsp;capacity and bandwidth,&amp;nbsp;while&amp;nbsp;facilitating&amp;nbsp;adoption of&amp;nbsp;novel memory technologies.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&amp;nbsp;possible future&amp;nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Two white line icons on a gradient background transitioning from blue to pink. From left to right: icon representing a set of gears; an icon representing three connected nodes each containing a user icon." class="wp-image-1148762" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&amp;nbsp;utilization&amp;nbsp;and overall efficiency,&amp;nbsp;ultimately preventing&amp;nbsp;infrastructure from reaching its full potential&amp;nbsp;despite enormous investments.&amp;nbsp;At the&amp;nbsp;core&amp;nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.&lt;/p&gt;



&lt;p&gt;Datacenters typically deploy two types of physical cables&amp;nbsp;for&amp;nbsp;communication between&amp;nbsp;GPUs.&amp;nbsp;Traditional copper links&amp;nbsp;are power-efficient and&amp;nbsp;reliable,&amp;nbsp;but&amp;nbsp;limited to&amp;nbsp;very short&amp;nbsp;distances&amp;nbsp;( 2 meters)&amp;nbsp;that&amp;nbsp;restrict their use&amp;nbsp;to within a single&amp;nbsp;GPU&amp;nbsp;rack. Optical&amp;nbsp;fiber&amp;nbsp;links&amp;nbsp;can&amp;nbsp;reach&amp;nbsp;tens of meters,&amp;nbsp;but&amp;nbsp;they&amp;nbsp;consume far more&amp;nbsp;power&amp;nbsp;and fail up to 100 times&amp;nbsp;as often as&amp;nbsp;copper. A&amp;nbsp;team working across&amp;nbsp;Microsoft&amp;nbsp;aims&amp;nbsp;to&amp;nbsp;resolve&amp;nbsp;this trade-off&amp;nbsp;by&amp;nbsp;developing&amp;nbsp;MOSAIC,&amp;nbsp;a novel optical link technology&amp;nbsp;that&amp;nbsp;can provide&amp;nbsp;low power and cost, high reliability, and long reach (up to 50 meters)&amp;nbsp;&lt;em&gt;simultaneously&lt;/em&gt;.&amp;nbsp;This approach leverages a hardware-system co-design and adopts&amp;nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&amp;nbsp;microLEDs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The fundamental trade-off&amp;nbsp;among&amp;nbsp;power, reliability, and reach&amp;nbsp;stems from&amp;nbsp;the&amp;nbsp;&lt;em&gt;narrow-and-fast&lt;/em&gt;&amp;nbsp;architecture&amp;nbsp;deployed&amp;nbsp;in&amp;nbsp;today’s copper and optical links,&amp;nbsp;comprising&amp;nbsp;a few channels&amp;nbsp;operating&amp;nbsp;at&amp;nbsp;very high&amp;nbsp;data rates. For example,&amp;nbsp;an&amp;nbsp;800 Gbps link&amp;nbsp;consists of eight 100 Gbps channels.&amp;nbsp;With&amp;nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&amp;nbsp;With optical&amp;nbsp;links,&amp;nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&amp;nbsp;complex electronics&amp;nbsp;to compensate for transmission impairments. These challenges&amp;nbsp;grow&amp;nbsp;as speeds increase&amp;nbsp;with&amp;nbsp;every&amp;nbsp;generation&amp;nbsp;of networks.&amp;nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&amp;nbsp;systems&amp;nbsp;margins&amp;nbsp;and increasing failure rates.&amp;nbsp;&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: AI-POWERED EXPERIENCE&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft research copilot experience&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-copilot-experience"&gt;Discover more about research at Microsoft through our AI-powered experience&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;These limitations force systems designers to make unpleasant&amp;nbsp;choices,&amp;nbsp;limiting the scalability of AI infrastructure.&amp;nbsp;For example,&amp;nbsp;scale-up networks connecting AI accelerators at&amp;nbsp;multi-Tbps&amp;nbsp;bandwidth&amp;nbsp;typically&amp;nbsp;must&amp;nbsp;rely on&amp;nbsp;copper links&amp;nbsp;to meet&amp;nbsp;the&amp;nbsp;power budget,&amp;nbsp;requiring&amp;nbsp;ultra-dense racks that&amp;nbsp;consume&amp;nbsp;hundreds of kilowatts&amp;nbsp;&lt;em&gt;per rack&lt;/em&gt;. This creates significant challenges in cooling&amp;nbsp;and&amp;nbsp;mechanical design,&amp;nbsp;which constrain&amp;nbsp;the practical scale of these networks and end-to-end performance. This imbalance&amp;nbsp;ultimately&amp;nbsp;erects&amp;nbsp;a&amp;nbsp;&lt;em&gt;networking wall&lt;/em&gt;&amp;nbsp;akin&amp;nbsp;to the&amp;nbsp;&lt;em&gt;memory wall&lt;/em&gt;, in&amp;nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.&lt;/p&gt;



&lt;p class="has-text-align-left"&gt;A technology offering copper-like power efficiency and reliability over long distances can overcome this networking&amp;nbsp;wall,&amp;nbsp;enabling&amp;nbsp;multi-rack&amp;nbsp;scale-up domains and unlocking&amp;nbsp;new architectures. This is a highly active R&amp;amp;D area, with many candidate technologies currently being developed across the industry.&amp;nbsp;In&amp;nbsp;our recent&amp;nbsp;paper,&amp;nbsp;&lt;em&gt;“MOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs”&lt;/em&gt;, which received the Best Paper award at ACM SIGCOMM&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, we present&amp;nbsp;one such promising&amp;nbsp;approach&amp;nbsp;that is&amp;nbsp;the result of a multi-year collaboration between Microsoft Research,&amp;nbsp;Azure, and M365.&amp;nbsp;This&amp;nbsp;work is&amp;nbsp;centered around&amp;nbsp;an optical&amp;nbsp;wide-and-slow architecture, shifting from a small number of high-speed serial channels towards&amp;nbsp;hundreds of parallel low-speed channels.&amp;nbsp;This&amp;nbsp;would be impractical&amp;nbsp;to realize with today’s copper and optical technologies because of&amp;nbsp;i)&amp;nbsp;electromagnetic interference challenges in high-density copper cables and ii) the&amp;nbsp;high cost&amp;nbsp;and power consumption of lasers&amp;nbsp;in optical links,&amp;nbsp;as well as the increase in packaging complexity.&amp;nbsp;MOSAIC overcomes these issues by&amp;nbsp;leveraging&amp;nbsp;directly modulated&amp;nbsp;microLEDs, a technology originally developed for&amp;nbsp;screen&amp;nbsp;displays.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MicroLEDs&amp;nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&amp;nbsp;microns) and, due to their&amp;nbsp;small size,&amp;nbsp;they&amp;nbsp;can be modulated at several Gbps.&amp;nbsp;They&amp;nbsp;are manufactured in large arrays,&amp;nbsp;with over half a million&amp;nbsp;in a small physical footprint for high-resolution displays&amp;nbsp;like&amp;nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&amp;nbsp;microLED&amp;nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20×20&amp;nbsp;microLED&amp;nbsp;array, which can fit in less than 1 mm×1 mm&amp;nbsp;silicon&amp;nbsp;die.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MOSAIC’s&amp;nbsp;wide-and-slow&amp;nbsp;design&amp;nbsp;provides four core benefits.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Operating&amp;nbsp;at low speed improves power efficiency&amp;nbsp;by&amp;nbsp;eliminating&amp;nbsp;the need for&amp;nbsp;complex&amp;nbsp;electronics&amp;nbsp;and&amp;nbsp;reducing optical power requirements.&lt;/li&gt;



&lt;li&gt;By&amp;nbsp;leveraging&amp;nbsp;optical transmission (via&amp;nbsp;microLEDs),&amp;nbsp;MOSAIC&amp;nbsp;sidesteps&amp;nbsp;copper’s reach issues, supporting distances up to 50 meters,&amp;nbsp;or&amp;nbsp;&amp;gt; 10x&amp;nbsp;further&amp;nbsp;than copper.&lt;/li&gt;



&lt;li&gt;MicroLEDs’&amp;nbsp;simpler structure&amp;nbsp;and temperature insensitivity&amp;nbsp;make them more reliable than lasers. The parallel nature of&amp;nbsp;wide-and-slow&amp;nbsp;also&amp;nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;The&amp;nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&amp;nbsp;1.6&amp;nbsp;Tbps&amp;nbsp;or 3.2&amp;nbsp;Tbps) can be achieved by increasing the number of&amp;nbsp;channels and/or raising per-channel speed&amp;nbsp;(e.g., to 4-8 Gbps).&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Further,&amp;nbsp;MOSAIC is fully compatible with today’s pluggable transceivers’ form&amp;nbsp;factor&amp;nbsp;and it provides a drop-in replacement for today’s copper and optical cables, without requiring any changes to existing server and network infrastructure.&amp;nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&amp;nbsp;terminating&amp;nbsp;or inspecting the connection&amp;nbsp;and, hence,&amp;nbsp;it’s&amp;nbsp;fully compatible with today’s protocols (e.g.,&amp;nbsp;Ethernet, PCIe, CXL).&amp;nbsp;We are currently working with our suppliers to&amp;nbsp;productize&amp;nbsp;this technology and&amp;nbsp;scale&amp;nbsp;to mass production.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While&amp;nbsp;conceptually simple, realizing this architecture posed a few key challenges&amp;nbsp;across the stack, which&amp;nbsp;required&amp;nbsp;a multi-disciplinary team with&amp;nbsp;expertise&amp;nbsp;spanning across integrated photonics, lens design, optical transmission, and&amp;nbsp;analog&amp;nbsp;and digital design.&amp;nbsp;For example, using individual&amp;nbsp;fibers&amp;nbsp;per channel would be prohibitively complex and costly due to the&amp;nbsp;large number&amp;nbsp;of channels. We addressed this by employing imaging&amp;nbsp;fibers,&amp;nbsp;which are typically used for medical applications (e.g., endoscopy).&amp;nbsp;They&amp;nbsp;can support thousands of cores&amp;nbsp;per&amp;nbsp;fiber, enabling multiplexing&amp;nbsp;of&amp;nbsp;many channels within a single&amp;nbsp;fiber.&amp;nbsp;Also,&amp;nbsp;microLEDs&amp;nbsp;are a less pure light source&amp;nbsp;than lasers,&amp;nbsp;with&amp;nbsp;a larger beam shape (which complicates&amp;nbsp;fiber&amp;nbsp;coupling) and&amp;nbsp;a broader spectrum (which&amp;nbsp;degrades&amp;nbsp;fiber&amp;nbsp;transmission due to chromatic dispersion).&amp;nbsp;We tackled these issues through&amp;nbsp;a novel&amp;nbsp;microLED and&amp;nbsp;optical lens design,&amp;nbsp;and&amp;nbsp;a power-efficient&amp;nbsp;analog-only electronic back&amp;nbsp;end, which does not require any expensive digital signal processing.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Based on our current estimates, this approach can save&amp;nbsp;up to 68% of power, i.e., more&amp;nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&amp;nbsp;reaching into&amp;nbsp;the tens of millions, this translates to over 100MW of power savings per year,&amp;nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&amp;nbsp;to rethink&amp;nbsp;AI&amp;nbsp;infrastructure from network and cluster architectures to compute and memory designs.&lt;/p&gt;



&lt;p&gt;For example,&amp;nbsp;by&amp;nbsp;supporting&amp;nbsp;low-power,&amp;nbsp;high-bandwidth connectivity at long reach,&amp;nbsp;MOSAIC&amp;nbsp;removes the need for ultra-dense racks and&amp;nbsp;enables&amp;nbsp;novel network topologies, which would be impractical today. The resulting redesign could&amp;nbsp;reduce&amp;nbsp;resource fragmentation and&amp;nbsp;simplify&amp;nbsp;collective optimization.&amp;nbsp;Similarly,&amp;nbsp;on the&amp;nbsp;compute&amp;nbsp;front,&amp;nbsp;the ability&amp;nbsp;to&amp;nbsp;connect&amp;nbsp;silicon&amp;nbsp;dies at low power over long distances&amp;nbsp;could&amp;nbsp;enable&amp;nbsp;resource&amp;nbsp;disaggregation, shifting from today’s&amp;nbsp;large,&amp;nbsp;multi-die packages to&amp;nbsp;smaller, more cost-effective, ones.&amp;nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&amp;nbsp;GPU&amp;nbsp;memory&amp;nbsp;capacity and bandwidth,&amp;nbsp;while&amp;nbsp;facilitating&amp;nbsp;adoption of&amp;nbsp;novel memory technologies.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&amp;nbsp;possible future&amp;nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/</guid><pubDate>Tue, 09 Sep 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Apple Intelligence: Everything you need to know about Apple’s AI model and services (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/apple-intelligence-everything-you-need-to-know-about-apples-ai-model-and-services/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If you’ve upgraded to a newer iPhone model recently, you’ve probably noticed that Apple Intelligence is showing up in some of your most-used apps, like Messages, Mail, and Notes. Apple Intelligence (yes, also abbreviated to AI) showed up in Apple’s ecosystem in October 2024, and it’s here to stay as Apple competes with Google, OpenAI, Anthropic, and others to build the best AI tools.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-apple-intelligence"&gt;What is Apple Intelligence?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2792508" height="383" src="https://techcrunch.com/wp-content/uploads/2024/06/wwdc24-Apple-intelligence-AI-for-the-rest-of-us-e1718051510774.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Cupertino marketing executives have branded Apple Intelligence: “AI for the rest of us.” The platform is designed to leverage the things that generative AI already does well, like text and image generation, to improve upon existing features. Like other platforms including ChatGPT and Google Gemini, Apple Intelligence was trained on large information models. These systems use deep learning to form connections, whether it be text, images, video or music.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The text offering, powered by LLM, presents itself as Writing Tools. The feature is available across various Apple apps, including Mail, Messages, Pages and Notifications. It can be used to provide summaries of long text, proofread and even write messages for you, using content and tone prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Image generation has been integrated as well, in similar fashion — albeit a bit less seamlessly. Users can prompt Apple Intelligence to generate custom emojis (Genmojis) in an Apple house style. Image Playground, meanwhile, is a standalone image generation app that utilizes prompts to create visual content that can be used in Messages, Keynote or shared via social media.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence also marks a long-awaited face-lift for Siri. The smart assistant was early to the game, but has mostly been neglected for the past several years. Siri is integrated much more deeply into Apple’s operating systems; for instance, instead of the familiar icon, users will see a glowing light around the edge of their iPhone screen when it’s doing its thing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;More importantly, new Siri works across apps. That means, for example, that you can ask Siri to edit a photo and then insert it directly into a text message. It’s a frictionless experience the assistant had previously lacked. Onscreen awareness means Siri uses the context of the content you’re currently engaged with to provide an appropriate answer.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Leading up to WWDC 2025, many expected that Apple would introduce us to an even more souped-up version of Siri, but we’re going to have to wait a bit longer.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“As we’ve shared, we’re continuing our work to deliver the features that make Siri even more personal,” said Apple SVP of Software Engineering Craig Federighi at WWDC 2025. “This work needed more time to reach our high-quality bar, and we look forward to sharing more about it in the coming year.”&lt;/p&gt;&lt;p&gt;This yet-to-be-released, more personalized version of Siri is supposed to be able to understand “personal context,” like your relationships, communications routine, and more. But according to a Bloomberg report, the in-development version of this new Siri is too error-ridden to ship, hence its delay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At WWDC 2025, Apple also unveiled a new AI feature called Visual Intelligence, which helps you do an image search for things you see as you browse. Apple also unveiled a Live Translation feature that can translate conversations in real time in the Messages, FaceTime, and Phone apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Visual Intelligence and Live Translation are expected to be available later in 2025, when iOS 26 launches to the public.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-when-was-apple-intelligence-unveiled"&gt;When was Apple Intelligence unveiled?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;After months of speculation, Apple Intelligence took center stage at WWDC 2024. The platform was announced in the wake of a torrent of generative AI news from companies like Google and Open AI, causing concern that the famously tight-lipped tech giant had missed the boat on the latest tech craze.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Contrary to such speculation, however, Apple had a team in place, working on what proved to be a very Apple approach to artificial intelligence. There was still pizzazz amid the demos — Apple always loves to put on a show — but Apple Intelligence is ultimately a very pragmatic take on the category.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence isn’t a standalone feature. Rather, it’s about integrating into existing offerings. While it is a branding exercise in a very real sense, the large language model (LLM) driven technology will operate behind the scenes. As far as the consumer is concerned, the technology will mostly present itself in the form of new features for existing apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We learned more during Apple’s iPhone 16 event in September 2024. During the event, Apple touted a number of AI-powered features coming to its devices, from translation on the Apple Watch Series 10, visual search on iPhones, and a number of tweaks to Siri’s capabilities. The first wave of Apple Intelligence is arriving at the end of October, as part of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The features launched first in U.S. English. Apple later added Australian, Canadian, New Zealand, South African, and U.K. English localizations. Support for Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, and Vietnamese will arrive in 2025. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-who-gets-apple-intelligence"&gt;Who gets Apple Intelligence?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="iPhone 15 Pro Max in natural titanium, being held, showing the back of the phone" class="wp-image-2602043" height="453" src="https://techcrunch.com/wp-content/uploads/2023/09/iPhone-15-Pro-31.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Darrell Etherington&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The first wave of Apple Intelligence arrived in October 2024 via iOS 18.1, iPadOS 18, and macOS Sequoia 15.1 updates. These updates included integrated writing tools, image cleanup, article summaries, and a typing input for the&amp;nbsp;redesigned Siri experience. A second wave of features became available as part of iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2. That list includes Genmoji, Image Playground, Visual Intelligence, Image Wand, and ChatGPT integration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These offerings are free to use, so long as you have one of the following pieces of hardware:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;All iPhone 16 models&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPhone 15 Pro Max (A17 Pro)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPhone 15 Pro (A17 Pro)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad Pro (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad Air (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad mini (A17 or later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MacBook Air (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MacBook Pro (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iMac (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac mini (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac Studio (M1 Max and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac Pro (M2 Ultra)&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, only the Pro versions of the iPhone 15 are getting access, owing to shortcomings on the standard model’s chipset. Presumably, however, the whole iPhone 16 line will be able to run Apple Intelligence when it arrives.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-apple-s-ai-work-without-an-internet-connection"&gt;How does Apple’s AI work without an internet connection?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2792348" height="383" src="https://techcrunch.com/wp-content/uploads/2024/06/wwdc24-apple-intelligence-private-cloud-compute-02.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When you ask GPT or Gemini a question, your query is being sent to external servers to generate a response, which requires an internet connection. But Apple has taken a small-model, bespoke approach to training. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The biggest benefit of this approach is that many of these tasks become far less resource intensive and can be performed on-device. This is because, rather than relying on the kind of kitchen sink approach that fuels platforms like GPT and Gemini, the company has compiled datasets in-house for specific tasks like, say, composing an email. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That doesn’t apply to everything, however. More complex queries will utilize the new Private Cloud Compute offering. The company now operates remote servers running on Apple Silicon, which it claims allows it to offer the same level of privacy as its consumer devices. Whether an action is being performed locally or via the cloud will be invisible to the user, unless their device is offline, at which point remote queries will toss up an error.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-apple-intelligence-with-third-party-apps"&gt;Apple Intelligence with third-party apps&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="OpenAI and ChatGPT logos" class="wp-image-2763309" height="383" src="https://techcrunch.com/wp-content/uploads/2024/05/OpenAI-and-ChatGPT.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Didem Mente/Anadolu Agency / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;A lot of noise was made about Apple’s pending partnership with OpenAI ahead of the launch of Apple Intelligence. Ultimately, however, it turned out that the deal was less about powering Apple Intelligence and more about offering an alternative platform for those things it’s not really built for. It’s a tacit acknowledgement that building a small-model system has its limitations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence is free. So, too, is access to ChatGPT. However, those with paid accounts to the latter will have access to premium features free users don’t, including unlimited queries. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT integration, which debuts on iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2, has two primary roles: supplementing Siri’s knowledge base and adding to the existing Writing Tools options.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the service enabled, certain questions will prompt the new Siri to ask the user to approve its accessing ChatGPT. Recipes and travel planning are examples of questions that may surface the option. Users can also directly prompt Siri to “ask ChatGPT.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Compose is the other primary ChatGPT feature available through Apple Intelligence. Users can access it in any app that supports the new Writing Tools feature. Compose adds the ability to write content based on a prompt. That joins existing writing tools like Style and Summary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We know for sure that Apple plans to partner with additional generative AI services. The company all but said that Google Gemini is next on that list.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-can-developers-build-on-apple-s-ai-models"&gt;Can developers build on Apple’s AI models?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At WWDC 2025, Apple announced what it calls the Foundation Models framework, which will let developers tap into its AI models while offline.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This makes it more possible for developers to build AI features into their third-party apps that leverage Apple’s existing systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For example, if you’re getting ready for an exam, an app like Kahoot can create a personalized quiz from your notes to make studying more engaging,” Federighi said at WWDC. “And because it happens using on-device models, this happens without cloud API costs&amp;nbsp;… We couldn’t be more excited about how developers can build on Apple intelligence to bring you new experiences that are smart, available when you’re offline, and that protect your privacy.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-when-is-siri-getting-its-next-overhaul"&gt;When is Siri getting its next overhaul?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Apple is expected to unveil a new-and-improved Siri experience in 2026, which is already a bit late compared to competitors. It may come as a blow to Apple, but in order to speed up development, they may have no choice but to partner with an outside company to power the new Siri. Apple has been rumored  to be in advanced talks with Google, its primary smartphone hardware competitor.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If you’ve upgraded to a newer iPhone model recently, you’ve probably noticed that Apple Intelligence is showing up in some of your most-used apps, like Messages, Mail, and Notes. Apple Intelligence (yes, also abbreviated to AI) showed up in Apple’s ecosystem in October 2024, and it’s here to stay as Apple competes with Google, OpenAI, Anthropic, and others to build the best AI tools.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-apple-intelligence"&gt;What is Apple Intelligence?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2792508" height="383" src="https://techcrunch.com/wp-content/uploads/2024/06/wwdc24-Apple-intelligence-AI-for-the-rest-of-us-e1718051510774.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Cupertino marketing executives have branded Apple Intelligence: “AI for the rest of us.” The platform is designed to leverage the things that generative AI already does well, like text and image generation, to improve upon existing features. Like other platforms including ChatGPT and Google Gemini, Apple Intelligence was trained on large information models. These systems use deep learning to form connections, whether it be text, images, video or music.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The text offering, powered by LLM, presents itself as Writing Tools. The feature is available across various Apple apps, including Mail, Messages, Pages and Notifications. It can be used to provide summaries of long text, proofread and even write messages for you, using content and tone prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Image generation has been integrated as well, in similar fashion — albeit a bit less seamlessly. Users can prompt Apple Intelligence to generate custom emojis (Genmojis) in an Apple house style. Image Playground, meanwhile, is a standalone image generation app that utilizes prompts to create visual content that can be used in Messages, Keynote or shared via social media.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence also marks a long-awaited face-lift for Siri. The smart assistant was early to the game, but has mostly been neglected for the past several years. Siri is integrated much more deeply into Apple’s operating systems; for instance, instead of the familiar icon, users will see a glowing light around the edge of their iPhone screen when it’s doing its thing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;More importantly, new Siri works across apps. That means, for example, that you can ask Siri to edit a photo and then insert it directly into a text message. It’s a frictionless experience the assistant had previously lacked. Onscreen awareness means Siri uses the context of the content you’re currently engaged with to provide an appropriate answer.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Leading up to WWDC 2025, many expected that Apple would introduce us to an even more souped-up version of Siri, but we’re going to have to wait a bit longer.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“As we’ve shared, we’re continuing our work to deliver the features that make Siri even more personal,” said Apple SVP of Software Engineering Craig Federighi at WWDC 2025. “This work needed more time to reach our high-quality bar, and we look forward to sharing more about it in the coming year.”&lt;/p&gt;&lt;p&gt;This yet-to-be-released, more personalized version of Siri is supposed to be able to understand “personal context,” like your relationships, communications routine, and more. But according to a Bloomberg report, the in-development version of this new Siri is too error-ridden to ship, hence its delay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At WWDC 2025, Apple also unveiled a new AI feature called Visual Intelligence, which helps you do an image search for things you see as you browse. Apple also unveiled a Live Translation feature that can translate conversations in real time in the Messages, FaceTime, and Phone apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Visual Intelligence and Live Translation are expected to be available later in 2025, when iOS 26 launches to the public.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-when-was-apple-intelligence-unveiled"&gt;When was Apple Intelligence unveiled?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;After months of speculation, Apple Intelligence took center stage at WWDC 2024. The platform was announced in the wake of a torrent of generative AI news from companies like Google and Open AI, causing concern that the famously tight-lipped tech giant had missed the boat on the latest tech craze.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Contrary to such speculation, however, Apple had a team in place, working on what proved to be a very Apple approach to artificial intelligence. There was still pizzazz amid the demos — Apple always loves to put on a show — but Apple Intelligence is ultimately a very pragmatic take on the category.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence isn’t a standalone feature. Rather, it’s about integrating into existing offerings. While it is a branding exercise in a very real sense, the large language model (LLM) driven technology will operate behind the scenes. As far as the consumer is concerned, the technology will mostly present itself in the form of new features for existing apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We learned more during Apple’s iPhone 16 event in September 2024. During the event, Apple touted a number of AI-powered features coming to its devices, from translation on the Apple Watch Series 10, visual search on iPhones, and a number of tweaks to Siri’s capabilities. The first wave of Apple Intelligence is arriving at the end of October, as part of iOS 18.1, iPadOS 18.1, and macOS Sequoia 15.1.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The features launched first in U.S. English. Apple later added Australian, Canadian, New Zealand, South African, and U.K. English localizations. Support for Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, and Vietnamese will arrive in 2025. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-who-gets-apple-intelligence"&gt;Who gets Apple Intelligence?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="iPhone 15 Pro Max in natural titanium, being held, showing the back of the phone" class="wp-image-2602043" height="453" src="https://techcrunch.com/wp-content/uploads/2023/09/iPhone-15-Pro-31.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Darrell Etherington&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The first wave of Apple Intelligence arrived in October 2024 via iOS 18.1, iPadOS 18, and macOS Sequoia 15.1 updates. These updates included integrated writing tools, image cleanup, article summaries, and a typing input for the&amp;nbsp;redesigned Siri experience. A second wave of features became available as part of iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2. That list includes Genmoji, Image Playground, Visual Intelligence, Image Wand, and ChatGPT integration.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These offerings are free to use, so long as you have one of the following pieces of hardware:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;All iPhone 16 models&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPhone 15 Pro Max (A17 Pro)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPhone 15 Pro (A17 Pro)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad Pro (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad Air (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iPad mini (A17 or later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MacBook Air (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;MacBook Pro (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;iMac (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac mini (M1 and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac Studio (M1 Max and later)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mac Pro (M2 Ultra)&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, only the Pro versions of the iPhone 15 are getting access, owing to shortcomings on the standard model’s chipset. Presumably, however, the whole iPhone 16 line will be able to run Apple Intelligence when it arrives.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-apple-s-ai-work-without-an-internet-connection"&gt;How does Apple’s AI work without an internet connection?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2792348" height="383" src="https://techcrunch.com/wp-content/uploads/2024/06/wwdc24-apple-intelligence-private-cloud-compute-02.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When you ask GPT or Gemini a question, your query is being sent to external servers to generate a response, which requires an internet connection. But Apple has taken a small-model, bespoke approach to training. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The biggest benefit of this approach is that many of these tasks become far less resource intensive and can be performed on-device. This is because, rather than relying on the kind of kitchen sink approach that fuels platforms like GPT and Gemini, the company has compiled datasets in-house for specific tasks like, say, composing an email. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That doesn’t apply to everything, however. More complex queries will utilize the new Private Cloud Compute offering. The company now operates remote servers running on Apple Silicon, which it claims allows it to offer the same level of privacy as its consumer devices. Whether an action is being performed locally or via the cloud will be invisible to the user, unless their device is offline, at which point remote queries will toss up an error.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-apple-intelligence-with-third-party-apps"&gt;Apple Intelligence with third-party apps&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="OpenAI and ChatGPT logos" class="wp-image-2763309" height="383" src="https://techcrunch.com/wp-content/uploads/2024/05/OpenAI-and-ChatGPT.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Didem Mente/Anadolu Agency / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;A lot of noise was made about Apple’s pending partnership with OpenAI ahead of the launch of Apple Intelligence. Ultimately, however, it turned out that the deal was less about powering Apple Intelligence and more about offering an alternative platform for those things it’s not really built for. It’s a tacit acknowledgement that building a small-model system has its limitations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple Intelligence is free. So, too, is access to ChatGPT. However, those with paid accounts to the latter will have access to premium features free users don’t, including unlimited queries. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT integration, which debuts on iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2, has two primary roles: supplementing Siri’s knowledge base and adding to the existing Writing Tools options.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the service enabled, certain questions will prompt the new Siri to ask the user to approve its accessing ChatGPT. Recipes and travel planning are examples of questions that may surface the option. Users can also directly prompt Siri to “ask ChatGPT.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Compose is the other primary ChatGPT feature available through Apple Intelligence. Users can access it in any app that supports the new Writing Tools feature. Compose adds the ability to write content based on a prompt. That joins existing writing tools like Style and Summary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We know for sure that Apple plans to partner with additional generative AI services. The company all but said that Google Gemini is next on that list.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-can-developers-build-on-apple-s-ai-models"&gt;Can developers build on Apple’s AI models?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At WWDC 2025, Apple announced what it calls the Foundation Models framework, which will let developers tap into its AI models while offline.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This makes it more possible for developers to build AI features into their third-party apps that leverage Apple’s existing systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For example, if you’re getting ready for an exam, an app like Kahoot can create a personalized quiz from your notes to make studying more engaging,” Federighi said at WWDC. “And because it happens using on-device models, this happens without cloud API costs&amp;nbsp;… We couldn’t be more excited about how developers can build on Apple intelligence to bring you new experiences that are smart, available when you’re offline, and that protect your privacy.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-when-is-siri-getting-its-next-overhaul"&gt;When is Siri getting its next overhaul?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Apple is expected to unveil a new-and-improved Siri experience in 2026, which is already a bit late compared to competitors. It may come as a blow to Apple, but in order to speed up development, they may have no choice but to partner with an outside company to power the new Siri. Apple has been rumored  to be in advanced talks with Google, its primary smartphone hardware competitor.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/apple-intelligence-everything-you-need-to-know-about-apples-ai-model-and-services/</guid><pubDate>Tue, 09 Sep 2025 14:51:52 +0000</pubDate></item><item><title>[NEW] Adapting to new threats with proactive risk management (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/09/1123083/adapting-to-new-threats-with-proactive-risk-management/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Hitachi Vantara&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;In July 2024, a botched update to the software defenses managed by cybersecurity firm CrowdStrike caused more than 8 million Windows systems to fail. From hospitals to manufacturers, stock markets to retail stores, the outage caused parts of the global economy to grind to a halt. Payment systems were disrupted, broadcasters went off the air, and flights were canceled. In all, the outage is estimated to have caused direct losses of more than $5 billion to Fortune 500 companies. For US air carrier Delta Air Lines, the error exposed the brittleness of its systems. The airline suffered weeks of disruptions, leading to $500 million in losses and 7,000 canceled flights.&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="alt" class="wp-image-1123086" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MIT_HitachiVantura_V5_072225Cover.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;The magnitude of the CrowdStrike incident revealed just how interconnected digital systems are, and the extensive vulnerabilities in some companies when confronted with an unexpected occurrence. “On any given day, there could be a major weather event or some event like what happened…with CrowdStrike,” said then-US secretary of transportation Pete Buttigieg on announcing an investigation into how Delta Air Lines handled the incident. “The question is, is your airline prepared to absorb something like that and get back on its feet and take care of customers?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Unplanned downtime poses a major challenge for organizations, and is estimated to cost Global 2000 companies on average $200 million per year. Beyond the financial impact, it can also erode customer trust and loyalty, decrease productivity, and even result in legal or privacy issues. &lt;/p&gt;  &lt;p&gt;A 2024 ransomware attack on Change Healthcare, the medical-billing subsidiary of industry giant UnitedHealth Group—the biggest health and medical data breach in US history—exposed the data of around 190 million people and led to weeks of outages for medical groups. Another ransomware attack in 2024, this time on CDK Global, a software firm that works with nearly 15,000 auto dealerships in North America, led to around $1 billion worth of losses for car dealers as a result of the three-week disruption. &lt;/p&gt; 
 &lt;p&gt;Managing risk and mitigating downtime is a growing challenge for businesses. As organizations become ever more interconnected, the expanding surface of networks and the rapid adoption of technologies like AI are exposing new vulnerabilities—and more opportunities for threat actors. Cyberattacks are also becoming increasingly sophisticated and damaging as AI-driven malware and malware-as-a-service platforms turbocharge attacks.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123091" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MITTR2025_HItachiSocials2.png" /&gt;&lt;/figure&gt;  &lt;p&gt;To prepare for these challenges head on, companies must take a more proactive approach to security and resilience. “We’ve had a traditional way of doing things that’s actually worked pretty well for maybe 15 to 20 years, but it’s been based on detecting an incident after the event,” says Chris Millington, global cyber resilience technical expert at Hitachi Vantara. “Now, we’ve got to be more preventative and use intelligence to focus on making the systems and business more resilient.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Hitachi Vantara&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;In July 2024, a botched update to the software defenses managed by cybersecurity firm CrowdStrike caused more than 8 million Windows systems to fail. From hospitals to manufacturers, stock markets to retail stores, the outage caused parts of the global economy to grind to a halt. Payment systems were disrupted, broadcasters went off the air, and flights were canceled. In all, the outage is estimated to have caused direct losses of more than $5 billion to Fortune 500 companies. For US air carrier Delta Air Lines, the error exposed the brittleness of its systems. The airline suffered weeks of disruptions, leading to $500 million in losses and 7,000 canceled flights.&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="alt" class="wp-image-1123086" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MIT_HitachiVantura_V5_072225Cover.png?w=1555" width="1555" /&gt;&lt;/figure&gt;  &lt;p&gt;The magnitude of the CrowdStrike incident revealed just how interconnected digital systems are, and the extensive vulnerabilities in some companies when confronted with an unexpected occurrence. “On any given day, there could be a major weather event or some event like what happened…with CrowdStrike,” said then-US secretary of transportation Pete Buttigieg on announcing an investigation into how Delta Air Lines handled the incident. “The question is, is your airline prepared to absorb something like that and get back on its feet and take care of customers?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;Unplanned downtime poses a major challenge for organizations, and is estimated to cost Global 2000 companies on average $200 million per year. Beyond the financial impact, it can also erode customer trust and loyalty, decrease productivity, and even result in legal or privacy issues. &lt;/p&gt;  &lt;p&gt;A 2024 ransomware attack on Change Healthcare, the medical-billing subsidiary of industry giant UnitedHealth Group—the biggest health and medical data breach in US history—exposed the data of around 190 million people and led to weeks of outages for medical groups. Another ransomware attack in 2024, this time on CDK Global, a software firm that works with nearly 15,000 auto dealerships in North America, led to around $1 billion worth of losses for car dealers as a result of the three-week disruption. &lt;/p&gt; 
 &lt;p&gt;Managing risk and mitigating downtime is a growing challenge for businesses. As organizations become ever more interconnected, the expanding surface of networks and the rapid adoption of technologies like AI are exposing new vulnerabilities—and more opportunities for threat actors. Cyberattacks are also becoming increasingly sophisticated and damaging as AI-driven malware and malware-as-a-service platforms turbocharge attacks.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1123091" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MITTR2025_HItachiSocials2.png" /&gt;&lt;/figure&gt;  &lt;p&gt;To prepare for these challenges head on, companies must take a more proactive approach to security and resilience. “We’ve had a traditional way of doing things that’s actually worked pretty well for maybe 15 to 20 years, but it’s been based on detecting an incident after the event,” says Chris Millington, global cyber resilience technical expert at Hitachi Vantara. “Now, we’ve got to be more preventative and use intelligence to focus on making the systems and business more resilient.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Download the report.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/09/1123083/adapting-to-new-threats-with-proactive-risk-management/</guid><pubDate>Tue, 09 Sep 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] NVIDIA Partners With AI Infrastructure Ecosystem to Unveil Reference Design for Giga-Scale AI Factories (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-factories-reference-design/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/inference-corp-blog-ai-factory-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At this week’s AI Infrastructure Summit in Silicon Valley, NVIDIA’s VP of Accelerated Computing Ian Buck unveiled a bold new vision: the transformation of traditional data centers into fully integrated AI factories.&lt;/p&gt;
&lt;p&gt;As part of this initiative, NVIDIA is developing reference designs to be shared with partners and enterprises worldwide — offering an NVIDIA Omniverse Blueprint for building high-performance, energy-efficient infrastructure optimized for the age of AI reasoning.&lt;/p&gt;
&lt;p&gt;Already, NVIDIA is collaborating with scores of companies across every layer of the stack, from building design and grid integration to power, cooling and orchestration.&lt;/p&gt;
&lt;p&gt;It’s a natural evolution for the company, scaling beyond chips and systems into a new class of industrial products — so complex and interconnected that no single player can build them alone.&lt;/p&gt;
&lt;p&gt;NVIDIA, along with a deep bench of industrial and technology partners, is reactivating decades of infrastructure expertise to build this new class of AI factories.&lt;/p&gt;
&lt;p&gt;Among those partners, Jacobs serves as the design integrator, helping to coordinate the physical and digital layers of the infrastructure to ensure seamless orchestration.&lt;/p&gt;
&lt;p&gt;The embodiment of the reference design will be a digital twin of the AI factory. This digital twin integrates the IT systems inside the data center with the operational technology for power and cooling systems inside and outside the data center.&lt;/p&gt;
&lt;p&gt;The new initiative expands the digital twin to integrate local power generation, energy storage systems, cooling technology and AI agents for operations.&lt;/p&gt;
&lt;p&gt;Longtime collaborators in power and cooling — Schneider Electric, Siemens Energy and Vertiv — have been instrumental in shaping resilient, high-efficiency environments tailored for AI-scale workloads.&lt;/p&gt;
&lt;p&gt;Siemens Energy plays a critical role in on-premises power delivery, supporting the need for rapidly deployable, continuous power to meet the gigawatt-scale energy demands of these facilities. GE Vernova collaborates in power generation and electrification to the rack.&lt;/p&gt;
&lt;p&gt;These companies, along with a growing ecosystem of specialists in infrastructure design and simulation, and orchestration — including Cadence, emeraldai, E Tech Group, phaidra.ai, PTC, Schneider Electric with ETAP, Siemens and Vertech — are helping NVIDIA activate a system-level transformation.&lt;/p&gt;
&lt;p&gt;At the heart of this vision lies a fundamental challenge: how to optimize every watt of energy that enters the facility so that it contributes directly to intelligence generation.&lt;/p&gt;
&lt;p&gt;In today’s data center paradigm, buildings are often designed independently of the compute platforms they house, leading to inefficiencies in power distribution, cooling and system orchestration.&lt;/p&gt;
&lt;p&gt;NVIDIA and its partners are flipping that model.&lt;/p&gt;
&lt;p&gt;By designing the infrastructure and technology stack in tandem, the company enables true system-level optimization — where power, cooling, compute and software are engineered as a unified whole.&lt;/p&gt;
&lt;p&gt;Simulation plays a central role in this shift.&lt;/p&gt;
&lt;p&gt;Companies will be able to share simulation-ready assets, allowing designers to model components in Omniverse using AI factory digital twins even before they’re physically available.&lt;/p&gt;
&lt;p&gt;These digital twins not only optimize AI factories before they’re built — they also help manage them once they’re operational.&lt;/p&gt;
&lt;p&gt;By adopting the OpenUSD framework, the simulation platform can accurately model every aspect of a facility’s operations, from power and cooling to networking infrastructure. This open and extensible approach allows for the creation of physically accurate assets, which in turn leads to the design of smarter, more reliable facilities.&lt;/p&gt;
&lt;p&gt;And the complexity doesn’t stop at the facility walls.&lt;/p&gt;
&lt;p&gt;AI factories must be plugged into broader systems — power grids, water supplies and transportation networks — that require careful coordination and simulation throughout their lifecycle to ensure reliability and scalability.&lt;/p&gt;
&lt;p&gt;This work has already begun.&lt;/p&gt;
&lt;p&gt;Earlier this year, NVIDIA introduced an Omniverse Blueprint for AI factory digital twins. This blueprint connects platforms like Cadence and ETAP, allowing partners to plug in their core tools to model gigawatt-scale facilities before a single physical AI factory site has even been selected.&lt;/p&gt;
&lt;p&gt;More recently, the company expanded its ecosystem with integrations from Jacobs, Siemens and Siemens Energy, enabling unified simulation of power, cooling and networking systems.&lt;/p&gt;
&lt;p&gt;When this blueprint is complete next year, it will allow partners to plug into the system via application programming interfaces and simulation-ready digital assets, enabling real-time collaboration and orchestration across the entire lifecycle — from design to deployment to operation.&lt;/p&gt;
&lt;p&gt;Thanks to this work, where traditional facilities operated in isolation, AI factories will be designed for composability, resilience and scale.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Call to Action:&lt;/b&gt; Join developers, industry leaders, and innovators at NVIDIA GTC Washington, D.C., to explore the latest breakthroughs in AI infrastructure and learn from expert sessions, hands-on training, and partner showcases.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/inference-corp-blog-ai-factory-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At this week’s AI Infrastructure Summit in Silicon Valley, NVIDIA’s VP of Accelerated Computing Ian Buck unveiled a bold new vision: the transformation of traditional data centers into fully integrated AI factories.&lt;/p&gt;
&lt;p&gt;As part of this initiative, NVIDIA is developing reference designs to be shared with partners and enterprises worldwide — offering an NVIDIA Omniverse Blueprint for building high-performance, energy-efficient infrastructure optimized for the age of AI reasoning.&lt;/p&gt;
&lt;p&gt;Already, NVIDIA is collaborating with scores of companies across every layer of the stack, from building design and grid integration to power, cooling and orchestration.&lt;/p&gt;
&lt;p&gt;It’s a natural evolution for the company, scaling beyond chips and systems into a new class of industrial products — so complex and interconnected that no single player can build them alone.&lt;/p&gt;
&lt;p&gt;NVIDIA, along with a deep bench of industrial and technology partners, is reactivating decades of infrastructure expertise to build this new class of AI factories.&lt;/p&gt;
&lt;p&gt;Among those partners, Jacobs serves as the design integrator, helping to coordinate the physical and digital layers of the infrastructure to ensure seamless orchestration.&lt;/p&gt;
&lt;p&gt;The embodiment of the reference design will be a digital twin of the AI factory. This digital twin integrates the IT systems inside the data center with the operational technology for power and cooling systems inside and outside the data center.&lt;/p&gt;
&lt;p&gt;The new initiative expands the digital twin to integrate local power generation, energy storage systems, cooling technology and AI agents for operations.&lt;/p&gt;
&lt;p&gt;Longtime collaborators in power and cooling — Schneider Electric, Siemens Energy and Vertiv — have been instrumental in shaping resilient, high-efficiency environments tailored for AI-scale workloads.&lt;/p&gt;
&lt;p&gt;Siemens Energy plays a critical role in on-premises power delivery, supporting the need for rapidly deployable, continuous power to meet the gigawatt-scale energy demands of these facilities. GE Vernova collaborates in power generation and electrification to the rack.&lt;/p&gt;
&lt;p&gt;These companies, along with a growing ecosystem of specialists in infrastructure design and simulation, and orchestration — including Cadence, emeraldai, E Tech Group, phaidra.ai, PTC, Schneider Electric with ETAP, Siemens and Vertech — are helping NVIDIA activate a system-level transformation.&lt;/p&gt;
&lt;p&gt;At the heart of this vision lies a fundamental challenge: how to optimize every watt of energy that enters the facility so that it contributes directly to intelligence generation.&lt;/p&gt;
&lt;p&gt;In today’s data center paradigm, buildings are often designed independently of the compute platforms they house, leading to inefficiencies in power distribution, cooling and system orchestration.&lt;/p&gt;
&lt;p&gt;NVIDIA and its partners are flipping that model.&lt;/p&gt;
&lt;p&gt;By designing the infrastructure and technology stack in tandem, the company enables true system-level optimization — where power, cooling, compute and software are engineered as a unified whole.&lt;/p&gt;
&lt;p&gt;Simulation plays a central role in this shift.&lt;/p&gt;
&lt;p&gt;Companies will be able to share simulation-ready assets, allowing designers to model components in Omniverse using AI factory digital twins even before they’re physically available.&lt;/p&gt;
&lt;p&gt;These digital twins not only optimize AI factories before they’re built — they also help manage them once they’re operational.&lt;/p&gt;
&lt;p&gt;By adopting the OpenUSD framework, the simulation platform can accurately model every aspect of a facility’s operations, from power and cooling to networking infrastructure. This open and extensible approach allows for the creation of physically accurate assets, which in turn leads to the design of smarter, more reliable facilities.&lt;/p&gt;
&lt;p&gt;And the complexity doesn’t stop at the facility walls.&lt;/p&gt;
&lt;p&gt;AI factories must be plugged into broader systems — power grids, water supplies and transportation networks — that require careful coordination and simulation throughout their lifecycle to ensure reliability and scalability.&lt;/p&gt;
&lt;p&gt;This work has already begun.&lt;/p&gt;
&lt;p&gt;Earlier this year, NVIDIA introduced an Omniverse Blueprint for AI factory digital twins. This blueprint connects platforms like Cadence and ETAP, allowing partners to plug in their core tools to model gigawatt-scale facilities before a single physical AI factory site has even been selected.&lt;/p&gt;
&lt;p&gt;More recently, the company expanded its ecosystem with integrations from Jacobs, Siemens and Siemens Energy, enabling unified simulation of power, cooling and networking systems.&lt;/p&gt;
&lt;p&gt;When this blueprint is complete next year, it will allow partners to plug into the system via application programming interfaces and simulation-ready digital assets, enabling real-time collaboration and orchestration across the entire lifecycle — from design to deployment to operation.&lt;/p&gt;
&lt;p&gt;Thanks to this work, where traditional facilities operated in isolation, AI factories will be designed for composability, resilience and scale.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Call to Action:&lt;/b&gt; Join developers, industry leaders, and innovators at NVIDIA GTC Washington, D.C., to explore the latest breakthroughs in AI infrastructure and learn from expert sessions, hands-on training, and partner showcases.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-factories-reference-design/</guid><pubDate>Tue, 09 Sep 2025 15:00:29 +0000</pubDate></item><item><title>[NEW] NVIDIA Blackwell Ultra Sets the Bar in New MLPerf Inference Benchmark (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/mlperf-inference-blackwell-ultra/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/grace-corp-blog-gb300-nvl72-1280x680-r1-2.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Inference performance is critical, as it directly influences the economics of an AI factory. The higher the throughput of AI factory infrastructure, the more tokens it can produce at a high speed — increasing revenue, driving down total cost of ownership (TCO) and enhancing the system’s overall productivity.&lt;/p&gt;
&lt;p&gt;Less than half a year since its debut at NVIDIA GTC, the NVIDIA GB300 NVL72 rack-scale system — powered by the NVIDIA Blackwell Ultra architecture — set records on the new reasoning inference benchmark in MLPerf Inference v5.1, delivering up to 1.4x more DeepSeek-R1 inference throughput compared with NVIDIA Blackwell-based GB200 NVL72 systems.&lt;/p&gt;
&lt;p&gt;Blackwell Ultra builds on the success of the Blackwell architecture, with the Blackwell Ultra architecture featuring 1.5x more NVFP4 AI compute and 2x more attention-layer acceleration than Blackwell, as well as up to 288GB of HBM3e memory per GPU.&lt;/p&gt;
&lt;p&gt;The NVIDIA platform also set performance records on all new data center benchmarks added to the MLPerf Inference v5.1 suite — including DeepSeek-R1, Llama 3.1 405B Interactive, Llama 3.1 8B and Whisper — while continuing to hold per-GPU records on every MLPerf data center benchmark.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Stacking It All Up&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Full-stack co-design plays an important role in delivering these latest benchmark results. Blackwell and Blackwell Ultra incorporate hardware acceleration for the NVFP4 data format — an NVIDIA-designed 4-bit floating point format that provides better accuracy compared with other FP4 formats, as well as comparable accuracy to higher-precision formats.&lt;/p&gt;
&lt;p&gt;NVIDIA TensorRT Model Optimizer software quantized DeepSeek-R1, Llama 3.1 405B, Llama 2 70B and Llama 3.1 8B to NVFP4. In concert with the open-source NVIDIA TensorRT-LLM library, this optimization enabled Blackwell and Blackwell Ultra to deliver higher performance while meeting strict accuracy requirements in submissions.&lt;/p&gt;
&lt;p&gt;Large language model inference consists of two workloads with distinct execution characteristics: 1) context for processing user input to produce the first output token and 2) generation to produce all subsequent output tokens.&lt;/p&gt;
&lt;p&gt;A technique called disaggregated serving splits context and generation tasks so each part can be optimized independently for best overall throughput. This technique was key to record-setting performance on the Llama 3.1 405B Interactive benchmark, helping to deliver a nearly 50% increase in performance per GPU with GB200 NVL72 systems compared with each Blackwell GPU in an NVIDIA DGX B200 server running the benchmark with traditional serving.&lt;/p&gt;
&lt;p&gt;NVIDIA also made its first submissions this round using the NVIDIA Dynamo inference framework.&lt;/p&gt;
&lt;p&gt;NVIDIA partners — including cloud service providers and server makers — submitted great results using the NVIDIA Blackwell and/or Hopper platform. These partners include Azure, Broadcom, Cisco, CoreWeave, Dell Technologies, Giga Computing, HPE, Lambda, Lenovo, Nebius, Oracle, Quanta Cloud Technology, Supermicro and the University of Florida.&lt;/p&gt;
&lt;p&gt;The market-leading inference performance on the NVIDIA AI platform is available from major cloud providers and server makers. This translates to lower TCO and enhanced return on investment for organizations deploying sophisticated AI applications.&lt;/p&gt;
&lt;p&gt;Learn more about these full-stack technologies by reading the NVIDIA Technical Blog on MLPerf Inference v5.1. Plus, visit the NVIDIA DGX Cloud Performance Explorer to learn more about NVIDIA performance, model TCO and generate custom reports.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/grace-corp-blog-gb300-nvl72-1280x680-r1-2.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Inference performance is critical, as it directly influences the economics of an AI factory. The higher the throughput of AI factory infrastructure, the more tokens it can produce at a high speed — increasing revenue, driving down total cost of ownership (TCO) and enhancing the system’s overall productivity.&lt;/p&gt;
&lt;p&gt;Less than half a year since its debut at NVIDIA GTC, the NVIDIA GB300 NVL72 rack-scale system — powered by the NVIDIA Blackwell Ultra architecture — set records on the new reasoning inference benchmark in MLPerf Inference v5.1, delivering up to 1.4x more DeepSeek-R1 inference throughput compared with NVIDIA Blackwell-based GB200 NVL72 systems.&lt;/p&gt;
&lt;p&gt;Blackwell Ultra builds on the success of the Blackwell architecture, with the Blackwell Ultra architecture featuring 1.5x more NVFP4 AI compute and 2x more attention-layer acceleration than Blackwell, as well as up to 288GB of HBM3e memory per GPU.&lt;/p&gt;
&lt;p&gt;The NVIDIA platform also set performance records on all new data center benchmarks added to the MLPerf Inference v5.1 suite — including DeepSeek-R1, Llama 3.1 405B Interactive, Llama 3.1 8B and Whisper — while continuing to hold per-GPU records on every MLPerf data center benchmark.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Stacking It All Up&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Full-stack co-design plays an important role in delivering these latest benchmark results. Blackwell and Blackwell Ultra incorporate hardware acceleration for the NVFP4 data format — an NVIDIA-designed 4-bit floating point format that provides better accuracy compared with other FP4 formats, as well as comparable accuracy to higher-precision formats.&lt;/p&gt;
&lt;p&gt;NVIDIA TensorRT Model Optimizer software quantized DeepSeek-R1, Llama 3.1 405B, Llama 2 70B and Llama 3.1 8B to NVFP4. In concert with the open-source NVIDIA TensorRT-LLM library, this optimization enabled Blackwell and Blackwell Ultra to deliver higher performance while meeting strict accuracy requirements in submissions.&lt;/p&gt;
&lt;p&gt;Large language model inference consists of two workloads with distinct execution characteristics: 1) context for processing user input to produce the first output token and 2) generation to produce all subsequent output tokens.&lt;/p&gt;
&lt;p&gt;A technique called disaggregated serving splits context and generation tasks so each part can be optimized independently for best overall throughput. This technique was key to record-setting performance on the Llama 3.1 405B Interactive benchmark, helping to deliver a nearly 50% increase in performance per GPU with GB200 NVL72 systems compared with each Blackwell GPU in an NVIDIA DGX B200 server running the benchmark with traditional serving.&lt;/p&gt;
&lt;p&gt;NVIDIA also made its first submissions this round using the NVIDIA Dynamo inference framework.&lt;/p&gt;
&lt;p&gt;NVIDIA partners — including cloud service providers and server makers — submitted great results using the NVIDIA Blackwell and/or Hopper platform. These partners include Azure, Broadcom, Cisco, CoreWeave, Dell Technologies, Giga Computing, HPE, Lambda, Lenovo, Nebius, Oracle, Quanta Cloud Technology, Supermicro and the University of Florida.&lt;/p&gt;
&lt;p&gt;The market-leading inference performance on the NVIDIA AI platform is available from major cloud providers and server makers. This translates to lower TCO and enhanced return on investment for organizations deploying sophisticated AI applications.&lt;/p&gt;
&lt;p&gt;Learn more about these full-stack technologies by reading the NVIDIA Technical Blog on MLPerf Inference v5.1. Plus, visit the NVIDIA DGX Cloud Performance Explorer to learn more about NVIDIA performance, model TCO and generate custom reports.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/mlperf-inference-blackwell-ultra/</guid><pubDate>Tue, 09 Sep 2025 15:00:44 +0000</pubDate></item><item><title>[NEW] Smart ring maker Oura’s CEO addresses recent backlash, says future is a ‘cloud of wearables’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/smart-ring-maker-ouras-ceo-addresses-recent-backlash-says-future-is-a-cloud-of-wearables/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/oura-ceo-hale.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Oura CEO Tom Hale is trying to set the record straight about the smart ring maker’s partnership with the Department of Defense (DoD) and data miner Palantir, which is used by defense, intelligence, and law enforcement agencies in the United States and elsewhere. At the Fortune Brainstorm Tech conference on Monday, Hale’s interview started off with a bang with his outright denial that the company was sharing user data with the government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There was a lot of misinformation about this,” he said, referring to the numerous influencer-driven reports that led to a viral backlash against the health tracker. Oura’s rings collect information about users’ heart rates, sleep, body temperature, movement, menstrual cycles, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Hale had already gone online to address the misleading reports and subsequent PR backlash, assuring users in his first-ever TikTok video that the company didn’t sell their data to third parties “without your explicit consent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, he explained that the DoD program Oura is involved in requires the company to run its enterprise solution in a separate, secure environment and that the government does not have access to users’ Oura health data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale reiterated these points on Monday, saying, “For the record, we will never share your data with anyone unless you direct us to do it. We will never sell your data to anyone ever.” He said the reports spreading online that Oura partnered with the U.S. government to share user data were “simply not true,” and he’s thankful the outrage had begun to calm down. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, he attempted to clear up confusion over the company’s relationship with Palantir, saying that calling it a “partnership” was “a bit of a strong sell.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Hale explained that Oura had acquired a company last year that had a SaaS (software-as-a-service) relationship with Palantir — meaning a business contract rather than a data-sharing agreement. That relationship was for something called Impact Level 5, or IL5, which is a DoD certification standard for handling sensitive, unclassified data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a component of their solution. That contract is still running, and that news — that relationship — became blown into a ‘massive partnership’ with Palantir&amp;nbsp;… We have a small commercial relationship. The systems are not connected. There’s no way Palantir has access to your data. No one in the government can see your data. No one at Palantir can see your data. Totally overblown,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale added that the privacy and security of user data are important to the company and its customers. He also pointed out that Oura’s terms of service state that it will oppose any efforts designed to use user data for surveillance or prosecution purposes. He even noted that when users authorize Oura to examine their data (e.g., for tech support purposes), the person who reviews it has a limited role in the company and can only see specifically what was authorized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t look at people’s data&amp;nbsp;… you can’t do that,” he said. (Technically, they can — the data isn’t end-to-end encrypted. Data is encrypted in transit between the Oura App and the Oura Cloud using TLS 1.2.) &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The CEO also briefly addressed Oura’s future, observing that the market was shifting — particularly in Asia and India — to smaller, cheaper wrist-borne wearables. Ring wearables, meanwhile, doubled in size.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re growing north of 100%,” Hale noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company sees its potential as becoming a “preventionist” health device, one that alerts users to issues before they become problems that make them sick. This is aided by the fact that Oura rings are designed to give users insights about how their health metrics are evolving. The company also leverages machine intelligence and offers a dedicated health adviser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oura does see itself working more with the government, just not in the way that influencers described. Hale said the company partnered with Medicare Advantage to provide rings to eligible patients, for example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale also hinted at the possibility of other wearables. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’d be really cool if there was one ring to rule them all, but we know practically that’s not true,” he said. [W]hether it’s metabolic [monitoring], maybe it’s blood pressure, maybe it’s activity, maybe it’s other things — maybe it’s other kinds of metrics that are going to be brought together. So I believe very much that we’ll see a cloud of wearables. And the choice of those wearables will be relevant to the clinical use you’re trying to put it to.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/oura-ceo-hale.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Oura CEO Tom Hale is trying to set the record straight about the smart ring maker’s partnership with the Department of Defense (DoD) and data miner Palantir, which is used by defense, intelligence, and law enforcement agencies in the United States and elsewhere. At the Fortune Brainstorm Tech conference on Monday, Hale’s interview started off with a bang with his outright denial that the company was sharing user data with the government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There was a lot of misinformation about this,” he said, referring to the numerous influencer-driven reports that led to a viral backlash against the health tracker. Oura’s rings collect information about users’ heart rates, sleep, body temperature, movement, menstrual cycles, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Hale had already gone online to address the misleading reports and subsequent PR backlash, assuring users in his first-ever TikTok video that the company didn’t sell their data to third parties “without your explicit consent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, he explained that the DoD program Oura is involved in requires the company to run its enterprise solution in a separate, secure environment and that the government does not have access to users’ Oura health data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale reiterated these points on Monday, saying, “For the record, we will never share your data with anyone unless you direct us to do it. We will never sell your data to anyone ever.” He said the reports spreading online that Oura partnered with the U.S. government to share user data were “simply not true,” and he’s thankful the outrage had begun to calm down. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, he attempted to clear up confusion over the company’s relationship with Palantir, saying that calling it a “partnership” was “a bit of a strong sell.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Hale explained that Oura had acquired a company last year that had a SaaS (software-as-a-service) relationship with Palantir — meaning a business contract rather than a data-sharing agreement. That relationship was for something called Impact Level 5, or IL5, which is a DoD certification standard for handling sensitive, unclassified data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a component of their solution. That contract is still running, and that news — that relationship — became blown into a ‘massive partnership’ with Palantir&amp;nbsp;… We have a small commercial relationship. The systems are not connected. There’s no way Palantir has access to your data. No one in the government can see your data. No one at Palantir can see your data. Totally overblown,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale added that the privacy and security of user data are important to the company and its customers. He also pointed out that Oura’s terms of service state that it will oppose any efforts designed to use user data for surveillance or prosecution purposes. He even noted that when users authorize Oura to examine their data (e.g., for tech support purposes), the person who reviews it has a limited role in the company and can only see specifically what was authorized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t look at people’s data&amp;nbsp;… you can’t do that,” he said. (Technically, they can — the data isn’t end-to-end encrypted. Data is encrypted in transit between the Oura App and the Oura Cloud using TLS 1.2.) &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The CEO also briefly addressed Oura’s future, observing that the market was shifting — particularly in Asia and India — to smaller, cheaper wrist-borne wearables. Ring wearables, meanwhile, doubled in size.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re growing north of 100%,” Hale noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company sees its potential as becoming a “preventionist” health device, one that alerts users to issues before they become problems that make them sick. This is aided by the fact that Oura rings are designed to give users insights about how their health metrics are evolving. The company also leverages machine intelligence and offers a dedicated health adviser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Oura does see itself working more with the government, just not in the way that influencers described. Hale said the company partnered with Medicare Advantage to provide rings to eligible patients, for example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hale also hinted at the possibility of other wearables. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’d be really cool if there was one ring to rule them all, but we know practically that’s not true,” he said. [W]hether it’s metabolic [monitoring], maybe it’s blood pressure, maybe it’s activity, maybe it’s other things — maybe it’s other kinds of metrics that are going to be brought together. So I believe very much that we’ll see a cloud of wearables. And the choice of those wearables will be relevant to the clinical use you’re trying to put it to.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/smart-ring-maker-ouras-ceo-addresses-recent-backlash-says-future-is-a-cloud-of-wearables/</guid><pubDate>Tue, 09 Sep 2025 15:37:58 +0000</pubDate></item><item><title>[NEW] ‘Safety First, Always,’ NVIDIA VP of Automotive Says, Unveiling the Future of AI-Defined Vehicles at IAA Mobility (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/iaa-mobility-ai-defined-vehicles/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At this week’s IAA Mobility conference in Munich, NVIDIA Vice President of Automotive Ali Kani outlined how cloud-to-car AI platforms are bringing new levels of safety, intelligence and trust to the road.&lt;/p&gt;
&lt;p&gt;NVIDIA and its partners didn’t just show off cars at the conference — they showed off what cars are becoming: AI-defined machines, built as much in the data center as they are in the factory.&lt;/p&gt;
&lt;p&gt;Kani framed this shift during his IAA keynote today: vehicles are moving from being dependent on horsepower to compute power, from mechanical systems to software stacks.&lt;/p&gt;
&lt;p&gt;In Germany and around the world, automotive engineering is now infused with silicon acceleration, as automakers and suppliers adopt NVIDIA’s cloud-to-car platform to drive safety, intelligence and efficiency into tomorrow’s vehicles.&lt;/p&gt;
&lt;p&gt;NVIDIA is the only company that offers an end-to-end compute stack for autonomous driving. Its three AI compute platforms critical for autonomy are:&lt;/p&gt;

&lt;p&gt;Together, these platforms form a feedback loop for learning, testing and deployment that tightens the cycle of innovation while keeping safety front and center.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;It’s All About Safety: NVIDIA Halos Sets the Standard&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Safety is a core theme at IAA. NVIDIA Halos is a full-stack, comprehensive safety system that unifies vehicle architecture, AI models, chips, software, tools and services to ensure the safe development of autonomous vehicles, from cloud to car.&lt;/p&gt;
&lt;p&gt;NVIDIA Halos brings together safety-assessed systems-on-a-chip, the safety-certified NVIDIA DriveOS operating system and the DRIVE AGX Hyperion architecture into a unified platform for autonomous driving. This platform is backed by the NVIDIA Halos Certified Program and its AI Systems Inspection Lab, which deliver rigorous validation to ensure real-time AI operates with end-to-end reliability.&lt;/p&gt;
&lt;p&gt;With AI-driven workflows and high-fidelity sensor simulations built with NVIDIA Omniverse and Cosmos, automakers can train, test and safely validate vehicle performance — even under conditions that are hard to experiment with in the real world, such as rare or hazardous traffic situations and edge-case events, and in complex environments.&lt;/p&gt;
&lt;p&gt;Simulation tools are increasingly critical for advancing safe, scalable autonomous vehicle development.&lt;/p&gt;
&lt;p&gt;The popular autonomous driving simulator CARLA now integrates the NVIDIA Cosmos Transfer world foundation model, along with NVIDIA Omniverse NuRec reconstruction libraries, to bring diverse, high-fidelity simulations directly into autonomous vehicle testing pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Capgemini&lt;/strong&gt; and &lt;strong&gt;TCS&lt;/strong&gt; are already tapping into this integration to expand their simulation capabilities and push the boundaries of software-defined vehicle development.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Expanding the Ecosystem: Automotive Leaders Embrace Cloud-to-Car AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Automotive leaders are embracing NVIDIA’s cloud-to-car AI platform to transform their next-generation vehicles.&lt;b&gt;&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lucid&lt;/strong&gt; headlined the IAA showcase with its all-electric Lucid Gravity SUV, which is accelerated by the NVIDIA DRIVE AGX platform, uses the NVIDIA Blackwell architecture and operates on NVIDIA DriveOS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mercedes-Benz&lt;/strong&gt; introduced its all-new GLC with EQ-technology and announced expansions to its CLA family with the first fully electric shooting brake — all built on NVIDIA AI, DRIVE AV software and accelerated compute.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84663 size-large" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/mercedes-benz-iaa-2025-1680x1120.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lotus&lt;/strong&gt; is featuring the all-electric Eletre SUV, the hyper-GT Emeya and the Theory 1 concept — all accelerated by NVIDIA DRIVE AGX to deliver high-performance, AI-driven functions for intelligent and safer mobility.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84666 size-full" height="619" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/lotus-iaa-2025.jpg" width="1100" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ZYT&lt;/strong&gt; is showcasing its autonomous vehicle software platforms built on NVIDIA DRIVE AGX, highlighting how this advanced technology accelerates safer and smarter mobility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Volvo Cars&lt;/strong&gt; highlighted its ES90 Single Motor Extended Range Ultra and EX90 Twin Motor Performance Ultra models, equipped with enhanced safety and driver-assistance capabilities and powered by NVIDIA DRIVE AGX and DriveOS for improved AI performance and safety.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84669 size-full" height="900" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/volvo-cars-iaa-2025.jpg" width="1350" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Global Tech Leaders Accelerate Software-Defined Vehicles With NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Beyond automakers, technology leaders across the globe are building on NVIDIA AI to accelerate the development of software-defined vehicles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MediaTek&lt;/strong&gt; is working closely with NVIDIA to bring GPU-powered intelligence into its Dimensity Auto Cockpit solutions, enabling advanced in-car experiences through premium graphics and intelligent assistants.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ThunderSoft&lt;/strong&gt; introduced its new AI Box built on DRIVE AGX, designed to run large-scale AI models for intelligent cockpits. The AI Box is complete with personalized copilots, safety monitoring and immersive cabin experiences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cerence&lt;/strong&gt; is presenting its xUI AI assistant at IAA, built on CaLLM models and running on NVIDIA DRIVE AGX with DriveOS. With NVIDIA NeMo Guardrails, it ensures safe, context-aware, brand-specific voice interactions across both edge and cloud.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ZF Group&lt;/strong&gt; is showcasing its ProAI supercomputer accelerated by NVIDIA DRIVE AGX. The supercomputer unifies advanced driver-assistance systems, automated driving or chassis control into a scalable architecture to unlock capabilities, from entry-level deployments to full autonomy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RoboSense&lt;/strong&gt; is integrating its high-performance automotive-grade digital lidar with the DRIVE AGX platform, enhancing system performance, while Desay SV is showcasing its NVIDIA DRIVE Thor-based domain controller, a next-generation smart mobility solution shaped by advanced AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Magna&lt;/strong&gt; is showcasing its future-ready, centralized advanced driver-assistance system platform designed for flexibility and scalability. This advanced system integrates a comprehensive suite of sensors, accelerated by NVIDIA DRIVE AGX Thor.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch &lt;/i&gt;&lt;i&gt;Kani’s IAA keynote&lt;/i&gt;&lt;i&gt; to see how NVIDIA is accelerating the future of autonomous driving with a cloud-to-car platform. &lt;/i&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;Learn more about NVIDIA’s work in &lt;/i&gt;&lt;i&gt;autonomous vehicles&lt;/i&gt; and the &lt;i&gt;NVIDIA automotive partner ecosystem&lt;/i&gt;&lt;i&gt;. &lt;/i&gt;&lt;i&gt;Follow NVIDIA DRIVE on&lt;/i&gt;&lt;i&gt; LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At this week’s IAA Mobility conference in Munich, NVIDIA Vice President of Automotive Ali Kani outlined how cloud-to-car AI platforms are bringing new levels of safety, intelligence and trust to the road.&lt;/p&gt;
&lt;p&gt;NVIDIA and its partners didn’t just show off cars at the conference — they showed off what cars are becoming: AI-defined machines, built as much in the data center as they are in the factory.&lt;/p&gt;
&lt;p&gt;Kani framed this shift during his IAA keynote today: vehicles are moving from being dependent on horsepower to compute power, from mechanical systems to software stacks.&lt;/p&gt;
&lt;p&gt;In Germany and around the world, automotive engineering is now infused with silicon acceleration, as automakers and suppliers adopt NVIDIA’s cloud-to-car platform to drive safety, intelligence and efficiency into tomorrow’s vehicles.&lt;/p&gt;
&lt;p&gt;NVIDIA is the only company that offers an end-to-end compute stack for autonomous driving. Its three AI compute platforms critical for autonomy are:&lt;/p&gt;

&lt;p&gt;Together, these platforms form a feedback loop for learning, testing and deployment that tightens the cycle of innovation while keeping safety front and center.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;It’s All About Safety: NVIDIA Halos Sets the Standard&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Safety is a core theme at IAA. NVIDIA Halos is a full-stack, comprehensive safety system that unifies vehicle architecture, AI models, chips, software, tools and services to ensure the safe development of autonomous vehicles, from cloud to car.&lt;/p&gt;
&lt;p&gt;NVIDIA Halos brings together safety-assessed systems-on-a-chip, the safety-certified NVIDIA DriveOS operating system and the DRIVE AGX Hyperion architecture into a unified platform for autonomous driving. This platform is backed by the NVIDIA Halos Certified Program and its AI Systems Inspection Lab, which deliver rigorous validation to ensure real-time AI operates with end-to-end reliability.&lt;/p&gt;
&lt;p&gt;With AI-driven workflows and high-fidelity sensor simulations built with NVIDIA Omniverse and Cosmos, automakers can train, test and safely validate vehicle performance — even under conditions that are hard to experiment with in the real world, such as rare or hazardous traffic situations and edge-case events, and in complex environments.&lt;/p&gt;
&lt;p&gt;Simulation tools are increasingly critical for advancing safe, scalable autonomous vehicle development.&lt;/p&gt;
&lt;p&gt;The popular autonomous driving simulator CARLA now integrates the NVIDIA Cosmos Transfer world foundation model, along with NVIDIA Omniverse NuRec reconstruction libraries, to bring diverse, high-fidelity simulations directly into autonomous vehicle testing pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Capgemini&lt;/strong&gt; and &lt;strong&gt;TCS&lt;/strong&gt; are already tapping into this integration to expand their simulation capabilities and push the boundaries of software-defined vehicle development.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Expanding the Ecosystem: Automotive Leaders Embrace Cloud-to-Car AI&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Automotive leaders are embracing NVIDIA’s cloud-to-car AI platform to transform their next-generation vehicles.&lt;b&gt;&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lucid&lt;/strong&gt; headlined the IAA showcase with its all-electric Lucid Gravity SUV, which is accelerated by the NVIDIA DRIVE AGX platform, uses the NVIDIA Blackwell architecture and operates on NVIDIA DriveOS.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mercedes-Benz&lt;/strong&gt; introduced its all-new GLC with EQ-technology and announced expansions to its CLA family with the first fully electric shooting brake — all built on NVIDIA AI, DRIVE AV software and accelerated compute.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84663 size-large" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/mercedes-benz-iaa-2025-1680x1120.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lotus&lt;/strong&gt; is featuring the all-electric Eletre SUV, the hyper-GT Emeya and the Theory 1 concept — all accelerated by NVIDIA DRIVE AGX to deliver high-performance, AI-driven functions for intelligent and safer mobility.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84666 size-full" height="619" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/lotus-iaa-2025.jpg" width="1100" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ZYT&lt;/strong&gt; is showcasing its autonomous vehicle software platforms built on NVIDIA DRIVE AGX, highlighting how this advanced technology accelerates safer and smarter mobility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Volvo Cars&lt;/strong&gt; highlighted its ES90 Single Motor Extended Range Ultra and EX90 Twin Motor Performance Ultra models, equipped with enhanced safety and driver-assistance capabilities and powered by NVIDIA DRIVE AGX and DriveOS for improved AI performance and safety.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-84669 size-full" height="900" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/volvo-cars-iaa-2025.jpg" width="1350" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Global Tech Leaders Accelerate Software-Defined Vehicles With NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Beyond automakers, technology leaders across the globe are building on NVIDIA AI to accelerate the development of software-defined vehicles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MediaTek&lt;/strong&gt; is working closely with NVIDIA to bring GPU-powered intelligence into its Dimensity Auto Cockpit solutions, enabling advanced in-car experiences through premium graphics and intelligent assistants.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ThunderSoft&lt;/strong&gt; introduced its new AI Box built on DRIVE AGX, designed to run large-scale AI models for intelligent cockpits. The AI Box is complete with personalized copilots, safety monitoring and immersive cabin experiences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cerence&lt;/strong&gt; is presenting its xUI AI assistant at IAA, built on CaLLM models and running on NVIDIA DRIVE AGX with DriveOS. With NVIDIA NeMo Guardrails, it ensures safe, context-aware, brand-specific voice interactions across both edge and cloud.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ZF Group&lt;/strong&gt; is showcasing its ProAI supercomputer accelerated by NVIDIA DRIVE AGX. The supercomputer unifies advanced driver-assistance systems, automated driving or chassis control into a scalable architecture to unlock capabilities, from entry-level deployments to full autonomy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RoboSense&lt;/strong&gt; is integrating its high-performance automotive-grade digital lidar with the DRIVE AGX platform, enhancing system performance, while Desay SV is showcasing its NVIDIA DRIVE Thor-based domain controller, a next-generation smart mobility solution shaped by advanced AI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Magna&lt;/strong&gt; is showcasing its future-ready, centralized advanced driver-assistance system platform designed for flexibility and scalability. This advanced system integrates a comprehensive suite of sensors, accelerated by NVIDIA DRIVE AGX Thor.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch &lt;/i&gt;&lt;i&gt;Kani’s IAA keynote&lt;/i&gt;&lt;i&gt; to see how NVIDIA is accelerating the future of autonomous driving with a cloud-to-car platform. &lt;/i&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;i&gt;Learn more about NVIDIA’s work in &lt;/i&gt;&lt;i&gt;autonomous vehicles&lt;/i&gt; and the &lt;i&gt;NVIDIA automotive partner ecosystem&lt;/i&gt;&lt;i&gt;. &lt;/i&gt;&lt;i&gt;Follow NVIDIA DRIVE on&lt;/i&gt;&lt;i&gt; LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/iaa-mobility-ai-defined-vehicles/</guid><pubDate>Tue, 09 Sep 2025 16:00:22 +0000</pubDate></item><item><title>[NEW] Judge: Anthropic’s $1.5B settlement is being shoved “down the throat of authors” (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/09/judge-anthropics-1-5b-settlement-is-being-shoved-down-the-throat-of-authors/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Feeling “misled,” judge refuses to rubber-stamp Anthropic's proposed settlement.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2206295220-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2206295220-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          SOPA Images / Contributor | LightRocket

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;At a hearing Monday, US District Judge William Alsup blasted a proposed $1.5 billion settlement over Anthropic's rampant piracy of books to train AI.&lt;/p&gt;
&lt;p&gt;The proposed settlement comes in a case where Anthropic could have owed more than $1 trillion in damages after Alsup certified a class that included up to 7 million claimants whose works were illegally downloaded by the AI company.&lt;/p&gt;
&lt;p&gt;Instead, critics fear Anthropic will get off cheaply, striking a deal with authors suing that covers less than 500,000 works and paying a small fraction of its total valuation (currently $183 billion) to get away with the massive theft. Defector noted that the settlement doesn't even require Anthropic to admit wrongdoing, while the company continues raising billions based on models trained on authors' works. Most recently, Anthropic raised $13 billion in a funding round, making back about 10 times the proposed settlement amount after announcing the deal.&lt;/p&gt;
&lt;p&gt;Alsup expressed grave concerns that lawyers rushed the deal, which he said now risks being shoved "down the throat of authors," Bloomberg Law reported.&lt;/p&gt;
&lt;p&gt;In an order, Alsup clarified why he thought the proposed settlement was a chaotic mess. The judge said he was "disappointed that counsel have left important questions to be answered in the future," seeking approval for the settlement despite the Works List, the Class List, the Claim Form, and the process for notification, allocation, and dispute resolution all remaining unresolved.&lt;/p&gt;
&lt;p&gt;Denying preliminary approval of the settlement, Alsup suggested that the agreement is "nowhere close to complete," forcing Anthropic and authors' lawyers to "recalibrate" the largest publicly reported copyright class-action settlement ever inked, Bloomberg reported.&lt;/p&gt;
&lt;p&gt;Of particular concern, the settlement failed to outline how disbursements would be managed for works with multiple claimants, Alsup noted. Until all these details are ironed out, Alsup intends to withhold approval, the order said.&lt;/p&gt;
&lt;p&gt;One big change the judge wants to see is the addition of instructions requiring "anyone with copyright ownership" to opt in, with the consequence that the work won't be covered if even one rights holder opts out, Bloomberg reported. There should also be instruction that any disputes over ownership or submitted claims should be settled in state court, Alsup said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To Alsup, the settlement likely risks setting up a future where courts are bogged down over disputes linked to the class action for years. That's perhaps a bigger concern if many authors and publishers miss out on filing claims or receiving payments, since the judge noted that class members frequently "get the shaft" in class actions where attorneys stop caring after monetary relief is granted, Bloomberg reported. Further, Alsup is worried that an improper notification scheme could leave Anthropic in a vulnerable position, facing future claimants "coming out of the woodwork later," Bloomberg reported, despite doling out more than $1 billion.&lt;/p&gt;
&lt;p&gt;"When they pay that kind of money, they’re going to get the relief in the form of a clean bill of health going forward," Alsup said at the hearing, suggesting that the settlement must get Anthropic completely off the hook for future legal claims over the AI training piracy. Warning class counsel that&amp;nbsp;he felt "misled," the judge asked for more information about the claims process, noting, "I have an uneasy feeling about hangers-on with all this money on the table."&lt;/p&gt;
&lt;p&gt;Following the hearing, the judge set a schedule to ensure that lists of covered works and class members would be finalized by September 15, followed by the claims process finalized by September 25. That schedule would position the court to potentially preliminarily approve the settlement by October 10, Alsup suggested.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Why the deal only covers about 500,000 works&lt;/h2&gt;
&lt;p&gt;As of this writing, the list of covered works spans about 465,000, Alsup said.&lt;/p&gt;
&lt;p&gt;That's a far cry from the 7 million works that he initially certified as covered in the class. A breakdown from the Authors Guild—which consulted on the case and is part of a Working Group helping to allocate claims of $3,000 per work to authors and publishers—explained that "after accounting for the many duplicates," foreign editions, unregistered works, and books missing class criteria, "only approximately 500,000 titles meet the definition required to be part of the class."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But duplicate downloads and other missing criteria don't explain why the payout per work seems so small, and that's a problem for authors who want higher payouts since this settlement could become a template in other cases where AI companies are accused of pirating works for AI training.&lt;/p&gt;
&lt;p&gt;According to the Authors Guild, "the Copyright Act gives courts discretion to award statutory damages of at least $750 and no more than $150,000 per infringed work when the infringement is willful, as is the case here."&lt;/p&gt;
&lt;p&gt;However, "when there are a large number of works at issue," it's rare that courts award maximum damages, the group said. Hoping to avoid a dragged-out legal battle that "could tie up the case for years," authors suing Anthropic settled on "a strong payout without the risks of trial," the Authors Guild said. Going that route, they supposedly "avoided years of delay through appeals" and "achieved a certain, immediate result that sends a powerful signal to the industry that piracy will cost you a lot," the Authors Guild suggested. The settlement will also likely serve to push more AI companies to avoid piracy and actually pay to license content for training, the group said.&lt;/p&gt;
&lt;p&gt;The Authors Guild confirmed that once the list is finalized, likely by October 10, a searchable database will be created for authors to confirm if their works are covered. Until then, authors can submit contact information through a website set up to manage the settlement process. That will ensure that authors are notified when the claims process begins, which, if the settlement is ultimately approved, will likely happen this fall, the Authors Guild said.&lt;/p&gt;
&lt;p&gt;"If your book is included in the class list, you will receive a formal notice by mail or email from the settlement administrator," the group said. "The notice will explain the terms of the settlement, your rights, and next steps. The Authors Guild will also share information to help authors understand the process."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Feeling “misled,” judge refuses to rubber-stamp Anthropic's proposed settlement.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2206295220-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2206295220-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          SOPA Images / Contributor | LightRocket

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;At a hearing Monday, US District Judge William Alsup blasted a proposed $1.5 billion settlement over Anthropic's rampant piracy of books to train AI.&lt;/p&gt;
&lt;p&gt;The proposed settlement comes in a case where Anthropic could have owed more than $1 trillion in damages after Alsup certified a class that included up to 7 million claimants whose works were illegally downloaded by the AI company.&lt;/p&gt;
&lt;p&gt;Instead, critics fear Anthropic will get off cheaply, striking a deal with authors suing that covers less than 500,000 works and paying a small fraction of its total valuation (currently $183 billion) to get away with the massive theft. Defector noted that the settlement doesn't even require Anthropic to admit wrongdoing, while the company continues raising billions based on models trained on authors' works. Most recently, Anthropic raised $13 billion in a funding round, making back about 10 times the proposed settlement amount after announcing the deal.&lt;/p&gt;
&lt;p&gt;Alsup expressed grave concerns that lawyers rushed the deal, which he said now risks being shoved "down the throat of authors," Bloomberg Law reported.&lt;/p&gt;
&lt;p&gt;In an order, Alsup clarified why he thought the proposed settlement was a chaotic mess. The judge said he was "disappointed that counsel have left important questions to be answered in the future," seeking approval for the settlement despite the Works List, the Class List, the Claim Form, and the process for notification, allocation, and dispute resolution all remaining unresolved.&lt;/p&gt;
&lt;p&gt;Denying preliminary approval of the settlement, Alsup suggested that the agreement is "nowhere close to complete," forcing Anthropic and authors' lawyers to "recalibrate" the largest publicly reported copyright class-action settlement ever inked, Bloomberg reported.&lt;/p&gt;
&lt;p&gt;Of particular concern, the settlement failed to outline how disbursements would be managed for works with multiple claimants, Alsup noted. Until all these details are ironed out, Alsup intends to withhold approval, the order said.&lt;/p&gt;
&lt;p&gt;One big change the judge wants to see is the addition of instructions requiring "anyone with copyright ownership" to opt in, with the consequence that the work won't be covered if even one rights holder opts out, Bloomberg reported. There should also be instruction that any disputes over ownership or submitted claims should be settled in state court, Alsup said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To Alsup, the settlement likely risks setting up a future where courts are bogged down over disputes linked to the class action for years. That's perhaps a bigger concern if many authors and publishers miss out on filing claims or receiving payments, since the judge noted that class members frequently "get the shaft" in class actions where attorneys stop caring after monetary relief is granted, Bloomberg reported. Further, Alsup is worried that an improper notification scheme could leave Anthropic in a vulnerable position, facing future claimants "coming out of the woodwork later," Bloomberg reported, despite doling out more than $1 billion.&lt;/p&gt;
&lt;p&gt;"When they pay that kind of money, they’re going to get the relief in the form of a clean bill of health going forward," Alsup said at the hearing, suggesting that the settlement must get Anthropic completely off the hook for future legal claims over the AI training piracy. Warning class counsel that&amp;nbsp;he felt "misled," the judge asked for more information about the claims process, noting, "I have an uneasy feeling about hangers-on with all this money on the table."&lt;/p&gt;
&lt;p&gt;Following the hearing, the judge set a schedule to ensure that lists of covered works and class members would be finalized by September 15, followed by the claims process finalized by September 25. That schedule would position the court to potentially preliminarily approve the settlement by October 10, Alsup suggested.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Why the deal only covers about 500,000 works&lt;/h2&gt;
&lt;p&gt;As of this writing, the list of covered works spans about 465,000, Alsup said.&lt;/p&gt;
&lt;p&gt;That's a far cry from the 7 million works that he initially certified as covered in the class. A breakdown from the Authors Guild—which consulted on the case and is part of a Working Group helping to allocate claims of $3,000 per work to authors and publishers—explained that "after accounting for the many duplicates," foreign editions, unregistered works, and books missing class criteria, "only approximately 500,000 titles meet the definition required to be part of the class."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But duplicate downloads and other missing criteria don't explain why the payout per work seems so small, and that's a problem for authors who want higher payouts since this settlement could become a template in other cases where AI companies are accused of pirating works for AI training.&lt;/p&gt;
&lt;p&gt;According to the Authors Guild, "the Copyright Act gives courts discretion to award statutory damages of at least $750 and no more than $150,000 per infringed work when the infringement is willful, as is the case here."&lt;/p&gt;
&lt;p&gt;However, "when there are a large number of works at issue," it's rare that courts award maximum damages, the group said. Hoping to avoid a dragged-out legal battle that "could tie up the case for years," authors suing Anthropic settled on "a strong payout without the risks of trial," the Authors Guild said. Going that route, they supposedly "avoided years of delay through appeals" and "achieved a certain, immediate result that sends a powerful signal to the industry that piracy will cost you a lot," the Authors Guild suggested. The settlement will also likely serve to push more AI companies to avoid piracy and actually pay to license content for training, the group said.&lt;/p&gt;
&lt;p&gt;The Authors Guild confirmed that once the list is finalized, likely by October 10, a searchable database will be created for authors to confirm if their works are covered. Until then, authors can submit contact information through a website set up to manage the settlement process. That will ensure that authors are notified when the claims process begins, which, if the settlement is ultimately approved, will likely happen this fall, the Authors Guild said.&lt;/p&gt;
&lt;p&gt;"If your book is included in the class list, you will receive a formal notice by mail or email from the settlement administrator," the group said. "The notice will explain the terms of the settlement, your rights, and next steps. The Authors Guild will also share information to help authors understand the process."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/09/judge-anthropics-1-5b-settlement-is-being-shoved-down-the-throat-of-authors/</guid><pubDate>Tue, 09 Sep 2025 16:21:02 +0000</pubDate></item><item><title>[NEW] Nvidia unveils new GPU designed for long-context inference (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/09/nvidia-unveils-new-gpu-designed-for-long-context-inference/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205210966.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At the AI Infrastructure Summit on Tuesday, Nvidia announced a new GPU called the Rubin CPX, designed for context windows larger than 1 million tokens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of the chip giant’s forthcoming Rubin series, the CPX is optimized for processing large sequences of context and is meant to be used as part of a broader “disaggregated inference” infrastructure approach. For users, the result will be better performance on long-context tasks like video generation or software development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia’s relentless development cycle has resulted in enormous profits for the company, which brought in $41.1 billion in data center sales in its most recent quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Rubin CPX is slated to be available at the end of 2026.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205210966.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At the AI Infrastructure Summit on Tuesday, Nvidia announced a new GPU called the Rubin CPX, designed for context windows larger than 1 million tokens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of the chip giant’s forthcoming Rubin series, the CPX is optimized for processing large sequences of context and is meant to be used as part of a broader “disaggregated inference” infrastructure approach. For users, the result will be better performance on long-context tasks like video generation or software development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia’s relentless development cycle has resulted in enormous profits for the company, which brought in $41.1 billion in data center sales in its most recent quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Rubin CPX is slated to be available at the end of 2026.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/09/nvidia-unveils-new-gpu-designed-for-long-context-inference/</guid><pubDate>Tue, 09 Sep 2025 16:35:47 +0000</pubDate></item><item><title>[NEW] Accelerating scientific discovery with AI-powered empirical software (The latest research from Google)</title><link>https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;How it works&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;The input to our system is a scorable task, which includes a problem description, a scoring metric, and data suitable for training, validation, and evaluation. A user can also provide context, such as ideas from external literature, or directives for methodologies to prioritize.&lt;/p&gt;&lt;p&gt;The system then generates research ideas, including programmatic reproduction, optimization, and recombination of known methods, leading to novel and highly performant approaches. Ideas are implemented as executable code and the system uses a tree search strategy with an upper confidence bound (inspired by AlphaZero) to create a tree of software candidates and decide which candidates warrant further exploration. It then uses an LLM to rewrite the code to attempt to improve its quality score, and can exhaustively and tirelessly carry out solution searches at an unprecedented scale, identifying high-quality solutions quickly, reducing exploration time from months to hours or days. Its outputs, as coded solutions, are verifiable, interpretable and reproducible.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;How it works&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;The input to our system is a scorable task, which includes a problem description, a scoring metric, and data suitable for training, validation, and evaluation. A user can also provide context, such as ideas from external literature, or directives for methodologies to prioritize.&lt;/p&gt;&lt;p&gt;The system then generates research ideas, including programmatic reproduction, optimization, and recombination of known methods, leading to novel and highly performant approaches. Ideas are implemented as executable code and the system uses a tree search strategy with an upper confidence bound (inspired by AlphaZero) to create a tree of software candidates and decide which candidates warrant further exploration. It then uses an LLM to rewrite the code to attempt to improve its quality score, and can exhaustively and tirelessly carry out solution searches at an unprecedented scale, identifying high-quality solutions quickly, reducing exploration time from months to hours or days. Its outputs, as coded solutions, are verifiable, interpretable and reproducible.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/</guid><pubDate>Tue, 09 Sep 2025 17:08:01 +0000</pubDate></item></channel></rss>