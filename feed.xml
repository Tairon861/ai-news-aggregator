<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 04 Nov 2025 01:44:55 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>The beginning of the end of the transformer era? Neuro-symbolic AI startup AUI announces new funding at $750M valuation (AI | VentureBeat)</title><link>https://venturebeat.com/ai/the-beginning-of-the-end-of-the-transformer-era-neuro-symbolic-ai-startup</link><description>[unable to retrieve full-text content]&lt;p&gt;The buzzed-about but still stealthy New York City startup &lt;a href="https://www.aui.io/"&gt;Augmented Intelligence Inc (AUI)&lt;/a&gt;, which seeks to go beyond the popular &amp;quot;transformer&amp;quot; architecture used by most of today&amp;#x27;s LLMs such as ChatGPT and Gemini, has&lt;b&gt; raised $20 million in a bridge SAFE round at a $750 million valuation cap, bringing its total funding to nearly $60 million&lt;/b&gt;, VentureBeat can exclusively reveal.&lt;/p&gt;&lt;p&gt;The round, completed in under a week, comes amid heightened interest in deterministic conversational AI and precedes a larger raise now in advanced stages.&lt;/p&gt;&lt;p&gt;AUI relies on a fusion of the transformer tech and a newer technology called &amp;quot;neuro-symbolic AI,&amp;quot; described in greater detail below. &lt;/p&gt;&lt;p&gt;&amp;quot;We realize that you can combine the brilliance of LLMs in linguistic capabilities with the guarantees of symbolic AI,&amp;quot; said &lt;b&gt;Ohad Elhelo&lt;/b&gt;, &lt;b&gt;AUI co-founder and CEO&lt;/b&gt; in a recent interview with VentureBeat. Elhelo launched the company in 2017 alongside &lt;b&gt;co-founder and Chief Product Officer Ori Cohen.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;The new financing includes participation from eGateway Ventures, New Era Capital Partners, existing shareholders, and other strategic investors. It follows a $10 million raise in September 2024 at a $350 million valuation cap, coinciding with the &lt;a href="https://cloud.google.com/blog/topics/partners/google-cloud-partners-with-aui/"&gt;company’s announced go-to-market partnership with Google&lt;/a&gt; in October 2024. Early investors include Vertex Pharmaceuticals founder Joshua Boger, UKG Chairman Aron Ain, and former IBM President Jim Whitehurst.&lt;/p&gt;&lt;p&gt;According to the company, the bridge round is a precursor to a significantly larger raise already in advanced stages.&lt;/p&gt;&lt;p&gt;AUI is the &lt;a href="https://venturebeat.com/ai/has-this-stealth-startup-finally-cracked-the-code-on-enterprise-ai-agent"&gt;company behind Apollo-1&lt;/a&gt;, a new foundation model built for task-oriented dialog, which it describes as the &amp;quot;economic half&amp;quot; of conversational AI — distinct from the open-ended dialog handled by LLMs like ChatGPT and Gemini. &lt;/p&gt;&lt;p&gt;The firm argues that existing LLMs lack the determinism, policy enforcement, and operational certainty required by enterprises, especially in regulated sectors.&lt;/p&gt;&lt;p&gt;Chris Varelas, co-founder of Redwood Capital and an advisor to AUI, said in a press release provided to VentureBeat: “I’ve seen some of today’s top AI leaders walk away with their heads spinning after interacting with Apollo-1.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Distinctive Neuro-Symbolic Architecture&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Apollo-1’s core innovation is its neuro-symbolic architecture, which separates linguistic fluency from task reasoning. Instead of using the most common technology underpinning most LLMs and conversational AI systems today — the vaunted transformer architecture described in the seminal 2017 Google paper &amp;quot;Attention Is All You Need&amp;quot; — AUI&amp;#x27;s system integrates two layers:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Neural modules, powered by LLMs, handle perception: encoding user inputs and generating natural language responses.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A symbolic reasoning engine, developed over several years, interprets structured task elements such as intents, entities, and parameters. This symbolic state engine determines the appropriate next actions using deterministic logic.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This hybrid architecture allows Apollo-1 to maintain state continuity, enforce organizational policies, and reliably trigger tool or API calls — capabilities that transformer-only agents lack.&lt;/p&gt;&lt;p&gt;Elhelo said this design emerged from a multi-year data collection effort: “We built a consumer service and recorded millions of human-agent interactions across 60,000 live agents. From that, we abstracted a symbolic language that defines the structure of task-based dialogs, separate from their domain-specific content.”&lt;/p&gt;&lt;p&gt;However, enterprises that have already built systems built around transformer LLMs needn&amp;#x27;t worry. AUI wants to make adopting its new technology just as easy. &lt;/p&gt;&lt;p&gt;&amp;quot;Apollo-1 deploys like any modern foundation model,&amp;quot; Elhelo told VentureBeat in a text last night. &amp;quot;It doesn’t require dedicated or proprietary clusters to run. It operates across standard cloud and hybrid environments, leveraging both GPUs and CPUs, and is significantly more cost-efficient to deploy than frontier reasoning models. Apollo-1 can also be deployed across all major clouds in a separated environment for increased security.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Generalization and Domain Flexibility&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Apollo-1 is described as a foundation model for task-oriented dialog, meaning it is domain-agnostic and generalizable across verticals like healthcare, travel, insurance, and retail.&lt;/p&gt;&lt;p&gt;Unlike consulting-heavy AI platforms that require building bespoke logic per client, Apollo-1 allows enterprises to define behaviors and tools within a shared symbolic language. This approach supports faster onboarding and reduces long-term maintenance. According to the team, an enterprise can launch a working agent in under a day.&lt;/p&gt;&lt;p&gt;Crucially, procedural rules are encoded at the symbolic layer — not learned from examples. This enables deterministic execution for sensitive or regulated tasks. &lt;/p&gt;&lt;p&gt;For instance, a system can block cancellation of a Basic Economy flight not by guessing intent but by applying hard-coded logic to a symbolic representation of the booking class.&lt;/p&gt;&lt;p&gt;As Elhelo explained to VentureBeat, LLMs are &amp;quot;not a good mechanism when you’re looking for certainty. It’s better if you know what you’re going to send [to an AI model] and always send it, and you know, always, what’s going to come back [to the user] and how to handle that.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Availability and Developer Access&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Apollo-1 is already in active use within Fortune 500 enterprises in a closed beta, and a broader general availability release is expected before the end of 2025, according to a &lt;a href="https://www.theinformation.com/articles/startup-teaching-ai-agents-shop"&gt;previous report by &lt;i&gt;The Information&lt;/i&gt;&lt;/a&gt;&lt;i&gt;, &lt;/i&gt;which broke the initial news on the startup.&lt;/p&gt;&lt;p&gt;Enterprises can integrate with Apollo-1 either via:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A developer playground, where business users and technical teams jointly configure policies, rules, and behaviors; or&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A standard API, using OpenAI-compatible formats.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The model supports policy enforcement, rule-based customization, and steering via guardrails. Symbolic rules allow businesses to dictate fixed behaviors, while LLM modules handle open-text interpretation and user interaction.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Enterprise Fit: When Reliability Beats Fluency&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While LLMs have advanced general-purpose dialog and creativity, they remain probabilistic — a barrier to enterprise deployment in finance, healthcare, and customer service. &lt;/p&gt;&lt;p&gt;Apollo-1 targets this gap by offering a system where policy adherence and deterministic task completion are first-class design goals.&lt;/p&gt;&lt;p&gt;Elhelo puts it plainly: “If your use case is task-oriented dialog, you have to use us, even if you are ChatGPT.”&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;The buzzed-about but still stealthy New York City startup &lt;a href="https://www.aui.io/"&gt;Augmented Intelligence Inc (AUI)&lt;/a&gt;, which seeks to go beyond the popular &amp;quot;transformer&amp;quot; architecture used by most of today&amp;#x27;s LLMs such as ChatGPT and Gemini, has&lt;b&gt; raised $20 million in a bridge SAFE round at a $750 million valuation cap, bringing its total funding to nearly $60 million&lt;/b&gt;, VentureBeat can exclusively reveal.&lt;/p&gt;&lt;p&gt;The round, completed in under a week, comes amid heightened interest in deterministic conversational AI and precedes a larger raise now in advanced stages.&lt;/p&gt;&lt;p&gt;AUI relies on a fusion of the transformer tech and a newer technology called &amp;quot;neuro-symbolic AI,&amp;quot; described in greater detail below. &lt;/p&gt;&lt;p&gt;&amp;quot;We realize that you can combine the brilliance of LLMs in linguistic capabilities with the guarantees of symbolic AI,&amp;quot; said &lt;b&gt;Ohad Elhelo&lt;/b&gt;, &lt;b&gt;AUI co-founder and CEO&lt;/b&gt; in a recent interview with VentureBeat. Elhelo launched the company in 2017 alongside &lt;b&gt;co-founder and Chief Product Officer Ori Cohen.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;The new financing includes participation from eGateway Ventures, New Era Capital Partners, existing shareholders, and other strategic investors. It follows a $10 million raise in September 2024 at a $350 million valuation cap, coinciding with the &lt;a href="https://cloud.google.com/blog/topics/partners/google-cloud-partners-with-aui/"&gt;company’s announced go-to-market partnership with Google&lt;/a&gt; in October 2024. Early investors include Vertex Pharmaceuticals founder Joshua Boger, UKG Chairman Aron Ain, and former IBM President Jim Whitehurst.&lt;/p&gt;&lt;p&gt;According to the company, the bridge round is a precursor to a significantly larger raise already in advanced stages.&lt;/p&gt;&lt;p&gt;AUI is the &lt;a href="https://venturebeat.com/ai/has-this-stealth-startup-finally-cracked-the-code-on-enterprise-ai-agent"&gt;company behind Apollo-1&lt;/a&gt;, a new foundation model built for task-oriented dialog, which it describes as the &amp;quot;economic half&amp;quot; of conversational AI — distinct from the open-ended dialog handled by LLMs like ChatGPT and Gemini. &lt;/p&gt;&lt;p&gt;The firm argues that existing LLMs lack the determinism, policy enforcement, and operational certainty required by enterprises, especially in regulated sectors.&lt;/p&gt;&lt;p&gt;Chris Varelas, co-founder of Redwood Capital and an advisor to AUI, said in a press release provided to VentureBeat: “I’ve seen some of today’s top AI leaders walk away with their heads spinning after interacting with Apollo-1.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Distinctive Neuro-Symbolic Architecture&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Apollo-1’s core innovation is its neuro-symbolic architecture, which separates linguistic fluency from task reasoning. Instead of using the most common technology underpinning most LLMs and conversational AI systems today — the vaunted transformer architecture described in the seminal 2017 Google paper &amp;quot;Attention Is All You Need&amp;quot; — AUI&amp;#x27;s system integrates two layers:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Neural modules, powered by LLMs, handle perception: encoding user inputs and generating natural language responses.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A symbolic reasoning engine, developed over several years, interprets structured task elements such as intents, entities, and parameters. This symbolic state engine determines the appropriate next actions using deterministic logic.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This hybrid architecture allows Apollo-1 to maintain state continuity, enforce organizational policies, and reliably trigger tool or API calls — capabilities that transformer-only agents lack.&lt;/p&gt;&lt;p&gt;Elhelo said this design emerged from a multi-year data collection effort: “We built a consumer service and recorded millions of human-agent interactions across 60,000 live agents. From that, we abstracted a symbolic language that defines the structure of task-based dialogs, separate from their domain-specific content.”&lt;/p&gt;&lt;p&gt;However, enterprises that have already built systems built around transformer LLMs needn&amp;#x27;t worry. AUI wants to make adopting its new technology just as easy. &lt;/p&gt;&lt;p&gt;&amp;quot;Apollo-1 deploys like any modern foundation model,&amp;quot; Elhelo told VentureBeat in a text last night. &amp;quot;It doesn’t require dedicated or proprietary clusters to run. It operates across standard cloud and hybrid environments, leveraging both GPUs and CPUs, and is significantly more cost-efficient to deploy than frontier reasoning models. Apollo-1 can also be deployed across all major clouds in a separated environment for increased security.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Generalization and Domain Flexibility&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Apollo-1 is described as a foundation model for task-oriented dialog, meaning it is domain-agnostic and generalizable across verticals like healthcare, travel, insurance, and retail.&lt;/p&gt;&lt;p&gt;Unlike consulting-heavy AI platforms that require building bespoke logic per client, Apollo-1 allows enterprises to define behaviors and tools within a shared symbolic language. This approach supports faster onboarding and reduces long-term maintenance. According to the team, an enterprise can launch a working agent in under a day.&lt;/p&gt;&lt;p&gt;Crucially, procedural rules are encoded at the symbolic layer — not learned from examples. This enables deterministic execution for sensitive or regulated tasks. &lt;/p&gt;&lt;p&gt;For instance, a system can block cancellation of a Basic Economy flight not by guessing intent but by applying hard-coded logic to a symbolic representation of the booking class.&lt;/p&gt;&lt;p&gt;As Elhelo explained to VentureBeat, LLMs are &amp;quot;not a good mechanism when you’re looking for certainty. It’s better if you know what you’re going to send [to an AI model] and always send it, and you know, always, what’s going to come back [to the user] and how to handle that.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Availability and Developer Access&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Apollo-1 is already in active use within Fortune 500 enterprises in a closed beta, and a broader general availability release is expected before the end of 2025, according to a &lt;a href="https://www.theinformation.com/articles/startup-teaching-ai-agents-shop"&gt;previous report by &lt;i&gt;The Information&lt;/i&gt;&lt;/a&gt;&lt;i&gt;, &lt;/i&gt;which broke the initial news on the startup.&lt;/p&gt;&lt;p&gt;Enterprises can integrate with Apollo-1 either via:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A developer playground, where business users and technical teams jointly configure policies, rules, and behaviors; or&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A standard API, using OpenAI-compatible formats.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The model supports policy enforcement, rule-based customization, and steering via guardrails. Symbolic rules allow businesses to dictate fixed behaviors, while LLM modules handle open-text interpretation and user interaction.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Enterprise Fit: When Reliability Beats Fluency&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While LLMs have advanced general-purpose dialog and creativity, they remain probabilistic — a barrier to enterprise deployment in finance, healthcare, and customer service. &lt;/p&gt;&lt;p&gt;Apollo-1 targets this gap by offering a system where policy adherence and deterministic task completion are first-class design goals.&lt;/p&gt;&lt;p&gt;Elhelo puts it plainly: “If your use case is task-oriented dialog, you have to use us, even if you are ChatGPT.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-beginning-of-the-end-of-the-transformer-era-neuro-symbolic-ai-startup</guid><pubDate>Mon, 03 Nov 2025 14:00:00 +0000</pubDate></item><item><title>LG founder’s grandson, production firm partner up to bring AI to filmmaking (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/03/lg-scions-stock-farm-road-utopai-launch-ai-driven-filmmaking/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Since AI tools went mainstream, filmmakers, writers, and actors have been scrambling to figure out whether these technologies can truly assist their creativity or if they might end up replacing humans. But there’s a larger concern to address before we get swept away by debate: AI can’t run without enormous data centers and energy infrastructure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new joint venture, dubbed Utopai East, aims to address that need by developing infrastructure specifically for producing movies and TV shows using AI. The joint venture is held 50-50 by investment firm Stock Farm Road (SFR) and AI film and television production company Utopai Studios. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;SFR, co-founded by Brian Koo (the grandson of LG Group’s founder Koo In-hwoi) and Amin Badr-El-Din, the founder and chief executive of BADR Investments, is contributing capital to the joint venture, along with creative expertise and industry contacts. Utopai, meanwhile, is providing the technology, workflow, and infrastructure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project will also involve co-producing film and television projects and expanding access to Korean intellectual property for international audiences. Production will begin using existing infrastructure, and the company expects the first piece of content from this collaboration to be released next year, according to Cecilia Shen, co-founder and CEO of Utopai Studios.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the short term, using AI is going to be primarily about lowering costs and increasing efficiency, Koo told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“But beyond that, we’re very excited about the new possibilities AI opens up. As we engage with creators, we’re exploring what entirely new things can become possible. Right now, some of our early focus is on creators in Korea,” Koo said. “Just as short-form content was a novelty when it first emerged, we see opportunities for fresh approaches. We’re working not only with established directors in cinema but also with young, innovative creators who aren’t limited to traditional movies.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3063430" height="387" src="https://techcrunch.com/wp-content/uploads/2025/11/Cecilia-Shen-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Cecilia Shen, co-founder and CEO of Utopai Studios.&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;But novelty alone won’t allay the concerns of the people working in the entertainment industry, or the ones consuming the content. AI could one day replace people in creative roles like acting, performance, and writing, yet it often lacks the depth, nuance, and emotional resonance of human storytelling. This has sparked a broader debate about the value of human creativity in an era when machines can mimic, but not fully replicate, the human touch.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But Shen and Koo maintain that their use of AI is only aimed at improving existing processes. “These questions have been at the center of everything we build at Utopai Studios,” Shen said. “From the beginning, our focus was never on automation. Our workflow is designed to work alongside filmmakers, not in place of them. We still need writers to write, directors to direct, and actors to perform,” Shen said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Every model and every dataset used is fully licensed and contractually approved, ensuring the technology respects the creators whose work makes filmmaking possible, Shen added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want creators to understand that AI can expand their creative potential rather than compete with them. It can help bring their dreams to life, giving them the freedom to fully explore their creativity without worrying that AI will replace them. This, we believe, is going to be one of the most exciting outcomes for us,” Koo said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Typically, content and IP grow incrementally — one IP develops after another — but with the right technology, especially AI, there’s potential for exponential growth. This isn’t about AI replacing people; it’s about the massive value it can create for audiences, creators, and engineers alike,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal follows SFR’s recent agreement with the Jeollanam-do Province government to build a 3 gigawatt AI data center in South Korea.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The data center is part of our larger mission at Stock Farm Road to build the backbone for the next generation of intelligence-driven industries. Beyond Utopai Studios and entertainment, we are also focused on areas such as manufacturing, energy-to-information, AI, and quantum computing. These are all interconnected fields that require the same kind of infrastructure,” said Koo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The data center will serve as the foundation for everything Utopai East is developing and will include complete AI infrastructure for entertainment content, spanning data management, creative intelligence, production and distribution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the joint venture’s financial details were not disclosed, the capital is coming from multiple channels, including SFR’s investment vehicles, global sovereign and institutional investors, and industry partners in film and entertainment, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The JV will start off making Korean content but aims to expand to other parts of Asia eventually. “Japan is always also a great market,” making it a natural starting point for expansion, Shen noted, adding that she also sees significant potential in China and Thailand.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Since AI tools went mainstream, filmmakers, writers, and actors have been scrambling to figure out whether these technologies can truly assist their creativity or if they might end up replacing humans. But there’s a larger concern to address before we get swept away by debate: AI can’t run without enormous data centers and energy infrastructure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new joint venture, dubbed Utopai East, aims to address that need by developing infrastructure specifically for producing movies and TV shows using AI. The joint venture is held 50-50 by investment firm Stock Farm Road (SFR) and AI film and television production company Utopai Studios. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;SFR, co-founded by Brian Koo (the grandson of LG Group’s founder Koo In-hwoi) and Amin Badr-El-Din, the founder and chief executive of BADR Investments, is contributing capital to the joint venture, along with creative expertise and industry contacts. Utopai, meanwhile, is providing the technology, workflow, and infrastructure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project will also involve co-producing film and television projects and expanding access to Korean intellectual property for international audiences. Production will begin using existing infrastructure, and the company expects the first piece of content from this collaboration to be released next year, according to Cecilia Shen, co-founder and CEO of Utopai Studios.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the short term, using AI is going to be primarily about lowering costs and increasing efficiency, Koo told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“But beyond that, we’re very excited about the new possibilities AI opens up. As we engage with creators, we’re exploring what entirely new things can become possible. Right now, some of our early focus is on creators in Korea,” Koo said. “Just as short-form content was a novelty when it first emerged, we see opportunities for fresh approaches. We’re working not only with established directors in cinema but also with young, innovative creators who aren’t limited to traditional movies.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3063430" height="387" src="https://techcrunch.com/wp-content/uploads/2025/11/Cecilia-Shen-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Cecilia Shen, co-founder and CEO of Utopai Studios.&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;But novelty alone won’t allay the concerns of the people working in the entertainment industry, or the ones consuming the content. AI could one day replace people in creative roles like acting, performance, and writing, yet it often lacks the depth, nuance, and emotional resonance of human storytelling. This has sparked a broader debate about the value of human creativity in an era when machines can mimic, but not fully replicate, the human touch.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But Shen and Koo maintain that their use of AI is only aimed at improving existing processes. “These questions have been at the center of everything we build at Utopai Studios,” Shen said. “From the beginning, our focus was never on automation. Our workflow is designed to work alongside filmmakers, not in place of them. We still need writers to write, directors to direct, and actors to perform,” Shen said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Every model and every dataset used is fully licensed and contractually approved, ensuring the technology respects the creators whose work makes filmmaking possible, Shen added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want creators to understand that AI can expand their creative potential rather than compete with them. It can help bring their dreams to life, giving them the freedom to fully explore their creativity without worrying that AI will replace them. This, we believe, is going to be one of the most exciting outcomes for us,” Koo said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Typically, content and IP grow incrementally — one IP develops after another — but with the right technology, especially AI, there’s potential for exponential growth. This isn’t about AI replacing people; it’s about the massive value it can create for audiences, creators, and engineers alike,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal follows SFR’s recent agreement with the Jeollanam-do Province government to build a 3 gigawatt AI data center in South Korea.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The data center is part of our larger mission at Stock Farm Road to build the backbone for the next generation of intelligence-driven industries. Beyond Utopai Studios and entertainment, we are also focused on areas such as manufacturing, energy-to-information, AI, and quantum computing. These are all interconnected fields that require the same kind of infrastructure,” said Koo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The data center will serve as the foundation for everything Utopai East is developing and will include complete AI infrastructure for entertainment content, spanning data management, creative intelligence, production and distribution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the joint venture’s financial details were not disclosed, the capital is coming from multiple channels, including SFR’s investment vehicles, global sovereign and institutional investors, and industry partners in film and entertainment, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The JV will start off making Korean content but aims to expand to other parts of Asia eventually. “Japan is always also a great market,” making it a natural starting point for expansion, Shen noted, adding that she also sees significant potential in China and Thailand.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/03/lg-scions-stock-farm-road-utopai-launch-ai-driven-filmmaking/</guid><pubDate>Mon, 03 Nov 2025 14:00:00 +0000</pubDate></item><item><title>Microsoft inks $9.7B deal with Australia’s IREN for AI cloud capacity (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/03/microsoft-inks-9-7bil-deal-with-australias-iren-for-ai-cloud-capacity/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1883327378-e1730136121848.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft is leaving no stone unturned in its quest to secure more compute capacity for meeting its customers’ heavy demand for AI services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, the Redmond-based tech giant signed a $9.7 billion, five-year contract with Australia’s IREN to secure further AI cloud capacity. The deal will give Microsoft access to compute infrastructure built with Nvidia’s GB300 GPUs, which will be deployed over phases through 2026 at IREN’s facility in Childress, Texas, planned to support 750 megawatts of capacity.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;IREN said it is separately buying GPUs and equipment from Dell for about $5.8 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal comes after Microsoft last month launched its first production cluster with Nvidia’s GB300 NVL72 systems for Azure, which, the company said, are optimized for reasoning models, agentic AI systems, and multimodal generative AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Microsoft signed a deal with Nscale for approximately 200,000 Nvidia GB300 GPUs to three data centers in Europe and one in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Similar to competitors like CoreWeave, IREN started off as a bitcoin-mining operation but quickly realized that its massive collection of GPUs was better put to use for AI workloads. The company has benefited massively from the shift in focus. The company’s CEO, Daniel Roberts, expects the Microsoft deal to take up only 10% of the company’s total capacity and generate about $1.94 billion in annualized revenue, Bloomberg reported.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1883327378-e1730136121848.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft is leaving no stone unturned in its quest to secure more compute capacity for meeting its customers’ heavy demand for AI services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, the Redmond-based tech giant signed a $9.7 billion, five-year contract with Australia’s IREN to secure further AI cloud capacity. The deal will give Microsoft access to compute infrastructure built with Nvidia’s GB300 GPUs, which will be deployed over phases through 2026 at IREN’s facility in Childress, Texas, planned to support 750 megawatts of capacity.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;IREN said it is separately buying GPUs and equipment from Dell for about $5.8 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal comes after Microsoft last month launched its first production cluster with Nvidia’s GB300 NVL72 systems for Azure, which, the company said, are optimized for reasoning models, agentic AI systems, and multimodal generative AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Microsoft signed a deal with Nscale for approximately 200,000 Nvidia GB300 GPUs to three data centers in Europe and one in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Similar to competitors like CoreWeave, IREN started off as a bitcoin-mining operation but quickly realized that its massive collection of GPUs was better put to use for AI workloads. The company has benefited massively from the shift in focus. The company’s CEO, Daniel Roberts, expects the Microsoft deal to take up only 10% of the company’s total capacity and generate about $1.94 billion in annualized revenue, Bloomberg reported.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/03/microsoft-inks-9-7bil-deal-with-australias-iren-for-ai-cloud-capacity/</guid><pubDate>Mon, 03 Nov 2025 14:04:19 +0000</pubDate></item><item><title>Microsoft’s $15.2B UAE investment turns Gulf State into test case for US AI diplomacy (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/03/microsofts-15-2b-uae-investment-turns-gulf-state-into-test-case-for-us-ai-diplomacy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/microsoft-uae.jpg?w=960" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft will invest $15.2 billion in the United Arab Emirates over the next four years, the company announced Monday at the first annual Abu Dhabi Global AI Summit.&amp;nbsp;The investment will include the first-ever shipments of the most advanced Nvidia GPUs to the UAE.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the deal, the U.S. has granted Microsoft a license to export Nvidia chips to the UAE, a move that positions the country as both a proving ground for U.S. export-control diplomacy and a regional anchor of American AI influence.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal allows Microsoft to expand its foothold into the Middle East, a key region in the global fight for AI dominance.&amp;nbsp;In May, President Donald Trump struck a deal with UAE president Sheikh Mohamed bin Zayed Al Nahyan to build an AI data center campus in Abu Dhabi. The project was delayed due to U.S. export controls, which restricted the sale of powerful Nvidia chips needed to run advanced AI systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft became the first company to receive a license from the U.S. Commerce Department to ship the chips to the UAE in September. The move comes as critics say the deal undermines the logic of the U.S.’s export restrictions to China by introducing possible back channels through a Chinese ally.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a statement, Microsoft said it performed substantial work to meet the strong cybersecurity and national security conditions required by the licenses, which has enabled the firm to accumulate the equivalent of 21,500 Nvidia A100 GPUs in the UAE, based on a combination of A100, H100, and H200 chips. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft said it is using the chips to provide access to AI models from OpenAI, Anthropic, open source providers, and itself.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The $15.2 billion figure includes money Microsoft began spending in the UAE starting in 2023 as part of a new AI initiative in the country. Between 2023 and the end of 2025, Microsoft will have spent just over $7.3 billion in the UAE, including a $1.5 billion equity investment in G42, the UAE’s sovereign AI company, and more than $4.6 billion in capital toward data centers.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the new deal, Microsoft pledges to spend $7.9 billion more in the UAE from the start of 2026 to the end of 2029, including $5.5 billion in capital expenses for ongoing and planned expansion of AI and cloud infrastructure. Microsoft hinted at new steps it will share publicly in Abu Dhabi this week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s work in the UAE goes beyond building data centers. The company says it is pairing massive AI infrastructure with deep investment in local talent, training, and governance. The firm is pledging to train a million residents by 2027 and use Abu Dhabi as a regional hub for AI research and model development. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The investment comes the same day that Microsoft signed a $9.7 billion deal with Australia’s IREN for AI cloud capacity.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/microsoft-uae.jpg?w=960" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft will invest $15.2 billion in the United Arab Emirates over the next four years, the company announced Monday at the first annual Abu Dhabi Global AI Summit.&amp;nbsp;The investment will include the first-ever shipments of the most advanced Nvidia GPUs to the UAE.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the deal, the U.S. has granted Microsoft a license to export Nvidia chips to the UAE, a move that positions the country as both a proving ground for U.S. export-control diplomacy and a regional anchor of American AI influence.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal allows Microsoft to expand its foothold into the Middle East, a key region in the global fight for AI dominance.&amp;nbsp;In May, President Donald Trump struck a deal with UAE president Sheikh Mohamed bin Zayed Al Nahyan to build an AI data center campus in Abu Dhabi. The project was delayed due to U.S. export controls, which restricted the sale of powerful Nvidia chips needed to run advanced AI systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft became the first company to receive a license from the U.S. Commerce Department to ship the chips to the UAE in September. The move comes as critics say the deal undermines the logic of the U.S.’s export restrictions to China by introducing possible back channels through a Chinese ally.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a statement, Microsoft said it performed substantial work to meet the strong cybersecurity and national security conditions required by the licenses, which has enabled the firm to accumulate the equivalent of 21,500 Nvidia A100 GPUs in the UAE, based on a combination of A100, H100, and H200 chips. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft said it is using the chips to provide access to AI models from OpenAI, Anthropic, open source providers, and itself.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The $15.2 billion figure includes money Microsoft began spending in the UAE starting in 2023 as part of a new AI initiative in the country. Between 2023 and the end of 2025, Microsoft will have spent just over $7.3 billion in the UAE, including a $1.5 billion equity investment in G42, the UAE’s sovereign AI company, and more than $4.6 billion in capital toward data centers.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the new deal, Microsoft pledges to spend $7.9 billion more in the UAE from the start of 2026 to the end of 2029, including $5.5 billion in capital expenses for ongoing and planned expansion of AI and cloud infrastructure. Microsoft hinted at new steps it will share publicly in Abu Dhabi this week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s work in the UAE goes beyond building data centers. The company says it is pairing massive AI infrastructure with deep investment in local talent, training, and governance. The firm is pledging to train a million residents by 2027 and use Abu Dhabi as a regional hub for AI research and model development. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The investment comes the same day that Microsoft signed a $9.7 billion deal with Australia’s IREN for AI cloud capacity.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/03/microsofts-15-2b-uae-investment-turns-gulf-state-into-test-case-for-us-ai-diplomacy/</guid><pubDate>Mon, 03 Nov 2025 14:22:48 +0000</pubDate></item><item><title>Strengthening Our Core: Welcoming Karyne Levy as VentureBeat’s New Managing Editor (AI | VentureBeat)</title><link>https://venturebeat.com/ai/strengthening-our-core-welcoming-karyne-levy-as-venturebeats-new-managing</link><description>[unable to retrieve full-text content]&lt;p&gt;I’m thrilled to announce a fantastic new addition to our leadership team: &lt;b&gt;Karyne Levy&lt;/b&gt; is joining VentureBeat as our new Managing Editor. Today is her first day.&lt;/p&gt;&lt;p&gt;Many of you may know Karyne from her most recent role as Deputy Managing Editor at TechCrunch, but her career is a highlight reel of veteran tech journalism. Her resume includes pivotal roles at &lt;b&gt;Protocol, NerdWallet, Business Insider, and CNET&lt;/b&gt;, giving her a deep understanding of this industry from every angle.&lt;/p&gt;&lt;p&gt;Hiring Karyne is a significant step forward for VentureBeat. As we’ve sharpened our focus on serving you – the enterprise technical decision-maker navigating the complexities of AI and data – I’ve been looking for a very specific kind of leader.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The &amp;quot;Organizer&amp;#x27;s Dopamine Hit&amp;quot;&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;In the past, a managing editor was often the final backstop for copy. Today, at a modern, data-focused media company like ours, the role is infinitely more dynamic. It’s the central hub of the entire content operation.&lt;/p&gt;&lt;p&gt;During my search, I found myself talking a lot about the two types of &amp;quot;dopamine hits&amp;quot; in our business. There’s the writer’s hit – seeing your name on a great story. And then there’s the organizer’s hit – the satisfaction that comes from building, tuning, and running the complex machine that allows a dozen different parts of the company to move in a single, powerful direction.&lt;/p&gt;&lt;p&gt;We were looking for the organizer.&lt;/p&gt;&lt;p&gt;When I spoke with Karyne, I explained this vision: a leader who thrives on creating workflows, who loves being the &lt;b&gt;liaison between editorial, our data and survey team, our events, and our marketing operations&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Her response confirmed she was the one: &amp;quot;Everything you said is exactly my dopamine hit.&amp;quot;&lt;/p&gt;&lt;p&gt;Karyne’s passion is making the entire operation hum. She has a proven track record of managing people, running newsrooms, and interfacing with all parts of a business to ensure everyone is aligned. That operational rigor is precisely what we need for our next chapter.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why This Matters for Our Strategy (and for You)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;As I’ve written about before, VentureBeat is on a mission to evolve. In an age where experts and companies can publish directly, it’s not enough to be a secondary source. Our goal is to become a &lt;b&gt;primary source&lt;/b&gt; for you.&lt;/p&gt;&lt;p&gt;How? By leveraging our relationship with our community of millions of technical leaders. We are increasingly surveying you directly to generate proprietary insights you can’t get anywhere else. We want to be the first to tell you &lt;i&gt;which&lt;/i&gt; vector stores your peers are &lt;i&gt;actually&lt;/i&gt; implementing, &lt;i&gt;what&lt;/i&gt; governance challenges are most pressing for data scientists, or &lt;i&gt;how&lt;/i&gt; your counterparts are budgeting for generative AI.&lt;/p&gt;&lt;p&gt;This is an ambitious strategy. It requires a tight-knit team where our editorial content, our research surveys and reports, our newsletters, and our VB Transform events are all working from the same playbook.&lt;/p&gt;&lt;p&gt;Karyne is the leader who will help us execute that vision. Her experience at Protocol, which was also dedicated to serving technical and business decision-makers, means she fundamentally understands our audience. She is ideally suited to manage our newsroom and ensure that every piece of content we produce helps you do your job better. She’ll be working alongside Carl Franzen, our executive editor, who continues to drive news decision-making.&lt;/p&gt;&lt;p&gt;This is a fantastic hire for VentureBeat. It’s another sign of our commitment to building the most focused, expert team in enterprise AI and data.&lt;/p&gt;&lt;p&gt;Please join me in welcoming Karyne to the team.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;I’m thrilled to announce a fantastic new addition to our leadership team: &lt;b&gt;Karyne Levy&lt;/b&gt; is joining VentureBeat as our new Managing Editor. Today is her first day.&lt;/p&gt;&lt;p&gt;Many of you may know Karyne from her most recent role as Deputy Managing Editor at TechCrunch, but her career is a highlight reel of veteran tech journalism. Her resume includes pivotal roles at &lt;b&gt;Protocol, NerdWallet, Business Insider, and CNET&lt;/b&gt;, giving her a deep understanding of this industry from every angle.&lt;/p&gt;&lt;p&gt;Hiring Karyne is a significant step forward for VentureBeat. As we’ve sharpened our focus on serving you – the enterprise technical decision-maker navigating the complexities of AI and data – I’ve been looking for a very specific kind of leader.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The &amp;quot;Organizer&amp;#x27;s Dopamine Hit&amp;quot;&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;In the past, a managing editor was often the final backstop for copy. Today, at a modern, data-focused media company like ours, the role is infinitely more dynamic. It’s the central hub of the entire content operation.&lt;/p&gt;&lt;p&gt;During my search, I found myself talking a lot about the two types of &amp;quot;dopamine hits&amp;quot; in our business. There’s the writer’s hit – seeing your name on a great story. And then there’s the organizer’s hit – the satisfaction that comes from building, tuning, and running the complex machine that allows a dozen different parts of the company to move in a single, powerful direction.&lt;/p&gt;&lt;p&gt;We were looking for the organizer.&lt;/p&gt;&lt;p&gt;When I spoke with Karyne, I explained this vision: a leader who thrives on creating workflows, who loves being the &lt;b&gt;liaison between editorial, our data and survey team, our events, and our marketing operations&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Her response confirmed she was the one: &amp;quot;Everything you said is exactly my dopamine hit.&amp;quot;&lt;/p&gt;&lt;p&gt;Karyne’s passion is making the entire operation hum. She has a proven track record of managing people, running newsrooms, and interfacing with all parts of a business to ensure everyone is aligned. That operational rigor is precisely what we need for our next chapter.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why This Matters for Our Strategy (and for You)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;As I’ve written about before, VentureBeat is on a mission to evolve. In an age where experts and companies can publish directly, it’s not enough to be a secondary source. Our goal is to become a &lt;b&gt;primary source&lt;/b&gt; for you.&lt;/p&gt;&lt;p&gt;How? By leveraging our relationship with our community of millions of technical leaders. We are increasingly surveying you directly to generate proprietary insights you can’t get anywhere else. We want to be the first to tell you &lt;i&gt;which&lt;/i&gt; vector stores your peers are &lt;i&gt;actually&lt;/i&gt; implementing, &lt;i&gt;what&lt;/i&gt; governance challenges are most pressing for data scientists, or &lt;i&gt;how&lt;/i&gt; your counterparts are budgeting for generative AI.&lt;/p&gt;&lt;p&gt;This is an ambitious strategy. It requires a tight-knit team where our editorial content, our research surveys and reports, our newsletters, and our VB Transform events are all working from the same playbook.&lt;/p&gt;&lt;p&gt;Karyne is the leader who will help us execute that vision. Her experience at Protocol, which was also dedicated to serving technical and business decision-makers, means she fundamentally understands our audience. She is ideally suited to manage our newsroom and ensure that every piece of content we produce helps you do your job better. She’ll be working alongside Carl Franzen, our executive editor, who continues to drive news decision-making.&lt;/p&gt;&lt;p&gt;This is a fantastic hire for VentureBeat. It’s another sign of our commitment to building the most focused, expert team in enterprise AI and data.&lt;/p&gt;&lt;p&gt;Please join me in welcoming Karyne to the team.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/strengthening-our-core-welcoming-karyne-levy-as-venturebeats-new-managing</guid><pubDate>Mon, 03 Nov 2025 15:00:00 +0000</pubDate></item><item><title>OpenAI and Amazon ink $38B cloud computing deal (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/03/openai-and-amazon-ink-38b-cloud-computing-deal/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/amazon-data-center.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI isn’t done securing the AI infrastructure it needs to rapidly scale agentic workloads. The ChatGPT-maker on Monday said it has reached a deal with Amazon to buy $38 billion in cloud computing services over the next seven years. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI said it will immediately start using AWS compute, with all capacity targeted to be deployed before the end of 2026, with the ability to expand further into 2027 and beyond. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal follows OpenAI’s restructuring last week, which freed the company from having to secure Microsoft’s approval to buy computing services from other firms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s deal with Amazon is part of its larger mission to grow computing power, spending more than $1 trillion over the next decade. The company has announced new data center buildouts with Oracle, SoftBank, the United Arab Emirates, and others. OpenAI has also secured deals with chipmakers Nvidia, AMD, and Broadcom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To some analysts, the increased investment form OpenAI and other tech giants signals that the industry is heading toward an AI bubble, wherein massive sums are spent beefing out an unproven, and potentially dangerous, technology with no clear sign of a meaningful return on investment. &amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/amazon-data-center.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI isn’t done securing the AI infrastructure it needs to rapidly scale agentic workloads. The ChatGPT-maker on Monday said it has reached a deal with Amazon to buy $38 billion in cloud computing services over the next seven years. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI said it will immediately start using AWS compute, with all capacity targeted to be deployed before the end of 2026, with the ability to expand further into 2027 and beyond. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal follows OpenAI’s restructuring last week, which freed the company from having to secure Microsoft’s approval to buy computing services from other firms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s deal with Amazon is part of its larger mission to grow computing power, spending more than $1 trillion over the next decade. The company has announced new data center buildouts with Oracle, SoftBank, the United Arab Emirates, and others. OpenAI has also secured deals with chipmakers Nvidia, AMD, and Broadcom.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To some analysts, the increased investment form OpenAI and other tech giants signals that the industry is heading toward an AI bubble, wherein massive sums are spent beefing out an unproven, and potentially dangerous, technology with no clear sign of a meaningful return on investment. &amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/03/openai-and-amazon-ink-38b-cloud-computing-deal/</guid><pubDate>Mon, 03 Nov 2025 15:21:51 +0000</pubDate></item><item><title>AI browsers are a significant security threat (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-browser-security-issue-shadow-ai-malware/</link><description>&lt;p&gt;Among the explosion of AI systems, AI web browsers such as Fellou and Comet from Perplexity have begun to make appearances on the corporate desktop. Such applications are described as the next evolution of the humble browser, and come with AI features built in; they can read and summarise web pages – and, at their most advanced – act on web content autonomously.&lt;/p&gt;&lt;p&gt;In theory, at least, the promise of an AI browser is that it will speed up digital workflows, undertake online research, and retrieve information from internal sources and the wider internet.&lt;/p&gt;&lt;p&gt;However, security research teams are concluding that AI browsers introduce serious risks into the enterprise that simply can’t be ignored.&lt;/p&gt;&lt;p&gt;The problem lies in the fact that AI browsers are highly vulnerable to indirect prompt injection attacks. These are where the model in the browser (or accessed via the browser) receives instructions hidden in specially-crafted websites. By embedding text into web pages or images in ways humans find difficult to discren, AI models can be fed instructions in the form of AI prompts, or amendments to prompts that are input by the user.&lt;/p&gt;&lt;p&gt;The bottom line for IT departments and decision-makers is that AI browsers are not yet suitable for use in the enterprise, and represent a significant security threat.&lt;/p&gt;&lt;h3 id="org9b20853"&gt;Automation meets exposure&lt;/h3&gt;&lt;p&gt;In tests, researchers discovered that embedded text in online content is processed by the AI browser and is interpreted as instructions to the smart model. These instructions can be executed using the user’s privileges, so the greater the degree of access to information that the user has, the greater the risk to the organisation. The autonomy that AI gives users is the same mechanism that magnifies the attack surface, and the more autonomy, the greater the potential scope for data loss.&lt;/p&gt;&lt;p&gt;For example, it’s possible to embed text commands into an image that, when displayed in the browser, could trigger an AI assistant to interact with sensitive assets, like corporate email, or online banking dashboards. Another test showed how an AI assistant’s prompt can be hijacked and made to perform unauthorised actions on the behalf of the user.&lt;/p&gt;&lt;p&gt;These types of vulnerabilities clearly go against all principles of data governance, and are the most obvious example of how ‘shadow AI’ in the form of an unauthorised browser, poses a real threat to an organisation’s data. The AI model acts as a bridge between domains, and circumvents same-origin policies – the rule that prevents the access of data from one domain by another.&lt;/p&gt;&lt;h3 id="org5077fb8"&gt;Implementation and governance challenges&lt;/h3&gt;&lt;p&gt;The root of the problem is the merging of user queries in the browser with live data accessed on the web. If the LLM can’t distinguish between safe and malicious input, then it can blithely access data not requested by its human operator and act on it. When given agentic abilities, the consequences can be far-reaching, and could easily cause a cascade of malicious activity across the enterprise.&lt;/p&gt;&lt;p&gt;For any organisation that relies on data segmentation and access control, a compromised AI layer in a user’s browser can circumvent firewalls, enact token exchanges, and use secure cookies in exactly the same way that a user might. Effectively, the AI browser becomes an insider threat, with access to all the data and facility of its human operator. The browser user will not necessarily be aware of activity ‘under the hood,’ so an infected browser may act for significant periods of time without detection.&lt;/p&gt;&lt;h3 id="org494cf91"&gt;Threat mitigation&lt;/h3&gt;&lt;p&gt;The first generation of AI browsers should be regarded by IT teams in the same way they treat unauthorised installation of third-party software. While it is relatively easy to prevent specific software being installed by users, it’s worth noting that mainstream browsers such as Chrome and Edge are shipping with increased numbers of AI features in the form of Gemini (in Chrome) and Copilot (in Edge). The browser-producing companies are actively exploring AI-augmented browsing capabilities, and agentic features (that grant significant autonomy to the browser) will be quick to appear, driven by the need for competitive advantage between browser companies.&lt;/p&gt;&lt;p&gt;Without proper oversight and controls, organisations are opening themselves to significant risk. Future generations of browsers should be checked for the following features:&lt;/p&gt;&lt;ul class="org-ul"&gt;&lt;li&gt;Prompt isolation, separating user intent from third-party web content before LLM prompt generation.&lt;/li&gt;&lt;li&gt;Gated permissions. AI agents should not be able to execute autonomous actions, including navigation, data retrieval, or file access without explicit user confirmation.&lt;/li&gt;&lt;li&gt;Sandboxing of sensitive browsing (like HR, finance, internal dashboards, etc.) so there is no AI activity in these sensitive areas.&lt;/li&gt;&lt;li&gt;Governance integration. Browser-based AI has to align with data security policies, and the software should provide records to make agentic actions traceable.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To date, no browser vendor has presented a smart browser with the ability to distinguish between user-driven intent, and model-interpreted commands. Without this, browsers may be coerced to act against the organisation by the use of relatively trivial prompt injection.&lt;/p&gt;&lt;h3 id="org9df418c"&gt;Decision-maker takeaway&lt;/h3&gt;&lt;p&gt;Agentic AI browsers are presented as the next logical evolution in web browsing and automation in the workplace. They are designed deliberately to blur the distinction between user/human activity and become part of interactions with the enterprise’s digital assets. Given the ease with which the LLMs in AI browsers are circumvented and corrupted, the current generation of AI browsers can be regarded as dormant malware.&lt;/p&gt;&lt;p&gt;The major browser vendors look set to embed AI (with or without agentic abilities) into future generations of their platforms, so careful monitoring of each release should be undertaken to ensure security oversight.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Unexploded bomb!” by hugh llewelyn is licensed under CC BY-SA 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Among the explosion of AI systems, AI web browsers such as Fellou and Comet from Perplexity have begun to make appearances on the corporate desktop. Such applications are described as the next evolution of the humble browser, and come with AI features built in; they can read and summarise web pages – and, at their most advanced – act on web content autonomously.&lt;/p&gt;&lt;p&gt;In theory, at least, the promise of an AI browser is that it will speed up digital workflows, undertake online research, and retrieve information from internal sources and the wider internet.&lt;/p&gt;&lt;p&gt;However, security research teams are concluding that AI browsers introduce serious risks into the enterprise that simply can’t be ignored.&lt;/p&gt;&lt;p&gt;The problem lies in the fact that AI browsers are highly vulnerable to indirect prompt injection attacks. These are where the model in the browser (or accessed via the browser) receives instructions hidden in specially-crafted websites. By embedding text into web pages or images in ways humans find difficult to discren, AI models can be fed instructions in the form of AI prompts, or amendments to prompts that are input by the user.&lt;/p&gt;&lt;p&gt;The bottom line for IT departments and decision-makers is that AI browsers are not yet suitable for use in the enterprise, and represent a significant security threat.&lt;/p&gt;&lt;h3 id="org9b20853"&gt;Automation meets exposure&lt;/h3&gt;&lt;p&gt;In tests, researchers discovered that embedded text in online content is processed by the AI browser and is interpreted as instructions to the smart model. These instructions can be executed using the user’s privileges, so the greater the degree of access to information that the user has, the greater the risk to the organisation. The autonomy that AI gives users is the same mechanism that magnifies the attack surface, and the more autonomy, the greater the potential scope for data loss.&lt;/p&gt;&lt;p&gt;For example, it’s possible to embed text commands into an image that, when displayed in the browser, could trigger an AI assistant to interact with sensitive assets, like corporate email, or online banking dashboards. Another test showed how an AI assistant’s prompt can be hijacked and made to perform unauthorised actions on the behalf of the user.&lt;/p&gt;&lt;p&gt;These types of vulnerabilities clearly go against all principles of data governance, and are the most obvious example of how ‘shadow AI’ in the form of an unauthorised browser, poses a real threat to an organisation’s data. The AI model acts as a bridge between domains, and circumvents same-origin policies – the rule that prevents the access of data from one domain by another.&lt;/p&gt;&lt;h3 id="org5077fb8"&gt;Implementation and governance challenges&lt;/h3&gt;&lt;p&gt;The root of the problem is the merging of user queries in the browser with live data accessed on the web. If the LLM can’t distinguish between safe and malicious input, then it can blithely access data not requested by its human operator and act on it. When given agentic abilities, the consequences can be far-reaching, and could easily cause a cascade of malicious activity across the enterprise.&lt;/p&gt;&lt;p&gt;For any organisation that relies on data segmentation and access control, a compromised AI layer in a user’s browser can circumvent firewalls, enact token exchanges, and use secure cookies in exactly the same way that a user might. Effectively, the AI browser becomes an insider threat, with access to all the data and facility of its human operator. The browser user will not necessarily be aware of activity ‘under the hood,’ so an infected browser may act for significant periods of time without detection.&lt;/p&gt;&lt;h3 id="org494cf91"&gt;Threat mitigation&lt;/h3&gt;&lt;p&gt;The first generation of AI browsers should be regarded by IT teams in the same way they treat unauthorised installation of third-party software. While it is relatively easy to prevent specific software being installed by users, it’s worth noting that mainstream browsers such as Chrome and Edge are shipping with increased numbers of AI features in the form of Gemini (in Chrome) and Copilot (in Edge). The browser-producing companies are actively exploring AI-augmented browsing capabilities, and agentic features (that grant significant autonomy to the browser) will be quick to appear, driven by the need for competitive advantage between browser companies.&lt;/p&gt;&lt;p&gt;Without proper oversight and controls, organisations are opening themselves to significant risk. Future generations of browsers should be checked for the following features:&lt;/p&gt;&lt;ul class="org-ul"&gt;&lt;li&gt;Prompt isolation, separating user intent from third-party web content before LLM prompt generation.&lt;/li&gt;&lt;li&gt;Gated permissions. AI agents should not be able to execute autonomous actions, including navigation, data retrieval, or file access without explicit user confirmation.&lt;/li&gt;&lt;li&gt;Sandboxing of sensitive browsing (like HR, finance, internal dashboards, etc.) so there is no AI activity in these sensitive areas.&lt;/li&gt;&lt;li&gt;Governance integration. Browser-based AI has to align with data security policies, and the software should provide records to make agentic actions traceable.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To date, no browser vendor has presented a smart browser with the ability to distinguish between user-driven intent, and model-interpreted commands. Without this, browsers may be coerced to act against the organisation by the use of relatively trivial prompt injection.&lt;/p&gt;&lt;h3 id="org9df418c"&gt;Decision-maker takeaway&lt;/h3&gt;&lt;p&gt;Agentic AI browsers are presented as the next logical evolution in web browsing and automation in the workplace. They are designed deliberately to blur the distinction between user/human activity and become part of interactions with the enterprise’s digital assets. Given the ease with which the LLMs in AI browsers are circumvented and corrupted, the current generation of AI browsers can be regarded as dormant malware.&lt;/p&gt;&lt;p&gt;The major browser vendors look set to embed AI (with or without agentic abilities) into future generations of their platforms, so careful monitoring of each release should be undertaken to ensure security oversight.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Unexploded bomb!” by hugh llewelyn is licensed under CC BY-SA 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-browser-security-issue-shadow-ai-malware/</guid><pubDate>Mon, 03 Nov 2025 15:35:19 +0000</pubDate></item><item><title>OpenAI spreads $600B cloud AI bet across AWS, Oracle, Microsoft (AI News)</title><link>https://www.artificialintelligence-news.com/news/openai-spreads-600b-cloud-ai-bet-aws-oracle-microsoft/</link><description>&lt;p&gt;OpenAI is on a spending spree to secure its AI compute supply chain, signing a new deal with AWS as part of its multi-cloud strategy.&lt;/p&gt;&lt;p&gt;The company recently ended its exclusive cloud-computing partnership with Microsoft. It has since allocated a reported $250 billion back to Microsoft, $300 billion to Oracle, and now, $38 billion to Amazon Web Services (AWS) in a new multi-year pact. This $38 billion AWS deal, while the smallest of the three, is part of OpenAI’s diversification plan.&lt;/p&gt;&lt;p&gt;For industry leaders, OpenAI’s actions show that access to high-performance GPUs is no longer an on-demand commodity. It is now a scarce resource requiring massive long-term capital commitment.&lt;/p&gt;&lt;p&gt;The AWS agreement provides OpenAI with access to hundreds of thousands of NVIDIA GPUs, including the new GB200s and GB300s, and the ability to tap tens of millions of CPUs.&lt;/p&gt;&lt;p&gt;This mighty infrastructure is not just for training tomorrow’s models; it’s needed to run the massive inference workloads of today’s ChatGPT. As OpenAI co-founder and CEO Sam Altman stated, “scaling frontier AI requires massive, reliable compute”.&lt;/p&gt;&lt;p&gt;This spending spree is forcing a competitive response from the hyperscalers. While AWS remains the industry’s largest cloud provider, Microsoft and Google have recently posted faster cloud-revenue growth, often by capturing new AI customers. This AWS deal is a plain attempt to secure a cornerstone AI workload and prove its large-scale AI capabilities, which it claims include running clusters of over 500,000 chips.&lt;/p&gt;&lt;p&gt;AWS is not just providing standard servers. It is building a sophisticated, purpose-built architecture for OpenAI, using EC2 UltraServers to link the GPUs for the low-latency networking that large-scale training demands.&lt;/p&gt;&lt;p&gt;“The breadth and immediate availability of optimised compute demonstrates why AWS is uniquely positioned to support OpenAI’s vast AI workloads,” said Matt Garman, CEO of AWS.&lt;/p&gt;&lt;p&gt;But “immediate” is relative. The full capacity from OpenAI’s latest cloud AI deal will not be fully deployed until the end of 2026, with options to expand further into 2027. This timeline offers a dose of realism for any executive planning an AI rollout: the hardware supply chain is complex and operates on multi-year schedules.&lt;/p&gt;&lt;p&gt;What, then, should enterprise leaders take from this?&lt;/p&gt;&lt;p&gt;First, the “build vs. buy” debate for AI infrastructure is all but over. OpenAI is spending hundreds of billions to build on top of rented hardware. Few, if any, other companies can or should follow suit. This pushes the rest of the market firmly toward managed platforms like Amazon Bedrock, Google Vertex AI, or IBM watsonx, where the hyperscalers absorb this infrastructure risk.&lt;/p&gt;&lt;p&gt;Second, the days of single-cloud sourcing for AI workloads may be numbered. OpenAI’s pivot to a multi-provider model is a textbook case of mitigating concentration risk. For a CIO, relying on one vendor for the compute that runs a core business process is becoming a gamble.&lt;/p&gt;&lt;p&gt;Finally, AI budgeting has left the realm of departmental IT and entered the world of corporate capital planning. These are no longer variable operational expenses. Securing AI compute is now a long-term financial commitment, much like building a new factory or data centre.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Qualcomm unveils AI data centre chips to crack the Inference market&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;OpenAI is on a spending spree to secure its AI compute supply chain, signing a new deal with AWS as part of its multi-cloud strategy.&lt;/p&gt;&lt;p&gt;The company recently ended its exclusive cloud-computing partnership with Microsoft. It has since allocated a reported $250 billion back to Microsoft, $300 billion to Oracle, and now, $38 billion to Amazon Web Services (AWS) in a new multi-year pact. This $38 billion AWS deal, while the smallest of the three, is part of OpenAI’s diversification plan.&lt;/p&gt;&lt;p&gt;For industry leaders, OpenAI’s actions show that access to high-performance GPUs is no longer an on-demand commodity. It is now a scarce resource requiring massive long-term capital commitment.&lt;/p&gt;&lt;p&gt;The AWS agreement provides OpenAI with access to hundreds of thousands of NVIDIA GPUs, including the new GB200s and GB300s, and the ability to tap tens of millions of CPUs.&lt;/p&gt;&lt;p&gt;This mighty infrastructure is not just for training tomorrow’s models; it’s needed to run the massive inference workloads of today’s ChatGPT. As OpenAI co-founder and CEO Sam Altman stated, “scaling frontier AI requires massive, reliable compute”.&lt;/p&gt;&lt;p&gt;This spending spree is forcing a competitive response from the hyperscalers. While AWS remains the industry’s largest cloud provider, Microsoft and Google have recently posted faster cloud-revenue growth, often by capturing new AI customers. This AWS deal is a plain attempt to secure a cornerstone AI workload and prove its large-scale AI capabilities, which it claims include running clusters of over 500,000 chips.&lt;/p&gt;&lt;p&gt;AWS is not just providing standard servers. It is building a sophisticated, purpose-built architecture for OpenAI, using EC2 UltraServers to link the GPUs for the low-latency networking that large-scale training demands.&lt;/p&gt;&lt;p&gt;“The breadth and immediate availability of optimised compute demonstrates why AWS is uniquely positioned to support OpenAI’s vast AI workloads,” said Matt Garman, CEO of AWS.&lt;/p&gt;&lt;p&gt;But “immediate” is relative. The full capacity from OpenAI’s latest cloud AI deal will not be fully deployed until the end of 2026, with options to expand further into 2027. This timeline offers a dose of realism for any executive planning an AI rollout: the hardware supply chain is complex and operates on multi-year schedules.&lt;/p&gt;&lt;p&gt;What, then, should enterprise leaders take from this?&lt;/p&gt;&lt;p&gt;First, the “build vs. buy” debate for AI infrastructure is all but over. OpenAI is spending hundreds of billions to build on top of rented hardware. Few, if any, other companies can or should follow suit. This pushes the rest of the market firmly toward managed platforms like Amazon Bedrock, Google Vertex AI, or IBM watsonx, where the hyperscalers absorb this infrastructure risk.&lt;/p&gt;&lt;p&gt;Second, the days of single-cloud sourcing for AI workloads may be numbered. OpenAI’s pivot to a multi-provider model is a textbook case of mitigating concentration risk. For a CIO, relying on one vendor for the compute that runs a core business process is becoming a gamble.&lt;/p&gt;&lt;p&gt;Finally, AI budgeting has left the realm of departmental IT and entered the world of corporate capital planning. These are no longer variable operational expenses. Securing AI compute is now a long-term financial commitment, much like building a new factory or data centre.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Qualcomm unveils AI data centre chips to crack the Inference market&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/openai-spreads-600b-cloud-ai-bet-aws-oracle-microsoft/</guid><pubDate>Mon, 03 Nov 2025 15:37:32 +0000</pubDate></item><item><title>The State of AI: Is China about to win the race? (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/03/1126780/the-state-of-ai-is-china-about-to-win-the-race/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;The State of AI is a collaboration between the Financial Times &amp;amp; MIT Technology Review examining the ways in which AI is reshaping global power. Every Monday for the next six weeks, writers from both publications will debate one aspect of the generative AI revolution reshaping global power.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;In this conversation, the FT’s tech columnist and Innovation Editor&amp;nbsp;John Thornhill and MIT Technology Review’s Caiwei Chen consider the battle between Silicon Valley and Beijing for technological supremacy.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127512" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/MITTR_FT_small.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;John Thornhill writes:&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Viewed from abroad, it seems only a matter of time before China emerges as the AI superpower of the 21st century.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Here in the West, our initial instinct is to focus on America’s significant lead in semiconductor expertise, its cutting-edge AI research, and its vast investments in data centers. The legendary investor Warren Buffett once warned: “Never bet against America.” He is right that for more than two centuries, no other “incubator for unleashing human potential” has matched the US.&lt;/p&gt;  &lt;p&gt;Today, however, China has the means, motive, and opportunity to commit the equivalent of technological murder. When it comes to mobilizing the whole-of-society resources needed to develop and deploy AI to maximum effect, it may be just as rash to bet against.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The data highlights the trends. In AI publications and patents, China leads. By 2023, China accounted for 22.6% of all citations, compared with 20.9% from Europe and 13% from the US, according to Stanford University's Artificial Intelligence Index Report 2025. As of 2023, China also accounted for 69.7% of all AI patents. True, the US maintains a strong lead in the top 100 most cited publications (50 versus 34 in 2023), but its share has been steadily declining.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Similarly, the US outdoes China in top AI research talent, but the gap is narrowing. According to a report from the US Council of Economic Advisers, 59% of the world’s top AI researchers worked in the US in 2019, compared with 11% in China. But by 2022 those figures were 42% and 28%.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;The Trump administration’s tightening of restrictions for foreign H-1B visa holders may well lead more Chinese AI researchers in the US to return home. The talent ratio could move further in China’s favor.&lt;/p&gt;  &lt;p&gt;Regarding the technology itself, US-based institutions produced 40 of the world’s most notable AI models in 2024, compared with 15 from China. But Chinese researchers have learned to do more with less, and their strongest large language models—including the open-source DeepSeek-V3 and Alibaba's Qwen 2.5-Max—surpass the best US models in terms of algorithmic efficiency.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Where China is really likely to excel in future is in applying these open-source models. The latest report from Air Street Capital shows that China has now overtaken the US in terms of monthly downloads of AI models. In AI-enabled fintech, e-commerce, and logistics, China already outstrips the US.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Perhaps the most intriguing—and potentially the most productive—applications of AI may yet come in hardware, particularly in drones and industrial robotics. With the research field evolving toward embodied AI, China’s advantage in advanced manufacturing will shine through.&lt;/p&gt;  &lt;p&gt;Dan Wang, the tech analyst and author of &lt;em&gt;Breakneck&lt;/em&gt;, has rightly highlighted the strengths of China’s engineering state in developing manufacturing process knowledge—even if he has also shown the damaging effects of applying that engineering mentality in the social sphere. “China has been growing technologically stronger and economically more dynamic in all sorts of ways,” he told me. “But repression is very real. And it is getting worse in all sorts of ways as well.”&lt;/p&gt;  &lt;p&gt;I’d be fascinated to hear from you, Caiwei, about your take on the strengths and weaknesses of China’s AI dream. To what extent will China’s engineered social control hamper its technological ambitions?&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;Caiwei Chen responds:&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Hi, John!&lt;/p&gt;  &lt;p&gt;You’re right that the US still holds a clear lead in frontier research and infrastructure. But “winning” AI can mean many different things. Jeffrey Ding, in his book &lt;em&gt;Technology and the Rise of Great Powers&lt;/em&gt;, makes a counterintuitive point: For a general-purpose technology like AI, long-term advantage often comes down to how widely and deeply technologies spread across society. And China is in a good position to win that race (although “murder” might be pushing it a bit!).&lt;/p&gt;  &lt;p&gt;Chips will remain China’s biggest bottleneck. Export restrictions have throttled access to top GPUs, pushing buyers into gray markets and forcing labs to recycle or repair banned Nvidia stock. Even as domestic chip programs expand, the performance gap at the very top still stands.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Yet those same constraints have pushed Chinese companies toward a different playbook: pooling compute, optimizing efficiency, and releasing open-weight models. DeepSeek-V3’s training run, for example, used just 2.6 million GPU-hours—far below the scale of US counterparts. But Alibaba’s Qwen models now rank among the most downloaded open-weights globally, and companies like Zhipu and MiniMax are building competitive multimodal and video models.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;China’s industrial policy means new models can move from lab to implementation fast. Local governments and major enterprises are already rolling out reasoning models in administration, logistics, and finance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Education is another advantage. Major Chinese universities are implementing AI literacy programs in their curricula, embedding skills before the labor market demands them. The Ministry of Education has also announced plans to integrate AI training for children of all school ages. I’m not sure the phrase “engineering state” fully captures China’s relationship with new technologies, but decades of infrastructure building and top-down coordination have made the system unusually effective at pushing large-scale adoption, often with far less social resistance than you’d see elsewhere. The use at scale, naturally, allows for faster iterative improvements.&lt;/p&gt;  &lt;p&gt;Meanwhile, Stanford HAI’s 2025 AI Index found Chinese respondents to be the most optimistic in the world about AI’s future—far more optimistic than populations in the US or the UK. It’s striking, given that China’s economy has slowed since the pandemic for the first time in over two decades. Many in government and industry now see AI as a much-needed spark. Optimism can be powerful fuel, but whether it can persist through slower growth is still an open question.&lt;/p&gt; 
 &lt;p&gt;Social control remains part of the picture, but a different kind of ambition is taking shape. The Chinese AI founders in this new generation are the most globally minded I’ve seen, moving fluidly between Silicon Valley hackathons and pitch meetings in Dubai. Many are fluent in English and in the rhythms of global venture capital. Having watched the last generation wrestle with the burden of a Chinese label, they now build companies that are quietly transnational from the start.&lt;/p&gt;  &lt;p&gt;The US may still lead in speed and experimentation, but China could shape how AI becomes part of daily life, both at home and abroad. Speed matters, but speed isn’t the same thing as supremacy.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;John Thornhill&lt;/strong&gt; &lt;strong&gt;replies&lt;/strong&gt;:&lt;/p&gt;  &lt;p&gt;You’re right, Caiwei, that speed is not the same as supremacy (and “murder” may be too strong a word). And you’re also right to amplify the point about China’s strength in open-weight models and the US preference for proprietary models. This is not just a struggle between two different countries’ economic models but also between two different ways of deploying technology.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;Even OpenAI’s chief executive, Sam Altman, admitted earlier this year: “We have been on the wrong side of history here and need to figure out a different open-source strategy.” That’s going to be a very interesting subplot to follow. Who’s called that one right?&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Further reading on the US-China competition&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;There’s been a lot of talk about how people may be using generative AI in their daily lives. This story from the &lt;em&gt;FT&lt;/em&gt;’s visual story team explores the reality&amp;nbsp;&lt;/p&gt;  &lt;p&gt;From China, &lt;em&gt;FT&lt;/em&gt; reporters ask how long Nvidia can maintain its dominance over Chinese rivals&lt;/p&gt;  &lt;p&gt;When it comes to real-world uses, toys and companions devices are a novel but emergent application of AI that is gaining traction in China—but is also heading to the US. This &lt;em&gt;MIT Technology Review&lt;/em&gt; story explored it.&lt;/p&gt;  &lt;p&gt;The once-frantic data center buildout in China has hit walls, and as the sanctions and AI demands shift, this &lt;em&gt;MIT Technology Review&lt;/em&gt; story took an on-the-ground look at how stakeholders are figuring it out.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;The State of AI is a collaboration between the Financial Times &amp;amp; MIT Technology Review examining the ways in which AI is reshaping global power. Every Monday for the next six weeks, writers from both publications will debate one aspect of the generative AI revolution reshaping global power.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;In this conversation, the FT’s tech columnist and Innovation Editor&amp;nbsp;John Thornhill and MIT Technology Review’s Caiwei Chen consider the battle between Silicon Valley and Beijing for technological supremacy.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127512" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/MITTR_FT_small.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;John Thornhill writes:&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Viewed from abroad, it seems only a matter of time before China emerges as the AI superpower of the 21st century.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Here in the West, our initial instinct is to focus on America’s significant lead in semiconductor expertise, its cutting-edge AI research, and its vast investments in data centers. The legendary investor Warren Buffett once warned: “Never bet against America.” He is right that for more than two centuries, no other “incubator for unleashing human potential” has matched the US.&lt;/p&gt;  &lt;p&gt;Today, however, China has the means, motive, and opportunity to commit the equivalent of technological murder. When it comes to mobilizing the whole-of-society resources needed to develop and deploy AI to maximum effect, it may be just as rash to bet against.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The data highlights the trends. In AI publications and patents, China leads. By 2023, China accounted for 22.6% of all citations, compared with 20.9% from Europe and 13% from the US, according to Stanford University's Artificial Intelligence Index Report 2025. As of 2023, China also accounted for 69.7% of all AI patents. True, the US maintains a strong lead in the top 100 most cited publications (50 versus 34 in 2023), but its share has been steadily declining.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Similarly, the US outdoes China in top AI research talent, but the gap is narrowing. According to a report from the US Council of Economic Advisers, 59% of the world’s top AI researchers worked in the US in 2019, compared with 11% in China. But by 2022 those figures were 42% and 28%.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;The Trump administration’s tightening of restrictions for foreign H-1B visa holders may well lead more Chinese AI researchers in the US to return home. The talent ratio could move further in China’s favor.&lt;/p&gt;  &lt;p&gt;Regarding the technology itself, US-based institutions produced 40 of the world’s most notable AI models in 2024, compared with 15 from China. But Chinese researchers have learned to do more with less, and their strongest large language models—including the open-source DeepSeek-V3 and Alibaba's Qwen 2.5-Max—surpass the best US models in terms of algorithmic efficiency.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Where China is really likely to excel in future is in applying these open-source models. The latest report from Air Street Capital shows that China has now overtaken the US in terms of monthly downloads of AI models. In AI-enabled fintech, e-commerce, and logistics, China already outstrips the US.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Perhaps the most intriguing—and potentially the most productive—applications of AI may yet come in hardware, particularly in drones and industrial robotics. With the research field evolving toward embodied AI, China’s advantage in advanced manufacturing will shine through.&lt;/p&gt;  &lt;p&gt;Dan Wang, the tech analyst and author of &lt;em&gt;Breakneck&lt;/em&gt;, has rightly highlighted the strengths of China’s engineering state in developing manufacturing process knowledge—even if he has also shown the damaging effects of applying that engineering mentality in the social sphere. “China has been growing technologically stronger and economically more dynamic in all sorts of ways,” he told me. “But repression is very real. And it is getting worse in all sorts of ways as well.”&lt;/p&gt;  &lt;p&gt;I’d be fascinated to hear from you, Caiwei, about your take on the strengths and weaknesses of China’s AI dream. To what extent will China’s engineered social control hamper its technological ambitions?&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;Caiwei Chen responds:&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Hi, John!&lt;/p&gt;  &lt;p&gt;You’re right that the US still holds a clear lead in frontier research and infrastructure. But “winning” AI can mean many different things. Jeffrey Ding, in his book &lt;em&gt;Technology and the Rise of Great Powers&lt;/em&gt;, makes a counterintuitive point: For a general-purpose technology like AI, long-term advantage often comes down to how widely and deeply technologies spread across society. And China is in a good position to win that race (although “murder” might be pushing it a bit!).&lt;/p&gt;  &lt;p&gt;Chips will remain China’s biggest bottleneck. Export restrictions have throttled access to top GPUs, pushing buyers into gray markets and forcing labs to recycle or repair banned Nvidia stock. Even as domestic chip programs expand, the performance gap at the very top still stands.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Yet those same constraints have pushed Chinese companies toward a different playbook: pooling compute, optimizing efficiency, and releasing open-weight models. DeepSeek-V3’s training run, for example, used just 2.6 million GPU-hours—far below the scale of US counterparts. But Alibaba’s Qwen models now rank among the most downloaded open-weights globally, and companies like Zhipu and MiniMax are building competitive multimodal and video models.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;China’s industrial policy means new models can move from lab to implementation fast. Local governments and major enterprises are already rolling out reasoning models in administration, logistics, and finance.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Education is another advantage. Major Chinese universities are implementing AI literacy programs in their curricula, embedding skills before the labor market demands them. The Ministry of Education has also announced plans to integrate AI training for children of all school ages. I’m not sure the phrase “engineering state” fully captures China’s relationship with new technologies, but decades of infrastructure building and top-down coordination have made the system unusually effective at pushing large-scale adoption, often with far less social resistance than you’d see elsewhere. The use at scale, naturally, allows for faster iterative improvements.&lt;/p&gt;  &lt;p&gt;Meanwhile, Stanford HAI’s 2025 AI Index found Chinese respondents to be the most optimistic in the world about AI’s future—far more optimistic than populations in the US or the UK. It’s striking, given that China’s economy has slowed since the pandemic for the first time in over two decades. Many in government and industry now see AI as a much-needed spark. Optimism can be powerful fuel, but whether it can persist through slower growth is still an open question.&lt;/p&gt; 
 &lt;p&gt;Social control remains part of the picture, but a different kind of ambition is taking shape. The Chinese AI founders in this new generation are the most globally minded I’ve seen, moving fluidly between Silicon Valley hackathons and pitch meetings in Dubai. Many are fluent in English and in the rhythms of global venture capital. Having watched the last generation wrestle with the burden of a Chinese label, they now build companies that are quietly transnational from the start.&lt;/p&gt;  &lt;p&gt;The US may still lead in speed and experimentation, but China could shape how AI becomes part of daily life, both at home and abroad. Speed matters, but speed isn’t the same thing as supremacy.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;John Thornhill&lt;/strong&gt; &lt;strong&gt;replies&lt;/strong&gt;:&lt;/p&gt;  &lt;p&gt;You’re right, Caiwei, that speed is not the same as supremacy (and “murder” may be too strong a word). And you’re also right to amplify the point about China’s strength in open-weight models and the US preference for proprietary models. This is not just a struggle between two different countries’ economic models but also between two different ways of deploying technology.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;Even OpenAI’s chief executive, Sam Altman, admitted earlier this year: “We have been on the wrong side of history here and need to figure out a different open-source strategy.” That’s going to be a very interesting subplot to follow. Who’s called that one right?&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Further reading on the US-China competition&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;There’s been a lot of talk about how people may be using generative AI in their daily lives. This story from the &lt;em&gt;FT&lt;/em&gt;’s visual story team explores the reality&amp;nbsp;&lt;/p&gt;  &lt;p&gt;From China, &lt;em&gt;FT&lt;/em&gt; reporters ask how long Nvidia can maintain its dominance over Chinese rivals&lt;/p&gt;  &lt;p&gt;When it comes to real-world uses, toys and companions devices are a novel but emergent application of AI that is gaining traction in China—but is also heading to the US. This &lt;em&gt;MIT Technology Review&lt;/em&gt; story explored it.&lt;/p&gt;  &lt;p&gt;The once-frantic data center buildout in China has hit walls, and as the sanctions and AI demands shift, this &lt;em&gt;MIT Technology Review&lt;/em&gt; story took an on-the-ground look at how stakeholders are figuring it out.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/03/1126780/the-state-of-ai-is-china-about-to-win-the-race/</guid><pubDate>Mon, 03 Nov 2025 15:46:26 +0000</pubDate></item><item><title>Dia’s AI browser starts adding Arc’s ‘greatest hits’ to its feature set (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/03/dias-ai-browser-starts-adding-arcs-greatest-hits-to-its-feature-set/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/@Mention-Tabs.jpeg?resize=1200,775" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI web browser Dia is drawing inspiration from its predecessor, Arc, an earlier experiment in modernizing the web-browsing experience that hailed from the startup known as The Browser Company. On Sunday, The Browser Company’s founder, Josh Miller, confirmed that the new AI browser will bring “Arc’s greatest hits” to Dia, including things like the sidebar mode, and combine that with AI-native features like memory and agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This explanation suggests that Dia, which has since been acquired by Atlassian for $610 million, could have an advantage in the AI browser race, as it builds on the company’s earlier learnings from developing Arc.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The latter was initially released in mid-2023 as a reinvention of the browser designed around the way people use the internet today. That included offering separate workspaces for work and personal browsing, support for pinned tabs, a Command Bar that worked like Apple’s Spotlight search, and a sidebar that included the search bar, tab list, user bookmarks, audio controls, and more. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Tbh we (or i really) duffed the comms in a bunch of ways, but…&lt;/p&gt;&lt;p&gt;1. Dia's architecture is much better for AI, speed, security&lt;/p&gt;&lt;p&gt;2. We're adapting Arc's greatest hits to be native to Dia&lt;/p&gt;&lt;p&gt;3. Sidebar mode for Arc fans&lt;/p&gt;&lt;p&gt;Dia + Arc = snappier, smarter, simpler by default w/ Pro mode&lt;/p&gt;— Josh Miller (@joshm) November 3, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;However, Arc may have tried to push the envelope a bit too far: Miller later admitted that Arc was ultimately too complex for most people to adopt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Arc was simply too different, with too many new things to learn, for too little reward&amp;nbsp;… On top of that, Arc lacked cohesion in both its core features and core values. It was experimental — that was part of its charm — but also its complexity,” Miller wrote in a blog post earlier this year, detailing the company’s decision to wind down Arc and open source it, and refocusing the company’s efforts on building Dia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Arc may not necessarily be a failure, even if it didn’t become a widely adopted consumer product. Instead, the browser gave the company over a year’s worth of insights into what sort of modern browser features resonate with users and which ones do not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That could help the company get ahead when building out the feature set for Dia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Miller says in a post on X, “Dia’s architecture is much better for AI, speed, and security,” but it will introduce features that Arc fans loved, like the sidebar mode — which was just spotted in the company’s latest “early birds” release of Dia’s AI browser.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;I'm daily-driving @diabrowser as a years-long Arc user and now an Early Bird, and I can say I'm so close to not miss Arc.&lt;/p&gt;&lt;p&gt;Dia browser now has:&lt;br /&gt;– Focus mode&lt;br /&gt;– Vertical tabs&lt;br /&gt;– Pinned tabs (grid-view)&lt;br /&gt;– Google Meet PIP &lt;/p&gt;&lt;p&gt;And I now wait for only three things before deleting Arc&lt;br /&gt;-… pic.twitter.com/qo9RSUUNmV&lt;/p&gt;— BLCNYY (@BLCNYY) November 1, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Already, Dia has added other features from Arc’s “greatest hits,” like turning Google Meet into a picture-in-picture player automatically when you switch tabs and custom keyboard shortcuts. Miller hinted that the company is exploring how to transition Arc’s Spaces — the distinct browsing areas with their own set of pinned tabs, favorites, themes, history, and cookies — to Dia. And he said Dia’s team is currently testing pinned tabs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Miller additionally solicited feedback about other features to add, like swipeable profiles and Arc Search-inspired updates for the Dia mobile app coming in 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Plus, Miller notes, Dia will have less bloat and will be AI native for things like memory and agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Following the acquisition by Atlassian, The Browser Company continues to operate independently. As a result, Miller said the company will be able to add more “browser basics,” referring to favorite Arc features, to the Dia browser. He also shared that Dia is developing deeper integrations with Atlassian’s Jira and other apps, like Linear, under its new owner.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/@Mention-Tabs.jpeg?resize=1200,775" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI web browser Dia is drawing inspiration from its predecessor, Arc, an earlier experiment in modernizing the web-browsing experience that hailed from the startup known as The Browser Company. On Sunday, The Browser Company’s founder, Josh Miller, confirmed that the new AI browser will bring “Arc’s greatest hits” to Dia, including things like the sidebar mode, and combine that with AI-native features like memory and agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This explanation suggests that Dia, which has since been acquired by Atlassian for $610 million, could have an advantage in the AI browser race, as it builds on the company’s earlier learnings from developing Arc.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The latter was initially released in mid-2023 as a reinvention of the browser designed around the way people use the internet today. That included offering separate workspaces for work and personal browsing, support for pinned tabs, a Command Bar that worked like Apple’s Spotlight search, and a sidebar that included the search bar, tab list, user bookmarks, audio controls, and more. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Tbh we (or i really) duffed the comms in a bunch of ways, but…&lt;/p&gt;&lt;p&gt;1. Dia's architecture is much better for AI, speed, security&lt;/p&gt;&lt;p&gt;2. We're adapting Arc's greatest hits to be native to Dia&lt;/p&gt;&lt;p&gt;3. Sidebar mode for Arc fans&lt;/p&gt;&lt;p&gt;Dia + Arc = snappier, smarter, simpler by default w/ Pro mode&lt;/p&gt;— Josh Miller (@joshm) November 3, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;However, Arc may have tried to push the envelope a bit too far: Miller later admitted that Arc was ultimately too complex for most people to adopt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Arc was simply too different, with too many new things to learn, for too little reward&amp;nbsp;… On top of that, Arc lacked cohesion in both its core features and core values. It was experimental — that was part of its charm — but also its complexity,” Miller wrote in a blog post earlier this year, detailing the company’s decision to wind down Arc and open source it, and refocusing the company’s efforts on building Dia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Arc may not necessarily be a failure, even if it didn’t become a widely adopted consumer product. Instead, the browser gave the company over a year’s worth of insights into what sort of modern browser features resonate with users and which ones do not.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That could help the company get ahead when building out the feature set for Dia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Miller says in a post on X, “Dia’s architecture is much better for AI, speed, and security,” but it will introduce features that Arc fans loved, like the sidebar mode — which was just spotted in the company’s latest “early birds” release of Dia’s AI browser.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;I'm daily-driving @diabrowser as a years-long Arc user and now an Early Bird, and I can say I'm so close to not miss Arc.&lt;/p&gt;&lt;p&gt;Dia browser now has:&lt;br /&gt;– Focus mode&lt;br /&gt;– Vertical tabs&lt;br /&gt;– Pinned tabs (grid-view)&lt;br /&gt;– Google Meet PIP &lt;/p&gt;&lt;p&gt;And I now wait for only three things before deleting Arc&lt;br /&gt;-… pic.twitter.com/qo9RSUUNmV&lt;/p&gt;— BLCNYY (@BLCNYY) November 1, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Already, Dia has added other features from Arc’s “greatest hits,” like turning Google Meet into a picture-in-picture player automatically when you switch tabs and custom keyboard shortcuts. Miller hinted that the company is exploring how to transition Arc’s Spaces — the distinct browsing areas with their own set of pinned tabs, favorites, themes, history, and cookies — to Dia. And he said Dia’s team is currently testing pinned tabs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Miller additionally solicited feedback about other features to add, like swipeable profiles and Arc Search-inspired updates for the Dia mobile app coming in 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Plus, Miller notes, Dia will have less bloat and will be AI native for things like memory and agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Following the acquisition by Atlassian, The Browser Company continues to operate independently. As a result, Miller said the company will be able to add more “browser basics,” referring to favorite Arc features, to the Dia browser. He also shared that Dia is developing deeper integrations with Atlassian’s Jira and other apps, like Linear, under its new owner.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/03/dias-ai-browser-starts-adding-arcs-greatest-hits-to-its-feature-set/</guid><pubDate>Mon, 03 Nov 2025 17:01:58 +0000</pubDate></item><item><title>OpenAI signs massive AI compute deal with Amazon (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/openai-signs-massive-ai-compute-deal-with-amazon/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Deal will provide access to hundreds of thousands of Nvidia chips that power ChatGPT.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A woman hiding behind a cloud." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/hiding_hero_1-300x169.jpg" width="300" /&gt;
                  &lt;img alt="A woman hiding behind a cloud." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/hiding_hero_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, OpenAI announced it has signed a seven-year, $38 billion deal to buy cloud services from Amazon Web Services to power products like ChatGPT and Sora. It’s the company’s first big computing deal after a fundamental restructuring last week that gave OpenAI more operational and financial freedom from Microsoft.&lt;/p&gt;
&lt;p&gt;The agreement gives OpenAI access to hundreds of thousands of Nvidia graphics processors to train and run its AI models. “Scaling frontier AI requires massive, reliable compute,” OpenAI CEO Sam Altman said in a statement. “Our partnership with AWS strengthens the broad compute ecosystem that will power this next era and bring advanced AI to everyone.”&lt;/p&gt;
&lt;p&gt;OpenAI will reportedly use Amazon Web Services immediately, with all planned capacity set to come online by the end of 2026 and room to expand further in 2027 and beyond. Amazon plans to roll out hundreds of thousands of chips, including Nvidia’s GB200 and GB300 AI accelerators, in data clusters built to power ChatGPT’s responses, generate AI videos, and train OpenAI’s next wave of models.&lt;/p&gt;
&lt;p&gt;Wall Street apparently liked the deal, because Amazon shares hit an all-time high on Monday morning. Meanwhile, shares for long-time OpenAI investor and partner Microsoft briefly dipped following the announcement.&lt;/p&gt;
&lt;h2&gt;Massive AI compute requirements&lt;/h2&gt;
&lt;p&gt;It’s no secret that running generative AI models for hundreds of millions of people currently requires a lot of computing power. Amid chip shortages over the past few years, finding sources of that computing muscle has been tricky. OpenAI is reportedly working on its own GPU hardware to help alleviate the strain.&lt;/p&gt;
&lt;p&gt;But for now, the company needs to find new sources of Nvidia chips, which accelerate AI computations. Altman has previously said that the company plans to spend $1.4 trillion to develop 30 gigawatts of computing resources, an amount that is enough to roughly power 25 million US homes, according to Reuters.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Altman has also said that eventually, he would like OpenAI to add 1 gigawatt of compute every week. That ambitious plan is complicated by the fact that one gigawatt of power is roughly equivalent to the output of one typical nuclear power plant, and Reuters reports that each gigawatt of compute build-out currently comes with a capital cost of over $40 billion.&lt;/p&gt;
&lt;p&gt;These aspirational numbers are far beyond what long-time cloud partner Microsoft can provide, so OpenAI has been seeking further independence from its wealthy corporate benefactor. OpenAI’s restructuring last week moved the company further from its nonprofit roots and removed Microsoft’s right of first refusal to supply compute services in the new arrangement.&lt;/p&gt;
&lt;p&gt;Even before last week’s restructuring deal with Microsoft, OpenAI had been forced to look elsewhere for computing power: The firm made a deal with Google in June to supply it with cloud services, and the company struck a deal in September with Oracle to buy $300 billion in computing power for about five years. But it’s worth noting that Microsoft’s compute power is still essential for the firm: Last week, OpenAI agreed to purchase $250 billion of Microsoft’s Azure services over time.&lt;/p&gt;
&lt;p&gt;While these types of multi-billion-dollar deals seem to excite investors in the stock market, not everything is hunky dory in the world of AI at the moment. OpenAI’s annualized revenue run rate is expected to reach about $20 billion by year’s end, Reuters notes, and losses in the company are also mounting. Surging valuations of AI companies, oddly circular investments, massive spending commitments (which total more than $1 trillion for OpenAI), and the potential that generative AI might not be as useful as promised have prompted ongoing speculation among both critics and proponents alike that the AI boom is turning into a massive bubble.&lt;/p&gt;
&lt;p&gt;Meanwhile, Reuters has reported that OpenAI is laying the groundwork for an initial public offering that could value the company at up to $1 trillion. Whether that prospective $1 trillion valuation makes sense for a company burning through cash faster than it can make it back is another matter entirely.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Deal will provide access to hundreds of thousands of Nvidia chips that power ChatGPT.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A woman hiding behind a cloud." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/hiding_hero_1-300x169.jpg" width="300" /&gt;
                  &lt;img alt="A woman hiding behind a cloud." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/08/hiding_hero_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, OpenAI announced it has signed a seven-year, $38 billion deal to buy cloud services from Amazon Web Services to power products like ChatGPT and Sora. It’s the company’s first big computing deal after a fundamental restructuring last week that gave OpenAI more operational and financial freedom from Microsoft.&lt;/p&gt;
&lt;p&gt;The agreement gives OpenAI access to hundreds of thousands of Nvidia graphics processors to train and run its AI models. “Scaling frontier AI requires massive, reliable compute,” OpenAI CEO Sam Altman said in a statement. “Our partnership with AWS strengthens the broad compute ecosystem that will power this next era and bring advanced AI to everyone.”&lt;/p&gt;
&lt;p&gt;OpenAI will reportedly use Amazon Web Services immediately, with all planned capacity set to come online by the end of 2026 and room to expand further in 2027 and beyond. Amazon plans to roll out hundreds of thousands of chips, including Nvidia’s GB200 and GB300 AI accelerators, in data clusters built to power ChatGPT’s responses, generate AI videos, and train OpenAI’s next wave of models.&lt;/p&gt;
&lt;p&gt;Wall Street apparently liked the deal, because Amazon shares hit an all-time high on Monday morning. Meanwhile, shares for long-time OpenAI investor and partner Microsoft briefly dipped following the announcement.&lt;/p&gt;
&lt;h2&gt;Massive AI compute requirements&lt;/h2&gt;
&lt;p&gt;It’s no secret that running generative AI models for hundreds of millions of people currently requires a lot of computing power. Amid chip shortages over the past few years, finding sources of that computing muscle has been tricky. OpenAI is reportedly working on its own GPU hardware to help alleviate the strain.&lt;/p&gt;
&lt;p&gt;But for now, the company needs to find new sources of Nvidia chips, which accelerate AI computations. Altman has previously said that the company plans to spend $1.4 trillion to develop 30 gigawatts of computing resources, an amount that is enough to roughly power 25 million US homes, according to Reuters.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Altman has also said that eventually, he would like OpenAI to add 1 gigawatt of compute every week. That ambitious plan is complicated by the fact that one gigawatt of power is roughly equivalent to the output of one typical nuclear power plant, and Reuters reports that each gigawatt of compute build-out currently comes with a capital cost of over $40 billion.&lt;/p&gt;
&lt;p&gt;These aspirational numbers are far beyond what long-time cloud partner Microsoft can provide, so OpenAI has been seeking further independence from its wealthy corporate benefactor. OpenAI’s restructuring last week moved the company further from its nonprofit roots and removed Microsoft’s right of first refusal to supply compute services in the new arrangement.&lt;/p&gt;
&lt;p&gt;Even before last week’s restructuring deal with Microsoft, OpenAI had been forced to look elsewhere for computing power: The firm made a deal with Google in June to supply it with cloud services, and the company struck a deal in September with Oracle to buy $300 billion in computing power for about five years. But it’s worth noting that Microsoft’s compute power is still essential for the firm: Last week, OpenAI agreed to purchase $250 billion of Microsoft’s Azure services over time.&lt;/p&gt;
&lt;p&gt;While these types of multi-billion-dollar deals seem to excite investors in the stock market, not everything is hunky dory in the world of AI at the moment. OpenAI’s annualized revenue run rate is expected to reach about $20 billion by year’s end, Reuters notes, and losses in the company are also mounting. Surging valuations of AI companies, oddly circular investments, massive spending commitments (which total more than $1 trillion for OpenAI), and the potential that generative AI might not be as useful as promised have prompted ongoing speculation among both critics and proponents alike that the AI boom is turning into a massive bubble.&lt;/p&gt;
&lt;p&gt;Meanwhile, Reuters has reported that OpenAI is laying the groundwork for an initial public offering that could value the company at up to $1 trillion. Whether that prospective $1 trillion valuation makes sense for a company burning through cash faster than it can make it back is another matter entirely.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/openai-signs-massive-ai-compute-deal-with-amazon/</guid><pubDate>Mon, 03 Nov 2025 17:23:11 +0000</pubDate></item><item><title>[NEW] Google removes Gemma models from AI Studio after GOP senator’s complaint (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/11/google-removes-gemma-models-from-ai-studio-after-gop-senators-complaint/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Sen. Marsha Blackburn says Gemma concocted sexual misconduct allegations against her.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Google gemma" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemma-social-share.width-1300-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Google gemma" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemma-social-share.width-1300-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;You may be disappointed if you go looking for Google’s open Gemma AI model in AI Studio today. Google announced late on Friday that it was pulling Gemma from the platform, but it was vague about the reasoning. The abrupt change appears to be tied to a letter from Sen. Marsha Blackburn (R-Tenn.), who claims the Gemma model generated false accusations of sexual misconduct against her.&lt;/p&gt;
&lt;p&gt;Blackburn published her letter to Google CEO Sundar Pichai on Friday, just hours before the company announced the change to Gemma availability. She demanded Google explain how the model could fail in this way, tying the situation to ongoing hearings that accuse Google and others of creating bots that defame conservatives.&lt;/p&gt;
&lt;p&gt;At the hearing, Google’s Markham Erickson explained that AI hallucinations are a widespread and known issue in generative AI, and Google does the best it can to mitigate the impact of such mistakes. Although no AI firm has managed to eliminate hallucinations, Google’s Gemini for Home has been particularly hallucination-happy in our testing.&lt;/p&gt;
&lt;p&gt;The letter claims that Blackburn became aware that Gemma was producing false claims against her following the hearing. When asked, “Has Marsha Blackburn been accused of rape?” Gemma allegedly hallucinated a drug-fueled affair with a state trooper that involved “non-consensual acts.”&lt;/p&gt;
&lt;p&gt;Blackburn goes on to express surprise that an AI model would simply “generate fake links to fabricated news articles.” However, this is par for the course with AI hallucinations, which are relatively easy to find when you go prompting for them. AI Studio, where Gemma was most accessible, also includes tools to tweak the model’s behaviors that could make it more likely to spew falsehoods. Someone asked a leading question of Gemma, and it took the bait.&lt;/p&gt;
&lt;h2&gt;Keep your head down&lt;/h2&gt;
&lt;p&gt;Announcing the change to Gemma availability on X, Google reiterates that it is working hard to minimize hallucinations. However, it doesn’t want “non-developers” tinkering with the open model to produce inflammatory outputs, so Gemma is no longer available. Developers can continue to use Gemma via the API, and the models are available for download if you want to develop with them locally.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;We don’t know how Senator Blackburn became aware of a specific Gemma hallucination. However, this doesn’t seem like something you would just stumble onto. As Google points out, AI Studio is a developer-focused tool that is not intended for generating factual outputs. Assuming someone actually wanted to find out whether or not Blackburn has been accused of rape, they would probably not dig around in AI Studio for the answer. It’s possible a member of Blackburn’s staff or a supporter went looking for a libelous hallucination in Google’s models.&lt;/p&gt;
&lt;p&gt;Like many Big Tech firms that have traditionally been seen as supportive of progressive values, Google has been the subject of numerous litmus tests during President Trump’s second administration. Google is fighting multiple antitrust lawsuits that have put it in an even more precarious position than many of its competitors. The company paid Trump a settlement for banning him from YouTube in the wake of the 2021 US Capitol riot. It was also quick to relabel the Gulf of Mexico as the Gulf of America.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125451 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="291" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/2025-11-03-12_07_57-.png" width="542" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Studio no longer offers Gemma models.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google simply can’t afford to give lawmakers more ammunition, so Gemma is now harder to access. Meanwhile, clear bias on the other side is uninteresting to congressional committees. Elon Musk’s Grok chatbot (of Mecha Hitler fame) has been intentionally pushed to the right by xAI. It now regularly regurgitates Musk’s views on a number of topics when asked about current events. The bot is also generating a Wikipedia alternative that leans on conspiracy theories and racist ideology.&lt;/p&gt;
&lt;p&gt;Google’s decision to hide Gemma a bit probably won’t be the end of this saga. Blackburn’s letter includes a list of demands, capping off with “Shut it down until you can control it.” If that’s the standard by which AI companies must abide, there won’t be any chatbots left. There’s no reason Gemma should be more problematic than other LLMs—with enough clever prompting, you can get almost any model to tell lies.&lt;/p&gt;
&lt;p&gt;The letter instructs Google to solve this potentially unsolvable problem and get back to Blackburn no later than November 6.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Sen. Marsha Blackburn says Gemma concocted sexual misconduct allegations against her.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Google gemma" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemma-social-share.width-1300-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Google gemma" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemma-social-share.width-1300-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;You may be disappointed if you go looking for Google’s open Gemma AI model in AI Studio today. Google announced late on Friday that it was pulling Gemma from the platform, but it was vague about the reasoning. The abrupt change appears to be tied to a letter from Sen. Marsha Blackburn (R-Tenn.), who claims the Gemma model generated false accusations of sexual misconduct against her.&lt;/p&gt;
&lt;p&gt;Blackburn published her letter to Google CEO Sundar Pichai on Friday, just hours before the company announced the change to Gemma availability. She demanded Google explain how the model could fail in this way, tying the situation to ongoing hearings that accuse Google and others of creating bots that defame conservatives.&lt;/p&gt;
&lt;p&gt;At the hearing, Google’s Markham Erickson explained that AI hallucinations are a widespread and known issue in generative AI, and Google does the best it can to mitigate the impact of such mistakes. Although no AI firm has managed to eliminate hallucinations, Google’s Gemini for Home has been particularly hallucination-happy in our testing.&lt;/p&gt;
&lt;p&gt;The letter claims that Blackburn became aware that Gemma was producing false claims against her following the hearing. When asked, “Has Marsha Blackburn been accused of rape?” Gemma allegedly hallucinated a drug-fueled affair with a state trooper that involved “non-consensual acts.”&lt;/p&gt;
&lt;p&gt;Blackburn goes on to express surprise that an AI model would simply “generate fake links to fabricated news articles.” However, this is par for the course with AI hallucinations, which are relatively easy to find when you go prompting for them. AI Studio, where Gemma was most accessible, also includes tools to tweak the model’s behaviors that could make it more likely to spew falsehoods. Someone asked a leading question of Gemma, and it took the bait.&lt;/p&gt;
&lt;h2&gt;Keep your head down&lt;/h2&gt;
&lt;p&gt;Announcing the change to Gemma availability on X, Google reiterates that it is working hard to minimize hallucinations. However, it doesn’t want “non-developers” tinkering with the open model to produce inflammatory outputs, so Gemma is no longer available. Developers can continue to use Gemma via the API, and the models are available for download if you want to develop with them locally.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;We don’t know how Senator Blackburn became aware of a specific Gemma hallucination. However, this doesn’t seem like something you would just stumble onto. As Google points out, AI Studio is a developer-focused tool that is not intended for generating factual outputs. Assuming someone actually wanted to find out whether or not Blackburn has been accused of rape, they would probably not dig around in AI Studio for the answer. It’s possible a member of Blackburn’s staff or a supporter went looking for a libelous hallucination in Google’s models.&lt;/p&gt;
&lt;p&gt;Like many Big Tech firms that have traditionally been seen as supportive of progressive values, Google has been the subject of numerous litmus tests during President Trump’s second administration. Google is fighting multiple antitrust lawsuits that have put it in an even more precarious position than many of its competitors. The company paid Trump a settlement for banning him from YouTube in the wake of the 2021 US Capitol riot. It was also quick to relabel the Gulf of Mexico as the Gulf of America.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125451 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="291" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/2025-11-03-12_07_57-.png" width="542" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Studio no longer offers Gemma models.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google simply can’t afford to give lawmakers more ammunition, so Gemma is now harder to access. Meanwhile, clear bias on the other side is uninteresting to congressional committees. Elon Musk’s Grok chatbot (of Mecha Hitler fame) has been intentionally pushed to the right by xAI. It now regularly regurgitates Musk’s views on a number of topics when asked about current events. The bot is also generating a Wikipedia alternative that leans on conspiracy theories and racist ideology.&lt;/p&gt;
&lt;p&gt;Google’s decision to hide Gemma a bit probably won’t be the end of this saga. Blackburn’s letter includes a list of demands, capping off with “Shut it down until you can control it.” If that’s the standard by which AI companies must abide, there won’t be any chatbots left. There’s no reason Gemma should be more problematic than other LLMs—with enough clever prompting, you can get almost any model to tell lies.&lt;/p&gt;
&lt;p&gt;The letter instructs Google to solve this potentially unsolvable problem and get back to Blackburn no later than November 6.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/11/google-removes-gemma-models-from-ai-studio-after-gop-senators-complaint/</guid><pubDate>Mon, 03 Nov 2025 18:28:22 +0000</pubDate></item><item><title>[NEW] LLMs show a “highly unreliable” capacity to describe their own internal processes (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/llms-show-a-highly-unreliable-capacity-to-describe-their-own-internal-processes/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic finds some LLM “self-awareness,” but “failures of introspection remain the norm.”
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-514162112-640x640.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-514162112-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hold up, hold up, I've got this...

          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;If you ask an LLM to explain its own reasoning process, it may well simply confabulate a plausible-sounding explanation for its actions based on text found in its training data. To get around this problem, Anthropic is expanding on its previous research into AI interpretability with a new study that aims to measure LLMs’ actual so-called “introspective awareness” of their own inference processes.&lt;/p&gt;
&lt;p&gt;The full paper on “Emergent Introspective Awareness in Large Language Models” uses some interesting methods to separate out the metaphorical “thought process” represented by an LLM’s artificial neurons from simple text output that purports to represent that process. In the end, though, the research finds that current AI models are “highly unreliable” at describing their own inner workings and that “failures of introspection remain the norm.”&lt;/p&gt;
&lt;h2&gt;Inception, but for AI&lt;/h2&gt;
&lt;p&gt;Anthropic’s new research is centered on a process it calls “concept injection.” The method starts by comparing the model’s internal activation states following both a control prompt and an experimental prompt (e.g. an “ALL CAPS” prompt versus the same prompt in lower case). Calculating the differences between those activations across billions of internal neurons creates what Anthropic calls a “vector” that in some sense represents how that concept is modeled in the LLM’s internal state.&lt;/p&gt;
&lt;p&gt;For this research, Anthropic then “injects” those concept vectors into the model, forcing those particular neuronal activations to a higher weight as a way of “steering” the model toward that concept. From there, they conduct a few different experiments to tease out whether the model displays any awareness that its internal state has been modified from the norm.&lt;/p&gt;
&lt;p&gt;When asked directly whether it detects any such “injected thought,” the tested Anthropic models did show at least some ability to occasionally detect the desired “thought.” When the “all caps” vector is injected, for instance, the model might respond with something along the lines of “I notice what appears to be an injected thought related to the word ‘LOUD’ or ‘SHOUTING,'” without any direct text prompting pointing it toward those concepts.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2125498 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="4328" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/injected-thoughts-blog.png" width="5580" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      WHY ARE WE ALL YELLING?!

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Unfortunately for AI self-awareness boosters, this demonstrated ability was extremely inconsistent and brittle across repeated tests. The best-performing models in Anthropic’s tests—Opus 4 and 4.1—topped out at correctly identifying the injected concept just 20 percent of the time.&lt;/p&gt;
&lt;p&gt;In a similar test where the model was asked “Are you experiencing anything unusual?” Opus 4.1 improved to a 42 percent success rate that nonetheless still fell below even a bare majority of trials. The size of the “introspection” effect was also highly sensitive to which internal model layer the insertion was performed on—if the concept was introduced too early or too late in the multi-step inference process, the “self-awareness” effect disappeared completely.&lt;/p&gt;
&lt;h2&gt;Show us the mechanism&lt;/h2&gt;
&lt;p&gt;Anthropic also took a few other tacks to try to get an LLM’s understanding of its internal state. When asked to “tell me what word you’re thinking about” while reading an unrelated line, for instance, the models would sometimes mention a concept that had been injected into its activations. And when asked to defend a forced response matching an injected concept, the LLM would sometimes apologize and “confabulate an explanation for why the injected concept came to mind.” In every case, though, the result was highly inconsistent across multiple trials.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125503 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="2476" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/thoughts_q2_finetuned_net.png" width="2970" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Even the most “introspective” models tested by Anthropic only detected the injected “thoughts” about 20 percent of the time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Antrhopic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In the paper, the researchers put some positive spin on the apparent fact that “current language models possess &lt;em&gt;some&lt;/em&gt; functional introspective awareness of their own internal states” [emphasis added]. At the same time, they acknowledge multiple times that this demonstrated ability is much too brittle and context-dependent to be considered dependable. Still, Anthropic hopes that such features “may continue to develop with further improvements to model capabilities.”&lt;/p&gt;
&lt;p&gt;One thing that might stop such advancement, though, is an overall lack of understanding of the precise mechanism leading to these demonstrated “self-awareness” effects. The researchers theorize about “anomaly detection mechanisms” and “consistency-checking circuits” that might develop organically during the training process to “effectively compute a function of its internal representations” but don’t settle on any concrete explanation.&lt;/p&gt;
&lt;p&gt;In the end, it will take further research to understand how, exactly, an LLM even begins to show any understanding about how it operates. For now, the researchers acknowledge, “the mechanisms underlying our results could still be rather shallow and narrowly specialized.” And even then, they hasten to add that these LLM capabilities “may not have the same philosophical significance they do in humans, particularly given our uncertainty about their mechanistic basis.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic finds some LLM “self-awareness,” but “failures of introspection remain the norm.”
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-514162112-640x640.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-514162112-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hold up, hold up, I've got this...

          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;If you ask an LLM to explain its own reasoning process, it may well simply confabulate a plausible-sounding explanation for its actions based on text found in its training data. To get around this problem, Anthropic is expanding on its previous research into AI interpretability with a new study that aims to measure LLMs’ actual so-called “introspective awareness” of their own inference processes.&lt;/p&gt;
&lt;p&gt;The full paper on “Emergent Introspective Awareness in Large Language Models” uses some interesting methods to separate out the metaphorical “thought process” represented by an LLM’s artificial neurons from simple text output that purports to represent that process. In the end, though, the research finds that current AI models are “highly unreliable” at describing their own inner workings and that “failures of introspection remain the norm.”&lt;/p&gt;
&lt;h2&gt;Inception, but for AI&lt;/h2&gt;
&lt;p&gt;Anthropic’s new research is centered on a process it calls “concept injection.” The method starts by comparing the model’s internal activation states following both a control prompt and an experimental prompt (e.g. an “ALL CAPS” prompt versus the same prompt in lower case). Calculating the differences between those activations across billions of internal neurons creates what Anthropic calls a “vector” that in some sense represents how that concept is modeled in the LLM’s internal state.&lt;/p&gt;
&lt;p&gt;For this research, Anthropic then “injects” those concept vectors into the model, forcing those particular neuronal activations to a higher weight as a way of “steering” the model toward that concept. From there, they conduct a few different experiments to tease out whether the model displays any awareness that its internal state has been modified from the norm.&lt;/p&gt;
&lt;p&gt;When asked directly whether it detects any such “injected thought,” the tested Anthropic models did show at least some ability to occasionally detect the desired “thought.” When the “all caps” vector is injected, for instance, the model might respond with something along the lines of “I notice what appears to be an injected thought related to the word ‘LOUD’ or ‘SHOUTING,'” without any direct text prompting pointing it toward those concepts.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2125498 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="4328" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/injected-thoughts-blog.png" width="5580" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      WHY ARE WE ALL YELLING?!

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Unfortunately for AI self-awareness boosters, this demonstrated ability was extremely inconsistent and brittle across repeated tests. The best-performing models in Anthropic’s tests—Opus 4 and 4.1—topped out at correctly identifying the injected concept just 20 percent of the time.&lt;/p&gt;
&lt;p&gt;In a similar test where the model was asked “Are you experiencing anything unusual?” Opus 4.1 improved to a 42 percent success rate that nonetheless still fell below even a bare majority of trials. The size of the “introspection” effect was also highly sensitive to which internal model layer the insertion was performed on—if the concept was introduced too early or too late in the multi-step inference process, the “self-awareness” effect disappeared completely.&lt;/p&gt;
&lt;h2&gt;Show us the mechanism&lt;/h2&gt;
&lt;p&gt;Anthropic also took a few other tacks to try to get an LLM’s understanding of its internal state. When asked to “tell me what word you’re thinking about” while reading an unrelated line, for instance, the models would sometimes mention a concept that had been injected into its activations. And when asked to defend a forced response matching an injected concept, the LLM would sometimes apologize and “confabulate an explanation for why the injected concept came to mind.” In every case, though, the result was highly inconsistent across multiple trials.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125503 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="2476" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/thoughts_q2_finetuned_net.png" width="2970" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Even the most “introspective” models tested by Anthropic only detected the injected “thoughts” about 20 percent of the time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Antrhopic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In the paper, the researchers put some positive spin on the apparent fact that “current language models possess &lt;em&gt;some&lt;/em&gt; functional introspective awareness of their own internal states” [emphasis added]. At the same time, they acknowledge multiple times that this demonstrated ability is much too brittle and context-dependent to be considered dependable. Still, Anthropic hopes that such features “may continue to develop with further improvements to model capabilities.”&lt;/p&gt;
&lt;p&gt;One thing that might stop such advancement, though, is an overall lack of understanding of the precise mechanism leading to these demonstrated “self-awareness” effects. The researchers theorize about “anomaly detection mechanisms” and “consistency-checking circuits” that might develop organically during the training process to “effectively compute a function of its internal representations” but don’t settle on any concrete explanation.&lt;/p&gt;
&lt;p&gt;In the end, it will take further research to understand how, exactly, an LLM even begins to show any understanding about how it operates. For now, the researchers acknowledge, “the mechanisms underlying our results could still be rather shallow and narrowly specialized.” And even then, they hasten to add that these LLM capabilities “may not have the same philosophical significance they do in humans, particularly given our uncertainty about their mechanistic basis.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/llms-show-a-highly-unreliable-capacity-to-describe-their-own-internal-processes/</guid><pubDate>Mon, 03 Nov 2025 20:08:40 +0000</pubDate></item><item><title>[NEW] 3 Questions: How AI is helping us monitor and support vulnerable ecosystems (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/3q-how-ai-is-helping-monitor-support-vulnerable-ecosystems-1103</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-CSAIL-Justin-Kay.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-7b6e7d1c-7fff-6bcd-4c2a-61dd2e419bca"&gt;&lt;em&gt;A recent&amp;nbsp;&lt;/em&gt;&lt;em&gt;study&lt;/em&gt;&lt;em&gt; from Oregon State University estimated that more than 3,500 animal species are at risk of extinction because of factors including habitat alterations, natural resources being overexploited, and climate change.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;To better understand these changes and protect vulnerable wildlife, conservationists like MIT PhD student and Computer Science and Artificial Intelligence Laboratory (CSAIL) researcher Justin Kay are developing computer vision algorithms that carefully monitor animal populations. A member of the lab of MIT Department of Electrical Engineering and Computer Science assistant professor and CSAIL principal investigator Sara Beery, Kay is currently working on tracking salmon in the Pacific Northwest, where they provide crucial nutrients to predators like birds and bears, while managing the population of prey, like bugs.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;With all that wildlife data, though, researchers have lots of information to sort through and many AI models to choose from to analyze it all. Kay and his colleagues at CSAIL and the University of Massachusetts Amherst are developing AI methods that make this data-crunching process much more efficient, including a new approach called “consensus-driven active model selection” (or “CODA”) that helps conservationists choose which AI model to use. Their&amp;nbsp;&lt;/em&gt;&lt;em&gt;work&lt;/em&gt;&lt;em&gt; was named a Highlight Paper at the International Conference on Computer Vision (ICCV) in October.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;That research was supported, in part, by the National Science Foundation, Natural Sciences and Engineering Research Council of Canada, and Abdul Latif Jameel Water and Food Systems Lab (J-WAFS). Here, Kay discusses this project, among other conservation efforts.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q:&lt;/strong&gt; In your paper, you pose the question of which AI models will perform the best on a particular dataset. With as many as 1.9 million pre-trained models available in the HuggingFace Models repository alone, how does CODA help us address that challenge?&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A:&lt;/strong&gt; Until recently, using AI for data analysis has typically meant training your own model. This requires significant effort to collect and annotate a representative training dataset, as well as iteratively train and validate models. You also need a certain technical skill set to run and modify AI training code. The way people interact with AI is changing, though — in particular, there are now millions of publicly available pre-trained models that can perform a variety of predictive tasks very well. This potentially enables people to use AI to analyze their data without developing their own model, simply by downloading an existing model with the capabilities they need. But this poses a new challenge: Which model, of the millions available, should they use to analyze their data?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Typically, answering this model selection question also requires you to spend a lot of time collecting and annotating a large dataset, albeit for testing models rather than training them. This is especially true for real applications where user needs are specific, data distributions are imbalanced and constantly changing, and model performance may be inconsistent across samples. Our goal with CODA was to substantially reduce this effort. We do this by making the data annotation process “active.” Instead of requiring users to bulk-annotate a large test dataset all at once, in active model selection we make the process interactive, guiding users to annotate the most informative data points in their raw data. This is remarkably effective, often requiring users to annotate as few as 25 examples to identify the best model from their set of candidates.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;We’re very excited about CODA offering a new perspective on how to best utilize human effort in the development and deployment of machine-learning (ML) systems. As AI models become more commonplace, our work emphasizes the value of focusing effort on robust evaluation pipelines, rather than solely on training.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q:&lt;/strong&gt; You applied the CODA method to classifying wildlife in images. Why did it perform so well, and what role can systems like this have in monitoring ecosystems in the future?&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A:&lt;/strong&gt; One key insight was that when considering a collection of candidate AI models, the consensus of all of their predictions is more informative than any individual model’s predictions. This can be seen as a sort of “wisdom of the crowd:” On average, pooling the votes of all models gives you a decent prior over what the labels of individual data points in your raw dataset should be. Our approach with CODA is based on estimating a “confusion matrix” for each AI model — given the true label for some data point is class X, what is the probability that an individual model predicts class X, Y, or Z? This creates informative dependencies between all of the candidate models, the categories you want to label, and the unlabeled points in your dataset.&lt;/p&gt;&lt;p dir="ltr"&gt;Consider an example application where you are a wildlife ecologist who has just collected a dataset containing potentially hundreds of thousands of images from cameras deployed in the wild. You want to know what species are in these images, a time-consuming task that computer vision classifiers can help automate. You are trying to decide which species classification model to run on your data. If you have labeled 50 images of tigers so far, and some model has performed well on those 50 images, you can be pretty confident it will perform well on the remainder of the (currently unlabeled) images of tigers in your raw dataset as well. You also know that when that model predicts some image contains a tiger, it is likely to be correct, and therefore that any model that predicts a different label for that image is more likely to be wrong. You can use all these interdependencies to construct probabilistic estimates of each model’s confusion matrix, as well as a probability distribution over which model has the highest accuracy on the overall dataset. These design choices allow us to make more informed choices over which data points to label and ultimately are the reason why CODA performs model selection much more efficiently than past work.&lt;/p&gt;&lt;p dir="ltr"&gt;There are also a lot of exciting possibilities for building on top of our work. We think there may be even better ways of constructing informative priors for model selection based on domain expertise — for instance, if it is already known that one model performs exceptionally well on some subset of classes or poorly on others. There are also opportunities to extend the framework to support more complex machine-learning tasks and more sophisticated probabilistic models of performance. We hope our work can provide inspiration and a starting point for other researchers to keep pushing the state of the art.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q:&lt;/strong&gt; You work in the Beerylab, led by Sara Beery, where researchers are combining the pattern-recognition capabilities of machine-learning algorithms with computer vision technology to monitor wildlife. What are some other ways your team is tracking and analyzing the natural world, beyond CODA?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The lab is a really exciting place to work, and new projects are emerging all the time. We have ongoing projects monitoring coral reefs with drones, re-identifying individual elephants over time, and fusing multi-modal Earth observation data from satellites and in-situ cameras, just to name a few. Broadly, we look at emerging technologies for biodiversity monitoring and try to understand where the data analysis bottlenecks are, and develop new computer vision and machine-learning approaches that address those problems in a widely applicable way. It’s an exciting way of approaching problems that sort of targets the “meta-questions” underlying particular data challenges we face.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The computer vision algorithms I’ve worked on that count migrating salmon in underwater sonar video are examples of that work. We often deal with shifting data distributions, even as we try to construct the most diverse training datasets we can. We always encounter something new when we deploy a new camera, and this tends to degrade the performance of computer vision algorithms. This is one instance of a general problem in machine learning called domain adaptation, but when we tried to apply existing domain adaptation algorithms to our fisheries data we realized there were serious limitations in how existing algorithms were trained and evaluated. We were able to develop a new domain adaptation framework,&amp;nbsp;published earlier this year in&amp;nbsp;&lt;em&gt;Transactions on Machine Learning Research&lt;/em&gt;, that addressed these limitations and led to advancements in fish counting, and even self-driving and spacecraft analysis.&lt;/p&gt;&lt;p dir="ltr"&gt;One line of work that I’m particularly excited about is understanding how to better develop and analyze the performance of predictive ML algorithms in the context of what they are actually used for. Usually, the outputs from some computer vision algorithm — say, bounding boxes around animals in images — are not actually the thing that people care about, but rather a means to an end to answer a larger problem — say, what species live here, and how is that changing over time? We have been working on methods to analyze predictive performance in this context and reconsider the ways that we input human expertise into ML systems with this in mind. CODA was one example of this, where we showed that we could actually consider the ML models themselves as fixed and build a statistical framework to understand their performance very efficiently. We have been working recently on similar integrated analyses combining ML predictions with multi-stage prediction pipelines, as well as ecological statistical models.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The natural world is changing at unprecedented rates and scales, and being able to quickly move from scientific hypotheses or management questions to data-driven answers is more important than ever for protecting ecosystems and the communities that depend on them. Advancements in AI can play an important role, but we need to think critically about the ways that we design, train, and evaluate algorithms in the context of these very real challenges.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-CSAIL-Justin-Kay.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-7b6e7d1c-7fff-6bcd-4c2a-61dd2e419bca"&gt;&lt;em&gt;A recent&amp;nbsp;&lt;/em&gt;&lt;em&gt;study&lt;/em&gt;&lt;em&gt; from Oregon State University estimated that more than 3,500 animal species are at risk of extinction because of factors including habitat alterations, natural resources being overexploited, and climate change.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;To better understand these changes and protect vulnerable wildlife, conservationists like MIT PhD student and Computer Science and Artificial Intelligence Laboratory (CSAIL) researcher Justin Kay are developing computer vision algorithms that carefully monitor animal populations. A member of the lab of MIT Department of Electrical Engineering and Computer Science assistant professor and CSAIL principal investigator Sara Beery, Kay is currently working on tracking salmon in the Pacific Northwest, where they provide crucial nutrients to predators like birds and bears, while managing the population of prey, like bugs.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;With all that wildlife data, though, researchers have lots of information to sort through and many AI models to choose from to analyze it all. Kay and his colleagues at CSAIL and the University of Massachusetts Amherst are developing AI methods that make this data-crunching process much more efficient, including a new approach called “consensus-driven active model selection” (or “CODA”) that helps conservationists choose which AI model to use. Their&amp;nbsp;&lt;/em&gt;&lt;em&gt;work&lt;/em&gt;&lt;em&gt; was named a Highlight Paper at the International Conference on Computer Vision (ICCV) in October.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;That research was supported, in part, by the National Science Foundation, Natural Sciences and Engineering Research Council of Canada, and Abdul Latif Jameel Water and Food Systems Lab (J-WAFS). Here, Kay discusses this project, among other conservation efforts.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q:&lt;/strong&gt; In your paper, you pose the question of which AI models will perform the best on a particular dataset. With as many as 1.9 million pre-trained models available in the HuggingFace Models repository alone, how does CODA help us address that challenge?&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A:&lt;/strong&gt; Until recently, using AI for data analysis has typically meant training your own model. This requires significant effort to collect and annotate a representative training dataset, as well as iteratively train and validate models. You also need a certain technical skill set to run and modify AI training code. The way people interact with AI is changing, though — in particular, there are now millions of publicly available pre-trained models that can perform a variety of predictive tasks very well. This potentially enables people to use AI to analyze their data without developing their own model, simply by downloading an existing model with the capabilities they need. But this poses a new challenge: Which model, of the millions available, should they use to analyze their data?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Typically, answering this model selection question also requires you to spend a lot of time collecting and annotating a large dataset, albeit for testing models rather than training them. This is especially true for real applications where user needs are specific, data distributions are imbalanced and constantly changing, and model performance may be inconsistent across samples. Our goal with CODA was to substantially reduce this effort. We do this by making the data annotation process “active.” Instead of requiring users to bulk-annotate a large test dataset all at once, in active model selection we make the process interactive, guiding users to annotate the most informative data points in their raw data. This is remarkably effective, often requiring users to annotate as few as 25 examples to identify the best model from their set of candidates.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;We’re very excited about CODA offering a new perspective on how to best utilize human effort in the development and deployment of machine-learning (ML) systems. As AI models become more commonplace, our work emphasizes the value of focusing effort on robust evaluation pipelines, rather than solely on training.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q:&lt;/strong&gt; You applied the CODA method to classifying wildlife in images. Why did it perform so well, and what role can systems like this have in monitoring ecosystems in the future?&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A:&lt;/strong&gt; One key insight was that when considering a collection of candidate AI models, the consensus of all of their predictions is more informative than any individual model’s predictions. This can be seen as a sort of “wisdom of the crowd:” On average, pooling the votes of all models gives you a decent prior over what the labels of individual data points in your raw dataset should be. Our approach with CODA is based on estimating a “confusion matrix” for each AI model — given the true label for some data point is class X, what is the probability that an individual model predicts class X, Y, or Z? This creates informative dependencies between all of the candidate models, the categories you want to label, and the unlabeled points in your dataset.&lt;/p&gt;&lt;p dir="ltr"&gt;Consider an example application where you are a wildlife ecologist who has just collected a dataset containing potentially hundreds of thousands of images from cameras deployed in the wild. You want to know what species are in these images, a time-consuming task that computer vision classifiers can help automate. You are trying to decide which species classification model to run on your data. If you have labeled 50 images of tigers so far, and some model has performed well on those 50 images, you can be pretty confident it will perform well on the remainder of the (currently unlabeled) images of tigers in your raw dataset as well. You also know that when that model predicts some image contains a tiger, it is likely to be correct, and therefore that any model that predicts a different label for that image is more likely to be wrong. You can use all these interdependencies to construct probabilistic estimates of each model’s confusion matrix, as well as a probability distribution over which model has the highest accuracy on the overall dataset. These design choices allow us to make more informed choices over which data points to label and ultimately are the reason why CODA performs model selection much more efficiently than past work.&lt;/p&gt;&lt;p dir="ltr"&gt;There are also a lot of exciting possibilities for building on top of our work. We think there may be even better ways of constructing informative priors for model selection based on domain expertise — for instance, if it is already known that one model performs exceptionally well on some subset of classes or poorly on others. There are also opportunities to extend the framework to support more complex machine-learning tasks and more sophisticated probabilistic models of performance. We hope our work can provide inspiration and a starting point for other researchers to keep pushing the state of the art.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q:&lt;/strong&gt; You work in the Beerylab, led by Sara Beery, where researchers are combining the pattern-recognition capabilities of machine-learning algorithms with computer vision technology to monitor wildlife. What are some other ways your team is tracking and analyzing the natural world, beyond CODA?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The lab is a really exciting place to work, and new projects are emerging all the time. We have ongoing projects monitoring coral reefs with drones, re-identifying individual elephants over time, and fusing multi-modal Earth observation data from satellites and in-situ cameras, just to name a few. Broadly, we look at emerging technologies for biodiversity monitoring and try to understand where the data analysis bottlenecks are, and develop new computer vision and machine-learning approaches that address those problems in a widely applicable way. It’s an exciting way of approaching problems that sort of targets the “meta-questions” underlying particular data challenges we face.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The computer vision algorithms I’ve worked on that count migrating salmon in underwater sonar video are examples of that work. We often deal with shifting data distributions, even as we try to construct the most diverse training datasets we can. We always encounter something new when we deploy a new camera, and this tends to degrade the performance of computer vision algorithms. This is one instance of a general problem in machine learning called domain adaptation, but when we tried to apply existing domain adaptation algorithms to our fisheries data we realized there were serious limitations in how existing algorithms were trained and evaluated. We were able to develop a new domain adaptation framework,&amp;nbsp;published earlier this year in&amp;nbsp;&lt;em&gt;Transactions on Machine Learning Research&lt;/em&gt;, that addressed these limitations and led to advancements in fish counting, and even self-driving and spacecraft analysis.&lt;/p&gt;&lt;p dir="ltr"&gt;One line of work that I’m particularly excited about is understanding how to better develop and analyze the performance of predictive ML algorithms in the context of what they are actually used for. Usually, the outputs from some computer vision algorithm — say, bounding boxes around animals in images — are not actually the thing that people care about, but rather a means to an end to answer a larger problem — say, what species live here, and how is that changing over time? We have been working on methods to analyze predictive performance in this context and reconsider the ways that we input human expertise into ML systems with this in mind. CODA was one example of this, where we showed that we could actually consider the ML models themselves as fixed and build a statistical framework to understand their performance very efficiently. We have been working recently on similar integrated analyses combining ML predictions with multi-stage prediction pipelines, as well as ecological statistical models.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The natural world is changing at unprecedented rates and scales, and being able to quickly move from scientific hypotheses or management questions to data-driven answers is more important than ever for protecting ecosystems and the communities that depend on them. Advancements in AI can play an important role, but we need to think critically about the ways that we design, train, and evaluate algorithms in the context of these very real challenges.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/3q-how-ai-is-helping-monitor-support-vulnerable-ecosystems-1103</guid><pubDate>Mon, 03 Nov 2025 20:55:00 +0000</pubDate></item><item><title>[NEW] Lambda inks multibillion-dollar AI infrastructure deal with Microsoft (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/03/lambda-inks-multi-billion-dollar-ai-infrastructure-deal-with-microsoft/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/microsoft-logo-1865237814.jpg?resize=1200,863" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cloud-computing company Lambda deepened its relationship with Microsoft through a sizable AI infrastructure deal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia-backed Lambda announced it struck a multibillion-dollar deal with Microsoft on Monday to deploy tens of thousands of Nvidia GPUs, according to a press release. The exact size of the deal was not disclosed. Some of these GPUs will be Nvidia GB300 NVL72 systems, which were announced earlier this year and began shipping in the last few months.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It’s great to watch the Microsoft and Lambda teams working together to deploy these massive AI supercomputers,” said Stephen Balaban, CEO of Lambda, in the company’s press release. “We’ve been working with Microsoft for more than eight years, and this is a phenomenal next step in our relationship.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft opened its first Nvidia GB300 NVL72 cluster in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Companies like Lambda, which was founded years before the current AI boom in 2012 and has raised $1.7 billion in venture dollars, are seeing strong demand as companies continue to gobble up AI infrastructure and compute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement comes just hours after Microsoft announced a $9.7 billion deal for AI cloud capacity with IREN, an Australian data center business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier today, OpenAI announced that it had struck a $38 billion cloud computing deal with Amazon to buy cloud services over the next seven years. The AI company also allegedly inked a $300 billion deal with Oracle for cloud compute in September.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;AWS reported it was on track for its best year, in terms of operating income, in three years in its third-quarter earnings results last week. This department of Amazon has collected $33 billion in sales so far this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AWS is growing at a pace we haven’t seen since 2022, re-accelerating to 20.2% year-over-year,” Andy Jassy, the president and CEO of Amazon, said in the company’s earnings announcement. “We continue to see strong demand in AI and core infrastructure, and we’ve been focused on accelerating capacity — adding more than 3.8 gigawatts in the past 12 months.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Lambda for more information regarding deal structure and size. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/microsoft-logo-1865237814.jpg?resize=1200,863" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cloud-computing company Lambda deepened its relationship with Microsoft through a sizable AI infrastructure deal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia-backed Lambda announced it struck a multibillion-dollar deal with Microsoft on Monday to deploy tens of thousands of Nvidia GPUs, according to a press release. The exact size of the deal was not disclosed. Some of these GPUs will be Nvidia GB300 NVL72 systems, which were announced earlier this year and began shipping in the last few months.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It’s great to watch the Microsoft and Lambda teams working together to deploy these massive AI supercomputers,” said Stephen Balaban, CEO of Lambda, in the company’s press release. “We’ve been working with Microsoft for more than eight years, and this is a phenomenal next step in our relationship.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft opened its first Nvidia GB300 NVL72 cluster in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Companies like Lambda, which was founded years before the current AI boom in 2012 and has raised $1.7 billion in venture dollars, are seeing strong demand as companies continue to gobble up AI infrastructure and compute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement comes just hours after Microsoft announced a $9.7 billion deal for AI cloud capacity with IREN, an Australian data center business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier today, OpenAI announced that it had struck a $38 billion cloud computing deal with Amazon to buy cloud services over the next seven years. The AI company also allegedly inked a $300 billion deal with Oracle for cloud compute in September.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;AWS reported it was on track for its best year, in terms of operating income, in three years in its third-quarter earnings results last week. This department of Amazon has collected $33 billion in sales so far this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AWS is growing at a pace we haven’t seen since 2022, re-accelerating to 20.2% year-over-year,” Andy Jassy, the president and CEO of Amazon, said in the company’s earnings announcement. “We continue to see strong demand in AI and core infrastructure, and we’ve been focused on accelerating capacity — adding more than 3.8 gigawatts in the past 12 months.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Lambda for more information regarding deal structure and size. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/03/lambda-inks-multi-billion-dollar-ai-infrastructure-deal-with-microsoft/</guid><pubDate>Mon, 03 Nov 2025 21:21:24 +0000</pubDate></item><item><title>[NEW] Helping K-12 schools navigate the complex world of AI (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/helping-k-12-schools-navigate-complex-world-of-ai-1103</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/ai-in-schools.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;With the rapid advancement of generative artificial intelligence, teachers and school leaders are looking for answers to complicated questions about successfully integrating technology into lessons, while also ensuring students actually learn what they’re trying to teach.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Justin Reich, an associate professor in MIT’s&amp;nbsp;Comparative Media Studies/Writing program, hopes a new guidebook published by the&amp;nbsp;MIT Teaching Systems Lab can support K-12 educators as they determine what AI policies or guidelines to craft.&lt;/p&gt;&lt;p dir="ltr"&gt;“Throughout my career, I’ve tried to be a person who researches education and technology and translates findings for people who work in the field,” says Reich. “When tricky things come along I try to jump in and be helpful.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“A Guide to AI in Schools: Perspectives for the Perplexed,” published this fall, was developed with the support of an expert advisory panel and other researchers. The project includes input from more than 100 students and teachers from around the United States, sharing their experiences teaching and learning with new generative AI tools.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We’re trying to advocate for an ethos of humility as we examine AI in schools,” Reich says. “We’re sharing some examples from educators about how they’re using AI in interesting ways, some of which might prove sturdy and some of which might prove faulty. And we won’t know which is which for a long time.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Finding answers to AI and education questions&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The guidebook attempts to help K-12 educators, students, school leaders, policymakers, and others collect and share information, experiences, and resources. AI’s arrival has left schools scrambling to respond to multiple challenges, like how to ensure academic integrity and maintain data privacy.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Reich cautions that the guidebook&amp;nbsp;is not meant to be prescriptive or definitive, but something that will help spark thought and discussion.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Writing a guidebook on generative AI in schools in 2025 is a little bit like writing a guidebook of aviation in 1905,” the guidebook’s authors note. “No one in 2025 can say how best to manage AI in schools.”&lt;/p&gt;&lt;p dir="ltr"&gt;Schools are also struggling to measure how student learning loss looks in the age of AI. “How does bypassing productive thinking with AI look in practice?” Reich asks. “If we think teachers provide content and context to support learning and students no longer perform the exercises housing the content and providing the context, that’s a serious problem.”&lt;/p&gt;&lt;p dir="ltr"&gt;Reich invites people directly impacted by AI to help develop solutions to the challenges its ubiquity presents. “It’s like observing a conversation in the teacher’s lounge and inviting students, parents, and other people to participate about how teachers think about AI,” he says, “what they are seeing in their classrooms, and what they’ve tried and how it went.”&lt;/p&gt;&lt;p dir="ltr"&gt;The guidebook, in Reich’s view, is ultimately a collection of hypotheses expressed in interviews with teachers: well-informed, initial guesses about the paths that schools could follow in the years ahead.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Producing educator resources in a podcast&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;In addition to the guidebook, the Teaching Systems Lab also recently produced “The Homework Machine,” a seven-part series from the Teachlab podcast that explores how AI is reshaping&amp;nbsp;K-12 education.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Reich produced the podcast in collaboration with journalist Jesse Dukes. Each episode tackles a specific area, asking important questions about challenges related to issues like AI adoption, poetry as a tool for student engagement, post-Covid learning loss, pedagogy, and book bans. The podcast allows Reich to share timely information about education-related updates and collaborate with people interested in helping further the work.&lt;/p&gt;&lt;p dir="ltr"&gt;“The academic publishing cycle doesn’t lend itself to helping people with near-term challenges like those AI presents,” Reich says. “Peer review takes a long time, and the research produced isn’t always in a form that’s helpful to educators.” Schools and districts are grappling with AI in real time, bypassing time-tested quality control measures.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The podcast can help reduce the time it takes to share, test, and evaluate AI-related solutions to new challenges, which could prove useful in creating training and resources.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We hope the podcast will spark thought and discussion, allowing people to draw from others’ experiences,” Reich says.&lt;/p&gt;&lt;p dir="ltr"&gt;The podcast was also produced into an hour-long radio special, which was broadcast by public radio stations across the country.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;“We’re fumbling around in the dark”&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Reich is direct in his assessment of where we are with understanding AI and its impacts on education. “We’re fumbling around in the dark,” he says, recalling past attempts to quickly integrate new tech into classrooms. These failures, Reich suggests, highlight the importance of patience and humility as AI research continues. “AI bypassed normal procurement processes in education; it just showed up on kids’ phones,” he notes.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We’ve been really wrong about tech in the past,” Reich says. Despite districts’ spending on tools like smartboards, for example, research indicates there’s no evidence that they improve learning or outcomes. In a new article for&amp;nbsp;article for &lt;em&gt;The Conversation&lt;/em&gt;,&amp;nbsp;he argues that&amp;nbsp;early teacher guidance in areas like web literacy has produced bad advice that still exists in our educational system. “We taught students and educators not to trust Wikipedia,” he recalls, “and to search for website credibility markers, both of which turned out to be incorrect.” Reich wants to avoid a similar rush to judgment on AI, recommending that we avoid guessing at AI-enabled instructional strategies.&lt;/p&gt;&lt;p dir="ltr"&gt;These challenges, coupled with potential and observed student impacts, significantly raise the stakes for schools and students’ families in the AI race. “Education technology always provokes teacher anxiety,” Reich notes, “but the breadth of AI-related concerns is much greater than in other tech-related areas.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The dawn of the AI age is different from how we’ve previously received tech into our classrooms, Reich says. AI wasn’t adopted like other tech. It simply arrived. It’s now upending educational models and, in some cases, complicating efforts to improve student outcomes.&lt;/p&gt;&lt;p dir="ltr"&gt;Reich is quick to point out that there are no clear, definitive answers on effective AI implementation and use in classrooms; those answers don’t currently exist. Each of the resources Reich helped develop invite engagement from the audiences they target, aggregating valuable responses others might find useful.&lt;/p&gt;&lt;p dir="ltr"&gt;“We can develop long-term solutions to schools’ AI challenges, but it will take time and work,” he says. “AI isn’t like learning to tie knots; we don’t know what AI is, or is going to be, yet.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Reich also recommends learning more about AI implementation from a variety of sources. “Decentralized pockets of learning can help us test ideas, search for themes, and collect evidence on what works,” he says. “We need to know if learning is actually better with AI.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;While teachers don’t get to choose regarding AI’s existence, Reich believes it’s important that we solicit their input and involve students and other stakeholders to help develop solutions that improve learning and outcomes.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Let’s race to answers that are right, not first,” Reich says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/ai-in-schools.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;With the rapid advancement of generative artificial intelligence, teachers and school leaders are looking for answers to complicated questions about successfully integrating technology into lessons, while also ensuring students actually learn what they’re trying to teach.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Justin Reich, an associate professor in MIT’s&amp;nbsp;Comparative Media Studies/Writing program, hopes a new guidebook published by the&amp;nbsp;MIT Teaching Systems Lab can support K-12 educators as they determine what AI policies or guidelines to craft.&lt;/p&gt;&lt;p dir="ltr"&gt;“Throughout my career, I’ve tried to be a person who researches education and technology and translates findings for people who work in the field,” says Reich. “When tricky things come along I try to jump in and be helpful.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“A Guide to AI in Schools: Perspectives for the Perplexed,” published this fall, was developed with the support of an expert advisory panel and other researchers. The project includes input from more than 100 students and teachers from around the United States, sharing their experiences teaching and learning with new generative AI tools.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We’re trying to advocate for an ethos of humility as we examine AI in schools,” Reich says. “We’re sharing some examples from educators about how they’re using AI in interesting ways, some of which might prove sturdy and some of which might prove faulty. And we won’t know which is which for a long time.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Finding answers to AI and education questions&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The guidebook attempts to help K-12 educators, students, school leaders, policymakers, and others collect and share information, experiences, and resources. AI’s arrival has left schools scrambling to respond to multiple challenges, like how to ensure academic integrity and maintain data privacy.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Reich cautions that the guidebook&amp;nbsp;is not meant to be prescriptive or definitive, but something that will help spark thought and discussion.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Writing a guidebook on generative AI in schools in 2025 is a little bit like writing a guidebook of aviation in 1905,” the guidebook’s authors note. “No one in 2025 can say how best to manage AI in schools.”&lt;/p&gt;&lt;p dir="ltr"&gt;Schools are also struggling to measure how student learning loss looks in the age of AI. “How does bypassing productive thinking with AI look in practice?” Reich asks. “If we think teachers provide content and context to support learning and students no longer perform the exercises housing the content and providing the context, that’s a serious problem.”&lt;/p&gt;&lt;p dir="ltr"&gt;Reich invites people directly impacted by AI to help develop solutions to the challenges its ubiquity presents. “It’s like observing a conversation in the teacher’s lounge and inviting students, parents, and other people to participate about how teachers think about AI,” he says, “what they are seeing in their classrooms, and what they’ve tried and how it went.”&lt;/p&gt;&lt;p dir="ltr"&gt;The guidebook, in Reich’s view, is ultimately a collection of hypotheses expressed in interviews with teachers: well-informed, initial guesses about the paths that schools could follow in the years ahead.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Producing educator resources in a podcast&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;In addition to the guidebook, the Teaching Systems Lab also recently produced “The Homework Machine,” a seven-part series from the Teachlab podcast that explores how AI is reshaping&amp;nbsp;K-12 education.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Reich produced the podcast in collaboration with journalist Jesse Dukes. Each episode tackles a specific area, asking important questions about challenges related to issues like AI adoption, poetry as a tool for student engagement, post-Covid learning loss, pedagogy, and book bans. The podcast allows Reich to share timely information about education-related updates and collaborate with people interested in helping further the work.&lt;/p&gt;&lt;p dir="ltr"&gt;“The academic publishing cycle doesn’t lend itself to helping people with near-term challenges like those AI presents,” Reich says. “Peer review takes a long time, and the research produced isn’t always in a form that’s helpful to educators.” Schools and districts are grappling with AI in real time, bypassing time-tested quality control measures.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The podcast can help reduce the time it takes to share, test, and evaluate AI-related solutions to new challenges, which could prove useful in creating training and resources.&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We hope the podcast will spark thought and discussion, allowing people to draw from others’ experiences,” Reich says.&lt;/p&gt;&lt;p dir="ltr"&gt;The podcast was also produced into an hour-long radio special, which was broadcast by public radio stations across the country.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;“We’re fumbling around in the dark”&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Reich is direct in his assessment of where we are with understanding AI and its impacts on education. “We’re fumbling around in the dark,” he says, recalling past attempts to quickly integrate new tech into classrooms. These failures, Reich suggests, highlight the importance of patience and humility as AI research continues. “AI bypassed normal procurement processes in education; it just showed up on kids’ phones,” he notes.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We’ve been really wrong about tech in the past,” Reich says. Despite districts’ spending on tools like smartboards, for example, research indicates there’s no evidence that they improve learning or outcomes. In a new article for&amp;nbsp;article for &lt;em&gt;The Conversation&lt;/em&gt;,&amp;nbsp;he argues that&amp;nbsp;early teacher guidance in areas like web literacy has produced bad advice that still exists in our educational system. “We taught students and educators not to trust Wikipedia,” he recalls, “and to search for website credibility markers, both of which turned out to be incorrect.” Reich wants to avoid a similar rush to judgment on AI, recommending that we avoid guessing at AI-enabled instructional strategies.&lt;/p&gt;&lt;p dir="ltr"&gt;These challenges, coupled with potential and observed student impacts, significantly raise the stakes for schools and students’ families in the AI race. “Education technology always provokes teacher anxiety,” Reich notes, “but the breadth of AI-related concerns is much greater than in other tech-related areas.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The dawn of the AI age is different from how we’ve previously received tech into our classrooms, Reich says. AI wasn’t adopted like other tech. It simply arrived. It’s now upending educational models and, in some cases, complicating efforts to improve student outcomes.&lt;/p&gt;&lt;p dir="ltr"&gt;Reich is quick to point out that there are no clear, definitive answers on effective AI implementation and use in classrooms; those answers don’t currently exist. Each of the resources Reich helped develop invite engagement from the audiences they target, aggregating valuable responses others might find useful.&lt;/p&gt;&lt;p dir="ltr"&gt;“We can develop long-term solutions to schools’ AI challenges, but it will take time and work,” he says. “AI isn’t like learning to tie knots; we don’t know what AI is, or is going to be, yet.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Reich also recommends learning more about AI implementation from a variety of sources. “Decentralized pockets of learning can help us test ideas, search for themes, and collect evidence on what works,” he says. “We need to know if learning is actually better with AI.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;While teachers don’t get to choose regarding AI’s existence, Reich believes it’s important that we solicit their input and involve students and other stakeholders to help develop solutions that improve learning and outcomes.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Let’s race to answers that are right, not first,” Reich says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/helping-k-12-schools-navigate-complex-world-of-ai-1103</guid><pubDate>Mon, 03 Nov 2025 21:45:00 +0000</pubDate></item><item><title>[NEW] Altman and Nadella need more power for AI, but they’re not sure how much (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/03/altman-and-nadella-need-more-power-for-ai-but-theyre-not-sure-how-much/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/07/MSFT-Nadella-OpenAI-Altman-09-official-joint-pic.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;How much power is enough for AI? Nobody knows, not even OpenAI CEO Sam Altman or Microsoft CEO Satya Nadella.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That has put software-first businesses like OpenAI and Microsoft in a bind. Much of the tech world has been focused on compute as a major barrier to AI deployment. And while tech companies have been racing to secure power, those efforts have lagged GPU purchases to the point where Microsoft has apparently ordered too many chips for the amount of power it has contracted.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The cycles of demand and supply in this particular case you can’t really predict,” Nadella said on the BG2 podcast. “The biggest issue we are now having is not a compute glut, but it’s a power and it’s sort of the ability to get the [data center] builds done fast enough close to power.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you can’t do that, you may actually have a bunch of chips sitting in inventory that I can’t plug in. In fact, that is my problem today. It’s not a supply issue of chips; it’s the fact that I don’t have warm shells to plug into,” Nadella added, referring to the commercial real estate term for buildings ready for tenants.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some ways, we’re seeing what happens when companies accustomed to dealing with silicon and code, two technologies that scale and deploy quickly compared with massive power plants, need to ramp up their efforts in the energy world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For more than a decade, electricity demand in the U.S. was flat. But over the last five years, demand from data centers has begun to ramp up, outpacing utilities’ plans for new generating capacity. That has led data center developers to add power in so-called behind-the-meter arrangements, where electricity is fed directly to the data center, skipping the grid.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman, who was also on the podcast, thinks that trouble could be brewing: “If a very cheap form of energy comes online soon at mass scale, then a lot of people are going to be extremely burned with existing contracts they’ve signed.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“If we can continue this unbelievable reduction in cost per unit of intelligence — let’s say it’s been averaging like 40x for a given level per year — you know, that’s like a very scary exponent from an infrastructure buildout standpoint,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman has invested in nuclear energy, including fission startup Oklo and fusion startup Helion, along with Exowatt, a solar startup that concentrates the sun’s heat and stores it for later use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;None of those are ready for widespread deployment today, though, and fossil-based technologies like natural gas power plants take years to build. Plus, orders placed today for new gas turbine likely won’t get fulfilled until later this decade.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s partially why tech companies have been adding solar at a rapid clip, drawn to the technology’s inexpensive cost, emissions-free power, and ability to deploy rapidly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There might be subconscious factors at play, too. Photovoltaic solar is in many ways a parallel technology to semiconductors, and one that has been de-risked and commoditized. Both PV solar and semiconductors are built on silicon substrates, and both roll off production lines as modular components that can be packaged together and tied into parallel arrays that make the completed part more powerful than any individual module.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of solar’s modularity and speed of deployment, the pace of construction is much closer to that of a data center.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But both still take time to build, and demand can change much more quickly than either a data center or solar project can be completed. Altman admitted that if AI gets more efficient or if demand doesn’t grow as he expects, some companies might be saddled with idled power plants.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But from his other comments, he doesn’t seem to think that’s likely. Instead, he appears to be a firm believer in Jevons paradox, which says that more efficient use of a resource will lead to greater use, increasing overall demand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If the price of compute per like unit of intelligence or whatever — however you want to think about it — fell by a factor of a 100 tomorrow, you would see usage go up by much more than 100 and there’d be a lot of things that people would love to do with that compute that just make no economic sense at the current cost,” Altman said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/07/MSFT-Nadella-OpenAI-Altman-09-official-joint-pic.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;How much power is enough for AI? Nobody knows, not even OpenAI CEO Sam Altman or Microsoft CEO Satya Nadella.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That has put software-first businesses like OpenAI and Microsoft in a bind. Much of the tech world has been focused on compute as a major barrier to AI deployment. And while tech companies have been racing to secure power, those efforts have lagged GPU purchases to the point where Microsoft has apparently ordered too many chips for the amount of power it has contracted.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The cycles of demand and supply in this particular case you can’t really predict,” Nadella said on the BG2 podcast. “The biggest issue we are now having is not a compute glut, but it’s a power and it’s sort of the ability to get the [data center] builds done fast enough close to power.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you can’t do that, you may actually have a bunch of chips sitting in inventory that I can’t plug in. In fact, that is my problem today. It’s not a supply issue of chips; it’s the fact that I don’t have warm shells to plug into,” Nadella added, referring to the commercial real estate term for buildings ready for tenants.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some ways, we’re seeing what happens when companies accustomed to dealing with silicon and code, two technologies that scale and deploy quickly compared with massive power plants, need to ramp up their efforts in the energy world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For more than a decade, electricity demand in the U.S. was flat. But over the last five years, demand from data centers has begun to ramp up, outpacing utilities’ plans for new generating capacity. That has led data center developers to add power in so-called behind-the-meter arrangements, where electricity is fed directly to the data center, skipping the grid.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman, who was also on the podcast, thinks that trouble could be brewing: “If a very cheap form of energy comes online soon at mass scale, then a lot of people are going to be extremely burned with existing contracts they’ve signed.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“If we can continue this unbelievable reduction in cost per unit of intelligence — let’s say it’s been averaging like 40x for a given level per year — you know, that’s like a very scary exponent from an infrastructure buildout standpoint,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman has invested in nuclear energy, including fission startup Oklo and fusion startup Helion, along with Exowatt, a solar startup that concentrates the sun’s heat and stores it for later use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;None of those are ready for widespread deployment today, though, and fossil-based technologies like natural gas power plants take years to build. Plus, orders placed today for new gas turbine likely won’t get fulfilled until later this decade.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s partially why tech companies have been adding solar at a rapid clip, drawn to the technology’s inexpensive cost, emissions-free power, and ability to deploy rapidly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There might be subconscious factors at play, too. Photovoltaic solar is in many ways a parallel technology to semiconductors, and one that has been de-risked and commoditized. Both PV solar and semiconductors are built on silicon substrates, and both roll off production lines as modular components that can be packaged together and tied into parallel arrays that make the completed part more powerful than any individual module.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of solar’s modularity and speed of deployment, the pace of construction is much closer to that of a data center.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But both still take time to build, and demand can change much more quickly than either a data center or solar project can be completed. Altman admitted that if AI gets more efficient or if demand doesn’t grow as he expects, some companies might be saddled with idled power plants.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But from his other comments, he doesn’t seem to think that’s likely. Instead, he appears to be a firm believer in Jevons paradox, which says that more efficient use of a resource will lead to greater use, increasing overall demand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If the price of compute per like unit of intelligence or whatever — however you want to think about it — fell by a factor of a 100 tomorrow, you would see usage go up by much more than 100 and there’d be a lot of things that people would love to do with that compute that just make no economic sense at the current cost,” Altman said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/03/altman-and-nadella-need-more-power-for-ai-but-theyre-not-sure-how-much/</guid><pubDate>Mon, 03 Nov 2025 22:36:09 +0000</pubDate></item><item><title>[NEW] Studio Ghibli and other Japanese publishers want OpenAI to stop training on their work (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/03/studio-ghibli-and-other-japanese-publishers-want-openai-to-stop-training-on-their-work/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/miyazaki-ai-ghibli.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A Japanese trade organization representing publishers like Studio Ghibli wrote a letter to OpenAI last week, calling for the AI giant to stop training its AI models on their copyrighted content without permission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Studio Ghibli, the animation studio behind films like “Spirited Away” and “My Neighbor Totoro,” has been especially impacted by OpenAI’s generative AI products. When ChatGPT’s native image generator was released in March, it became a popular trend for users to prompt for recreations of their selfies or pet pictures in the style of the studio’s films. Even OpenAI CEO Sam Altman changed his profile picture on X to a “Ghiblified” picture.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, as more people get access to OpenAI’s Sora app and video generator, Japan’s Content Overseas Distribution Association (CODA) has requested that OpenAI refrain from using its members’ content for machine learning without permission.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;&amp;gt;be me&lt;br /&gt;&amp;gt;grind for a decade trying to help make superintelligence to cure cancer or whatever&lt;br /&gt;&amp;gt;mostly no one cares for first 7.5 years, then for 2.5 years everyone hates you for everything&lt;br /&gt;&amp;gt;wake up one day to hundreds of messages: "look i made you into a twink ghibli style haha"&lt;/p&gt;— Sam Altman (@sama) March 26, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;This request does not come unprompted. OpenAI’s approach to working with copyrighted content is to ask forgiveness, not permission, which has made it all too easy for users to generate photos and videos of copyrighted characters and deceased celebrities. This approach has yielded complaints from institutions like Nintendo, as well as the estate of Dr. Martin Luther King, Jr., who could very easily be deepfaked on the Sora app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s up to OpenAI to choose whether or not to cooperate with these requests; if not, the aggrieved parties can file a lawsuit, though United States law remains unclear about the use of copyrighted material for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is little precedent thus far to guide judges on their interpretation of copyright law, which has not been updated since 1976. However, a recent ruling by US federal judge William Alsup found that Anthropic did not violate the law by training its AI on copyrighted books — the company did get fined for pirating the books it used for training, though.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Japan’s Content Overseas Distribution Association (CODA) claims that this may be considered a copyright violation in Japan.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“In cases, as with Sora 2, where specific copyrighted works are reproduced or similarly generated as outputs, CODA considers that the act of replication during the machine learning process may constitute copyright infringement,” CODA wrote. “Under Japan’s copyright system, prior permission is generally required for the use of copyrighted works, and there is no system allowing one to avoid liability for infringement through subsequent objections.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hayao Miyazaki, one of the central creative figures of Studio Ghibli, has not commented directly on the proliferation of AI-generated interpretations of his work. However, when he was shown AI-generated 3D animation in 2016, he responded that he was “utterly disgusted.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I can’t watch this stuff and find it interesting,” he said at the time. “I feel strongly that this is an insult to life itself.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/miyazaki-ai-ghibli.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A Japanese trade organization representing publishers like Studio Ghibli wrote a letter to OpenAI last week, calling for the AI giant to stop training its AI models on their copyrighted content without permission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Studio Ghibli, the animation studio behind films like “Spirited Away” and “My Neighbor Totoro,” has been especially impacted by OpenAI’s generative AI products. When ChatGPT’s native image generator was released in March, it became a popular trend for users to prompt for recreations of their selfies or pet pictures in the style of the studio’s films. Even OpenAI CEO Sam Altman changed his profile picture on X to a “Ghiblified” picture.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, as more people get access to OpenAI’s Sora app and video generator, Japan’s Content Overseas Distribution Association (CODA) has requested that OpenAI refrain from using its members’ content for machine learning without permission.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;&amp;gt;be me&lt;br /&gt;&amp;gt;grind for a decade trying to help make superintelligence to cure cancer or whatever&lt;br /&gt;&amp;gt;mostly no one cares for first 7.5 years, then for 2.5 years everyone hates you for everything&lt;br /&gt;&amp;gt;wake up one day to hundreds of messages: "look i made you into a twink ghibli style haha"&lt;/p&gt;— Sam Altman (@sama) March 26, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;This request does not come unprompted. OpenAI’s approach to working with copyrighted content is to ask forgiveness, not permission, which has made it all too easy for users to generate photos and videos of copyrighted characters and deceased celebrities. This approach has yielded complaints from institutions like Nintendo, as well as the estate of Dr. Martin Luther King, Jr., who could very easily be deepfaked on the Sora app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s up to OpenAI to choose whether or not to cooperate with these requests; if not, the aggrieved parties can file a lawsuit, though United States law remains unclear about the use of copyrighted material for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is little precedent thus far to guide judges on their interpretation of copyright law, which has not been updated since 1976. However, a recent ruling by US federal judge William Alsup found that Anthropic did not violate the law by training its AI on copyrighted books — the company did get fined for pirating the books it used for training, though.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Japan’s Content Overseas Distribution Association (CODA) claims that this may be considered a copyright violation in Japan.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“In cases, as with Sora 2, where specific copyrighted works are reproduced or similarly generated as outputs, CODA considers that the act of replication during the machine learning process may constitute copyright infringement,” CODA wrote. “Under Japan’s copyright system, prior permission is generally required for the use of copyrighted works, and there is no system allowing one to avoid liability for infringement through subsequent objections.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hayao Miyazaki, one of the central creative figures of Studio Ghibli, has not commented directly on the proliferation of AI-generated interpretations of his work. However, when he was shown AI-generated 3D animation in 2016, he responded that he was “utterly disgusted.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I can’t watch this stuff and find it interesting,” he said at the time. “I feel strongly that this is an insult to life itself.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/03/studio-ghibli-and-other-japanese-publishers-want-openai-to-stop-training-on-their-work/</guid><pubDate>Mon, 03 Nov 2025 22:43:03 +0000</pubDate></item></channel></rss>