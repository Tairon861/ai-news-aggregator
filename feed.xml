<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 30 Jul 2025 01:59:51 +0000</lastBuildDate><item><title>Stack Overflow data reveals the hidden productivity tax of ‘almost right’ AI code (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/stack-overflow-data-reveals-the-hidden-productivity-tax-of-almost-right-ai-code/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;More developers than ever before are using AI tools to both assist and generate code.&lt;/p&gt;&lt;p&gt;While enterprise AI adoption accelerates, new data from Stack Overflow’s 2025 Developer Survey exposes a critical blind spot: the mounting technical debt created by AI tools that generate “almost right” solutions, potentially undermining the productivity gains they promise to deliver.&lt;/p&gt;&lt;p&gt;Stack Overflow’s annual developer survey is one of the largest such reports in any given year. In 2024 the report found that developers were not worried that AI would still their jobs. Somewhat ironically,&amp;nbsp; Stack Overflow was initially negatively impacted by the growth of gen AI, with declining traffic and resulting layoffs in 2023.&lt;/p&gt;&lt;p&gt;The 2025 survey of over 49,000 developers across 177 countries reveals a troubling paradox in enterprise AI adoption. AI usage continues climbing—84% of developers now use or plan to use AI tools, up from 76% in 2024. Yet trust in these tools has cratered.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;“One of the most surprising findings was a significant shift in developer preferences for AI compared to previous years, while most developers use AI, they like it less and trust it less this year,” Erin Yepis, Senior Analyst for Market Research and Insights at Stack Overflow, told VentureBeat. “This response is surprising because with all of the investment in and focus on AI in tech news, I would expect that the trust would grow as the technology gets better.”&lt;/p&gt;



&lt;p&gt;The numbers tell the story. Only 33% of developers trust AI accuracy in 2025, down from 43% in 2024 and 42% in 2023. AI favorability dropped from 77% in 2023 to 72% in 2024 to just 60% this year.&lt;/p&gt;



&lt;p&gt;But the survey data reveals a more urgent concern for technical decision-makers. Developers cite “AI solutions that are almost right, but not quite” as their top frustration—66% report this problem. Meanwhile, 45% say debugging AI-generated code takes more time than expected. AI tools promise productivity gains but may actually create new categories of technical debt.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-almost-right-phenomenon-disrupts-developer-workflows"&gt;The ‘almost right’ phenomenon disrupts developer workflows&lt;/h2&gt;



&lt;p&gt;AI tools don’t just produce obviously broken code. They generate plausible solutions that require significant developer intervention to become production-ready. This creates a particularly insidious productivity problem.&lt;/p&gt;



&lt;p&gt;“AI tools seem to have a universal promise of saving time and increasing productivity, but developers are spending time addressing the unintended breakdowns in the workflow caused by AI,” Yepis explained. “Most developers say AI tools do not address complexity, only 29% believed AI tools could handle complex problems this year, down from 35% last year.”&lt;/p&gt;



&lt;p&gt;Unlike obviously broken code that developers quickly identify and discard, “almost right” solutions demand careful analysis. Developers must understand what’s wrong and how to fix it. Many report it would be faster to write the code from scratch than to debug and correct AI-generated solutions.&lt;/p&gt;



&lt;p&gt;The workflow disruption extends beyond individual coding tasks. The survey found 54% of developers use six or more tools to complete their jobs. This adds context-switching overhead to an already complex development process.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-enterprise-governance-frameworks-trail-behind-adoption"&gt;Enterprise governance frameworks trail behind adoption&lt;/h2&gt;



&lt;p&gt;Rapid AI adoption has outpaced enterprise governance capabilities. Organizations now face potential security and technical debt risks they haven’t fully addressed.&lt;/p&gt;



&lt;p&gt;“Vibe coding requires a higher level of trust in the AI’s output, and sacrifices confidence and potential security concerns in the code for a faster turnaround,” Ben Matthews, Senior Director of Engineering at Stack Overflow, told VentureBeat.&lt;/p&gt;



&lt;p&gt;Developers largely reject vibe coding for professional work, with 77% noting that it’s not part of their professional development process. Yet the survey reveals gaps in how enterprises manage AI-generated code quality.&lt;/p&gt;



&lt;p&gt;Matthews warns that AI coding tools powered by LLMs can and do produce mistakes. He noted that while knowledgeable developers are able to identify and test vulnerable code themselves, LLMs are sometimes simply unable to even register any mistakes they may produce.&lt;/p&gt;



&lt;p&gt;Security risks compound these quality issues. The survey data shows that when developers would still turn to humans for coding help, 61.7% cite “ethical or security concerns about code” as a key reason. This suggests that AI tools introduce integration challenges around data access, performance and security that organizations are still learning to manage.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-developers-still-use-stack-overflow-and-other-human-sources-of-expertise"&gt;Developers still use Stack Overflow and other human sources of expertise&lt;/h2&gt;



&lt;p&gt;Despite declining trust, developers aren’t abandoning AI tools. They’re developing more sophisticated strategies for integrating them into workflows. The survey shows 69% of developers spent time learning new coding techniques or programming languages in the past year. Of these, 44% used AI-enabled tools for learning, up from 37% in 2024.&lt;/p&gt;



&lt;p&gt;Even with the rise of vibe coding and AI, the survey data shows that developers maintain strong connections to human expertise and community resources. Stack Overflow remains the top community platform at 84% usage. GitHub follows at 67% and YouTube at 61%. Most tellingly, 89% of developers visit Stack Overflow multiple times per month. Among these, 35% turn to the platform specifically after encountering issues with AI responses.&lt;/p&gt;



&lt;p&gt;“Although we have seen a decline in traffic, in no way is it as dramatic as some would indicate,” Jody Bailey, Chief Product &amp;amp; Technology Officer, told VentureBeat.&lt;/p&gt;



&lt;p&gt;That said, Bailey did admit that times change and the day-to-day needs of users are not the same as they were 16 years ago when Stack Overflow got started. He noted that there is not a single site or company not seeing a shift in where users come from or how they are now engaging with gen AI tools. That shift is causing Stack Overflow to critically reassess how it gauges success in the modern digital age.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The future vitality of the internet and the broader tech ecosystem will no longer be solely defined by metrics of success outlined in the 90s or early 00s,” Bailey said. “Instead, the emphasis is increasingly on the caliber of data, the reliability of information, and the incredibly vital role of expert communities and individuals in meticulously creating, sharing and curating knowledge. “&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-strategic-recommendations-for-technical-decision-makers"&gt;&lt;strong&gt;Strategic recommendations for technical decision-makers&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The Stack Overflow data suggests several key considerations for enterprise teams evaluating AI development tools.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Invest in debugging and code review capabilities&lt;/strong&gt;: With 45% of developers reporting increased debugging time for AI code, organizations need stronger code review processes. They need debugging tools specifically designed for AI-generated solutions.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Maintain human expertise pipelines&lt;/strong&gt;: Continued reliance on community platforms and human consultation shows that AI tools amplify rather than replace the need for experienced developers. These experts can identify and correct AI-generated code issues.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Implement staged AI adoption&lt;/strong&gt;: Successful AI adoption requires careful integration with existing tools and processes rather than wholesale replacement of development workflows. This allows developers to leverage AI strengths while mitigating “almost right” solution risks.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Focus on AI tool literacy&lt;/strong&gt;: Developers using AI tools daily show 88% favorability compared to 64% for weekly users. This suggests proper training and integration strategies significantly impact outcomes.&lt;/p&gt;



&lt;p&gt;For enterprises looking to lead in AI-driven development, this data indicates competitive advantage will come not from AI adoption speed, but from developing superior capabilities in AI-human workflow integration and AI-generated code quality management.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Organizations that solve the “almost right” problem,turning AI tools into reliable productivity multipliers rather than sources of technical debt,will gain significant advantages in development speed and code quality.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;More developers than ever before are using AI tools to both assist and generate code.&lt;/p&gt;&lt;p&gt;While enterprise AI adoption accelerates, new data from Stack Overflow’s 2025 Developer Survey exposes a critical blind spot: the mounting technical debt created by AI tools that generate “almost right” solutions, potentially undermining the productivity gains they promise to deliver.&lt;/p&gt;&lt;p&gt;Stack Overflow’s annual developer survey is one of the largest such reports in any given year. In 2024 the report found that developers were not worried that AI would still their jobs. Somewhat ironically,&amp;nbsp; Stack Overflow was initially negatively impacted by the growth of gen AI, with declining traffic and resulting layoffs in 2023.&lt;/p&gt;&lt;p&gt;The 2025 survey of over 49,000 developers across 177 countries reveals a troubling paradox in enterprise AI adoption. AI usage continues climbing—84% of developers now use or plan to use AI tools, up from 76% in 2024. Yet trust in these tools has cratered.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;“One of the most surprising findings was a significant shift in developer preferences for AI compared to previous years, while most developers use AI, they like it less and trust it less this year,” Erin Yepis, Senior Analyst for Market Research and Insights at Stack Overflow, told VentureBeat. “This response is surprising because with all of the investment in and focus on AI in tech news, I would expect that the trust would grow as the technology gets better.”&lt;/p&gt;



&lt;p&gt;The numbers tell the story. Only 33% of developers trust AI accuracy in 2025, down from 43% in 2024 and 42% in 2023. AI favorability dropped from 77% in 2023 to 72% in 2024 to just 60% this year.&lt;/p&gt;



&lt;p&gt;But the survey data reveals a more urgent concern for technical decision-makers. Developers cite “AI solutions that are almost right, but not quite” as their top frustration—66% report this problem. Meanwhile, 45% say debugging AI-generated code takes more time than expected. AI tools promise productivity gains but may actually create new categories of technical debt.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-almost-right-phenomenon-disrupts-developer-workflows"&gt;The ‘almost right’ phenomenon disrupts developer workflows&lt;/h2&gt;



&lt;p&gt;AI tools don’t just produce obviously broken code. They generate plausible solutions that require significant developer intervention to become production-ready. This creates a particularly insidious productivity problem.&lt;/p&gt;



&lt;p&gt;“AI tools seem to have a universal promise of saving time and increasing productivity, but developers are spending time addressing the unintended breakdowns in the workflow caused by AI,” Yepis explained. “Most developers say AI tools do not address complexity, only 29% believed AI tools could handle complex problems this year, down from 35% last year.”&lt;/p&gt;



&lt;p&gt;Unlike obviously broken code that developers quickly identify and discard, “almost right” solutions demand careful analysis. Developers must understand what’s wrong and how to fix it. Many report it would be faster to write the code from scratch than to debug and correct AI-generated solutions.&lt;/p&gt;



&lt;p&gt;The workflow disruption extends beyond individual coding tasks. The survey found 54% of developers use six or more tools to complete their jobs. This adds context-switching overhead to an already complex development process.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-enterprise-governance-frameworks-trail-behind-adoption"&gt;Enterprise governance frameworks trail behind adoption&lt;/h2&gt;



&lt;p&gt;Rapid AI adoption has outpaced enterprise governance capabilities. Organizations now face potential security and technical debt risks they haven’t fully addressed.&lt;/p&gt;



&lt;p&gt;“Vibe coding requires a higher level of trust in the AI’s output, and sacrifices confidence and potential security concerns in the code for a faster turnaround,” Ben Matthews, Senior Director of Engineering at Stack Overflow, told VentureBeat.&lt;/p&gt;



&lt;p&gt;Developers largely reject vibe coding for professional work, with 77% noting that it’s not part of their professional development process. Yet the survey reveals gaps in how enterprises manage AI-generated code quality.&lt;/p&gt;



&lt;p&gt;Matthews warns that AI coding tools powered by LLMs can and do produce mistakes. He noted that while knowledgeable developers are able to identify and test vulnerable code themselves, LLMs are sometimes simply unable to even register any mistakes they may produce.&lt;/p&gt;



&lt;p&gt;Security risks compound these quality issues. The survey data shows that when developers would still turn to humans for coding help, 61.7% cite “ethical or security concerns about code” as a key reason. This suggests that AI tools introduce integration challenges around data access, performance and security that organizations are still learning to manage.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-developers-still-use-stack-overflow-and-other-human-sources-of-expertise"&gt;Developers still use Stack Overflow and other human sources of expertise&lt;/h2&gt;



&lt;p&gt;Despite declining trust, developers aren’t abandoning AI tools. They’re developing more sophisticated strategies for integrating them into workflows. The survey shows 69% of developers spent time learning new coding techniques or programming languages in the past year. Of these, 44% used AI-enabled tools for learning, up from 37% in 2024.&lt;/p&gt;



&lt;p&gt;Even with the rise of vibe coding and AI, the survey data shows that developers maintain strong connections to human expertise and community resources. Stack Overflow remains the top community platform at 84% usage. GitHub follows at 67% and YouTube at 61%. Most tellingly, 89% of developers visit Stack Overflow multiple times per month. Among these, 35% turn to the platform specifically after encountering issues with AI responses.&lt;/p&gt;



&lt;p&gt;“Although we have seen a decline in traffic, in no way is it as dramatic as some would indicate,” Jody Bailey, Chief Product &amp;amp; Technology Officer, told VentureBeat.&lt;/p&gt;



&lt;p&gt;That said, Bailey did admit that times change and the day-to-day needs of users are not the same as they were 16 years ago when Stack Overflow got started. He noted that there is not a single site or company not seeing a shift in where users come from or how they are now engaging with gen AI tools. That shift is causing Stack Overflow to critically reassess how it gauges success in the modern digital age.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The future vitality of the internet and the broader tech ecosystem will no longer be solely defined by metrics of success outlined in the 90s or early 00s,” Bailey said. “Instead, the emphasis is increasingly on the caliber of data, the reliability of information, and the incredibly vital role of expert communities and individuals in meticulously creating, sharing and curating knowledge. “&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-strategic-recommendations-for-technical-decision-makers"&gt;&lt;strong&gt;Strategic recommendations for technical decision-makers&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The Stack Overflow data suggests several key considerations for enterprise teams evaluating AI development tools.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Invest in debugging and code review capabilities&lt;/strong&gt;: With 45% of developers reporting increased debugging time for AI code, organizations need stronger code review processes. They need debugging tools specifically designed for AI-generated solutions.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Maintain human expertise pipelines&lt;/strong&gt;: Continued reliance on community platforms and human consultation shows that AI tools amplify rather than replace the need for experienced developers. These experts can identify and correct AI-generated code issues.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Implement staged AI adoption&lt;/strong&gt;: Successful AI adoption requires careful integration with existing tools and processes rather than wholesale replacement of development workflows. This allows developers to leverage AI strengths while mitigating “almost right” solution risks.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Focus on AI tool literacy&lt;/strong&gt;: Developers using AI tools daily show 88% favorability compared to 64% for weekly users. This suggests proper training and integration strategies significantly impact outcomes.&lt;/p&gt;



&lt;p&gt;For enterprises looking to lead in AI-driven development, this data indicates competitive advantage will come not from AI adoption speed, but from developing superior capabilities in AI-human workflow integration and AI-generated code quality management.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Organizations that solve the “almost right” problem,turning AI tools into reliable productivity multipliers rather than sources of technical debt,will gain significant advantages in development speed and code quality.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/stack-overflow-data-reveals-the-hidden-productivity-tax-of-almost-right-ai-code/</guid><pubDate>Tue, 29 Jul 2025 14:00:00 +0000</pubDate></item><item><title>Microsoft in talks to maintain access to OpenAI’s tech beyond AGI milestone (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/29/microsoft-in-talks-to-maintain-access-to-openais-tech-beyond-agi-milestone/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778706501.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft is reportedly in advanced talks with OpenAI for a new agreement that would give it ongoing access to the startup’s technology even if OpenAI achieves what it defines as AGI, or advanced general intelligence. If the deal goes through, it would clear a key hurdle in OpenAI’s transition toward becoming a fully commercial enterprise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companies have been negotiating regularly, and they could come to an agreement in a few weeks, Bloomberg reports, citing three anonymous sources. The report cited some of the sources as saying that while the talks have been positive, other roadblocks could emerge in the form of regulatory scrutiny and Elon Musk’s lawsuit to block OpenAI’s for-profit transition.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI is currently structured as a mission-driven nonprofit that oversees a capped for-profit company — a setup that’s meant to limit how fully it can commercialize or raise money. That structure hasn’t stopped it from raising billions and operating like a traditional tech company, but OpenAI still wants to shake off its constraints.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft, OpenAI’s biggest backer with $13.75 billion invested and rights to some of the ChatGPT maker’s IP, has put up meaningful roadblocks to OpenAI’s future as a for-profit company, with talks dragging on for months.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft wants a bigger stake in the restructured company and seeks to secure its access to OpenAI’s tech beyond the current deal, which ends in 2030 or whenever OpenAI says it has achieved AGI — though no one can really agree on what that means.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft has built its Azure OpenAI Service around the smaller company’s models, and has integrated the startup’s tech into Copilot across Windows, Office, and GitHub. If OpenAI suddenly declares it has achieved AGI and cuts off access, Microsoft would lose a huge strategic advantage. The ChatGPT maker has reportedly also told its investors that it expects to pay Microsoft a lower share of its revenue as it progresses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A source told Bloomberg that OpenAI also hopes to guarantee that Microsoft deploys OpenAI’s technology safely, especially as it nears AGI.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft also stands to gain from OpenAI becoming a for-profit company. The current setup caps investor returns, so a more standard structure would give Microsoft a chance to receive formal equity and significant returns, in addition to access to OpenAI’s tech.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bloomberg reports that the two companies have been negotiating an equity stake for Microsoft in the low- to mid-30% range.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778706501.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft is reportedly in advanced talks with OpenAI for a new agreement that would give it ongoing access to the startup’s technology even if OpenAI achieves what it defines as AGI, or advanced general intelligence. If the deal goes through, it would clear a key hurdle in OpenAI’s transition toward becoming a fully commercial enterprise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companies have been negotiating regularly, and they could come to an agreement in a few weeks, Bloomberg reports, citing three anonymous sources. The report cited some of the sources as saying that while the talks have been positive, other roadblocks could emerge in the form of regulatory scrutiny and Elon Musk’s lawsuit to block OpenAI’s for-profit transition.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI is currently structured as a mission-driven nonprofit that oversees a capped for-profit company — a setup that’s meant to limit how fully it can commercialize or raise money. That structure hasn’t stopped it from raising billions and operating like a traditional tech company, but OpenAI still wants to shake off its constraints.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft, OpenAI’s biggest backer with $13.75 billion invested and rights to some of the ChatGPT maker’s IP, has put up meaningful roadblocks to OpenAI’s future as a for-profit company, with talks dragging on for months.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft wants a bigger stake in the restructured company and seeks to secure its access to OpenAI’s tech beyond the current deal, which ends in 2030 or whenever OpenAI says it has achieved AGI — though no one can really agree on what that means.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft has built its Azure OpenAI Service around the smaller company’s models, and has integrated the startup’s tech into Copilot across Windows, Office, and GitHub. If OpenAI suddenly declares it has achieved AGI and cuts off access, Microsoft would lose a huge strategic advantage. The ChatGPT maker has reportedly also told its investors that it expects to pay Microsoft a lower share of its revenue as it progresses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A source told Bloomberg that OpenAI also hopes to guarantee that Microsoft deploys OpenAI’s technology safely, especially as it nears AGI.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft also stands to gain from OpenAI becoming a for-profit company. The current setup caps investor returns, so a more standard structure would give Microsoft a chance to receive formal equity and significant returns, in addition to access to OpenAI’s tech.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bloomberg reports that the two companies have been negotiating an equity stake for Microsoft in the low- to mid-30% range.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/29/microsoft-in-talks-to-maintain-access-to-openais-tech-beyond-agi-milestone/</guid><pubDate>Tue, 29 Jul 2025 14:44:10 +0000</pubDate></item><item><title>Google’s AI Mode gets new ‘Canvas’ feature, real-time help with Search Live, and more (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/29/googles-ai-mode-gets-new-canvas-feature-real-time-help-with-search-live-and-more/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it’s adding new capabilities to&amp;nbsp;AI Mode, its experimental feature that allows users to ask complex questions and follow-ups to dig deeper on a topic directly within Search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the new features, Canvas, helps you build study plans and organize information over multiple sessions in a side panel. For example, if you want to create a study plan for an upcoming test, you can click the new “Create Canvas” button to get started.&amp;nbsp;From there, AI Mode will start putting things together in the Canvas side panel, and you can keep refining the output with follow-up prompts until it fits what you’re looking for.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Soon, you will also be able to upload files like class notes or a syllabus to customize your study guide. Users enrolled in the AI Mode Labs experiment in the U.S. will see Canvas in the coming weeks.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3031925" height="680" src="https://techcrunch.com/wp-content/uploads/2025/07/Canvas-in-AI-Mode.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also bringing Project Astra capabilities directly into AI Mode via Search Live, which is integrated with Google Lens, the tech giant’s visual search tool.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When you go Live with Search, it’s like having an expert on speed dial who can see what you see and talk through tricky concepts in real-time, all with easy access to helpful links on the web,” wrote Robby Stein, VP of Product, Google Search, in a press release. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the feature, open Lens in the Google app, tap the Live icon, and ask a question while pointing the camera at something. With this feature, users can have a back-and-forth conversation with Search in AI Mode using visual context from their camera feed.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3031927" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/Search-Live-with-video-3-of-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Search Live with video input is rolling out this week on mobile in the U.S. for users enrolled in the AI Mode Labs experiment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Google announced that users will soon be able to use Lens in AI Mode to ask about what’s on their desktop screen. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Perhaps you’re looking at a geometry problem and want to better understand one of the diagrams,” Stein said. “Click on ‘Ask Google about this page’ from the address bar and select the diagram. You’ll get an AI Overview with a snapshot of key information directly in the side panel. And this week, you’ll be able to follow up with more questions through AI Mode, by selecting AI Mode at the top of the Lens search results or by clicking the ‘Dive deeper’ button at the bottom of the AI Overview.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3031929" height="384" src="https://techcrunch.com/wp-content/uploads/2025/07/Screenshot-2025-07-29-at-11.42.03AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, while you can already use AI Mode in the Google app to ask questions about images, you can now do so on desktop as well. Google is also adding support for PDF uploads on desktop, letting you ask detailed questions about documents.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For example, you can upload PDF slides from a school lecture and ask follow-up questions to deepen your understanding beyond the class materials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says AI Mode will support additional file types beyond PDFs and images later this year, including Google Drive files.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it’s adding new capabilities to&amp;nbsp;AI Mode, its experimental feature that allows users to ask complex questions and follow-ups to dig deeper on a topic directly within Search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the new features, Canvas, helps you build study plans and organize information over multiple sessions in a side panel. For example, if you want to create a study plan for an upcoming test, you can click the new “Create Canvas” button to get started.&amp;nbsp;From there, AI Mode will start putting things together in the Canvas side panel, and you can keep refining the output with follow-up prompts until it fits what you’re looking for.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Soon, you will also be able to upload files like class notes or a syllabus to customize your study guide. Users enrolled in the AI Mode Labs experiment in the U.S. will see Canvas in the coming weeks.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3031925" height="680" src="https://techcrunch.com/wp-content/uploads/2025/07/Canvas-in-AI-Mode.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also bringing Project Astra capabilities directly into AI Mode via Search Live, which is integrated with Google Lens, the tech giant’s visual search tool.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When you go Live with Search, it’s like having an expert on speed dial who can see what you see and talk through tricky concepts in real-time, all with easy access to helpful links on the web,” wrote Robby Stein, VP of Product, Google Search, in a press release. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the feature, open Lens in the Google app, tap the Live icon, and ask a question while pointing the camera at something. With this feature, users can have a back-and-forth conversation with Search in AI Mode using visual context from their camera feed.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3031927" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/Search-Live-with-video-3-of-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Search Live with video input is rolling out this week on mobile in the U.S. for users enrolled in the AI Mode Labs experiment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Google announced that users will soon be able to use Lens in AI Mode to ask about what’s on their desktop screen. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Perhaps you’re looking at a geometry problem and want to better understand one of the diagrams,” Stein said. “Click on ‘Ask Google about this page’ from the address bar and select the diagram. You’ll get an AI Overview with a snapshot of key information directly in the side panel. And this week, you’ll be able to follow up with more questions through AI Mode, by selecting AI Mode at the top of the Lens search results or by clicking the ‘Dive deeper’ button at the bottom of the AI Overview.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3031929" height="384" src="https://techcrunch.com/wp-content/uploads/2025/07/Screenshot-2025-07-29-at-11.42.03AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, while you can already use AI Mode in the Google app to ask questions about images, you can now do so on desktop as well. Google is also adding support for PDF uploads on desktop, letting you ask detailed questions about documents.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For example, you can upload PDF slides from a school lecture and ask follow-up questions to deepen your understanding beyond the class materials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says AI Mode will support additional file types beyond PDFs and images later this year, including Google Drive files.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/29/googles-ai-mode-gets-new-canvas-feature-real-time-help-with-search-live-and-more/</guid><pubDate>Tue, 29 Jul 2025 16:00:00 +0000</pubDate></item><item><title>Google’s Veo 3 AI video creation tools are now widely available (AI News)</title><link>https://www.artificialintelligence-news.com/news/google-veo-3-ai-video-creation-tools-now-widely-available/</link><description>&lt;p&gt;Google has made its most powerful AI video creator, Veo 3, available for everyone to use on its Vertex AI platform. And for those who need to work quickly, a speedier version called Veo 3 Fast is also ready-to-go for quick creative work.&lt;/p&gt;&lt;p&gt;Ever had a brilliant idea for a video but found yourself held back by the cost, time, or technical skills needed to create it? This tool aims to offer a faster way to turn your text ideas into everything from short films to product demos.&lt;/p&gt;&lt;p&gt;70 million videos have been created since May, showing a huge global appetite for these AI video creation tools. Businesses are diving in as well, generating over 6 million videos since they got early access in June.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-real-world-applications-for-veo-3"&gt;The real-world applications for Veo 3&lt;/h3&gt;&lt;p&gt;So, what does this look like in the real world? From global design platforms to major advertising agencies, companies are already putting Veo 3 to work. Take design platform Canva, they are building Veo directly into their software to make video creation simple for their users.&lt;/p&gt;&lt;p&gt;Cameron Adams, Co-Founder and Chief Product Officer at Canva, said: “Enabling anyone to bring their ideas to life – especially their most creative ones – has been core to Canva’s mission ever since we set out to empower the world to design.&lt;/p&gt;&lt;p&gt;“By democratising access to a powerful technology like Google’s Veo 3 inside Canva AI, your big ideas can now be brought to life in the highest quality video and sound, all from within your existing Canva subscription. In true Canva fashion, we’ve built this with an intuitive interface and simple editing tools in place, all backed by Canva Shield.”&lt;/p&gt;&lt;p&gt;For creative agencies like BarkleyOKRP, the big wins are speed and quality. They claim to have been so impressed with the latest version that they went back and remade videos.&lt;/p&gt;&lt;p&gt;Julie Ray Barr, Senior Vice President Client Experience at BarkleyOKRP, commented: “The rapid advancements from Veo 2 to Veo 3 within such a short time frame on this project have been nothing short of remarkable.&lt;/p&gt;&lt;p&gt;“Our team undertook the task of re-creating numerous music videos initially produced with Veo 2 once Veo 3 was released, primarily due to the significantly improved synchronization between voice and mouth movements. The continuous daily progress we are witnessing is truly extraordinary.”&lt;/p&gt;&lt;p&gt;It’s even changing how global companies connect with local customers. The investing platform eToro used Veo 3 to create 15 different, fully AI-generated versions of a single advertisement, each customised to a specific country with its own native language.&lt;/p&gt;&lt;p&gt;Shay Chikotay, Head of Creative &amp;amp; Content at eToro, said: “With Veo 3, we produced 15 fully AI‑generated versions of our ad, each in the native language of its market, all while capturing real emotion at scale.&lt;/p&gt;&lt;p&gt;“Ironically, AI didn’t reduce humanity; it amplified it. Veo 3 lets us tell more stories, in more tongues, with more impact.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-google-gives-creators-a-powerful-ai-video-creation-tool"&gt;Google gives creators a powerful AI video creation tool&lt;/h3&gt;&lt;p&gt;Veo 3 and Veo 3 Fast are packed with features designed to give you the control to tell complete stories.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Create scenes with sound.&lt;/strong&gt; The AI generates video and audio at the same time, so you can have characters that speak with accurate lip-syncing and sound effects that fit the scene.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;High quality results.&lt;/strong&gt; The models produce video in high-definition (1080p), making it good enough for professional marketing campaigns and demos.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Reach a global audience easily.&lt;/strong&gt; Veo 3’s ability to generate dialogue natively makes it much simpler to produce a video once and then translate the dialogue for many different languages.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Bring still images to life.&lt;/strong&gt; A new feature, coming in August, will let you take a single photo, add a text prompt, and watch as Veo animates it into an 8-second video clip.&lt;/li&gt;&lt;/ul&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt; [embedded content]&lt;/p&gt;&lt;/figure&gt;&lt;p&gt;Of course, with such powerful technology, safety is a key concern. Google has built Veo 3 for responsible enterprise use. Every video frame is embedded with an invisible digital watermark from SynthID to help combat misinformation. The service is also covered by Google’s indemnity for generative AI, giving businesses that extra layer of security.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google’s newest Gemini 2.5 model aims for ‘intelligence per dollar’&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Google has made its most powerful AI video creator, Veo 3, available for everyone to use on its Vertex AI platform. And for those who need to work quickly, a speedier version called Veo 3 Fast is also ready-to-go for quick creative work.&lt;/p&gt;&lt;p&gt;Ever had a brilliant idea for a video but found yourself held back by the cost, time, or technical skills needed to create it? This tool aims to offer a faster way to turn your text ideas into everything from short films to product demos.&lt;/p&gt;&lt;p&gt;70 million videos have been created since May, showing a huge global appetite for these AI video creation tools. Businesses are diving in as well, generating over 6 million videos since they got early access in June.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-real-world-applications-for-veo-3"&gt;The real-world applications for Veo 3&lt;/h3&gt;&lt;p&gt;So, what does this look like in the real world? From global design platforms to major advertising agencies, companies are already putting Veo 3 to work. Take design platform Canva, they are building Veo directly into their software to make video creation simple for their users.&lt;/p&gt;&lt;p&gt;Cameron Adams, Co-Founder and Chief Product Officer at Canva, said: “Enabling anyone to bring their ideas to life – especially their most creative ones – has been core to Canva’s mission ever since we set out to empower the world to design.&lt;/p&gt;&lt;p&gt;“By democratising access to a powerful technology like Google’s Veo 3 inside Canva AI, your big ideas can now be brought to life in the highest quality video and sound, all from within your existing Canva subscription. In true Canva fashion, we’ve built this with an intuitive interface and simple editing tools in place, all backed by Canva Shield.”&lt;/p&gt;&lt;p&gt;For creative agencies like BarkleyOKRP, the big wins are speed and quality. They claim to have been so impressed with the latest version that they went back and remade videos.&lt;/p&gt;&lt;p&gt;Julie Ray Barr, Senior Vice President Client Experience at BarkleyOKRP, commented: “The rapid advancements from Veo 2 to Veo 3 within such a short time frame on this project have been nothing short of remarkable.&lt;/p&gt;&lt;p&gt;“Our team undertook the task of re-creating numerous music videos initially produced with Veo 2 once Veo 3 was released, primarily due to the significantly improved synchronization between voice and mouth movements. The continuous daily progress we are witnessing is truly extraordinary.”&lt;/p&gt;&lt;p&gt;It’s even changing how global companies connect with local customers. The investing platform eToro used Veo 3 to create 15 different, fully AI-generated versions of a single advertisement, each customised to a specific country with its own native language.&lt;/p&gt;&lt;p&gt;Shay Chikotay, Head of Creative &amp;amp; Content at eToro, said: “With Veo 3, we produced 15 fully AI‑generated versions of our ad, each in the native language of its market, all while capturing real emotion at scale.&lt;/p&gt;&lt;p&gt;“Ironically, AI didn’t reduce humanity; it amplified it. Veo 3 lets us tell more stories, in more tongues, with more impact.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-google-gives-creators-a-powerful-ai-video-creation-tool"&gt;Google gives creators a powerful AI video creation tool&lt;/h3&gt;&lt;p&gt;Veo 3 and Veo 3 Fast are packed with features designed to give you the control to tell complete stories.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Create scenes with sound.&lt;/strong&gt; The AI generates video and audio at the same time, so you can have characters that speak with accurate lip-syncing and sound effects that fit the scene.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;High quality results.&lt;/strong&gt; The models produce video in high-definition (1080p), making it good enough for professional marketing campaigns and demos.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Reach a global audience easily.&lt;/strong&gt; Veo 3’s ability to generate dialogue natively makes it much simpler to produce a video once and then translate the dialogue for many different languages.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Bring still images to life.&lt;/strong&gt; A new feature, coming in August, will let you take a single photo, add a text prompt, and watch as Veo animates it into an 8-second video clip.&lt;/li&gt;&lt;/ul&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt; [embedded content]&lt;/p&gt;&lt;/figure&gt;&lt;p&gt;Of course, with such powerful technology, safety is a key concern. Google has built Veo 3 for responsible enterprise use. Every video frame is embedded with an invisible digital watermark from SynthID to help combat misinformation. The service is also covered by Google’s indemnity for generative AI, giving businesses that extra layer of security.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google’s newest Gemini 2.5 model aims for ‘intelligence per dollar’&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/google-veo-3-ai-video-creation-tools-now-widely-available/</guid><pubDate>Tue, 29 Jul 2025 16:01:39 +0000</pubDate></item><item><title>Spotify hints at a more chatty voice AI interface in the future (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/29/spotify-hints-at-a-more-chatty-voice-ai-interface-in-the-future/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/10/spotify-app-GettyImages-1689920063-e1702479483636.jpeg?resize=1200,676" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify has experimented with various voice interfaces in the past, and more recently, added an AI DJ that introduces tunes the company thinks you’ll like and lets you make your own requests. Now, Spotify is teasing that generative AI advances may pave the way for an even more conversational interface with the streaming service in the future, according to comments made during Spotify’s second-quarter earnings call with investors on Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked about how AI was affecting its business, Spotify Chief Product and Technology Officer Gustav Söderström noted that consumers today are able to talk to Spotify in plain English to request music, allowing the company to associate which phrases connect with which songs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“You can think of it as us getting a new dataset,” he said. “Spotify’s got this unique dataset from all of its playlists, which was really song-to-song — like which song goes well with another song, kind of the Amazon ‘people who bought this also bought that,’” he said. Meanwhile, the voice interface is delivering new insights to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And that’s completely new to us, and it’s a very, very valuable dataset that we are collecting very quickly,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result, the exec says we can expect Spotify’s consumer experiences to get “much more interactive.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can already write to Spotify, talk to Spotify. You’re just going to see that expand,” Söderström said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, AI technology would allow Spotify to do more than just predict what someone might want to listen to, whether music, podcasts, or audiobooks, as it does today. Instead of only making predictions, it would be able to “reason” over the user’s listening history and what they said to the AI DJ. The reference to reasoning suggests that Spotify is looking toward AI reasoning models that can perform more complex tasks across multiple steps.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The voice data in question comes from Spotify’s AI DJ feature. In May 2025, Spotify introduced a way for Premium subscribers to make voice requests to the English-language AI DJ by pressing a button in the app. This lets users change the music, the genre, or the mood of a playlist using a voice command.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Consumer features aren’t the only way Spotify is leveraging AI. During the call, the exec also mentioned that Spotify is using generative AI internally to more rapidly prototype products and create efficiencies in other areas of its business, like finance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the quarter, Spotify hit 276 million paying subscribers, up 12% year-over-year, and 696 million monthly active users, but swung to a loss after missing revenue targets. The stock fell 10% on weak guidance and Spotify CEO Daniel Ek’s comments about his unhappiness with the company’s ads business.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/10/spotify-app-GettyImages-1689920063-e1702479483636.jpeg?resize=1200,676" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify has experimented with various voice interfaces in the past, and more recently, added an AI DJ that introduces tunes the company thinks you’ll like and lets you make your own requests. Now, Spotify is teasing that generative AI advances may pave the way for an even more conversational interface with the streaming service in the future, according to comments made during Spotify’s second-quarter earnings call with investors on Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked about how AI was affecting its business, Spotify Chief Product and Technology Officer Gustav Söderström noted that consumers today are able to talk to Spotify in plain English to request music, allowing the company to associate which phrases connect with which songs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“You can think of it as us getting a new dataset,” he said. “Spotify’s got this unique dataset from all of its playlists, which was really song-to-song — like which song goes well with another song, kind of the Amazon ‘people who bought this also bought that,’” he said. Meanwhile, the voice interface is delivering new insights to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And that’s completely new to us, and it’s a very, very valuable dataset that we are collecting very quickly,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result, the exec says we can expect Spotify’s consumer experiences to get “much more interactive.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can already write to Spotify, talk to Spotify. You’re just going to see that expand,” Söderström said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, AI technology would allow Spotify to do more than just predict what someone might want to listen to, whether music, podcasts, or audiobooks, as it does today. Instead of only making predictions, it would be able to “reason” over the user’s listening history and what they said to the AI DJ. The reference to reasoning suggests that Spotify is looking toward AI reasoning models that can perform more complex tasks across multiple steps.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The voice data in question comes from Spotify’s AI DJ feature. In May 2025, Spotify introduced a way for Premium subscribers to make voice requests to the English-language AI DJ by pressing a button in the app. This lets users change the music, the genre, or the mood of a playlist using a voice command.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Consumer features aren’t the only way Spotify is leveraging AI. During the call, the exec also mentioned that Spotify is using generative AI internally to more rapidly prototype products and create efficiencies in other areas of its business, like finance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the quarter, Spotify hit 276 million paying subscribers, up 12% year-over-year, and 696 million monthly active users, but swung to a loss after missing revenue targets. The stock fell 10% on weak guidance and Spotify CEO Daniel Ek’s comments about his unhappiness with the company’s ads business.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/29/spotify-hints-at-a-more-chatty-voice-ai-interface-in-the-future/</guid><pubDate>Tue, 29 Jul 2025 16:58:53 +0000</pubDate></item><item><title>ChatGPT just got smarter: OpenAI’s Study Mode helps students learn step-by-step (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/chatgpt-just-got-smarter-openais-study-mode-helps-students-learn-step-by-step/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;OpenAI announced Study Mode for ChatGPT on Tuesday, a new feature that fundamentally changes how students interact with artificial intelligence by withholding direct answers in favor of Socratic questioning and step-by-step guidance.&lt;/p&gt;&lt;p&gt;The launch represents OpenAI’s most significant push into the education technology market, which analysts project will reach $80.5 billion by 2030. Rather than simply providing solutions to homework problems, Study Mode acts more like a patient tutor, asking follow-up questions and calibrating responses to individual skill levels.&lt;/p&gt;&lt;p&gt;“We set out to understand how students are using ChatGPT and how we might make it an even better tool for education,” said Leah Belsky, OpenAI’s VP of Education, during a press conference ahead of the launch. “Early research shows that how ChatGPT is used in learning makes a difference in the learning outcomes that it drives. When ChatGPT is prompted to teach or tutor, it can significantly improve academic performance. But when it’s just used as an answer machine, it can hinder learning.”&lt;/p&gt;&lt;p&gt;The feature addresses a fundamental tension that has emerged since ChatGPT’s explosive adoption among students. While one in three college-aged Americans now use the AI tool, with learning as the top use case, educators have grappled with whether such tools enhance understanding or encourage academic shortcuts.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-openai-s-study-mode-uses-socratic-method-to-replace-direct-answers"&gt;How OpenAI’s Study Mode Uses Socratic Method to Replace Direct Answers&lt;/h2&gt;



&lt;p&gt;Study Mode employs what OpenAI calls “custom system instructions” developed in collaboration with pedagogy experts from over 40 institutions worldwide. When students ask questions, the AI responds with guided prompts rather than direct answers.&lt;/p&gt;



&lt;p&gt;During a demonstration, Abhi Muchhal, an OpenAI product manager, showed how asking ChatGPT to “teach me about game theory” in regular mode produces a comprehensive, textbook-like response. In Study Mode, however, the AI instead asks: “What’s your current level? What are you optimizing for?” before providing tailored, bite-sized explanations.&lt;/p&gt;



&lt;p&gt;“We want this to be learner-led,” Muchhal explained. “At each step, there’s a question that is asking students to try to build on top. What we’re doing here is scaffolding learning and teaching one topic, asking a question, and building on top of that.”&lt;/p&gt;



&lt;p&gt;The system even resists students’ attempts to obtain quick answers. When prompted with “just give me the answer,” Study Mode responds that “the point of this is to learn, not just to give you the answer.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-college-students-report-dramatic-learning-confidence-boost-with-ai-tutoring"&gt;College Students Report Dramatic Learning Confidence Boost with AI Tutoring&lt;/h2&gt;



&lt;p&gt;Three college students who tested Study Mode early provided compelling testimonials about its impact on their learning confidence and outcomes.&lt;/p&gt;



&lt;p&gt;Maggie Wang, a Princeton computer science senior, described how the tool helped her finally understand sinusoidal positional encodings, a concept she had struggled with despite taking NLP courses and attending office hours.&lt;/p&gt;



&lt;p&gt;“I truly think that there’s nothing I can’t learn,” Wang said. “It’s given me a confidence that has absolutely changed my experience as a student. ChatGPT has really enabled me to think critically about being a researcher, reading papers, brainstorming research directions.”&lt;/p&gt;



&lt;p&gt;Praja Tickoo, a Wharton student studying economics, noted the stark difference between regular ChatGPT and Study Mode when reviewing accounting materials: “It felt like it really understood where to start… it made sure that I was ready to move on at each step. The biggest difference between regular ChatGPT and ChatGPT with study mode is kind of feels like a tool to me. ChatGPT with study mode felt like a learning partner.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ai-education-battle-heats-up-as-google-anthropic-race-to-capture-80-billion-market"&gt;AI Education Battle Heats Up as Google, Anthropic Race to Capture $80 Billion Market&lt;/h2&gt;



&lt;p&gt;The Study Mode launch comes as major AI companies race to capture the lucrative education market. Anthropic recently announced Claude for Education with its own “Learning Mode” that similarly emphasizes Socratic questioning over direct answers. Google has tested “Guided Learning for Gemini,” while making its $20 Gemini AI Pro subscription free for students.&lt;/p&gt;



&lt;p&gt;This competitive landscape reflects the sector’s recognition that educational applications represent both a massive market opportunity and a chance to demonstrate AI’s beneficial societal impact. Unlike consumer applications focused on convenience, educational AI tools must balance accessibility with pedagogical principles that promote genuine learning.&lt;/p&gt;



&lt;p&gt;“The research landscape is still taking shape on the best ways to apply AI in education,” OpenAI noted in its announcement, signaling that Study Mode represents an early experiment rather than a definitive solution.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-behind-the-scenes-how-openai-built-study-mode-and-what-comes-next"&gt;Behind the Scenes: How OpenAI Built Study Mode and What Comes Next&lt;/h2&gt;



&lt;p&gt;OpenAI built Study Mode using custom system instructions rather than training the behavior directly into its underlying models. This approach allows for rapid iteration based on student feedback, though it may result in some inconsistent behavior across conversations.&lt;/p&gt;



&lt;p&gt;The company plans to eventually integrate these behaviors directly into its main models once it has gathered sufficient data on what works best. Future enhancements under consideration include clearer visualizations for complex concepts, goal setting and progress tracking across conversations, and deeper personalization.&lt;/p&gt;



&lt;p&gt;Study Mode launched Tuesday for ChatGPT’s Free, Plus, Pro, and Team users, with availability for ChatGPT Edu coming in the following weeks. The company has not yet implemented admin-level controls that would allow educational institutions to mandate Study Mode usage, though Belsky indicated this is “definitely something that we’re seeing our early customers ask for.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-gpt-5-launch-and-ai-agent-breakthroughs-signal-new-era-for-educational-technology"&gt;GPT-5 Launch and AI Agent Breakthroughs Signal New Era for Educational Technology&lt;/h2&gt;



&lt;p&gt;The educational AI push comes amid rapid advancement in AI capabilities that both excite and concern educators. Last week, OpenAI’s ChatGPT Agent demonstrated it could pass through “I am not a robot” verification tests, highlighting how AI systems increasingly navigate digital environments designed to exclude them.&lt;/p&gt;



&lt;p&gt;Meanwhile, reports suggest OpenAI is preparing to launch GPT-5 in early August, which would unify the company’s reasoning and multi-modal capabilities into a single, more powerful model. Such advances raise the stakes for educational applications, as more capable AI could either enhance learning outcomes or make academic integrity challenges more acute.&lt;/p&gt;



&lt;p&gt;The timing also coincides with ongoing contract negotiations between Microsoft and OpenAI regarding access to future AI technologies, underscoring the commercial importance of maintaining competitive advantages in key markets like education.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-study-mode-means-for-the-future-of-ai-powered-learning"&gt;What Study Mode Means for the Future of AI-Powered Learning&lt;/h2&gt;



&lt;p&gt;OpenAI emphasized that Study Mode represents “a first step in a longer journey to improve learning in ChatGPT.” The company is collaborating with Stanford University’s SCALE Initiative to conduct longer-term studies on how students learn best with AI, with plans to publish findings on the links between model design and cognitive outcomes.&lt;/p&gt;



&lt;p&gt;For educational institutions weighing AI adoption, Study Mode offers a middle path between outright bans and unrestricted access. By building pedagogical principles directly into the AI interface, OpenAI has created a tool that could satisfy both educators’ concerns about learning integrity and students’ desire for AI assistance.&lt;/p&gt;



&lt;p&gt;As Caleb Masi, a University of Minnesota student in the testing program, noted: “We’re really just scratching the surface of what AI tools can do to support students. The more we lean into these tools thoughtfully as a community, the more empowered we can become, not just as students, but as lifelong learners.”&lt;/p&gt;



&lt;p&gt;The ultimate test won’t be whether AI can provide the right answers, but whether it can teach students to ask better questions.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;OpenAI announced Study Mode for ChatGPT on Tuesday, a new feature that fundamentally changes how students interact with artificial intelligence by withholding direct answers in favor of Socratic questioning and step-by-step guidance.&lt;/p&gt;&lt;p&gt;The launch represents OpenAI’s most significant push into the education technology market, which analysts project will reach $80.5 billion by 2030. Rather than simply providing solutions to homework problems, Study Mode acts more like a patient tutor, asking follow-up questions and calibrating responses to individual skill levels.&lt;/p&gt;&lt;p&gt;“We set out to understand how students are using ChatGPT and how we might make it an even better tool for education,” said Leah Belsky, OpenAI’s VP of Education, during a press conference ahead of the launch. “Early research shows that how ChatGPT is used in learning makes a difference in the learning outcomes that it drives. When ChatGPT is prompted to teach or tutor, it can significantly improve academic performance. But when it’s just used as an answer machine, it can hinder learning.”&lt;/p&gt;&lt;p&gt;The feature addresses a fundamental tension that has emerged since ChatGPT’s explosive adoption among students. While one in three college-aged Americans now use the AI tool, with learning as the top use case, educators have grappled with whether such tools enhance understanding or encourage academic shortcuts.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-openai-s-study-mode-uses-socratic-method-to-replace-direct-answers"&gt;How OpenAI’s Study Mode Uses Socratic Method to Replace Direct Answers&lt;/h2&gt;



&lt;p&gt;Study Mode employs what OpenAI calls “custom system instructions” developed in collaboration with pedagogy experts from over 40 institutions worldwide. When students ask questions, the AI responds with guided prompts rather than direct answers.&lt;/p&gt;



&lt;p&gt;During a demonstration, Abhi Muchhal, an OpenAI product manager, showed how asking ChatGPT to “teach me about game theory” in regular mode produces a comprehensive, textbook-like response. In Study Mode, however, the AI instead asks: “What’s your current level? What are you optimizing for?” before providing tailored, bite-sized explanations.&lt;/p&gt;



&lt;p&gt;“We want this to be learner-led,” Muchhal explained. “At each step, there’s a question that is asking students to try to build on top. What we’re doing here is scaffolding learning and teaching one topic, asking a question, and building on top of that.”&lt;/p&gt;



&lt;p&gt;The system even resists students’ attempts to obtain quick answers. When prompted with “just give me the answer,” Study Mode responds that “the point of this is to learn, not just to give you the answer.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-college-students-report-dramatic-learning-confidence-boost-with-ai-tutoring"&gt;College Students Report Dramatic Learning Confidence Boost with AI Tutoring&lt;/h2&gt;



&lt;p&gt;Three college students who tested Study Mode early provided compelling testimonials about its impact on their learning confidence and outcomes.&lt;/p&gt;



&lt;p&gt;Maggie Wang, a Princeton computer science senior, described how the tool helped her finally understand sinusoidal positional encodings, a concept she had struggled with despite taking NLP courses and attending office hours.&lt;/p&gt;



&lt;p&gt;“I truly think that there’s nothing I can’t learn,” Wang said. “It’s given me a confidence that has absolutely changed my experience as a student. ChatGPT has really enabled me to think critically about being a researcher, reading papers, brainstorming research directions.”&lt;/p&gt;



&lt;p&gt;Praja Tickoo, a Wharton student studying economics, noted the stark difference between regular ChatGPT and Study Mode when reviewing accounting materials: “It felt like it really understood where to start… it made sure that I was ready to move on at each step. The biggest difference between regular ChatGPT and ChatGPT with study mode is kind of feels like a tool to me. ChatGPT with study mode felt like a learning partner.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ai-education-battle-heats-up-as-google-anthropic-race-to-capture-80-billion-market"&gt;AI Education Battle Heats Up as Google, Anthropic Race to Capture $80 Billion Market&lt;/h2&gt;



&lt;p&gt;The Study Mode launch comes as major AI companies race to capture the lucrative education market. Anthropic recently announced Claude for Education with its own “Learning Mode” that similarly emphasizes Socratic questioning over direct answers. Google has tested “Guided Learning for Gemini,” while making its $20 Gemini AI Pro subscription free for students.&lt;/p&gt;



&lt;p&gt;This competitive landscape reflects the sector’s recognition that educational applications represent both a massive market opportunity and a chance to demonstrate AI’s beneficial societal impact. Unlike consumer applications focused on convenience, educational AI tools must balance accessibility with pedagogical principles that promote genuine learning.&lt;/p&gt;



&lt;p&gt;“The research landscape is still taking shape on the best ways to apply AI in education,” OpenAI noted in its announcement, signaling that Study Mode represents an early experiment rather than a definitive solution.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-behind-the-scenes-how-openai-built-study-mode-and-what-comes-next"&gt;Behind the Scenes: How OpenAI Built Study Mode and What Comes Next&lt;/h2&gt;



&lt;p&gt;OpenAI built Study Mode using custom system instructions rather than training the behavior directly into its underlying models. This approach allows for rapid iteration based on student feedback, though it may result in some inconsistent behavior across conversations.&lt;/p&gt;



&lt;p&gt;The company plans to eventually integrate these behaviors directly into its main models once it has gathered sufficient data on what works best. Future enhancements under consideration include clearer visualizations for complex concepts, goal setting and progress tracking across conversations, and deeper personalization.&lt;/p&gt;



&lt;p&gt;Study Mode launched Tuesday for ChatGPT’s Free, Plus, Pro, and Team users, with availability for ChatGPT Edu coming in the following weeks. The company has not yet implemented admin-level controls that would allow educational institutions to mandate Study Mode usage, though Belsky indicated this is “definitely something that we’re seeing our early customers ask for.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-gpt-5-launch-and-ai-agent-breakthroughs-signal-new-era-for-educational-technology"&gt;GPT-5 Launch and AI Agent Breakthroughs Signal New Era for Educational Technology&lt;/h2&gt;



&lt;p&gt;The educational AI push comes amid rapid advancement in AI capabilities that both excite and concern educators. Last week, OpenAI’s ChatGPT Agent demonstrated it could pass through “I am not a robot” verification tests, highlighting how AI systems increasingly navigate digital environments designed to exclude them.&lt;/p&gt;



&lt;p&gt;Meanwhile, reports suggest OpenAI is preparing to launch GPT-5 in early August, which would unify the company’s reasoning and multi-modal capabilities into a single, more powerful model. Such advances raise the stakes for educational applications, as more capable AI could either enhance learning outcomes or make academic integrity challenges more acute.&lt;/p&gt;



&lt;p&gt;The timing also coincides with ongoing contract negotiations between Microsoft and OpenAI regarding access to future AI technologies, underscoring the commercial importance of maintaining competitive advantages in key markets like education.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-study-mode-means-for-the-future-of-ai-powered-learning"&gt;What Study Mode Means for the Future of AI-Powered Learning&lt;/h2&gt;



&lt;p&gt;OpenAI emphasized that Study Mode represents “a first step in a longer journey to improve learning in ChatGPT.” The company is collaborating with Stanford University’s SCALE Initiative to conduct longer-term studies on how students learn best with AI, with plans to publish findings on the links between model design and cognitive outcomes.&lt;/p&gt;



&lt;p&gt;For educational institutions weighing AI adoption, Study Mode offers a middle path between outright bans and unrestricted access. By building pedagogical principles directly into the AI interface, OpenAI has created a tool that could satisfy both educators’ concerns about learning integrity and students’ desire for AI assistance.&lt;/p&gt;



&lt;p&gt;As Caleb Masi, a University of Minnesota student in the testing program, noted: “We’re really just scratching the surface of what AI tools can do to support students. The more we lean into these tools thoughtfully as a community, the more empowered we can become, not just as students, but as lifelong learners.”&lt;/p&gt;



&lt;p&gt;The ultimate test won’t be whether AI can provide the right answers, but whether it can teach students to ask better questions.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/chatgpt-just-got-smarter-openais-study-mode-helps-students-learn-step-by-step/</guid><pubDate>Tue, 29 Jul 2025 17:00:00 +0000</pubDate></item><item><title>OpenAI launches Study Mode in ChatGPT (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/29/openai-launches-study-mode-in-chatgpt/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/OpenAI-and-ChatGPT.jpeg?resize=1200,676" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Tuesday the launch of Study Mode, a new feature within ChatGPT&amp;nbsp;that aims to help students develop their own critical thinking skills, rather than simply obtain answers to questions.&amp;nbsp;With Study Mode enabled, ChatGPT will ask users questions to test their understanding and, in some cases, refuse to offer direct answers unless students engage with the material.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says Study Mode is rolling out to logged in users on ChatGPT’s Free, Plus, Pro, and Team plans starting Tuesday. The company expects to introduce Study Mode to its Edu subscribers, which largely consists of young people whose school administrator’s have purchased a plan for the entire student body, in the coming weeks.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Study Mode is OpenAI’s attempt to address the millions of students who use ChatGPT in school. Studies have shown that using ChatGPT can be a helpful tutor for young people, but it also may harm their critical thinking skills. A research paper released in June found that people who use ChatGPT to write essays exhibit lower brain activity during the process compared to those who use Google Search or nothing at all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When ChatGPT first launched in 2022, its widespread use in school settings sparked fear among educators, leading to generative AI bans in many American school districts. By 2023, some of those schools repealed their ChatGPT bans, and teachers around the country came to terms with the fact that ChatGPT would be a part of young people’s lives from now on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now with the launch of Study Mode, OpenAI hopes to improve ChatGPT as a learning tool, and not just an answer engine. Anthropic launched a similar tool for its AI chatbot Claude, called Learning Mode, in April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, there are limitations to how effective Study Mode truly is. Students can easily switch into the regular mode of ChatGPT if they just want an answer to a question. OpenAI’s VP of Education, Leah Belsky, told TechCrunch in a briefing that the company is not offering tools for parents or administrators to lock students into Study Mode. However, Belsky said OpenAI may explore administrative or parental controls in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That means it will take a committed student to use Study Mode — the kids have to really want to learn, not just finish their assignment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says Study Mode is the company’s first step to improving learning in ChatGPT and aims to publish more information in the future about how students use generative AI throughout their education.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/OpenAI-and-ChatGPT.jpeg?resize=1200,676" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Tuesday the launch of Study Mode, a new feature within ChatGPT&amp;nbsp;that aims to help students develop their own critical thinking skills, rather than simply obtain answers to questions.&amp;nbsp;With Study Mode enabled, ChatGPT will ask users questions to test their understanding and, in some cases, refuse to offer direct answers unless students engage with the material.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says Study Mode is rolling out to logged in users on ChatGPT’s Free, Plus, Pro, and Team plans starting Tuesday. The company expects to introduce Study Mode to its Edu subscribers, which largely consists of young people whose school administrator’s have purchased a plan for the entire student body, in the coming weeks.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Study Mode is OpenAI’s attempt to address the millions of students who use ChatGPT in school. Studies have shown that using ChatGPT can be a helpful tutor for young people, but it also may harm their critical thinking skills. A research paper released in June found that people who use ChatGPT to write essays exhibit lower brain activity during the process compared to those who use Google Search or nothing at all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When ChatGPT first launched in 2022, its widespread use in school settings sparked fear among educators, leading to generative AI bans in many American school districts. By 2023, some of those schools repealed their ChatGPT bans, and teachers around the country came to terms with the fact that ChatGPT would be a part of young people’s lives from now on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now with the launch of Study Mode, OpenAI hopes to improve ChatGPT as a learning tool, and not just an answer engine. Anthropic launched a similar tool for its AI chatbot Claude, called Learning Mode, in April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, there are limitations to how effective Study Mode truly is. Students can easily switch into the regular mode of ChatGPT if they just want an answer to a question. OpenAI’s VP of Education, Leah Belsky, told TechCrunch in a briefing that the company is not offering tools for parents or administrators to lock students into Study Mode. However, Belsky said OpenAI may explore administrative or parental controls in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That means it will take a committed student to use Study Mode — the kids have to really want to learn, not just finish their assignment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says Study Mode is the company’s first step to improving learning in ChatGPT and aims to publish more information in the future about how students use generative AI throughout their education.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/29/openai-launches-study-mode-in-chatgpt/</guid><pubDate>Tue, 29 Jul 2025 17:00:00 +0000</pubDate></item><item><title>Google’s NotebookLM rolls out Video Overviews (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/29/googles-notebooklm-rolls-out-video-overviews/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it’s rolling out Video Overviews to NotebookLM, its AI-based note-taking and research assistant. First introduced at Google I/O in May, Video Overviews allow users to&amp;nbsp;turn dense multimedia, such as raw notes, PDFs, and images, into digestible visual presentations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Previously, the service took an audio approach to helping users understand materials with Audio Overviews, a feature that gives users the ability to generate a podcast with AI virtual hosts based on documents they have shared with NotebookLM, such as course readings or legal briefs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With this new capability, NotebookLM is taking a more visual approach to helping users understand different topics and ideas. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says users can think of Video Overviews as a visual alternative to Audio Overviews. The feature creates new visuals while pulling in images, diagrams, quotes, and numbers from uploaded documents to explain the content. Google says the feature is good for explaining data, demonstrating processes, and making abstract concepts easier to understand. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3032032" height="381" src="https://techcrunch.com/wp-content/uploads/2025/07/Screenshot-2025-07-29-at-1.01.05PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Users can customize their Video Overviews, just as they do with Audio Overviews. They can specify topics to focus on, indicate their learning goals, describe the target audience, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They can ask simple questions like, “I know nothing about this topic; help me understand the diagrams in the paper,” or specific ones like, “I’m already an expert on X and my team works on Y; focus on Z.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Video Overviews are now rolling out to all users in English, with support for more languages coming soon, Google says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google also announced that it’s rolling out updates to NotebookLM’s Studio panel. Users can now&amp;nbsp;create and store multiple studio outputs of the same type in a single notebook. Plus, users will now see four tiles at the top of the Studio panel for creating Audio Overviews, Video Overviews, Mind Maps, and Reports with a single click. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, users can now multitask within the Studio panel. For example, they can listen to an Audio Overview while simultaneously exploring a Mind Map or reviewing a Study Guide.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it’s rolling out Video Overviews to NotebookLM, its AI-based note-taking and research assistant. First introduced at Google I/O in May, Video Overviews allow users to&amp;nbsp;turn dense multimedia, such as raw notes, PDFs, and images, into digestible visual presentations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Previously, the service took an audio approach to helping users understand materials with Audio Overviews, a feature that gives users the ability to generate a podcast with AI virtual hosts based on documents they have shared with NotebookLM, such as course readings or legal briefs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With this new capability, NotebookLM is taking a more visual approach to helping users understand different topics and ideas. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says users can think of Video Overviews as a visual alternative to Audio Overviews. The feature creates new visuals while pulling in images, diagrams, quotes, and numbers from uploaded documents to explain the content. Google says the feature is good for explaining data, demonstrating processes, and making abstract concepts easier to understand. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3032032" height="381" src="https://techcrunch.com/wp-content/uploads/2025/07/Screenshot-2025-07-29-at-1.01.05PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Users can customize their Video Overviews, just as they do with Audio Overviews. They can specify topics to focus on, indicate their learning goals, describe the target audience, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They can ask simple questions like, “I know nothing about this topic; help me understand the diagrams in the paper,” or specific ones like, “I’m already an expert on X and my team works on Y; focus on Z.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Video Overviews are now rolling out to all users in English, with support for more languages coming soon, Google says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google also announced that it’s rolling out updates to NotebookLM’s Studio panel. Users can now&amp;nbsp;create and store multiple studio outputs of the same type in a single notebook. Plus, users will now see four tiles at the top of the Studio panel for creating Audio Overviews, Video Overviews, Mind Maps, and Reports with a single click. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, users can now multitask within the Studio panel. For example, they can listen to an Audio Overview while simultaneously exploring a Mind Map or reviewing a Study Guide.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/29/googles-notebooklm-rolls-out-video-overviews/</guid><pubDate>Tue, 29 Jul 2025 17:11:20 +0000</pubDate></item><item><title>OpenAI is launching a version of ChatGPT for college students (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/29/1120801/openai-is-launching-a-version-of-chatgpt-for-college-students/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/StudyMode_Flow_7-29.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;OpenAI is launching Study Mode, a version of ChatGPT for college students that it promises will act less like a lookup tool and more like a friendly, always-available tutor. It’s part of a wider push by the company to get AI more embedded into classrooms when the new academic year starts in September.&lt;/p&gt;  &lt;p&gt;A demonstration for reporters from OpenAI showed what happens when a student asks Study Mode about an academic subject like game theory. The chatbot begins by asking what the student wants to know and then attempts to build an exchange, where the pair work methodically toward the answer together. OpenAI says the tool was built after consulting with pedagogy experts from over 40 institutions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;A handful of college students who were part of OpenAI’s testing cohort—hailing from Princeton, Wharton, and the University of Minnesota—shared positive reviews of Study Mode, saying it did a good job of checking their understanding and adapting to their pace.&lt;/p&gt;  &lt;p&gt;The learning approaches that OpenAI has programmed into Study Mode, which are based partially on Socratic methods, appear sound, says Christopher Harris, an educator in New York who has created a curriculum aimed at AI literacy. They might grant educators more confidence about allowing, or even encouraging, their students to use AI. “Professors will see this as working with them in support of learning as opposed to just being a way for students to cheat on assignments,” he says.&lt;/p&gt; 
 &lt;p&gt;But there’s a more ambitious vision behind Study Mode. As demonstrated in OpenAI’s recent partnership with leading teachers’ unions, the company is currently trying to rebrand chatbots as tools for personalized learning rather than cheating. Part of this promise is that AI will act like the expensive human tutors that currently only the most well-off students’ families can typically afford.&lt;/p&gt;  &lt;p&gt;“We can begin to close the gap between those with access to learning resources and high-quality education and those who have been historically left behind,” says OpenAI’s head of education. Leah Belsky.&lt;/p&gt; 
 &lt;p&gt;But painting Study Mode as an education equalizer obfuscates one glaring problem. Underneath the hood, it is not a tool trained exclusively on academic textbooks and other approved materials—it’s more like the same old ChatGPT, tuned with a new conversation filter that simply governs how it responds to students, encouraging fewer answers and more explanations.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This AI tutor, therefore, more resembles what you’d get if you hired a human tutor who has read every required textbook, but also every flawed explanation of the subject ever posted to Reddit, Tumblr, and the farthest reaches of the web. And because of the way AI works, you can’t expect it to distinguish right information from wrong.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Professors encouraging their students to use it run the risk of it teaching them to approach problems in the wrong way—or worse, being taught material that is fabricated or entirely false.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Given this limitation, I asked OpenAI if Study Mode is limited to particular subjects. The company said no—students will be able to use it to discuss anything they’d normally talk to ChatGPT about.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;It’s true that access to human tutors—which for certain subjects can cost upward of $200 an hour—is typically for the elite few. The notion that AI models can spread the benefits of tutoring to the masses holds an allure. Indeed, it is backed up by at least some early research that shows AI models can adapt to individual learning styles and backgrounds.&lt;/p&gt;  &lt;p&gt;But this improvement comes with a hidden cost. Tools like Study Mode, at least for now, take a shortcut by using large language models’ humanlike conversational style without fixing their inherent flaws.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI also acknowledges that this tool won’t prevent a student who’s frustrated and wants an answer from simply going back to normal ChatGPT. “If someone wants to subvert learning, and sort of get answers and take the easier route, that is possible,” Belsky says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, one thing going for Study Mode, the students say, is that it’s simply more fun to study with a chatbot that’s always encouraging you along than to stare at a textbook on Bayesian theorem for the hundredth time. “It’s like the reward signal of like, oh, wait, I can learn this small thing,” says Maggie Wang, a student from Princeton who tested it. The tool is free for now, but Praja Tickoo, a student from Wharton, says it wouldn’t have to be for him to use it. “I think it’s absolutely something I would be willing to pay for,” he says.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/StudyMode_Flow_7-29.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;OpenAI is launching Study Mode, a version of ChatGPT for college students that it promises will act less like a lookup tool and more like a friendly, always-available tutor. It’s part of a wider push by the company to get AI more embedded into classrooms when the new academic year starts in September.&lt;/p&gt;  &lt;p&gt;A demonstration for reporters from OpenAI showed what happens when a student asks Study Mode about an academic subject like game theory. The chatbot begins by asking what the student wants to know and then attempts to build an exchange, where the pair work methodically toward the answer together. OpenAI says the tool was built after consulting with pedagogy experts from over 40 institutions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;A handful of college students who were part of OpenAI’s testing cohort—hailing from Princeton, Wharton, and the University of Minnesota—shared positive reviews of Study Mode, saying it did a good job of checking their understanding and adapting to their pace.&lt;/p&gt;  &lt;p&gt;The learning approaches that OpenAI has programmed into Study Mode, which are based partially on Socratic methods, appear sound, says Christopher Harris, an educator in New York who has created a curriculum aimed at AI literacy. They might grant educators more confidence about allowing, or even encouraging, their students to use AI. “Professors will see this as working with them in support of learning as opposed to just being a way for students to cheat on assignments,” he says.&lt;/p&gt; 
 &lt;p&gt;But there’s a more ambitious vision behind Study Mode. As demonstrated in OpenAI’s recent partnership with leading teachers’ unions, the company is currently trying to rebrand chatbots as tools for personalized learning rather than cheating. Part of this promise is that AI will act like the expensive human tutors that currently only the most well-off students’ families can typically afford.&lt;/p&gt;  &lt;p&gt;“We can begin to close the gap between those with access to learning resources and high-quality education and those who have been historically left behind,” says OpenAI’s head of education. Leah Belsky.&lt;/p&gt; 
 &lt;p&gt;But painting Study Mode as an education equalizer obfuscates one glaring problem. Underneath the hood, it is not a tool trained exclusively on academic textbooks and other approved materials—it’s more like the same old ChatGPT, tuned with a new conversation filter that simply governs how it responds to students, encouraging fewer answers and more explanations.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This AI tutor, therefore, more resembles what you’d get if you hired a human tutor who has read every required textbook, but also every flawed explanation of the subject ever posted to Reddit, Tumblr, and the farthest reaches of the web. And because of the way AI works, you can’t expect it to distinguish right information from wrong.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Professors encouraging their students to use it run the risk of it teaching them to approach problems in the wrong way—or worse, being taught material that is fabricated or entirely false.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Given this limitation, I asked OpenAI if Study Mode is limited to particular subjects. The company said no—students will be able to use it to discuss anything they’d normally talk to ChatGPT about.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;It’s true that access to human tutors—which for certain subjects can cost upward of $200 an hour—is typically for the elite few. The notion that AI models can spread the benefits of tutoring to the masses holds an allure. Indeed, it is backed up by at least some early research that shows AI models can adapt to individual learning styles and backgrounds.&lt;/p&gt;  &lt;p&gt;But this improvement comes with a hidden cost. Tools like Study Mode, at least for now, take a shortcut by using large language models’ humanlike conversational style without fixing their inherent flaws.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI also acknowledges that this tool won’t prevent a student who’s frustrated and wants an answer from simply going back to normal ChatGPT. “If someone wants to subvert learning, and sort of get answers and take the easier route, that is possible,” Belsky says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, one thing going for Study Mode, the students say, is that it’s simply more fun to study with a chatbot that’s always encouraging you along than to stare at a textbook on Bayesian theorem for the hundredth time. “It’s like the reward signal of like, oh, wait, I can learn this small thing,” says Maggie Wang, a student from Princeton who tested it. The tool is free for now, but Praja Tickoo, a student from Wharton, says it wouldn’t have to be for him to use it. “I think it’s absolutely something I would be willing to pay for,” he says.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/29/1120801/openai-is-launching-a-version-of-chatgpt-for-college-students/</guid><pubDate>Tue, 29 Jul 2025 17:18:45 +0000</pubDate></item><item><title>Exclusive: A record-breaking baby has been born from an embryo that’s over 30 years old (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/29/1120769/exclusive-record-breaking-baby-born-embryo-over-30-years-old/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;A baby boy born over the weekend holds the new record for the “oldest baby.” Thaddeus Daniel Pierce, who arrived on July 26, developed from an embryo that had been in storage for 30 and a half years.&lt;/p&gt;  &lt;p&gt;“We had a rough birth but we are both doing well now,” says Lindsey Pierce, his mother.&amp;nbsp;"He is so chill. We are in awe that we have this precious baby!"&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Lindsey and her husband, Tim Pierce, who live in London, Ohio, “adopted” the embryo from a woman who had it created in 1994.&amp;nbsp;She says her family and church family think “it’s like something from a sci-fi movie.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The baby has a 30-year-old sister,” she adds. Tim was a toddler when the embryos were first created.&lt;/p&gt; 
 &lt;p&gt;“It’s been pretty surreal,” says Linda Archerd, 62, who donated the embryo. “It’s hard to even believe.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Three little hopes&lt;/h3&gt;  &lt;p&gt;The story starts back in the early 1990s. Archerd had been trying—and failing—to get pregnant for six years. She and her husband decided to try IVF, a fairly new technology at the time. “People were [unfamiliar] with it,” says Archerd. “A lot of people were like, what are you doing?”&lt;/p&gt; 
 &lt;p&gt;They did it anyway, and in May 1994, they managed to create four embryos. One of them was transferred to Linda’s uterus. It resulted in a healthy baby girl. “I was so blessed to have a baby,” Archerd says. The remaining three embryos were cryopreserved and kept in a storage tank.&lt;/p&gt;  &lt;p&gt;That was 31 years ago. The healthy baby girl is now a 30-year-old woman who has her own 10-year-old daughter. But the other three embryos remained frozen in time.&lt;/p&gt;  &lt;p&gt;Archerd originally planned to use the embryos herself. “I always wanted another baby desperately,” she says. “I called them my three little hopes.” Her then husband felt differently, she says. Archerd went on to divorce him, but she won custody of the embryos and kept them in storage, still hopeful she might use them one day, perhaps with another partner.&lt;/p&gt;  &lt;p&gt;That meant paying annual storage fees, which increased over time and ended up costing Archerd around a thousand dollars a year, she says. To her, it was worth it. “I always thought it was the right thing to do,” she says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Things changed when she started going through menopause, she says. She considered her options. She didn’t want to discard the embryos or donate them for research. And she didn’t want to donate them to another family anonymously—she wanted to meet the parents and any resulting babies. “It’s my DNA; it came from me … and [it’s] my daughter’s sibling,” she says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Then she found out about embryo “adoption.” This is a type of embryo donation in which both donors and recipients have a say in whom they “place” their embryos with or “adopt” them from. It is overseen by agencies—usually explicitly religious ones—that believe an embryo is morally equivalent to a born human. Archerd is Christian.&lt;/p&gt;  &lt;p&gt;There are several agencies that offer these adoption services in the US, but not all of them accept embryos that have been stored for a very long time. That’s partly because those embryos will have been frozen and stored in unfamiliar, old-fashioned ways, and partly because old embryos are thought to be less likely to survive thawing and transfer to successfully develop into a baby.&lt;/p&gt;  &lt;p&gt;“So many places wouldn’t even take my information,” says Archerd. Then she came across the Snowflakes program run by the Nightlight Christian Adoptions agency. The agency was willing to accept her embryos, but it needed Archerd’s medical records from the time the embryos had been created, as well as the embryos’ lab records.&lt;/p&gt; 

 &lt;p&gt;So Archerd called the fertility doctor who had treated her decades before. “I still remembered his phone number by heart,” she says. That doctor, now in his 70s, is still practicing at a clinic in Oregon. He dug Archerd’s records out from his basement, she says. “Some of [them] were handwritten,” she adds. Her embryos entered Nightlight’s “matching pool” in 2022.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Making a match&lt;/h3&gt;  &lt;p&gt;“Our matching process is really driven by the preferences of the placing family,” says Beth Button, executive director of the Snowflakes program. Archerd’s preference was for a married Caucasian, Christian couple living in the US. “I didn’t want them to go out of the country,” says Archerd. “And being Christian is very important to me, because I am.”&lt;/p&gt;  &lt;p&gt;It took a while to find a match. Most of the “adopting parents” signed up for the Snowflakes program were already registered at fertility clinics that wouldn’t have accepted the embryos, says Button. “I would say that over 90% of clinics in the US would not have accepted these embryos,” she says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Expecting parents Tim and Lindsey Pierce." class="wp-image-1120633" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/250724_recordbaby_parents2.jpg?w=1727" width="1727" /&gt;&lt;figcaption class="wp-element-caption"&gt;Lindsey and Tim Pierce at Rejoice Fertility.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY LINDSEY PIERCE&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Archerd’s embryos were assigned to the agency’s Open Hearts program for embryos that are “hard to place,” along with others that have been in storage for a long time or are otherwise thought to be less likely to result in a healthy birth.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Lindsey and Tim Pierce had also signed up for the Open Hearts program. The couple, aged 35 and 34, respectively, had been trying for a baby for seven years and had seen multiple doctors.&lt;/p&gt;  &lt;p&gt;Lindsey was researching child adoption when she came across the Snowflakes program.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When the couple were considering their criteria for embryos they might receive, they decided that they’d be open to any. “We checkmarked anything and everything,” says Tim. That’s how they ended up being matched with Archerd’s embryos. “We thought it was wild,” says Lindsey. “We didn’t know they froze embryos that long ago.”&lt;/p&gt;  &lt;p&gt;Lindsey and Tim had registered with Rejoice Fertility, an IVF clinic in Knoxville, Tennessee, run by John Gordon, a reproductive endocrinologist who prides himself on his efforts to reduce the number of embryos in storage. The huge numbers of embryos left in storage tanks was weighing on his conscience, he says, so around six years ago, he set up Rejoice Fertility with the aim of doing things differently.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“Now we’re here in the belt buckle of the Bible Belt,” says Gordon, who is Reformed Presbyterian. “I’ve changed my mode of practice.” IVF treatments performed at the clinic are designed to create as few excess embryos as possible. The clinic works with multiple embryo adoption agencies and will accept any embryo, no matter how long it has been in storage.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A portrait of Linda Archerd." class="wp-image-1120605" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/250724_recordbaby_donor.jpg?w=1500" width="1500" /&gt;&lt;div class="image-credit"&gt;COURTESY LINDA ARCHERD&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;It was his clinic that treated the parents who previously held the record for the longest-stored embryo—in 2022, Rachel and Philip Ridgeway had twins from embryos created more than 30 years earlier. “They’re such a lovely couple,” says Gordon. When we spoke, he was making plans to meet the family for breakfast. The twins are “growing like weeds,” he says with a laugh.&lt;/p&gt; 
 &lt;p&gt;“We have certain guiding principles, and they’re coming from our faith,” says Gordon, although he adds that he sees patients who hold alternative views. One of those principles is that “every embryo deserves a chance at life and that the only embryo that cannot result in a healthy baby is the embryo not given the opportunity to be transferred into a patient.”&lt;/p&gt;  &lt;p&gt;That’s why his team will endeavor to transfer any embryo they receive, no matter the age or conditions. That can be challenging, especially when the embryos have been frozen or stored in unusual or outdated ways. “It’s scary for people who don’t know how to do it,” says Sarah Atkinson, lab supervisor and head embryologist at Rejoice Fertility. “You don’t want to kill someone’s embryos if you don’t know what you’re doing.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;h3 class="wp-block-heading"&gt;Cumbersome and explosive&lt;/h3&gt;  &lt;p&gt;In the early days of IVF, embryos earmarked for storage were slow-frozen. This technique involves gradually lowering the temperature of the embryos. But because slow freezing can cause harmful ice crystals to form, clinics switched in the 2000s to a technique called vitrification, in which the embryos are placed in thin plastic tubes called straws and lowered into tanks of liquid nitrogen. This rapidly freezes the embryos and converts them into a glass-like state.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The embryos can later be thawed by removing them from the tanks and rapidly—within two seconds—plunging them into warm “thaw media,” says Atkinson. Thawing slow-frozen embryos is more complicated. And the exact thawing method required varies, depending on how the embryos were preserved and what they were stored in. Some of the devices need to be opened while they are inside the storage tank, which can involve using forceps, diamond-bladed knives, and other tools in the liquid nitrogen, says Atkinson.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/McElyea-ICSI-1.mp4"&gt;&lt;/video&gt;&lt;figcaption class="wp-element-caption"&gt;Sarah Atkinson, lab supervisor and head embryologist at Rejoice Fertility, directly injects sperm into two eggs to fertilize them.&lt;/figcaption&gt;&lt;div class="video-credit"&gt;COURTESY OF SARAH ATKINSON AT REJOICE FERTILITY.&lt;/div&gt; &lt;/figure&gt;  &lt;p&gt;Recently, she was tasked with retrieving embryos that had been stored inside a glass vial. The vial was made from blown glass and had been heat-sealed with the embryo inside. Atkinson had to use her diamond-bladed knife to snap open the seal inside the nitrogen tank. It was fiddly work, and when the device snapped, a small shard of glass flew out and hit Atkinson’s face. “Hit me on the cheek, cut my cheek, blood running down my face, and I’m like, &lt;em&gt;Oh shit,&lt;/em&gt;” she says. Luckily, she had her safety goggles on. And the embryos survived, she adds.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1120770" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/Pierce-Lindsey.jpg" /&gt;&lt;figcaption class="wp-element-caption"&gt;The two embryos that were transferred to Lindsey Pierce.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;p&gt;Atkinson has a folder in her office with notes she’s collected on various devices over the years. She flicks through it over a video call and points to the notes she made about the glass vial. “Might explode; wear face shield and eye protection,” she reads. A few pages later, she points to another embryo-storage device. “You have to thaw this one in your fingers,” she tells me. “I don’t like it.”&lt;/p&gt; 
 &lt;p&gt;The record-breaking embryos had been slow-frozen and stored in a plastic vial, says Atkinson. Thawing them was a cumbersome process. But all three embryos survived it.&lt;/p&gt;  &lt;p&gt;The Pierces had to travel from their home in Ohio to the clinic in Tennessee five times over a two-week period. “It was like a five-hour drive,” says Lindsey. One of the three embryos stopped growing. The other two were transferred to Lindsey’s uterus on November 14, she says. And one developed into a fetus.&lt;/p&gt;  &lt;p&gt;Now that the baby has arrived, Archerd is keen to meet him. “The first thing that I noticed when Lindsey sent me his pictures is how much he looks like my daughter when she was a baby,” she says.&amp;nbsp;“I pulled out my baby book and compared them side by side, and there is no doubt that they are siblings.”&lt;/p&gt;  &lt;p&gt;She doesn’t yet have plans to meet the baby, but doing so would be “a dream come true,” she says. “I wish that they didn’t live so far away from me ... He is perfect!”&lt;/p&gt;  &lt;p&gt;“We didn’t go into it thinking we would break any records,” says Lindsey. “We just wanted to have a baby.”&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1120800" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/1000004661.jpg?w=480" /&gt; &lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;A baby boy born over the weekend holds the new record for the “oldest baby.” Thaddeus Daniel Pierce, who arrived on July 26, developed from an embryo that had been in storage for 30 and a half years.&lt;/p&gt;  &lt;p&gt;“We had a rough birth but we are both doing well now,” says Lindsey Pierce, his mother.&amp;nbsp;"He is so chill. We are in awe that we have this precious baby!"&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Lindsey and her husband, Tim Pierce, who live in London, Ohio, “adopted” the embryo from a woman who had it created in 1994.&amp;nbsp;She says her family and church family think “it’s like something from a sci-fi movie.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The baby has a 30-year-old sister,” she adds. Tim was a toddler when the embryos were first created.&lt;/p&gt; 
 &lt;p&gt;“It’s been pretty surreal,” says Linda Archerd, 62, who donated the embryo. “It’s hard to even believe.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Three little hopes&lt;/h3&gt;  &lt;p&gt;The story starts back in the early 1990s. Archerd had been trying—and failing—to get pregnant for six years. She and her husband decided to try IVF, a fairly new technology at the time. “People were [unfamiliar] with it,” says Archerd. “A lot of people were like, what are you doing?”&lt;/p&gt; 
 &lt;p&gt;They did it anyway, and in May 1994, they managed to create four embryos. One of them was transferred to Linda’s uterus. It resulted in a healthy baby girl. “I was so blessed to have a baby,” Archerd says. The remaining three embryos were cryopreserved and kept in a storage tank.&lt;/p&gt;  &lt;p&gt;That was 31 years ago. The healthy baby girl is now a 30-year-old woman who has her own 10-year-old daughter. But the other three embryos remained frozen in time.&lt;/p&gt;  &lt;p&gt;Archerd originally planned to use the embryos herself. “I always wanted another baby desperately,” she says. “I called them my three little hopes.” Her then husband felt differently, she says. Archerd went on to divorce him, but she won custody of the embryos and kept them in storage, still hopeful she might use them one day, perhaps with another partner.&lt;/p&gt;  &lt;p&gt;That meant paying annual storage fees, which increased over time and ended up costing Archerd around a thousand dollars a year, she says. To her, it was worth it. “I always thought it was the right thing to do,” she says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Things changed when she started going through menopause, she says. She considered her options. She didn’t want to discard the embryos or donate them for research. And she didn’t want to donate them to another family anonymously—she wanted to meet the parents and any resulting babies. “It’s my DNA; it came from me … and [it’s] my daughter’s sibling,” she says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Then she found out about embryo “adoption.” This is a type of embryo donation in which both donors and recipients have a say in whom they “place” their embryos with or “adopt” them from. It is overseen by agencies—usually explicitly religious ones—that believe an embryo is morally equivalent to a born human. Archerd is Christian.&lt;/p&gt;  &lt;p&gt;There are several agencies that offer these adoption services in the US, but not all of them accept embryos that have been stored for a very long time. That’s partly because those embryos will have been frozen and stored in unfamiliar, old-fashioned ways, and partly because old embryos are thought to be less likely to survive thawing and transfer to successfully develop into a baby.&lt;/p&gt;  &lt;p&gt;“So many places wouldn’t even take my information,” says Archerd. Then she came across the Snowflakes program run by the Nightlight Christian Adoptions agency. The agency was willing to accept her embryos, but it needed Archerd’s medical records from the time the embryos had been created, as well as the embryos’ lab records.&lt;/p&gt; 

 &lt;p&gt;So Archerd called the fertility doctor who had treated her decades before. “I still remembered his phone number by heart,” she says. That doctor, now in his 70s, is still practicing at a clinic in Oregon. He dug Archerd’s records out from his basement, she says. “Some of [them] were handwritten,” she adds. Her embryos entered Nightlight’s “matching pool” in 2022.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Making a match&lt;/h3&gt;  &lt;p&gt;“Our matching process is really driven by the preferences of the placing family,” says Beth Button, executive director of the Snowflakes program. Archerd’s preference was for a married Caucasian, Christian couple living in the US. “I didn’t want them to go out of the country,” says Archerd. “And being Christian is very important to me, because I am.”&lt;/p&gt;  &lt;p&gt;It took a while to find a match. Most of the “adopting parents” signed up for the Snowflakes program were already registered at fertility clinics that wouldn’t have accepted the embryos, says Button. “I would say that over 90% of clinics in the US would not have accepted these embryos,” she says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Expecting parents Tim and Lindsey Pierce." class="wp-image-1120633" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/250724_recordbaby_parents2.jpg?w=1727" width="1727" /&gt;&lt;figcaption class="wp-element-caption"&gt;Lindsey and Tim Pierce at Rejoice Fertility.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY LINDSEY PIERCE&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Archerd’s embryos were assigned to the agency’s Open Hearts program for embryos that are “hard to place,” along with others that have been in storage for a long time or are otherwise thought to be less likely to result in a healthy birth.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Lindsey and Tim Pierce had also signed up for the Open Hearts program. The couple, aged 35 and 34, respectively, had been trying for a baby for seven years and had seen multiple doctors.&lt;/p&gt;  &lt;p&gt;Lindsey was researching child adoption when she came across the Snowflakes program.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When the couple were considering their criteria for embryos they might receive, they decided that they’d be open to any. “We checkmarked anything and everything,” says Tim. That’s how they ended up being matched with Archerd’s embryos. “We thought it was wild,” says Lindsey. “We didn’t know they froze embryos that long ago.”&lt;/p&gt;  &lt;p&gt;Lindsey and Tim had registered with Rejoice Fertility, an IVF clinic in Knoxville, Tennessee, run by John Gordon, a reproductive endocrinologist who prides himself on his efforts to reduce the number of embryos in storage. The huge numbers of embryos left in storage tanks was weighing on his conscience, he says, so around six years ago, he set up Rejoice Fertility with the aim of doing things differently.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“Now we’re here in the belt buckle of the Bible Belt,” says Gordon, who is Reformed Presbyterian. “I’ve changed my mode of practice.” IVF treatments performed at the clinic are designed to create as few excess embryos as possible. The clinic works with multiple embryo adoption agencies and will accept any embryo, no matter how long it has been in storage.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A portrait of Linda Archerd." class="wp-image-1120605" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/250724_recordbaby_donor.jpg?w=1500" width="1500" /&gt;&lt;div class="image-credit"&gt;COURTESY LINDA ARCHERD&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;It was his clinic that treated the parents who previously held the record for the longest-stored embryo—in 2022, Rachel and Philip Ridgeway had twins from embryos created more than 30 years earlier. “They’re such a lovely couple,” says Gordon. When we spoke, he was making plans to meet the family for breakfast. The twins are “growing like weeds,” he says with a laugh.&lt;/p&gt; 
 &lt;p&gt;“We have certain guiding principles, and they’re coming from our faith,” says Gordon, although he adds that he sees patients who hold alternative views. One of those principles is that “every embryo deserves a chance at life and that the only embryo that cannot result in a healthy baby is the embryo not given the opportunity to be transferred into a patient.”&lt;/p&gt;  &lt;p&gt;That’s why his team will endeavor to transfer any embryo they receive, no matter the age or conditions. That can be challenging, especially when the embryos have been frozen or stored in unusual or outdated ways. “It’s scary for people who don’t know how to do it,” says Sarah Atkinson, lab supervisor and head embryologist at Rejoice Fertility. “You don’t want to kill someone’s embryos if you don’t know what you’re doing.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;h3 class="wp-block-heading"&gt;Cumbersome and explosive&lt;/h3&gt;  &lt;p&gt;In the early days of IVF, embryos earmarked for storage were slow-frozen. This technique involves gradually lowering the temperature of the embryos. But because slow freezing can cause harmful ice crystals to form, clinics switched in the 2000s to a technique called vitrification, in which the embryos are placed in thin plastic tubes called straws and lowered into tanks of liquid nitrogen. This rapidly freezes the embryos and converts them into a glass-like state.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The embryos can later be thawed by removing them from the tanks and rapidly—within two seconds—plunging them into warm “thaw media,” says Atkinson. Thawing slow-frozen embryos is more complicated. And the exact thawing method required varies, depending on how the embryos were preserved and what they were stored in. Some of the devices need to be opened while they are inside the storage tank, which can involve using forceps, diamond-bladed knives, and other tools in the liquid nitrogen, says Atkinson.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/McElyea-ICSI-1.mp4"&gt;&lt;/video&gt;&lt;figcaption class="wp-element-caption"&gt;Sarah Atkinson, lab supervisor and head embryologist at Rejoice Fertility, directly injects sperm into two eggs to fertilize them.&lt;/figcaption&gt;&lt;div class="video-credit"&gt;COURTESY OF SARAH ATKINSON AT REJOICE FERTILITY.&lt;/div&gt; &lt;/figure&gt;  &lt;p&gt;Recently, she was tasked with retrieving embryos that had been stored inside a glass vial. The vial was made from blown glass and had been heat-sealed with the embryo inside. Atkinson had to use her diamond-bladed knife to snap open the seal inside the nitrogen tank. It was fiddly work, and when the device snapped, a small shard of glass flew out and hit Atkinson’s face. “Hit me on the cheek, cut my cheek, blood running down my face, and I’m like, &lt;em&gt;Oh shit,&lt;/em&gt;” she says. Luckily, she had her safety goggles on. And the embryos survived, she adds.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1120770" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/Pierce-Lindsey.jpg" /&gt;&lt;figcaption class="wp-element-caption"&gt;The two embryos that were transferred to Lindsey Pierce.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;p&gt;Atkinson has a folder in her office with notes she’s collected on various devices over the years. She flicks through it over a video call and points to the notes she made about the glass vial. “Might explode; wear face shield and eye protection,” she reads. A few pages later, she points to another embryo-storage device. “You have to thaw this one in your fingers,” she tells me. “I don’t like it.”&lt;/p&gt; 
 &lt;p&gt;The record-breaking embryos had been slow-frozen and stored in a plastic vial, says Atkinson. Thawing them was a cumbersome process. But all three embryos survived it.&lt;/p&gt;  &lt;p&gt;The Pierces had to travel from their home in Ohio to the clinic in Tennessee five times over a two-week period. “It was like a five-hour drive,” says Lindsey. One of the three embryos stopped growing. The other two were transferred to Lindsey’s uterus on November 14, she says. And one developed into a fetus.&lt;/p&gt;  &lt;p&gt;Now that the baby has arrived, Archerd is keen to meet him. “The first thing that I noticed when Lindsey sent me his pictures is how much he looks like my daughter when she was a baby,” she says.&amp;nbsp;“I pulled out my baby book and compared them side by side, and there is no doubt that they are siblings.”&lt;/p&gt;  &lt;p&gt;She doesn’t yet have plans to meet the baby, but doing so would be “a dream come true,” she says. “I wish that they didn’t live so far away from me ... He is perfect!”&lt;/p&gt;  &lt;p&gt;“We didn’t go into it thinking we would break any records,” says Lindsey. “We just wanted to have a baby.”&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1120800" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/1000004661.jpg?w=480" /&gt; &lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/29/1120769/exclusive-record-breaking-baby-born-embryo-over-30-years-old/</guid><pubDate>Tue, 29 Jul 2025 17:36:56 +0000</pubDate></item><item><title>ChatGPT’s new Study Mode is designed to help you learn, not just give answers (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/chatgpts-new-study-mode-is-designed-to-help-you-learn-not-just-give-answers/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New set of system prompts promote understanding with guided questions, "Socratic dialogue."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="448" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2169079907-640x448.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2169079907-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Oh, I know the answer Mr. Robot!

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The rise of large language models like ChatGPT has led to widespread concern that "everyone is cheating their way through college," as a recent New York magazine article memorably put it. Now, OpenAI is rolling out a new "Study Mode" that it claims is less about providing answers or doing the work for students and more about helping them "build [a] deep understanding" of complex topics.&lt;/p&gt;
&lt;p&gt;Study Mode isn't a new ChatGPT model but a series of "custom system instructions" written for the LLM "in collaboration with teachers, scientists, and pedagogy experts to reflect a core set of behaviors that support deeper learning," OpenAI said. Instead of the usual summary of a subject that stock ChatGPT might give—which one OpenAI employee likened to "a mini textbook chapter"—Study Mode slowly rolls out new information in a "scaffolded" structure. The mode is designed to ask "guiding questions" in the Socratic style and to pause for periodic "knowledge checks" and personalized feedback to make sure the user understands before moving on.&lt;/p&gt;
&lt;p&gt;It's unknown how many students will use this guided learning tool instead of just asking ChatGPT to generate answers from the start.&lt;/p&gt;
&lt;p&gt;In an early hands-off demo attended by Ars Technica, Study Mode responded to a request to "teach me about game theory" by first asking about the user's overall familiarity with the subject and what they'll be using the information for. ChatGPT introduced a short overview of some core game theory concepts, then paused to ask a question before providing a relevant real-world example.&lt;/p&gt;
&lt;p&gt;In another example involving a classic "train traveling at speed" math problem, Study Mode resisted multiple simulated attempts by the frustrated "student" to simply ask for the answer and instead tried to gently redirect the conversation to how the available information could be used to generate that answer. An OpenAI representative told Ars that Study Mode will eventually provide direct solutions if asked repeatedly, but the default behavior is more tuned to a Socratic tutoring style.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A 24/7 tutorbot&lt;/h2&gt;
&lt;p&gt;In building Study Mode, OpenAI said it was inspired by "power users" who were already trying to adapt ChatGPT into a personal tutor or test prep tool via specific sets of prompts. Through Study Mode, OpenAI says it hopes to give less technically minded users the ability to access "a personal tutor that never gets tired of their questions."&lt;/p&gt;
&lt;p&gt;OpenAI says it worked with pedagogy experts who evaluated the mode's behaviors and gave the model "golden examples" of how ideal tutors would respond in certain situations. It also consulted with groups of college students who were given advance access to the new feature.&lt;/p&gt;
&lt;p&gt;In a press conference attended by Ars, some of those students waxed rhapsodical about Study Mode's ability to push them toward the next important bit of knowledge and the confidence they gained in their ability to learn when using the tool. One student talked about the vulnerability and embarrassment of having to go to a TA's office hours for help and the flexibility of having multi-hour tutoring sessions available via ChatGPT at any time of the day.&lt;/p&gt;
&lt;h2&gt;Can you trust it?&lt;/h2&gt;
&lt;p&gt;People who are familiar with LLMs' long-standing tendency to confabulate completely false information might be reluctant to use these kinds of models as a study aid. In a press release, OpenAI allowed that the current Study Mode prompt "results in some inconsistent behavior and mistakes across conversations."&lt;/p&gt;
&lt;p&gt;That said, a company spokesperson told Ars that the risk of hallucination was lower with Study Mode because the model goes through information in smaller chunks, calibrating along the way.&lt;/p&gt;
&lt;p&gt;While Study Mode wasn't designed explicitly to address concerns about ChatGPT being used to cheat on assignments, a spokesperson told Ars that it could help allay educators' fears that students are simply using LLMs to get out of doing their own work. Study Mode will be added to its ChatGPT Edu product "in a few weeks" for subscribing schools that want to offer a different kind of AI experience to students.&lt;/p&gt;
&lt;p&gt;For now, though, OpenAI says the specialized Study Mode system prompts are a "first step" to training similar behaviors "directly into our main models" in the future.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New set of system prompts promote understanding with guided questions, "Socratic dialogue."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="448" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2169079907-640x448.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2169079907-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Oh, I know the answer Mr. Robot!

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The rise of large language models like ChatGPT has led to widespread concern that "everyone is cheating their way through college," as a recent New York magazine article memorably put it. Now, OpenAI is rolling out a new "Study Mode" that it claims is less about providing answers or doing the work for students and more about helping them "build [a] deep understanding" of complex topics.&lt;/p&gt;
&lt;p&gt;Study Mode isn't a new ChatGPT model but a series of "custom system instructions" written for the LLM "in collaboration with teachers, scientists, and pedagogy experts to reflect a core set of behaviors that support deeper learning," OpenAI said. Instead of the usual summary of a subject that stock ChatGPT might give—which one OpenAI employee likened to "a mini textbook chapter"—Study Mode slowly rolls out new information in a "scaffolded" structure. The mode is designed to ask "guiding questions" in the Socratic style and to pause for periodic "knowledge checks" and personalized feedback to make sure the user understands before moving on.&lt;/p&gt;
&lt;p&gt;It's unknown how many students will use this guided learning tool instead of just asking ChatGPT to generate answers from the start.&lt;/p&gt;
&lt;p&gt;In an early hands-off demo attended by Ars Technica, Study Mode responded to a request to "teach me about game theory" by first asking about the user's overall familiarity with the subject and what they'll be using the information for. ChatGPT introduced a short overview of some core game theory concepts, then paused to ask a question before providing a relevant real-world example.&lt;/p&gt;
&lt;p&gt;In another example involving a classic "train traveling at speed" math problem, Study Mode resisted multiple simulated attempts by the frustrated "student" to simply ask for the answer and instead tried to gently redirect the conversation to how the available information could be used to generate that answer. An OpenAI representative told Ars that Study Mode will eventually provide direct solutions if asked repeatedly, but the default behavior is more tuned to a Socratic tutoring style.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A 24/7 tutorbot&lt;/h2&gt;
&lt;p&gt;In building Study Mode, OpenAI said it was inspired by "power users" who were already trying to adapt ChatGPT into a personal tutor or test prep tool via specific sets of prompts. Through Study Mode, OpenAI says it hopes to give less technically minded users the ability to access "a personal tutor that never gets tired of their questions."&lt;/p&gt;
&lt;p&gt;OpenAI says it worked with pedagogy experts who evaluated the mode's behaviors and gave the model "golden examples" of how ideal tutors would respond in certain situations. It also consulted with groups of college students who were given advance access to the new feature.&lt;/p&gt;
&lt;p&gt;In a press conference attended by Ars, some of those students waxed rhapsodical about Study Mode's ability to push them toward the next important bit of knowledge and the confidence they gained in their ability to learn when using the tool. One student talked about the vulnerability and embarrassment of having to go to a TA's office hours for help and the flexibility of having multi-hour tutoring sessions available via ChatGPT at any time of the day.&lt;/p&gt;
&lt;h2&gt;Can you trust it?&lt;/h2&gt;
&lt;p&gt;People who are familiar with LLMs' long-standing tendency to confabulate completely false information might be reluctant to use these kinds of models as a study aid. In a press release, OpenAI allowed that the current Study Mode prompt "results in some inconsistent behavior and mistakes across conversations."&lt;/p&gt;
&lt;p&gt;That said, a company spokesperson told Ars that the risk of hallucination was lower with Study Mode because the model goes through information in smaller chunks, calibrating along the way.&lt;/p&gt;
&lt;p&gt;While Study Mode wasn't designed explicitly to address concerns about ChatGPT being used to cheat on assignments, a spokesperson told Ars that it could help allay educators' fears that students are simply using LLMs to get out of doing their own work. Study Mode will be added to its ChatGPT Edu product "in a few weeks" for subscribing schools that want to offer a different kind of AI experience to students.&lt;/p&gt;
&lt;p&gt;For now, though, OpenAI says the specialized Study Mode system prompts are a "first step" to training similar behaviors "directly into our main models" in the future.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/chatgpts-new-study-mode-is-designed-to-help-you-learn-not-just-give-answers/</guid><pubDate>Tue, 29 Jul 2025 18:00:34 +0000</pubDate></item><item><title>[NEW] Positron believes it has found the secret to take on Nvidia in AI inference chips — here’s how it could benefit enterprises (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/positron-believes-it-has-found-the-secret-to-take-on-nvidia-in-ai-inference-chips-heres-how-it-could-benefit-enterprises/</link><description>&lt;p&gt;As demand for large-scale AI deployment skyrockets, the lesser-known,&lt;strong&gt; private chip startup Positron is positioning itself as a direct challenger to market leader Nvidia &lt;/strong&gt;by offering dedicated, energy-efficient, memory-optimized inference chips aimed at relieving the industry’s mounting cost, power, and availability bottlenecks. &lt;/p&gt;&lt;p&gt;&lt;strong&gt;“A key differentiator is our ability to run frontier AI models with better efficiency—achieving 2x to 5x performance per watt and dollar compared to Nvidia,” said Thomas Sohmers, Positron co-founder and CTO,&lt;/strong&gt; in a recent video call interview with VentureBeat.&lt;/p&gt;&lt;p&gt;Obviously, that’s good news for big AI model providers, but Positron’s leadership contends it is helpful for many more enterprises beyond, including those using AI models in their workflows, not as service offerings to customers.&lt;/p&gt;&lt;p&gt;“We build chips that can be deployed in hundreds of existing data centers because they don’t require liquid cooling or extreme power densities,” &lt;strong&gt;pointed out Mitesh Agrawal, Positron’s CEO and the former chief operating officer of AI cloud inference provider Lambda&lt;/strong&gt;, also in the same video call interview with VentureBeat. &lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Venture capitalists and early users seem to agree.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Positron yesterday announced an oversubscribed $51.6 million Series A funding round&lt;/strong&gt;&amp;nbsp;led by Valor Equity Partners, Atreides Management and DFJ Growth, with support from Flume Ventures, Resilience Reserve, 1517 Fund and Unless.&lt;/p&gt;



&lt;p&gt;As for Positron’s early customer base, that includes both name-brand enterprises and companies operating in inference-heavy sectors. Confirmed deployments include the major security and cloud content networking provider &lt;strong&gt;Cloudflare&lt;/strong&gt;, which uses Positron’s Atlas hardware in its globally distributed, power-constrained data centers, and &lt;strong&gt;Parasail&lt;/strong&gt;, via its AI-native data infrastructure platform SnapServe. &lt;/p&gt;



&lt;p&gt;Beyond these, Positron reports adoption across several key verticals where efficient inference is critical, such as &lt;strong&gt;networking, gaming, content moderation, content delivery networks (CDNs), and Token-as-a-Service providers&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;These early users are reportedly drawn in by Atlas’s ability to deliver high throughput and lower power consumption without requiring specialized cooling or reworked infrastructure, making it an attractive drop-in option for AI workloads across enterprise environments.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-entering-a-challenging-market-that-is-decreasing-ai-model-size-and-increasing-efficiency"&gt;Entering a challenging market that is decreasing AI model size and increasing efficiency&lt;/h2&gt;



&lt;p&gt;But Positron is also entering a challenging market. &lt;em&gt;The Information&lt;/em&gt; just reported that r&lt;strong&gt;ival buzzy AI inference chip startup Groq&lt;/strong&gt; — &lt;strong&gt;where Sohmers previously worked as Director of Technology Strategy&lt;/strong&gt; — has reduced its 2025 revenue projection from $2 billion+ to $500 million,  highlighting just how volatile the AI hardware space can be. &lt;/p&gt;



&lt;p&gt;Even well-funded firms face headwinds as they compete for data center capacity and enterprise mindshare against entrenched GPU providers like Nvidia, not to mention the elephant in the room: the rise of more efficient, smaller large language models (LLMs) and specialized small language models (SLMs) that can even run on devices as small and low-powered as smartphones.&lt;/p&gt;



&lt;p&gt;Yet Positron’s leadership is for now embracing the trend and shrugging off the possible impacts on its growth trajectory.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;“There’s always been this duality—lightweight applications on local devices and heavyweight processing in centralized infrastructure,” said Agrawal. “We believe both will keep growing.”&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Sohmers agreed, stating: “We see a future where every person might have a capable model on their phone, but those will still rely on large models in data centers to generate deeper insights.”&lt;/strong&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-atlas-is-an-inference-first-ai-chip"&gt;Atlas is an inference-first AI chip&lt;/h2&gt;



&lt;p&gt;While Nvidia GPUs helped catalyze the deep learning boom by accelerating model training, Positron argues that inference — the stage where models generate output in production — is now the true bottleneck. &lt;/p&gt;



&lt;p&gt;Its founders call it the most under-optimized part of the “AI stack,” especially for generative AI workloads that depend on fast, efficient model serving.&lt;/p&gt;



&lt;p&gt;Positron’s solution is &lt;strong&gt;Atlas, its first-generation inference accelerator built specifically to handle large transformer models. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Unlike general-purpose GPUs, Atlas is optimized for the unique memory and throughput needs of modern inference tasks. &lt;/p&gt;



&lt;p&gt;The company claims Atlas delivers 3.5x better performance per dollar and up to 66% lower power usage than Nvidia’s H100, while also achieving 93% memory bandwidth utilization—far above the typical 10–30% range seen in GPUs.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-from-atlas-to-titan-supporting-multi-trillion-parameter-models"&gt;From Atlas to Titan, supporting multi-trillion parameter models&lt;/h2&gt;



&lt;p&gt;Launched just 15 months after founding — and with only $12.5 million in seed capital — Atlas is already shipping and in production.&lt;/p&gt;



&lt;p&gt; The system supports up to 0.5 trillion-parameter models in a single 2kW server and is compatible with Hugging Face transformer models via an OpenAI API-compatible endpoint.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Positron is now preparing to launch its next-generation platform, Titan, in 2026. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Built on custom-designed “Asimov” silicon, &lt;strong&gt;Titan will feature up to two terabytes of high-speed memory per accelerator and support models up to 16 trillion parameters&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;Today’s frontier models are in the hundred billions and single digit trillions of parameters, but newer models like OpenAI’s GPT-5 are presumed to be in the multi-trillions, and larger models are currently thought to be required to reach artificial general intelligence (AGI), AI that outperforms humans on most economically valuable work, and superintelligence, AI that exceeds the ability for humans to understand and control. &lt;/p&gt;



&lt;p&gt;Crucially, Titan is designed to operate with standard air cooling in conventional data center environments, avoiding the high-density, liquid-cooled configurations that next-gen GPUs increasingly require.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-engineering-for-efficiency-and-compatibility"&gt;Engineering for efficiency and compatibility&lt;/h2&gt;



&lt;p&gt;From the start, Positron designed its system to be a drop-in replacement, allowing customers to use existing model binaries without code rewrites. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;“If a customer had to change their behavior or their actions in any way, shape or form, that was a barrier,” said Sohmers.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Sohmers explained that instead of building a complex compiler stack or rearchitecting software ecosystems, Positron focused narrowly on inference, designing hardware that ingests Nvidia-trained models directly.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;“CUDA mode isn’t something to fight,” said Agrawal. “It’s an ecosystem to participate in.”&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This pragmatic approach helped the company ship its first product quickly, validate performance with real enterprise users, and secure significant follow-on investment. In addition, its focus on air cooling versus liquid cooling makes its Atlas chips the only option for some deployments. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;“We’re focused entirely on purely air-cooled deployments… all these Nvidia Hopper- and Blackwell-based solutions going forward are required liquid cooling&lt;/strong&gt;… The only place you can put those racks are in data centers that are being newly built now in the middle of nowhere,” said Sohmers.&lt;/p&gt;



&lt;p&gt;All told, Positron’s ability to execute quickly and capital-efficiently has helped distinguish it in a crowded AI hardware market.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-memory-is-what-you-need"&gt;Memory is what you need&lt;/h2&gt;



&lt;p&gt;Sohmers and Agrawal point to a fundamental shift in AI workloads: from compute-bound convolutional neural networks to memory-bound transformer architectures. &lt;/p&gt;



&lt;p&gt;Whereas older models demanded high FLOPs (floating-point operations), modern transformers require massive memory capacity and bandwidth to run efficiently.&lt;/p&gt;



&lt;p&gt;While Nvidia and others continue to focus on compute scaling, Positron is betting on memory-first design. &lt;/p&gt;



&lt;p&gt;Sohmers noted that with transformer inference, the ratio of compute to memory operations flips to near 1:1, meaning that boosting memory utilization has a direct and dramatic impact on performance and power efficiency.&lt;/p&gt;



&lt;p&gt;With Atlas already outperforming contemporary GPUs on key efficiency metrics, Titan aims to take this further by offering the highest memory capacity per chip in the industry. &lt;/p&gt;



&lt;p&gt;At launch, Titan is expected to offer an order-of-magnitude increase over typical GPU memory configurations — without demanding specialized cooling or boutique networking setups.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-u-s-built-chips"&gt;U.S.-built chips&lt;/h2&gt;



&lt;p&gt;Positron’s production pipeline is proudly domestic. The company’s first-generation chips were fabricated in the U.S. using Intel facilities, with final server assembly and integration also based domestically. &lt;/p&gt;



&lt;p&gt;For the Asimov chip, fabrication will shift to TSMC, though the team is aiming to keep as much of the rest of the production chain in the U.S. as possible, depending on foundry capacity.&lt;/p&gt;



&lt;p&gt;Geopolitical resilience and supply chain stability are becoming key purchasing criteria for many customers — another reason Positron believes its U.S.-made hardware offers a compelling alternative.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next"&gt;What’s next?&lt;/h2&gt;



&lt;p&gt;Agrawal noted that Positron’s silicon targets not just broad compatibility but maximum utility for enterprise, cloud, and research labs alike. &lt;/p&gt;



&lt;p&gt;While the company has not named any frontier model providers as customers yet, he confirmed that outreach and conversations are underway.&lt;/p&gt;



&lt;p&gt;Agrawal emphasized that selling physical infrastructure based on economics and performance—not bundling it with proprietary APIs or business models—is part of what gives Positron credibility in a skeptical market.&lt;/p&gt;



&lt;p&gt;“If you can’t convince a customer to deploy your hardware based on its economics, you’re not going to be profitable,” he said.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;As demand for large-scale AI deployment skyrockets, the lesser-known,&lt;strong&gt; private chip startup Positron is positioning itself as a direct challenger to market leader Nvidia &lt;/strong&gt;by offering dedicated, energy-efficient, memory-optimized inference chips aimed at relieving the industry’s mounting cost, power, and availability bottlenecks. &lt;/p&gt;&lt;p&gt;&lt;strong&gt;“A key differentiator is our ability to run frontier AI models with better efficiency—achieving 2x to 5x performance per watt and dollar compared to Nvidia,” said Thomas Sohmers, Positron co-founder and CTO,&lt;/strong&gt; in a recent video call interview with VentureBeat.&lt;/p&gt;&lt;p&gt;Obviously, that’s good news for big AI model providers, but Positron’s leadership contends it is helpful for many more enterprises beyond, including those using AI models in their workflows, not as service offerings to customers.&lt;/p&gt;&lt;p&gt;“We build chips that can be deployed in hundreds of existing data centers because they don’t require liquid cooling or extreme power densities,” &lt;strong&gt;pointed out Mitesh Agrawal, Positron’s CEO and the former chief operating officer of AI cloud inference provider Lambda&lt;/strong&gt;, also in the same video call interview with VentureBeat. &lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Venture capitalists and early users seem to agree.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Positron yesterday announced an oversubscribed $51.6 million Series A funding round&lt;/strong&gt;&amp;nbsp;led by Valor Equity Partners, Atreides Management and DFJ Growth, with support from Flume Ventures, Resilience Reserve, 1517 Fund and Unless.&lt;/p&gt;



&lt;p&gt;As for Positron’s early customer base, that includes both name-brand enterprises and companies operating in inference-heavy sectors. Confirmed deployments include the major security and cloud content networking provider &lt;strong&gt;Cloudflare&lt;/strong&gt;, which uses Positron’s Atlas hardware in its globally distributed, power-constrained data centers, and &lt;strong&gt;Parasail&lt;/strong&gt;, via its AI-native data infrastructure platform SnapServe. &lt;/p&gt;



&lt;p&gt;Beyond these, Positron reports adoption across several key verticals where efficient inference is critical, such as &lt;strong&gt;networking, gaming, content moderation, content delivery networks (CDNs), and Token-as-a-Service providers&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;These early users are reportedly drawn in by Atlas’s ability to deliver high throughput and lower power consumption without requiring specialized cooling or reworked infrastructure, making it an attractive drop-in option for AI workloads across enterprise environments.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-entering-a-challenging-market-that-is-decreasing-ai-model-size-and-increasing-efficiency"&gt;Entering a challenging market that is decreasing AI model size and increasing efficiency&lt;/h2&gt;



&lt;p&gt;But Positron is also entering a challenging market. &lt;em&gt;The Information&lt;/em&gt; just reported that r&lt;strong&gt;ival buzzy AI inference chip startup Groq&lt;/strong&gt; — &lt;strong&gt;where Sohmers previously worked as Director of Technology Strategy&lt;/strong&gt; — has reduced its 2025 revenue projection from $2 billion+ to $500 million,  highlighting just how volatile the AI hardware space can be. &lt;/p&gt;



&lt;p&gt;Even well-funded firms face headwinds as they compete for data center capacity and enterprise mindshare against entrenched GPU providers like Nvidia, not to mention the elephant in the room: the rise of more efficient, smaller large language models (LLMs) and specialized small language models (SLMs) that can even run on devices as small and low-powered as smartphones.&lt;/p&gt;



&lt;p&gt;Yet Positron’s leadership is for now embracing the trend and shrugging off the possible impacts on its growth trajectory.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;“There’s always been this duality—lightweight applications on local devices and heavyweight processing in centralized infrastructure,” said Agrawal. “We believe both will keep growing.”&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Sohmers agreed, stating: “We see a future where every person might have a capable model on their phone, but those will still rely on large models in data centers to generate deeper insights.”&lt;/strong&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-atlas-is-an-inference-first-ai-chip"&gt;Atlas is an inference-first AI chip&lt;/h2&gt;



&lt;p&gt;While Nvidia GPUs helped catalyze the deep learning boom by accelerating model training, Positron argues that inference — the stage where models generate output in production — is now the true bottleneck. &lt;/p&gt;



&lt;p&gt;Its founders call it the most under-optimized part of the “AI stack,” especially for generative AI workloads that depend on fast, efficient model serving.&lt;/p&gt;



&lt;p&gt;Positron’s solution is &lt;strong&gt;Atlas, its first-generation inference accelerator built specifically to handle large transformer models. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Unlike general-purpose GPUs, Atlas is optimized for the unique memory and throughput needs of modern inference tasks. &lt;/p&gt;



&lt;p&gt;The company claims Atlas delivers 3.5x better performance per dollar and up to 66% lower power usage than Nvidia’s H100, while also achieving 93% memory bandwidth utilization—far above the typical 10–30% range seen in GPUs.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-from-atlas-to-titan-supporting-multi-trillion-parameter-models"&gt;From Atlas to Titan, supporting multi-trillion parameter models&lt;/h2&gt;



&lt;p&gt;Launched just 15 months after founding — and with only $12.5 million in seed capital — Atlas is already shipping and in production.&lt;/p&gt;



&lt;p&gt; The system supports up to 0.5 trillion-parameter models in a single 2kW server and is compatible with Hugging Face transformer models via an OpenAI API-compatible endpoint.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Positron is now preparing to launch its next-generation platform, Titan, in 2026. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Built on custom-designed “Asimov” silicon, &lt;strong&gt;Titan will feature up to two terabytes of high-speed memory per accelerator and support models up to 16 trillion parameters&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;Today’s frontier models are in the hundred billions and single digit trillions of parameters, but newer models like OpenAI’s GPT-5 are presumed to be in the multi-trillions, and larger models are currently thought to be required to reach artificial general intelligence (AGI), AI that outperforms humans on most economically valuable work, and superintelligence, AI that exceeds the ability for humans to understand and control. &lt;/p&gt;



&lt;p&gt;Crucially, Titan is designed to operate with standard air cooling in conventional data center environments, avoiding the high-density, liquid-cooled configurations that next-gen GPUs increasingly require.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-engineering-for-efficiency-and-compatibility"&gt;Engineering for efficiency and compatibility&lt;/h2&gt;



&lt;p&gt;From the start, Positron designed its system to be a drop-in replacement, allowing customers to use existing model binaries without code rewrites. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;“If a customer had to change their behavior or their actions in any way, shape or form, that was a barrier,” said Sohmers.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Sohmers explained that instead of building a complex compiler stack or rearchitecting software ecosystems, Positron focused narrowly on inference, designing hardware that ingests Nvidia-trained models directly.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;“CUDA mode isn’t something to fight,” said Agrawal. “It’s an ecosystem to participate in.”&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This pragmatic approach helped the company ship its first product quickly, validate performance with real enterprise users, and secure significant follow-on investment. In addition, its focus on air cooling versus liquid cooling makes its Atlas chips the only option for some deployments. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;“We’re focused entirely on purely air-cooled deployments… all these Nvidia Hopper- and Blackwell-based solutions going forward are required liquid cooling&lt;/strong&gt;… The only place you can put those racks are in data centers that are being newly built now in the middle of nowhere,” said Sohmers.&lt;/p&gt;



&lt;p&gt;All told, Positron’s ability to execute quickly and capital-efficiently has helped distinguish it in a crowded AI hardware market.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-memory-is-what-you-need"&gt;Memory is what you need&lt;/h2&gt;



&lt;p&gt;Sohmers and Agrawal point to a fundamental shift in AI workloads: from compute-bound convolutional neural networks to memory-bound transformer architectures. &lt;/p&gt;



&lt;p&gt;Whereas older models demanded high FLOPs (floating-point operations), modern transformers require massive memory capacity and bandwidth to run efficiently.&lt;/p&gt;



&lt;p&gt;While Nvidia and others continue to focus on compute scaling, Positron is betting on memory-first design. &lt;/p&gt;



&lt;p&gt;Sohmers noted that with transformer inference, the ratio of compute to memory operations flips to near 1:1, meaning that boosting memory utilization has a direct and dramatic impact on performance and power efficiency.&lt;/p&gt;



&lt;p&gt;With Atlas already outperforming contemporary GPUs on key efficiency metrics, Titan aims to take this further by offering the highest memory capacity per chip in the industry. &lt;/p&gt;



&lt;p&gt;At launch, Titan is expected to offer an order-of-magnitude increase over typical GPU memory configurations — without demanding specialized cooling or boutique networking setups.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-u-s-built-chips"&gt;U.S.-built chips&lt;/h2&gt;



&lt;p&gt;Positron’s production pipeline is proudly domestic. The company’s first-generation chips were fabricated in the U.S. using Intel facilities, with final server assembly and integration also based domestically. &lt;/p&gt;



&lt;p&gt;For the Asimov chip, fabrication will shift to TSMC, though the team is aiming to keep as much of the rest of the production chain in the U.S. as possible, depending on foundry capacity.&lt;/p&gt;



&lt;p&gt;Geopolitical resilience and supply chain stability are becoming key purchasing criteria for many customers — another reason Positron believes its U.S.-made hardware offers a compelling alternative.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next"&gt;What’s next?&lt;/h2&gt;



&lt;p&gt;Agrawal noted that Positron’s silicon targets not just broad compatibility but maximum utility for enterprise, cloud, and research labs alike. &lt;/p&gt;



&lt;p&gt;While the company has not named any frontier model providers as customers yet, he confirmed that outreach and conversations are underway.&lt;/p&gt;



&lt;p&gt;Agrawal emphasized that selling physical infrastructure based on economics and performance—not bundling it with proprietary APIs or business models—is part of what gives Positron credibility in a skeptical market.&lt;/p&gt;



&lt;p&gt;“If you can’t convince a customer to deploy your hardware based on its economics, you’re not going to be profitable,” he said.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/positron-believes-it-has-found-the-secret-to-take-on-nvidia-in-ai-inference-chips-heres-how-it-could-benefit-enterprises/</guid><pubDate>Tue, 29 Jul 2025 18:45:40 +0000</pubDate></item><item><title>[NEW] Anthropic reportedly nears $170B valuation with potential $5B round (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/29/anthropic-reportedly-nears-170b-valuation-with-potential-5b-round/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2153561878.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is nearing a deal to raise between $3 billion and $5 billion in funding, valuing the large language model developer at $170 billion, Bloomberg reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Iconiq Capital is leading this funding round, but there’s a possibility of a second lead investor joining the deal. The company has also been in talks with Qatar Investment Authority and GIC, Singapore’s sovereign wealth fund, according to the report.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If finalized, the deal would nearly triple Anthropic’s valuation, which was $61.5 billion after a $3.5 billion funding round led by Lightspeed Venture Partners announced in March. Other participants in the startup’s last round included Bessemer Venture Partners, Cisco Investments, D1 Capital Partners, Fidelity Management &amp;amp; Research Company, General Catalyst, Jane Street, Menlo Ventures, and Salesforce Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite Anthropic’s mission as a safety-conscious AI model developer, the company’s CEO, Dario Amodei, recently confessed in a memo to employees, reported by Wired, that he’s “not thrilled” about taking money from sovereign wealth funds of dictatorial governments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To keep pace with the massive capital requirements of developing AI models, the company has been forced to turn to Middle Eastern capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Unfortunately, I think ‘No bad person should ever benefit from our success’ is a pretty difficult principle to run a business on,” Amodei wrote in a leaked memo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic didn’t immediately respond to a request for comment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2153561878.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is nearing a deal to raise between $3 billion and $5 billion in funding, valuing the large language model developer at $170 billion, Bloomberg reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Iconiq Capital is leading this funding round, but there’s a possibility of a second lead investor joining the deal. The company has also been in talks with Qatar Investment Authority and GIC, Singapore’s sovereign wealth fund, according to the report.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If finalized, the deal would nearly triple Anthropic’s valuation, which was $61.5 billion after a $3.5 billion funding round led by Lightspeed Venture Partners announced in March. Other participants in the startup’s last round included Bessemer Venture Partners, Cisco Investments, D1 Capital Partners, Fidelity Management &amp;amp; Research Company, General Catalyst, Jane Street, Menlo Ventures, and Salesforce Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite Anthropic’s mission as a safety-conscious AI model developer, the company’s CEO, Dario Amodei, recently confessed in a memo to employees, reported by Wired, that he’s “not thrilled” about taking money from sovereign wealth funds of dictatorial governments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To keep pace with the massive capital requirements of developing AI models, the company has been forced to turn to Middle Eastern capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Unfortunately, I think ‘No bad person should ever benefit from our success’ is a pretty difficult principle to run a business on,” Amodei wrote in a leaked memo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic didn’t immediately respond to a request for comment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/29/anthropic-reportedly-nears-170b-valuation-with-potential-5b-round/</guid><pubDate>Tue, 29 Jul 2025 18:59:36 +0000</pubDate></item><item><title>[NEW] Luma and Runway expect robotics to eventually be a big revenue driver for them (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/29/luma-and-runway-expect-robotics-to-eventually-be-a-big-revenue-driver-for-them/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/06/GettyImages-1474076387.jpg?resize=1200,768" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI video-generating startups Luma and Runway are looking beyond movie studios.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These video-generating AI companies have their sights set on other markets for future revenue streams and have been in talks with both robotics and self-driving car companies, according to reporting from The Information. The report didn’t identify which companies Luma and Runway are in talks with.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This potential revenue stream makes sense for Luma in particular. The company announced it was looking to build 3D AI world models in early 2024 with the goal of having these models understand how to see and interact with the world around them, TechCrunch reported at the time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Runway is also targeting video games as a potential future revenue stream, too, according to The Information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Luma and Runway for more information.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/06/GettyImages-1474076387.jpg?resize=1200,768" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI video-generating startups Luma and Runway are looking beyond movie studios.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These video-generating AI companies have their sights set on other markets for future revenue streams and have been in talks with both robotics and self-driving car companies, according to reporting from The Information. The report didn’t identify which companies Luma and Runway are in talks with.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This potential revenue stream makes sense for Luma in particular. The company announced it was looking to build 3D AI world models in early 2024 with the goal of having these models understand how to see and interact with the world around them, TechCrunch reported at the time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Runway is also targeting video games as a potential future revenue stream, too, according to The Information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Luma and Runway for more information.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/29/luma-and-runway-expect-robotics-to-eventually-be-a-big-revenue-driver-for-them/</guid><pubDate>Tue, 29 Jul 2025 20:12:43 +0000</pubDate></item><item><title>[NEW] “FUTURE PHASES” showcases new frontiers in music technology and interactive performance (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/future-phases-showcase-new-frontiers-music-technology-interactive-performance-0729</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/MIT-MTA-6-13-25-335_0.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Music technology took center stage at MIT during “FUTURE PHASES,” an evening of works for string orchestra and electronics, presented by the&amp;nbsp;MIT Music Technology and Computation Graduate Program as part of the 2025 International Computer Music Conference (ICMC).&amp;nbsp;&lt;/p&gt;&lt;p&gt;The well-attended event was held last month in the Thomas Tull Concert Hall within the new Edward and Joyce Linde Music Building. Produced in collaboration with the MIT Media Lab’s Opera of the Future Group and Boston’s self-conducted chamber orchestra A Far Cry, “FUTURE PHASES” was the first event to be presented by the MIT Music Technology and Computation Graduate Program in MIT Music’s new space.&lt;/p&gt;&lt;p&gt;“FUTURE PHASES” offerings included two new works by MIT composers: the world premiere of&amp;nbsp;“EV6,” by MIT Music’s Kenan Sahin Distinguished Professor Evan Ziporyn and professor of the practice Eran Egozy; and the U.S. premiere of&amp;nbsp;“FLOW Symphony,” by the MIT Media Lab’s Muriel R. Cooper Professor of Music and Media Tod Machover. Three additional works were selected by a jury from&amp;nbsp;an open call for works:&amp;nbsp;“The Wind Will Carry Us Away,” by Ali Balighi; “A Blank Page,” by Celeste Betancur&amp;nbsp;Gutiérrez and Luna Valentin; and “Coastal Portrait: Cycles and Thresholds,”&amp;nbsp;by Peter Lane. Each work was performed by&amp;nbsp;Boston’s own&amp;nbsp;multi-Grammy-nominated string orchestra, A Far Cry.&lt;/p&gt;&lt;p&gt;“The ICMC is all about presenting the latest research, compositions, and performances in electronic music,” says Egozy,&amp;nbsp;director of the new Music Technology and Computation Graduate Program at MIT. When approached to be a part of this year’s conference, “it seemed the perfect opportunity to showcase MIT’s commitment&amp;nbsp;to music technology, and in particular the exciting new areas being developed right now: a new master’s program in music technology and computation, the new Edward and Joyce Linde Music Building with its enhanced music technology facilities, and new faculty arriving at MIT with joint appointments between MIT Music and Theater Arts (MTA) and the Department of Electrical Engineering and Computer Science (EECS).” These recently hired professors include Anna Huang, a keynote speaker for the conference and creator of the machine learning model&amp;nbsp;Coconet that powered Google’s first AI Doodle, the Bach Doodle.&lt;/p&gt;&lt;p&gt;Egozy emphasizes the uniqueness of this occasion: “You have to understand that this is a very special situation. Having a full 18-member string orchestra [A Far Cry] perform new works that include electronics does not happen very often. In most cases, ICMC performances consist either entirely of electronics and computer-generated music, or perhaps a small ensemble of two-to-four musicians. So the opportunity we could present to the larger community of music technology was particularly exciting.”&lt;/p&gt;&lt;p&gt;To take advantage of this exciting opportunity, an open call was put out internationally to select the other pieces that would accompany Ziporyn and Egozy’s “EV6” and Machover’s “FLOW Symphony.” Three pieces were selected from a total of 46 entries to be a part of the evening’s program by a panel of judges that included Egozy, Machover, and other distinguished composers and technologists.&lt;/p&gt;&lt;p&gt;“We received a huge variety of works from this call,” says Egozy. “We saw all kinds of musical styles and ways that electronics would be used. No two pieces were very similar to each other, and I think because of that, our audience got a sense of how varied and interesting a concert can be for this format. A Far Cry was really the unifying presence. They played all pieces with great passion and nuance. They have a way of really drawing audiences into the music. And, of course, with the Thomas Tull Concert Hall being in the round, the audience felt even more connected to the music.”&lt;/p&gt;&lt;p&gt;Egozy continues, “we took advantage of the technology built into the Thomas Tull Concert Hall, which has 24 built-in speakers for surround sound allowing us to broadcast unique, amplified sound to every seat in the house. Chances are that every person might have experienced the sound slightly differently, but there was always some sense of a multidimensional evolution of sound and music as the pieces unfolded.”&lt;/p&gt;&lt;p&gt;The five works of the evening employed a range of technological components that included playing synthesized, prerecorded, or electronically manipulated sounds; attaching microphones to instruments for use in real-time signal processing algorithms; broadcasting custom-generated musical notation to&amp;nbsp;the musicians; utilizing generative AI to process live sound and play it back in interesting and unpredictable ways; and audience participation, where spectators use their cellphones as musical instruments to become a part of the ensemble.&lt;/p&gt;&lt;p&gt;Ziporyn and Egozy’s piece, “EV6&lt;em&gt;,”&lt;/em&gt; took particular advantage of this last innovation: “Evan and I had previously collaborated on a system called&amp;nbsp;Tutti, which means ‘together’ in Italian. Tutti gives an audience the ability to use their smartphones as musical instruments so that we can all play together.” Egozy developed the technology, which was first used in the MIT Campaign for a Better World in 2017. The original application involved a three-minute piece for cellphones only. “But for this concert,” Egozy explains, “Evan had the idea that we could use the same technology to write a new piece — this time, for audience phones and a live string orchestra as well.”&lt;/p&gt;&lt;p&gt;To explain the piece’s title, Ziporyn says, “I drive an EV6; it’s my first electric car, and when I first got it, it felt like I was driving an iPhone. But of course it’s still just a car: it’s got wheels and an engine, and it gets me from one place to another. It seemed like a good metaphor for this piece, in which a lot of the sound is literally played on cellphones, but still has to work like any other piece of music. It’s also a bit of an homage to David Bowie’s song ‘TVC 15,’ which is about falling in love with a robot.”&lt;/p&gt;&lt;p&gt;Egozy adds, “We wanted audience members to feel what it is like to play together in an orchestra. Through this technology, each audience member becomes a part of an orchestral section (winds, brass, strings, etc.). As they play together, they can hear their whole section playing similar music while also hearing other sections in different parts of the hall play different music. This allows an audience to feel a responsibility to their section, hear how music can move between different sections of an orchestra, and experience the thrill of live performance. In ‘EV6,’ this experience was even more electrifying because everyone in the audience got to play with a live string orchestra — perhaps for the first time in recorded history.”&lt;/p&gt;&lt;p&gt;After the concert, guests were treated to six music technology demonstrations that showcased the research of undergraduate and graduate students from both the MIT Music program and the MIT Media Lab. These included a gamified interface for harnessing just intonation systems (Antonis Christou); insights from a human-AI co-created concert (Lancelot Blanchard and Perry Naseck); a system for analyzing piano playing data across campus (Ayyub Abdulrezak ’24, MEng ’25); capturing music features from audio using latent frequency-masked autoencoders (Mason Wang); a device that turns any surface into a drum machine (Matthew Caren ’25); and a play-along interface for learning traditional Senegalese rhythms (Mariano Salcedo ’25). This last example led to the creation of Senegroove, a drumming-based application specifically designed for an upcoming edX online course taught by ethnomusicologist and MIT associate professor in music Patricia Tang, and world-renowned Senegalese drummer and MIT lecturer in music Lamine Touré, who provided performance videos of the foundational rhythms used in the system.&lt;/p&gt;&lt;p&gt;Ultimately, Egozy muses, “'FUTURE PHASES' showed how having the right space — in this case, the new Edward and Joyce Linde Music Building — really can be a driving force for new ways of thinking, new projects, and new ways of collaborating. My hope is that everyone in the MIT community, the Boston area, and beyond soon discovers what a truly amazing place and space we have built, and are still building here, for music and music technology at MIT.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/MIT-MTA-6-13-25-335_0.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Music technology took center stage at MIT during “FUTURE PHASES,” an evening of works for string orchestra and electronics, presented by the&amp;nbsp;MIT Music Technology and Computation Graduate Program as part of the 2025 International Computer Music Conference (ICMC).&amp;nbsp;&lt;/p&gt;&lt;p&gt;The well-attended event was held last month in the Thomas Tull Concert Hall within the new Edward and Joyce Linde Music Building. Produced in collaboration with the MIT Media Lab’s Opera of the Future Group and Boston’s self-conducted chamber orchestra A Far Cry, “FUTURE PHASES” was the first event to be presented by the MIT Music Technology and Computation Graduate Program in MIT Music’s new space.&lt;/p&gt;&lt;p&gt;“FUTURE PHASES” offerings included two new works by MIT composers: the world premiere of&amp;nbsp;“EV6,” by MIT Music’s Kenan Sahin Distinguished Professor Evan Ziporyn and professor of the practice Eran Egozy; and the U.S. premiere of&amp;nbsp;“FLOW Symphony,” by the MIT Media Lab’s Muriel R. Cooper Professor of Music and Media Tod Machover. Three additional works were selected by a jury from&amp;nbsp;an open call for works:&amp;nbsp;“The Wind Will Carry Us Away,” by Ali Balighi; “A Blank Page,” by Celeste Betancur&amp;nbsp;Gutiérrez and Luna Valentin; and “Coastal Portrait: Cycles and Thresholds,”&amp;nbsp;by Peter Lane. Each work was performed by&amp;nbsp;Boston’s own&amp;nbsp;multi-Grammy-nominated string orchestra, A Far Cry.&lt;/p&gt;&lt;p&gt;“The ICMC is all about presenting the latest research, compositions, and performances in electronic music,” says Egozy,&amp;nbsp;director of the new Music Technology and Computation Graduate Program at MIT. When approached to be a part of this year’s conference, “it seemed the perfect opportunity to showcase MIT’s commitment&amp;nbsp;to music technology, and in particular the exciting new areas being developed right now: a new master’s program in music technology and computation, the new Edward and Joyce Linde Music Building with its enhanced music technology facilities, and new faculty arriving at MIT with joint appointments between MIT Music and Theater Arts (MTA) and the Department of Electrical Engineering and Computer Science (EECS).” These recently hired professors include Anna Huang, a keynote speaker for the conference and creator of the machine learning model&amp;nbsp;Coconet that powered Google’s first AI Doodle, the Bach Doodle.&lt;/p&gt;&lt;p&gt;Egozy emphasizes the uniqueness of this occasion: “You have to understand that this is a very special situation. Having a full 18-member string orchestra [A Far Cry] perform new works that include electronics does not happen very often. In most cases, ICMC performances consist either entirely of electronics and computer-generated music, or perhaps a small ensemble of two-to-four musicians. So the opportunity we could present to the larger community of music technology was particularly exciting.”&lt;/p&gt;&lt;p&gt;To take advantage of this exciting opportunity, an open call was put out internationally to select the other pieces that would accompany Ziporyn and Egozy’s “EV6” and Machover’s “FLOW Symphony.” Three pieces were selected from a total of 46 entries to be a part of the evening’s program by a panel of judges that included Egozy, Machover, and other distinguished composers and technologists.&lt;/p&gt;&lt;p&gt;“We received a huge variety of works from this call,” says Egozy. “We saw all kinds of musical styles and ways that electronics would be used. No two pieces were very similar to each other, and I think because of that, our audience got a sense of how varied and interesting a concert can be for this format. A Far Cry was really the unifying presence. They played all pieces with great passion and nuance. They have a way of really drawing audiences into the music. And, of course, with the Thomas Tull Concert Hall being in the round, the audience felt even more connected to the music.”&lt;/p&gt;&lt;p&gt;Egozy continues, “we took advantage of the technology built into the Thomas Tull Concert Hall, which has 24 built-in speakers for surround sound allowing us to broadcast unique, amplified sound to every seat in the house. Chances are that every person might have experienced the sound slightly differently, but there was always some sense of a multidimensional evolution of sound and music as the pieces unfolded.”&lt;/p&gt;&lt;p&gt;The five works of the evening employed a range of technological components that included playing synthesized, prerecorded, or electronically manipulated sounds; attaching microphones to instruments for use in real-time signal processing algorithms; broadcasting custom-generated musical notation to&amp;nbsp;the musicians; utilizing generative AI to process live sound and play it back in interesting and unpredictable ways; and audience participation, where spectators use their cellphones as musical instruments to become a part of the ensemble.&lt;/p&gt;&lt;p&gt;Ziporyn and Egozy’s piece, “EV6&lt;em&gt;,”&lt;/em&gt; took particular advantage of this last innovation: “Evan and I had previously collaborated on a system called&amp;nbsp;Tutti, which means ‘together’ in Italian. Tutti gives an audience the ability to use their smartphones as musical instruments so that we can all play together.” Egozy developed the technology, which was first used in the MIT Campaign for a Better World in 2017. The original application involved a three-minute piece for cellphones only. “But for this concert,” Egozy explains, “Evan had the idea that we could use the same technology to write a new piece — this time, for audience phones and a live string orchestra as well.”&lt;/p&gt;&lt;p&gt;To explain the piece’s title, Ziporyn says, “I drive an EV6; it’s my first electric car, and when I first got it, it felt like I was driving an iPhone. But of course it’s still just a car: it’s got wheels and an engine, and it gets me from one place to another. It seemed like a good metaphor for this piece, in which a lot of the sound is literally played on cellphones, but still has to work like any other piece of music. It’s also a bit of an homage to David Bowie’s song ‘TVC 15,’ which is about falling in love with a robot.”&lt;/p&gt;&lt;p&gt;Egozy adds, “We wanted audience members to feel what it is like to play together in an orchestra. Through this technology, each audience member becomes a part of an orchestral section (winds, brass, strings, etc.). As they play together, they can hear their whole section playing similar music while also hearing other sections in different parts of the hall play different music. This allows an audience to feel a responsibility to their section, hear how music can move between different sections of an orchestra, and experience the thrill of live performance. In ‘EV6,’ this experience was even more electrifying because everyone in the audience got to play with a live string orchestra — perhaps for the first time in recorded history.”&lt;/p&gt;&lt;p&gt;After the concert, guests were treated to six music technology demonstrations that showcased the research of undergraduate and graduate students from both the MIT Music program and the MIT Media Lab. These included a gamified interface for harnessing just intonation systems (Antonis Christou); insights from a human-AI co-created concert (Lancelot Blanchard and Perry Naseck); a system for analyzing piano playing data across campus (Ayyub Abdulrezak ’24, MEng ’25); capturing music features from audio using latent frequency-masked autoencoders (Mason Wang); a device that turns any surface into a drum machine (Matthew Caren ’25); and a play-along interface for learning traditional Senegalese rhythms (Mariano Salcedo ’25). This last example led to the creation of Senegroove, a drumming-based application specifically designed for an upcoming edX online course taught by ethnomusicologist and MIT associate professor in music Patricia Tang, and world-renowned Senegalese drummer and MIT lecturer in music Lamine Touré, who provided performance videos of the foundational rhythms used in the system.&lt;/p&gt;&lt;p&gt;Ultimately, Egozy muses, “'FUTURE PHASES' showed how having the right space — in this case, the new Edward and Joyce Linde Music Building — really can be a driving force for new ways of thinking, new projects, and new ways of collaborating. My hope is that everyone in the MIT community, the Boston area, and beyond soon discovers what a truly amazing place and space we have built, and are still building here, for music and music technology at MIT.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/future-phases-showcase-new-frontiers-music-technology-interactive-performance-0729</guid><pubDate>Tue, 29 Jul 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] AI in Wyoming may soon use more electricity than state’s human residents (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/07/ai-in-wyoming-may-soon-use-more-electricity-than-states-human-residents/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Proposed datacenter would demand 5x Wyoming's current power use at full deployment.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/delivs_tower_1-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/delivs_tower_1-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, Mayor Patrick Collins of Cheyenne, Wyoming, announced plans for an AI data center that would consume more electricity than all homes in the state combined, according to the Associated Press. The facility, a joint venture between energy infrastructure company Tallgrass and AI data center developer Crusoe, would start at 1.8 gigawatts and scale up to 10 gigawatts of power use.&lt;/p&gt;
&lt;p&gt;The project's energy demands are difficult to overstate for Wyoming, the least populous US state. The initial 1.8-gigawatt phase, consuming 15.8 terawatt-hours (TWh) annually, is more than five times the electricity used by every household in the state combined. That figure represents 91 percent of the 17.3 TWh currently consumed by all of Wyoming's residential, commercial, and industrial sectors combined. At its full 10-gigawatt capacity, the proposed data center would consume 87.6 TWh of electricity annually—double the 43.2 TWh the entire state currently generates.&lt;/p&gt;
&lt;p&gt;Because drawing this much power from the public grid is untenable, the project will rely on its own dedicated gas generation and renewable energy sources, according to Collins and company officials. However, this massive local demand for electricity—even if self-generated—represents a fundamental shift for a state that currently sends nearly 60 percent of its generated power to other states.&lt;/p&gt;
&lt;p&gt;Wyoming Governor Mark Gordon praised the project's potential benefits for the state's natural gas industry in a company statement. "This is exciting news for Wyoming and for Wyoming natural gas producers," Gordon said.&lt;/p&gt;
&lt;p&gt;The proposed site for the new datacenter sits several miles south of Cheyenne near the Colorado border off US Route 85. While state and local regulators still need to approve the project, Collins expressed optimism about a quick start. "I believe their plans are to go sooner rather than later," he said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Wyoming’s data center boom&lt;/h2&gt;
&lt;p&gt;Cheyenne is no stranger to data centers, having attracted facilities from Microsoft and Meta since 2012 due to its cool climate and energy access. However, the new project pushes the state into uncharted territory. While Wyoming is the nation's third-biggest net energy supplier, producing 12 times more total energy than it consumes (dominated by fossil fuels), its electricity supply is finite.&lt;/p&gt;
&lt;p&gt;While Tallgrass and Crusoe have announced the partnership, they haven't revealed who will ultimately use all this computing power—leading to speculation about potential tenants.&lt;/p&gt;
&lt;p&gt;A potential connection to OpenAI's Stargate AI infrastructure project, announced in January, remains a subject of speculation. When asked by the Associated Press if the Cheyenne project was part of this effort, Crusoe spokesperson Andrew Schmitt was noncommittal. "We are not at a stage that we are ready to announce our tenant there," Schmitt said. "I can't confirm or deny that is going to be one of the Stargate."&lt;/p&gt;
&lt;p&gt;OpenAI recently activated the first phase of a Crusoe-built data center complex in Abilene, Texas, in partnership with Oracle. Chris Lehane, OpenAI's chief global affairs officer, told the Associated Press last week that the Texas facility generates "roughly and depending how you count, about a gigawatt of energy" and represents "the largest data center—we think of it as a campus—in the world."&lt;/p&gt;
&lt;p&gt;OpenAI has committed to developing an additional 4.5 gigawatts of data center capacity through an agreement with Oracle. "We're now in a position where we have, in a really concrete way, identified over five gigawatts of energy that we're going to be able to build around," Lehane told the AP. The company has not disclosed locations for these expansions, and Wyoming was not among the 16 states where OpenAI said it was searching for data center sites earlier this year.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    
      Greg Meland via Getty Images
    
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Proposed datacenter would demand 5x Wyoming's current power use at full deployment.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/delivs_tower_1-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/delivs_tower_1-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, Mayor Patrick Collins of Cheyenne, Wyoming, announced plans for an AI data center that would consume more electricity than all homes in the state combined, according to the Associated Press. The facility, a joint venture between energy infrastructure company Tallgrass and AI data center developer Crusoe, would start at 1.8 gigawatts and scale up to 10 gigawatts of power use.&lt;/p&gt;
&lt;p&gt;The project's energy demands are difficult to overstate for Wyoming, the least populous US state. The initial 1.8-gigawatt phase, consuming 15.8 terawatt-hours (TWh) annually, is more than five times the electricity used by every household in the state combined. That figure represents 91 percent of the 17.3 TWh currently consumed by all of Wyoming's residential, commercial, and industrial sectors combined. At its full 10-gigawatt capacity, the proposed data center would consume 87.6 TWh of electricity annually—double the 43.2 TWh the entire state currently generates.&lt;/p&gt;
&lt;p&gt;Because drawing this much power from the public grid is untenable, the project will rely on its own dedicated gas generation and renewable energy sources, according to Collins and company officials. However, this massive local demand for electricity—even if self-generated—represents a fundamental shift for a state that currently sends nearly 60 percent of its generated power to other states.&lt;/p&gt;
&lt;p&gt;Wyoming Governor Mark Gordon praised the project's potential benefits for the state's natural gas industry in a company statement. "This is exciting news for Wyoming and for Wyoming natural gas producers," Gordon said.&lt;/p&gt;
&lt;p&gt;The proposed site for the new datacenter sits several miles south of Cheyenne near the Colorado border off US Route 85. While state and local regulators still need to approve the project, Collins expressed optimism about a quick start. "I believe their plans are to go sooner rather than later," he said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Wyoming’s data center boom&lt;/h2&gt;
&lt;p&gt;Cheyenne is no stranger to data centers, having attracted facilities from Microsoft and Meta since 2012 due to its cool climate and energy access. However, the new project pushes the state into uncharted territory. While Wyoming is the nation's third-biggest net energy supplier, producing 12 times more total energy than it consumes (dominated by fossil fuels), its electricity supply is finite.&lt;/p&gt;
&lt;p&gt;While Tallgrass and Crusoe have announced the partnership, they haven't revealed who will ultimately use all this computing power—leading to speculation about potential tenants.&lt;/p&gt;
&lt;p&gt;A potential connection to OpenAI's Stargate AI infrastructure project, announced in January, remains a subject of speculation. When asked by the Associated Press if the Cheyenne project was part of this effort, Crusoe spokesperson Andrew Schmitt was noncommittal. "We are not at a stage that we are ready to announce our tenant there," Schmitt said. "I can't confirm or deny that is going to be one of the Stargate."&lt;/p&gt;
&lt;p&gt;OpenAI recently activated the first phase of a Crusoe-built data center complex in Abilene, Texas, in partnership with Oracle. Chris Lehane, OpenAI's chief global affairs officer, told the Associated Press last week that the Texas facility generates "roughly and depending how you count, about a gigawatt of energy" and represents "the largest data center—we think of it as a campus—in the world."&lt;/p&gt;
&lt;p&gt;OpenAI has committed to developing an additional 4.5 gigawatts of data center capacity through an agreement with Oracle. "We're now in a position where we have, in a really concrete way, identified over five gigawatts of energy that we're going to be able to build around," Lehane told the AP. The company has not disclosed locations for these expansions, and Wyoming was not among the 16 states where OpenAI said it was searching for data center sites earlier this year.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    
      Greg Meland via Getty Images
    
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/07/ai-in-wyoming-may-soon-use-more-electricity-than-states-human-residents/</guid><pubDate>Tue, 29 Jul 2025 21:24:16 +0000</pubDate></item><item><title>[NEW] Acree opens up new enterprise-focused, customizable AI model AFM-4.5B trained on ‘clean, rigorously filtered data’ (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/acree-opens-up-new-enterprise-focused-customizable-ai-model-afm-4-5b-trained-on-clean-rigorously-filtered-data/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Designed for real-world enterprise use, the 4.5-billion-parameter model — much smaller than the tens of billions to trillions of leading frontier models — combines cost efficiency, regulatory compliance, and strong performance in a compact footprint. &lt;/p&gt;&lt;p&gt;AFM-4.5B was one of a two part release made by Acree last month, and is already “instruction tuned,” or an “instruct” model, which is designed for chat, retrieval, and creative writing and can be deployed immediately for these use cases in enterprises. Another base model was also released at the time that was not instruction tuned, only pre-trained, allowing more customizability by customers. However, both were only available through commercial licensing terms — until now.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Acree’s chief technology officer (CTO) Lucas Atkins&lt;/strong&gt; also noted in a post on X that more &lt;strong&gt;“dedicated models for reasoning and tool use are on the way,” as well. &lt;/strong&gt;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;“Building AFM-4.5B has been a huge team effort, and we’re deeply grateful to everyone who supported us We can’t wait to see what you build with it,” he wrote in another post. “We’re just getting started. If you have feedback or ideas, please don’t hesitate to reach out at any time.”&lt;/p&gt;



&lt;p&gt;The model is available now for deployment across a variety of environments —from cloud to smartphones to edge hardware. &lt;/p&gt;



&lt;p&gt;It’s also geared toward Acree’s growing list of enterprise customers and their needs and wants — specifically, a model trained without violating intellectual property. &lt;/p&gt;



&lt;p&gt;As Acree wrote in its initial AFM-4.5B announcement post last month: “Tremendous effort was put towards excluding copyrighted books and material with unclear licensing.” &lt;/p&gt;



&lt;p&gt;Acree notes it worked with third-party data curation firm DatologyAI to apply techniques like source mixing, embedding-based filtering, and quality control — all aimed at minimizing hallucinations and IP risks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-focused-on-enterprise-customer-needs"&gt;Focused on enterprise customer needs&lt;/h2&gt;



&lt;p&gt;AFM-4.5B is Arcee.ai’s response to what it sees as major pain points in enterprise adoption of generative AI: high cost, limited customizability, and regulatory concerns around proprietary large language models (LLMs). &lt;/p&gt;



&lt;p&gt;Over the past year, the Arcee team held discussions with more than 150 organizations, ranging from startups to Fortune 100 companies, to understand the limitations of existing LLMs and define their own model goals.&lt;/p&gt;



&lt;p&gt;According to the company, many businesses found mainstream LLMs — such as those from OpenAI, Anthropic, or DeepSeek — too expensive and difficult to tailor to industry-specific needs. Meanwhile, while smaller open-weight models like Llama, Mistral, and Qwen offered more flexibility, they introduced concerns around licensing, IP provenance, and geopolitical risk.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFM-4.5B was developed as a “no-trade-offs” alternative: customizable, compliant, and cost-efficient without sacrificing model quality or usability.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;AFM-4.5B is designed with deployment flexibility in mind. It can operate in cloud, on-premise, hybrid, or even edge environments—thanks to its efficiency and compatibility with open frameworks such as Hugging Face Transformers, llama.cpp, and (pending release) vLLM. &lt;/p&gt;



&lt;p&gt;The model supports quantized formats, allowing it to run on lower-RAM GPUs or even CPUs, making it practical for applications with constrained resources.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-company-vision-secures-backing"&gt;Company vision secures backing&lt;/h2&gt;



&lt;p&gt;Arcee.ai’s broader strategy focuses on building domain-adaptable, small language models (SLMs) that can power &lt;strong&gt;many use cases within the same organization. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;As CEO Mark McQuade explained in a VentureBeat interview last year, “You don’t need to go that big for business use cases.” The company emphasizes fast iteration and model customization as core to its offering.&lt;/p&gt;



&lt;p&gt;This vision gained investor backing with a $24 million Series A round back in 2024. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-inside-afm-4-5b-s-architecture-and-training-process"&gt;Inside AFM-4.5B’s architecture and training process&lt;/h2&gt;



&lt;p&gt;The AFM-4.5B model uses a decoder-only transformer architecture with several optimizations for performance and deployment flexibility. &lt;/p&gt;



&lt;p&gt;It incorporates grouped query attention for faster inference and ReLU² activations in place of SwiGLU to support sparsification without degrading accuracy.&lt;/p&gt;



&lt;p&gt;Training followed a three-phase approach:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Pretraining on 6.5 trillion tokens of general data&lt;/li&gt;



&lt;li&gt;Midtraining on 1.5 trillion tokens emphasizing math and code&lt;/li&gt;



&lt;li&gt;Instruction tuning using high-quality instruction-following datasets and reinforcement learning with verifiable and preference-based feedback&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;To meet strict compliance and IP standards, the model was trained on nearly 7 trillion tokens of data curated for cleanliness and licensing safety. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-competitive-model-but-not-a-leader"&gt;A competitive model, but not a leader&lt;/h2&gt;



&lt;p&gt;Despite its smaller size, AFM-4.5B performs competitively across a broad range of benchmarks. The instruction-tuned version averages a score of 50.13 across evaluation suites such as MMLU, MixEval, TriviaQA, and Agieval—matching or outperforming similar-sized models like Gemma-3 4B-it, Qwen3-4B, and SmolLM3-3B.&lt;/p&gt;



&lt;p&gt;Multilingual testing shows the model delivers strong performance across more than 10 languages, including Arabic, Mandarin, German, and Portuguese. &lt;/p&gt;



&lt;p&gt;According to Arcee, adding support for additional dialects is straightforward due to its modular architecture.&lt;/p&gt;



&lt;p&gt;AFM-4.5B has also shown strong early traction in public evaluation environments. In a leaderboard that ranks conversational model quality by user votes and win rate, the model ranks third overall, trailing only Claude Opus 4 and Gemini 2.5 Pro. &lt;/p&gt;



&lt;p&gt;It boasts a win rate of 59.2% and the fastest latency of any top model at 0.2 seconds, paired with a generation speed of 179 tokens per second.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-built-in-support-for-agents"&gt;Built-in support for agents&lt;/h2&gt;



&lt;p&gt;In addition to general capabilities, AFM-4.5B comes with built-in support for function calling and agentic reasoning. &lt;/p&gt;



&lt;p&gt;These &lt;strong&gt;features aim to simplify the process of building AI agents and workflow automation tools&lt;/strong&gt;, reducing the need for complex prompt engineering or orchestration layers.&lt;/p&gt;



&lt;p&gt;This functionality aligns with Arcee’s broader strategy of enabling enterprises to build custom, production-ready models faster, with lower total cost of ownership (TCO) and easier integration into business operations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-acree"&gt;What’s next for Acree?&lt;/h2&gt;



&lt;p&gt;AFM-4.5B represents &lt;strong&gt;Arcee.ai’s push to define a new category of enterprise-ready language models: small, performant, and fully customizable,&lt;/strong&gt; without the compromises that often come with either proprietary LLMs or open-weight SLMs. &lt;/p&gt;



&lt;p&gt;With competitive benchmarks, multilingual support, strong compliance standards, and flexible deployment options, the model aims to meet enterprise needs for speed, sovereignty, and scale.&lt;/p&gt;



&lt;p&gt;Whether Arcee can carve out a lasting role in the rapidly shifting generative AI landscape will depend on its ability to deliver on this promise. But with AFM-4.5B, the company has made a confident first move.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Designed for real-world enterprise use, the 4.5-billion-parameter model — much smaller than the tens of billions to trillions of leading frontier models — combines cost efficiency, regulatory compliance, and strong performance in a compact footprint. &lt;/p&gt;&lt;p&gt;AFM-4.5B was one of a two part release made by Acree last month, and is already “instruction tuned,” or an “instruct” model, which is designed for chat, retrieval, and creative writing and can be deployed immediately for these use cases in enterprises. Another base model was also released at the time that was not instruction tuned, only pre-trained, allowing more customizability by customers. However, both were only available through commercial licensing terms — until now.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Acree’s chief technology officer (CTO) Lucas Atkins&lt;/strong&gt; also noted in a post on X that more &lt;strong&gt;“dedicated models for reasoning and tool use are on the way,” as well. &lt;/strong&gt;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;“Building AFM-4.5B has been a huge team effort, and we’re deeply grateful to everyone who supported us We can’t wait to see what you build with it,” he wrote in another post. “We’re just getting started. If you have feedback or ideas, please don’t hesitate to reach out at any time.”&lt;/p&gt;



&lt;p&gt;The model is available now for deployment across a variety of environments —from cloud to smartphones to edge hardware. &lt;/p&gt;



&lt;p&gt;It’s also geared toward Acree’s growing list of enterprise customers and their needs and wants — specifically, a model trained without violating intellectual property. &lt;/p&gt;



&lt;p&gt;As Acree wrote in its initial AFM-4.5B announcement post last month: “Tremendous effort was put towards excluding copyrighted books and material with unclear licensing.” &lt;/p&gt;



&lt;p&gt;Acree notes it worked with third-party data curation firm DatologyAI to apply techniques like source mixing, embedding-based filtering, and quality control — all aimed at minimizing hallucinations and IP risks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-focused-on-enterprise-customer-needs"&gt;Focused on enterprise customer needs&lt;/h2&gt;



&lt;p&gt;AFM-4.5B is Arcee.ai’s response to what it sees as major pain points in enterprise adoption of generative AI: high cost, limited customizability, and regulatory concerns around proprietary large language models (LLMs). &lt;/p&gt;



&lt;p&gt;Over the past year, the Arcee team held discussions with more than 150 organizations, ranging from startups to Fortune 100 companies, to understand the limitations of existing LLMs and define their own model goals.&lt;/p&gt;



&lt;p&gt;According to the company, many businesses found mainstream LLMs — such as those from OpenAI, Anthropic, or DeepSeek — too expensive and difficult to tailor to industry-specific needs. Meanwhile, while smaller open-weight models like Llama, Mistral, and Qwen offered more flexibility, they introduced concerns around licensing, IP provenance, and geopolitical risk.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AFM-4.5B was developed as a “no-trade-offs” alternative: customizable, compliant, and cost-efficient without sacrificing model quality or usability.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;AFM-4.5B is designed with deployment flexibility in mind. It can operate in cloud, on-premise, hybrid, or even edge environments—thanks to its efficiency and compatibility with open frameworks such as Hugging Face Transformers, llama.cpp, and (pending release) vLLM. &lt;/p&gt;



&lt;p&gt;The model supports quantized formats, allowing it to run on lower-RAM GPUs or even CPUs, making it practical for applications with constrained resources.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-company-vision-secures-backing"&gt;Company vision secures backing&lt;/h2&gt;



&lt;p&gt;Arcee.ai’s broader strategy focuses on building domain-adaptable, small language models (SLMs) that can power &lt;strong&gt;many use cases within the same organization. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;As CEO Mark McQuade explained in a VentureBeat interview last year, “You don’t need to go that big for business use cases.” The company emphasizes fast iteration and model customization as core to its offering.&lt;/p&gt;



&lt;p&gt;This vision gained investor backing with a $24 million Series A round back in 2024. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-inside-afm-4-5b-s-architecture-and-training-process"&gt;Inside AFM-4.5B’s architecture and training process&lt;/h2&gt;



&lt;p&gt;The AFM-4.5B model uses a decoder-only transformer architecture with several optimizations for performance and deployment flexibility. &lt;/p&gt;



&lt;p&gt;It incorporates grouped query attention for faster inference and ReLU² activations in place of SwiGLU to support sparsification without degrading accuracy.&lt;/p&gt;



&lt;p&gt;Training followed a three-phase approach:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Pretraining on 6.5 trillion tokens of general data&lt;/li&gt;



&lt;li&gt;Midtraining on 1.5 trillion tokens emphasizing math and code&lt;/li&gt;



&lt;li&gt;Instruction tuning using high-quality instruction-following datasets and reinforcement learning with verifiable and preference-based feedback&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;To meet strict compliance and IP standards, the model was trained on nearly 7 trillion tokens of data curated for cleanliness and licensing safety. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-competitive-model-but-not-a-leader"&gt;A competitive model, but not a leader&lt;/h2&gt;



&lt;p&gt;Despite its smaller size, AFM-4.5B performs competitively across a broad range of benchmarks. The instruction-tuned version averages a score of 50.13 across evaluation suites such as MMLU, MixEval, TriviaQA, and Agieval—matching or outperforming similar-sized models like Gemma-3 4B-it, Qwen3-4B, and SmolLM3-3B.&lt;/p&gt;



&lt;p&gt;Multilingual testing shows the model delivers strong performance across more than 10 languages, including Arabic, Mandarin, German, and Portuguese. &lt;/p&gt;



&lt;p&gt;According to Arcee, adding support for additional dialects is straightforward due to its modular architecture.&lt;/p&gt;



&lt;p&gt;AFM-4.5B has also shown strong early traction in public evaluation environments. In a leaderboard that ranks conversational model quality by user votes and win rate, the model ranks third overall, trailing only Claude Opus 4 and Gemini 2.5 Pro. &lt;/p&gt;



&lt;p&gt;It boasts a win rate of 59.2% and the fastest latency of any top model at 0.2 seconds, paired with a generation speed of 179 tokens per second.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-built-in-support-for-agents"&gt;Built-in support for agents&lt;/h2&gt;



&lt;p&gt;In addition to general capabilities, AFM-4.5B comes with built-in support for function calling and agentic reasoning. &lt;/p&gt;



&lt;p&gt;These &lt;strong&gt;features aim to simplify the process of building AI agents and workflow automation tools&lt;/strong&gt;, reducing the need for complex prompt engineering or orchestration layers.&lt;/p&gt;



&lt;p&gt;This functionality aligns with Arcee’s broader strategy of enabling enterprises to build custom, production-ready models faster, with lower total cost of ownership (TCO) and easier integration into business operations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-acree"&gt;What’s next for Acree?&lt;/h2&gt;



&lt;p&gt;AFM-4.5B represents &lt;strong&gt;Arcee.ai’s push to define a new category of enterprise-ready language models: small, performant, and fully customizable,&lt;/strong&gt; without the compromises that often come with either proprietary LLMs or open-weight SLMs. &lt;/p&gt;



&lt;p&gt;With competitive benchmarks, multilingual support, strong compliance standards, and flexible deployment options, the model aims to meet enterprise needs for speed, sovereignty, and scale.&lt;/p&gt;



&lt;p&gt;Whether Arcee can carve out a lasting role in the rapidly shifting generative AI landscape will depend on its ability to deliver on this promise. But with AFM-4.5B, the company has made a confident first move.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/acree-opens-up-new-enterprise-focused-customizable-ai-model-afm-4-5b-trained-on-clean-rigorously-filtered-data/</guid><pubDate>Tue, 29 Jul 2025 21:26:29 +0000</pubDate></item><item><title>[NEW] AI vs. AI: Prophet Security raises $30M to replace human analysts with autonomous defenders (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/ai-vs-ai-prophet-security-raises-30m-to-replace-human-analysts-with-autonomous-defenders/</link><description>&lt;p&gt;Prophet Security, a startup developing autonomous artificial intelligence systems for cybersecurity defense, announced Tuesday it has raised $30 million in Series A funding to accelerate what its founders describe as a fundamental shift from human-versus-human to “agent-versus-agent” warfare in cybersecurity.&lt;/p&gt;&lt;p&gt;The Menlo Park-based company’s funding round, led by venture capital firm Accel with participation from Bain Capital Ventures, comes as organizations struggle with an overwhelming volume of security alerts while sophisticated attackers increasingly leverage AI to scale and automate their operations. Prophet’s approach represents a marked departure from the “copilot” AI tools that have dominated the market, instead deploying fully autonomous agents that can investigate and respond to threats without human intervention.&lt;/p&gt;&lt;p&gt;“Every security operations team is faced with a dual mandate of reducing risk while driving operational efficiency,” said Kamal Shah, Prophet Security’s co-founder and CEO, in an exclusive interview with VentureBeat. “Our Agentic AI SOC Platform addresses both challenges by automating manual, repetitive tasks in security operations with speed, accuracy and explainability.”&lt;/p&gt;&lt;p&gt;The funding announcement coincides with Prophet’s launch of what it calls the industry’s most comprehensive Agentic AI SOC Platform, expanding beyond its initial Prophet AI SOC Analyst to include Prophet AI Threat Hunter and Prophet AI Detection Advisor. The platform represents a significant evolution from traditional Security Operations Center (SOC) automation tools, which typically rely on rigid, pre-programmed playbooks.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-security-teams-drowning-in-960-daily-alerts-face-unprecedented-capacity-crisis"&gt;Security teams drowning in 960 daily alerts face unprecedented capacity crisis&lt;/h2&gt;



&lt;p&gt;The cybersecurity industry faces a crisis of capacity and capability. Shah, who previously served as CEO of container security company StackRox before its acquisition by Red Hat, experienced these challenges firsthand. According to his observations, organizations receive an average of 960 security alerts daily, with up to 40% going uninvestigated due to resource constraints.&lt;/p&gt;



&lt;p&gt;“The number one complaint that I see from customers every single day is too many alerts, too many false positives,” Shah explained. “If you think about the world that we live in today, on average, a company gets 960 alerts a day from all the security tools that they have in their environment, and 40% of those alerts are ignored because they just don’t have the capacity to go and investigate all those alerts.”&lt;/p&gt;



&lt;p&gt;The problem is compounded by a severe shortage of skilled cybersecurity professionals. Shah points to what he calls a critical talent gap, noting there are 5 million open positions in cybersecurity globally, creating a situation where even organizations with budget to hire cannot find qualified personnel.&lt;/p&gt;



&lt;p&gt;Prophet’s solution directly addresses this capacity crunch. Over the past six months, the company’s AI SOC Analyst has performed more than 1 million autonomous investigations across its customer base, saving an estimated 360,000 hours of investigation time while delivering 10 times faster response times and reducing false positives by 96%.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-autonomous-ai-agents-differ-from-reactive-copilot-systems-transforming-cybersecurity"&gt;How autonomous AI agents differ from reactive copilot systems transforming cybersecurity&lt;/h2&gt;



&lt;p&gt;The distinction between Prophet’s “agentic” AI and the copilot models deployed by larger cybersecurity vendors like CrowdStrike, Microsoft, and Sentinel One is fundamental to understanding the company’s value proposition. Traditional copilot systems require human analysts to initiate queries and interpret responses, essentially serving as sophisticated search interfaces for security data.&lt;/p&gt;



&lt;p&gt;“Copilot is reactive,” Shah explained. “You have an alert come in and a security analyst has to go and write questions, ask the question to say, hey, what does this mean? And you have to know what questions to ask. The analyst is still in the loop for every single alert that comes in because they’re interacting with it.”&lt;/p&gt;



&lt;p&gt;By contrast, Prophet’s agentic AI proactively initiates investigations the moment an alert is triggered, autonomously gathering evidence, reasoning through the data, and reaching conclusions without human intervention. The system documents every step of its investigation process, creating an audit trail that allows security teams to understand and verify its reasoning.&lt;/p&gt;



&lt;p&gt;“What Prophet AI is able to do is immediately, once an alert is triggered, it proactively goes and completes the investigation,” Shah said. “Within a matter of minutes, your investigation is complete and it knows what questions to ask, and it’s been trained to act like an expert analyst.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-building-enterprise-trust-through-transparent-ai-decision-making-and-data-protection"&gt;Building enterprise trust through transparent AI decision-making and data protection&lt;/h2&gt;



&lt;p&gt;Prophet’s system leverages multiple frontier AI models, including offerings from OpenAI, Anthropic, and others, selecting the most appropriate model for each specific task. The company has built what Shah describes as an “evals framework” to ensure accuracy, repeatability, and consistency while preventing AI hallucinations—a critical concern in security contexts where false information can lead to inappropriate responses.&lt;/p&gt;



&lt;p&gt;“In security, you are in a trust building exercise with the security teams, and if you hallucinate, you’re going to lose trust and they’re not going to use your product,” Shah emphasized. The company employs a retrieval-augmented generation (RAG) architecture combined with rigorous evaluation processes to maintain what Shah calls “a high bar for security teams.”&lt;/p&gt;



&lt;p&gt;Data privacy and security represent paramount concerns for Prophet’s enterprise customers. The company employs a single-tenant architecture ensuring customer data remains isolated, and maintains contractual agreements with AI model providers preventing customer data from being used to train or fine-tune models.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-early-customers-report-dramatic-efficiency-gains-as-ai-handles-thousands-of-security-alerts"&gt;Early customers report dramatic efficiency gains as AI handles thousands of security alerts&lt;/h2&gt;



&lt;p&gt;Prophet’s customer base includes Docker, which provided a testimonial for the funding announcement. Tushar Jain, Docker’s EVP of Engineering and Product, noted that “Prophet AI is already helping streamline parts of our security workflow, and we’re just getting started. With the recent release of Threat Hunter and growing integration with our systems, we see a clear path to faster response times, reduced noise, and a more focused security team.”&lt;/p&gt;



&lt;p&gt;The company has also published case studies demonstrating dramatic improvements in SOC efficiency. Eric Wille, CISO at Cabinet Works, reported reducing his team’s alert volume from 33,200 down to just six alerts requiring human attention, effectively allowing his small team to operate with the efficiency of a much larger organization.&lt;/p&gt;



&lt;p&gt;“Prophet AI cut our alert queue from thousands to dozens,” Wille said in a video testimonial. “It’s a force multiplier that removes investigation bottlenecks, improves analyst focus, and helps us respond to real threats faster.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-rising-cyber-threats-and-evolving-attack-methods-drive-demand-for-ai-powered-defense"&gt;Rising cyber threats and evolving attack methods drive demand for AI-powered defense&lt;/h2&gt;



&lt;p&gt;Prophet’s emergence occurs against a backdrop of rapidly evolving cyber threats. CrowdStrike’s 2025 Global Threat Report documented a 150% increase in China-nexus cyber activity and a 442% growth in voice phishing operations, while noting that 79% of detected threats were malware-free, making them harder to identify through traditional signature-based detection methods.&lt;/p&gt;



&lt;p&gt;The company’s approach to integration across existing security tools provides a key competitive advantage. Rather than requiring organizations to replace their current security stack, Prophet integrates with existing Security Information and Event Management (SIEM) systems, Endpoint Detection and Response (EDR) platforms, and other security tools.&lt;/p&gt;



&lt;p&gt;“If you’ve got to go get five or six different copilots to use within your organization, it’s going to be very confusing,” Shah explained. “What customers are telling us is that, hey, I want an independent AI SOC platform that can help me triage, investigate and respond to alerts from all of my security tools, not just one or two.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-accel-s-preemptive-investment-signals-growing-confidence-in-autonomous-security-systems"&gt;Accel’s preemptive investment signals growing confidence in autonomous security systems&lt;/h2&gt;



&lt;p&gt;Eric Wolford, Partner at Accel, emphasized the combination of technical innovation and proven market traction that drove the investment decision. “What stood out to us about Prophet wasn’t just the technical ambition, but the real-world traction: they’re delivering autonomy and speed while showing their work—a critical differentiator in an industry that runs on trust,” Wolford said in a statement.&lt;/p&gt;



&lt;p&gt;Accel’s cybersecurity investment portfolio includes CrowdStrike, Tenable, and BlackPoint Cyber, providing the firm with deep expertise in evaluating security technologies. The preemptive nature of the funding round — Prophet was not actively seeking capital — underscores investor confidence in the company’s trajectory.&lt;/p&gt;



&lt;p&gt;The funding will primarily support engineering expansion and go-to-market acceleration as Prophet scales its platform capabilities. The company plans to continue expanding its agentic AI platform, potentially adding new modules for additional security operations workflows.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-industry-experts-predict-widespread-adoption-of-ai-agents-will-reshape-cybersecurity-landscape"&gt;Industry experts predict widespread adoption of AI agents will reshape cybersecurity landscape&lt;/h2&gt;



&lt;p&gt;Prophet’s success reflects broader trends reshaping cybersecurity. Deloitte’s 2025 cybersecurity forecasts predict widespread adoption of agentic AI systems, with 40% of large enterprises expected to deploy such systems in their SOCs by 2025. The consulting firm characterizes this shift as moving from “automation that follows instructions to automation that thinks.”&lt;/p&gt;



&lt;p&gt;The company’s “role elevation” philosophy — enhancing rather than replacing human analysts — addresses concerns about AI displacing cybersecurity professionals. Shah emphasized that automation should free analysts from repetitive tasks to focus on higher-value security work.&lt;/p&gt;



&lt;p&gt;“This is not about eliminating jobs,” Shah said. “It’s about ensuring an analyst doesn’t have to spend time triaging and investigating alerts, because who wants to do that all day, every day? Instead, they can focus on the 4% of issues that truly matter to an organization. They’re advancing their careers and doing more higher-order security work.”&lt;/p&gt;



&lt;p&gt;As cyber threats continue evolving and incorporating AI capabilities, the arms race between attackers and defenders increasingly relies on technological sophistication rather than human capacity alone. Prophet’s approach suggests a future where cybersecurity becomes primarily a contest between AI systems, with human expertise focused on strategic oversight and complex decision-making.&lt;/p&gt;



&lt;p&gt;The company’s ability to demonstrate measurable improvements in SOC efficiency while maintaining transparency and explainability positions it to capture market share as organizations grapple with the dual pressures of increasing threats and persistent talent shortages. With the new funding, Prophet Security aims to accelerate this transition, potentially setting the standard for how organizations defend against AI-powered attacks in an era where the speed and scale of threats exceed human capacity to respond manually.&lt;/p&gt;



&lt;p&gt;But perhaps the most telling indicator of this shift isn’t Prophet’s technology or funding — it’s what happened when Shah’s team wasn’t actively seeking investment. Accel approached them anyway, recognizing that in a world where attackers launch AI-powered assaults at machine speed, the old playbook of human-driven defense isn’t just insufficient — it’s obsolete.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;Prophet Security, a startup developing autonomous artificial intelligence systems for cybersecurity defense, announced Tuesday it has raised $30 million in Series A funding to accelerate what its founders describe as a fundamental shift from human-versus-human to “agent-versus-agent” warfare in cybersecurity.&lt;/p&gt;&lt;p&gt;The Menlo Park-based company’s funding round, led by venture capital firm Accel with participation from Bain Capital Ventures, comes as organizations struggle with an overwhelming volume of security alerts while sophisticated attackers increasingly leverage AI to scale and automate their operations. Prophet’s approach represents a marked departure from the “copilot” AI tools that have dominated the market, instead deploying fully autonomous agents that can investigate and respond to threats without human intervention.&lt;/p&gt;&lt;p&gt;“Every security operations team is faced with a dual mandate of reducing risk while driving operational efficiency,” said Kamal Shah, Prophet Security’s co-founder and CEO, in an exclusive interview with VentureBeat. “Our Agentic AI SOC Platform addresses both challenges by automating manual, repetitive tasks in security operations with speed, accuracy and explainability.”&lt;/p&gt;&lt;p&gt;The funding announcement coincides with Prophet’s launch of what it calls the industry’s most comprehensive Agentic AI SOC Platform, expanding beyond its initial Prophet AI SOC Analyst to include Prophet AI Threat Hunter and Prophet AI Detection Advisor. The platform represents a significant evolution from traditional Security Operations Center (SOC) automation tools, which typically rely on rigid, pre-programmed playbooks.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-security-teams-drowning-in-960-daily-alerts-face-unprecedented-capacity-crisis"&gt;Security teams drowning in 960 daily alerts face unprecedented capacity crisis&lt;/h2&gt;



&lt;p&gt;The cybersecurity industry faces a crisis of capacity and capability. Shah, who previously served as CEO of container security company StackRox before its acquisition by Red Hat, experienced these challenges firsthand. According to his observations, organizations receive an average of 960 security alerts daily, with up to 40% going uninvestigated due to resource constraints.&lt;/p&gt;



&lt;p&gt;“The number one complaint that I see from customers every single day is too many alerts, too many false positives,” Shah explained. “If you think about the world that we live in today, on average, a company gets 960 alerts a day from all the security tools that they have in their environment, and 40% of those alerts are ignored because they just don’t have the capacity to go and investigate all those alerts.”&lt;/p&gt;



&lt;p&gt;The problem is compounded by a severe shortage of skilled cybersecurity professionals. Shah points to what he calls a critical talent gap, noting there are 5 million open positions in cybersecurity globally, creating a situation where even organizations with budget to hire cannot find qualified personnel.&lt;/p&gt;



&lt;p&gt;Prophet’s solution directly addresses this capacity crunch. Over the past six months, the company’s AI SOC Analyst has performed more than 1 million autonomous investigations across its customer base, saving an estimated 360,000 hours of investigation time while delivering 10 times faster response times and reducing false positives by 96%.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-autonomous-ai-agents-differ-from-reactive-copilot-systems-transforming-cybersecurity"&gt;How autonomous AI agents differ from reactive copilot systems transforming cybersecurity&lt;/h2&gt;



&lt;p&gt;The distinction between Prophet’s “agentic” AI and the copilot models deployed by larger cybersecurity vendors like CrowdStrike, Microsoft, and Sentinel One is fundamental to understanding the company’s value proposition. Traditional copilot systems require human analysts to initiate queries and interpret responses, essentially serving as sophisticated search interfaces for security data.&lt;/p&gt;



&lt;p&gt;“Copilot is reactive,” Shah explained. “You have an alert come in and a security analyst has to go and write questions, ask the question to say, hey, what does this mean? And you have to know what questions to ask. The analyst is still in the loop for every single alert that comes in because they’re interacting with it.”&lt;/p&gt;



&lt;p&gt;By contrast, Prophet’s agentic AI proactively initiates investigations the moment an alert is triggered, autonomously gathering evidence, reasoning through the data, and reaching conclusions without human intervention. The system documents every step of its investigation process, creating an audit trail that allows security teams to understand and verify its reasoning.&lt;/p&gt;



&lt;p&gt;“What Prophet AI is able to do is immediately, once an alert is triggered, it proactively goes and completes the investigation,” Shah said. “Within a matter of minutes, your investigation is complete and it knows what questions to ask, and it’s been trained to act like an expert analyst.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-building-enterprise-trust-through-transparent-ai-decision-making-and-data-protection"&gt;Building enterprise trust through transparent AI decision-making and data protection&lt;/h2&gt;



&lt;p&gt;Prophet’s system leverages multiple frontier AI models, including offerings from OpenAI, Anthropic, and others, selecting the most appropriate model for each specific task. The company has built what Shah describes as an “evals framework” to ensure accuracy, repeatability, and consistency while preventing AI hallucinations—a critical concern in security contexts where false information can lead to inappropriate responses.&lt;/p&gt;



&lt;p&gt;“In security, you are in a trust building exercise with the security teams, and if you hallucinate, you’re going to lose trust and they’re not going to use your product,” Shah emphasized. The company employs a retrieval-augmented generation (RAG) architecture combined with rigorous evaluation processes to maintain what Shah calls “a high bar for security teams.”&lt;/p&gt;



&lt;p&gt;Data privacy and security represent paramount concerns for Prophet’s enterprise customers. The company employs a single-tenant architecture ensuring customer data remains isolated, and maintains contractual agreements with AI model providers preventing customer data from being used to train or fine-tune models.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-early-customers-report-dramatic-efficiency-gains-as-ai-handles-thousands-of-security-alerts"&gt;Early customers report dramatic efficiency gains as AI handles thousands of security alerts&lt;/h2&gt;



&lt;p&gt;Prophet’s customer base includes Docker, which provided a testimonial for the funding announcement. Tushar Jain, Docker’s EVP of Engineering and Product, noted that “Prophet AI is already helping streamline parts of our security workflow, and we’re just getting started. With the recent release of Threat Hunter and growing integration with our systems, we see a clear path to faster response times, reduced noise, and a more focused security team.”&lt;/p&gt;



&lt;p&gt;The company has also published case studies demonstrating dramatic improvements in SOC efficiency. Eric Wille, CISO at Cabinet Works, reported reducing his team’s alert volume from 33,200 down to just six alerts requiring human attention, effectively allowing his small team to operate with the efficiency of a much larger organization.&lt;/p&gt;



&lt;p&gt;“Prophet AI cut our alert queue from thousands to dozens,” Wille said in a video testimonial. “It’s a force multiplier that removes investigation bottlenecks, improves analyst focus, and helps us respond to real threats faster.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-rising-cyber-threats-and-evolving-attack-methods-drive-demand-for-ai-powered-defense"&gt;Rising cyber threats and evolving attack methods drive demand for AI-powered defense&lt;/h2&gt;



&lt;p&gt;Prophet’s emergence occurs against a backdrop of rapidly evolving cyber threats. CrowdStrike’s 2025 Global Threat Report documented a 150% increase in China-nexus cyber activity and a 442% growth in voice phishing operations, while noting that 79% of detected threats were malware-free, making them harder to identify through traditional signature-based detection methods.&lt;/p&gt;



&lt;p&gt;The company’s approach to integration across existing security tools provides a key competitive advantage. Rather than requiring organizations to replace their current security stack, Prophet integrates with existing Security Information and Event Management (SIEM) systems, Endpoint Detection and Response (EDR) platforms, and other security tools.&lt;/p&gt;



&lt;p&gt;“If you’ve got to go get five or six different copilots to use within your organization, it’s going to be very confusing,” Shah explained. “What customers are telling us is that, hey, I want an independent AI SOC platform that can help me triage, investigate and respond to alerts from all of my security tools, not just one or two.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-accel-s-preemptive-investment-signals-growing-confidence-in-autonomous-security-systems"&gt;Accel’s preemptive investment signals growing confidence in autonomous security systems&lt;/h2&gt;



&lt;p&gt;Eric Wolford, Partner at Accel, emphasized the combination of technical innovation and proven market traction that drove the investment decision. “What stood out to us about Prophet wasn’t just the technical ambition, but the real-world traction: they’re delivering autonomy and speed while showing their work—a critical differentiator in an industry that runs on trust,” Wolford said in a statement.&lt;/p&gt;



&lt;p&gt;Accel’s cybersecurity investment portfolio includes CrowdStrike, Tenable, and BlackPoint Cyber, providing the firm with deep expertise in evaluating security technologies. The preemptive nature of the funding round — Prophet was not actively seeking capital — underscores investor confidence in the company’s trajectory.&lt;/p&gt;



&lt;p&gt;The funding will primarily support engineering expansion and go-to-market acceleration as Prophet scales its platform capabilities. The company plans to continue expanding its agentic AI platform, potentially adding new modules for additional security operations workflows.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-industry-experts-predict-widespread-adoption-of-ai-agents-will-reshape-cybersecurity-landscape"&gt;Industry experts predict widespread adoption of AI agents will reshape cybersecurity landscape&lt;/h2&gt;



&lt;p&gt;Prophet’s success reflects broader trends reshaping cybersecurity. Deloitte’s 2025 cybersecurity forecasts predict widespread adoption of agentic AI systems, with 40% of large enterprises expected to deploy such systems in their SOCs by 2025. The consulting firm characterizes this shift as moving from “automation that follows instructions to automation that thinks.”&lt;/p&gt;



&lt;p&gt;The company’s “role elevation” philosophy — enhancing rather than replacing human analysts — addresses concerns about AI displacing cybersecurity professionals. Shah emphasized that automation should free analysts from repetitive tasks to focus on higher-value security work.&lt;/p&gt;



&lt;p&gt;“This is not about eliminating jobs,” Shah said. “It’s about ensuring an analyst doesn’t have to spend time triaging and investigating alerts, because who wants to do that all day, every day? Instead, they can focus on the 4% of issues that truly matter to an organization. They’re advancing their careers and doing more higher-order security work.”&lt;/p&gt;



&lt;p&gt;As cyber threats continue evolving and incorporating AI capabilities, the arms race between attackers and defenders increasingly relies on technological sophistication rather than human capacity alone. Prophet’s approach suggests a future where cybersecurity becomes primarily a contest between AI systems, with human expertise focused on strategic oversight and complex decision-making.&lt;/p&gt;



&lt;p&gt;The company’s ability to demonstrate measurable improvements in SOC efficiency while maintaining transparency and explainability positions it to capture market share as organizations grapple with the dual pressures of increasing threats and persistent talent shortages. With the new funding, Prophet Security aims to accelerate this transition, potentially setting the standard for how organizations defend against AI-powered attacks in an era where the speed and scale of threats exceed human capacity to respond manually.&lt;/p&gt;



&lt;p&gt;But perhaps the most telling indicator of this shift isn’t Prophet’s technology or funding — it’s what happened when Shah’s team wasn’t actively seeking investment. Accel approached them anyway, recognizing that in a world where attackers launch AI-powered assaults at machine speed, the old playbook of human-driven defense isn’t just insufficient — it’s obsolete.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/ai-vs-ai-prophet-security-raises-30m-to-replace-human-analysts-with-autonomous-defenders/</guid><pubDate>Tue, 29 Jul 2025 21:39:24 +0000</pubDate></item><item><title>[NEW] Nvidia AI chip challenger Groq said to be nearing new fundraising at $6B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/29/nvidia-ai-chip-challenger-groq-said-to-be-nearing-new-fundraising-at-6b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/1_byjKY2ta4_34GHPvmN9Suw.jpg?resize=1200,640" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI chip startup Groq is in talks to raise a fresh $600 million at a near $6 billion valuation, sources tell Bloomberg, although the deal isn’t yet final and terms could change.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Groq raised $640 million at a $2.8 billion valuation in August 2024, making this double the valuation in about a year. Groq previously raised about $1 billion.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new round is led by Austin-based firm Disruptive, Bloomberg reports. The November round was led by BlackRock, with participation from Neuberger Berman, Type One Ventures, Cisco, KDDI, and Samsung Catalyst Fund.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Groq was founded by Jonathan Ross, who previously worked at Google developing its Tensor Processing Unit chip. The startup emerged from stealth in 2016.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new raise comes after Groq announced in May an exclusive partnership with Bell Canada to power the telco’s large AI infrastructure project. In April, Groq partnered with Meta to offer AI infrastructure to speed Llama 4 inference. Neither Disruptive nor Groq immediately returned our request for comment.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Correction: This story originally incorrectly reported the date of the last raise. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/1_byjKY2ta4_34GHPvmN9Suw.jpg?resize=1200,640" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI chip startup Groq is in talks to raise a fresh $600 million at a near $6 billion valuation, sources tell Bloomberg, although the deal isn’t yet final and terms could change.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Groq raised $640 million at a $2.8 billion valuation in August 2024, making this double the valuation in about a year. Groq previously raised about $1 billion.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new round is led by Austin-based firm Disruptive, Bloomberg reports. The November round was led by BlackRock, with participation from Neuberger Berman, Type One Ventures, Cisco, KDDI, and Samsung Catalyst Fund.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Groq was founded by Jonathan Ross, who previously worked at Google developing its Tensor Processing Unit chip. The startup emerged from stealth in 2016.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new raise comes after Groq announced in May an exclusive partnership with Bell Canada to power the telco’s large AI infrastructure project. In April, Groq partnered with Meta to offer AI infrastructure to speed Llama 4 inference. Neither Disruptive nor Groq immediately returned our request for comment.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Correction: This story originally incorrectly reported the date of the last raise. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/29/nvidia-ai-chip-challenger-groq-said-to-be-nearing-new-fundraising-at-6b-valuation/</guid><pubDate>Tue, 29 Jul 2025 22:02:51 +0000</pubDate></item></channel></rss>