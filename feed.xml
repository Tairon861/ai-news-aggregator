<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 05 Aug 2025 02:02:27 +0000</lastBuildDate><item><title>Tencent releases versatile open-source Hunyuan AI models (AI News)</title><link>https://www.artificialintelligence-news.com/news/tencent-releases-versatile-open-source-hunyuan-ai-models/</link><description>&lt;p&gt;Tencent has expanded its family of open-source Hunyuan AI models that are versatile enough for broad use. This new family of models is engineered to deliver powerful performance across computational environments, from small edge devices to demanding, high-concurrency production systems.&lt;/p&gt;&lt;p&gt;The release includes a comprehensive set of pre-trained and instruction-tuned models available on the developer platform Hugging Face. The models come in several sizes, specifically with parameter scales of 0.5B, 1.8B, 4B, and 7B, providing substantial flexibility for developers and businesses.&lt;/p&gt;&lt;p&gt;Tencent has indicated that these models were developed using training strategies similar to its more powerful Hunyuan-A13B model, allowing them to inherit its performance characteristics. This approach enables users to select the optimal model for their needs, whether it‚Äôs a smaller variant for resource-constrained edge computing or a larger model for high-throughput production workloads, all while ensuring strong capabilities.&lt;/p&gt;&lt;p&gt;One of the most notable features of the Hunyuan series is its native support for an ultra-long 256K context window. This allows the models to handle and maintain stable performance on long-text tasks, a vital capability for complex document analysis, extended conversations, and in-depth content generation. The models support what Tencent calls ‚Äúhybrid reasoning,‚Äù which allows for both fast and slow thinking modes that users can choose between depending on their specific requirements.&lt;/p&gt;&lt;p&gt;The company has also placed a strong emphasis on agentic capabilities. The models have been optimised for agent-based tasks and have demonstrated leading results on established benchmarks such as BFCL-v3, œÑ-Bench, and C3-Bench, suggesting a high degree of proficiency in complex, multi-step problem-solving. For instance, on the C3-Bench, the Hunyuan-7B-Instruct model achieves a score of 68.5, while the Hunyuan-4B-Instruct model scores 64.3.&lt;/p&gt;&lt;p&gt;The series‚Äô performance is a focus on efficient inference. Tencent‚Äôs Hunyuan models utilise Grouped Query Attention (GQA), a technique known for improving processing speed and reducing computational overhead. This efficiency is further enhanced by advanced quantisation support, a key element of the Hunyuan architecture designed to lower deployment barriers.&lt;/p&gt;&lt;p&gt;Tencent has developed its own compression toolset, AngleSlim, to create a more user-friendly and effective model compression solution. Using this tool, the company offers two main types of quantisation for the Hunyuan series.&lt;/p&gt;&lt;p&gt;The first is FP8 static quantisation, which employs an 8-bit floating-point format. This method uses a small amount of calibration data to pre-determine the quantisation scale without requiring full retraining, converting model weights and activation values into the FP8 format to boost inference efficiency.&lt;/p&gt;&lt;p&gt;The second method is INT4 quantisation, which achieves W4A16 quantisation through the GPTQ and AWQ algorithms:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;The &lt;strong&gt;GPTQ&lt;/strong&gt; approach processes model weights layer by layer, using calibration data to minimise errors in the quantised weights. This process avoids requiring model retraining and improves inference speed.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;The &lt;strong&gt;AWQ &lt;/strong&gt;algorithm works by statistically analysing the amplitude of activation values from a small set of calibration data. It then calculates a scaling coefficient for each weight channel, which expands the numerical range of important weights to retain more information during the compression process.&amp;nbsp;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Developers can either use the AngleSlim tool themselves or download the pre-quantised models directly.&lt;/p&gt;&lt;p&gt;Performance benchmarks confirm the strong capabilities of the Tencent Hunyuan models across a range of tasks. The pre-trained Hunyuan-7B model, for example, achieves a score of 79.82 on the MMLU benchmark, 88.25 on GSM8K, and 74.85 on the MATH benchmark, demonstrating solid reasoning and mathematical skills.&lt;/p&gt;&lt;p&gt;The instruction-tuned variants show impressive results in specialised areas. In mathematics, the Hunyuan-7B-Instruct model scores 81.1 on the AIME 2024 benchmark, while the 4B version scores 78.3. In science, the 7B model reaches 76.5 on OlympiadBench, and in coding, it scores 42 on Livecodebench.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;üöÄWe're expanding the Tencent Hunyuan open-source LLM ecosystem with four compact models (0.5B, 1.8B, 4B, 7B)! Designed for low-power scenarios like consumer-grade GPUs, smart vehicles, smart home devices, mobile phones, and PCs, these models support cost-effective fine-tuning‚Ä¶ pic.twitter.com/CknskVqPem&lt;/p&gt;‚Äî Hunyuan (@TencentHunyuan) August 4, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;The quantisation benchmarks show minimal performance degradation. On the DROP benchmark, the Hunyuan-7B-Instruct model scores 85.9 in its base B16 format, 86.0 with FP8, and 85.7 with Int4 GPTQ, indicating that efficiency gains do not come at a cost to accuracy.&lt;/p&gt;&lt;p&gt;For deployment, Tencent recommends using established frameworks like TensorRT-LLM, vLLM, or SGLang to serve the Hunyuan models and create OpenAI-compatible API endpoints, ensuring they can be integrated smoothly into existing development workflows. This combination of performance, efficiency, and deployment flexibility positions the Hunyuan series as a continuing powerful contender in open-source AI.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Deep Cogito v2: Open-source AI that hones its reasoning skills&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Tencent has expanded its family of open-source Hunyuan AI models that are versatile enough for broad use. This new family of models is engineered to deliver powerful performance across computational environments, from small edge devices to demanding, high-concurrency production systems.&lt;/p&gt;&lt;p&gt;The release includes a comprehensive set of pre-trained and instruction-tuned models available on the developer platform Hugging Face. The models come in several sizes, specifically with parameter scales of 0.5B, 1.8B, 4B, and 7B, providing substantial flexibility for developers and businesses.&lt;/p&gt;&lt;p&gt;Tencent has indicated that these models were developed using training strategies similar to its more powerful Hunyuan-A13B model, allowing them to inherit its performance characteristics. This approach enables users to select the optimal model for their needs, whether it‚Äôs a smaller variant for resource-constrained edge computing or a larger model for high-throughput production workloads, all while ensuring strong capabilities.&lt;/p&gt;&lt;p&gt;One of the most notable features of the Hunyuan series is its native support for an ultra-long 256K context window. This allows the models to handle and maintain stable performance on long-text tasks, a vital capability for complex document analysis, extended conversations, and in-depth content generation. The models support what Tencent calls ‚Äúhybrid reasoning,‚Äù which allows for both fast and slow thinking modes that users can choose between depending on their specific requirements.&lt;/p&gt;&lt;p&gt;The company has also placed a strong emphasis on agentic capabilities. The models have been optimised for agent-based tasks and have demonstrated leading results on established benchmarks such as BFCL-v3, œÑ-Bench, and C3-Bench, suggesting a high degree of proficiency in complex, multi-step problem-solving. For instance, on the C3-Bench, the Hunyuan-7B-Instruct model achieves a score of 68.5, while the Hunyuan-4B-Instruct model scores 64.3.&lt;/p&gt;&lt;p&gt;The series‚Äô performance is a focus on efficient inference. Tencent‚Äôs Hunyuan models utilise Grouped Query Attention (GQA), a technique known for improving processing speed and reducing computational overhead. This efficiency is further enhanced by advanced quantisation support, a key element of the Hunyuan architecture designed to lower deployment barriers.&lt;/p&gt;&lt;p&gt;Tencent has developed its own compression toolset, AngleSlim, to create a more user-friendly and effective model compression solution. Using this tool, the company offers two main types of quantisation for the Hunyuan series.&lt;/p&gt;&lt;p&gt;The first is FP8 static quantisation, which employs an 8-bit floating-point format. This method uses a small amount of calibration data to pre-determine the quantisation scale without requiring full retraining, converting model weights and activation values into the FP8 format to boost inference efficiency.&lt;/p&gt;&lt;p&gt;The second method is INT4 quantisation, which achieves W4A16 quantisation through the GPTQ and AWQ algorithms:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;The &lt;strong&gt;GPTQ&lt;/strong&gt; approach processes model weights layer by layer, using calibration data to minimise errors in the quantised weights. This process avoids requiring model retraining and improves inference speed.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;The &lt;strong&gt;AWQ &lt;/strong&gt;algorithm works by statistically analysing the amplitude of activation values from a small set of calibration data. It then calculates a scaling coefficient for each weight channel, which expands the numerical range of important weights to retain more information during the compression process.&amp;nbsp;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Developers can either use the AngleSlim tool themselves or download the pre-quantised models directly.&lt;/p&gt;&lt;p&gt;Performance benchmarks confirm the strong capabilities of the Tencent Hunyuan models across a range of tasks. The pre-trained Hunyuan-7B model, for example, achieves a score of 79.82 on the MMLU benchmark, 88.25 on GSM8K, and 74.85 on the MATH benchmark, demonstrating solid reasoning and mathematical skills.&lt;/p&gt;&lt;p&gt;The instruction-tuned variants show impressive results in specialised areas. In mathematics, the Hunyuan-7B-Instruct model scores 81.1 on the AIME 2024 benchmark, while the 4B version scores 78.3. In science, the 7B model reaches 76.5 on OlympiadBench, and in coding, it scores 42 on Livecodebench.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;üöÄWe're expanding the Tencent Hunyuan open-source LLM ecosystem with four compact models (0.5B, 1.8B, 4B, 7B)! Designed for low-power scenarios like consumer-grade GPUs, smart vehicles, smart home devices, mobile phones, and PCs, these models support cost-effective fine-tuning‚Ä¶ pic.twitter.com/CknskVqPem&lt;/p&gt;‚Äî Hunyuan (@TencentHunyuan) August 4, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;The quantisation benchmarks show minimal performance degradation. On the DROP benchmark, the Hunyuan-7B-Instruct model scores 85.9 in its base B16 format, 86.0 with FP8, and 85.7 with Int4 GPTQ, indicating that efficiency gains do not come at a cost to accuracy.&lt;/p&gt;&lt;p&gt;For deployment, Tencent recommends using established frameworks like TensorRT-LLM, vLLM, or SGLang to serve the Hunyuan models and create OpenAI-compatible API endpoints, ensuring they can be integrated smoothly into existing development workflows. This combination of performance, efficiency, and deployment flexibility positions the Hunyuan series as a continuing powerful contender in open-source AI.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Deep Cogito v2: Open-source AI that hones its reasoning skills&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/tencent-releases-versatile-open-source-hunyuan-ai-models/</guid><pubDate>Mon, 04 Aug 2025 14:58:20 +0000</pubDate></item><item><title>These protocols will help AI agents navigate our messy lives (Artificial intelligence ‚Äì MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/04/1120996/protocols-help-agents-navigate-lives-mcp-a2a/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/250728-AIAgents2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;A growing number of companies are launching AI agents that can do things on your behalf‚Äîactions like sending an email, making a document, or editing a database. Initial reviews for these agents have been mixed at best, though, because they struggle to interact with all the different components of our digital lives.&lt;/p&gt;  &lt;p&gt;Part of the problem is that we are still building the necessary infrastructure to help agents navigate the world. If we want agents to complete tasks for us, we need to give them the necessary tools while also making sure they use that power responsibly.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Anthropic and Google are among the companies and groups working to do those. Over the past year, they have both introduced protocols that try to define how AI agents should interact with each other and the world around them. These protocols could make it easier for agents to control other programs like email clients and note-taking apps.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The reason has to do with application programming interfaces, the connections between computers or programs that govern much of our online world. APIs currently reply to ‚Äúpings‚Äù with standardized information. But AI models aren‚Äôt made to work exactly the same every time. The very randomness that helps them come across as conversational and expressive also makes it difficult for them to both call an API and understand the response.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;‚ÄúModels speak a natural language,‚Äù says Theo Chu, a project manager at Anthropic. ‚ÄúFor [a model] to get context and do something with that context, there is a translation layer that has to happen for it to make sense to the model.‚Äù Chu works on one such translation technique, the Model Context Protocol (MCP), which Anthropic introduced at the end of last year.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;MCP attempts to standardize how AI agents interact with the world via various programs, and it‚Äôs already very popular. One web aggregator for MCP servers (essentially, the portals for different programs or tools that agents can access) lists over 15,000 servers already.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Working out how to govern how AI agents interact with each other is arguably an even steeper challenge, and it‚Äôs one the Agent2Agent protocol (A2A), introduced by Google in April, tries to take on. Whereas MCP translates requests between words and code, A2A tries to moderate exchanges between agents, which is an ‚Äúessential next step for the industry to move beyond single-purpose agents,‚Äù Rao Surapaneni, who works with A2A at Google Cloud, wrote in an email to &lt;em&gt;MIT Technology Review&lt;/em&gt;.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google says 150 companies have already partnered with it to develop and adopt A2A, including Adobe and Salesforce. At a high level, both MCP and A2A tell an AI agent what it absolutely needs to do, what it should do, and what it should not do to ensure a safe interaction with other services. In a way, they are complementary‚Äîeach agent in an A2A interaction could individually be using MCP to fetch information the other asks for.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, Chu stresses that it is ‚Äúdefinitely still early days‚Äù for MCP, and the A2A road map lists plenty of tasks still to be done. We‚Äôve identified the three main areas of growth for MCP, A2A, and other agent protocols: security, openness, and efficiency.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;What should these protocols say about security?&lt;/h3&gt;  &lt;p&gt;Researchers and developers still don‚Äôt really understand how AI models work, and new vulnerabilities are being discovered all the time. For chatbot-style AI applications, malicious attacks can cause models to do all sorts of bad things, including regurgitating training data and spouting slurs. But for AI agents, which interact with the world on someone‚Äôs behalf, the possibilities are far riskier.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For example, one AI agent, made to read and send emails for someone, has already been shown to be vulnerable to what‚Äôs known as an indirect prompt injection attack. Essentially, an email could be written in a way that hijacks the AI model and causes it to malfunction. Then, if that agent has access to the user‚Äôs files, it could be instructed to send private documents to the attacker.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Some researchers believe that protocols like MCP should prevent agents from carrying out harmful actions like this. However, it does not at the moment. ‚ÄúBasically, it does not have any security design,‚Äù says Zhaorun Chen, a&amp;nbsp; University of Chicago PhD student who works on AI agent security and uses MCP servers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Bruce Schneier, a security researcher and activist, is skeptical that protocols like MCP will be able to do much to reduce the inherent risks that come with AI and is concerned that giving such technology more power will just give it more ability to cause harm in the real, physical world. ‚ÄúWe just don‚Äôt have good answers on how to secure this stuff,‚Äù says Schneier. ‚ÄúIt‚Äôs going to be a security cesspool really fast.‚Äù&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Others are more hopeful. Security design could be added to MCP and A2A similar to the way it is for internet protocols like HTTPS (though the nature of attacks on AI systems is very different). And Chen and Anthropic believe that standardizing protocols like MCP and A2A can help make it easier to catch and resolve security issues even as is. Chen uses MCP in his research to test the roles different programs can play in attacks to better understand vulnerabilities. Chu at Anthropic believes that these tools could let cybersecurity companies more easily deal with attacks against agents, because it will be easier to unpack who sent what.&amp;nbsp;&lt;/p&gt;    &lt;h3 class="wp-block-heading"&gt;How open should these protocols be?&lt;/h3&gt;  &lt;p&gt;Although MCP and A2A are two of the most popular agent protocols available today, there are plenty of others in the works. Large companies like Cisco and IBM are working on their own protocols, and other groups have put forth different designs like Agora, designed by researchers at the University of Oxford, which upgrades an agent-service communication from human language to structured data in real time.&lt;/p&gt;  &lt;p&gt;Many developers hope there could eventually be a registry of safe, trusted systems to navigate the proliferation of agents and tools. Others, including Chen, want users to be able to rate different services in something like a Yelp for AI agent tools. Some more niche protocols have even built blockchains on top of MCP and A2A so that servers can show they are not just spam.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Both MCP and A2A are open-source, which is common for would-be standards as it lets others work on building them. This can help protocols develop faster and more transparently.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúIf we go build something together, we spend less time overall, because we‚Äôre not having to each reinvent the wheel,‚Äù says David Nalley, who leads developer experience at Amazon Web Services and works with a lot of open-source systems, including A2A and MCP.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Nalley oversaw Google‚Äôs donation of A2A to the Linux Foundation, a nonprofit organization that guides open-source projects, back in June. With the foundation‚Äôs stewardship, the developers who work on A2A (including employees at Google and many others) all get a say in how it should evolve. MCP, on the other hand, is owned by Anthropic and licensed for free. That is a sticking point for some open-source advocates, who want others to have a say in how the code base itself is developed.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúThere‚Äôs admittedly some increased risk around a single person or a single entity being in absolute control,‚Äù says Nalley. He says most people would prefer multiple groups to have a ‚Äúseat at the table‚Äù to make sure that these protocols are serving everyone‚Äôs best interests.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;However, Nalley does believe Anthropic is acting in good faith‚Äîits license, he says, is incredibly permissive, allowing other groups to create their own modified versions of the code (a process known as ‚Äúforking‚Äù).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúSomeone could fork it if they needed to, if something went completely off the rails,‚Äù says Nalley. IBM‚Äôs Agent Communication Protocol was actually spun off of MCP.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Anthropic is still deciding exactly how to develop MCP. For now, it works with a steering committee of outside companies that help make decisions on MCP‚Äôs development, but Anthropic seems open to changing this approach. ‚ÄúWe are looking to evolve how we think about both ownership and governance in the future,‚Äù says Chu.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;h3 class="wp-block-heading"&gt;Is natural language fast enough?&lt;/h3&gt;  &lt;p&gt;MCP and A2A work on the agents‚Äô terms‚Äîthey use words and phrases (termed natural language in AI), just as AI models do when they are responding to a person. This is part of the selling point for these protocols, because it means the model doesn‚Äôt have to be trained to talk in a way that is unnatural to it. ‚ÄúAllowing a natural-language interface to be used between agents and not just with humans unlocks sharing the intelligence that is built into these agents,‚Äù says Surapaneni.&lt;/p&gt;  &lt;p&gt;But this choice does come with drawbacks. Natural-language interfaces lack the precision of APIs, and that could result in incorrect responses. And it creates inefficiencies.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;Usually, an AI model reads and responds to text by splitting words into tokens. The AI model will read a prompt, split it into input tokens, generate a response in the form of output tokens, and then put these tokens into words to send back. These tokens define in some sense how much work the AI model has to do‚Äîthat‚Äôs why most AI platforms charge users according to the number of tokens used.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;But the whole point of working in tokens is so that people can understand the output‚Äîit‚Äôs usually faster and more efficient for machine-to-machine communication to just work over code. MCP and A2A both work in natural language, so they require the model to spend tokens as the agent talks to other machines, like tools and other agents. The user never even sees these exchanges‚Äîall the effort of making everything human-readable doesn‚Äôt ever get read by a human. ‚ÄúYou waste a lot of tokens if you want to use MCP,‚Äù says Chen.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Chen describes this process as potentially very costly. For example, suppose the user wants the agent to read a document and summarize it. If the agent uses another program to summarize here, it needs to read the document, write the document to the program, read back the summary, and write it back to the user. Since the agent needed to read and write everything, both the document and the summary get doubled up. According to Chen, ‚ÄúIt‚Äôs actually a lot of tokens.‚Äù&lt;/p&gt;  &lt;p&gt;As with so many aspects of MCP and A2A‚Äôs designs, their benefits also create new challenges. ‚ÄúThere‚Äôs a long way to go if we want to scale up and actually make them useful,‚Äù says Chen.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/250728-AIAgents2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;A growing number of companies are launching AI agents that can do things on your behalf‚Äîactions like sending an email, making a document, or editing a database. Initial reviews for these agents have been mixed at best, though, because they struggle to interact with all the different components of our digital lives.&lt;/p&gt;  &lt;p&gt;Part of the problem is that we are still building the necessary infrastructure to help agents navigate the world. If we want agents to complete tasks for us, we need to give them the necessary tools while also making sure they use that power responsibly.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Anthropic and Google are among the companies and groups working to do those. Over the past year, they have both introduced protocols that try to define how AI agents should interact with each other and the world around them. These protocols could make it easier for agents to control other programs like email clients and note-taking apps.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The reason has to do with application programming interfaces, the connections between computers or programs that govern much of our online world. APIs currently reply to ‚Äúpings‚Äù with standardized information. But AI models aren‚Äôt made to work exactly the same every time. The very randomness that helps them come across as conversational and expressive also makes it difficult for them to both call an API and understand the response.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;‚ÄúModels speak a natural language,‚Äù says Theo Chu, a project manager at Anthropic. ‚ÄúFor [a model] to get context and do something with that context, there is a translation layer that has to happen for it to make sense to the model.‚Äù Chu works on one such translation technique, the Model Context Protocol (MCP), which Anthropic introduced at the end of last year.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;MCP attempts to standardize how AI agents interact with the world via various programs, and it‚Äôs already very popular. One web aggregator for MCP servers (essentially, the portals for different programs or tools that agents can access) lists over 15,000 servers already.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Working out how to govern how AI agents interact with each other is arguably an even steeper challenge, and it‚Äôs one the Agent2Agent protocol (A2A), introduced by Google in April, tries to take on. Whereas MCP translates requests between words and code, A2A tries to moderate exchanges between agents, which is an ‚Äúessential next step for the industry to move beyond single-purpose agents,‚Äù Rao Surapaneni, who works with A2A at Google Cloud, wrote in an email to &lt;em&gt;MIT Technology Review&lt;/em&gt;.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google says 150 companies have already partnered with it to develop and adopt A2A, including Adobe and Salesforce. At a high level, both MCP and A2A tell an AI agent what it absolutely needs to do, what it should do, and what it should not do to ensure a safe interaction with other services. In a way, they are complementary‚Äîeach agent in an A2A interaction could individually be using MCP to fetch information the other asks for.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, Chu stresses that it is ‚Äúdefinitely still early days‚Äù for MCP, and the A2A road map lists plenty of tasks still to be done. We‚Äôve identified the three main areas of growth for MCP, A2A, and other agent protocols: security, openness, and efficiency.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;What should these protocols say about security?&lt;/h3&gt;  &lt;p&gt;Researchers and developers still don‚Äôt really understand how AI models work, and new vulnerabilities are being discovered all the time. For chatbot-style AI applications, malicious attacks can cause models to do all sorts of bad things, including regurgitating training data and spouting slurs. But for AI agents, which interact with the world on someone‚Äôs behalf, the possibilities are far riskier.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For example, one AI agent, made to read and send emails for someone, has already been shown to be vulnerable to what‚Äôs known as an indirect prompt injection attack. Essentially, an email could be written in a way that hijacks the AI model and causes it to malfunction. Then, if that agent has access to the user‚Äôs files, it could be instructed to send private documents to the attacker.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Some researchers believe that protocols like MCP should prevent agents from carrying out harmful actions like this. However, it does not at the moment. ‚ÄúBasically, it does not have any security design,‚Äù says Zhaorun Chen, a&amp;nbsp; University of Chicago PhD student who works on AI agent security and uses MCP servers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Bruce Schneier, a security researcher and activist, is skeptical that protocols like MCP will be able to do much to reduce the inherent risks that come with AI and is concerned that giving such technology more power will just give it more ability to cause harm in the real, physical world. ‚ÄúWe just don‚Äôt have good answers on how to secure this stuff,‚Äù says Schneier. ‚ÄúIt‚Äôs going to be a security cesspool really fast.‚Äù&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Others are more hopeful. Security design could be added to MCP and A2A similar to the way it is for internet protocols like HTTPS (though the nature of attacks on AI systems is very different). And Chen and Anthropic believe that standardizing protocols like MCP and A2A can help make it easier to catch and resolve security issues even as is. Chen uses MCP in his research to test the roles different programs can play in attacks to better understand vulnerabilities. Chu at Anthropic believes that these tools could let cybersecurity companies more easily deal with attacks against agents, because it will be easier to unpack who sent what.&amp;nbsp;&lt;/p&gt;    &lt;h3 class="wp-block-heading"&gt;How open should these protocols be?&lt;/h3&gt;  &lt;p&gt;Although MCP and A2A are two of the most popular agent protocols available today, there are plenty of others in the works. Large companies like Cisco and IBM are working on their own protocols, and other groups have put forth different designs like Agora, designed by researchers at the University of Oxford, which upgrades an agent-service communication from human language to structured data in real time.&lt;/p&gt;  &lt;p&gt;Many developers hope there could eventually be a registry of safe, trusted systems to navigate the proliferation of agents and tools. Others, including Chen, want users to be able to rate different services in something like a Yelp for AI agent tools. Some more niche protocols have even built blockchains on top of MCP and A2A so that servers can show they are not just spam.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Both MCP and A2A are open-source, which is common for would-be standards as it lets others work on building them. This can help protocols develop faster and more transparently.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúIf we go build something together, we spend less time overall, because we‚Äôre not having to each reinvent the wheel,‚Äù says David Nalley, who leads developer experience at Amazon Web Services and works with a lot of open-source systems, including A2A and MCP.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Nalley oversaw Google‚Äôs donation of A2A to the Linux Foundation, a nonprofit organization that guides open-source projects, back in June. With the foundation‚Äôs stewardship, the developers who work on A2A (including employees at Google and many others) all get a say in how it should evolve. MCP, on the other hand, is owned by Anthropic and licensed for free. That is a sticking point for some open-source advocates, who want others to have a say in how the code base itself is developed.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúThere‚Äôs admittedly some increased risk around a single person or a single entity being in absolute control,‚Äù says Nalley. He says most people would prefer multiple groups to have a ‚Äúseat at the table‚Äù to make sure that these protocols are serving everyone‚Äôs best interests.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;However, Nalley does believe Anthropic is acting in good faith‚Äîits license, he says, is incredibly permissive, allowing other groups to create their own modified versions of the code (a process known as ‚Äúforking‚Äù).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúSomeone could fork it if they needed to, if something went completely off the rails,‚Äù says Nalley. IBM‚Äôs Agent Communication Protocol was actually spun off of MCP.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Anthropic is still deciding exactly how to develop MCP. For now, it works with a steering committee of outside companies that help make decisions on MCP‚Äôs development, but Anthropic seems open to changing this approach. ‚ÄúWe are looking to evolve how we think about both ownership and governance in the future,‚Äù says Chu.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;h3 class="wp-block-heading"&gt;Is natural language fast enough?&lt;/h3&gt;  &lt;p&gt;MCP and A2A work on the agents‚Äô terms‚Äîthey use words and phrases (termed natural language in AI), just as AI models do when they are responding to a person. This is part of the selling point for these protocols, because it means the model doesn‚Äôt have to be trained to talk in a way that is unnatural to it. ‚ÄúAllowing a natural-language interface to be used between agents and not just with humans unlocks sharing the intelligence that is built into these agents,‚Äù says Surapaneni.&lt;/p&gt;  &lt;p&gt;But this choice does come with drawbacks. Natural-language interfaces lack the precision of APIs, and that could result in incorrect responses. And it creates inefficiencies.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;Usually, an AI model reads and responds to text by splitting words into tokens. The AI model will read a prompt, split it into input tokens, generate a response in the form of output tokens, and then put these tokens into words to send back. These tokens define in some sense how much work the AI model has to do‚Äîthat‚Äôs why most AI platforms charge users according to the number of tokens used.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;But the whole point of working in tokens is so that people can understand the output‚Äîit‚Äôs usually faster and more efficient for machine-to-machine communication to just work over code. MCP and A2A both work in natural language, so they require the model to spend tokens as the agent talks to other machines, like tools and other agents. The user never even sees these exchanges‚Äîall the effort of making everything human-readable doesn‚Äôt ever get read by a human. ‚ÄúYou waste a lot of tokens if you want to use MCP,‚Äù says Chen.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Chen describes this process as potentially very costly. For example, suppose the user wants the agent to read a document and summarize it. If the agent uses another program to summarize here, it needs to read the document, write the document to the program, read back the summary, and write it back to the user. Since the agent needed to read and write everything, both the document and the summary get doubled up. According to Chen, ‚ÄúIt‚Äôs actually a lot of tokens.‚Äù&lt;/p&gt;  &lt;p&gt;As with so many aspects of MCP and A2A‚Äôs designs, their benefits also create new challenges. ‚ÄúThere‚Äôs a long way to go if we want to scale up and actually make them useful,‚Äù says Chen.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/04/1120996/protocols-help-agents-navigate-lives-mcp-a2a/</guid><pubDate>Mon, 04 Aug 2025 15:00:13 +0000</pubDate></item><item><title>Grok Imagine, xAI‚Äôs new AI image and video generator, lets you make NSFW content (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/04/grok-imagine-xais-new-ai-image-and-video-generator-lets-you-make-nsfw-content/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2207699717.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk‚Äôs AI company has officially rolled out Grok Imagine, xAI‚Äôs image and video generator, to all SuperGrok and Premium+ X subscribers on its iOS app. And true to form for Musk, who positions Grok as an unfiltered, boundary-pushing AI, the generator allows users to make NSFW content.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Grok Imagine, which promises to turn text or image prompts into a 15-second video featuring native audio, has a ‚Äúspicy mode‚Äù that allows users to generate sexually explicit content, including partial female nudity . There are limits to how explicit one can get. Many of our spicier prompts ‚Äî made in the name of Journalism! ‚Äî generate blurred-out images that are ‚Äúmoderated‚Äù and therefore inaccessible. We were, however, able to generate semi-nude imagery.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The NSFW content is unsurprising for xAI, given the release last month of a raunchy, hyper-sexualized anime AI companion. But just as Grok‚Äôs unrestrained nature was entertaining until it started spewing hateful, antisemitic, misogynistic content, Grok Imagine could be poised to bring its own set of unintended consequences.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;CNBC first reported the existence of a ‚Äúspicy mode‚Äù last week after xAI employee Mati Roy said in a now-deleted post on X: ‚ÄúGrok Imagine videos have a spicy mode that can do nudity.‚Äù&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI for more information.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The existing limitations with Grok Imagine are somewhat heartening given the model also lets you create content of celebrities ‚Äî anyone from Donald Trump to Taylor Swift ‚Äî and there appear to be additional restrictions on those. For example, TechCrunch tried, and failed, to generate an image of Trump pregnant. Grok Imagine only generated images of Trump holding a baby or next to a pregnant woman.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While it‚Äôs still early days for Grok Imagine, which aims to compete with incumbents like Google DeepMind, OpenAI, Runway, and Chinese rivals, the images and videos generated of humans are still a bit lost in the uncanny valley, with waxy-looking skin that verges on cartoonish at times.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the generator is impressive. It produces images in seconds from a text prompt, and continues to auto-generate new images as you scroll. Those images can then be animated into stylized videos. The user interface is also seamless and intuitive.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk said on X that the model would ‚Äúget better every day.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2207699717.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk‚Äôs AI company has officially rolled out Grok Imagine, xAI‚Äôs image and video generator, to all SuperGrok and Premium+ X subscribers on its iOS app. And true to form for Musk, who positions Grok as an unfiltered, boundary-pushing AI, the generator allows users to make NSFW content.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Grok Imagine, which promises to turn text or image prompts into a 15-second video featuring native audio, has a ‚Äúspicy mode‚Äù that allows users to generate sexually explicit content, including partial female nudity . There are limits to how explicit one can get. Many of our spicier prompts ‚Äî made in the name of Journalism! ‚Äî generate blurred-out images that are ‚Äúmoderated‚Äù and therefore inaccessible. We were, however, able to generate semi-nude imagery.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The NSFW content is unsurprising for xAI, given the release last month of a raunchy, hyper-sexualized anime AI companion. But just as Grok‚Äôs unrestrained nature was entertaining until it started spewing hateful, antisemitic, misogynistic content, Grok Imagine could be poised to bring its own set of unintended consequences.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;CNBC first reported the existence of a ‚Äúspicy mode‚Äù last week after xAI employee Mati Roy said in a now-deleted post on X: ‚ÄúGrok Imagine videos have a spicy mode that can do nudity.‚Äù&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI for more information.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The existing limitations with Grok Imagine are somewhat heartening given the model also lets you create content of celebrities ‚Äî anyone from Donald Trump to Taylor Swift ‚Äî and there appear to be additional restrictions on those. For example, TechCrunch tried, and failed, to generate an image of Trump pregnant. Grok Imagine only generated images of Trump holding a baby or next to a pregnant woman.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While it‚Äôs still early days for Grok Imagine, which aims to compete with incumbents like Google DeepMind, OpenAI, Runway, and Chinese rivals, the images and videos generated of humans are still a bit lost in the uncanny valley, with waxy-looking skin that verges on cartoonish at times.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the generator is impressive. It produces images in seconds from a text prompt, and continues to auto-generate new images as you scroll. Those images can then be animated into stylized videos. The user interface is also seamless and intuitive.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk said on X that the model would ‚Äúget better every day.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/04/grok-imagine-xais-new-ai-image-and-video-generator-lets-you-make-nsfw-content/</guid><pubDate>Mon, 04 Aug 2025 15:04:18 +0000</pubDate></item><item><title>OpenAI says ChatGPT is on track to reach 700M weekly users (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/04/openai-says-chatgpt-is-on-track-to-reach-700m-weekly-users/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2205105208.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT‚Äôs impressive growth as a consumer app continues as the chatbot is on track to hit 700 million weekly active users this week, the company says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app had earlier reached 500 million weekly active users as of the end of March, noted Nick Turley, OpenAI VP and head of ChatGPT‚Äôs app, in a post on X. He also said the app has grown 4x since last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúEvery day, people and teams are learning, creating, and solving harder problems. Big week ahead. Grateful to the team for making ChatGPT more useful and delivering on our mission so everyone can benefit from AI,‚Äù he posted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app‚Äôs popularity increased after OpenAI launched an upgraded image-generation feature, powered by the GPT-4 model, in March. In early April, the company‚Äôs COO, Brad Lightcap, said that more than 130 million users had created over 700 million images in just a few days after the launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also seen an increase in subscribers. Last week, Lightcap said that ChatGPT had 5 million paying business users, up from 3 million in June.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent report, market intelligence firm Sensor Tower noted that users are using ChatGPT for more than 12 days a month on average, only behind Google and X. The report also said that in H1 2025, users spent an average of 16 minutes per day on the app.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2205105208.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT‚Äôs impressive growth as a consumer app continues as the chatbot is on track to hit 700 million weekly active users this week, the company says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app had earlier reached 500 million weekly active users as of the end of March, noted Nick Turley, OpenAI VP and head of ChatGPT‚Äôs app, in a post on X. He also said the app has grown 4x since last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúEvery day, people and teams are learning, creating, and solving harder problems. Big week ahead. Grateful to the team for making ChatGPT more useful and delivering on our mission so everyone can benefit from AI,‚Äù he posted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app‚Äôs popularity increased after OpenAI launched an upgraded image-generation feature, powered by the GPT-4 model, in March. In early April, the company‚Äôs COO, Brad Lightcap, said that more than 130 million users had created over 700 million images in just a few days after the launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also seen an increase in subscribers. Last week, Lightcap said that ChatGPT had 5 million paying business users, up from 3 million in June.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent report, market intelligence firm Sensor Tower noted that users are using ChatGPT for more than 12 days a month on average, only behind Google and X. The report also said that in H1 2025, users spent an average of 16 minutes per day on the app.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/04/openai-says-chatgpt-is-on-track-to-reach-700m-weekly-users/</guid><pubDate>Mon, 04 Aug 2025 15:25:13 +0000</pubDate></item><item><title>Perplexity accused of scraping websites that explicitly blocked AI scraping (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/04/perplexity-accused-of-scraping-websites-that-explicitly-blocked-ai-scraping/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2181996346.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI startup Perplexity is crawling and scraping content from websites that have explicitly indicated they don‚Äôt want to be scraped, according to internet infrastructure provider Cloudflare.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, Cloudflare published research saying it observed the AI startup ignore blocks and hide its crawling and scraping activities. The network infrastructure giant accused Perplexity of obscuring its identity when trying to scrape web pages ‚Äúin an attempt to circumvent the website‚Äôs preferences,‚Äù Cloudflare‚Äôs researchers wrote.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI products like those offered by Perplexity rely on gobbling up large amounts of data from the internet, and AI startups have long scraped text, images, and videos from the internet many times without permission to make their products work. In recent times, websites have tried to fight back by using the web standard Robots.txt file, which tells search engines and AI companies which pages can be indexed and which shouldn‚Äôt, efforts that have seen mixed results so far.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity appears to be willingly circumventing these blocks by changing its bots‚Äô ‚Äúuser agent,‚Äù meaning a signal that identifies a website visitor by their device and version type, as well as changing their autonomous system networks, or ASN, essentially a number that identifies large networks on the internet, according to Cloudflare.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThis activity was observed across tens of thousands of domains and millions of requests per day. We were able to fingerprint this crawler using a combination of machine learning and network signals,‚Äù read Cloudflare‚Äôs post.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity spokesperson Jesse Dwyer dismissed Cloudflare‚Äôs blog post as a ‚Äúsales pitch,‚Äù adding in an email to TechCrunch that the screenshots in the post ‚Äúshow that no content was accessed.‚Äù In a follow-up email, Dwyer claimed the bot named in the Cloudflare blog ‚Äúisn‚Äôt even ours.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare said it first noticed the behavior after its customers complained that Perplexity was crawling and scraping their sites, even after they added rules on their Robots file and for specifically blocking Perplexity‚Äôs known bots. Cloudflare said it then performed tests to check and confirmed that Perplexity was circumventing these blocks.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe observed that Perplexity uses not only their declared user-agent, but also a generic browser intended to impersonate Google Chrome on macOS when their declared crawler was blocked,‚Äù according to Cloudflare.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also said that it has de-listed Perplexity‚Äôs bots from its verified list and added new techniques to block them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare has recently taken a public stance against AI crawlers. Last month, Cloudflare announced the launch of a marketplace allowing website owners and publishers to charge AI scrapers who visit their sites. Cloudflare‚Äôs chief executive Matthew Prince sounded the alarm at the time, saying AI is breaking the business model of the internet, particularly publishers. Last year, Cloudflare also launched a free tool to prevent bots from scraping websites to train AI.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This is not the first time Perplexity is accused of scraping without authorization.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, news outlets, such as Wired, alleged Perplexity was plagiarizing their content. Weeks later, Perplexity‚Äôs CEO Aravind Srinivas was unable to immediately answer when asked to provide the company‚Äôs definition of plagiarism during an interview with TechCrunch‚Äôs Devin Coldewey at the Disrupt 2024 conference.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2181996346.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI startup Perplexity is crawling and scraping content from websites that have explicitly indicated they don‚Äôt want to be scraped, according to internet infrastructure provider Cloudflare.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, Cloudflare published research saying it observed the AI startup ignore blocks and hide its crawling and scraping activities. The network infrastructure giant accused Perplexity of obscuring its identity when trying to scrape web pages ‚Äúin an attempt to circumvent the website‚Äôs preferences,‚Äù Cloudflare‚Äôs researchers wrote.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI products like those offered by Perplexity rely on gobbling up large amounts of data from the internet, and AI startups have long scraped text, images, and videos from the internet many times without permission to make their products work. In recent times, websites have tried to fight back by using the web standard Robots.txt file, which tells search engines and AI companies which pages can be indexed and which shouldn‚Äôt, efforts that have seen mixed results so far.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity appears to be willingly circumventing these blocks by changing its bots‚Äô ‚Äúuser agent,‚Äù meaning a signal that identifies a website visitor by their device and version type, as well as changing their autonomous system networks, or ASN, essentially a number that identifies large networks on the internet, according to Cloudflare.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThis activity was observed across tens of thousands of domains and millions of requests per day. We were able to fingerprint this crawler using a combination of machine learning and network signals,‚Äù read Cloudflare‚Äôs post.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity spokesperson Jesse Dwyer dismissed Cloudflare‚Äôs blog post as a ‚Äúsales pitch,‚Äù adding in an email to TechCrunch that the screenshots in the post ‚Äúshow that no content was accessed.‚Äù In a follow-up email, Dwyer claimed the bot named in the Cloudflare blog ‚Äúisn‚Äôt even ours.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare said it first noticed the behavior after its customers complained that Perplexity was crawling and scraping their sites, even after they added rules on their Robots file and for specifically blocking Perplexity‚Äôs known bots. Cloudflare said it then performed tests to check and confirmed that Perplexity was circumventing these blocks.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe observed that Perplexity uses not only their declared user-agent, but also a generic browser intended to impersonate Google Chrome on macOS when their declared crawler was blocked,‚Äù according to Cloudflare.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also said that it has de-listed Perplexity‚Äôs bots from its verified list and added new techniques to block them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare has recently taken a public stance against AI crawlers. Last month, Cloudflare announced the launch of a marketplace allowing website owners and publishers to charge AI scrapers who visit their sites. Cloudflare‚Äôs chief executive Matthew Prince sounded the alarm at the time, saying AI is breaking the business model of the internet, particularly publishers. Last year, Cloudflare also launched a free tool to prevent bots from scraping websites to train AI.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This is not the first time Perplexity is accused of scraping without authorization.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, news outlets, such as Wired, alleged Perplexity was plagiarizing their content. Weeks later, Perplexity‚Äôs CEO Aravind Srinivas was unable to immediately answer when asked to provide the company‚Äôs definition of plagiarism during an interview with TechCrunch‚Äôs Devin Coldewey at the Disrupt 2024 conference.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/04/perplexity-accused-of-scraping-websites-that-explicitly-blocked-ai-scraping/</guid><pubDate>Mon, 04 Aug 2025 15:41:39 +0000</pubDate></item><item><title>Rethinking how we measure AI intelligence (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/rethinking-how-we-measure-ai-intelligence/</link><description>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="A stylized illustration showing elements of various strategy games. A large chess queen, playing cards, and a Go board are displayed next to a list, representing strategic analysis." class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/game_arena_keyword_blog_header.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
              







            

            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Current AI benchmarks are struggling to keep pace with modern models. As helpful as they are to measure model performance on specific tasks, it can be hard to know if models trained on internet data are actually solving problems or just remembering answers they've already seen. As models reach closer to 100% on certain benchmarks, they also become less effective at revealing meaningful performance differences. We continue to invest in new and more challenging benchmarks, but on the path to general intelligence, we need to continue to look for new ways to evaluate. The more recent shift towards dynamic, human-judged testing solves these issues of memorization and saturation, but in turn, creates new difficulties stemming from the inherent subjectivity of human preferences.&lt;/p&gt;&lt;p&gt;While we continue to evolve and pursue current AI benchmarks, we‚Äôre also consistently looking to test new approaches to evaluating models. That‚Äôs why today, we're introducing the Kaggle Game Arena: a new, public AI benchmarking platform where AI models compete head-to-head in strategic games, providing a verifiable, and dynamic measure of their capabilities.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Why games are a meaningful evaluation benchmark&lt;/h2&gt;&lt;p&gt;Games provide a clear, unambiguous signal of success. Their structured nature and measurable outcomes make them the perfect testbed for evaluating models and agents. They force models to demonstrate many skills including strategic reasoning, long-term planning and dynamic adaptation against an intelligent opponent, providing a robust signal of their general problem-solving intelligence. The value of games as a benchmark is further enhanced by their scalability‚Äîdifficulty increases with the opponent's intelligence‚Äîand by our ability to inspect and visualize a model's "reasoning," which offers a glimpse into its strategic thought process.&lt;/p&gt;&lt;p&gt;Specialized engines like Stockfish and general game playing AI models like AlphaZero have been able to play games at a superhuman level for many years and would beat every frontier model without a doubt. Today‚Äôs large language models, however, are not built to specialize in any specific games, and as a result they do not play them nearly as well. While the immediate challenge for the models is to close this gap, in the long-term we would hope for them to achieve a level of play beyond what is currently possible. And with an endlessly increasing set of novel environments we can continue to challenge them even further.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How Game Arena promotes fair and open evaluation&lt;/h2&gt;&lt;p&gt;Game Arena is built on Kaggle to provide a fair, standardized environment for model evaluation. For transparency, game harnesses ‚Äî the frameworks that connect each AI model to the game environment and enforce the rules ‚Äî as well as the game environments are all open-sourced. Final rankings are determined by a rigorous all-play-all system, where an extensive number of matches between each model pair ensures a statistically robust result.&lt;/p&gt;&lt;p&gt;Google DeepMind has long used games as a benchmark, from Atari to AlphaGo and AlphaStar, to demonstrate complex AI capabilities. By testing these models in a competitive arena, we can establish a clear baseline for their strategic reasoning and track progress. The goal is to build an ever-expanding benchmark that grows in difficulty as models face tougher competition. Over time, this could lead to novel strategies, much like AlphaGo's famous and creative ‚ÄúMove 37‚Äù that baffled human experts. The ability to plan, adapt and reason under pressure in a game is analogous to the thinking needed to solve complex challenges in science and business.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How you can watch the chess exhibition matches&lt;/h2&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;On August 5 at 10:30 a.m. Pacific Time, join us for a special chess exhibition where eight frontier models will face off in a single elimination showdown. We selected a sample from the matches for this exhibition. Hosted by the world's best chess experts, this event is the premiere demonstration of the Game Arena methodology.&lt;/p&gt;&lt;p&gt;While the fun exhibition matches are in a tournament format, the final leaderboard rankings will be determined by the all-play-all system and released after the exhibition. This more extensive method runs over a hundred matches between every pair of models to ensure a statistically robust and definitive measure of performance. You can find more details and how to watch the games at kaggle.com/game-arena.&lt;/p&gt;&lt;p&gt;We plan to run more tournaments in the future on a regular basis, more on that soon.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How we‚Äôre building the future of AI benchmarks&lt;/h2&gt;&lt;p&gt;This is only the beginning. Our vision for the Game Arena extends far beyond a single game. Kaggle will soon expand Game Arena with new challenges, starting with classics like Go and poker. These games, along with future additions like video games, are excellent tests of AI‚Äôs ability to perform long-horizon planning and reasoning, helping us create a comprehensive and ever-evolving benchmark for AI. We‚Äôre committed to continuously adding new models and harnesses to the mix, pushing the boundaries of what AI models can achieve. For more details about the Game Arena and the inaugural chess exhibition tournament, see Kaggle‚Äôs blog post.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</description><content:encoded>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="A stylized illustration showing elements of various strategy games. A large chess queen, playing cards, and a Go board are displayed next to a list, representing strategic analysis." class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/game_arena_keyword_blog_header.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
              







            

            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Current AI benchmarks are struggling to keep pace with modern models. As helpful as they are to measure model performance on specific tasks, it can be hard to know if models trained on internet data are actually solving problems or just remembering answers they've already seen. As models reach closer to 100% on certain benchmarks, they also become less effective at revealing meaningful performance differences. We continue to invest in new and more challenging benchmarks, but on the path to general intelligence, we need to continue to look for new ways to evaluate. The more recent shift towards dynamic, human-judged testing solves these issues of memorization and saturation, but in turn, creates new difficulties stemming from the inherent subjectivity of human preferences.&lt;/p&gt;&lt;p&gt;While we continue to evolve and pursue current AI benchmarks, we‚Äôre also consistently looking to test new approaches to evaluating models. That‚Äôs why today, we're introducing the Kaggle Game Arena: a new, public AI benchmarking platform where AI models compete head-to-head in strategic games, providing a verifiable, and dynamic measure of their capabilities.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;Why games are a meaningful evaluation benchmark&lt;/h2&gt;&lt;p&gt;Games provide a clear, unambiguous signal of success. Their structured nature and measurable outcomes make them the perfect testbed for evaluating models and agents. They force models to demonstrate many skills including strategic reasoning, long-term planning and dynamic adaptation against an intelligent opponent, providing a robust signal of their general problem-solving intelligence. The value of games as a benchmark is further enhanced by their scalability‚Äîdifficulty increases with the opponent's intelligence‚Äîand by our ability to inspect and visualize a model's "reasoning," which offers a glimpse into its strategic thought process.&lt;/p&gt;&lt;p&gt;Specialized engines like Stockfish and general game playing AI models like AlphaZero have been able to play games at a superhuman level for many years and would beat every frontier model without a doubt. Today‚Äôs large language models, however, are not built to specialize in any specific games, and as a result they do not play them nearly as well. While the immediate challenge for the models is to close this gap, in the long-term we would hope for them to achieve a level of play beyond what is currently possible. And with an endlessly increasing set of novel environments we can continue to challenge them even further.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How Game Arena promotes fair and open evaluation&lt;/h2&gt;&lt;p&gt;Game Arena is built on Kaggle to provide a fair, standardized environment for model evaluation. For transparency, game harnesses ‚Äî the frameworks that connect each AI model to the game environment and enforce the rules ‚Äî as well as the game environments are all open-sourced. Final rankings are determined by a rigorous all-play-all system, where an extensive number of matches between each model pair ensures a statistically robust result.&lt;/p&gt;&lt;p&gt;Google DeepMind has long used games as a benchmark, from Atari to AlphaGo and AlphaStar, to demonstrate complex AI capabilities. By testing these models in a competitive arena, we can establish a clear baseline for their strategic reasoning and track progress. The goal is to build an ever-expanding benchmark that grows in difficulty as models face tougher competition. Over time, this could lead to novel strategies, much like AlphaGo's famous and creative ‚ÄúMove 37‚Äù that baffled human experts. The ability to plan, adapt and reason under pressure in a game is analogous to the thinking needed to solve complex challenges in science and business.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How you can watch the chess exhibition matches&lt;/h2&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;On August 5 at 10:30 a.m. Pacific Time, join us for a special chess exhibition where eight frontier models will face off in a single elimination showdown. We selected a sample from the matches for this exhibition. Hosted by the world's best chess experts, this event is the premiere demonstration of the Game Arena methodology.&lt;/p&gt;&lt;p&gt;While the fun exhibition matches are in a tournament format, the final leaderboard rankings will be determined by the all-play-all system and released after the exhibition. This more extensive method runs over a hundred matches between every pair of models to ensure a statistically robust and definitive measure of performance. You can find more details and how to watch the games at kaggle.com/game-arena.&lt;/p&gt;&lt;p&gt;We plan to run more tournaments in the future on a regular basis, more on that soon.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How we‚Äôre building the future of AI benchmarks&lt;/h2&gt;&lt;p&gt;This is only the beginning. Our vision for the Game Arena extends far beyond a single game. Kaggle will soon expand Game Arena with new challenges, starting with classics like Go and poker. These games, along with future additions like video games, are excellent tests of AI‚Äôs ability to perform long-horizon planning and reasoning, helping us create a comprehensive and ever-evolving benchmark for AI. We‚Äôre committed to continuously adding new models and harnesses to the mix, pushing the boundaries of what AI models can achieve. For more details about the Game Arena and the inaugural chess exhibition tournament, see Kaggle‚Äôs blog post.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/rethinking-how-we-measure-ai-intelligence/</guid><pubDate>Mon, 04 Aug 2025 16:07:18 +0000</pubDate></item><item><title>[NEW] Qwen-Image is a powerful, open source new AI image generator with support for embedded text in English &amp; Chinese (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/qwen-image-is-a-powerful-open-source-new-ai-image-generator-with-support-for-embedded-text-in-english-chinese/</link><description>&lt;p&gt;After seizing the summer with a blitz of powerful, freely available new open source language and coding focused AI models that matched or in some cases bested closed-source/proprietary U.S. rivals,&lt;strong&gt; Alibaba‚Äôs crack ‚ÄúQwen Team‚Äù of AI researchers is back again today with the release of a highly ranked new AI image generator model &lt;/strong&gt;‚Äî also open source.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Qwen-Image stands out in a crowded field of generative image models&lt;/strong&gt; due to its &lt;strong&gt;emphasis on rendering text accurately within visuals&lt;/strong&gt; ‚Äî an area where many rivals still struggle. &lt;/p&gt;&lt;p&gt;Supporting both alphabetic and logographic scripts, the model is particularly adept at managing complex typography, multi-line layouts, paragraph-level semantics, and&lt;strong&gt; bilingual content (e.g., English-Chinese).&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In practice, this allows users to &lt;strong&gt;generate content like movie posters, presentation slides, storefront scenes, handwritten poetry, and stylized infographics&lt;/strong&gt; ‚Äî with crisp text that aligns with their prompts.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Qwen-Image‚Äôs output examples include a wide variety of real-world use cases:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Marketing &amp;amp; Branding&lt;/strong&gt;: Bilingual posters with brand logos, stylistic calligraphy, and consistent design motifs&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Presentation Design&lt;/strong&gt;: Layout-aware slide decks with title hierarchies and theme-appropriate visuals&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Education&lt;/strong&gt;: Generation of classroom materials featuring diagrams and precisely rendered instructional text&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Retail &amp;amp; E-commerce&lt;/strong&gt;: Storefront scenes where product labels, signage, and environmental context must all be readable&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Creative Content&lt;/strong&gt;: Handwritten poetry, scene narratives, anime-style illustration with embedded story text&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Users can interact with the model on the Qwen Chat website by selecting ‚ÄúImage Generation‚Äù mode from the buttons below the prompt entry field.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015099" height="328" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-04-at-1.42.41%E2%80%AFPM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;However, my brief initial tests revealed the text and prompt adherence was not noticeably better than Midjourney, the popular proprietary AI image generator from the U.S. company of the same name. My session through Qwen chat produced multiple errors in prompt comprehension and text fidelity, much to my disappointment, even after repeated attempts and prompt rewording: &lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015104" height="403" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-04-at-2.03.19%E2%80%AFPM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015105" height="339" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-04-at-2.06.23%E2%80%AFPM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;Yet Midjourney only offers a limited number of free generations and requires subscriptions for any more, compared to Qwen Image, which, thanks to its open source licensing and weights posted on Hugging Face, can be adopted by any enterprise or third-party provider free-of-charge.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-licensing-and-availability"&gt;Licensing and availability&lt;/h2&gt;



&lt;p&gt;&lt;strong&gt;Qwen-Image is distributed under the Apache 2.0&lt;/strong&gt; &lt;strong&gt;license&lt;/strong&gt;, allowing commercial and non-commercial use, redistribution, and modification ‚Äî though attribution and inclusion of the license text are required for derivative works. &lt;/p&gt;



&lt;p&gt;This may make it attractive to enterprises looking for an open source image generation tool to use for making internal or external-facing collateral like flyers, ads, notices, newsletters, and other digital communications. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;But the fact that the model‚Äôs training data remains a tightly guarded secret &lt;/strong&gt;‚Äî like with most other leading AI image generators ‚Äî &lt;strong&gt;may sour some enterprises on the idea of using it&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;Qwen, unlike Adobe Firefly or OpenAI‚Äôs GPT-4o native image generation, for example,&lt;strong&gt; does not offer indemnification for commercial uses of its product&lt;/strong&gt; (i.e., if a user gets sued for copyright infringement, Adobe and OpenAI will help support them in court). &lt;/p&gt;



&lt;p&gt;The model and associated assets ‚Äî including demo notebooks, evaluation tools, and fine-tuning scripts ‚Äî are available through multiple repositories:&lt;/p&gt;







&lt;p&gt;In addition, a live evaluation portal called AI Arena allows users to compare image generations in pairwise rounds, contributing to a public Elo-style leaderboard.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-training-and-development"&gt;Training and development&lt;/h2&gt;



&lt;p&gt;Behind Qwen-Image‚Äôs performance is an&lt;strong&gt; extensive training process grounded in progressive learning, multi-modal task alignment, and aggressive data curation&lt;/strong&gt;, according to the technical paper the research team released today.&lt;/p&gt;



&lt;p&gt;The training corpus includes billions of image-text pairs sourced from four domains: natural imagery, human portraits, artistic and design content (such as posters and UI layouts), and synthetic text-focused data.&lt;strong&gt; The Qwen Team did not specify the size of the training data corpus&lt;/strong&gt;, aside from ‚Äúbillions of image-text pairs.‚Äù They did provide a breakdown of the rough percentage of each category of content it included:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Nature:&lt;/strong&gt; ~55%&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Design (UI, posters, art):&lt;/strong&gt; ~27%&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;People (portraits, human activity):&lt;/strong&gt; ~13%&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Synthetic text rendering data:&lt;/strong&gt; ~5%&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Notably, Qwen emphasizes that all synthetic data was generated in-house, and no images created by other AI models were used. Despite the detailed curation and filtering stages described, &lt;strong&gt;the documentation does not clarify whether any of the data was licensed or drawn from public or proprietary datasets.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Unlike many generative models that exclude synthetic text due to noise risks, Qwen-Image uses tightly controlled synthetic rendering pipelines to improve character coverage ‚Äî especially for low-frequency characters in Chinese.&lt;/p&gt;



&lt;p&gt;A curriculum-style strategy is employed: the&lt;strong&gt; model starts with simple captioned images and non-text content&lt;/strong&gt;, then advances to layout-sensitive text scenarios, mixed-language rendering, and dense paragraphs. This &lt;strong&gt;gradual exposure is shown to help the model generalize across scripts and formatting types.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Qwen-Image integrates three key modules:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Qwen2.5-VL&lt;/strong&gt;, the multimodal language model, extracts contextual meaning and guides generation through system prompts.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;VAE Encoder/Decoder&lt;/strong&gt;, trained on high-resolution documents and real-world layouts, handles detailed visual representations, especially small or dense text.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;MMDiT&lt;/strong&gt;, the diffusion model backbone, coordinates joint learning across image and text modalities. A novel MSRoPE (Multimodal Scalable Rotary Positional Encoding) system improves spatial alignment between tokens.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Together, these components allow Qwen-Image to operate effectively in tasks that involve image understanding, generation, and precise editing.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-benchmarks"&gt;Performance benchmarks&lt;/h2&gt;



&lt;p&gt;Qwen-Image was evaluated against several public benchmarks:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;GenEval&lt;/strong&gt; and &lt;strong&gt;DPG&lt;/strong&gt; for prompt-following and object attribute consistency&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;OneIG-Bench&lt;/strong&gt; and &lt;strong&gt;TIIF&lt;/strong&gt; for compositional reasoning and layout fidelity&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;CVTG-2K&lt;/strong&gt;, &lt;strong&gt;ChineseWord&lt;/strong&gt;, and &lt;strong&gt;LongText-Bench&lt;/strong&gt; for text rendering, especially in multilingual contexts&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;In nearly every case, Qwen-Image either matches or surpasses existing closed-source models like GPT Image 1 [High], Seedream 3.0, and FLUX.1 Kontext [Pro]. Notably, its performance on Chinese text rendering was significantly better than all compared systems.&lt;/p&gt;



&lt;p&gt;On the public AI Arena leaderboard ‚Äî based on 10,000+ human pairwise comparisons ‚Äî Qwen-Image ranks third overall and is the top open-source model.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-implications-for-enterprise-technical-decision-makers"&gt;Implications for enterprise technical decision-makers&lt;/h2&gt;



&lt;p&gt;For enterprise AI teams managing complex multimodal workflows, Qwen-Image introduces several functional advantages that align with the operational needs of different roles.&lt;/p&gt;



&lt;p&gt;Those managing the lifecycle of vision-language models ‚Äî from training to deployment ‚Äî wil&lt;strong&gt;l find value in Qwen-Image‚Äôs consistent output quality and its integration-ready components. &lt;/strong&gt;The open-source nature reduces licensing costs, while the modular architecture (Qwen2.5-VL + VAE + MMDiT) facilitates adaptation to custom datasets or fine-tuning for domain-specific outputs.&lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;curriculum-style training data and clear benchmark results help teams evaluate fitness for purpose. &lt;/strong&gt;Whether deploying marketing visuals, document renderings, or e-commerce product graphics, Qwen-Image allows rapid experimentation without proprietary constraints.&lt;/p&gt;



&lt;p&gt;Engineers&lt;strong&gt; tasked with building AI pipelines or deploying models across distributed systems will appreciate the detailed infrastructure documentation. &lt;/strong&gt;The model has been trained using a Producer-Consumer architecture, supports scalable multi-resolution processing (256p to 1328p), and is built to run with Megatron-LM and tensor parallelism. This &lt;strong&gt;makes Qwen-Image a candidate for deployment in hybrid cloud environments where reliability and throughput matter.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Moreover, support for image-to-image editing workflows (TI2I) and task-specific prompts enables its use in real-time or interactive applications.&lt;/p&gt;



&lt;p&gt;Professionals focused on data ingestion, validation, and transformation &lt;strong&gt;can use Qwen-Image as a tool to generate synthetic datasets for training or augmenting computer vision models.&lt;/strong&gt; Its ability to generate high-resolution images with embedded, multilingual annotations can improve performance in downstream OCR, object detection, or layout parsing tasks.&lt;/p&gt;



&lt;p&gt;Since Qwen-Image was &lt;strong&gt;also trained to avoid artifacts like QR codes&lt;/strong&gt;, distorted text, and watermarks, it offers higher-quality synthetic input than many public models ‚Äî helping enterprise teams preserve training set integrity.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-looking-for-feedback-and-opportunities-to-collaborate"&gt;Looking for feedback and opportunities to collaborate&lt;/h2&gt;



&lt;p&gt;The Qwen Team emphasizes openness and community collaboration in the model‚Äôs release. &lt;/p&gt;



&lt;p&gt;Developers are encouraged to test and fine-tune Qwen-Image, offer pull requests, and participate in the evaluation leaderboard. Feedback on text rendering, editing fidelity, and multilingual use cases will shape future iterations.&lt;/p&gt;



&lt;p&gt;With a stated goal to ‚Äúlower the technical barriers to visual content creation,‚Äù the team hopes Qwen-Image will serve not just as a model, but as a foundation for further research and practical deployment across industries.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;After seizing the summer with a blitz of powerful, freely available new open source language and coding focused AI models that matched or in some cases bested closed-source/proprietary U.S. rivals,&lt;strong&gt; Alibaba‚Äôs crack ‚ÄúQwen Team‚Äù of AI researchers is back again today with the release of a highly ranked new AI image generator model &lt;/strong&gt;‚Äî also open source.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Qwen-Image stands out in a crowded field of generative image models&lt;/strong&gt; due to its &lt;strong&gt;emphasis on rendering text accurately within visuals&lt;/strong&gt; ‚Äî an area where many rivals still struggle. &lt;/p&gt;&lt;p&gt;Supporting both alphabetic and logographic scripts, the model is particularly adept at managing complex typography, multi-line layouts, paragraph-level semantics, and&lt;strong&gt; bilingual content (e.g., English-Chinese).&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In practice, this allows users to &lt;strong&gt;generate content like movie posters, presentation slides, storefront scenes, handwritten poetry, and stylized infographics&lt;/strong&gt; ‚Äî with crisp text that aligns with their prompts.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Qwen-Image‚Äôs output examples include a wide variety of real-world use cases:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Marketing &amp;amp; Branding&lt;/strong&gt;: Bilingual posters with brand logos, stylistic calligraphy, and consistent design motifs&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Presentation Design&lt;/strong&gt;: Layout-aware slide decks with title hierarchies and theme-appropriate visuals&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Education&lt;/strong&gt;: Generation of classroom materials featuring diagrams and precisely rendered instructional text&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Retail &amp;amp; E-commerce&lt;/strong&gt;: Storefront scenes where product labels, signage, and environmental context must all be readable&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Creative Content&lt;/strong&gt;: Handwritten poetry, scene narratives, anime-style illustration with embedded story text&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Users can interact with the model on the Qwen Chat website by selecting ‚ÄúImage Generation‚Äù mode from the buttons below the prompt entry field.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015099" height="328" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-04-at-1.42.41%E2%80%AFPM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;However, my brief initial tests revealed the text and prompt adherence was not noticeably better than Midjourney, the popular proprietary AI image generator from the U.S. company of the same name. My session through Qwen chat produced multiple errors in prompt comprehension and text fidelity, much to my disappointment, even after repeated attempts and prompt rewording: &lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015104" height="403" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-04-at-2.03.19%E2%80%AFPM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015105" height="339" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-04-at-2.06.23%E2%80%AFPM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;Yet Midjourney only offers a limited number of free generations and requires subscriptions for any more, compared to Qwen Image, which, thanks to its open source licensing and weights posted on Hugging Face, can be adopted by any enterprise or third-party provider free-of-charge.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-licensing-and-availability"&gt;Licensing and availability&lt;/h2&gt;



&lt;p&gt;&lt;strong&gt;Qwen-Image is distributed under the Apache 2.0&lt;/strong&gt; &lt;strong&gt;license&lt;/strong&gt;, allowing commercial and non-commercial use, redistribution, and modification ‚Äî though attribution and inclusion of the license text are required for derivative works. &lt;/p&gt;



&lt;p&gt;This may make it attractive to enterprises looking for an open source image generation tool to use for making internal or external-facing collateral like flyers, ads, notices, newsletters, and other digital communications. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;But the fact that the model‚Äôs training data remains a tightly guarded secret &lt;/strong&gt;‚Äî like with most other leading AI image generators ‚Äî &lt;strong&gt;may sour some enterprises on the idea of using it&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;Qwen, unlike Adobe Firefly or OpenAI‚Äôs GPT-4o native image generation, for example,&lt;strong&gt; does not offer indemnification for commercial uses of its product&lt;/strong&gt; (i.e., if a user gets sued for copyright infringement, Adobe and OpenAI will help support them in court). &lt;/p&gt;



&lt;p&gt;The model and associated assets ‚Äî including demo notebooks, evaluation tools, and fine-tuning scripts ‚Äî are available through multiple repositories:&lt;/p&gt;







&lt;p&gt;In addition, a live evaluation portal called AI Arena allows users to compare image generations in pairwise rounds, contributing to a public Elo-style leaderboard.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-training-and-development"&gt;Training and development&lt;/h2&gt;



&lt;p&gt;Behind Qwen-Image‚Äôs performance is an&lt;strong&gt; extensive training process grounded in progressive learning, multi-modal task alignment, and aggressive data curation&lt;/strong&gt;, according to the technical paper the research team released today.&lt;/p&gt;



&lt;p&gt;The training corpus includes billions of image-text pairs sourced from four domains: natural imagery, human portraits, artistic and design content (such as posters and UI layouts), and synthetic text-focused data.&lt;strong&gt; The Qwen Team did not specify the size of the training data corpus&lt;/strong&gt;, aside from ‚Äúbillions of image-text pairs.‚Äù They did provide a breakdown of the rough percentage of each category of content it included:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Nature:&lt;/strong&gt; ~55%&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Design (UI, posters, art):&lt;/strong&gt; ~27%&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;People (portraits, human activity):&lt;/strong&gt; ~13%&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Synthetic text rendering data:&lt;/strong&gt; ~5%&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Notably, Qwen emphasizes that all synthetic data was generated in-house, and no images created by other AI models were used. Despite the detailed curation and filtering stages described, &lt;strong&gt;the documentation does not clarify whether any of the data was licensed or drawn from public or proprietary datasets.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Unlike many generative models that exclude synthetic text due to noise risks, Qwen-Image uses tightly controlled synthetic rendering pipelines to improve character coverage ‚Äî especially for low-frequency characters in Chinese.&lt;/p&gt;



&lt;p&gt;A curriculum-style strategy is employed: the&lt;strong&gt; model starts with simple captioned images and non-text content&lt;/strong&gt;, then advances to layout-sensitive text scenarios, mixed-language rendering, and dense paragraphs. This &lt;strong&gt;gradual exposure is shown to help the model generalize across scripts and formatting types.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Qwen-Image integrates three key modules:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Qwen2.5-VL&lt;/strong&gt;, the multimodal language model, extracts contextual meaning and guides generation through system prompts.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;VAE Encoder/Decoder&lt;/strong&gt;, trained on high-resolution documents and real-world layouts, handles detailed visual representations, especially small or dense text.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;MMDiT&lt;/strong&gt;, the diffusion model backbone, coordinates joint learning across image and text modalities. A novel MSRoPE (Multimodal Scalable Rotary Positional Encoding) system improves spatial alignment between tokens.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Together, these components allow Qwen-Image to operate effectively in tasks that involve image understanding, generation, and precise editing.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-benchmarks"&gt;Performance benchmarks&lt;/h2&gt;



&lt;p&gt;Qwen-Image was evaluated against several public benchmarks:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;GenEval&lt;/strong&gt; and &lt;strong&gt;DPG&lt;/strong&gt; for prompt-following and object attribute consistency&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;OneIG-Bench&lt;/strong&gt; and &lt;strong&gt;TIIF&lt;/strong&gt; for compositional reasoning and layout fidelity&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;CVTG-2K&lt;/strong&gt;, &lt;strong&gt;ChineseWord&lt;/strong&gt;, and &lt;strong&gt;LongText-Bench&lt;/strong&gt; for text rendering, especially in multilingual contexts&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;In nearly every case, Qwen-Image either matches or surpasses existing closed-source models like GPT Image 1 [High], Seedream 3.0, and FLUX.1 Kontext [Pro]. Notably, its performance on Chinese text rendering was significantly better than all compared systems.&lt;/p&gt;



&lt;p&gt;On the public AI Arena leaderboard ‚Äî based on 10,000+ human pairwise comparisons ‚Äî Qwen-Image ranks third overall and is the top open-source model.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-implications-for-enterprise-technical-decision-makers"&gt;Implications for enterprise technical decision-makers&lt;/h2&gt;



&lt;p&gt;For enterprise AI teams managing complex multimodal workflows, Qwen-Image introduces several functional advantages that align with the operational needs of different roles.&lt;/p&gt;



&lt;p&gt;Those managing the lifecycle of vision-language models ‚Äî from training to deployment ‚Äî wil&lt;strong&gt;l find value in Qwen-Image‚Äôs consistent output quality and its integration-ready components. &lt;/strong&gt;The open-source nature reduces licensing costs, while the modular architecture (Qwen2.5-VL + VAE + MMDiT) facilitates adaptation to custom datasets or fine-tuning for domain-specific outputs.&lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;curriculum-style training data and clear benchmark results help teams evaluate fitness for purpose. &lt;/strong&gt;Whether deploying marketing visuals, document renderings, or e-commerce product graphics, Qwen-Image allows rapid experimentation without proprietary constraints.&lt;/p&gt;



&lt;p&gt;Engineers&lt;strong&gt; tasked with building AI pipelines or deploying models across distributed systems will appreciate the detailed infrastructure documentation. &lt;/strong&gt;The model has been trained using a Producer-Consumer architecture, supports scalable multi-resolution processing (256p to 1328p), and is built to run with Megatron-LM and tensor parallelism. This &lt;strong&gt;makes Qwen-Image a candidate for deployment in hybrid cloud environments where reliability and throughput matter.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Moreover, support for image-to-image editing workflows (TI2I) and task-specific prompts enables its use in real-time or interactive applications.&lt;/p&gt;



&lt;p&gt;Professionals focused on data ingestion, validation, and transformation &lt;strong&gt;can use Qwen-Image as a tool to generate synthetic datasets for training or augmenting computer vision models.&lt;/strong&gt; Its ability to generate high-resolution images with embedded, multilingual annotations can improve performance in downstream OCR, object detection, or layout parsing tasks.&lt;/p&gt;



&lt;p&gt;Since Qwen-Image was &lt;strong&gt;also trained to avoid artifacts like QR codes&lt;/strong&gt;, distorted text, and watermarks, it offers higher-quality synthetic input than many public models ‚Äî helping enterprise teams preserve training set integrity.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-looking-for-feedback-and-opportunities-to-collaborate"&gt;Looking for feedback and opportunities to collaborate&lt;/h2&gt;



&lt;p&gt;The Qwen Team emphasizes openness and community collaboration in the model‚Äôs release. &lt;/p&gt;



&lt;p&gt;Developers are encouraged to test and fine-tune Qwen-Image, offer pull requests, and participate in the evaluation leaderboard. Feedback on text rendering, editing fidelity, and multilingual use cases will shape future iterations.&lt;/p&gt;



&lt;p&gt;With a stated goal to ‚Äúlower the technical barriers to visual content creation,‚Äù the team hopes Qwen-Image will serve not just as a model, but as a foundation for further research and practical deployment across industries.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/qwen-image-is-a-powerful-open-source-new-ai-image-generator-with-support-for-embedded-text-in-english-chinese/</guid><pubDate>Mon, 04 Aug 2025 18:07:53 +0000</pubDate></item><item><title>[NEW] AI site Perplexity uses ‚Äústealth tactics‚Äù to flout no-crawl edicts, Cloudflare says (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/08/ai-site-perplexity-uses-stealth-tactics-to-flout-no-crawl-edicts-cloudflare-says/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The allegations are the latest to accuse Perplexity of improper web crawling.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Benj Edwards / Kirillm via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;AI search engine Perplexity is using stealth bots and other tactics to evade websites‚Äô no-crawl directives, an allegation that if true violates Internet norms that have been in place for more than three decades, network security and optimization service Cloudflare said Monday.&lt;/p&gt;
&lt;p&gt;In a blog post, Cloudflare researchers said the company received complaints from customers who had disallowed Perplexity scraping bots by implementing settings in their sites‚Äô robots.txt files and through Web application firewalls that blocked the declared Perplexity crawlers. Despite those steps, Cloudflare said, Perplexity continued to access the sites‚Äô content.&lt;/p&gt;
&lt;p&gt;The researchers said they then set out to test it for themselves and found that when known Perplexity crawlers encountered blocks from robots.txt files or firewall rules, Perplexity then searched the sites using a stealth bot that followed a range of tactics to mask its activity.&lt;/p&gt;
&lt;h2&gt;&amp;gt;10,000 domains and millions of requests&lt;/h2&gt;
&lt;p&gt;‚ÄúThis undeclared crawler utilized multiple IPs not listed in Perplexity‚Äôs official IP range, and would rotate through these IPs in response to the restrictive robots.txt policy and block from Cloudflare," the researchers wrote. ‚ÄúIn addition to rotating IPs, we observed requests coming from different ASNs in attempts to further evade website blocks. This activity was observed across tens of thousands of domains and millions of requests per day.‚Äù&lt;/p&gt;
&lt;p&gt;The researchers provided the following diagram to illustrate the flow of the technique they allege Perplexity used.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2110007 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="623" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/alleged-perplexity-stealth-crawler-1024x623.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Cloudflare

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;If true, the evasion flouts Internet norms in place for more than three decades. In 1994, engineer Martijn Koster proposed the Robots Exclusion Protocol, which provided a machine-readable format for informing crawlers they weren‚Äôt permitted on a given site. Sites that their content indexed installed the simple robots.txt file at the top of their homepage. The standard, which has been widely observed and endorsed ever since, formally became a standard under the Internet Engineering Task Force in 2022.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Cloudflare isn‚Äôt the first to say that Perplexity violates the spirit if not the letter of the norm. Last year, Reddit CEO Steve Huffman told The Verge that stopping Perplexity‚Äîand two other AI engines from Microsoft and Anthropic‚Äîwas a real pain in the ass.‚Äù Huffman went on to say: ‚ÄúWe‚Äôve had Microsoft, Anthropic, and Perplexity act as though all of the content on the Internet is free for them to use. That‚Äôs their real position.‚Äù&lt;/p&gt;
&lt;p&gt;Perplexity has faced allegations from several other publishers that it plagiarized their content. Forbes, for instance, accused Perplexity of ‚Äúcynical theft‚Äù after publishing a post that was ‚Äúextremely similar to Forbes‚Äô proprietary article‚Äù posted a day earlier. Ars Technica sister publication Wired has leveled similar claims. It cited what it said were suspicious traffic patterns from IP addresses, likely linked to Perplexity, that were ignoring robots.txt exclusions. Perplexity was also found to have manipulated its crawling bots' ID string to bypass website blocks.&lt;/p&gt;
&lt;p&gt;The Cloudflare researchers said that in response to their findings, the company is taking actions to prevent crawlers from accessing sites that use its content-delivery service.&lt;/p&gt;
&lt;p&gt;‚ÄúThere are clear preferences that crawlers should be transparent, serve a clear purpose, perform a specific activity, and, most importantly, follow website directives and preferences,‚Äù they wrote. ‚ÄúBased on Perplexity‚Äôs observed behavior, which is incompatible with those preferences, we have de-listed them as a verified bot and added heuristics to our managed rules that block this stealth crawling.‚Äù&lt;/p&gt;
&lt;p&gt;Perplexity representatives didn‚Äôt respond to an email asking if the allegations are true.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The allegations are the latest to accuse Perplexity of improper web crawling.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Benj Edwards / Kirillm via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;AI search engine Perplexity is using stealth bots and other tactics to evade websites‚Äô no-crawl directives, an allegation that if true violates Internet norms that have been in place for more than three decades, network security and optimization service Cloudflare said Monday.&lt;/p&gt;
&lt;p&gt;In a blog post, Cloudflare researchers said the company received complaints from customers who had disallowed Perplexity scraping bots by implementing settings in their sites‚Äô robots.txt files and through Web application firewalls that blocked the declared Perplexity crawlers. Despite those steps, Cloudflare said, Perplexity continued to access the sites‚Äô content.&lt;/p&gt;
&lt;p&gt;The researchers said they then set out to test it for themselves and found that when known Perplexity crawlers encountered blocks from robots.txt files or firewall rules, Perplexity then searched the sites using a stealth bot that followed a range of tactics to mask its activity.&lt;/p&gt;
&lt;h2&gt;&amp;gt;10,000 domains and millions of requests&lt;/h2&gt;
&lt;p&gt;‚ÄúThis undeclared crawler utilized multiple IPs not listed in Perplexity‚Äôs official IP range, and would rotate through these IPs in response to the restrictive robots.txt policy and block from Cloudflare," the researchers wrote. ‚ÄúIn addition to rotating IPs, we observed requests coming from different ASNs in attempts to further evade website blocks. This activity was observed across tens of thousands of domains and millions of requests per day.‚Äù&lt;/p&gt;
&lt;p&gt;The researchers provided the following diagram to illustrate the flow of the technique they allege Perplexity used.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2110007 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="623" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/alleged-perplexity-stealth-crawler-1024x623.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Cloudflare

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;If true, the evasion flouts Internet norms in place for more than three decades. In 1994, engineer Martijn Koster proposed the Robots Exclusion Protocol, which provided a machine-readable format for informing crawlers they weren‚Äôt permitted on a given site. Sites that their content indexed installed the simple robots.txt file at the top of their homepage. The standard, which has been widely observed and endorsed ever since, formally became a standard under the Internet Engineering Task Force in 2022.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Cloudflare isn‚Äôt the first to say that Perplexity violates the spirit if not the letter of the norm. Last year, Reddit CEO Steve Huffman told The Verge that stopping Perplexity‚Äîand two other AI engines from Microsoft and Anthropic‚Äîwas a real pain in the ass.‚Äù Huffman went on to say: ‚ÄúWe‚Äôve had Microsoft, Anthropic, and Perplexity act as though all of the content on the Internet is free for them to use. That‚Äôs their real position.‚Äù&lt;/p&gt;
&lt;p&gt;Perplexity has faced allegations from several other publishers that it plagiarized their content. Forbes, for instance, accused Perplexity of ‚Äúcynical theft‚Äù after publishing a post that was ‚Äúextremely similar to Forbes‚Äô proprietary article‚Äù posted a day earlier. Ars Technica sister publication Wired has leveled similar claims. It cited what it said were suspicious traffic patterns from IP addresses, likely linked to Perplexity, that were ignoring robots.txt exclusions. Perplexity was also found to have manipulated its crawling bots' ID string to bypass website blocks.&lt;/p&gt;
&lt;p&gt;The Cloudflare researchers said that in response to their findings, the company is taking actions to prevent crawlers from accessing sites that use its content-delivery service.&lt;/p&gt;
&lt;p&gt;‚ÄúThere are clear preferences that crawlers should be transparent, serve a clear purpose, perform a specific activity, and, most importantly, follow website directives and preferences,‚Äù they wrote. ‚ÄúBased on Perplexity‚Äôs observed behavior, which is incompatible with those preferences, we have de-listed them as a verified bot and added heuristics to our managed rules that block this stealth crawling.‚Äù&lt;/p&gt;
&lt;p&gt;Perplexity representatives didn‚Äôt respond to an email asking if the allegations are true.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/08/ai-site-perplexity-uses-stealth-tactics-to-flout-no-crawl-edicts-cloudflare-says/</guid><pubDate>Mon, 04 Aug 2025 19:16:26 +0000</pubDate></item><item><title>[NEW] Google says its AI-based bug hunter found 20 security vulnerabilities (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/04/google-says-its-ai-based-bug-hunter-found-20-security-vulnerabilities/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/google-io-2023-google-deepmind.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google‚Äôs AI-powered bug hunter has just reported its first batch of security vulnerabilities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Heather Adkins, Google‚Äôs vice president of security, announced Monday that its LLM-based vulnerability researcher Big Sleep found and reported 20 flaws in various popular open source software.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Adkins said that Big Sleep, which is developed by the company‚Äôs AI department DeepMind as well as its elite team of hackers Project Zero, reported its first-ever vulnerabilities, mostly in open source software such as audio and video library FFmpeg and image-editing suite ImageMagick.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given that the vulnerabilities are not fixed yet, we don‚Äôt have details of their impact or severity, as Google does not yet want to provide details, which is a standard policy when waiting for bugs to be fixed. But the simple fact that Big Sleep found these vulnerabilities is significant, as it shows these tools are starting to get real results, even if there was a human involved in this case.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúTo ensure high quality and actionable reports, we have a human expert in the loop before reporting, but each vulnerability was found and reproduced by the AI agent without human intervention,‚Äù Google‚Äôs spokesperson Kimberly Samra told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Royal Hansen, Google‚Äôs vice president of engineering, wrote on X that the findings demonstrate ‚Äúa new frontier in automated vulnerability discovery.‚Äù&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LLM-powered tools that can look for and find vulnerabilities are already a reality. Other than Big Sleep, there‚Äôs RunSybil and XBOW, among others.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;XBOW has garnered headlines after it reached the top of one of the U.S. leaderboards at bug bounty platform HackerOne. It‚Äôs important to note that in most cases, these reports have a human at some point of the process to verify that the AI-powered bug hunter found a legitimate vulnerability, as is the case with Big Sleep.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vlad Ionescu, co-founder and chief technology officer at RunSybil, a startup that develops AI-powered bug hunters, told TechCrunch that Big Sleep is a ‚Äúlegit‚Äù project, given that it has ‚Äúgood design, people behind it know what they‚Äôre doing, Project Zero has the bug finding experience and DeepMind has the firepower and tokens to throw at it.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is obviously a lot of promise with these tools, but also significant downsides. Several people who maintain different software projects have complained of bug reports that are actually hallucinations, with some calling them the bug bounty equivalent of AI slop.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúThat‚Äôs the problem people are running into, is we‚Äôre getting a lot of stuff that looks like gold, but it‚Äôs actually just crap,‚Äù Ionescu previously told TechCrunch.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/google-io-2023-google-deepmind.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google‚Äôs AI-powered bug hunter has just reported its first batch of security vulnerabilities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Heather Adkins, Google‚Äôs vice president of security, announced Monday that its LLM-based vulnerability researcher Big Sleep found and reported 20 flaws in various popular open source software.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Adkins said that Big Sleep, which is developed by the company‚Äôs AI department DeepMind as well as its elite team of hackers Project Zero, reported its first-ever vulnerabilities, mostly in open source software such as audio and video library FFmpeg and image-editing suite ImageMagick.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given that the vulnerabilities are not fixed yet, we don‚Äôt have details of their impact or severity, as Google does not yet want to provide details, which is a standard policy when waiting for bugs to be fixed. But the simple fact that Big Sleep found these vulnerabilities is significant, as it shows these tools are starting to get real results, even if there was a human involved in this case.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúTo ensure high quality and actionable reports, we have a human expert in the loop before reporting, but each vulnerability was found and reproduced by the AI agent without human intervention,‚Äù Google‚Äôs spokesperson Kimberly Samra told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Royal Hansen, Google‚Äôs vice president of engineering, wrote on X that the findings demonstrate ‚Äúa new frontier in automated vulnerability discovery.‚Äù&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LLM-powered tools that can look for and find vulnerabilities are already a reality. Other than Big Sleep, there‚Äôs RunSybil and XBOW, among others.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;XBOW has garnered headlines after it reached the top of one of the U.S. leaderboards at bug bounty platform HackerOne. It‚Äôs important to note that in most cases, these reports have a human at some point of the process to verify that the AI-powered bug hunter found a legitimate vulnerability, as is the case with Big Sleep.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vlad Ionescu, co-founder and chief technology officer at RunSybil, a startup that develops AI-powered bug hunters, told TechCrunch that Big Sleep is a ‚Äúlegit‚Äù project, given that it has ‚Äúgood design, people behind it know what they‚Äôre doing, Project Zero has the bug finding experience and DeepMind has the firepower and tokens to throw at it.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is obviously a lot of promise with these tools, but also significant downsides. Several people who maintain different software projects have complained of bug reports that are actually hallucinations, with some calling them the bug bounty equivalent of AI slop.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúThat‚Äôs the problem people are running into, is we‚Äôre getting a lot of stuff that looks like gold, but it‚Äôs actually just crap,‚Äù Ionescu previously told TechCrunch.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/04/google-says-its-ai-based-bug-hunter-found-20-security-vulnerabilities/</guid><pubDate>Mon, 04 Aug 2025 19:22:04 +0000</pubDate></item><item><title>[NEW] MIT tool visualizes and edits ‚Äúphysically impossible‚Äù objects (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-meschers-tool-visualizes-edits-physically-impossible-objects-0804</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-csail-Meschers.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-00f766d6-7fff-403a-378d-5aaf2f5b25af"&gt;M.C. Escher‚Äôs artwork is a gateway into a world of depth-defying optical illusions, featuring ‚Äúimpossible objects‚Äù that break the laws of physics with convoluted geometries. What you perceive his illustrations to be depends on your point of view ‚Äî for example, a person seemingly walking upstairs may be heading down the steps if you tilt your head&amp;nbsp;sideways.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Computer graphics scientists and designers can recreate these illusions in 3D, but only by bending or cutting a real shape and positioning it at a particular angle. This workaround has downsides, though: Changing the smoothness or lighting of the structure will expose that it isn‚Äôt actually an optical illusion, which also means you can‚Äôt accurately solve geometry problems on it.&lt;/p&gt;&lt;p&gt;Researchers at MIT‚Äôs Computer Science and Artificial Intelligence Laboratory (CSAIL) have developed a unique approach to represent ‚Äúimpossible‚Äù objects in a more versatile way. Their ‚ÄúMeschers‚Äù tool converts images and 3D models into 2.5-dimensional structures, creating Escher-like depictions of things like windows, buildings, and even donuts. The approach helps users relight, smooth out, and study unique geometries while preserving their optical illusion.&lt;/p&gt;&lt;p&gt;This tool could assist geometry researchers with calculating the distance between two points on a curved impossible surface (‚Äúgeodesics‚Äù) and simulating how heat dissipates over it (‚Äúheat diffusion‚Äù). It could also help artists and computer graphics scientists create physics-breaking designs in multiple dimensions.&lt;/p&gt;&lt;p&gt;Lead author and MIT PhD student Ana Dodik aims to design computer graphics tools that aren‚Äôt limited to replicating reality, enabling artists to express their intent independently of whether a shape can be realized in the physical world. ‚ÄúUsing Meschers, we‚Äôve unlocked a new class of shapes for artists to work with on the computer,‚Äù she says. ‚ÄúThey could also help perception scientists understand the point at which an object truly becomes impossible.‚Äù&lt;/p&gt;&lt;p dir="ltr"&gt;Dodik and her colleagues will present their&amp;nbsp;paper at the SIGGRAPH conference in August.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Making impossible objects possible&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Impossible objects can‚Äôt be fully replicated in 3D. Their constituent parts often look plausible, but these parts don‚Äôt glue together properly when assembled in 3D. But what can be computationally imitated, as the CSAIL researchers found out, is the process of how we perceive these shapes.&lt;/p&gt;&lt;p&gt;Take the&amp;nbsp;Penrose Triangle, for instance. The object as a whole is physically impossible because the depths don‚Äôt ‚Äúadd up,‚Äù but we can recognize real-world 3D shapes (like its three L-shaped corners) within it. These smaller regions can be realized in 3D ‚Äî a property called ‚Äúlocal consistency‚Äù ‚Äî but when we try to assemble them together, they don‚Äôt form a globally consistent shape.&lt;/p&gt;&lt;p&gt;The Meschers approach models‚Äô locally consistent regions without forcing them to be globally consistent, piecing together an Escher-esque structure. Behind the scenes, Meschers represents impossible objects as if we know their x and y coordinates in the image, as well as differences in z coordinates (depth) between neighboring pixels; the tool uses these differences in depth to reason about impossible objects indirectly.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The many uses of Meschers&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In addition to rendering impossible objects, Meschers can subdivide their structures into smaller shapes for more precise geometry calculations and smoothing operations. This process enabled the researchers to reduce visual imperfections of impossible shapes, such as a red heart outline they thinned out.&lt;/p&gt;&lt;p&gt;The researchers also tested their tool on an ‚Äúimpossibagel,‚Äù where a bagel is shaded in a physically impossible way. Meschers helped Dodik and her colleagues simulate heat diffusion and calculate geodesic distances between different points of the model.&lt;/p&gt;&lt;p&gt;‚ÄúImagine you‚Äôre an ant traversing this bagel, and you want to know how long it‚Äôll take you to get across, for example,‚Äù says Dodik. ‚ÄúIn the same way, our tool could help mathematicians analyze the underlying geometry of impossible shapes up close, much like how we study real-world ones.‚Äù&lt;/p&gt;&lt;p dir="ltr"&gt;Much like a magician, the tool can create optical illusions out of otherwise practical objects, making it easier for computer graphics artists to create impossible objects. It can also use ‚Äúinverse rendering‚Äù tools to convert drawings and images of impossible objects into high-dimensional designs.&amp;nbsp;&lt;/p&gt;&lt;p&gt;‚ÄúMeschers demonstrates how computer graphics tools don‚Äôt have to be constrained by the rules of physical reality,‚Äù says senior author Justin Solomon, associate professor of electrical engineering and computer science and leader of the CSAIL Geometric Data Processing Group. ‚ÄúIncredibly, artists using Meschers can reason about shapes that we will never find in the real world.‚Äù&lt;/p&gt;&lt;p dir="ltr"&gt;Meschers can also aid computer graphics artists with tweaking the shading of their creations, while still preserving an optical illusion. This versatility would allow creatives to change the lighting of their art to depict a wider variety of scenes (like a sunrise or sunset) ‚Äî as Meschers demonstrated by relighting a model of a dog on a skateboard.&lt;/p&gt;&lt;p dir="ltr"&gt;Despite its versatility, Meschers is just the start for Dodik and her colleagues. The team is considering designing an interface to make the tool easier to use while building more elaborate scenes. They‚Äôre also working with perception scientists to see how the computer graphics tool can be used more broadly.&lt;/p&gt;&lt;p&gt;Dodik and Solomon wrote the paper with CSAIL affiliates Isabella Yu ‚Äô24, SM ‚Äô25; PhD student Kartik Chandra SM ‚Äô23; MIT professors Jonathan Ragan-Kelley and Joshua Tenenbaum; and MIT Assistant Professor Vincent Sitzmann.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Their work was supported, in part, by the MIT Presidential Fellowship, the Mathworks Fellowship, the Hertz Foundation, the U.S. National Science Foundation, the Schmidt Sciences AI2050 fellowship, MIT Quest for Intelligence, the U.S. Army Research Office, U.S. Air Force Office of Scientific Research, SystemsThatLearn@CSAIL initiative, Google, the MIT‚ÄìIBM Watson AI Laboratory, from the Toyota‚ÄìCSAIL Joint Research Center, Adobe Systems, the Singapore Defence Science and Technology Agency, and the U.S. Intelligence Advanced Research Projects Activity.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-csail-Meschers.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-00f766d6-7fff-403a-378d-5aaf2f5b25af"&gt;M.C. Escher‚Äôs artwork is a gateway into a world of depth-defying optical illusions, featuring ‚Äúimpossible objects‚Äù that break the laws of physics with convoluted geometries. What you perceive his illustrations to be depends on your point of view ‚Äî for example, a person seemingly walking upstairs may be heading down the steps if you tilt your head&amp;nbsp;sideways.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Computer graphics scientists and designers can recreate these illusions in 3D, but only by bending or cutting a real shape and positioning it at a particular angle. This workaround has downsides, though: Changing the smoothness or lighting of the structure will expose that it isn‚Äôt actually an optical illusion, which also means you can‚Äôt accurately solve geometry problems on it.&lt;/p&gt;&lt;p&gt;Researchers at MIT‚Äôs Computer Science and Artificial Intelligence Laboratory (CSAIL) have developed a unique approach to represent ‚Äúimpossible‚Äù objects in a more versatile way. Their ‚ÄúMeschers‚Äù tool converts images and 3D models into 2.5-dimensional structures, creating Escher-like depictions of things like windows, buildings, and even donuts. The approach helps users relight, smooth out, and study unique geometries while preserving their optical illusion.&lt;/p&gt;&lt;p&gt;This tool could assist geometry researchers with calculating the distance between two points on a curved impossible surface (‚Äúgeodesics‚Äù) and simulating how heat dissipates over it (‚Äúheat diffusion‚Äù). It could also help artists and computer graphics scientists create physics-breaking designs in multiple dimensions.&lt;/p&gt;&lt;p&gt;Lead author and MIT PhD student Ana Dodik aims to design computer graphics tools that aren‚Äôt limited to replicating reality, enabling artists to express their intent independently of whether a shape can be realized in the physical world. ‚ÄúUsing Meschers, we‚Äôve unlocked a new class of shapes for artists to work with on the computer,‚Äù she says. ‚ÄúThey could also help perception scientists understand the point at which an object truly becomes impossible.‚Äù&lt;/p&gt;&lt;p dir="ltr"&gt;Dodik and her colleagues will present their&amp;nbsp;paper at the SIGGRAPH conference in August.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Making impossible objects possible&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Impossible objects can‚Äôt be fully replicated in 3D. Their constituent parts often look plausible, but these parts don‚Äôt glue together properly when assembled in 3D. But what can be computationally imitated, as the CSAIL researchers found out, is the process of how we perceive these shapes.&lt;/p&gt;&lt;p&gt;Take the&amp;nbsp;Penrose Triangle, for instance. The object as a whole is physically impossible because the depths don‚Äôt ‚Äúadd up,‚Äù but we can recognize real-world 3D shapes (like its three L-shaped corners) within it. These smaller regions can be realized in 3D ‚Äî a property called ‚Äúlocal consistency‚Äù ‚Äî but when we try to assemble them together, they don‚Äôt form a globally consistent shape.&lt;/p&gt;&lt;p&gt;The Meschers approach models‚Äô locally consistent regions without forcing them to be globally consistent, piecing together an Escher-esque structure. Behind the scenes, Meschers represents impossible objects as if we know their x and y coordinates in the image, as well as differences in z coordinates (depth) between neighboring pixels; the tool uses these differences in depth to reason about impossible objects indirectly.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The many uses of Meschers&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In addition to rendering impossible objects, Meschers can subdivide their structures into smaller shapes for more precise geometry calculations and smoothing operations. This process enabled the researchers to reduce visual imperfections of impossible shapes, such as a red heart outline they thinned out.&lt;/p&gt;&lt;p&gt;The researchers also tested their tool on an ‚Äúimpossibagel,‚Äù where a bagel is shaded in a physically impossible way. Meschers helped Dodik and her colleagues simulate heat diffusion and calculate geodesic distances between different points of the model.&lt;/p&gt;&lt;p&gt;‚ÄúImagine you‚Äôre an ant traversing this bagel, and you want to know how long it‚Äôll take you to get across, for example,‚Äù says Dodik. ‚ÄúIn the same way, our tool could help mathematicians analyze the underlying geometry of impossible shapes up close, much like how we study real-world ones.‚Äù&lt;/p&gt;&lt;p dir="ltr"&gt;Much like a magician, the tool can create optical illusions out of otherwise practical objects, making it easier for computer graphics artists to create impossible objects. It can also use ‚Äúinverse rendering‚Äù tools to convert drawings and images of impossible objects into high-dimensional designs.&amp;nbsp;&lt;/p&gt;&lt;p&gt;‚ÄúMeschers demonstrates how computer graphics tools don‚Äôt have to be constrained by the rules of physical reality,‚Äù says senior author Justin Solomon, associate professor of electrical engineering and computer science and leader of the CSAIL Geometric Data Processing Group. ‚ÄúIncredibly, artists using Meschers can reason about shapes that we will never find in the real world.‚Äù&lt;/p&gt;&lt;p dir="ltr"&gt;Meschers can also aid computer graphics artists with tweaking the shading of their creations, while still preserving an optical illusion. This versatility would allow creatives to change the lighting of their art to depict a wider variety of scenes (like a sunrise or sunset) ‚Äî as Meschers demonstrated by relighting a model of a dog on a skateboard.&lt;/p&gt;&lt;p dir="ltr"&gt;Despite its versatility, Meschers is just the start for Dodik and her colleagues. The team is considering designing an interface to make the tool easier to use while building more elaborate scenes. They‚Äôre also working with perception scientists to see how the computer graphics tool can be used more broadly.&lt;/p&gt;&lt;p&gt;Dodik and Solomon wrote the paper with CSAIL affiliates Isabella Yu ‚Äô24, SM ‚Äô25; PhD student Kartik Chandra SM ‚Äô23; MIT professors Jonathan Ragan-Kelley and Joshua Tenenbaum; and MIT Assistant Professor Vincent Sitzmann.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Their work was supported, in part, by the MIT Presidential Fellowship, the Mathworks Fellowship, the Hertz Foundation, the U.S. National Science Foundation, the Schmidt Sciences AI2050 fellowship, MIT Quest for Intelligence, the U.S. Army Research Office, U.S. Air Force Office of Scientific Research, SystemsThatLearn@CSAIL initiative, Google, the MIT‚ÄìIBM Watson AI Laboratory, from the Toyota‚ÄìCSAIL Joint Research Center, Adobe Systems, the Singapore Defence Science and Technology Agency, and the U.S. Intelligence Advanced Research Projects Activity.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-meschers-tool-visualizes-edits-physically-impossible-objects-0804</guid><pubDate>Mon, 04 Aug 2025 20:40:00 +0000</pubDate></item><item><title>[NEW] ChatGPT rockets to 700M weekly users ahead of GPT-5 launch with reasoning superpowers (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/chatgpt-rockets-to-700m-weekly-users-ahead-of-gpt-5-launch-with-reasoning-superpowers/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;OpenAI‚Äôs ChatGPT will reach 700 million weekly active users this week, the company announced Monday, cementing its position as one of the fastest-adopted software products in history just as the company prepares to release its most powerful language model yet.&lt;/p&gt;&lt;p&gt;The surge is a 40 percent jump from the 500 million weekly users ChatGPT had at the end of March and marks a fourfold increase from the same period last year. The explosive growth rivals the adoption rates of platforms like Zoom during the pandemic and early social media networks, underscoring how quickly AI tools have moved from experimental to essential.&lt;/p&gt;&lt;p&gt;The milestone comes at a strategic moment for OpenAI, which reportedly plans to launch GPT-5 in early August, citing sources familiar with the company‚Äôs plans. The timing suggests OpenAI is orchestrating a coordinated push to dominate the AI landscape before competitors can close the gap.&lt;/p&gt;&lt;p&gt;‚ÄúEvery day, people and teams are learning, creating, and solving harder problems,‚Äù said Nick Turley, OpenAI‚Äôs vice president of product for ChatGPT, in announcing the user benchmark. ‚ÄúBig week ahead.‚Äù&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-gpt-5-will-combine-reasoning-powers-into-single-ai-system"&gt;GPT-5 will combine reasoning powers into single AI system&lt;/h2&gt;



&lt;p&gt;The upcoming model goes beyond an incremental upgrade. According to people briefed on the project who spoke to The Information, GPT-5 will integrate OpenAI‚Äôs advanced reasoning capabilities from its o3 series directly into the flagship GPT platform, creating what CEO Sam Altman has described as ‚Äúa system that integrates a lot of our technology.‚Äù&lt;/p&gt;



&lt;p&gt;This integration marks a strategic shift for OpenAI, which has previously released reasoning models separately from its general-purpose language models. By combining these capabilities, the company aims to reduce user confusion about which model to deploy for specific tasks while creating a more powerful unified system.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;turns out yes! pic.twitter.com/yVsZXKSmKR&lt;/p&gt;‚Äî Sam Altman (@sama) August 3, 2025&lt;/blockquote&gt; 



&lt;p&gt;The consolidation also serves OpenAI‚Äôs broader ambition to achieve artificial general intelligence, or AGI ‚Äî a milestone that would trigger significant changes to its partnership with Microsoft. Under their current agreement, achieving AGI would force Microsoft to relinquish its rights to OpenAI‚Äôs revenue and future models, potentially reshaping one of the most consequential partnerships in technology.&lt;/p&gt;



&lt;p&gt;Altman has tempered expectations, however, stating that GPT-5 won‚Äôt reach ‚Äúgold level of capability for many months‚Äù after launch, suggesting the AGI threshold remains beyond immediate reach.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-business-customers-jump-to-5-million-as-revenue-hits-13-billion"&gt;Business customers jump to 5 million as revenue hits $13 billion&lt;/h2&gt;



&lt;p&gt;The user growth reflects ChatGPT‚Äôs expanding role in corporate America. OpenAI now serves 5 million paying business customers, up from 3 million in June, as enterprises increasingly integrate AI tools into core operations. Daily user messages have surpassed 3 billion, reflecting not just growth in users but intensifying engagement with the platform.&lt;/p&gt;



&lt;p&gt;This surge in business adoption has driven OpenAI‚Äôs annual recurring revenue to $13 billion, up from $10 billion in June, with projections suggesting it could exceed $20 billion by year-end. The revenue growth, combined with a recent $8.3 billion funding round that valued OpenAI at $300 billion, provides the financial foundation for the massive infrastructure investments required to maintain its technological edge.&lt;/p&gt;



&lt;p&gt;Those investments are substantial. OpenAI has committed to a $30 billion annual lease with Oracle for data center capacity and struck an $11.9 billion deal with cloud provider CoreWeave, while planning international expansion through partnerships like Stargate Norway and a major data center project in Abu Dhabi.&lt;/p&gt;







&lt;p&gt;The rapid growth comes as OpenAI faces mounting pressure from well-funded rivals eager to capture market share. Google‚Äôs AI search product, AI Overviews, claims 2 billion monthly users across more than 200 countries, while its Gemini App reports 450 million monthly active users. Anthropic, backed by significant investments from Amazon and others, is reportedly seeking to raise up to $5 billion at a $170 billion valuation, according to Bloomberg.&lt;/p&gt;



&lt;p&gt;Meta has made significant strides with its Llama models, while Elon Musk‚Äôs xAI continues to attract attention and investment. The competitive landscape has intensified the AI arms race, with companies pouring billions into compute infrastructure and talent acquisition.&lt;/p&gt;



&lt;p&gt;The competition has triggered a talent war among tech giants. Microsoft has reportedly hired more than 20 employees from Google‚Äôs DeepMind team in recent months, including former Gemini engineering head Amar Subramanya, The Information reported, as companies raid each other‚Äôs AI talent pools.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-chatgpt-adds-wellness-features-as-ai-safety-concerns-grow"&gt;ChatGPT adds wellness features as AI safety concerns grow&lt;/h2&gt;



&lt;p&gt;As OpenAI pursues raw capability improvements, the company has also emphasized optimizing ChatGPT for user well-being and productivity. The company recently outlined efforts to help users ‚Äúthrive in the ways you choose‚Äînot to hold your attention, but to help you use it well.‚Äù&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;We build ChatGPT to help you thrive in the ways you choose ‚Äî not to hold your attention, but to help you use it well. We‚Äôre improving support for tough moments, have rolled out break reminders, and are developing better life advice, all guided by expert input.‚Ä¶&lt;/p&gt;‚Äî OpenAI (@OpenAI) August 4, 2025&lt;/blockquote&gt; 



&lt;p&gt;New features include break reminders and improved support for challenging situations, reflecting growing awareness of AI‚Äôs psychological and social impacts. This focus on responsible deployment could prove crucial as regulatory scrutiny intensifies and public debate about AI‚Äôs societal effects continues.&lt;/p&gt;



&lt;p&gt;When GPT-5 launches, it will include multiple variants ‚Äî including mini and nano versions available through OpenAI‚Äôs API ‚Äî providing developers and enterprises with options tailored to different use cases and computational requirements.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-700-million-users-signal-ai-s-mainstream-business-adoption"&gt;700 million users signal AI‚Äôs mainstream business adoption&lt;/h2&gt;



&lt;p&gt;The convergence of ChatGPT‚Äôs user growth and GPT-5‚Äôs launch marks a pivotal moment for the AI industry. OpenAI‚Äôs ability to maintain its lead while competitors rapidly advance will likely determine the sector‚Äôs trajectory for years to come.&lt;/p&gt;



&lt;p&gt;The company‚Äôs success has already reshaped how businesses think about AI integration, moving the technology from experimental projects to core operational tools. The 700 million user figure shows this transformation is accelerating, with implications extending far beyond technology into education, creative industries, and knowledge work.&lt;/p&gt;



&lt;p&gt;For enterprise customers, the user growth provides confidence in ChatGPT‚Äôs stability and longevity ‚Äî crucial factors for organizations making long-term AI investments. The platform‚Äôs scale also creates network effects, as widespread adoption drives improvements in model training and capability development.&lt;/p&gt;



&lt;p&gt;OpenAI now faces a test that will define the company‚Äôs future: whether it can convert unprecedented user growth into sustained market dominance. In a field where yesterday‚Äôs breakthrough becomes tomorrow‚Äôs baseline, 700 million users might just be the beginning.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;OpenAI‚Äôs ChatGPT will reach 700 million weekly active users this week, the company announced Monday, cementing its position as one of the fastest-adopted software products in history just as the company prepares to release its most powerful language model yet.&lt;/p&gt;&lt;p&gt;The surge is a 40 percent jump from the 500 million weekly users ChatGPT had at the end of March and marks a fourfold increase from the same period last year. The explosive growth rivals the adoption rates of platforms like Zoom during the pandemic and early social media networks, underscoring how quickly AI tools have moved from experimental to essential.&lt;/p&gt;&lt;p&gt;The milestone comes at a strategic moment for OpenAI, which reportedly plans to launch GPT-5 in early August, citing sources familiar with the company‚Äôs plans. The timing suggests OpenAI is orchestrating a coordinated push to dominate the AI landscape before competitors can close the gap.&lt;/p&gt;&lt;p&gt;‚ÄúEvery day, people and teams are learning, creating, and solving harder problems,‚Äù said Nick Turley, OpenAI‚Äôs vice president of product for ChatGPT, in announcing the user benchmark. ‚ÄúBig week ahead.‚Äù&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-gpt-5-will-combine-reasoning-powers-into-single-ai-system"&gt;GPT-5 will combine reasoning powers into single AI system&lt;/h2&gt;



&lt;p&gt;The upcoming model goes beyond an incremental upgrade. According to people briefed on the project who spoke to The Information, GPT-5 will integrate OpenAI‚Äôs advanced reasoning capabilities from its o3 series directly into the flagship GPT platform, creating what CEO Sam Altman has described as ‚Äúa system that integrates a lot of our technology.‚Äù&lt;/p&gt;



&lt;p&gt;This integration marks a strategic shift for OpenAI, which has previously released reasoning models separately from its general-purpose language models. By combining these capabilities, the company aims to reduce user confusion about which model to deploy for specific tasks while creating a more powerful unified system.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;turns out yes! pic.twitter.com/yVsZXKSmKR&lt;/p&gt;‚Äî Sam Altman (@sama) August 3, 2025&lt;/blockquote&gt; 



&lt;p&gt;The consolidation also serves OpenAI‚Äôs broader ambition to achieve artificial general intelligence, or AGI ‚Äî a milestone that would trigger significant changes to its partnership with Microsoft. Under their current agreement, achieving AGI would force Microsoft to relinquish its rights to OpenAI‚Äôs revenue and future models, potentially reshaping one of the most consequential partnerships in technology.&lt;/p&gt;



&lt;p&gt;Altman has tempered expectations, however, stating that GPT-5 won‚Äôt reach ‚Äúgold level of capability for many months‚Äù after launch, suggesting the AGI threshold remains beyond immediate reach.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-business-customers-jump-to-5-million-as-revenue-hits-13-billion"&gt;Business customers jump to 5 million as revenue hits $13 billion&lt;/h2&gt;



&lt;p&gt;The user growth reflects ChatGPT‚Äôs expanding role in corporate America. OpenAI now serves 5 million paying business customers, up from 3 million in June, as enterprises increasingly integrate AI tools into core operations. Daily user messages have surpassed 3 billion, reflecting not just growth in users but intensifying engagement with the platform.&lt;/p&gt;



&lt;p&gt;This surge in business adoption has driven OpenAI‚Äôs annual recurring revenue to $13 billion, up from $10 billion in June, with projections suggesting it could exceed $20 billion by year-end. The revenue growth, combined with a recent $8.3 billion funding round that valued OpenAI at $300 billion, provides the financial foundation for the massive infrastructure investments required to maintain its technological edge.&lt;/p&gt;



&lt;p&gt;Those investments are substantial. OpenAI has committed to a $30 billion annual lease with Oracle for data center capacity and struck an $11.9 billion deal with cloud provider CoreWeave, while planning international expansion through partnerships like Stargate Norway and a major data center project in Abu Dhabi.&lt;/p&gt;







&lt;p&gt;The rapid growth comes as OpenAI faces mounting pressure from well-funded rivals eager to capture market share. Google‚Äôs AI search product, AI Overviews, claims 2 billion monthly users across more than 200 countries, while its Gemini App reports 450 million monthly active users. Anthropic, backed by significant investments from Amazon and others, is reportedly seeking to raise up to $5 billion at a $170 billion valuation, according to Bloomberg.&lt;/p&gt;



&lt;p&gt;Meta has made significant strides with its Llama models, while Elon Musk‚Äôs xAI continues to attract attention and investment. The competitive landscape has intensified the AI arms race, with companies pouring billions into compute infrastructure and talent acquisition.&lt;/p&gt;



&lt;p&gt;The competition has triggered a talent war among tech giants. Microsoft has reportedly hired more than 20 employees from Google‚Äôs DeepMind team in recent months, including former Gemini engineering head Amar Subramanya, The Information reported, as companies raid each other‚Äôs AI talent pools.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-chatgpt-adds-wellness-features-as-ai-safety-concerns-grow"&gt;ChatGPT adds wellness features as AI safety concerns grow&lt;/h2&gt;



&lt;p&gt;As OpenAI pursues raw capability improvements, the company has also emphasized optimizing ChatGPT for user well-being and productivity. The company recently outlined efforts to help users ‚Äúthrive in the ways you choose‚Äînot to hold your attention, but to help you use it well.‚Äù&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;We build ChatGPT to help you thrive in the ways you choose ‚Äî not to hold your attention, but to help you use it well. We‚Äôre improving support for tough moments, have rolled out break reminders, and are developing better life advice, all guided by expert input.‚Ä¶&lt;/p&gt;‚Äî OpenAI (@OpenAI) August 4, 2025&lt;/blockquote&gt; 



&lt;p&gt;New features include break reminders and improved support for challenging situations, reflecting growing awareness of AI‚Äôs psychological and social impacts. This focus on responsible deployment could prove crucial as regulatory scrutiny intensifies and public debate about AI‚Äôs societal effects continues.&lt;/p&gt;



&lt;p&gt;When GPT-5 launches, it will include multiple variants ‚Äî including mini and nano versions available through OpenAI‚Äôs API ‚Äî providing developers and enterprises with options tailored to different use cases and computational requirements.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-700-million-users-signal-ai-s-mainstream-business-adoption"&gt;700 million users signal AI‚Äôs mainstream business adoption&lt;/h2&gt;



&lt;p&gt;The convergence of ChatGPT‚Äôs user growth and GPT-5‚Äôs launch marks a pivotal moment for the AI industry. OpenAI‚Äôs ability to maintain its lead while competitors rapidly advance will likely determine the sector‚Äôs trajectory for years to come.&lt;/p&gt;



&lt;p&gt;The company‚Äôs success has already reshaped how businesses think about AI integration, moving the technology from experimental projects to core operational tools. The 700 million user figure shows this transformation is accelerating, with implications extending far beyond technology into education, creative industries, and knowledge work.&lt;/p&gt;



&lt;p&gt;For enterprise customers, the user growth provides confidence in ChatGPT‚Äôs stability and longevity ‚Äî crucial factors for organizations making long-term AI investments. The platform‚Äôs scale also creates network effects, as widespread adoption drives improvements in model training and capability development.&lt;/p&gt;



&lt;p&gt;OpenAI now faces a test that will define the company‚Äôs future: whether it can convert unprecedented user growth into sustained market dominance. In a field where yesterday‚Äôs breakthrough becomes tomorrow‚Äôs baseline, 700 million users might just be the beginning.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/chatgpt-rockets-to-700m-weekly-users-ahead-of-gpt-5-launch-with-reasoning-superpowers/</guid><pubDate>Mon, 04 Aug 2025 20:42:05 +0000</pubDate></item></channel></rss>