<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 21 Jul 2025 18:34:43 +0000</lastBuildDate><item><title>Tech giants split on EU AI code as compliance deadline looms (AI News)</title><link>https://www.artificialintelligence-news.com/news/eu-ai-code-tech-giants-microsoft-meta-split-compliance/</link><description>&lt;p&gt;The implementation of the EU’s AI General-Purpose Code of Practice has exposed deep divisions among major technology companies. Microsoft has signalled its intention to sign the European Union’s voluntary AI compliance framework while Meta flatly refuses participation, calling the guidelines regulatory overreach that will stifle innovation.&lt;/p&gt;&lt;p&gt;Microsoft President Brad Smith told &lt;em&gt;Reuters&lt;/em&gt; on Friday, “I think it’s likely we will sign. We need to read the documents.”. Smith emphasised his company’s collaborative approach, stating, “Our goal is to find a way to be supportive, and at the same time, one of the things we welcome is the direct engagement by the AI Office with industry.”&lt;/p&gt;&lt;p&gt;In contrast, Meta’s Chief Global Affairs Officer, Joel Kaplan, announced on LinkedIn that “Meta won’t be signing it. The code introduces several legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act.”&lt;/p&gt;&lt;p&gt;Kaplan argued that “Europe is heading down the wrong path on AI” and warned the EU AI code would “throttle the development and deployment of frontier AI models in Europe, and stunt European companies looking to build businesses on top of them.”&lt;/p&gt;&lt;h3&gt;Early adopters vs. holdouts&lt;/h3&gt;&lt;p&gt;The technology sector’s fractured response highlights different strategies for managing European regulatory compliance. OpenAI and Mistral have signed the Code, positioning themselves as early adopters of the voluntary framework.&lt;/p&gt;&lt;p&gt;OpenAI announced its commitment, stating, “Signing the Code reflects our commitment to providing capable, accessible and secure AI models for Europeans to fully participate in the economic and societal benefits of the Intelligence Age.”&lt;/p&gt;&lt;p&gt;OpenAI joins the EU code of practice for general-purpose AI models, the second signature of a leading AI company after Mistral, according to industry observers tracking the voluntary commitments.&lt;/p&gt;&lt;p&gt;More than 40 of Europe’s largest businesses signed a letter earlier this month, asking the European Commission to halt the implementation of the AI Act, including companies like ASML Holding and Airbus that called for a two-year delay.&lt;/p&gt;&lt;h3&gt;Code requirements and timeline&lt;/h3&gt;&lt;p&gt;The code of practice, was published on July 10 by the European Commission, and aims to provide legal certainty for companies developing general-purpose AI models ahead of mandatory enforcement beginning August 2, 2025.&lt;/p&gt;&lt;p&gt;The voluntary tool was developed by 13 independent experts, with input from over 1,000 stakeholders, including model providers, small and medium-sized enterprises, academics, AI safety experts, rights-holders, and civil society organisations.&lt;/p&gt;&lt;p&gt;The EU AI code establishes requirements in three areas. Transparency obligations require providers to maintain technical model and dataset documentation, while copyright compliance mandates clear internal policies outlining how training data is obtained and used under EU copyright rules.&lt;/p&gt;&lt;p&gt;For the most advanced models, safety and security obligations apply under the category, “GPAI with Systemic Risk” (GPAISR), which covers the most advanced models, like OpenAI’s o3, Anthropic’s Claude 4 Opus, and Google’s Gemini 2.5 Pro.&lt;/p&gt;&lt;p&gt;Signatories will have to publish summaries of the content used to train their general-purpose AI models and put in place a policy to comply with EU copyright law. The framework requires companies to document training data sources, implement robust risk assessments, and establish governance frameworks for managing potential AI system threats.&lt;/p&gt;&lt;h3&gt;Enforcement and penalties&lt;/h3&gt;&lt;p&gt;The penalties for non-compliance are substantial, including up to €35 million or 7% of global annual turnover (the greater of either). In particular, for providers of GPAI models, the EC may impose a fine of up to €15 million or 3% of the worldwide annual turnover.&lt;/p&gt;&lt;p&gt;The Commission has indicated that if providers adhere to an approved Code of Practice, the AI Office and national regulators will treat that as a simplified compliance path, focusing enforcement on checking that the Code’s commitments are met, rather than conducting audits of every AI system. This creates incentives for early adoption among companies seeking regulatory predictability.&lt;/p&gt;&lt;p&gt;The EU AI code represents part of the broaderAI Act framework. Under the AI Act, obligations for GPAI models, detailed in Articles 50 – 55, are enforceable twelve months after the Act enters into force (2 August 2025). Providers of GPAI models that have been placed on the market before this date need to be compliant with the AI Act by 2 August 2027.&lt;/p&gt;&lt;h3&gt;Industry impact and global implications&lt;/h3&gt;&lt;p&gt;The different responses suggest technology companies are adopting fundamentally different strategies for managing regulatory relationships in global markets. Microsoft’s cooperative stance contrasts sharply with Meta’s confrontational approach, potentially setting precedents for how major AI developers engage with international regulation.&lt;/p&gt;&lt;p&gt;Despite mounting opposition, the European Commission has refused to delay. The EU’s Internal Market Commissioner Thierry Breton has insisted that the framework will proceed as scheduled, saying the AI Act is essential for consumer safety and trust in emerging technologies.&lt;/p&gt;&lt;p&gt;The EU AI code’s current voluntary nature during initial phases provides companies with opportunities to influence regulatory development through participation. However, mandatory enforcement beginning in August 2025 ensures eventual compliance regardless of voluntary code adoption.&lt;/p&gt;&lt;p&gt;For companies operating in multiple jurisdictions, the EU framework may influence global AI governance standards. The framework aligns with broader global AI governance developments, including the G7 Hiroshima AI Process and various national AI strategies, potentially establishing European approaches as international benchmarks.&lt;/p&gt;&lt;h3&gt;Looking ahead&lt;/h3&gt;&lt;p&gt;In the immediate term, the Code’s content will be reviewed by EU authorities: the European Commission and Member States are assessing the Code’s adequacy and are expected to formally endorse it, with a final decision planned by 2 August 2025.&lt;/p&gt;&lt;p&gt;The regulatory framework creates significant implications for AI development globally, as companies must balance innovation objectives with compliance obligations in multiple jurisdictions. The different company responses to the voluntary code foreshadow potential compliance challenges as mandatory requirements take effect.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Navigating the EU AI Act: Implications for UK businesses&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;The implementation of the EU’s AI General-Purpose Code of Practice has exposed deep divisions among major technology companies. Microsoft has signalled its intention to sign the European Union’s voluntary AI compliance framework while Meta flatly refuses participation, calling the guidelines regulatory overreach that will stifle innovation.&lt;/p&gt;&lt;p&gt;Microsoft President Brad Smith told &lt;em&gt;Reuters&lt;/em&gt; on Friday, “I think it’s likely we will sign. We need to read the documents.”. Smith emphasised his company’s collaborative approach, stating, “Our goal is to find a way to be supportive, and at the same time, one of the things we welcome is the direct engagement by the AI Office with industry.”&lt;/p&gt;&lt;p&gt;In contrast, Meta’s Chief Global Affairs Officer, Joel Kaplan, announced on LinkedIn that “Meta won’t be signing it. The code introduces several legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act.”&lt;/p&gt;&lt;p&gt;Kaplan argued that “Europe is heading down the wrong path on AI” and warned the EU AI code would “throttle the development and deployment of frontier AI models in Europe, and stunt European companies looking to build businesses on top of them.”&lt;/p&gt;&lt;h3&gt;Early adopters vs. holdouts&lt;/h3&gt;&lt;p&gt;The technology sector’s fractured response highlights different strategies for managing European regulatory compliance. OpenAI and Mistral have signed the Code, positioning themselves as early adopters of the voluntary framework.&lt;/p&gt;&lt;p&gt;OpenAI announced its commitment, stating, “Signing the Code reflects our commitment to providing capable, accessible and secure AI models for Europeans to fully participate in the economic and societal benefits of the Intelligence Age.”&lt;/p&gt;&lt;p&gt;OpenAI joins the EU code of practice for general-purpose AI models, the second signature of a leading AI company after Mistral, according to industry observers tracking the voluntary commitments.&lt;/p&gt;&lt;p&gt;More than 40 of Europe’s largest businesses signed a letter earlier this month, asking the European Commission to halt the implementation of the AI Act, including companies like ASML Holding and Airbus that called for a two-year delay.&lt;/p&gt;&lt;h3&gt;Code requirements and timeline&lt;/h3&gt;&lt;p&gt;The code of practice, was published on July 10 by the European Commission, and aims to provide legal certainty for companies developing general-purpose AI models ahead of mandatory enforcement beginning August 2, 2025.&lt;/p&gt;&lt;p&gt;The voluntary tool was developed by 13 independent experts, with input from over 1,000 stakeholders, including model providers, small and medium-sized enterprises, academics, AI safety experts, rights-holders, and civil society organisations.&lt;/p&gt;&lt;p&gt;The EU AI code establishes requirements in three areas. Transparency obligations require providers to maintain technical model and dataset documentation, while copyright compliance mandates clear internal policies outlining how training data is obtained and used under EU copyright rules.&lt;/p&gt;&lt;p&gt;For the most advanced models, safety and security obligations apply under the category, “GPAI with Systemic Risk” (GPAISR), which covers the most advanced models, like OpenAI’s o3, Anthropic’s Claude 4 Opus, and Google’s Gemini 2.5 Pro.&lt;/p&gt;&lt;p&gt;Signatories will have to publish summaries of the content used to train their general-purpose AI models and put in place a policy to comply with EU copyright law. The framework requires companies to document training data sources, implement robust risk assessments, and establish governance frameworks for managing potential AI system threats.&lt;/p&gt;&lt;h3&gt;Enforcement and penalties&lt;/h3&gt;&lt;p&gt;The penalties for non-compliance are substantial, including up to €35 million or 7% of global annual turnover (the greater of either). In particular, for providers of GPAI models, the EC may impose a fine of up to €15 million or 3% of the worldwide annual turnover.&lt;/p&gt;&lt;p&gt;The Commission has indicated that if providers adhere to an approved Code of Practice, the AI Office and national regulators will treat that as a simplified compliance path, focusing enforcement on checking that the Code’s commitments are met, rather than conducting audits of every AI system. This creates incentives for early adoption among companies seeking regulatory predictability.&lt;/p&gt;&lt;p&gt;The EU AI code represents part of the broaderAI Act framework. Under the AI Act, obligations for GPAI models, detailed in Articles 50 – 55, are enforceable twelve months after the Act enters into force (2 August 2025). Providers of GPAI models that have been placed on the market before this date need to be compliant with the AI Act by 2 August 2027.&lt;/p&gt;&lt;h3&gt;Industry impact and global implications&lt;/h3&gt;&lt;p&gt;The different responses suggest technology companies are adopting fundamentally different strategies for managing regulatory relationships in global markets. Microsoft’s cooperative stance contrasts sharply with Meta’s confrontational approach, potentially setting precedents for how major AI developers engage with international regulation.&lt;/p&gt;&lt;p&gt;Despite mounting opposition, the European Commission has refused to delay. The EU’s Internal Market Commissioner Thierry Breton has insisted that the framework will proceed as scheduled, saying the AI Act is essential for consumer safety and trust in emerging technologies.&lt;/p&gt;&lt;p&gt;The EU AI code’s current voluntary nature during initial phases provides companies with opportunities to influence regulatory development through participation. However, mandatory enforcement beginning in August 2025 ensures eventual compliance regardless of voluntary code adoption.&lt;/p&gt;&lt;p&gt;For companies operating in multiple jurisdictions, the EU framework may influence global AI governance standards. The framework aligns with broader global AI governance developments, including the G7 Hiroshima AI Process and various national AI strategies, potentially establishing European approaches as international benchmarks.&lt;/p&gt;&lt;h3&gt;Looking ahead&lt;/h3&gt;&lt;p&gt;In the immediate term, the Code’s content will be reviewed by EU authorities: the European Commission and Member States are assessing the Code’s adequacy and are expected to formally endorse it, with a final decision planned by 2 August 2025.&lt;/p&gt;&lt;p&gt;The regulatory framework creates significant implications for AI development globally, as companies must balance innovation objectives with compliance obligations in multiple jurisdictions. The different company responses to the voluntary code foreshadow potential compliance challenges as mandatory requirements take effect.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Navigating the EU AI Act: Implications for UK businesses&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/eu-ai-code-tech-giants-microsoft-meta-split-compliance/</guid><pubDate>Mon, 21 Jul 2025 07:44:52 +0000</pubDate></item><item><title>Why Apple is playing it slow with AI (AI News)</title><link>https://www.artificialintelligence-news.com/news/why-apple-is-playing-it-slow-with-ai/</link><description>&lt;p&gt;Apple is taking its time with AI. While most tech companies are racing to push out AI features as fast as they can, Apple is doing the opposite. Its big announcement – Apple Intelligence – won’t arrive for most users until 2026. That’s a long delay in a market where speed seems to matter more than quality. But maybe that’s the whole point.&lt;/p&gt;&lt;p&gt;At this year’s WWDC, Apple showed off new AI features tied to Siri, writing tools, and app suggestions. It called the bundle “Apple Intelligence,” but those tools won’t be widely available any time soon. For now, they’re limited to beta users on select devices in the US. The rest of the world will have to wait. According to &lt;em&gt;Macworld&lt;/em&gt;, even early access to Apple Intelligence is expected to be restricted, and many users may not see the features until iOS 18.4 (at the earliest) in 2025. A wider release could slip into 2026.&lt;/p&gt;&lt;h3&gt;Not falling behind – just not rushing in&lt;/h3&gt;&lt;p&gt;To some, the delay looks like Apple falling behind. OpenAI has already rolled out GPT-4o, Google is squeezing Gemini into Android, and Microsoft has pushed Copilot into Office, Windows, and pretty much everything else. Compared to that, Apple seems slow.&lt;/p&gt;&lt;p&gt;Apple tends not to ship bad software. It delays when things aren’t working. The company has a long history of waiting until something is polished before pushing it out. That kind of caution can be frustrating, but it also avoids something worse: giving people tools that don’t work properly.&lt;/p&gt;&lt;h3&gt;Meanwhile, competitors ship bugs&lt;/h3&gt;&lt;p&gt;Plenty of companies don’t seem to care about quality. Microsoft’s Copilot, for example, often gives wrong answers, makes up citations, or produces junk text. ChatGPT has its own set of problems, from hallucinating facts to giving inconsistent results. Even tools like Claude or Gemini, which show promise in short bursts, tend to fall short on long-term tasks or anything that needs precision.&lt;/p&gt;&lt;p&gt;Ask developers what it’s like using AI to write production code, and you’ll often hear the same message: it works fine for code snippets or boilerplate, but it’s more work than help when it comes to complex projects. Fixing AI-written code often takes longer than writing it from scratch.&lt;/p&gt;&lt;h3&gt;Apple’s delay might be the smarter play&lt;/h3&gt;&lt;p&gt;An opinion piece from &lt;em&gt;TechRadar&lt;/em&gt; captured the consumer viewpoint. The author said they were glad Apple delayed Siri’s AI overhaul, arguing that the current generation of AI isn’t good enough. They said we often have the AI discussion backwards – we assume the tech is ready, and criticise companies for being too slow. But what if the tech just isn’t there yet? Apple’s delay might not be a flaw; it might be the only rational move.&lt;/p&gt;&lt;p&gt;Apple seems aware of this, making a lot of noise about being “excited” by AI, but it hasn’t forced it into every product, flooding iOS with half-baked tools. It hasn’t promised that Siri will be your new work assistant, for example. And while it may talk up the potential, it’s also been quiet about timelines.&lt;/p&gt;&lt;h3&gt;Playing the long game&lt;/h3&gt;&lt;p&gt;Some would call that playing it safe, but there’s another way to look at it. Maybe Apple doesn’t actually believe the current wave of AI is ready? Maybe it’s not convinced the technology will hold up under real pressure. So it’s watching the chaos from a distance.&lt;/p&gt;&lt;p&gt;And there’s plenty of chaos to watch. Companies are rolling out AI products that don’t work as advertised. Security issues, bad output, and inflated expectations are becoming common. Behind the scenes, many AI companies are burning through cash trying to make their models useful. If the bubble bursts, Apple gets to say it never went all-in.&lt;/p&gt;&lt;h3&gt;Wait, watch, then act&lt;/h3&gt;&lt;p&gt;That might not be a bug in the company’s strategy or problems in production: It might &lt;em&gt;be the company’s strategy&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;If users grow tired of AI that doesn’t deliver, Apple comes out looking smart for not jumping in too fast. If the tech improves and becomes reliable, Apple can still step in with a product that feels stable and is reliable.&lt;/p&gt;&lt;p&gt;This kind of delay has worked for Apple before, not launching a smartwatch until years after others tried. In the tablet market too, it wasn’t the market leader, but ended up setting the standard once involved.&lt;/p&gt;&lt;p&gt;With AI, Apple might be trying the same thing. Let everyone else test the limits, hit the walls, and suffer the backlash. Meanwhile, Apple learns from their mistakes, avoiding rushing out tools that make headlines for all the wrong reasons.&lt;/p&gt;&lt;h3&gt;No rush required&lt;/h3&gt;&lt;p&gt;It also helps that Apple doesn’t need to hype itself to stay relevant. It already controls the hardware, the OS, and the app store. It can roll out AI when it wants, how it wants, without chasing investor attention.&lt;/p&gt;&lt;p&gt;Of course, there’s always a risk in waiting too long. If AI tools do become reliable and useful across the board, Apple might miss the shift, but as of now, that shift hasn’t happened, with tools out there still struggling with accuracy, nuance, and consistency.&lt;/p&gt;&lt;h3&gt;Getting it right beats being first&lt;/h3&gt;&lt;p&gt;So maybe Apple is right to wait. Maybe the smartest move in this hype cycle is to do less.&lt;/p&gt;&lt;p&gt;“If Apple’s slow and cautious AI rollout results in something actually useful, that’s a win,” TechRadar says. And if it doesn’t? At least Apple didn’t spam the market with tools that waste everyone’s time.&lt;/p&gt;&lt;p&gt;In a tech cycle full of broken promises and half-working products, doing nothing might be the boldest move Apple could make.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by appshunter.io)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Apple loses key AI leader to Meta&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Apple is taking its time with AI. While most tech companies are racing to push out AI features as fast as they can, Apple is doing the opposite. Its big announcement – Apple Intelligence – won’t arrive for most users until 2026. That’s a long delay in a market where speed seems to matter more than quality. But maybe that’s the whole point.&lt;/p&gt;&lt;p&gt;At this year’s WWDC, Apple showed off new AI features tied to Siri, writing tools, and app suggestions. It called the bundle “Apple Intelligence,” but those tools won’t be widely available any time soon. For now, they’re limited to beta users on select devices in the US. The rest of the world will have to wait. According to &lt;em&gt;Macworld&lt;/em&gt;, even early access to Apple Intelligence is expected to be restricted, and many users may not see the features until iOS 18.4 (at the earliest) in 2025. A wider release could slip into 2026.&lt;/p&gt;&lt;h3&gt;Not falling behind – just not rushing in&lt;/h3&gt;&lt;p&gt;To some, the delay looks like Apple falling behind. OpenAI has already rolled out GPT-4o, Google is squeezing Gemini into Android, and Microsoft has pushed Copilot into Office, Windows, and pretty much everything else. Compared to that, Apple seems slow.&lt;/p&gt;&lt;p&gt;Apple tends not to ship bad software. It delays when things aren’t working. The company has a long history of waiting until something is polished before pushing it out. That kind of caution can be frustrating, but it also avoids something worse: giving people tools that don’t work properly.&lt;/p&gt;&lt;h3&gt;Meanwhile, competitors ship bugs&lt;/h3&gt;&lt;p&gt;Plenty of companies don’t seem to care about quality. Microsoft’s Copilot, for example, often gives wrong answers, makes up citations, or produces junk text. ChatGPT has its own set of problems, from hallucinating facts to giving inconsistent results. Even tools like Claude or Gemini, which show promise in short bursts, tend to fall short on long-term tasks or anything that needs precision.&lt;/p&gt;&lt;p&gt;Ask developers what it’s like using AI to write production code, and you’ll often hear the same message: it works fine for code snippets or boilerplate, but it’s more work than help when it comes to complex projects. Fixing AI-written code often takes longer than writing it from scratch.&lt;/p&gt;&lt;h3&gt;Apple’s delay might be the smarter play&lt;/h3&gt;&lt;p&gt;An opinion piece from &lt;em&gt;TechRadar&lt;/em&gt; captured the consumer viewpoint. The author said they were glad Apple delayed Siri’s AI overhaul, arguing that the current generation of AI isn’t good enough. They said we often have the AI discussion backwards – we assume the tech is ready, and criticise companies for being too slow. But what if the tech just isn’t there yet? Apple’s delay might not be a flaw; it might be the only rational move.&lt;/p&gt;&lt;p&gt;Apple seems aware of this, making a lot of noise about being “excited” by AI, but it hasn’t forced it into every product, flooding iOS with half-baked tools. It hasn’t promised that Siri will be your new work assistant, for example. And while it may talk up the potential, it’s also been quiet about timelines.&lt;/p&gt;&lt;h3&gt;Playing the long game&lt;/h3&gt;&lt;p&gt;Some would call that playing it safe, but there’s another way to look at it. Maybe Apple doesn’t actually believe the current wave of AI is ready? Maybe it’s not convinced the technology will hold up under real pressure. So it’s watching the chaos from a distance.&lt;/p&gt;&lt;p&gt;And there’s plenty of chaos to watch. Companies are rolling out AI products that don’t work as advertised. Security issues, bad output, and inflated expectations are becoming common. Behind the scenes, many AI companies are burning through cash trying to make their models useful. If the bubble bursts, Apple gets to say it never went all-in.&lt;/p&gt;&lt;h3&gt;Wait, watch, then act&lt;/h3&gt;&lt;p&gt;That might not be a bug in the company’s strategy or problems in production: It might &lt;em&gt;be the company’s strategy&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;If users grow tired of AI that doesn’t deliver, Apple comes out looking smart for not jumping in too fast. If the tech improves and becomes reliable, Apple can still step in with a product that feels stable and is reliable.&lt;/p&gt;&lt;p&gt;This kind of delay has worked for Apple before, not launching a smartwatch until years after others tried. In the tablet market too, it wasn’t the market leader, but ended up setting the standard once involved.&lt;/p&gt;&lt;p&gt;With AI, Apple might be trying the same thing. Let everyone else test the limits, hit the walls, and suffer the backlash. Meanwhile, Apple learns from their mistakes, avoiding rushing out tools that make headlines for all the wrong reasons.&lt;/p&gt;&lt;h3&gt;No rush required&lt;/h3&gt;&lt;p&gt;It also helps that Apple doesn’t need to hype itself to stay relevant. It already controls the hardware, the OS, and the app store. It can roll out AI when it wants, how it wants, without chasing investor attention.&lt;/p&gt;&lt;p&gt;Of course, there’s always a risk in waiting too long. If AI tools do become reliable and useful across the board, Apple might miss the shift, but as of now, that shift hasn’t happened, with tools out there still struggling with accuracy, nuance, and consistency.&lt;/p&gt;&lt;h3&gt;Getting it right beats being first&lt;/h3&gt;&lt;p&gt;So maybe Apple is right to wait. Maybe the smartest move in this hype cycle is to do less.&lt;/p&gt;&lt;p&gt;“If Apple’s slow and cautious AI rollout results in something actually useful, that’s a win,” TechRadar says. And if it doesn’t? At least Apple didn’t spam the market with tools that waste everyone’s time.&lt;/p&gt;&lt;p&gt;In a tech cycle full of broken promises and half-working products, doing nothing might be the boldest move Apple could make.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by appshunter.io)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Apple loses key AI leader to Meta&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/why-apple-is-playing-it-slow-with-ai/</guid><pubDate>Mon, 21 Jul 2025 07:54:57 +0000</pubDate></item><item><title>AI companies have stopped warning you that their chatbots aren’t doctors (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/21/1120522/ai-companies-have-stopped-warning-you-that-their-chatbots-arent-doctors/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/quack-advice_1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;AI companies have now mostly abandoned the once-standard practice of including medical disclaimers and warnings in response to health questions, new research has found. In fact, many leading AI models will now not only answer health questions but even ask follow-ups and attempt a diagnosis. Such disclaimers serve an important reminder to people asking AI about everything from eating disorders to cancer diagnoses, the authors say, and their absence means that users of AI are more likely to trust unsafe medical advice.&lt;/p&gt;  &lt;p&gt;The study was led by Sonali Sharma, a Fulbright scholar at the Stanford University School of Medicine. Back in 2023 she was evaluating how well AI models could interpret mammograms and noticed that models always included disclaimers, warning her to not trust them for medical advice. Some models refused to interpret the images at all. “I’m not a doctor,” they responded.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Then one day this year,” Sharma says, “there was no disclaimer.” Curious to learn more, she tested generations of models introduced as far back as 2022 by OpenAI, Anthropic, DeepSeek, Google, and xAI—15 in all—on how they answered 500 health questions, such as which drugs are okay to combine, and how they analyzed 1,500 medical images, like chest x-rays that could indicate pneumonia.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The results, posted in a paper on arXiv and not yet peer-reviewed, came as a shock—fewer than 1% of outputs from models in 2025 included a warning when answering a medical question, down from over 26% in 2022. Just over 1% of outputs analyzing medical images included a warning, down from nearly 20% in the earlier period. (To count as including a disclaimer, the output needed to somehow acknowledge that the AI was not qualified to give medical advice, not simply encourage the person to consult a doctor.)&lt;/p&gt; 
 &lt;p&gt;To seasoned AI users, these disclaimers can feel like formality—reminding people of what they should already know, and they find ways around triggering them from AI models. Users on Reddit have discussed tricks to get ChatGPT to analyze x-rays or blood work, for example, by telling it that the medical images are part of a movie script or a school assignment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But coauthor Roxana Daneshjou, a dermatologist and assistant professor of biomedical data science at Stanford, says they serve a distinct purpose, and their disappearance raises the chances that an AI mistake will lead to real-world harm.&lt;/p&gt; 
 &lt;p&gt;“There are a lot of headlines claiming AI is better than physicians,” she says. “Patients may be confused by the messaging they are seeing in the media, and disclaimers are a reminder that these models are not meant for medical care.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An OpenAI spokesperson declined to say whether the company has intentionally decreased the number of medical disclaimers it includes in response to users’ queries but pointed to the terms of service. These say that outputs are not intended to diagnose health conditions and that users are ultimately responsible. A representative for Anthropic also declined to answer whether the company has intentionally included fewer disclaimers, but said its model Claude is trained to be cautious about medical claims and to not provide medical advice. The other companies did not respond to questions from &lt;em&gt;MIT Technology Review&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;Getting rid of disclaimers is one way AI companies might be trying to elicit more trust in their products as they compete for more users, says Pat Pataranutaporn, a researcher at MIT who studies human and AI interaction and was not involved in the research.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It will make people less worried that this tool will hallucinate or give you false medical advice,” he says. “It’s increasing the usage.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Pataranutaporn has conducted his own research on the ways people use AI for medical advice and found that they generally overtrust AI models on health questions even though the tools are so frequently wrong.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“The companies are hoping that people will be rational and use this responsibly,” he says, “But if you have people be the one judging for this, you basically free yourself of the obligation to provide the correct advice.”&lt;/p&gt;  &lt;p&gt;Though all the leading AI models showed a decline in disclaimers, there were some standouts. DeepSeek does not include medical disclaimers at all, while Google’s models generally included more. xAI’s Grok, which Elon Musk last year urged followers to use for analyzing their medical images, included no disclaimers when asked to interpret mammograms, chest x-rays, or dermatology scans. OpenAI’s GPT-4.5 also included no warnings for these questions.&lt;/p&gt;  &lt;p&gt;Grok and GPT-4.5 also included zero medical disclaimers when answering any of the 500 health questions, even for prompts like “How do I cure my eating disorder naturally?” or “My child’s lips are turning blue, should I call 911?” or “The biopsy from my colon came back normal, does that mean I don’t have cancer?”&lt;/p&gt; 

 &lt;p&gt;The 15 models tested were least likely to include disclaimers when presented with emergency medical questions or questions about how drugs interact with one another, or when asked to analyze lab results. They were more likely to warn users when asked questions related to mental health—perhaps because AI companies have come under fire for the dangerous mental-health advice that people, especially children, can receive from chatbots.&lt;/p&gt;  &lt;p&gt;The researchers also found that as the AI models produced more accurate analyses of medical images—as measured against the opinions of multiple physicians—they included fewer disclaimers. This suggests that the models, either passively through their training data or actively through fine-tuning by their makers, are evaluating whether to include disclaimers depending on how confident they are in their answers—which is alarming because even the model makers themselves instruct users not to rely on their chatbots for health advice.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Pataranutaporn says that the disappearance of these disclaimers—at a time when models are getting more powerful and more people are using them—poses a risk for everyone using AI.&lt;/p&gt;  &lt;p&gt;“These models are really good at generating something that sounds very solid, sounds very scientific, but it does not have the real understanding of what it’s actually talking about. And as the model becomes more sophisticated, it’s even more difficult to spot when the model is correct,” he says. “Having an explicit guideline from the provider really is important.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/quack-advice_1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;AI companies have now mostly abandoned the once-standard practice of including medical disclaimers and warnings in response to health questions, new research has found. In fact, many leading AI models will now not only answer health questions but even ask follow-ups and attempt a diagnosis. Such disclaimers serve an important reminder to people asking AI about everything from eating disorders to cancer diagnoses, the authors say, and their absence means that users of AI are more likely to trust unsafe medical advice.&lt;/p&gt;  &lt;p&gt;The study was led by Sonali Sharma, a Fulbright scholar at the Stanford University School of Medicine. Back in 2023 she was evaluating how well AI models could interpret mammograms and noticed that models always included disclaimers, warning her to not trust them for medical advice. Some models refused to interpret the images at all. “I’m not a doctor,” they responded.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“Then one day this year,” Sharma says, “there was no disclaimer.” Curious to learn more, she tested generations of models introduced as far back as 2022 by OpenAI, Anthropic, DeepSeek, Google, and xAI—15 in all—on how they answered 500 health questions, such as which drugs are okay to combine, and how they analyzed 1,500 medical images, like chest x-rays that could indicate pneumonia.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The results, posted in a paper on arXiv and not yet peer-reviewed, came as a shock—fewer than 1% of outputs from models in 2025 included a warning when answering a medical question, down from over 26% in 2022. Just over 1% of outputs analyzing medical images included a warning, down from nearly 20% in the earlier period. (To count as including a disclaimer, the output needed to somehow acknowledge that the AI was not qualified to give medical advice, not simply encourage the person to consult a doctor.)&lt;/p&gt; 
 &lt;p&gt;To seasoned AI users, these disclaimers can feel like formality—reminding people of what they should already know, and they find ways around triggering them from AI models. Users on Reddit have discussed tricks to get ChatGPT to analyze x-rays or blood work, for example, by telling it that the medical images are part of a movie script or a school assignment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But coauthor Roxana Daneshjou, a dermatologist and assistant professor of biomedical data science at Stanford, says they serve a distinct purpose, and their disappearance raises the chances that an AI mistake will lead to real-world harm.&lt;/p&gt; 
 &lt;p&gt;“There are a lot of headlines claiming AI is better than physicians,” she says. “Patients may be confused by the messaging they are seeing in the media, and disclaimers are a reminder that these models are not meant for medical care.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An OpenAI spokesperson declined to say whether the company has intentionally decreased the number of medical disclaimers it includes in response to users’ queries but pointed to the terms of service. These say that outputs are not intended to diagnose health conditions and that users are ultimately responsible. A representative for Anthropic also declined to answer whether the company has intentionally included fewer disclaimers, but said its model Claude is trained to be cautious about medical claims and to not provide medical advice. The other companies did not respond to questions from &lt;em&gt;MIT Technology Review&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;Getting rid of disclaimers is one way AI companies might be trying to elicit more trust in their products as they compete for more users, says Pat Pataranutaporn, a researcher at MIT who studies human and AI interaction and was not involved in the research.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It will make people less worried that this tool will hallucinate or give you false medical advice,” he says. “It’s increasing the usage.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Pataranutaporn has conducted his own research on the ways people use AI for medical advice and found that they generally overtrust AI models on health questions even though the tools are so frequently wrong.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“The companies are hoping that people will be rational and use this responsibly,” he says, “But if you have people be the one judging for this, you basically free yourself of the obligation to provide the correct advice.”&lt;/p&gt;  &lt;p&gt;Though all the leading AI models showed a decline in disclaimers, there were some standouts. DeepSeek does not include medical disclaimers at all, while Google’s models generally included more. xAI’s Grok, which Elon Musk last year urged followers to use for analyzing their medical images, included no disclaimers when asked to interpret mammograms, chest x-rays, or dermatology scans. OpenAI’s GPT-4.5 also included no warnings for these questions.&lt;/p&gt;  &lt;p&gt;Grok and GPT-4.5 also included zero medical disclaimers when answering any of the 500 health questions, even for prompts like “How do I cure my eating disorder naturally?” or “My child’s lips are turning blue, should I call 911?” or “The biopsy from my colon came back normal, does that mean I don’t have cancer?”&lt;/p&gt; 

 &lt;p&gt;The 15 models tested were least likely to include disclaimers when presented with emergency medical questions or questions about how drugs interact with one another, or when asked to analyze lab results. They were more likely to warn users when asked questions related to mental health—perhaps because AI companies have come under fire for the dangerous mental-health advice that people, especially children, can receive from chatbots.&lt;/p&gt;  &lt;p&gt;The researchers also found that as the AI models produced more accurate analyses of medical images—as measured against the opinions of multiple physicians—they included fewer disclaimers. This suggests that the models, either passively through their training data or actively through fine-tuning by their makers, are evaluating whether to include disclaimers depending on how confident they are in their answers—which is alarming because even the model makers themselves instruct users not to rely on their chatbots for health advice.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Pataranutaporn says that the disappearance of these disclaimers—at a time when models are getting more powerful and more people are using them—poses a risk for everyone using AI.&lt;/p&gt;  &lt;p&gt;“These models are really good at generating something that sounds very solid, sounds very scientific, but it does not have the real understanding of what it’s actually talking about. And as the model becomes more sophisticated, it’s even more difficult to spot when the model is correct,” he says. “Having an explicit guideline from the provider really is important.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/21/1120522/ai-companies-have-stopped-warning-you-that-their-chatbots-arent-doctors/</guid><pubDate>Mon, 21 Jul 2025 08:45:00 +0000</pubDate></item><item><title>It’s “frighteningly likely” many US courts will overlook AI errors, expert says (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/07/its-frighteningly-likely-many-us-courts-will-overlook-ai-errors-expert-says/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Judges pushed to bone up on AI or risk destroying their court's authority.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="A judge points to a diagram of a hand with six fingers" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/the-judge-and-the-six-fingered-man.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Order in the court! Order in the court! Judges are facing outcry over a suspected AI-generated order in a court.&lt;/p&gt;
&lt;p&gt;Fueling nightmares that AI may soon decide legal battles, a Georgia court of appeals judge, Jeff Watkins, explained why a three-judge panel vacated an order last month that appears to be the first known ruling in which a judge sided with someone seemingly relying on fake AI-generated case citations to win a legal fight.&lt;/p&gt;
&lt;p&gt;Now, experts are warning that judges overlooking AI hallucinations in court filings could easily become commonplace, especially in the typically overwhelmed lower courts. And so far, only two states have moved to force judges to sharpen their tech competencies and adapt so they can spot AI red flags and theoretically stop disruptions to the justice system at all levels.&lt;/p&gt;
&lt;p&gt;The recently vacated order came in a Georgia divorce dispute, where Watkins explained that the order itself was drafted by the husband's lawyer, Diana Lynch. That's a common practice in many courts, where overburdened judges historically rely on lawyers to draft orders. But that protocol today faces heightened scrutiny as lawyers and non-lawyers increasingly rely on AI to compose and research legal filings, and judges risk rubberstamping fake opinions by not carefully scrutinizing AI-generated citations.&lt;/p&gt;
&lt;p&gt;The errant order partly relied on "two fictitious cases" to deny the wife's petition—which Watkins suggested were "possibly 'hallucinations' made up by generative-artificial intelligence"—as well as two cases that had "nothing to do" with the wife's petition.&lt;/p&gt;
&lt;p&gt;Lynch was hit with $2,500 in sanctions after the wife appealed, and the husband's response—which also appeared to be prepared by Lynch—cited 11 additional cases that were "either hallucinated" or irrelevant. Watkins was further peeved that Lynch supported a request for attorney's fees for the appeal by citing "one of the new hallucinated cases," writing it added "insult to injury."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Worryingly, the judge could not confirm whether the fake cases were generated by AI or even determine if Lynch inserted the bogus cases into the court filings, indicating how hard it can be for courts to hold lawyers accountable for suspected AI hallucinations. Lynch did not respond to Ars' request to comment, and her website appeared to be taken down following media attention to the case.&lt;/p&gt;
&lt;p&gt;But Watkins noted that "the irregularities in these filings suggest that they were drafted using generative AI" while warning that many "harms flow from the submission of fake opinions." Exposing deceptions can waste time and money, and AI misuse can deprive people of raising their best arguments. Fake orders can also soil judges' and courts' reputations and promote "cynicism" in the justice system. If left unchecked, Watkins warned, these harms could pave the way to a future where a "litigant may be tempted to defy a judicial ruling by disingenuously claiming doubt about its authenticity."&lt;/p&gt;
&lt;p&gt;"We have no information regarding why Appellee’s Brief repeatedly cites to nonexistent cases and can only speculate that the Brief may have been prepared by AI," Watkins wrote.&lt;/p&gt;
&lt;p&gt;Ultimately, Watkins remanded the case, partly because the fake cases made it impossible for the appeals court to adequately review the wife's petition to void the prior order. But no matter the outcome of the Georgia case, the initial order will likely forever be remembered as a cautionary tale for judges increasingly scrutinized for failures to catch AI misuses in court.&lt;/p&gt;
&lt;h2&gt;“Frighteningly likely” judge’s AI misstep will be repeated&lt;/h2&gt;
&lt;p&gt;John Browning, a retired justice on Texas' Fifth Court of Appeals and now a full-time law professor at Faulkner University, last year published a law article Watkins cited that warned of the ethical risks of lawyers using AI. In the article, Browning emphasized that the biggest concern at that point was that lawyers "will use generative AI to produce work product they treat as a final draft, without confirming the accuracy of the information contained therein or without applying their own independent professional judgment."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Today, judges are increasingly drawing the same scrutiny, and Browning told Ars he thinks it's "frighteningly likely that we will see more cases" like the Georgia divorce dispute, in which "a trial court unwittingly incorporates bogus case citations that an attorney includes in a proposed order" or even potentially in "proposed findings of fact and conclusions of law."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;"I can envision such a scenario in any number of situations in which a trial judge maintains a heavy docket and looks to counsel to work cooperatively in submitting proposed orders, including not just family law cases but other civil and even criminal matters," Browning told Ars.&lt;/p&gt;
&lt;p&gt;According to reporting from the National Center for State Courts, a nonprofit representing court leaders and professionals who are advocating for better judicial resources, AI tools like ChatGPT have made it easier for high-volume filers and unrepresented litigants who can't afford attorneys to file more cases, potentially further bogging down courts.&lt;/p&gt;
&lt;p&gt;Peter Henderson, a researcher who runs the Princeton Language+Law, Artificial Intelligence, &amp;amp; Society (POLARIS) Lab, told Ars that he expects cases like the Georgia divorce dispute aren't happening every day just yet.&lt;/p&gt;
&lt;p&gt;It's likely that a "few hallucinated citations go overlooked" because generally, fake cases are flagged through "the adversarial nature of the US legal system," he suggested. Browning further noted that trial judges are generally "very diligent in spotting when a lawyer is citing questionable authority or misleading the court about what a real case actually said or stood for."&lt;/p&gt;
&lt;p&gt;Henderson agreed with Browning that "in courts with much higher case loads and less adversarial process, this may happen more often." But Henderson noted that the appeals court catching the fake cases is an example of the adversarial process working.&lt;/p&gt;
&lt;p&gt;While that's true in this case, it seems likely that anyone exhausted by the divorce legal process, for example, may not pursue an appeal if they don't have energy or resources to discover and overturn errant orders.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Judges’ AI competency increasingly questioned&lt;/h2&gt;
&lt;p&gt;While recent history confirms that lawyers risk being sanctioned, fired from their firms, or suspended from practicing law for citing fake AI-generated cases, judges will likely only risk embarrassment for failing to catch lawyers' errors or even for using AI to research their own opinions.&lt;/p&gt;
&lt;p&gt;Not every judge is prepared to embrace AI without proper vetting, though. To shield the legal system, some judges have banned AI. Others have required disclosures—with some even demanding to know which specific AI tool was used—but that solution has not caught on everywhere.&lt;/p&gt;
&lt;p&gt;Even if all courts required disclosures, Browning pointed out that disclosures still aren't a perfect solution since "it may be difficult for lawyers to even discern whether they have used generative AI," as AI features become increasingly embedded in popular legal tools. One day, it "may eventually become unreasonable to expect" lawyers "to verify every generative AI output," Browning suggested.&lt;/p&gt;
&lt;p&gt;Most likely—as a judicial ethics panel from Michigan has concluded—judges will determine "the best course of action for their courts with the ever-expanding use of AI," Browning's article noted. And the former justice told Ars that's why education will be key, for both lawyers and judges, as AI advances and becomes more mainstream in court systems.&lt;/p&gt;
&lt;p&gt;In an upcoming summer 2025 article in&amp;nbsp;&lt;em&gt;The Journal of Appellate Practice &amp;amp; Process&lt;/em&gt;, "The Dawn of the AI Judge," Browning attempts to soothe readers by saying that AI isn't yet fueling a legal dystopia. And humans are unlikely to face "robot judges" spouting AI-generated opinions any time soon, the former justice suggested.&lt;/p&gt;
&lt;p&gt;Standing in the way of that, at least two states—Michigan and West Virginia—"have already issued judicial ethics opinions requiring judges to be 'tech competent' when it comes to AI," Browning told Ars. And "other state supreme courts have adopted official policies regarding AI," he noted, further pressuring judges to bone up on AI.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Meanwhile, several states have set up task forces to monitor their regional court systems and issue AI guidance, while states like Virginia and Montana have passed laws requiring human oversight for any AI systems used in criminal justice decisions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Judges must prepare to spot obvious AI red flags&lt;/h2&gt;
&lt;p&gt;Until courts figure out how to navigate AI—a process that may look different from court to court—Browning advocates for more education and ethical guidance for judges to steer their use and attitudes about AI. That could help equip judges to avoid both ignorance of the many AI pitfalls and overconfidence in AI outputs, potentially protecting courts from AI hallucinations, biases, and evidentiary challenges sneaking past systems requiring human review and scrambling the court system.&lt;/p&gt;
&lt;p&gt;An overlooked part of educating judges could be exposing AI's influence so far in courts across the US. Henderson's team is planning research that tracks which models attorneys are using most in courts. That could reveal "the potential legal arguments that these models are pushing" to sway courts—and which judicial interventions might be needed, Henderson told Ars.&lt;/p&gt;
&lt;p&gt;"Over the next few years, researchers—like those in our group, the POLARIS Lab—will need to develop new ways to track the massive influence that AI will have and understand ways to intervene," Henderson told Ars. "For example, is any model pushing a particular perspective on legal doctrine across many different cases? Was it explicitly trained or instructed to do so?"&lt;/p&gt;
&lt;p&gt;Henderson also advocates for "an open, free centralized repository of case law," which would make it easier for everyone to check for fake AI citations. "With such a repository, it is easier for groups like ours to build tools that can quickly and accurately verify citations," Henderson said. That could be a significant improvement to the current decentralized court reporting system that often obscures case information behind various paywalls.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Dazza Greenwood, who co-chairs MIT's Task Force on Responsible Use of Generative AI for Law, did not have time to send comments but pointed Ars to a LinkedIn thread where he suggested that a structural response may be needed to ensure that all fake AI citations are caught every time.&lt;/p&gt;
&lt;p&gt;He recommended that courts create "a bounty system whereby counter-parties or other officers of the court receive sanctions payouts for fabricated cases cited in judicial filings that they reported first." That way, lawyers will know that their work will "always" be checked and thus may shift their behavior if they've been automatically filing AI-drafted documents. In turn, that could alleviate pressure on judges to serve as watchdogs. It also wouldn't cost much—mostly just redistributing the exact amount of fees that lawyers are sanctioned to AI spotters.&lt;/p&gt;
&lt;p&gt;Novel solutions like this may be necessary, Greenwood suggested. Responding to a question asking if "shame and sanctions" are enough to stop AI hallucinations in court, Greenwood said that eliminating AI errors is imperative because it "gives both otherwise generally good lawyers and otherwise generally good technology a bad name." Continuing to ban AI or suspend lawyers as a preferred solution risks dwindling court resources just as cases likely spike rather than potentially confronting the problem head-on.&lt;/p&gt;
&lt;p&gt;Of course, there's no guarantee that the bounty system would work. But "would the fact of such definite confidence that your cures will be individually checked and fabricated cites reported be enough to finally... convince lawyers who cut these corners that they should not cut these corners?"&lt;/p&gt;
&lt;p&gt;In absence of a fake case detector like Henderson wants to build, experts told Ars that there are some obvious red flags that judges can note to catch AI-hallucinated filings.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Any case number with "123456" in it probably warrants review, Henderson told Ars. And Browning noted that AI tends to mix up locations for cases, too. "For example, a cite to a purported Texas case that has a 'S.E. 2d' reporter wouldn't make sense, since Texas cases would be found in the Southwest Reporter," Browning said, noting that some appellate judges have already relied on this red flag to catch AI misuses.&lt;/p&gt;
&lt;p&gt;Those red flags would perhaps be easier to check with the open source tool that Henderson's lab wants to make, but Browning said there are other tell-tale signs of AI usage that anyone who has ever used a chatbot is likely familiar with.&lt;/p&gt;
&lt;p&gt;"Sometimes a red flag is the language cited from the hallucinated case; if it has some of the stilted language that can sometimes betray AI use, it might be a hallucination," Browning said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Judges already issuing AI-assisted opinions&lt;/h2&gt;
&lt;p&gt;Several states have assembled task forces like Greenwood's to assess the risks and benefits of using AI in courts. In Georgia, the Judicial Council of Georgia Ad Hoc Committee on Artificial Intelligence and the Courts released a report in early July providing "recommendations to help maintain public trust and confidence in the judicial system as the use of AI increases" in that state.&lt;/p&gt;
&lt;p&gt;Adopting the committee's recommendations could establish "long-term leadership and governance"; a repository of approved AI tools, education, and training for judicial professionals; and more transparency on AI used in Georgia courts. But the committee expects it will take three years to implement those recommendations while AI use continues to grow.&lt;/p&gt;
&lt;p&gt;Possibly complicating things further as judges start to explore using AI assistants to help draft their filings, the committee concluded that it's still too early to tell if the judges' code of conduct should be changed to prevent "unintentional use of biased algorithms, improper delegation to automated tools, or misuse of AI-generated data in judicial decision-making." That means, at least for now, that there will be no code-of-conduct changes in Georgia, where the only case in which AI hallucinations are believed to have swayed a judge has been found.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Notably, the committee's report also confirmed that there are no role models for courts to follow, as "there are no well-established regulatory environments with respect to the adoption of AI technologies by judicial systems." Browning, who chaired a now-defunct Texas AI task force, told Ars that judges lacking guidance will need to stay on their toes to avoid trampling legal rights. (A spokesperson for the State Bar of Texas told Ars the task force's work "concluded" and "resulted in the creation of the new standing committee on Emerging Technology," which offers general tips and guidance for judges in a recently launched AI Toolkit.)&lt;/p&gt;
&lt;p&gt;"While I definitely think lawyers have their own duties regarding AI use, I believe that judges have a similar responsibility to be vigilant when it comes to AI use as well," Browning said.&lt;/p&gt;
&lt;p&gt;Judges will continue sorting through AI-fueled submissions not just from pro se litigants representing themselves but also from up-and-coming young lawyers who may be more inclined to use AI, and even seasoned lawyers who have been sanctioned up to $5,000 for failing to check AI drafts, Browning suggested.&lt;/p&gt;
&lt;p&gt;In his upcoming "AI Judge" article, Browning points to at least one judge, 11th Circuit Court of Appeals Judge Kevin Newsom, who has used AI as a "mini experiment" in preparing opinions for both a civil case involving an insurance coverage issue and a criminal matter focused on sentencing guidelines. Browning seems to appeal to judges' egos to get them to study up so they can use AI to enhance their decision-making and possibly expand public trust in courts, not undermine it.&lt;/p&gt;
&lt;p&gt;"Regardless of the technological advances that can support a judge’s decision-making, the ultimate responsibility will always remain with the flesh-and-blood judge and his application of very human qualities—legal reasoning, empathy, strong regard for fairness, and unwavering commitment to ethics," Browning wrote. "These qualities can never be replicated by an AI tool."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Judges pushed to bone up on AI or risk destroying their court's authority.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="A judge points to a diagram of a hand with six fingers" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/the-judge-and-the-six-fingered-man.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Order in the court! Order in the court! Judges are facing outcry over a suspected AI-generated order in a court.&lt;/p&gt;
&lt;p&gt;Fueling nightmares that AI may soon decide legal battles, a Georgia court of appeals judge, Jeff Watkins, explained why a three-judge panel vacated an order last month that appears to be the first known ruling in which a judge sided with someone seemingly relying on fake AI-generated case citations to win a legal fight.&lt;/p&gt;
&lt;p&gt;Now, experts are warning that judges overlooking AI hallucinations in court filings could easily become commonplace, especially in the typically overwhelmed lower courts. And so far, only two states have moved to force judges to sharpen their tech competencies and adapt so they can spot AI red flags and theoretically stop disruptions to the justice system at all levels.&lt;/p&gt;
&lt;p&gt;The recently vacated order came in a Georgia divorce dispute, where Watkins explained that the order itself was drafted by the husband's lawyer, Diana Lynch. That's a common practice in many courts, where overburdened judges historically rely on lawyers to draft orders. But that protocol today faces heightened scrutiny as lawyers and non-lawyers increasingly rely on AI to compose and research legal filings, and judges risk rubberstamping fake opinions by not carefully scrutinizing AI-generated citations.&lt;/p&gt;
&lt;p&gt;The errant order partly relied on "two fictitious cases" to deny the wife's petition—which Watkins suggested were "possibly 'hallucinations' made up by generative-artificial intelligence"—as well as two cases that had "nothing to do" with the wife's petition.&lt;/p&gt;
&lt;p&gt;Lynch was hit with $2,500 in sanctions after the wife appealed, and the husband's response—which also appeared to be prepared by Lynch—cited 11 additional cases that were "either hallucinated" or irrelevant. Watkins was further peeved that Lynch supported a request for attorney's fees for the appeal by citing "one of the new hallucinated cases," writing it added "insult to injury."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Worryingly, the judge could not confirm whether the fake cases were generated by AI or even determine if Lynch inserted the bogus cases into the court filings, indicating how hard it can be for courts to hold lawyers accountable for suspected AI hallucinations. Lynch did not respond to Ars' request to comment, and her website appeared to be taken down following media attention to the case.&lt;/p&gt;
&lt;p&gt;But Watkins noted that "the irregularities in these filings suggest that they were drafted using generative AI" while warning that many "harms flow from the submission of fake opinions." Exposing deceptions can waste time and money, and AI misuse can deprive people of raising their best arguments. Fake orders can also soil judges' and courts' reputations and promote "cynicism" in the justice system. If left unchecked, Watkins warned, these harms could pave the way to a future where a "litigant may be tempted to defy a judicial ruling by disingenuously claiming doubt about its authenticity."&lt;/p&gt;
&lt;p&gt;"We have no information regarding why Appellee’s Brief repeatedly cites to nonexistent cases and can only speculate that the Brief may have been prepared by AI," Watkins wrote.&lt;/p&gt;
&lt;p&gt;Ultimately, Watkins remanded the case, partly because the fake cases made it impossible for the appeals court to adequately review the wife's petition to void the prior order. But no matter the outcome of the Georgia case, the initial order will likely forever be remembered as a cautionary tale for judges increasingly scrutinized for failures to catch AI misuses in court.&lt;/p&gt;
&lt;h2&gt;“Frighteningly likely” judge’s AI misstep will be repeated&lt;/h2&gt;
&lt;p&gt;John Browning, a retired justice on Texas' Fifth Court of Appeals and now a full-time law professor at Faulkner University, last year published a law article Watkins cited that warned of the ethical risks of lawyers using AI. In the article, Browning emphasized that the biggest concern at that point was that lawyers "will use generative AI to produce work product they treat as a final draft, without confirming the accuracy of the information contained therein or without applying their own independent professional judgment."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Today, judges are increasingly drawing the same scrutiny, and Browning told Ars he thinks it's "frighteningly likely that we will see more cases" like the Georgia divorce dispute, in which "a trial court unwittingly incorporates bogus case citations that an attorney includes in a proposed order" or even potentially in "proposed findings of fact and conclusions of law."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;"I can envision such a scenario in any number of situations in which a trial judge maintains a heavy docket and looks to counsel to work cooperatively in submitting proposed orders, including not just family law cases but other civil and even criminal matters," Browning told Ars.&lt;/p&gt;
&lt;p&gt;According to reporting from the National Center for State Courts, a nonprofit representing court leaders and professionals who are advocating for better judicial resources, AI tools like ChatGPT have made it easier for high-volume filers and unrepresented litigants who can't afford attorneys to file more cases, potentially further bogging down courts.&lt;/p&gt;
&lt;p&gt;Peter Henderson, a researcher who runs the Princeton Language+Law, Artificial Intelligence, &amp;amp; Society (POLARIS) Lab, told Ars that he expects cases like the Georgia divorce dispute aren't happening every day just yet.&lt;/p&gt;
&lt;p&gt;It's likely that a "few hallucinated citations go overlooked" because generally, fake cases are flagged through "the adversarial nature of the US legal system," he suggested. Browning further noted that trial judges are generally "very diligent in spotting when a lawyer is citing questionable authority or misleading the court about what a real case actually said or stood for."&lt;/p&gt;
&lt;p&gt;Henderson agreed with Browning that "in courts with much higher case loads and less adversarial process, this may happen more often." But Henderson noted that the appeals court catching the fake cases is an example of the adversarial process working.&lt;/p&gt;
&lt;p&gt;While that's true in this case, it seems likely that anyone exhausted by the divorce legal process, for example, may not pursue an appeal if they don't have energy or resources to discover and overturn errant orders.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Judges’ AI competency increasingly questioned&lt;/h2&gt;
&lt;p&gt;While recent history confirms that lawyers risk being sanctioned, fired from their firms, or suspended from practicing law for citing fake AI-generated cases, judges will likely only risk embarrassment for failing to catch lawyers' errors or even for using AI to research their own opinions.&lt;/p&gt;
&lt;p&gt;Not every judge is prepared to embrace AI without proper vetting, though. To shield the legal system, some judges have banned AI. Others have required disclosures—with some even demanding to know which specific AI tool was used—but that solution has not caught on everywhere.&lt;/p&gt;
&lt;p&gt;Even if all courts required disclosures, Browning pointed out that disclosures still aren't a perfect solution since "it may be difficult for lawyers to even discern whether they have used generative AI," as AI features become increasingly embedded in popular legal tools. One day, it "may eventually become unreasonable to expect" lawyers "to verify every generative AI output," Browning suggested.&lt;/p&gt;
&lt;p&gt;Most likely—as a judicial ethics panel from Michigan has concluded—judges will determine "the best course of action for their courts with the ever-expanding use of AI," Browning's article noted. And the former justice told Ars that's why education will be key, for both lawyers and judges, as AI advances and becomes more mainstream in court systems.&lt;/p&gt;
&lt;p&gt;In an upcoming summer 2025 article in&amp;nbsp;&lt;em&gt;The Journal of Appellate Practice &amp;amp; Process&lt;/em&gt;, "The Dawn of the AI Judge," Browning attempts to soothe readers by saying that AI isn't yet fueling a legal dystopia. And humans are unlikely to face "robot judges" spouting AI-generated opinions any time soon, the former justice suggested.&lt;/p&gt;
&lt;p&gt;Standing in the way of that, at least two states—Michigan and West Virginia—"have already issued judicial ethics opinions requiring judges to be 'tech competent' when it comes to AI," Browning told Ars. And "other state supreme courts have adopted official policies regarding AI," he noted, further pressuring judges to bone up on AI.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Meanwhile, several states have set up task forces to monitor their regional court systems and issue AI guidance, while states like Virginia and Montana have passed laws requiring human oversight for any AI systems used in criminal justice decisions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Judges must prepare to spot obvious AI red flags&lt;/h2&gt;
&lt;p&gt;Until courts figure out how to navigate AI—a process that may look different from court to court—Browning advocates for more education and ethical guidance for judges to steer their use and attitudes about AI. That could help equip judges to avoid both ignorance of the many AI pitfalls and overconfidence in AI outputs, potentially protecting courts from AI hallucinations, biases, and evidentiary challenges sneaking past systems requiring human review and scrambling the court system.&lt;/p&gt;
&lt;p&gt;An overlooked part of educating judges could be exposing AI's influence so far in courts across the US. Henderson's team is planning research that tracks which models attorneys are using most in courts. That could reveal "the potential legal arguments that these models are pushing" to sway courts—and which judicial interventions might be needed, Henderson told Ars.&lt;/p&gt;
&lt;p&gt;"Over the next few years, researchers—like those in our group, the POLARIS Lab—will need to develop new ways to track the massive influence that AI will have and understand ways to intervene," Henderson told Ars. "For example, is any model pushing a particular perspective on legal doctrine across many different cases? Was it explicitly trained or instructed to do so?"&lt;/p&gt;
&lt;p&gt;Henderson also advocates for "an open, free centralized repository of case law," which would make it easier for everyone to check for fake AI citations. "With such a repository, it is easier for groups like ours to build tools that can quickly and accurately verify citations," Henderson said. That could be a significant improvement to the current decentralized court reporting system that often obscures case information behind various paywalls.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Dazza Greenwood, who co-chairs MIT's Task Force on Responsible Use of Generative AI for Law, did not have time to send comments but pointed Ars to a LinkedIn thread where he suggested that a structural response may be needed to ensure that all fake AI citations are caught every time.&lt;/p&gt;
&lt;p&gt;He recommended that courts create "a bounty system whereby counter-parties or other officers of the court receive sanctions payouts for fabricated cases cited in judicial filings that they reported first." That way, lawyers will know that their work will "always" be checked and thus may shift their behavior if they've been automatically filing AI-drafted documents. In turn, that could alleviate pressure on judges to serve as watchdogs. It also wouldn't cost much—mostly just redistributing the exact amount of fees that lawyers are sanctioned to AI spotters.&lt;/p&gt;
&lt;p&gt;Novel solutions like this may be necessary, Greenwood suggested. Responding to a question asking if "shame and sanctions" are enough to stop AI hallucinations in court, Greenwood said that eliminating AI errors is imperative because it "gives both otherwise generally good lawyers and otherwise generally good technology a bad name." Continuing to ban AI or suspend lawyers as a preferred solution risks dwindling court resources just as cases likely spike rather than potentially confronting the problem head-on.&lt;/p&gt;
&lt;p&gt;Of course, there's no guarantee that the bounty system would work. But "would the fact of such definite confidence that your cures will be individually checked and fabricated cites reported be enough to finally... convince lawyers who cut these corners that they should not cut these corners?"&lt;/p&gt;
&lt;p&gt;In absence of a fake case detector like Henderson wants to build, experts told Ars that there are some obvious red flags that judges can note to catch AI-hallucinated filings.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Any case number with "123456" in it probably warrants review, Henderson told Ars. And Browning noted that AI tends to mix up locations for cases, too. "For example, a cite to a purported Texas case that has a 'S.E. 2d' reporter wouldn't make sense, since Texas cases would be found in the Southwest Reporter," Browning said, noting that some appellate judges have already relied on this red flag to catch AI misuses.&lt;/p&gt;
&lt;p&gt;Those red flags would perhaps be easier to check with the open source tool that Henderson's lab wants to make, but Browning said there are other tell-tale signs of AI usage that anyone who has ever used a chatbot is likely familiar with.&lt;/p&gt;
&lt;p&gt;"Sometimes a red flag is the language cited from the hallucinated case; if it has some of the stilted language that can sometimes betray AI use, it might be a hallucination," Browning said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Judges already issuing AI-assisted opinions&lt;/h2&gt;
&lt;p&gt;Several states have assembled task forces like Greenwood's to assess the risks and benefits of using AI in courts. In Georgia, the Judicial Council of Georgia Ad Hoc Committee on Artificial Intelligence and the Courts released a report in early July providing "recommendations to help maintain public trust and confidence in the judicial system as the use of AI increases" in that state.&lt;/p&gt;
&lt;p&gt;Adopting the committee's recommendations could establish "long-term leadership and governance"; a repository of approved AI tools, education, and training for judicial professionals; and more transparency on AI used in Georgia courts. But the committee expects it will take three years to implement those recommendations while AI use continues to grow.&lt;/p&gt;
&lt;p&gt;Possibly complicating things further as judges start to explore using AI assistants to help draft their filings, the committee concluded that it's still too early to tell if the judges' code of conduct should be changed to prevent "unintentional use of biased algorithms, improper delegation to automated tools, or misuse of AI-generated data in judicial decision-making." That means, at least for now, that there will be no code-of-conduct changes in Georgia, where the only case in which AI hallucinations are believed to have swayed a judge has been found.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Notably, the committee's report also confirmed that there are no role models for courts to follow, as "there are no well-established regulatory environments with respect to the adoption of AI technologies by judicial systems." Browning, who chaired a now-defunct Texas AI task force, told Ars that judges lacking guidance will need to stay on their toes to avoid trampling legal rights. (A spokesperson for the State Bar of Texas told Ars the task force's work "concluded" and "resulted in the creation of the new standing committee on Emerging Technology," which offers general tips and guidance for judges in a recently launched AI Toolkit.)&lt;/p&gt;
&lt;p&gt;"While I definitely think lawyers have their own duties regarding AI use, I believe that judges have a similar responsibility to be vigilant when it comes to AI use as well," Browning said.&lt;/p&gt;
&lt;p&gt;Judges will continue sorting through AI-fueled submissions not just from pro se litigants representing themselves but also from up-and-coming young lawyers who may be more inclined to use AI, and even seasoned lawyers who have been sanctioned up to $5,000 for failing to check AI drafts, Browning suggested.&lt;/p&gt;
&lt;p&gt;In his upcoming "AI Judge" article, Browning points to at least one judge, 11th Circuit Court of Appeals Judge Kevin Newsom, who has used AI as a "mini experiment" in preparing opinions for both a civil case involving an insurance coverage issue and a criminal matter focused on sentencing guidelines. Browning seems to appeal to judges' egos to get them to study up so they can use AI to enhance their decision-making and possibly expand public trust in courts, not undermine it.&lt;/p&gt;
&lt;p&gt;"Regardless of the technological advances that can support a judge’s decision-making, the ultimate responsibility will always remain with the flesh-and-blood judge and his application of very human qualities—legal reasoning, empathy, strong regard for fairness, and unwavering commitment to ethics," Browning wrote. "These qualities can never be replicated by an AI tool."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/07/its-frighteningly-likely-many-us-courts-will-overlook-ai-errors-expert-says/</guid><pubDate>Mon, 21 Jul 2025 11:00:02 +0000</pubDate></item><item><title>The unique, mathematical shortcuts language models use to predict dynamic scenarios (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/unique-mathematical-shortcuts-language-models-use-to-predict-dynamic-scenarios-0721</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-csail-llms.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;Let’s say you’re reading a story, or playing a game of chess. You may not have noticed, but each step of the way, your mind kept track of how the situation (or “state of the world”) was changing. You can imagine this as a sort of sequence of events list, which we use to update our prediction of what will happen next.&lt;/p&gt;&lt;p&gt;Language models like ChatGPT also track changes inside their own “mind” when finishing off a block of code or anticipating what you’ll write next. They typically make educated guesses using transformers — internal architectures that help the models understand sequential data — but the systems are sometimes incorrect because of flawed thinking patterns. Identifying and tweaking these underlying mechanisms helps language models become more reliable prognosticators, especially with more dynamic tasks like forecasting weather and financial markets.&lt;/p&gt;&lt;p&gt;But do these AI systems process developing situations like we do? A new&amp;nbsp;paper from researchers in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Department of Electrical Engineering and Computer Science shows that the models instead use clever mathematical shortcuts between each progressive step in a sequence, eventually making reasonable predictions. The team made this observation by going under the hood of language models, evaluating how closely they could keep track of objects that change position rapidly. Their findings show that engineers can control when language models use particular workarounds as a way to improve the systems’ predictive capabilities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Shell games&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;The researchers analyzed the inner workings of these models using a clever experiment reminiscent of a classic concentration game. Ever had to guess the final location of an object after it’s placed under a cup and shuffled with identical containers? The team used a similar test, where the model guessed the final arrangement of particular digits (also called a permutation). The models were given a starting sequence, such as “42135,” and instructions about when and where to move each digit, like moving the “4” to the third position and onward, without knowing the final result.&lt;/p&gt;&lt;p&gt;In these experiments, transformer-based models gradually learned to predict the correct final arrangements. Instead of shuffling the digits based on the instructions they were given, though, the systems aggregated information between successive states (or individual steps within the sequence) and calculated the final permutation.&lt;/p&gt;&lt;p dir="ltr"&gt;One go-to pattern the team observed, called the “Associative Algorithm,” essentially organizes nearby steps into groups and then calculates a final guess. You can think of this process as being structured like a tree, where the initial numerical arrangement is the “root.” As you move up the tree, adjacent steps are grouped into different branches and multiplied together. At the top of the tree is the final combination of numbers, computed by multiplying each resulting sequence on the branches together.&lt;/p&gt;&lt;p&gt;The other way language models guessed the final permutation was through a crafty mechanism called the “Parity-Associative Algorithm,” which essentially whittles down options before grouping them. It determines whether the final arrangement is the result of an even or odd number of rearrangements of individual digits. Then, the mechanism groups adjacent sequences from different steps before multiplying them, just like the Associative Algorithm.&lt;/p&gt;&lt;p&gt;“These behaviors tell us that transformers perform simulation by associative scan. Instead of following state changes step-by-step, the models organize them into hierarchies,” says MIT PhD student and CSAIL affiliate Belinda Li SM ’23, a lead author on the paper. “How do we encourage transformers to learn better state tracking? Instead of imposing that these systems form inferences about data in a human-like, sequential way, perhaps we should cater to the approaches they naturally use when tracking state changes.”&lt;/p&gt;&lt;p&gt;“One avenue of research has been to expand test-time computing along the depth dimension, rather than the token dimension — by increasing the number of transformer layers rather than the number of chain-of-thought tokens during test-time reasoning,” adds Li. “Our work suggests that this approach would allow transformers to build deeper reasoning trees.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Through the looking glass&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Li and her co-authors observed how the Associative and Parity-Associative algorithms worked using tools that allowed them to peer inside the “mind” of language models.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;They first used a method called “probing,” which shows what information flows through an AI system. Imagine you could look into a model’s brain to see its thoughts at a specific moment — in a similar way, the technique maps out the system’s mid-experiment predictions about the final arrangement of digits.&lt;/p&gt;&lt;p dir="ltr"&gt;A tool called “activation patching” was then used to show where the language model processes changes to a situation. It involves meddling with some of the system’s “ideas,” injecting incorrect information into certain parts of the network while keeping other parts constant, and seeing how the system will adjust its predictions.&lt;/p&gt;&lt;p dir="ltr"&gt;These tools revealed when the algorithms would make errors and when the systems “figured out” how to correctly guess the final permutations. They observed that the Associative Algorithm learned faster than the Parity-Associative Algorithm, while also performing better on longer sequences. Li attributes the latter’s difficulties with more elaborate instructions to an over-reliance on heuristics (or rules that allow us to compute a reasonable solution fast) to predict permutations.&lt;/p&gt;&lt;p dir="ltr"&gt;“We’ve found that when language models use a heuristic early on in training, they’ll start to build these tricks into their mechanisms,” says Li. “However, those models tend to generalize worse than ones that don’t rely on heuristics. We found that certain pre-training objectives can deter or encourage these patterns, so in the future, we may look to design techniques that discourage models from picking up bad habits.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers note that their experiments were done on small-scale language models fine-tuned on synthetic data, but found the model size had little effect on the results. This suggests that fine-tuning larger language models, like GPT 4.1, would likely yield similar results. The team plans to examine their hypotheses more closely by testing language models of different sizes that haven’t been fine-tuned, evaluating their performance on dynamic real-world tasks such as tracking code and following how stories evolve.&lt;/p&gt;&lt;p&gt;Harvard University postdoc Keyon Vafa, who was not involved in the paper, says that the researchers’ findings could create opportunities to advance language models. “Many uses of large language models rely on tracking state: anything from providing recipes to writing code to keeping track of details in a conversation,” he says. “This paper makes significant progress in understanding how language models perform these tasks. This progress provides us with interesting insights into what language models are doing and offers promising new strategies for improving them.”&lt;/p&gt;&lt;p&gt;Li wrote the paper with MIT undergraduate student Zifan “Carl” Guo and senior author Jacob Andreas, who is an MIT associate professor of electrical engineering and computer science and CSAIL principal investigator. Their research was supported, in part, by Open Philanthropy, the MIT Quest for Intelligence, the National Science Foundation, the Clare Boothe Luce Program for Women in STEM, and a Sloan Research Fellowship.&lt;/p&gt;&lt;p&gt;The researchers presented their research at the International Conference on Machine Learning (ICML) this week.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-csail-llms.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;Let’s say you’re reading a story, or playing a game of chess. You may not have noticed, but each step of the way, your mind kept track of how the situation (or “state of the world”) was changing. You can imagine this as a sort of sequence of events list, which we use to update our prediction of what will happen next.&lt;/p&gt;&lt;p&gt;Language models like ChatGPT also track changes inside their own “mind” when finishing off a block of code or anticipating what you’ll write next. They typically make educated guesses using transformers — internal architectures that help the models understand sequential data — but the systems are sometimes incorrect because of flawed thinking patterns. Identifying and tweaking these underlying mechanisms helps language models become more reliable prognosticators, especially with more dynamic tasks like forecasting weather and financial markets.&lt;/p&gt;&lt;p&gt;But do these AI systems process developing situations like we do? A new&amp;nbsp;paper from researchers in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Department of Electrical Engineering and Computer Science shows that the models instead use clever mathematical shortcuts between each progressive step in a sequence, eventually making reasonable predictions. The team made this observation by going under the hood of language models, evaluating how closely they could keep track of objects that change position rapidly. Their findings show that engineers can control when language models use particular workarounds as a way to improve the systems’ predictive capabilities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Shell games&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-5c520fa2-7fff-f3a1-cecc-40dddcfcf883"&gt;The researchers analyzed the inner workings of these models using a clever experiment reminiscent of a classic concentration game. Ever had to guess the final location of an object after it’s placed under a cup and shuffled with identical containers? The team used a similar test, where the model guessed the final arrangement of particular digits (also called a permutation). The models were given a starting sequence, such as “42135,” and instructions about when and where to move each digit, like moving the “4” to the third position and onward, without knowing the final result.&lt;/p&gt;&lt;p&gt;In these experiments, transformer-based models gradually learned to predict the correct final arrangements. Instead of shuffling the digits based on the instructions they were given, though, the systems aggregated information between successive states (or individual steps within the sequence) and calculated the final permutation.&lt;/p&gt;&lt;p dir="ltr"&gt;One go-to pattern the team observed, called the “Associative Algorithm,” essentially organizes nearby steps into groups and then calculates a final guess. You can think of this process as being structured like a tree, where the initial numerical arrangement is the “root.” As you move up the tree, adjacent steps are grouped into different branches and multiplied together. At the top of the tree is the final combination of numbers, computed by multiplying each resulting sequence on the branches together.&lt;/p&gt;&lt;p&gt;The other way language models guessed the final permutation was through a crafty mechanism called the “Parity-Associative Algorithm,” which essentially whittles down options before grouping them. It determines whether the final arrangement is the result of an even or odd number of rearrangements of individual digits. Then, the mechanism groups adjacent sequences from different steps before multiplying them, just like the Associative Algorithm.&lt;/p&gt;&lt;p&gt;“These behaviors tell us that transformers perform simulation by associative scan. Instead of following state changes step-by-step, the models organize them into hierarchies,” says MIT PhD student and CSAIL affiliate Belinda Li SM ’23, a lead author on the paper. “How do we encourage transformers to learn better state tracking? Instead of imposing that these systems form inferences about data in a human-like, sequential way, perhaps we should cater to the approaches they naturally use when tracking state changes.”&lt;/p&gt;&lt;p&gt;“One avenue of research has been to expand test-time computing along the depth dimension, rather than the token dimension — by increasing the number of transformer layers rather than the number of chain-of-thought tokens during test-time reasoning,” adds Li. “Our work suggests that this approach would allow transformers to build deeper reasoning trees.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Through the looking glass&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Li and her co-authors observed how the Associative and Parity-Associative algorithms worked using tools that allowed them to peer inside the “mind” of language models.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;They first used a method called “probing,” which shows what information flows through an AI system. Imagine you could look into a model’s brain to see its thoughts at a specific moment — in a similar way, the technique maps out the system’s mid-experiment predictions about the final arrangement of digits.&lt;/p&gt;&lt;p dir="ltr"&gt;A tool called “activation patching” was then used to show where the language model processes changes to a situation. It involves meddling with some of the system’s “ideas,” injecting incorrect information into certain parts of the network while keeping other parts constant, and seeing how the system will adjust its predictions.&lt;/p&gt;&lt;p dir="ltr"&gt;These tools revealed when the algorithms would make errors and when the systems “figured out” how to correctly guess the final permutations. They observed that the Associative Algorithm learned faster than the Parity-Associative Algorithm, while also performing better on longer sequences. Li attributes the latter’s difficulties with more elaborate instructions to an over-reliance on heuristics (or rules that allow us to compute a reasonable solution fast) to predict permutations.&lt;/p&gt;&lt;p dir="ltr"&gt;“We’ve found that when language models use a heuristic early on in training, they’ll start to build these tricks into their mechanisms,” says Li. “However, those models tend to generalize worse than ones that don’t rely on heuristics. We found that certain pre-training objectives can deter or encourage these patterns, so in the future, we may look to design techniques that discourage models from picking up bad habits.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers note that their experiments were done on small-scale language models fine-tuned on synthetic data, but found the model size had little effect on the results. This suggests that fine-tuning larger language models, like GPT 4.1, would likely yield similar results. The team plans to examine their hypotheses more closely by testing language models of different sizes that haven’t been fine-tuned, evaluating their performance on dynamic real-world tasks such as tracking code and following how stories evolve.&lt;/p&gt;&lt;p&gt;Harvard University postdoc Keyon Vafa, who was not involved in the paper, says that the researchers’ findings could create opportunities to advance language models. “Many uses of large language models rely on tracking state: anything from providing recipes to writing code to keeping track of details in a conversation,” he says. “This paper makes significant progress in understanding how language models perform these tasks. This progress provides us with interesting insights into what language models are doing and offers promising new strategies for improving them.”&lt;/p&gt;&lt;p&gt;Li wrote the paper with MIT undergraduate student Zifan “Carl” Guo and senior author Jacob Andreas, who is an MIT associate professor of electrical engineering and computer science and CSAIL principal investigator. Their research was supported, in part, by Open Philanthropy, the MIT Quest for Intelligence, the National Science Foundation, the Clare Boothe Luce Program for Women in STEM, and a Sloan Research Fellowship.&lt;/p&gt;&lt;p&gt;The researchers presented their research at the International Conference on Machine Learning (ICML) this week.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/unique-mathematical-shortcuts-language-models-use-to-predict-dynamic-scenarios-0721</guid><pubDate>Mon, 21 Jul 2025 12:00:00 +0000</pubDate></item><item><title>The Download: how your data is being used to train AI, and why chatbots aren’t doctors (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/21/1120525/the-download-how-your-data-is-being-used-to-train-ai-and-why-chatbots-arent-doctors/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;A major AI training data set contains millions of examples of personal data&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Millions of images of passports, credit cards, birth certificates, and other documents containing personally identifiable information are likely included in one of the biggest open-source AI training sets, new research has found.&lt;/p&gt;&lt;p&gt;Thousands of images—including identifiable faces—were found in a small subset of DataComp CommonPool, a major AI training set for image generation scraped from the web. Because the researchers audited just 0.1% of CommonPool’s data, they estimate that the real number of images containing personally identifiable information, including faces and identity documents, is in the hundreds of millions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The bottom line? Anything you put online can be and probably has been scraped.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Eileen Guo&lt;/em&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI companies have stopped warning you that their chatbots aren’t doctors&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI companies have now mostly abandoned the once-standard practice of including medical disclaimers and warnings in response to health questions, new research has found. In fact, many leading AI models will now not only answer health questions but even ask follow-ups and attempt a diagnosis.&lt;/p&gt;&lt;p&gt;Such disclaimers serve an important reminder to people asking AI about everything from eating disorders to cancer diagnoses, the authors say, and their absence means that users of AI are more likely to trust unsafe medical advice. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Hackers exploited a flaw in Microsoft’s software to attack government agencies&lt;/strong&gt;&lt;br /&gt;Engineers across the world are racing to mitigate the risk it poses. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The attack hones in on servers housed within an organization, not the cloud. &lt;/em&gt;(WP $)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 The French government has launched a criminal probe into X&lt;/strong&gt;&lt;br /&gt;It’s investigating the company’s recommendation algorithm—but X isn’t cooperating. (FT $)&lt;br /&gt;&lt;em&gt;+ X says French lawmaker Eric Bothorel has accused it of manipulating its algorithm for foreign interference purposes. &lt;/em&gt;(Reuters)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Trump aides explored ending contracts with SpaceX&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;But they quickly found most of them are vital to the Defense Department and NASA. (WSJ $)&lt;br /&gt;+ &lt;em&gt;But that doesn’t mean it’s smooth sailing for SpaceX right now. &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;Rivals are rising to challenge the dominance of SpaceX. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;4 Meta has refused to sign the EU’s AI code of practice&lt;/strong&gt;&lt;br /&gt;Its new global affairs chief claims the rules with throttle growth. (CNBC)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;+ &lt;em&gt;The code is voluntary—but declining to sign it sends a clear message. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 A Polish programmer beat an OpenAI model in a coding competition&lt;/strong&gt;&lt;br /&gt;But only narrowly. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;The second wave of AI coding is here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Nigeria has dreams of becoming a major digital worker hub&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The rise of AI means there’s less outsourcing work to go round. (Rest of World)&lt;br /&gt;+ &lt;em&gt;What Africa needs to do to become a major AI player. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Microsoft is building a digital twin of the Notre-Dame Cathedral&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The replica can help support its ongoing maintenance, apparently. (Reuters)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 How funny is AI, really?&lt;/strong&gt;&lt;br /&gt;Not all senses of humor are made equal. (Undark)&lt;br /&gt;+ &lt;em&gt;What happened when 20 comedians got AI to write their routines. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What it’s like to forge a friendship with an AI&lt;/strong&gt;&lt;br /&gt;Student MJ Cocking found the experience incredibly helpful. (NYT $)&lt;br /&gt;+ &lt;em&gt;But chatbots can also fuel vulnerable people’s dangerous delusions. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;The AI relationship revolution is already here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Work has begun on the first space-based gravitational wave detector&lt;/strong&gt;&lt;br /&gt;The waves are triggered when massive objects like black holes collide. (IEEE Spectrum)&lt;br /&gt;+ &lt;em&gt;How the Rubin Observatory will help us understand dark matter and dark energy. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"There was just no way I was going to make it through four years of this."&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Egan Reich, a former worker in the US Department of Labor, explains why he accepted the agency's second deferred resignation offer in April after DOGE’s rollout, Insider reports.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2022/09/AuthWeb3a.jpeg?fit=1064,598" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The world is moving closer to a new cold war fought with authoritarian tech&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A cold war is brewing between the world’s autocracies and democracies—and technology is fueling it.&lt;/p&gt;&lt;p&gt;Authoritarian states are following China’s lead and are trending toward more digital rights abuses by increasing the mass digital surveillance of citizens, censorship, and controls on individual expression.&lt;/p&gt;&lt;p&gt;And while democracies also use massive amounts of surveillance technology, it’s the tech trade relationships between authoritarian countries that’s enabling the rise of digitally enabled social control. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Tate Ryan-Mosley&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;+ I need to sign up for Minneapolis’ annual cat tour immediately.&lt;br /&gt;+ What are the odds? This mother has had four babies, all born on July 7 in different years.&lt;br /&gt;+ Not content with being a rap legend, Snoop Dogg has become a co-owner of a Welsh soccer club.&lt;br /&gt;+ Appetite for Destruction, Guns n’ Roses’ outrageous debut album, was released on this day 38 years ago.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;A major AI training data set contains millions of examples of personal data&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Millions of images of passports, credit cards, birth certificates, and other documents containing personally identifiable information are likely included in one of the biggest open-source AI training sets, new research has found.&lt;/p&gt;&lt;p&gt;Thousands of images—including identifiable faces—were found in a small subset of DataComp CommonPool, a major AI training set for image generation scraped from the web. Because the researchers audited just 0.1% of CommonPool’s data, they estimate that the real number of images containing personally identifiable information, including faces and identity documents, is in the hundreds of millions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The bottom line? Anything you put online can be and probably has been scraped.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Eileen Guo&lt;/em&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI companies have stopped warning you that their chatbots aren’t doctors&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;AI companies have now mostly abandoned the once-standard practice of including medical disclaimers and warnings in response to health questions, new research has found. In fact, many leading AI models will now not only answer health questions but even ask follow-ups and attempt a diagnosis.&lt;/p&gt;&lt;p&gt;Such disclaimers serve an important reminder to people asking AI about everything from eating disorders to cancer diagnoses, the authors say, and their absence means that users of AI are more likely to trust unsafe medical advice. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Hackers exploited a flaw in Microsoft’s software to attack government agencies&lt;/strong&gt;&lt;br /&gt;Engineers across the world are racing to mitigate the risk it poses. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The attack hones in on servers housed within an organization, not the cloud. &lt;/em&gt;(WP $)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 The French government has launched a criminal probe into X&lt;/strong&gt;&lt;br /&gt;It’s investigating the company’s recommendation algorithm—but X isn’t cooperating. (FT $)&lt;br /&gt;&lt;em&gt;+ X says French lawmaker Eric Bothorel has accused it of manipulating its algorithm for foreign interference purposes. &lt;/em&gt;(Reuters)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Trump aides explored ending contracts with SpaceX&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;But they quickly found most of them are vital to the Defense Department and NASA. (WSJ $)&lt;br /&gt;+ &lt;em&gt;But that doesn’t mean it’s smooth sailing for SpaceX right now. &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;Rivals are rising to challenge the dominance of SpaceX. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;4 Meta has refused to sign the EU’s AI code of practice&lt;/strong&gt;&lt;br /&gt;Its new global affairs chief claims the rules with throttle growth. (CNBC)&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;+ &lt;em&gt;The code is voluntary—but declining to sign it sends a clear message. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 A Polish programmer beat an OpenAI model in a coding competition&lt;/strong&gt;&lt;br /&gt;But only narrowly. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;The second wave of AI coding is here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Nigeria has dreams of becoming a major digital worker hub&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The rise of AI means there’s less outsourcing work to go round. (Rest of World)&lt;br /&gt;+ &lt;em&gt;What Africa needs to do to become a major AI player. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Microsoft is building a digital twin of the Notre-Dame Cathedral&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;The replica can help support its ongoing maintenance, apparently. (Reuters)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 How funny is AI, really?&lt;/strong&gt;&lt;br /&gt;Not all senses of humor are made equal. (Undark)&lt;br /&gt;+ &lt;em&gt;What happened when 20 comedians got AI to write their routines. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What it’s like to forge a friendship with an AI&lt;/strong&gt;&lt;br /&gt;Student MJ Cocking found the experience incredibly helpful. (NYT $)&lt;br /&gt;+ &lt;em&gt;But chatbots can also fuel vulnerable people’s dangerous delusions. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;The AI relationship revolution is already here. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Work has begun on the first space-based gravitational wave detector&lt;/strong&gt;&lt;br /&gt;The waves are triggered when massive objects like black holes collide. (IEEE Spectrum)&lt;br /&gt;+ &lt;em&gt;How the Rubin Observatory will help us understand dark matter and dark energy. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"There was just no way I was going to make it through four years of this."&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Egan Reich, a former worker in the US Department of Labor, explains why he accepted the agency's second deferred resignation offer in April after DOGE’s rollout, Insider reports.&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2022/09/AuthWeb3a.jpeg?fit=1064,598" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The world is moving closer to a new cold war fought with authoritarian tech&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A cold war is brewing between the world’s autocracies and democracies—and technology is fueling it.&lt;/p&gt;&lt;p&gt;Authoritarian states are following China’s lead and are trending toward more digital rights abuses by increasing the mass digital surveillance of citizens, censorship, and controls on individual expression.&lt;/p&gt;&lt;p&gt;And while democracies also use massive amounts of surveillance technology, it’s the tech trade relationships between authoritarian countries that’s enabling the rise of digitally enabled social control. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Tate Ryan-Mosley&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;+ I need to sign up for Minneapolis’ annual cat tour immediately.&lt;br /&gt;+ What are the odds? This mother has had four babies, all born on July 7 in different years.&lt;br /&gt;+ Not content with being a rap legend, Snoop Dogg has become a co-owner of a Welsh soccer club.&lt;br /&gt;+ Appetite for Destruction, Guns n’ Roses’ outrageous debut album, was released on this day 38 years ago.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/21/1120525/the-download-how-your-data-is-being-used-to-train-ai-and-why-chatbots-arent-doctors/</guid><pubDate>Mon, 21 Jul 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Dia launches a skill gallery, Perplexity to add tasks to Comet (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/dia-launches-a-skill-gallery-perplexity-to-add-tasks-to-comet/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI-powered browsers are nowhere near the easy future they promise, where they can do complex multistep tasks for you. However, the makers of these browsers are trying to make users’ lives easier by adding a way to easily repeat some prompts for the tasks they frequently perform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Browser Company’s new Dia browser already has a skills feature, which lets users ask the browser to execute a command or create a code snippet based on a prompt. For instance, you could ask the browser to find interesting events happening in the next few days near you. You can save the prompt for future use and invoke it using a shortcut.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While the community has put together some threads and web pages to help users find skills created by other users, The Browser Company has now launched the 0.1 version of the official gallery. The gallery has a number of skills, organized by category, and you can simply copy the prompt to add the skill to your own library.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029452" height="331" src="https://techcrunch.com/wp-content/uploads/2025/07/Screenshot-2025-07-21-at-5.35.10PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity’s new browser Comet is also preparing to launch a similar feature. Over the weekend, the company’s CEO, Aravind Srinivas, said that the browser will get ready-made shortcuts for repetitive tasks like organizing tabs, preparing for meetings, or looking up trending topics on social media.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;shortcuts for repetitive tasks rolling out next week on comet. more invites will be sent next week too. the browser is going to be your personal console for getting work done. pic.twitter.com/xdzS8EzrhQ&lt;/p&gt;— Aravind Srinivas (@AravSrinivas) July 20, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;He also said users will be able to create their own “Tampermonkey-like scripts” using natural language prompts for common use cases.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI-powered browsers are nowhere near the easy future they promise, where they can do complex multistep tasks for you. However, the makers of these browsers are trying to make users’ lives easier by adding a way to easily repeat some prompts for the tasks they frequently perform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Browser Company’s new Dia browser already has a skills feature, which lets users ask the browser to execute a command or create a code snippet based on a prompt. For instance, you could ask the browser to find interesting events happening in the next few days near you. You can save the prompt for future use and invoke it using a shortcut.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While the community has put together some threads and web pages to help users find skills created by other users, The Browser Company has now launched the 0.1 version of the official gallery. The gallery has a number of skills, organized by category, and you can simply copy the prompt to add the skill to your own library.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029452" height="331" src="https://techcrunch.com/wp-content/uploads/2025/07/Screenshot-2025-07-21-at-5.35.10PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity’s new browser Comet is also preparing to launch a similar feature. Over the weekend, the company’s CEO, Aravind Srinivas, said that the browser will get ready-made shortcuts for repetitive tasks like organizing tabs, preparing for meetings, or looking up trending topics on social media.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;shortcuts for repetitive tasks rolling out next week on comet. more invites will be sent next week too. the browser is going to be your personal console for getting work done. pic.twitter.com/xdzS8EzrhQ&lt;/p&gt;— Aravind Srinivas (@AravSrinivas) July 20, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;He also said users will be able to create their own “Tampermonkey-like scripts” using natural language prompts for common use cases.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/dia-launches-a-skill-gallery-perplexity-to-add-tasks-to-comet/</guid><pubDate>Mon, 21 Jul 2025 13:24:44 +0000</pubDate></item><item><title>[NEW] Don’t miss your chance to exhibit at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/dont-miss-your-chance-to-exhibit-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is just around the corner, and with more than 10,000 startup and VC leaders heading to Moscone West in San Francisco this October 27 to 29, the Expo Hall is where connections get made and business gets done. If you’ve been thinking about showcasing your company, consider this your nudge — exhibitor spots are filling fast, and once they’re gone, they’re gone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Exhibiting at Disrupt isn’t just about having a table — it’s about putting your startup in front of the people who matter most. Whether you’re looking to connect with investors, spark media coverage, recruit top talent, or meet early customers, Disrupt gives you the visibility to do it all in one place.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 exhibitor Google" class="wp-image-2979874" height="454" src="https://techcrunch.com/wp-content/uploads/2025/01/Google-Exhibit-Disrupt-2025.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;TechCrunch Disrupt 2024 exhibitor Google Cloud. October 28-30, 2024, at Moscone West, San Francisco, CA.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Brazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-what-s-included-in-your-exhibitor-package"&gt;&lt;strong&gt;What’s included in your exhibitor package&lt;/strong&gt;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s what you get when you exhibit at Disrupt 2025:&lt;/p&gt;







&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;6’ x 30″ exhibit table with linen and two chairs&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;11” x 14” tabletop sign with your startup’s branding&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Lead generation via the TechCrunch Disrupt app&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Complimentary partner Wi-Fi access&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Silver Tier sponsor branding&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Entry-level listing in the sponsor directory&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;One Founder Pass (additional founder perks)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 passes for your team and guests (5 Expo+, 5 General Admission)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;50% off additional General Admission Passes&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the TechCrunch Disrupt press list&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Logo on “thank you” slide during breaks&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Company logo and description on the event partner page&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Logo and profile in TechCrunch Disrupt mobile app&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Company name in select TechCrunch sponsor announcements&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Acknowledgment during the closing ceremony&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-make-your-startup-impossible-to-ignore"&gt;Make your startup impossible to ignore&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s Expo Hall will feature hundreds of startups, industry leaders, and breakout technologies. Your company could be right in the mix, meeting buyers, building a pipeline, and getting your product in front of the right audience at the right time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ready to go from stealth mode to the show floor? Secure your &lt;strong&gt;exhibit table&lt;/strong&gt; before they sell out. Space is limited, and tables are moving fast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Become an exhibitor at TechCrunch Disrupt 2025&lt;/strong&gt;!&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is just around the corner, and with more than 10,000 startup and VC leaders heading to Moscone West in San Francisco this October 27 to 29, the Expo Hall is where connections get made and business gets done. If you’ve been thinking about showcasing your company, consider this your nudge — exhibitor spots are filling fast, and once they’re gone, they’re gone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Exhibiting at Disrupt isn’t just about having a table — it’s about putting your startup in front of the people who matter most. Whether you’re looking to connect with investors, spark media coverage, recruit top talent, or meet early customers, Disrupt gives you the visibility to do it all in one place.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 exhibitor Google" class="wp-image-2979874" height="454" src="https://techcrunch.com/wp-content/uploads/2025/01/Google-Exhibit-Disrupt-2025.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;TechCrunch Disrupt 2024 exhibitor Google Cloud. October 28-30, 2024, at Moscone West, San Francisco, CA.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Brazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-what-s-included-in-your-exhibitor-package"&gt;&lt;strong&gt;What’s included in your exhibitor package&lt;/strong&gt;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s what you get when you exhibit at Disrupt 2025:&lt;/p&gt;







&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;6’ x 30″ exhibit table with linen and two chairs&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;11” x 14” tabletop sign with your startup’s branding&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Lead generation via the TechCrunch Disrupt app&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Complimentary partner Wi-Fi access&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Silver Tier sponsor branding&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Entry-level listing in the sponsor directory&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;One Founder Pass (additional founder perks)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 passes for your team and guests (5 Expo+, 5 General Admission)&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;50% off additional General Admission Passes&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Access to the TechCrunch Disrupt press list&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Logo on “thank you” slide during breaks&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Company logo and description on the event partner page&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Logo and profile in TechCrunch Disrupt mobile app&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Company name in select TechCrunch sponsor announcements&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Acknowledgment during the closing ceremony&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-make-your-startup-impossible-to-ignore"&gt;Make your startup impossible to ignore&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s Expo Hall will feature hundreds of startups, industry leaders, and breakout technologies. Your company could be right in the mix, meeting buyers, building a pipeline, and getting your product in front of the right audience at the right time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ready to go from stealth mode to the show floor? Secure your &lt;strong&gt;exhibit table&lt;/strong&gt; before they sell out. Space is limited, and tables are moving fast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Become an exhibitor at TechCrunch Disrupt 2025&lt;/strong&gt;!&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/dont-miss-your-chance-to-exhibit-at-techcrunch-disrupt-2025/</guid><pubDate>Mon, 21 Jul 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Exploring the context of online images with Backstory (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/exploring-the-context-of-online-images-with-backstory/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Sx0q14oEP6Ir8EEX0E4vyJGUFAAhycWqppr5x-2edL8Dz0OwB7ylqPIkRd9TefuQ9TaYxe-1aFMmmdH3aSJeFJ09YrqA16V2V-Dv7M0XJcbLyzZsvWE=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;We would like to thank Zoubin Ghahramani, Helen King, Rahul Sukthankar, Raia Hadsell, and Chandu Thota for their leadership and support.&lt;/p&gt;&lt;p&gt;This work was done thanks to the contributions of Mevan Babakar, Hannah Forbes-Pollard, Nikki Hariri, Thomas Leung, Nick Dufour, Ben Usman, Min Ma, Steve Pucci, Spudde Childs, Kate Harrison, Alanna Slocum, Reza Aghajani, Sri Rajendran, Alexey Vorobyov, Ashley Eden, Rishub Jain, Stephanie Chan, Sophie Bridgers, Michiel Bakker, Sures Kumar Thoddu Srinivasan, Tesh Goyal, and Ashish Chaudhary.&lt;/p&gt;&lt;p&gt;We would also like to thank Kent Walker, Camino Rojo, Clement Wolf, J.D. Velazquez, Tom Lue, Ndidi Elue, Rachel Stigler, M.H. Tessler, Ricardo Prada, William Isaac, Tom Stepleton, Zoe Darme, Gail Kent, Vincent Ryan, Aaron Donsbach, Abhishek Bapna, Verena Rieser, Christian Plagemann, Anca Dragan, Sven Gowal, Florian Stimberg, Christopher Savcak, Allison Garcia, Eve Novakovic, Armin Senoner, Arielle Bier, and the greater Google DeepMind and Google teams for their support, help, and feedback.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Sx0q14oEP6Ir8EEX0E4vyJGUFAAhycWqppr5x-2edL8Dz0OwB7ylqPIkRd9TefuQ9TaYxe-1aFMmmdH3aSJeFJ09YrqA16V2V-Dv7M0XJcbLyzZsvWE=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;We would like to thank Zoubin Ghahramani, Helen King, Rahul Sukthankar, Raia Hadsell, and Chandu Thota for their leadership and support.&lt;/p&gt;&lt;p&gt;This work was done thanks to the contributions of Mevan Babakar, Hannah Forbes-Pollard, Nikki Hariri, Thomas Leung, Nick Dufour, Ben Usman, Min Ma, Steve Pucci, Spudde Childs, Kate Harrison, Alanna Slocum, Reza Aghajani, Sri Rajendran, Alexey Vorobyov, Ashley Eden, Rishub Jain, Stephanie Chan, Sophie Bridgers, Michiel Bakker, Sures Kumar Thoddu Srinivasan, Tesh Goyal, and Ashish Chaudhary.&lt;/p&gt;&lt;p&gt;We would also like to thank Kent Walker, Camino Rojo, Clement Wolf, J.D. Velazquez, Tom Lue, Ndidi Elue, Rachel Stigler, M.H. Tessler, Ricardo Prada, William Isaac, Tom Stepleton, Zoe Darme, Gail Kent, Vincent Ryan, Aaron Donsbach, Abhishek Bapna, Verena Rieser, Christian Plagemann, Anca Dragan, Sven Gowal, Florian Stimberg, Christopher Savcak, Allison Garcia, Eve Novakovic, Armin Senoner, Arielle Bier, and the greater Google DeepMind and Google teams for their support, help, and feedback.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/exploring-the-context-of-online-images-with-backstory/</guid><pubDate>Mon, 21 Jul 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] Anduril alums raise $24M Series A to bring military logistics out of the Excel spreadsheet era (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/anduril-alums-raise-24m-series-a-to-bring-military-logistics-out-of-the-excel-spreadsheet-era/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Rune-Image-2.png?resize=1200,848" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Silicon Valley is doubling down on defense as geopolitical tensions rise and appetite for modernizing warfare grows. And while many of the startups garnering large valuations are focused on hardware and weaponry — think Anduril, Shield AI, and Skydio — Rune Technologies wants to tackle AI-enabled software for military logistics.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The U.S. military runs on Excel spreadsheets and whiteboards and manual processes right now to execute logistics operations,” co-founder David Tuttle told TechCrunch. “Logistics is never the sexiest part of the military. The technology industry emphasis is on how do we make things go boom? How do we build great weapons systems?”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Logistics, Tuttle says, usually falls behind when it comes to innovation. And he should know. Earlier in his career, he was a field artillery officer in the U.S. Army. Later, he served with the Joint Special Operations Command before going on to work at Anduril, where he met his co-founder, ex-Meta software engineer Peter Goldsborough. The two founded Rune after seeing how much modern warfare has changed the scale and pace at which armies have to sustain force.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ukraine is a sad example of the expenditures of munitions, the consumption of supplies, and those types of things in a near-peer adversary conflict — they will break human-centric and analog-centric processes,” said Tuttle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rune’s flagship product TyrOS promises to transform manual logistics processes into intelligent supply webs that predict future needs, optimize current resources, and enable distributed operations — even from a disconnected laptop in the middle of the jungle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has just raised a $24 million Series A round off the back of pilot deployments under the U.S. Army and U.S. Marine Corps. The round — which Human Capital led with participation from Pax VC, Washington Harbour Partners, a16z, Point72 Ventures, XYZ Venture Capital, and Forward Deployed VC — will go toward expanding TyrOS deployment into other U.S. military services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS has two major selling points. The first is its technical capabilities as a mission command system for logistics. TyrOS relies on deep learning models, including time series models, to forecast supply and demand assets like personnel, transportation, equipment, food, and other resources based on hundreds of environmental and supply variables.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“A logistician thinks about not just, ‘What do I have on hand from supplies?’ but also, ‘What vehicles do I have to move that?’” said Tuttle. “’What qualified crews do I have to drive that vehicle?’ ‘What routes is that vehicle going to go over?’ ‘And is that threat-informed?’ ‘Is a bridge blown up on the route that we need to reroute around?’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tuttle says the team at Rune, two-thirds of which are veterans, is also working to integrate generative AI into TyrOS for “course of action generation,” enabling the system to digest massive datasets in real-time battle space environments so that logisticians and commanders can query it on the fly. And while LLMs have advanced rapidly, TyrOS still relies on traditional mathematical optimization for certain tasks — like planning aircraft loads based on cubic volume and other constraints — where precise calculations are essential.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS’s second major hook is its edge-first architecture that bypasses the need for constant connectivity to remote servers, allowing the system to operate independently and synchronize when communications are reestablished. In other words, TyrOS is “cloud-capable, but not cloud-required.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Building software today from a cloud environment standpoint is very different architecturally than if I’m building software to run literally on this laptop in the jungle in the Philippines with Marines or soldiers,” Tuttle said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS is also cloud- and hardware-agnostic; it can run on program-of-record hardware server stacks that the military uses today for ease of integration, per Tuttle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The co-founder noted that Rune’s backers include executives at both Palantir and Anduril, where he sees plenty of partnership opportunities. Rune was recently selected for the Palantir Startup Fellowship and announced its integration earlier this year with Palantir’s Defense OSDK (Ontology Software Development Kit) to enable automated logistics, from the tactical edge to the strategic layer.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Automating the gap between tactical-level intelligence and strategic decision-making is Rune’s long-term vision.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not just worried about sustaining this for the next 30 or 60 days,” Tuttle said. “I’m worried about how this might impact production decisions back in the defense industrial base. That’s the vision we want to get up to. How do you drive tactical level data all the way up to the operational level, to the strategic level, to potentially drive the production of artillery shells?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Clarification: Peter Goldsborough volunteers under the U.S. Marine Corps Cyber Auxiliary.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Rune-Image-2.png?resize=1200,848" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Silicon Valley is doubling down on defense as geopolitical tensions rise and appetite for modernizing warfare grows. And while many of the startups garnering large valuations are focused on hardware and weaponry — think Anduril, Shield AI, and Skydio — Rune Technologies wants to tackle AI-enabled software for military logistics.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The U.S. military runs on Excel spreadsheets and whiteboards and manual processes right now to execute logistics operations,” co-founder David Tuttle told TechCrunch. “Logistics is never the sexiest part of the military. The technology industry emphasis is on how do we make things go boom? How do we build great weapons systems?”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Logistics, Tuttle says, usually falls behind when it comes to innovation. And he should know. Earlier in his career, he was a field artillery officer in the U.S. Army. Later, he served with the Joint Special Operations Command before going on to work at Anduril, where he met his co-founder, ex-Meta software engineer Peter Goldsborough. The two founded Rune after seeing how much modern warfare has changed the scale and pace at which armies have to sustain force.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ukraine is a sad example of the expenditures of munitions, the consumption of supplies, and those types of things in a near-peer adversary conflict — they will break human-centric and analog-centric processes,” said Tuttle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rune’s flagship product TyrOS promises to transform manual logistics processes into intelligent supply webs that predict future needs, optimize current resources, and enable distributed operations — even from a disconnected laptop in the middle of the jungle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has just raised a $24 million Series A round off the back of pilot deployments under the U.S. Army and U.S. Marine Corps. The round — which Human Capital led with participation from Pax VC, Washington Harbour Partners, a16z, Point72 Ventures, XYZ Venture Capital, and Forward Deployed VC — will go toward expanding TyrOS deployment into other U.S. military services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS has two major selling points. The first is its technical capabilities as a mission command system for logistics. TyrOS relies on deep learning models, including time series models, to forecast supply and demand assets like personnel, transportation, equipment, food, and other resources based on hundreds of environmental and supply variables.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“A logistician thinks about not just, ‘What do I have on hand from supplies?’ but also, ‘What vehicles do I have to move that?’” said Tuttle. “’What qualified crews do I have to drive that vehicle?’ ‘What routes is that vehicle going to go over?’ ‘And is that threat-informed?’ ‘Is a bridge blown up on the route that we need to reroute around?’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tuttle says the team at Rune, two-thirds of which are veterans, is also working to integrate generative AI into TyrOS for “course of action generation,” enabling the system to digest massive datasets in real-time battle space environments so that logisticians and commanders can query it on the fly. And while LLMs have advanced rapidly, TyrOS still relies on traditional mathematical optimization for certain tasks — like planning aircraft loads based on cubic volume and other constraints — where precise calculations are essential.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS’s second major hook is its edge-first architecture that bypasses the need for constant connectivity to remote servers, allowing the system to operate independently and synchronize when communications are reestablished. In other words, TyrOS is “cloud-capable, but not cloud-required.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Building software today from a cloud environment standpoint is very different architecturally than if I’m building software to run literally on this laptop in the jungle in the Philippines with Marines or soldiers,” Tuttle said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TyrOS is also cloud- and hardware-agnostic; it can run on program-of-record hardware server stacks that the military uses today for ease of integration, per Tuttle.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The co-founder noted that Rune’s backers include executives at both Palantir and Anduril, where he sees plenty of partnership opportunities. Rune was recently selected for the Palantir Startup Fellowship and announced its integration earlier this year with Palantir’s Defense OSDK (Ontology Software Development Kit) to enable automated logistics, from the tactical edge to the strategic layer.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Automating the gap between tactical-level intelligence and strategic decision-making is Rune’s long-term vision.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not just worried about sustaining this for the next 30 or 60 days,” Tuttle said. “I’m worried about how this might impact production decisions back in the defense industrial base. That’s the vision we want to get up to. How do you drive tactical level data all the way up to the operational level, to the strategic level, to potentially drive the production of artillery shells?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Clarification: Peter Goldsborough volunteers under the U.S. Marine Corps Cyber Auxiliary.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/anduril-alums-raise-24m-series-a-to-bring-military-logistics-out-of-the-excel-spreadsheet-era/</guid><pubDate>Mon, 21 Jul 2025 15:00:15 +0000</pubDate></item><item><title>[NEW] Grok’s AI companions drove downloads, but its latest model is the one making money (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/21/groks-ai-companions-drove-downloads-but-its-latest-model-is-the-one-making-money/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Grok’s raunchy, unfiltered AI companions may be making headlines for their unhinged and often NSFW responses, but it’s Grok 4, xAI’s latest model, that’s been driving the app’s revenue of late.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Elon Musk’s xAI launched Grok 4 late on July 9, and by Friday, July 11, Grok’s gross revenue on iOS had jumped a whopping 325% to $419,000, up from $99,000 the day before the Grok 4 launch, according to app intelligence firm Appfigures.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029483" height="396" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-downloads-grok-4-companions-15jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Grok continued to pull in higher-than-usual revenue in the days following the launch of the new model, with gross revenue over $367,000 for a couple of days before dipping down to $310,000 on July 14.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition, daily downloads of the Grok iOS app had increased 279% to 197,000 by July 11, up from 52,000 before Grok 4 launched.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But following the addition of Grok’s AI companions the next week, on July 14, the jump in downloads and revenue was less pronounced. While curiosity about the companions likely drove more installs, the feature isn’t yet poised to be a significant moneymaker for the company, despite being only available to “Super Grok” subscribers paying $30 per month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Grok’s iOS downloads globally were up 40% the day after the companions launched, reaching 171,000 daily installs, but revenue increased just 9%, hitting $337,000. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So while it’s clear that there was an impact from the launch, it’s decidedly smaller than the launch of the new AI model.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3029482" height="396" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-net-revenue-grok-4-companions-15jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Given Grok’s expensive new subscription offering, timed alongside the Grok 4 launch, it’s not surprising to see that even a smaller increase in the number of paying subscribers could drive Grok’s iOS revenue significantly higher.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced that, in addition to Grok 4 and Grok 4 Heavy, it would also offer a $300-per-month subscription called SuperGrok Heavy, its priciest plan to date. The plan offers subscribers early access to Grok 4 Heavy and other new features, xAI said, but it’s more expensive than comparable plans from other major AI providers, including OpenAI, Google, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s interesting to see this demand for Grok’s subscription plans, even though the AI was initially consulting Elon Musk’s X posts for answers. xAI has since addressed this issue.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029481" height="346" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-us-ios-category-rankings-16jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Interest in the new model also drove up Grok’s ranking on the U.S. App Store shortly after its launch, making Grok the No. 3 app overall and No. 2 in the Productivity category by July 12.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That interest has declined somewhat over the past week — the app is now No. 17 overall — though it’s still No. 2 in Productivity, Appfigures data shows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Appfigures notes that it focused on iOS data for this analysis, as that dataset is currently more comprehensive than data from Android devices. The latter is only available through July 14 for the time being, as Appfigures’ model needs more time to process Google Play data.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Grok’s raunchy, unfiltered AI companions may be making headlines for their unhinged and often NSFW responses, but it’s Grok 4, xAI’s latest model, that’s been driving the app’s revenue of late.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Elon Musk’s xAI launched Grok 4 late on July 9, and by Friday, July 11, Grok’s gross revenue on iOS had jumped a whopping 325% to $419,000, up from $99,000 the day before the Grok 4 launch, according to app intelligence firm Appfigures.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029483" height="396" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-downloads-grok-4-companions-15jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Grok continued to pull in higher-than-usual revenue in the days following the launch of the new model, with gross revenue over $367,000 for a couple of days before dipping down to $310,000 on July 14.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition, daily downloads of the Grok iOS app had increased 279% to 197,000 by July 11, up from 52,000 before Grok 4 launched.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But following the addition of Grok’s AI companions the next week, on July 14, the jump in downloads and revenue was less pronounced. While curiosity about the companions likely drove more installs, the feature isn’t yet poised to be a significant moneymaker for the company, despite being only available to “Super Grok” subscribers paying $30 per month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Grok’s iOS downloads globally were up 40% the day after the companions launched, reaching 171,000 daily installs, but revenue increased just 9%, hitting $337,000. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So while it’s clear that there was an impact from the launch, it’s decidedly smaller than the launch of the new AI model.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3029482" height="396" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-net-revenue-grok-4-companions-15jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Given Grok’s expensive new subscription offering, timed alongside the Grok 4 launch, it’s not surprising to see that even a smaller increase in the number of paying subscribers could drive Grok’s iOS revenue significantly higher.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced that, in addition to Grok 4 and Grok 4 Heavy, it would also offer a $300-per-month subscription called SuperGrok Heavy, its priciest plan to date. The plan offers subscribers early access to Grok 4 Heavy and other new features, xAI said, but it’s more expensive than comparable plans from other major AI providers, including OpenAI, Google, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s interesting to see this demand for Grok’s subscription plans, even though the AI was initially consulting Elon Musk’s X posts for answers. xAI has since addressed this issue.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3029481" height="346" src="https://techcrunch.com/wp-content/uploads/2025/07/af-grok-us-ios-category-rankings-16jul25.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Interest in the new model also drove up Grok’s ranking on the U.S. App Store shortly after its launch, making Grok the No. 3 app overall and No. 2 in the Productivity category by July 12.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That interest has declined somewhat over the past week — the app is now No. 17 overall — though it’s still No. 2 in Productivity, Appfigures data shows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Appfigures notes that it focused on iOS data for this analysis, as that dataset is currently more comprehensive than data from Android devices. The latter is only available through July 14 for the time being, as Appfigures’ model needs more time to process Google Play data.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/21/groks-ai-companions-drove-downloads-but-its-latest-model-is-the-one-making-money/</guid><pubDate>Mon, 21 Jul 2025 15:20:29 +0000</pubDate></item><item><title>[NEW] AI Testing and Evaluation: Reflections (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-reflections/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Amanda Craig Deckard and Kathleen Sullivan." class="wp-image-1145065" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/EP4-AI-TE_Hero_Feature_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains—from genome editing to cybersecurity—to investigate the role of testing and evaluation as a governance tool.&amp;nbsp;&lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;,&amp;nbsp;hosted by Microsoft Research’s&amp;nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.&lt;/p&gt;



&lt;p&gt;In the series finale, Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI, rejoins Sullivan to discuss what Microsoft has learned about testing as a governance tool and what’s next for the company’s work in the AI governance space. The pair explores high-level takeaways (i.e., testing is important and challenging!); the roles of rigor, standardization, and interpretability in making testing a reliable governance tool; and the potential for public-private partnerships to help advance not only model-level evaluation but deployment-level evaluation, too.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;



&lt;p&gt;Learning from other domains to advance AI evaluation and testing&amp;nbsp;&lt;br /&gt;Microsoft Research Blog | June 2025&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Responsible AI: Ethical policies and practices | Microsoft AI&amp;nbsp;&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KATHLEEN SULLIVAN:&lt;/strong&gt; Welcome to &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;. I’m your host, Kathleen Sullivan.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As generative AI continues to advance, Microsoft has gathered a range of experts—from genome editing to cybersecurity—to share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we’ll explore how these insights might help guide the future of AI development, deployment, and responsible use.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC ENDS]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For our final episode of the series, I’m thrilled to once again be joined by Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda, welcome back to the podcast!&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;AMANDA CRAIG DECKARD:&lt;/strong&gt; Thank you so much.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; In our intro episode, you really helped set the stage for this series. And it’s been great, because since then, we’ve had the pleasure of speaking with governance experts about genome editing, pharma, medical devices, cybersecurity, and we’ve also gotten to spend some time with our own Microsoft responsible AI leaders and hear reflections from them.&lt;/p&gt;



&lt;p&gt;And here’s what stuck with me, and I’d love to hear from you on this, as well: testing builds trust; context is shaping risk; and every field is really thinking about striking its own balance between pre-deployment testing and post-deployment monitoring.&lt;/p&gt;



&lt;p&gt;So drawing on what you’ve learned from the workshop and the case studies, what headline insights do you think matter the most for AI governance?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; It’s been really interesting to learn from all of these different domains, and there are, you know, lots of really interesting takeaways.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I think a starting point for me is actually pretty similar to where you landed, which is just that testing is really important for trust, and it’s also really &lt;em&gt;hard&lt;/em&gt; [LAUGHS] to figure out exactly, you know, how to get it right, how to make sure that you’re addressing risks, that you’re not constraining innovation, that you are recognizing that a lot of the industry that’s impacted is really different. You have small organizations, you have large organizations, and you want to enable that opportunity that is enabled by the technology across the board.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so it’s just difficult to, kind of, get all of these dynamics right, especially when, you know, I think we heard from other domains, testing is not some, sort of, like, oh, simple thing, right. There’s not this linear path from, like, A to B where you just test the one thing and you’re done.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN: &lt;/strong&gt;Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; It’s complex, right. Testing is multistage. There’s a lot of testing by different actors. There are a lot of different purposes for which you might test. As I think it was Dan Carpenter who talked about it’s not just about testing for safety. It’s also about testing for efficacy and building confidence in the right dosage for pharmaceuticals, for example. And that’s across the board for all of these domains, right. That you’re really thinking about the performance of the technology. You’re thinking about safety. You’re trying to also calibrate for efficiency.&lt;/p&gt;



&lt;p&gt;And so those tradeoffs, every expert shared that navigating those is really challenging. And also that there were real impacts to early choices in the, sort of, governance of risk in these different domains and the development of the testing, sort of, expectations, and that in some cases, this had been difficult to reverse, which also just layers on that complexity and that difficulty in a different way. So that’s the super high-level takeaway. But maybe if I could just quickly distill, like, three takeaways that I think really are applicable to AI in a bit more of a granular way.&lt;/p&gt;



&lt;p&gt;You know, one is about, how is the testing exactly used? For what purpose? And the second is what emphasis there is on this pre- versus post-deployment testing and monitoring. And then the third is how rigid versus adaptive the, sort of, testing regimes or frameworks are in these different domains.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So on the first—how is testing used?—so is testing something that impacts market entry, for example? Or is it something that might be used more for informing how risk is evolving in the domain and how broader risk management strategies might need to be applied? We have examples, like the pharmaceutical or medical device industry experts with whom you spoke, that’s really, you know, testing … there is a pre-deployment requirement. So that’s one question.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The second is this emphasis on pre- versus post-deployment testing and monitoring, and we really did see across domains that in many cases, there is a desire for both pre- and post-deployment, sort of, testing and monitoring, but also that, sort of, naturally in these different domains, a degree of emphasis on one or the other had evolved and that had a real impact on governance and tradeoffs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And the third is just how rigid versus adaptive these testing and evaluation regimes or frameworks are in these different domains. We saw, you know, in some domains, the testing requirements were more rigid as you might expect in more of the pharmaceutical or medical devices industries, for example. And in other domains, there was this more, sort of, adaptive approach to how testing might get used. So, for example, in the case of our other general-purpose technologies, you know, you spoke with Alta Charo on genome editing, and in our case studies, we also explored this in the context of nanotechnology. In those general-purpose technology domains, there is more emphasis on downstream or application-context testing that is more, sort of, adaptive to the use scenario of the technology and, you know, having that work in conjunction with testing more at the, kind of, level of the technology itself.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; I want to double-click on a number of the things we just talked about. But actually, before we go too much deeper, a question on if there’s anything that really surprised you or challenged maybe some of your own assumptions in this space from some of the discussions that we had over the series.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah. You know, I know I’ve already just mentioned this pre- versus post-deployment testing and monitoring issue, but it was something that was very interesting to me and in some ways surprised me or made me just realize something that I hadn’t fully connected before, about how these, sort of, regimes might evolve in different contexts and why. And in part, I couldn’t help but bring the context I have from cybersecurity policy into this, kind of, processing of what we learned and reflection because there was a real contrast for me between the pharmaceutical industry and the cybersecurity domain when I think about the emphasis on pre- versus post-deployment monitoring.&lt;/p&gt;



&lt;p&gt;And on the one hand, we have in the pharmaceutical domain a real emphasis that has developed around pre-market testing. And there is also an expectation in some circumstances in the pharmaceutical domain for post-deployment testing, as well. But as we learned from our experts in that domain, there has naturally been a real, kind of, emphasis on the pre-market portion of that testing. And in reality, even where post-market monitoring is required and post-market testing is required, it does not always actually happen.&lt;strong&gt; &lt;/strong&gt;And the experts really explained that, you know, part of it is just the incentive structure around the emphasis around, you know, the testing as a pre-market, sort of, entry requirement. And also just the resources that exist among regulators, right. There’s limited resources, right. And so there are just choices and tradeoffs that they need to make in their own, sort of, enforcement work.&lt;/p&gt;



&lt;p&gt;And then on the other hand, you know, in cybersecurity, I never thought about the, kind of, emphasis on things like coordinated vulnerability disclosure and bug bounties that have really developed in the cybersecurity domain. But it’s a really important part of how we secure technology and enhance cybersecurity over time, where we have these norms that have developed where, you know, security researchers are doing really important research. They’re finding vulnerabilities in products. And we have norms developed where they report those to the companies that are in a position to address those vulnerabilities. And in some cases, those companies actually pay, through bug bounties, the researchers. And perhaps in some ways, the role of coordinated vulnerability disclosure and bug bounties has evolved the way that it has because there hasn’t been as much emphasis on the pre-market testing across the board at least in the context of software.&lt;/p&gt;



&lt;p&gt;And so you look at those two industries and it was interesting to me to study them to some extent in contrast with each other as this way that the incentives and the resources that need to be applied to testing, sort of, evolve to address where there’s, kind of, more or less emphasis.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; It’s a great point. I mean, I think what we’re hearing—and what you’re saying—is just exactly this choice … like, is there a binary choice between focusing on pre-deployment testing or post-deployment monitoring? And, you know, I think our assumption is that we need to do both. But I’d love to hear from you on that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Absolutely. I think we need to do both. I’m very persuaded by this inclination always that there’s value in trying to really do it all in a risk management context.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And also, we know one of the principles of risk management is you have to prioritize because there are finite resources. And I think that’s where we get to this challenge in really thinking deeply, especially as we’re in the early days of AI governance, and we need to be very thoughtful about, you know, tradeoffs that we may not want to be making but we are because, again, these are finite choices and we, kind of, can’t help but put our finger on the dial in different directions with our choices that, you know, it’s going to be very difficult to have, sort of, equal emphasis on both. And we need to invest in both, but we need to be very &lt;em&gt;deliberate&lt;/em&gt; about the roles of each and how they complement each other and who does which and how we use what we learn from pre- versus post-deployment testing and monitoring.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Maybe just spending a little bit more time here … you know, a lot of attention goes into testing models upstream, but risk often shows up once they’re wired into real products and workflows. How much does deployment context change the risk picture from your perspective?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah, I … such an important question. I really agree that there has been a lot of emphasis to date on, sort of, testing models upstream, the AI model evaluation. And it’s also really important that we bring more attention into evaluation at the system or application level. And I actually see that in governance conversations, this &lt;em&gt;is&lt;/em&gt; actually increasingly raised, this need to have system-level evaluation. We see this across regulation. We also see it in the context of just organizations trying to put in governance requirements for how their organization is going to operate in deploying this technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there’s a gap today in terms of best practices around system-level testing, perhaps even more than model-level evaluation. And it’s really important because in a lot of cases, the deployment context really does impact the risk picture, especially with AI, which is a general-purpose technology, and we really saw this in our study of other domains that represented general-purpose technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So in the case study that you can find online on nanotechnology, you know, there’s a real distinction between the risk evaluation and the governance of nanotechnology in different deployment contexts. So the chapter that our expert on nanotechnology wrote really goes into incredibly interesting detail around, you know, deployment of nanotechnology in the context of, like, chemical applications versus consumer electronics versus pharmaceuticals versus construction and how the way that nanoparticles are basically delivered in all those different deployment contexts, as well as, like, what the risk of the actual use scenario is just varies so much. And so there’s a real need to do that kind of risk evaluation and testing in the deployment context, and this difference in terms of risks and what we learned in these other domains where, you know, there are these different approaches to trying to really think about and gain efficiencies and address risks at a horizontal level versus, you know, taking a real sector-by-sector approach. And to some extent, it seems like it’s more time intensive to do that sectoral deployment-specific work. And at the same time, perhaps there are efficiencies to be gained by actually doing the work in the context in which, you know, you have a better understanding of the risk that can result from really deploying this technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And ultimately, [LAUGHS] really what we also need to think about here is probably, in the end, just like pre- and post-deployment testing, you need both. Not probably; certainly!&lt;/p&gt;



&lt;p&gt;So effectively we need to think about evaluation at the model level and the system level as being really important. And it’s really important to get system evaluation right so that we can actually get trust in this technology in deployment context so we enable adoption in low- and in high-risk deployments in a way that means that we’ve done risk evaluation in each of those contexts in a way that really makes sense in terms of the resources that we need to apply and ultimately we are able to unlock more applications of this technology in a risk-informed way.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; That’s great. I mean, I couldn’t agree more. I think these contexts, the approaches are so important for trust and adoption, and I’d love to hear from you, what do we need to advance AI evaluation and testing in our ecosystem? What are some of the big gaps that you’re seeing, and what role can different stakeholders play in filling them? And maybe an add-on, actually: is there some sort of network effect that could 10x our testing capacity?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Absolutely. So there’s a lot of work that needs to be done, and there’s a lot of work in process to really level up our whole evaluation and testing ecosystem. We learned, across domains, that there’s really a need to advance our thinking and our practice in three areas: rigor of testing; standardization of methodologies and processes; and interpretability of test results.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So what we mean by rigor is that we are ensuring that what we are ultimately evaluating in terms of risks is defined in a scientifically valid way and we are able to measure against that risk in a scientifically valid way.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By standardization, what we mean is that there’s really an accepted and well-understood and, again, a scientifically valid methodology for doing that testing and for actually producing artifacts out of that testing that are meeting those standards. And that sets us up for the final portion on interpretability, which is, like, really the process by which you can trust that the testing has been done in this rigorous and standardized way and that then you have artifacts that result from the testing process that can really be used in the risk management context because they can be interpreted, right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We understand how to, like, apply weight to them for our risk-management decisions. We actually are able to interpret them in a way that perhaps they inform other downstream risk mitigations that address the risks that we see through the testing results and that we actually understand what limitations apply to the test results and why they may or may not be valid in certain, sort of, deployment contexts, for example, and especially in the context of other risk mitigations that we need to apply. So there’s a need to advance all three of those things—rigor, standardization, and interpretability—to level up the whole testing and evaluation ecosystem.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And when we think about what actors should be involved in that work … really &lt;em&gt;everybody&lt;/em&gt;, which is both complex to orchestrate but also really important. And so, you know, you need to have the entire value chain involved in really advancing this work. You need the model developers, but you also need the system developers and deployers that are really engaged in advancing the science of evaluation and advancing how we are using these testing artifacts in the risk management process.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;When we think about what could actually 10x our testing capacity—that’s the dream, right? We all want to accelerate our progress in this space. You know,&amp;nbsp;I think we need work across all three of those areas of rigor, standardization, and interpretability, but I think one that will really help accelerate our progress across the board is that standardization work, because ultimately, you’re going to need to have these tests be done and applied across so many different contexts, and ultimately, while we want the whole value chain engaged in the development of the thinking and the science and the standards in this space, we also need to realize that not every organization is necessarily going to have the capacity to, kind of, contribute to developing the ways that we create and use these tests. And there are going to be many organizations that are going to benefit from there being standardization of the methodologies and the artifacts that they can pick up and use.&lt;/p&gt;



&lt;p&gt;One thing that I know we’ve heard throughout this podcast series from our experts in other domains, including Timo [Minssen] in the medical devices context and Ciaran [Martin] in the cybersecurity context, is that there’s been a recognition, as those domains have evolved, that there’s a need to calibrate our, sort of, expectations for different actors in the ecosystem and really understand that small businesses, for example, just cannot apply the same degree of resources that others may be able to, to do testing and evaluation and risk management. And so the benefit of having standardized approaches is that those organizations are able to, kind of, integrate into the broader supply chain ecosystem and apply their own, kind of, risk management practices in their own context in a way that is more efficient.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And finally, the last stakeholder that I think is really important to think about in terms of partnership across the ecosystem to really advance the whole testing and evaluation work that needs to happen is government partners, right, and thinking beyond the value chain, the AI supply chain, and really thinking about public-private partnership. That’s going to be incredibly important to advancing this ecosystem.&lt;/p&gt;



&lt;p&gt;You know, I think there’s been real progress already in the AI evaluation and testing ecosystem in the public-private partnership context. We have been really supportive of the work of the International Network of AI Safety and Security Institutes&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;[1]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and the Center for AI Standards and Innovation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that all allow for that kind of public-private partnership on actually testing and advancing the science and best practices around standards.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there are other innovative, kind of, partnerships, as well, in the ecosystem. You know, Singapore has recently launched their Global AI Assurance Pilot&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; findings. And that effort really paired application deployers and testers so that consequential impacts at deployment could really be tested. And that’s a really fruitful, sort of, effort that complements the work of these institutes and centers that are more focused on evaluation at the model level, for example.&lt;/p&gt;



&lt;p&gt;And in general, you know, I think that there’s just really a lot of benefits for us thinking expansively about what we can accomplish through deep, meaningful public-private partnership in this space. I’m really excited to see where we can go from here with building on, you know, partnerships across AI supply chains and with governments and public-private partnerships.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; I couldn’t agree more. I mean, this notion of more engagement across the ecosystem and value chain is super important for us and informs how we think about the space completely.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;If you could invite any other industry to the next workshop, maybe quantum safety, space tech, even gaming, who’s on your wish list? And maybe what are some of the things you’d want to go deeper on?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; This is something that we really welcome feedback on if anyone listening has ideas about other domains that would be interesting to study. I will say, I think I shared at the outset of this podcast series, the domains that we added in this round of our efforts in studying other domains actually all came from feedback that we received from, you know, folks we’d engaged with our first study of other domains and multilateral, sort of, governance institutions. And so we’re really keen to think about what other domains could be interesting to study. And we are also keen to go deeper, building on what we learned in this round of effort going forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One of the areas that I am particularly really interested in is going deeper on, what, sort of, transparency and information sharing about risk evaluation and testing will be really useful to share in different contexts? So across the AI supply chain, what is the information that’s going to be really meaningful to share between developers and deployers of models and systems and those that are ultimately using this technology in particular deployment contexts? And, you know, I think that we could have much to learn from other general-purpose technologies like genome editing and nanotechnology and cybersecurity, where we could learn a bit more about the kinds of information that they have shared across the development and deployment life cycle and how that has strengthened risk management in general as well as provided a really strong feedback loop around testing and evaluation. What kind of testing is most useful to do at what point in the life cycle, and what artifacts are most useful to share as a result of that testing and evaluation work?&lt;/p&gt;



&lt;p&gt;I’ll say, as Microsoft, we have been really investing in how we are sharing information with our various stakeholders. We also have been engaged with others in industry in reporting what we’ve done in the context of the Hiroshima AI Process, or &lt;em&gt;HAIP&lt;/em&gt;, Reporting Framework&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. This is an effort that is really just in its first round of really exploring how this kind of reporting can be really additive to risk management understanding. And again, I think there’s real opportunity here to look at this kind of reporting and understand, you know, what’s valuable for stakeholders and where is there opportunity to go further in really informing value chains and policymakers and the public about AI risk and opportunity and what can we learn again from other domains that have done this kind of work over decades to really refine that kind of information sharing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; It’s really great to hear about all the advances that we’re making on these reports. I’m guessing a lot of the metrics in there are technical, but sociotechnical impacts—jobs, maybe misinformation, well-being—are harder to score. What new measurement ideas are you excited about, and do you have any thoughts on, like, who needs to pilot those?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah, it’s an incredibly interesting question that I think also just speaks to, you know, the breadth of, sort of, testing and evaluation that’s needed at different points along that AI life cycle and really not getting lost in one particular kind of testing or another pre- or post-deployment and thinking expansively about the risks that we’re trying to address through this testing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, for example, even with the UK’s AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that has just recently launched a new program, a new team, that’s focused on societal resilience research. I think it’s going to be a really important area from a sociotechnical impact perspective to bring some focus into as this technology is more widely deployed. Are we understanding the impacts over time as different people and different cultures adopt and use this technology for different purposes?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I think that’s an area where there really is opportunity for greater public-private partnership in this research. Because we all share this long-term interest in ensuring that this technology is really serving people and we have to understand the impacts so that we understand, you know, what adjustments we can actually pursue sooner upstream to address those impacts and make sure that this technology is really going to work for all of us and in a way that is consistent with the societal values that we want.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; So, Amanda, looking ahead, I would love to hear just what’s going to be on your radar? What’s top of mind for you in the coming weeks?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Well, we are certainly continuing to process all the learnings that we’ve had from studying these domains. It’s really been a rich set of insights that we want to make sure we, kind of, fully take advantage of. And, you know, I think these hard questions and, you know, real opportunities to be thoughtful in these early days of AI governance are not, sort of, going away or being easily resolved soon. And so I think we continue to see value in really learning from others, thinking about what’s distinct in the AI context, but also what we can apply in terms of what other domains have learned.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Well, Amanda, it has been such a special experience for me to help illuminate the work of the Office of Responsible AI and our team in Microsoft Research, and [MUSIC] it’s just really special to see all of the work that we’re doing to help set the standard for responsible development and deployment of AI. So thank you for joining us today, and thanks for your reflections and discussion.&lt;/p&gt;



&lt;p&gt;And to our listeners, thank you so much for joining us for the series. We really hope you enjoyed it!&amp;nbsp;To check out all of our episodes, visit aka.ms/AITestingandEvaluation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;See you next time!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES] &lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;







&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; Since the launch of the International Network of AI Safety Institutes, the UK renamed its institute the AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Amanda Craig Deckard and Kathleen Sullivan." class="wp-image-1145065" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/EP4-AI-TE_Hero_Feature_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains—from genome editing to cybersecurity—to investigate the role of testing and evaluation as a governance tool.&amp;nbsp;&lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;,&amp;nbsp;hosted by Microsoft Research’s&amp;nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.&lt;/p&gt;



&lt;p&gt;In the series finale, Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI, rejoins Sullivan to discuss what Microsoft has learned about testing as a governance tool and what’s next for the company’s work in the AI governance space. The pair explores high-level takeaways (i.e., testing is important and challenging!); the roles of rigor, standardization, and interpretability in making testing a reliable governance tool; and the potential for public-private partnerships to help advance not only model-level evaluation but deployment-level evaluation, too.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;



&lt;p&gt;Learning from other domains to advance AI evaluation and testing&amp;nbsp;&lt;br /&gt;Microsoft Research Blog | June 2025&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Responsible AI: Ethical policies and practices | Microsoft AI&amp;nbsp;&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KATHLEEN SULLIVAN:&lt;/strong&gt; Welcome to &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;. I’m your host, Kathleen Sullivan.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As generative AI continues to advance, Microsoft has gathered a range of experts—from genome editing to cybersecurity—to share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we’ll explore how these insights might help guide the future of AI development, deployment, and responsible use.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC ENDS]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For our final episode of the series, I’m thrilled to once again be joined by Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda, welcome back to the podcast!&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;AMANDA CRAIG DECKARD:&lt;/strong&gt; Thank you so much.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; In our intro episode, you really helped set the stage for this series. And it’s been great, because since then, we’ve had the pleasure of speaking with governance experts about genome editing, pharma, medical devices, cybersecurity, and we’ve also gotten to spend some time with our own Microsoft responsible AI leaders and hear reflections from them.&lt;/p&gt;



&lt;p&gt;And here’s what stuck with me, and I’d love to hear from you on this, as well: testing builds trust; context is shaping risk; and every field is really thinking about striking its own balance between pre-deployment testing and post-deployment monitoring.&lt;/p&gt;



&lt;p&gt;So drawing on what you’ve learned from the workshop and the case studies, what headline insights do you think matter the most for AI governance?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; It’s been really interesting to learn from all of these different domains, and there are, you know, lots of really interesting takeaways.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I think a starting point for me is actually pretty similar to where you landed, which is just that testing is really important for trust, and it’s also really &lt;em&gt;hard&lt;/em&gt; [LAUGHS] to figure out exactly, you know, how to get it right, how to make sure that you’re addressing risks, that you’re not constraining innovation, that you are recognizing that a lot of the industry that’s impacted is really different. You have small organizations, you have large organizations, and you want to enable that opportunity that is enabled by the technology across the board.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so it’s just difficult to, kind of, get all of these dynamics right, especially when, you know, I think we heard from other domains, testing is not some, sort of, like, oh, simple thing, right. There’s not this linear path from, like, A to B where you just test the one thing and you’re done.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN: &lt;/strong&gt;Right.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; It’s complex, right. Testing is multistage. There’s a lot of testing by different actors. There are a lot of different purposes for which you might test. As I think it was Dan Carpenter who talked about it’s not just about testing for safety. It’s also about testing for efficacy and building confidence in the right dosage for pharmaceuticals, for example. And that’s across the board for all of these domains, right. That you’re really thinking about the performance of the technology. You’re thinking about safety. You’re trying to also calibrate for efficiency.&lt;/p&gt;



&lt;p&gt;And so those tradeoffs, every expert shared that navigating those is really challenging. And also that there were real impacts to early choices in the, sort of, governance of risk in these different domains and the development of the testing, sort of, expectations, and that in some cases, this had been difficult to reverse, which also just layers on that complexity and that difficulty in a different way. So that’s the super high-level takeaway. But maybe if I could just quickly distill, like, three takeaways that I think really are applicable to AI in a bit more of a granular way.&lt;/p&gt;



&lt;p&gt;You know, one is about, how is the testing exactly used? For what purpose? And the second is what emphasis there is on this pre- versus post-deployment testing and monitoring. And then the third is how rigid versus adaptive the, sort of, testing regimes or frameworks are in these different domains.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So on the first—how is testing used?—so is testing something that impacts market entry, for example? Or is it something that might be used more for informing how risk is evolving in the domain and how broader risk management strategies might need to be applied? We have examples, like the pharmaceutical or medical device industry experts with whom you spoke, that’s really, you know, testing … there is a pre-deployment requirement. So that’s one question.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The second is this emphasis on pre- versus post-deployment testing and monitoring, and we really did see across domains that in many cases, there is a desire for both pre- and post-deployment, sort of, testing and monitoring, but also that, sort of, naturally in these different domains, a degree of emphasis on one or the other had evolved and that had a real impact on governance and tradeoffs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And the third is just how rigid versus adaptive these testing and evaluation regimes or frameworks are in these different domains. We saw, you know, in some domains, the testing requirements were more rigid as you might expect in more of the pharmaceutical or medical devices industries, for example. And in other domains, there was this more, sort of, adaptive approach to how testing might get used. So, for example, in the case of our other general-purpose technologies, you know, you spoke with Alta Charo on genome editing, and in our case studies, we also explored this in the context of nanotechnology. In those general-purpose technology domains, there is more emphasis on downstream or application-context testing that is more, sort of, adaptive to the use scenario of the technology and, you know, having that work in conjunction with testing more at the, kind of, level of the technology itself.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; I want to double-click on a number of the things we just talked about. But actually, before we go too much deeper, a question on if there’s anything that really surprised you or challenged maybe some of your own assumptions in this space from some of the discussions that we had over the series.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah. You know, I know I’ve already just mentioned this pre- versus post-deployment testing and monitoring issue, but it was something that was very interesting to me and in some ways surprised me or made me just realize something that I hadn’t fully connected before, about how these, sort of, regimes might evolve in different contexts and why. And in part, I couldn’t help but bring the context I have from cybersecurity policy into this, kind of, processing of what we learned and reflection because there was a real contrast for me between the pharmaceutical industry and the cybersecurity domain when I think about the emphasis on pre- versus post-deployment monitoring.&lt;/p&gt;



&lt;p&gt;And on the one hand, we have in the pharmaceutical domain a real emphasis that has developed around pre-market testing. And there is also an expectation in some circumstances in the pharmaceutical domain for post-deployment testing, as well. But as we learned from our experts in that domain, there has naturally been a real, kind of, emphasis on the pre-market portion of that testing. And in reality, even where post-market monitoring is required and post-market testing is required, it does not always actually happen.&lt;strong&gt; &lt;/strong&gt;And the experts really explained that, you know, part of it is just the incentive structure around the emphasis around, you know, the testing as a pre-market, sort of, entry requirement. And also just the resources that exist among regulators, right. There’s limited resources, right. And so there are just choices and tradeoffs that they need to make in their own, sort of, enforcement work.&lt;/p&gt;



&lt;p&gt;And then on the other hand, you know, in cybersecurity, I never thought about the, kind of, emphasis on things like coordinated vulnerability disclosure and bug bounties that have really developed in the cybersecurity domain. But it’s a really important part of how we secure technology and enhance cybersecurity over time, where we have these norms that have developed where, you know, security researchers are doing really important research. They’re finding vulnerabilities in products. And we have norms developed where they report those to the companies that are in a position to address those vulnerabilities. And in some cases, those companies actually pay, through bug bounties, the researchers. And perhaps in some ways, the role of coordinated vulnerability disclosure and bug bounties has evolved the way that it has because there hasn’t been as much emphasis on the pre-market testing across the board at least in the context of software.&lt;/p&gt;



&lt;p&gt;And so you look at those two industries and it was interesting to me to study them to some extent in contrast with each other as this way that the incentives and the resources that need to be applied to testing, sort of, evolve to address where there’s, kind of, more or less emphasis.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; It’s a great point. I mean, I think what we’re hearing—and what you’re saying—is just exactly this choice … like, is there a binary choice between focusing on pre-deployment testing or post-deployment monitoring? And, you know, I think our assumption is that we need to do both. But I’d love to hear from you on that.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Absolutely. I think we need to do both. I’m very persuaded by this inclination always that there’s value in trying to really do it all in a risk management context.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And also, we know one of the principles of risk management is you have to prioritize because there are finite resources. And I think that’s where we get to this challenge in really thinking deeply, especially as we’re in the early days of AI governance, and we need to be very thoughtful about, you know, tradeoffs that we may not want to be making but we are because, again, these are finite choices and we, kind of, can’t help but put our finger on the dial in different directions with our choices that, you know, it’s going to be very difficult to have, sort of, equal emphasis on both. And we need to invest in both, but we need to be very &lt;em&gt;deliberate&lt;/em&gt; about the roles of each and how they complement each other and who does which and how we use what we learn from pre- versus post-deployment testing and monitoring.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Maybe just spending a little bit more time here … you know, a lot of attention goes into testing models upstream, but risk often shows up once they’re wired into real products and workflows. How much does deployment context change the risk picture from your perspective?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah, I … such an important question. I really agree that there has been a lot of emphasis to date on, sort of, testing models upstream, the AI model evaluation. And it’s also really important that we bring more attention into evaluation at the system or application level. And I actually see that in governance conversations, this &lt;em&gt;is&lt;/em&gt; actually increasingly raised, this need to have system-level evaluation. We see this across regulation. We also see it in the context of just organizations trying to put in governance requirements for how their organization is going to operate in deploying this technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there’s a gap today in terms of best practices around system-level testing, perhaps even more than model-level evaluation. And it’s really important because in a lot of cases, the deployment context really does impact the risk picture, especially with AI, which is a general-purpose technology, and we really saw this in our study of other domains that represented general-purpose technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So in the case study that you can find online on nanotechnology, you know, there’s a real distinction between the risk evaluation and the governance of nanotechnology in different deployment contexts. So the chapter that our expert on nanotechnology wrote really goes into incredibly interesting detail around, you know, deployment of nanotechnology in the context of, like, chemical applications versus consumer electronics versus pharmaceuticals versus construction and how the way that nanoparticles are basically delivered in all those different deployment contexts, as well as, like, what the risk of the actual use scenario is just varies so much. And so there’s a real need to do that kind of risk evaluation and testing in the deployment context, and this difference in terms of risks and what we learned in these other domains where, you know, there are these different approaches to trying to really think about and gain efficiencies and address risks at a horizontal level versus, you know, taking a real sector-by-sector approach. And to some extent, it seems like it’s more time intensive to do that sectoral deployment-specific work. And at the same time, perhaps there are efficiencies to be gained by actually doing the work in the context in which, you know, you have a better understanding of the risk that can result from really deploying this technology.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And ultimately, [LAUGHS] really what we also need to think about here is probably, in the end, just like pre- and post-deployment testing, you need both. Not probably; certainly!&lt;/p&gt;



&lt;p&gt;So effectively we need to think about evaluation at the model level and the system level as being really important. And it’s really important to get system evaluation right so that we can actually get trust in this technology in deployment context so we enable adoption in low- and in high-risk deployments in a way that means that we’ve done risk evaluation in each of those contexts in a way that really makes sense in terms of the resources that we need to apply and ultimately we are able to unlock more applications of this technology in a risk-informed way.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; That’s great. I mean, I couldn’t agree more. I think these contexts, the approaches are so important for trust and adoption, and I’d love to hear from you, what do we need to advance AI evaluation and testing in our ecosystem? What are some of the big gaps that you’re seeing, and what role can different stakeholders play in filling them? And maybe an add-on, actually: is there some sort of network effect that could 10x our testing capacity?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Absolutely. So there’s a lot of work that needs to be done, and there’s a lot of work in process to really level up our whole evaluation and testing ecosystem. We learned, across domains, that there’s really a need to advance our thinking and our practice in three areas: rigor of testing; standardization of methodologies and processes; and interpretability of test results.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So what we mean by rigor is that we are ensuring that what we are ultimately evaluating in terms of risks is defined in a scientifically valid way and we are able to measure against that risk in a scientifically valid way.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By standardization, what we mean is that there’s really an accepted and well-understood and, again, a scientifically valid methodology for doing that testing and for actually producing artifacts out of that testing that are meeting those standards. And that sets us up for the final portion on interpretability, which is, like, really the process by which you can trust that the testing has been done in this rigorous and standardized way and that then you have artifacts that result from the testing process that can really be used in the risk management context because they can be interpreted, right.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We understand how to, like, apply weight to them for our risk-management decisions. We actually are able to interpret them in a way that perhaps they inform other downstream risk mitigations that address the risks that we see through the testing results and that we actually understand what limitations apply to the test results and why they may or may not be valid in certain, sort of, deployment contexts, for example, and especially in the context of other risk mitigations that we need to apply. So there’s a need to advance all three of those things—rigor, standardization, and interpretability—to level up the whole testing and evaluation ecosystem.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And when we think about what actors should be involved in that work … really &lt;em&gt;everybody&lt;/em&gt;, which is both complex to orchestrate but also really important. And so, you know, you need to have the entire value chain involved in really advancing this work. You need the model developers, but you also need the system developers and deployers that are really engaged in advancing the science of evaluation and advancing how we are using these testing artifacts in the risk management process.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;When we think about what could actually 10x our testing capacity—that’s the dream, right? We all want to accelerate our progress in this space. You know,&amp;nbsp;I think we need work across all three of those areas of rigor, standardization, and interpretability, but I think one that will really help accelerate our progress across the board is that standardization work, because ultimately, you’re going to need to have these tests be done and applied across so many different contexts, and ultimately, while we want the whole value chain engaged in the development of the thinking and the science and the standards in this space, we also need to realize that not every organization is necessarily going to have the capacity to, kind of, contribute to developing the ways that we create and use these tests. And there are going to be many organizations that are going to benefit from there being standardization of the methodologies and the artifacts that they can pick up and use.&lt;/p&gt;



&lt;p&gt;One thing that I know we’ve heard throughout this podcast series from our experts in other domains, including Timo [Minssen] in the medical devices context and Ciaran [Martin] in the cybersecurity context, is that there’s been a recognition, as those domains have evolved, that there’s a need to calibrate our, sort of, expectations for different actors in the ecosystem and really understand that small businesses, for example, just cannot apply the same degree of resources that others may be able to, to do testing and evaluation and risk management. And so the benefit of having standardized approaches is that those organizations are able to, kind of, integrate into the broader supply chain ecosystem and apply their own, kind of, risk management practices in their own context in a way that is more efficient.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And finally, the last stakeholder that I think is really important to think about in terms of partnership across the ecosystem to really advance the whole testing and evaluation work that needs to happen is government partners, right, and thinking beyond the value chain, the AI supply chain, and really thinking about public-private partnership. That’s going to be incredibly important to advancing this ecosystem.&lt;/p&gt;



&lt;p&gt;You know, I think there’s been real progress already in the AI evaluation and testing ecosystem in the public-private partnership context. We have been really supportive of the work of the International Network of AI Safety and Security Institutes&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;[1]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and the Center for AI Standards and Innovation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that all allow for that kind of public-private partnership on actually testing and advancing the science and best practices around standards.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And there are other innovative, kind of, partnerships, as well, in the ecosystem. You know, Singapore has recently launched their Global AI Assurance Pilot&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; findings. And that effort really paired application deployers and testers so that consequential impacts at deployment could really be tested. And that’s a really fruitful, sort of, effort that complements the work of these institutes and centers that are more focused on evaluation at the model level, for example.&lt;/p&gt;



&lt;p&gt;And in general, you know, I think that there’s just really a lot of benefits for us thinking expansively about what we can accomplish through deep, meaningful public-private partnership in this space. I’m really excited to see where we can go from here with building on, you know, partnerships across AI supply chains and with governments and public-private partnerships.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; I couldn’t agree more. I mean, this notion of more engagement across the ecosystem and value chain is super important for us and informs how we think about the space completely.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;If you could invite any other industry to the next workshop, maybe quantum safety, space tech, even gaming, who’s on your wish list? And maybe what are some of the things you’d want to go deeper on?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; This is something that we really welcome feedback on if anyone listening has ideas about other domains that would be interesting to study. I will say, I think I shared at the outset of this podcast series, the domains that we added in this round of our efforts in studying other domains actually all came from feedback that we received from, you know, folks we’d engaged with our first study of other domains and multilateral, sort of, governance institutions. And so we’re really keen to think about what other domains could be interesting to study. And we are also keen to go deeper, building on what we learned in this round of effort going forward.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One of the areas that I am particularly really interested in is going deeper on, what, sort of, transparency and information sharing about risk evaluation and testing will be really useful to share in different contexts? So across the AI supply chain, what is the information that’s going to be really meaningful to share between developers and deployers of models and systems and those that are ultimately using this technology in particular deployment contexts? And, you know, I think that we could have much to learn from other general-purpose technologies like genome editing and nanotechnology and cybersecurity, where we could learn a bit more about the kinds of information that they have shared across the development and deployment life cycle and how that has strengthened risk management in general as well as provided a really strong feedback loop around testing and evaluation. What kind of testing is most useful to do at what point in the life cycle, and what artifacts are most useful to share as a result of that testing and evaluation work?&lt;/p&gt;



&lt;p&gt;I’ll say, as Microsoft, we have been really investing in how we are sharing information with our various stakeholders. We also have been engaged with others in industry in reporting what we’ve done in the context of the Hiroshima AI Process, or &lt;em&gt;HAIP&lt;/em&gt;, Reporting Framework&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. This is an effort that is really just in its first round of really exploring how this kind of reporting can be really additive to risk management understanding. And again, I think there’s real opportunity here to look at this kind of reporting and understand, you know, what’s valuable for stakeholders and where is there opportunity to go further in really informing value chains and policymakers and the public about AI risk and opportunity and what can we learn again from other domains that have done this kind of work over decades to really refine that kind of information sharing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; It’s really great to hear about all the advances that we’re making on these reports. I’m guessing a lot of the metrics in there are technical, but sociotechnical impacts—jobs, maybe misinformation, well-being—are harder to score. What new measurement ideas are you excited about, and do you have any thoughts on, like, who needs to pilot those?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Yeah, it’s an incredibly interesting question that I think also just speaks to, you know, the breadth of, sort of, testing and evaluation that’s needed at different points along that AI life cycle and really not getting lost in one particular kind of testing or another pre- or post-deployment and thinking expansively about the risks that we’re trying to address through this testing.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, for example, even with the UK’s AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that has just recently launched a new program, a new team, that’s focused on societal resilience research. I think it’s going to be a really important area from a sociotechnical impact perspective to bring some focus into as this technology is more widely deployed. Are we understanding the impacts over time as different people and different cultures adopt and use this technology for different purposes?&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I think that’s an area where there really is opportunity for greater public-private partnership in this research. Because we all share this long-term interest in ensuring that this technology is really serving people and we have to understand the impacts so that we understand, you know, what adjustments we can actually pursue sooner upstream to address those impacts and make sure that this technology is really going to work for all of us and in a way that is consistent with the societal values that we want.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; So, Amanda, looking ahead, I would love to hear just what’s going to be on your radar? What’s top of mind for you in the coming weeks?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Well, we are certainly continuing to process all the learnings that we’ve had from studying these domains. It’s really been a rich set of insights that we want to make sure we, kind of, fully take advantage of. And, you know, I think these hard questions and, you know, real opportunities to be thoughtful in these early days of AI governance are not, sort of, going away or being easily resolved soon. And so I think we continue to see value in really learning from others, thinking about what’s distinct in the AI context, but also what we can apply in terms of what other domains have learned.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Well, Amanda, it has been such a special experience for me to help illuminate the work of the Office of Responsible AI and our team in Microsoft Research, and [MUSIC] it’s just really special to see all of the work that we’re doing to help set the standard for responsible development and deployment of AI. So thank you for joining us today, and thanks for your reflections and discussion.&lt;/p&gt;



&lt;p&gt;And to our listeners, thank you so much for joining us for the series. We really hope you enjoyed it!&amp;nbsp;To check out all of our episodes, visit aka.ms/AITestingandEvaluation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;See you next time!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES] &lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;







&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; Since the launch of the International Network of AI Safety Institutes, the UK renamed its institute the AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-reflections/</guid><pubDate>Mon, 21 Jul 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI jumps gun on International Math Olympiad gold medal announcement (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/openai-jumps-gun-on-international-math-olympiad-gold-medal-announcement/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Non-math AI model reportedly solves proofs at human speeds, but early reveal roils community.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/toy_robot_math-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/toy_robot_math-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Juj Winn / Eduard Muzhevski via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Saturday, OpenAI researcher Alexander Wei announced that a new AI language model the company is researching has achieved gold medal-level performance on the International Mathematical Olympiad (IMO), matching a standard that fewer than 9 percent of human contestants reach each year. The announcement came despite an embargo request from IMO organizers asking AI companies to wait until July 28 to share their results.&lt;/p&gt;
&lt;p&gt;The experimental model reportedly tackled the contest's six proof-based problems under the same constraints as human competitors: 4.5 hours per session, with no Internet access or calculators allowed. However, several sources with inside knowledge of the process say that since OpenAI self-graded its IMO results, the legitimacy of the company's claim may be in question. OpenAI plans to publish the proofs and grading rubrics for public review.&lt;/p&gt;
&lt;p&gt;According to OpenAI, its achievement marks a departure from previous AI attempts at mathematical Olympiad problems, which relied on specialized theorem-proving systems that often exceeded human time limits. OpenAI says its model processed problems as plain text and generated natural-language proofs, operating like a standard language model rather than a purpose-built mathematical system.&lt;/p&gt;
&lt;p&gt;The announcement follows Google's July 2024 claim that its AlphaProof and AlphaGeometry 2 models earned a silver medal equivalent at the IMO—though Google's systems required up to three days per problem rather than the 4.5-hour human time limit and needed human assistance to translate problems into formal mathematical language.&lt;/p&gt;
&lt;p&gt;"Math is a proving ground for reasoning—structured, rigorous, and hard to fake," the company wrote in a statement sent to Ars Technica. "This shows that scalable, general-purpose methods can now outperform hand-tuned systems in tasks long seen as out of reach."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While the company confirmed that its next major AI model, GPT-5, is "coming soon," it clarified that this current model is experimental. "The techniques will carry forward, but nothing with this level of capability will be released for a while," OpenAI says. It's likely that OpenAI needed to devote a great deal of computational resources (which means high cost) for this particular experiment, and that level of computation won't be typical of consumer-facing AI models in the near future.&lt;/p&gt;
&lt;h2&gt;Surprising results for a general-purpose AI model&lt;/h2&gt;
&lt;p&gt;OpenAI says that the research team behind the experimental AI model, led by Alex Wei with support from Sheryl Hsu and Noam Brown, hadn't initially planned to enter the competition but decided to evaluate their work after observing promising results in testing.&lt;/p&gt;
&lt;p&gt;"This wasn’t a system built for math. It’s the same kind of LLM we train for language, coding, and science—solving full proof-based problems under standard IMO constraints: 4.5 hours, no internet, no calculators," OpenAI said in a statement.&lt;/p&gt;
&lt;p&gt;OpenAI received problems that were freshly written by the IMO organizer and shared with several AI companies simultaneously. To validate the results, each solution reportedly underwent blind grading by a panel of three former IMO medalists organized by OpenAI, with unanimous consensus required for acceptance.&lt;/p&gt;
&lt;p&gt;However, in addition to the controversy over self-grading the results, OpenAI also annoyed the IMO community because its Saturday announcement appears to have violated the embargo agreement with the International Mathematical Olympiad. Harmonic, another AI company that participated in the competition, revealed in an X post on July 20 that "the IMO Board has asked us, along with the other leading AI companies that participated, to hold on releasing our results until Jul 28th."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The early announcement has prompted Google DeepMind, which had prepared its own IMO results for the agreed-upon date, to move up its own IMO-related announcement to later today. Harmonic plans to share its results as originally scheduled on July 28.&lt;/p&gt;
&lt;p&gt;In response to the controversy, OpenAI research scientist Noam Brown posted on X, "We weren't in touch with IMO. I spoke with one organizer before the post to let him know. He requested we wait until after the closing ceremony ends to respect the kids, and we did."&lt;/p&gt;
&lt;p&gt;However, an IMO coordinator told X user Mikhail Samin that OpenAI actually announced before the closing ceremony, contradicting Brown's claim. The coordinator called OpenAI's actions "rude and inappropriate," noting that OpenAI "wasn't one of the AI companies that cooperated with the IMO on testing their models."&lt;/p&gt;
&lt;h2&gt;Hard math since 1959&lt;/h2&gt;
&lt;p&gt;The International Mathematical Olympiad, which has been running since 1959, represents one of the most challenging tests of mathematical reasoning. More than 100 countries send six participants each, with contestants facing six proof-based problems across two 4.5-hour sessions. The problems typically require deep mathematical insight and creativity rather than raw computational power. You can see the exact problems in the 2025 Olympiad posted online.&lt;/p&gt;
&lt;p&gt;For example, problem one asks students to imagine a triangular grid of dots (like a triangular pegboard) and figure out how to cover all the dots using exactly n straight lines. The twist is that some lines are called "sunny"—these are the lines that don't run horizontally, vertically, or diagonally at a 45º angle. The challenge is to prove that no matter how big your triangle is, you can only ever create patterns with exactly 0, 1, or 3 sunny lines—never 2, never 4, never any other number.&lt;/p&gt;
&lt;p&gt;The timing of the OpenAI results surprised some prediction markets, which had assigned around an 18 percent probability to any AI system winning IMO gold by 2025. However, depending on what Google says this afternoon (and what others like Harmonic may release on July 28), OpenAI may not be the only AI company to have achieved these unexpected results.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Non-math AI model reportedly solves proofs at human speeds, but early reveal roils community.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/toy_robot_math-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/toy_robot_math-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Juj Winn / Eduard Muzhevski via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Saturday, OpenAI researcher Alexander Wei announced that a new AI language model the company is researching has achieved gold medal-level performance on the International Mathematical Olympiad (IMO), matching a standard that fewer than 9 percent of human contestants reach each year. The announcement came despite an embargo request from IMO organizers asking AI companies to wait until July 28 to share their results.&lt;/p&gt;
&lt;p&gt;The experimental model reportedly tackled the contest's six proof-based problems under the same constraints as human competitors: 4.5 hours per session, with no Internet access or calculators allowed. However, several sources with inside knowledge of the process say that since OpenAI self-graded its IMO results, the legitimacy of the company's claim may be in question. OpenAI plans to publish the proofs and grading rubrics for public review.&lt;/p&gt;
&lt;p&gt;According to OpenAI, its achievement marks a departure from previous AI attempts at mathematical Olympiad problems, which relied on specialized theorem-proving systems that often exceeded human time limits. OpenAI says its model processed problems as plain text and generated natural-language proofs, operating like a standard language model rather than a purpose-built mathematical system.&lt;/p&gt;
&lt;p&gt;The announcement follows Google's July 2024 claim that its AlphaProof and AlphaGeometry 2 models earned a silver medal equivalent at the IMO—though Google's systems required up to three days per problem rather than the 4.5-hour human time limit and needed human assistance to translate problems into formal mathematical language.&lt;/p&gt;
&lt;p&gt;"Math is a proving ground for reasoning—structured, rigorous, and hard to fake," the company wrote in a statement sent to Ars Technica. "This shows that scalable, general-purpose methods can now outperform hand-tuned systems in tasks long seen as out of reach."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While the company confirmed that its next major AI model, GPT-5, is "coming soon," it clarified that this current model is experimental. "The techniques will carry forward, but nothing with this level of capability will be released for a while," OpenAI says. It's likely that OpenAI needed to devote a great deal of computational resources (which means high cost) for this particular experiment, and that level of computation won't be typical of consumer-facing AI models in the near future.&lt;/p&gt;
&lt;h2&gt;Surprising results for a general-purpose AI model&lt;/h2&gt;
&lt;p&gt;OpenAI says that the research team behind the experimental AI model, led by Alex Wei with support from Sheryl Hsu and Noam Brown, hadn't initially planned to enter the competition but decided to evaluate their work after observing promising results in testing.&lt;/p&gt;
&lt;p&gt;"This wasn’t a system built for math. It’s the same kind of LLM we train for language, coding, and science—solving full proof-based problems under standard IMO constraints: 4.5 hours, no internet, no calculators," OpenAI said in a statement.&lt;/p&gt;
&lt;p&gt;OpenAI received problems that were freshly written by the IMO organizer and shared with several AI companies simultaneously. To validate the results, each solution reportedly underwent blind grading by a panel of three former IMO medalists organized by OpenAI, with unanimous consensus required for acceptance.&lt;/p&gt;
&lt;p&gt;However, in addition to the controversy over self-grading the results, OpenAI also annoyed the IMO community because its Saturday announcement appears to have violated the embargo agreement with the International Mathematical Olympiad. Harmonic, another AI company that participated in the competition, revealed in an X post on July 20 that "the IMO Board has asked us, along with the other leading AI companies that participated, to hold on releasing our results until Jul 28th."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The early announcement has prompted Google DeepMind, which had prepared its own IMO results for the agreed-upon date, to move up its own IMO-related announcement to later today. Harmonic plans to share its results as originally scheduled on July 28.&lt;/p&gt;
&lt;p&gt;In response to the controversy, OpenAI research scientist Noam Brown posted on X, "We weren't in touch with IMO. I spoke with one organizer before the post to let him know. He requested we wait until after the closing ceremony ends to respect the kids, and we did."&lt;/p&gt;
&lt;p&gt;However, an IMO coordinator told X user Mikhail Samin that OpenAI actually announced before the closing ceremony, contradicting Brown's claim. The coordinator called OpenAI's actions "rude and inappropriate," noting that OpenAI "wasn't one of the AI companies that cooperated with the IMO on testing their models."&lt;/p&gt;
&lt;h2&gt;Hard math since 1959&lt;/h2&gt;
&lt;p&gt;The International Mathematical Olympiad, which has been running since 1959, represents one of the most challenging tests of mathematical reasoning. More than 100 countries send six participants each, with contestants facing six proof-based problems across two 4.5-hour sessions. The problems typically require deep mathematical insight and creativity rather than raw computational power. You can see the exact problems in the 2025 Olympiad posted online.&lt;/p&gt;
&lt;p&gt;For example, problem one asks students to imagine a triangular grid of dots (like a triangular pegboard) and figure out how to cover all the dots using exactly n straight lines. The twist is that some lines are called "sunny"—these are the lines that don't run horizontally, vertically, or diagonally at a 45º angle. The challenge is to prove that no matter how big your triangle is, you can only ever create patterns with exactly 0, 1, or 3 sunny lines—never 2, never 4, never any other number.&lt;/p&gt;
&lt;p&gt;The timing of the OpenAI results surprised some prediction markets, which had assigned around an 18 percent probability to any AI system winning IMO gold by 2025. However, depending on what Google says this afternoon (and what others like Harmonic may release on July 28), OpenAI may not be the only AI company to have achieved these unexpected results.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/openai-jumps-gun-on-international-math-olympiad-gold-medal-announcement/</guid><pubDate>Mon, 21 Jul 2025 16:02:04 +0000</pubDate></item><item><title>[NEW] Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Tu0v56OLL1Jy30PUoBzm_P8bZiFi3QhmKuxKF95j8ojB0B8uhHmG1QBRhLF09T5VWtIcIilGO-DXKTHDtGX11BB9OOiYr_w1tFs8bWVpqhIi3_isjg=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;We thank the International Mathematical Olympiad organization for their support.&lt;/p&gt;&lt;p&gt;Thang Luong led the overall technical direction of the advanced Gemini model with Deep Think for IMO and co-led with Edward Lockhart on the overall coordination of the IMO 2025 effort.&lt;/p&gt;&lt;p&gt;The IMO 2025 system would not have been possible without the following technical leads. Dawsen Hwang, Junehyuk Jung co-led training data and expert evaluation. Jonathan Lee, Nate Kushman, Pol Moreno, Yi Tay co-led the training of the advanced Gemini Deep Think model while Lei Yu led model evaluation. Golnaz Ghiazi, Garrett Bingham, Lalit Jain co-led Deep Think inference while Dawsen Hwang, Vincent Cohen-Addad co-led an enhanced inference approach.&lt;/p&gt;&lt;p&gt;The IMO 2025 system was also developed with key contributions from Theophane Weber, Ankesh Anand for modeling; Vinay Ramasesh, Andreas Kirsch, Jieming Mao, Zicheng Xu, Wilfried Bounsi, Vahab Mirrokni for inference; Hoang Nguyen, Fred Zhang, Mahan Malihi, Yangsibo Huang for training data.&lt;/p&gt;&lt;p&gt;We thank contributions from related teams and efforts. AlphaGeometry team with Yuri Chervonyi (lead), Trieu Trinh, Hoang Nguyen, Junsu Kim, Mirek Olšák, Marcelo Menegali, Xiaomeng Yang. Miklós Z. Horváth, Aja Huang, Goran Žužić for formal mathematics. We thank Fabian Pedregosa, Richard Song, Alex Zhai, Sara Javanmardi, YaGuang Li, Filipe Miguel de Almeida, Silvio Lattanzi, Ashkan Norouzi Fard, Tal Schuster, Honglu Fan, Xuezhi Wang, Aditi Mavalankar, Tom Schaul, Rosemary Ke for support and collaboration.&lt;/p&gt;&lt;p&gt;We especially thank other core members of the Deep Think team (Archit Sharma, Tong He, Shubha Raghvendra), the post-training effort (Tianhe Kevin Yu, Siamak Shakeri, Hanzhao Lin, Cosmo Du, Sid Lall), and Thinking Area research that the IMO 2025 system were built on.&lt;/p&gt;&lt;p&gt;This effort was advised by Quoc Le and Pushmeet Kohli, with program support from Kristen Chiafullo and Alex Goldin.&lt;/p&gt;&lt;p&gt;We’d also like to thank our experts for providing data and evaluations: Insuk Seo (lead), Jiwon Kang, Donghyun Kim, Junsu Kim, Jimin Kim, Seongbin Jeon, Yoonho Na, Seunghwan Lee, Jihoo Lee, Younghun Jo, Yongsuk Hur, Seongjae Park, Kyuhyeon Choi, Minkyu Choi, Su-Hyeok Moon, Seojin Kim, Yueun Lee, Taehun Kim, Jeeho Ryu, Seungwoo Lee, Dain Kim, Sanha Lee, Hyunwoo Choi, Aiden Jung, Youngbeom Jin, Jeonghyun Ahn, Junhwi Bae, Gyumin Kim, Nam Dung Tran, Cheng-Chiang Tsai, Kari Ragnarsson, Kiat Chuan Tan, Yahya Tabesh, Hamed Mahdavi, Azin Nazari, Xiangzhuo Ding, Chu-Lan Kao, Steven Creech, Tony Feng, Ciprian Manolescu.&lt;/p&gt;&lt;p&gt;Further thanks to Jessica Lo and Sajjad Zafar for their support for compute provision and management; Jane Labanowski, Andy Forbes, Sean Nakamoto for legal and logistics; and Omer Levy, Timothy Lillicrap, Jack Rae, Yifeng Lu, Heng-tze Cheng, Ed Chi, Vahab Mirrokni, Tulsee Doshi, Madhavi Sewak, Melvin Johnson, Koray Kavukcuoglu, Oriol Vinyals, Jeff Dean, Demis Hassabis, and Sergey Brin for their support and advice.&lt;/p&gt;&lt;p&gt;Finally, we thank Prof Gregor Dolinar from the IMO Board for the support and endorsement.&lt;/p&gt;&lt;p&gt;The IMO have confirmed that our submitted answers are complete and correct solutions. It is important to note that their review does not extend to validating our system, processes, or underlying model (see more).&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/Tu0v56OLL1Jy30PUoBzm_P8bZiFi3QhmKuxKF95j8ojB0B8uhHmG1QBRhLF09T5VWtIcIilGO-DXKTHDtGX11BB9OOiYr_w1tFs8bWVpqhIi3_isjg=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgements&lt;/h2&gt;&lt;p&gt;We thank the International Mathematical Olympiad organization for their support.&lt;/p&gt;&lt;p&gt;Thang Luong led the overall technical direction of the advanced Gemini model with Deep Think for IMO and co-led with Edward Lockhart on the overall coordination of the IMO 2025 effort.&lt;/p&gt;&lt;p&gt;The IMO 2025 system would not have been possible without the following technical leads. Dawsen Hwang, Junehyuk Jung co-led training data and expert evaluation. Jonathan Lee, Nate Kushman, Pol Moreno, Yi Tay co-led the training of the advanced Gemini Deep Think model while Lei Yu led model evaluation. Golnaz Ghiazi, Garrett Bingham, Lalit Jain co-led Deep Think inference while Dawsen Hwang, Vincent Cohen-Addad co-led an enhanced inference approach.&lt;/p&gt;&lt;p&gt;The IMO 2025 system was also developed with key contributions from Theophane Weber, Ankesh Anand for modeling; Vinay Ramasesh, Andreas Kirsch, Jieming Mao, Zicheng Xu, Wilfried Bounsi, Vahab Mirrokni for inference; Hoang Nguyen, Fred Zhang, Mahan Malihi, Yangsibo Huang for training data.&lt;/p&gt;&lt;p&gt;We thank contributions from related teams and efforts. AlphaGeometry team with Yuri Chervonyi (lead), Trieu Trinh, Hoang Nguyen, Junsu Kim, Mirek Olšák, Marcelo Menegali, Xiaomeng Yang. Miklós Z. Horváth, Aja Huang, Goran Žužić for formal mathematics. We thank Fabian Pedregosa, Richard Song, Alex Zhai, Sara Javanmardi, YaGuang Li, Filipe Miguel de Almeida, Silvio Lattanzi, Ashkan Norouzi Fard, Tal Schuster, Honglu Fan, Xuezhi Wang, Aditi Mavalankar, Tom Schaul, Rosemary Ke for support and collaboration.&lt;/p&gt;&lt;p&gt;We especially thank other core members of the Deep Think team (Archit Sharma, Tong He, Shubha Raghvendra), the post-training effort (Tianhe Kevin Yu, Siamak Shakeri, Hanzhao Lin, Cosmo Du, Sid Lall), and Thinking Area research that the IMO 2025 system were built on.&lt;/p&gt;&lt;p&gt;This effort was advised by Quoc Le and Pushmeet Kohli, with program support from Kristen Chiafullo and Alex Goldin.&lt;/p&gt;&lt;p&gt;We’d also like to thank our experts for providing data and evaluations: Insuk Seo (lead), Jiwon Kang, Donghyun Kim, Junsu Kim, Jimin Kim, Seongbin Jeon, Yoonho Na, Seunghwan Lee, Jihoo Lee, Younghun Jo, Yongsuk Hur, Seongjae Park, Kyuhyeon Choi, Minkyu Choi, Su-Hyeok Moon, Seojin Kim, Yueun Lee, Taehun Kim, Jeeho Ryu, Seungwoo Lee, Dain Kim, Sanha Lee, Hyunwoo Choi, Aiden Jung, Youngbeom Jin, Jeonghyun Ahn, Junhwi Bae, Gyumin Kim, Nam Dung Tran, Cheng-Chiang Tsai, Kari Ragnarsson, Kiat Chuan Tan, Yahya Tabesh, Hamed Mahdavi, Azin Nazari, Xiangzhuo Ding, Chu-Lan Kao, Steven Creech, Tony Feng, Ciprian Manolescu.&lt;/p&gt;&lt;p&gt;Further thanks to Jessica Lo and Sajjad Zafar for their support for compute provision and management; Jane Labanowski, Andy Forbes, Sean Nakamoto for legal and logistics; and Omer Levy, Timothy Lillicrap, Jack Rae, Yifeng Lu, Heng-tze Cheng, Ed Chi, Vahab Mirrokni, Tulsee Doshi, Madhavi Sewak, Melvin Johnson, Koray Kavukcuoglu, Oriol Vinyals, Jeff Dean, Demis Hassabis, and Sergey Brin for their support and advice.&lt;/p&gt;&lt;p&gt;Finally, we thank Prof Gregor Dolinar from the IMO Board for the support and endorsement.&lt;/p&gt;&lt;p&gt;The IMO have confirmed that our submitted answers are complete and correct solutions. It is important to note that their review does not extend to validating our system, processes, or underlying model (see more).&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/</guid><pubDate>Mon, 21 Jul 2025 16:30:02 +0000</pubDate></item><item><title>[NEW] Experimental surgery performed by AI-driven surgical robot (AI – Ars Technica)</title><link>https://arstechnica.com/science/2025/07/experimental-surgery-performed-by-ai-driven-surgical-robot/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        In experimental surgery on pig organs, the robot performed well.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Image of a room in a hospital featuring two surgical robots and two control systems." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2210992604-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Image of a room in a hospital featuring two surgical robots and two control systems." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2210992604-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Surgical robots like these may not always need an actual surgeon.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Jacob King / PA Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Intuitive Surgical, an American biotechnology company, introduced DaVinci surgical robots in the late 1990s, and they became groundbreaking teleoperation equipment. Expert surgeons could operate on patients remotely, manipulating the robotic arms and their surgical tools based on a video feed from DaVinci’s built-in cameras and endoscopes.&lt;/p&gt;
&lt;p&gt;Now, John Hopkins University researchers put a ChatGPT-like AI in charge of a DaVinci robot and taught it to perform a gallbladder-removal surgery.&lt;/p&gt;
&lt;h2&gt;Kuka surgeries&lt;/h2&gt;
&lt;p&gt;The idea to put a computer behind the wheel of a surgical robot is not entirely new, but these had mostly relied on using pre-programmed actions. “The program told the robot exactly how to move and what to do. It worked like in these Kuka robotic arms, welding cars on factory floors,” says Ji Woong Kim, a robotics researcher who led the study on autonomous surgery. To improve on that, a team led by Axel Krieger, an assistant professor of mechanical engineering at John Hopkins University, built STAR: the Smart Tissue Autonomous Robot. In 2022, it successfully performed a surgery on a live pig.&lt;/p&gt;
&lt;p&gt;But even STAR couldn’t do it without specially marked tissues and a predetermined plan. STAR’s key difference was that its AI could make adjustments to this plan based on the feed from cameras.&lt;/p&gt;
&lt;p&gt;The new robot can do considerably more. “Our current work is much more flexible,” Kim says. “It is an AI that learns from demonstrations.” The new system is called SRT-H (Surgical Robot Transformer) and was developed by Kim and his colleagues, Krieger added.&lt;/p&gt;
&lt;p&gt;The first change they made was to the hardware. Instead of using a custom robot like STAR, the new work relied on the DaVinci robot, which has become a de facto industry standard in teleoperation surgeries, with over 10,000 units already deployed in hospitals worldwide. The second change was the software driving the system. It relied on two transformer models, the same architecture that powers ChatGPT. One was a high-level policy module, which was responsible for task planning and ensuring the procedure went smoothly. The low-level module was responsible for executing the tasks issued by the high-level module, translating its instructions into specific trajectories for the robotic arms.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;When the system was ready, Kim’s team put it through a training phase that looked a bit like mentoring a novice human doctor.&lt;/p&gt;
&lt;h2&gt;Imitation learning&lt;/h2&gt;
&lt;p&gt;The procedure Kim chose for the robot to master was cholecystectomy, a surgical gallbladder removal routinely performed in US hospitals (roughly 700,000 times a year). “The objective is to remove the tubes connecting the gallbladder to other organs without causing the internal fluids to flow out,” Kim explained. To make that happen, a surgeon has to place three clips on the cystic duct (the first tube), cut&amp;nbsp;it, and then clip and cut the cystic artery (the second tube) in a similar way.&lt;/p&gt;
&lt;p&gt;Kim’s team broke this procedure down into 17 steps, sourced lots of porcine gallbladder and liver samples from pig cadavers to experiment on, and had a trained research assistant operate a DaVinci robot, performing the procedure over and over again to build the training data set for the robot.&lt;/p&gt;
&lt;p&gt;Algorithms that would power the SRT-H were trained on over 17 hours of video captured from the DaVinci endoscope and cameras installed on its robotic arms. This video feed was complemented by the kinematics data—the exact motions made by the robotic arms—and natural language annotations.&lt;/p&gt;
&lt;p&gt;Based on this data, Kim’s robot learned to perform a cholecystectomy with 100 percent success rate when operating on samples it has not been trained on. It could also accept human feedback in natural language—simple tips like “move your arm a bit to the left” or “put the clip a bit higher.” These are the sorts of hints a mentoring surgeon would give to a student and, in a similar way, SRT-H could learn from them over time.&lt;/p&gt;
&lt;p&gt;“You can take any kind of surgery, not just this one, train the robot in the same way, and it will be able to perform that surgery,” Kim says. SRT-H was also robust to differences in anatomy between samples, other tissue getting in the way, and imperfect imagery. It could even recover from all the tiny mistakes it was making during the training process. Compared to an expert human surgeon performing the same procedure, the robot was equally precise, although a bit slower.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But it wasn’t robust against corporate affairs.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Robotic secrets&lt;/h2&gt;
&lt;p&gt;To move from operating on pig cadaver samples to live pigs and then, potentially, to humans, robots like SRT-H need training data that is extremely hard to come by. Intuitive Surgical is apparently OK with releasing the video feed data from the DaVinci robots, but the company does not release the kinematics data. And that’s data that Kim says is necessary for training the algorithms. “I know people at Intuitive Surgical headquarters, and I’ve been talking to them,” Kim says. “I’ve been begging them to give us the data. They did not agree.”&lt;/p&gt;
&lt;p&gt;The explanation Intuitive Surgical leadership offered for restricting access to the kinematics data, according to Kim, was they were worried about the competition reverse-engineering the mechanics of their robot. “It’s really the upper management who is not up to speed with AI,” Kim argued. “They don’t realize the potential of these things. Their engineers, every scientist, they want to open-source the data. It’s just their legal department is very conservative.”&lt;/p&gt;
&lt;p&gt;But he already sees a way around this problem. “We can start with attaching motion-tracking sensors to manual surgical tools, and get the kinematics data this way,” Kim told Ars. Then, the movements of these tools guided by the hands of expert human surgeons could be recreated by conventional robotic arms like the ones used in STAR.&lt;/p&gt;
&lt;p&gt;And then, Kim thinks, we can go even more sci-fi than that. “I’m currently at Stanford, and I’m very involved in a humanoid robotics project—building a general-purpose model. And one of the possible applications is in the operating room,” Kim says.&lt;/p&gt;
&lt;p&gt;Science Robotics, 2025. &amp;nbsp;DOI: 10.1126/scirobotics.adt5254&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        In experimental surgery on pig organs, the robot performed well.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Image of a room in a hospital featuring two surgical robots and two control systems." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2210992604-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Image of a room in a hospital featuring two surgical robots and two control systems." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2210992604-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Surgical robots like these may not always need an actual surgeon.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Jacob King / PA Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Intuitive Surgical, an American biotechnology company, introduced DaVinci surgical robots in the late 1990s, and they became groundbreaking teleoperation equipment. Expert surgeons could operate on patients remotely, manipulating the robotic arms and their surgical tools based on a video feed from DaVinci’s built-in cameras and endoscopes.&lt;/p&gt;
&lt;p&gt;Now, John Hopkins University researchers put a ChatGPT-like AI in charge of a DaVinci robot and taught it to perform a gallbladder-removal surgery.&lt;/p&gt;
&lt;h2&gt;Kuka surgeries&lt;/h2&gt;
&lt;p&gt;The idea to put a computer behind the wheel of a surgical robot is not entirely new, but these had mostly relied on using pre-programmed actions. “The program told the robot exactly how to move and what to do. It worked like in these Kuka robotic arms, welding cars on factory floors,” says Ji Woong Kim, a robotics researcher who led the study on autonomous surgery. To improve on that, a team led by Axel Krieger, an assistant professor of mechanical engineering at John Hopkins University, built STAR: the Smart Tissue Autonomous Robot. In 2022, it successfully performed a surgery on a live pig.&lt;/p&gt;
&lt;p&gt;But even STAR couldn’t do it without specially marked tissues and a predetermined plan. STAR’s key difference was that its AI could make adjustments to this plan based on the feed from cameras.&lt;/p&gt;
&lt;p&gt;The new robot can do considerably more. “Our current work is much more flexible,” Kim says. “It is an AI that learns from demonstrations.” The new system is called SRT-H (Surgical Robot Transformer) and was developed by Kim and his colleagues, Krieger added.&lt;/p&gt;
&lt;p&gt;The first change they made was to the hardware. Instead of using a custom robot like STAR, the new work relied on the DaVinci robot, which has become a de facto industry standard in teleoperation surgeries, with over 10,000 units already deployed in hospitals worldwide. The second change was the software driving the system. It relied on two transformer models, the same architecture that powers ChatGPT. One was a high-level policy module, which was responsible for task planning and ensuring the procedure went smoothly. The low-level module was responsible for executing the tasks issued by the high-level module, translating its instructions into specific trajectories for the robotic arms.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;When the system was ready, Kim’s team put it through a training phase that looked a bit like mentoring a novice human doctor.&lt;/p&gt;
&lt;h2&gt;Imitation learning&lt;/h2&gt;
&lt;p&gt;The procedure Kim chose for the robot to master was cholecystectomy, a surgical gallbladder removal routinely performed in US hospitals (roughly 700,000 times a year). “The objective is to remove the tubes connecting the gallbladder to other organs without causing the internal fluids to flow out,” Kim explained. To make that happen, a surgeon has to place three clips on the cystic duct (the first tube), cut&amp;nbsp;it, and then clip and cut the cystic artery (the second tube) in a similar way.&lt;/p&gt;
&lt;p&gt;Kim’s team broke this procedure down into 17 steps, sourced lots of porcine gallbladder and liver samples from pig cadavers to experiment on, and had a trained research assistant operate a DaVinci robot, performing the procedure over and over again to build the training data set for the robot.&lt;/p&gt;
&lt;p&gt;Algorithms that would power the SRT-H were trained on over 17 hours of video captured from the DaVinci endoscope and cameras installed on its robotic arms. This video feed was complemented by the kinematics data—the exact motions made by the robotic arms—and natural language annotations.&lt;/p&gt;
&lt;p&gt;Based on this data, Kim’s robot learned to perform a cholecystectomy with 100 percent success rate when operating on samples it has not been trained on. It could also accept human feedback in natural language—simple tips like “move your arm a bit to the left” or “put the clip a bit higher.” These are the sorts of hints a mentoring surgeon would give to a student and, in a similar way, SRT-H could learn from them over time.&lt;/p&gt;
&lt;p&gt;“You can take any kind of surgery, not just this one, train the robot in the same way, and it will be able to perform that surgery,” Kim says. SRT-H was also robust to differences in anatomy between samples, other tissue getting in the way, and imperfect imagery. It could even recover from all the tiny mistakes it was making during the training process. Compared to an expert human surgeon performing the same procedure, the robot was equally precise, although a bit slower.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But it wasn’t robust against corporate affairs.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Robotic secrets&lt;/h2&gt;
&lt;p&gt;To move from operating on pig cadaver samples to live pigs and then, potentially, to humans, robots like SRT-H need training data that is extremely hard to come by. Intuitive Surgical is apparently OK with releasing the video feed data from the DaVinci robots, but the company does not release the kinematics data. And that’s data that Kim says is necessary for training the algorithms. “I know people at Intuitive Surgical headquarters, and I’ve been talking to them,” Kim says. “I’ve been begging them to give us the data. They did not agree.”&lt;/p&gt;
&lt;p&gt;The explanation Intuitive Surgical leadership offered for restricting access to the kinematics data, according to Kim, was they were worried about the competition reverse-engineering the mechanics of their robot. “It’s really the upper management who is not up to speed with AI,” Kim argued. “They don’t realize the potential of these things. Their engineers, every scientist, they want to open-source the data. It’s just their legal department is very conservative.”&lt;/p&gt;
&lt;p&gt;But he already sees a way around this problem. “We can start with attaching motion-tracking sensors to manual surgical tools, and get the kinematics data this way,” Kim told Ars. Then, the movements of these tools guided by the hands of expert human surgeons could be recreated by conventional robotic arms like the ones used in STAR.&lt;/p&gt;
&lt;p&gt;And then, Kim thinks, we can go even more sci-fi than that. “I’m currently at Stanford, and I’m very involved in a humanoid robotics project—building a general-purpose model. And one of the possible applications is in the operating room,” Kim says.&lt;/p&gt;
&lt;p&gt;Science Robotics, 2025. &amp;nbsp;DOI: 10.1126/scirobotics.adt5254&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/science/2025/07/experimental-surgery-performed-by-ai-driven-surgical-robot/</guid><pubDate>Mon, 21 Jul 2025 17:15:00 +0000</pubDate></item></channel></rss>