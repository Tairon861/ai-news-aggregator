<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 03 Dec 2025 01:48:36 +0000</lastBuildDate><item><title>Ascentra Labs raises $2 million to help consultants use AI instead of all-night Excel marathons (AI | VentureBeat)</title><link>https://venturebeat.com/ai/ascentra-labs-raises-usd2-million-to-help-consultants-use-ai-instead-of-all</link><description>[unable to retrieve full-text content]&lt;p&gt;
&lt;/p&gt;&lt;p&gt;While artificial intelligence has stormed into law firms and accounting practices with billion-dollar startups like Harvey leading the charge, the global consulting industry—a $250 billion behemoth—has remained stubbornly analog. A London-based startup founded by former McKinsey consultants is betting $2 million that it can crack open this resistant market, one Excel spreadsheet at a time.&lt;/p&gt;&lt;p&gt;&lt;a href="https://ascentralabs.ai/"&gt;&lt;u&gt;Ascentra Labs&lt;/u&gt;&lt;/a&gt; announced Tuesday that it has closed a $2 million seed round led by &lt;a href="https://www.nap.vc/"&gt;&lt;u&gt;NAP&lt;/u&gt;&lt;/a&gt;, a Berlin-based venture capital firm formerly known as Cavalry Ventures. The funding comes with participation from notable founder-angels including Alan Chang, chief executive of Fuse and former chief revenue officer at Revolut, and Fredrik Hjelm, chief executive of European e-scooter company Voi.&lt;/p&gt;&lt;p&gt;The investment is modest by the standards of enterprise AI — a sector that has seen funding rounds routinely reach into the hundreds of millions. But Ascentra&amp;#x27;s founders argue that their focused approach to a narrow but painful problem could give them an edge in a market where broad AI solutions have repeatedly failed to gain traction.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Consultants spend countless hours on Excel survey analysis that even top firms haven&amp;#x27;t automated&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://ascentralabs.ai/about"&gt;&lt;u&gt;Paritosh Devbhandari&lt;/u&gt;&lt;/a&gt;, Ascentra&amp;#x27;s co-founder and chief executive, spent years at &lt;a href="https://www.mckinsey.com/"&gt;&lt;u&gt;McKinsey &amp;amp; Company&lt;/u&gt;&lt;/a&gt;, including a stint at &lt;a href="https://www.mckinsey.com/capabilities/quantumblack/how-we-help-clients"&gt;&lt;u&gt;QuantumBlack&lt;/u&gt;&lt;/a&gt;, the firm&amp;#x27;s AI and advanced analytics division. He knows intimately the late nights consultants spend wrestling with survey data—the kind of quantitative research that forms the backbone of private equity due diligence.&lt;/p&gt;&lt;p&gt;&amp;quot;Before starting the company, I was working at McKinsey, specifically on the private equity team,&amp;quot; Devbhandari explained in an exclusive interview with VentureBeat. The work, he said, involves analyzing encoded survey responses from customers, suppliers, and market participants during potential acquisitions.&lt;/p&gt;&lt;p&gt;&amp;quot;Consultants typically spend a lot of time doing this in Excel,&amp;quot; he said. &amp;quot;One of the things that surprised me, having worked at a couple of different places, is that the workflow — even at the best firms — really isn&amp;#x27;t that different from some of the boutiques. I always expected there would be some smarter way of doing things, and often there just isn&amp;#x27;t.&amp;quot;&lt;/p&gt;&lt;p&gt;That gap between expectation and reality became the foundation for &lt;a href="https://ascentralabs.ai/"&gt;&lt;u&gt;Ascentra&lt;/u&gt;&lt;/a&gt;. The company&amp;#x27;s platform ingests raw survey data files and outputs formatted Excel workbooks complete with traceable formulas — the kind of deliverable a junior associate would spend hours constructing manually.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI has transformed legal work but consulting presents unique technical challenges that have blocked adoption&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The disparity between AI adoption in law versus consulting raises an obvious question: if the consulting market is so large and the workflows so manual, why hasn&amp;#x27;t venture capital flooded the space the way it has legal tech?&lt;/p&gt;&lt;p&gt;Devbhandari offered a frank assessment. &amp;quot;It&amp;#x27;s not like people haven&amp;#x27;t tried,&amp;quot; he said. &amp;quot;The top of the funnel in our space is crowded. When we speak to our consulting clients, the partners say they get another pitch deck in their LinkedIn inbox or email every week—sometimes several. There are plenty of people trying.&amp;quot;&lt;/p&gt;&lt;p&gt;The barriers, he argued, are structural. Professional services firms move slowly on technology adoption, demanding extensive security credentials and customer references before granting even a pilot opportunity. &amp;quot;I think that&amp;#x27;s where 90% of startups in professional services, writ large, fall down,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;But consulting presents unique technical challenges beyond the sales cycle. Unlike legal work, which largely involves text documents that modern large language models handle well, consulting spans multiple data modalities — PowerPoint presentations, Excel spreadsheets, Word documents — with information that can be tabular, graphical, or textual.&lt;/p&gt;&lt;p&gt;&amp;quot;You can have multiple formats of Excel in itself,&amp;quot; Devbhandari noted. &amp;quot;And that&amp;#x27;s a big contrast to the legal space, where you could have a multi-purpose AI agent, or collection of agents, which can actually do a lot of the tasks that lawyers do day to day. Consulting is the opposite of that.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ascentra&amp;#x27;s private equity focus reflects a calculated bet on repeatable workflows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Ascentra&amp;#x27;s strategy hinges on extreme specificity. Rather than attempting to automate the full spectrum of consulting work, the company focuses exclusively on survey analysis within private equity due diligence — a niche within a niche.&lt;/p&gt;&lt;p&gt;The logic is both technical and commercial. Private equity work tends to be more standardized than other consulting engagements, with similar analyses recurring across deals. That repeatability makes automation feasible. It also positions Ascentra against a less formidable competitive set: even the largest consulting firms, Devbhandari claimed, lack dedicated internal tools for this particular workflow.&lt;/p&gt;&lt;p&gt;&amp;quot;Survey analysis automation is so specific that even the biggest and best firms haven&amp;#x27;t developed anything in-house for it,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;The company claims that three of the world&amp;#x27;s top five consulting firms now use its platform, with early adopters reporting time savings of 60 to 80 percent on active due diligence projects. But there&amp;#x27;s a notable caveat: Ascentra cannot publicly name any of these clients.&lt;/p&gt;&lt;p&gt;&amp;quot;It&amp;#x27;s a very private industry, so at the moment, we can&amp;#x27;t announce any clients publicly,&amp;quot; Devbhandari acknowledged. &amp;quot;What I can say is that we&amp;#x27;re working with three of the top five consulting firms. We&amp;#x27;ve passed pilots at multiple organizations and have submitted business cases for enterprise rollouts.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Eliminating AI hallucinations becomes critical when billion-dollar deals hang in the balance&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For an AI company selling into quantitative workflows, accuracy is existential. Consultants delivering analysis to private equity clients face enormous pressure to be precise—a single error in a financial model can undermine credibility and, potentially, billion-dollar investment decisions.&lt;/p&gt;&lt;p&gt;Devbhandari described this as Ascentra&amp;#x27;s central design challenge. &amp;quot;Consultants require a very, very high degree of fidelity when they&amp;#x27;re doing their analysis,&amp;quot; he said. &amp;quot;So with quantitative data, even if it&amp;#x27;s 95% accurate, they will revert to Excel because they know it, they trust it, and they don&amp;#x27;t want there to be any margin for error.&amp;quot;&lt;/p&gt;&lt;p&gt;Ascentra&amp;#x27;s technical approach attempts to address this by limiting where AI models operate within the workflow. The company uses GPT-based models from OpenAI to interpret and ingest incoming data, but the actual analysis relies on deterministic Python scripts that produce consistent, verifiable outputs.&lt;/p&gt;&lt;p&gt;&amp;quot;What&amp;#x27;s different is the steps that follow are deterministic,&amp;quot; Devbhandari explained. &amp;quot;There&amp;#x27;s no room for error. There&amp;#x27;s no hallucinations, and the Excel writer that we&amp;#x27;ve connected to the product on the back end converts this analysis into Excel formula, which are live and traceable, so consultants can get that assurance that they can follow along with the maths.&amp;quot;&lt;/p&gt;&lt;p&gt;Whether this hybrid approach delivers on its promise of eliminating hallucinations while maintaining useful AI capabilities will be tested as the platform scales across more complex use cases and client environments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Enterprise security certifications give Ascentra an edge over less prepared competitors&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Selling software to major consulting firms requires clearing an unusually high security bar. These organizations handle sensitive client data across industries, and their vendor security assessments can take months to complete.&lt;/p&gt;&lt;p&gt;Ascentra invested early in obtaining enterprise-grade certifications, a strategic choice that Devbhandari framed as essential table stakes. The company has achieved &lt;a href="https://secureframe.com/blog/soc-2-type-ii"&gt;&lt;u&gt;SOC 2 Type II&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.iso.org/standard/27001"&gt;&lt;u&gt;ISO 27001&lt;/u&gt;&lt;/a&gt; certifications and claims to be under audit for &lt;a href="https://www.iso.org/standard/42001"&gt;&lt;u&gt;ISO 42001&lt;/u&gt;&lt;/a&gt;, an emerging standard for AI management systems.&lt;/p&gt;&lt;p&gt;Data handling policies also reflect the sensitivity of the target market. Client data is deleted within 30 to 45 days, depending on contractual terms, and Ascentra does not use customer data to train its models.&lt;/p&gt;&lt;p&gt;There&amp;#x27;s also an argument that survey data carries somewhat lower sensitivity than other consulting materials. &amp;quot;Survey data is unique in consulting data because it&amp;#x27;s collected during the course of a project, and it is market data,&amp;quot; Devbhandari noted. &amp;quot;You interview people in the market, and you collect a bunch of data in an Excel, as opposed to—you look at Rogo or some of the other finance AI startups—they use client data, so financials, which is confidential and strictly non-public.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Per-project pricing aligns with how consulting firms actually spend money&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Ascentra&amp;#x27;s pricing model departs from the subscription-based approach that dominates enterprise software. The company charges on a per-project basis, a structure Devbhandari said aligns with how consulting firms allocate budgets.&lt;/p&gt;&lt;p&gt;&amp;quot;Project budgets are in consulting set on a per project basis,&amp;quot; he explained. &amp;quot;You&amp;#x27;ll have central budgets which are for things like Microsoft, right, very central things that every team will use all of the time. And then you have project budgets which are for the teams that are using specific resources, teams or products nowadays.&amp;quot;&lt;/p&gt;&lt;p&gt;This approach may ease initial adoption by avoiding the need for central IT procurement approval, but it also introduces revenue unpredictability. The company&amp;#x27;s success will depend on converting project-level usage into broader enterprise relationships—a path Devbhandari suggested is already underway through submitted business cases for enterprise rollouts.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI may not eliminate consulting jobs, but it will fundamentally transform what consultants do&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Perhaps the most interesting tension in Devbhandari&amp;#x27;s vision concerns what AI ultimately means for consulting employment. He pushed back on predictions that AI will eliminate consulting jobs while simultaneously describing an industry on the cusp of fundamental transformation.&lt;/p&gt;&lt;p&gt;&amp;quot;People love to talk about how AI is going to remove the need for consultants, and I disagree,&amp;quot; he said. &amp;quot;Yes, the role will change, but I don&amp;#x27;t think the industry goes away. I think the best solutions will come from people within the industry building products around the work they know.&amp;quot;&lt;/p&gt;&lt;p&gt;Yet he also painted a picture of dramatic change. &amp;quot;At the moment, you have a big intake of graduates who just do—for the most part, you know, they have the strategic work as part of what they do, but they also have a lot of work in Excel and PowerPoint. I think in a few years&amp;#x27; time, we&amp;#x27;ll look back at these times and think, you know, very, very different.&amp;quot;&lt;/p&gt;&lt;p&gt;The honest answer, he acknowledged, is that no one truly knows how this plays out. &amp;quot;I don&amp;#x27;t think even AI leaders truly know what that looks like yet,&amp;quot; he said of whether productivity gains will translate to more work or fewer workers.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ascentra plans to use seed funding to expand its U.S. presence and go-to-market team&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The $2 million will primarily fund Ascentra&amp;#x27;s expansion into the United States, where more than 80 percent of its customers are already based. Devbhandari plans to relocate there personally as the company builds out go-to-market capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;One of the things that we&amp;#x27;ve really noticed is that with consulting being an American industry, and I think America being a great place for innovation and trying new things, we&amp;#x27;ve definitely drawn ourselves to the U.S.,&amp;quot; he said. &amp;quot;American hires are very expensive, and I&amp;#x27;m sure that a lot of the raise will go towards that.&amp;quot;&lt;/p&gt;&lt;p&gt;The seed round represents a bet by NAP on what its co-founder Stefan Walter called an overdue disruption. &amp;quot;While most knowledge work has been reshaped by new technology, consulting has remained stubbornly manual,&amp;quot; Walter said. &amp;quot;AI won&amp;#x27;t replace consultants, but consultants using Ascentra might.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The startup now faces the hard work of converting pilot wins into lasting enterprise contracts&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Ascentra enters 2026 with momentum but no guarantee of success. The company must transform pilot programs at elite firms into sticky enterprise contracts — all while fending off the inevitable well-funded competitors who will flood into the space once the opportunity becomes undeniable. Its deliberately narrow focus on survey analysis provides a defensible beachhead, but expanding into adjacent workflows will require building entirely new products without sacrificing the domain expertise that Devbhandari argues is the company&amp;#x27;s core advantage.&lt;/p&gt;&lt;p&gt;Oliver Thurston, Ascentra&amp;#x27;s co-founder and chief technology officer, who previously led machine learning at Mathison AI, offered a clear-eyed assessment of the challenge. &amp;quot;Consulting workflows are uniquely complex and difficult to build products around,&amp;quot; he said in a statement. &amp;quot;It&amp;#x27;s not surprising the space hasn&amp;#x27;t changed yet. This will change though, and there&amp;#x27;s no doubt that the industry is going to look completely different in five years&amp;#x27; time.&amp;quot;&lt;/p&gt;&lt;p&gt;For now, Ascentra is placing a focused wager: that the consultants who once spent their nights formatting spreadsheets will be the ones who finally bring AI into an industry that has long resisted it. The irony is hard to miss. After years of advising Fortune 500 companies on digital transformation, consulting may finally have to take its own medicine.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;
&lt;/p&gt;&lt;p&gt;While artificial intelligence has stormed into law firms and accounting practices with billion-dollar startups like Harvey leading the charge, the global consulting industry—a $250 billion behemoth—has remained stubbornly analog. A London-based startup founded by former McKinsey consultants is betting $2 million that it can crack open this resistant market, one Excel spreadsheet at a time.&lt;/p&gt;&lt;p&gt;&lt;a href="https://ascentralabs.ai/"&gt;&lt;u&gt;Ascentra Labs&lt;/u&gt;&lt;/a&gt; announced Tuesday that it has closed a $2 million seed round led by &lt;a href="https://www.nap.vc/"&gt;&lt;u&gt;NAP&lt;/u&gt;&lt;/a&gt;, a Berlin-based venture capital firm formerly known as Cavalry Ventures. The funding comes with participation from notable founder-angels including Alan Chang, chief executive of Fuse and former chief revenue officer at Revolut, and Fredrik Hjelm, chief executive of European e-scooter company Voi.&lt;/p&gt;&lt;p&gt;The investment is modest by the standards of enterprise AI — a sector that has seen funding rounds routinely reach into the hundreds of millions. But Ascentra&amp;#x27;s founders argue that their focused approach to a narrow but painful problem could give them an edge in a market where broad AI solutions have repeatedly failed to gain traction.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Consultants spend countless hours on Excel survey analysis that even top firms haven&amp;#x27;t automated&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://ascentralabs.ai/about"&gt;&lt;u&gt;Paritosh Devbhandari&lt;/u&gt;&lt;/a&gt;, Ascentra&amp;#x27;s co-founder and chief executive, spent years at &lt;a href="https://www.mckinsey.com/"&gt;&lt;u&gt;McKinsey &amp;amp; Company&lt;/u&gt;&lt;/a&gt;, including a stint at &lt;a href="https://www.mckinsey.com/capabilities/quantumblack/how-we-help-clients"&gt;&lt;u&gt;QuantumBlack&lt;/u&gt;&lt;/a&gt;, the firm&amp;#x27;s AI and advanced analytics division. He knows intimately the late nights consultants spend wrestling with survey data—the kind of quantitative research that forms the backbone of private equity due diligence.&lt;/p&gt;&lt;p&gt;&amp;quot;Before starting the company, I was working at McKinsey, specifically on the private equity team,&amp;quot; Devbhandari explained in an exclusive interview with VentureBeat. The work, he said, involves analyzing encoded survey responses from customers, suppliers, and market participants during potential acquisitions.&lt;/p&gt;&lt;p&gt;&amp;quot;Consultants typically spend a lot of time doing this in Excel,&amp;quot; he said. &amp;quot;One of the things that surprised me, having worked at a couple of different places, is that the workflow — even at the best firms — really isn&amp;#x27;t that different from some of the boutiques. I always expected there would be some smarter way of doing things, and often there just isn&amp;#x27;t.&amp;quot;&lt;/p&gt;&lt;p&gt;That gap between expectation and reality became the foundation for &lt;a href="https://ascentralabs.ai/"&gt;&lt;u&gt;Ascentra&lt;/u&gt;&lt;/a&gt;. The company&amp;#x27;s platform ingests raw survey data files and outputs formatted Excel workbooks complete with traceable formulas — the kind of deliverable a junior associate would spend hours constructing manually.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI has transformed legal work but consulting presents unique technical challenges that have blocked adoption&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The disparity between AI adoption in law versus consulting raises an obvious question: if the consulting market is so large and the workflows so manual, why hasn&amp;#x27;t venture capital flooded the space the way it has legal tech?&lt;/p&gt;&lt;p&gt;Devbhandari offered a frank assessment. &amp;quot;It&amp;#x27;s not like people haven&amp;#x27;t tried,&amp;quot; he said. &amp;quot;The top of the funnel in our space is crowded. When we speak to our consulting clients, the partners say they get another pitch deck in their LinkedIn inbox or email every week—sometimes several. There are plenty of people trying.&amp;quot;&lt;/p&gt;&lt;p&gt;The barriers, he argued, are structural. Professional services firms move slowly on technology adoption, demanding extensive security credentials and customer references before granting even a pilot opportunity. &amp;quot;I think that&amp;#x27;s where 90% of startups in professional services, writ large, fall down,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;But consulting presents unique technical challenges beyond the sales cycle. Unlike legal work, which largely involves text documents that modern large language models handle well, consulting spans multiple data modalities — PowerPoint presentations, Excel spreadsheets, Word documents — with information that can be tabular, graphical, or textual.&lt;/p&gt;&lt;p&gt;&amp;quot;You can have multiple formats of Excel in itself,&amp;quot; Devbhandari noted. &amp;quot;And that&amp;#x27;s a big contrast to the legal space, where you could have a multi-purpose AI agent, or collection of agents, which can actually do a lot of the tasks that lawyers do day to day. Consulting is the opposite of that.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ascentra&amp;#x27;s private equity focus reflects a calculated bet on repeatable workflows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Ascentra&amp;#x27;s strategy hinges on extreme specificity. Rather than attempting to automate the full spectrum of consulting work, the company focuses exclusively on survey analysis within private equity due diligence — a niche within a niche.&lt;/p&gt;&lt;p&gt;The logic is both technical and commercial. Private equity work tends to be more standardized than other consulting engagements, with similar analyses recurring across deals. That repeatability makes automation feasible. It also positions Ascentra against a less formidable competitive set: even the largest consulting firms, Devbhandari claimed, lack dedicated internal tools for this particular workflow.&lt;/p&gt;&lt;p&gt;&amp;quot;Survey analysis automation is so specific that even the biggest and best firms haven&amp;#x27;t developed anything in-house for it,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;The company claims that three of the world&amp;#x27;s top five consulting firms now use its platform, with early adopters reporting time savings of 60 to 80 percent on active due diligence projects. But there&amp;#x27;s a notable caveat: Ascentra cannot publicly name any of these clients.&lt;/p&gt;&lt;p&gt;&amp;quot;It&amp;#x27;s a very private industry, so at the moment, we can&amp;#x27;t announce any clients publicly,&amp;quot; Devbhandari acknowledged. &amp;quot;What I can say is that we&amp;#x27;re working with three of the top five consulting firms. We&amp;#x27;ve passed pilots at multiple organizations and have submitted business cases for enterprise rollouts.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Eliminating AI hallucinations becomes critical when billion-dollar deals hang in the balance&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For an AI company selling into quantitative workflows, accuracy is existential. Consultants delivering analysis to private equity clients face enormous pressure to be precise—a single error in a financial model can undermine credibility and, potentially, billion-dollar investment decisions.&lt;/p&gt;&lt;p&gt;Devbhandari described this as Ascentra&amp;#x27;s central design challenge. &amp;quot;Consultants require a very, very high degree of fidelity when they&amp;#x27;re doing their analysis,&amp;quot; he said. &amp;quot;So with quantitative data, even if it&amp;#x27;s 95% accurate, they will revert to Excel because they know it, they trust it, and they don&amp;#x27;t want there to be any margin for error.&amp;quot;&lt;/p&gt;&lt;p&gt;Ascentra&amp;#x27;s technical approach attempts to address this by limiting where AI models operate within the workflow. The company uses GPT-based models from OpenAI to interpret and ingest incoming data, but the actual analysis relies on deterministic Python scripts that produce consistent, verifiable outputs.&lt;/p&gt;&lt;p&gt;&amp;quot;What&amp;#x27;s different is the steps that follow are deterministic,&amp;quot; Devbhandari explained. &amp;quot;There&amp;#x27;s no room for error. There&amp;#x27;s no hallucinations, and the Excel writer that we&amp;#x27;ve connected to the product on the back end converts this analysis into Excel formula, which are live and traceable, so consultants can get that assurance that they can follow along with the maths.&amp;quot;&lt;/p&gt;&lt;p&gt;Whether this hybrid approach delivers on its promise of eliminating hallucinations while maintaining useful AI capabilities will be tested as the platform scales across more complex use cases and client environments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Enterprise security certifications give Ascentra an edge over less prepared competitors&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Selling software to major consulting firms requires clearing an unusually high security bar. These organizations handle sensitive client data across industries, and their vendor security assessments can take months to complete.&lt;/p&gt;&lt;p&gt;Ascentra invested early in obtaining enterprise-grade certifications, a strategic choice that Devbhandari framed as essential table stakes. The company has achieved &lt;a href="https://secureframe.com/blog/soc-2-type-ii"&gt;&lt;u&gt;SOC 2 Type II&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.iso.org/standard/27001"&gt;&lt;u&gt;ISO 27001&lt;/u&gt;&lt;/a&gt; certifications and claims to be under audit for &lt;a href="https://www.iso.org/standard/42001"&gt;&lt;u&gt;ISO 42001&lt;/u&gt;&lt;/a&gt;, an emerging standard for AI management systems.&lt;/p&gt;&lt;p&gt;Data handling policies also reflect the sensitivity of the target market. Client data is deleted within 30 to 45 days, depending on contractual terms, and Ascentra does not use customer data to train its models.&lt;/p&gt;&lt;p&gt;There&amp;#x27;s also an argument that survey data carries somewhat lower sensitivity than other consulting materials. &amp;quot;Survey data is unique in consulting data because it&amp;#x27;s collected during the course of a project, and it is market data,&amp;quot; Devbhandari noted. &amp;quot;You interview people in the market, and you collect a bunch of data in an Excel, as opposed to—you look at Rogo or some of the other finance AI startups—they use client data, so financials, which is confidential and strictly non-public.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Per-project pricing aligns with how consulting firms actually spend money&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Ascentra&amp;#x27;s pricing model departs from the subscription-based approach that dominates enterprise software. The company charges on a per-project basis, a structure Devbhandari said aligns with how consulting firms allocate budgets.&lt;/p&gt;&lt;p&gt;&amp;quot;Project budgets are in consulting set on a per project basis,&amp;quot; he explained. &amp;quot;You&amp;#x27;ll have central budgets which are for things like Microsoft, right, very central things that every team will use all of the time. And then you have project budgets which are for the teams that are using specific resources, teams or products nowadays.&amp;quot;&lt;/p&gt;&lt;p&gt;This approach may ease initial adoption by avoiding the need for central IT procurement approval, but it also introduces revenue unpredictability. The company&amp;#x27;s success will depend on converting project-level usage into broader enterprise relationships—a path Devbhandari suggested is already underway through submitted business cases for enterprise rollouts.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI may not eliminate consulting jobs, but it will fundamentally transform what consultants do&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Perhaps the most interesting tension in Devbhandari&amp;#x27;s vision concerns what AI ultimately means for consulting employment. He pushed back on predictions that AI will eliminate consulting jobs while simultaneously describing an industry on the cusp of fundamental transformation.&lt;/p&gt;&lt;p&gt;&amp;quot;People love to talk about how AI is going to remove the need for consultants, and I disagree,&amp;quot; he said. &amp;quot;Yes, the role will change, but I don&amp;#x27;t think the industry goes away. I think the best solutions will come from people within the industry building products around the work they know.&amp;quot;&lt;/p&gt;&lt;p&gt;Yet he also painted a picture of dramatic change. &amp;quot;At the moment, you have a big intake of graduates who just do—for the most part, you know, they have the strategic work as part of what they do, but they also have a lot of work in Excel and PowerPoint. I think in a few years&amp;#x27; time, we&amp;#x27;ll look back at these times and think, you know, very, very different.&amp;quot;&lt;/p&gt;&lt;p&gt;The honest answer, he acknowledged, is that no one truly knows how this plays out. &amp;quot;I don&amp;#x27;t think even AI leaders truly know what that looks like yet,&amp;quot; he said of whether productivity gains will translate to more work or fewer workers.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ascentra plans to use seed funding to expand its U.S. presence and go-to-market team&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The $2 million will primarily fund Ascentra&amp;#x27;s expansion into the United States, where more than 80 percent of its customers are already based. Devbhandari plans to relocate there personally as the company builds out go-to-market capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;One of the things that we&amp;#x27;ve really noticed is that with consulting being an American industry, and I think America being a great place for innovation and trying new things, we&amp;#x27;ve definitely drawn ourselves to the U.S.,&amp;quot; he said. &amp;quot;American hires are very expensive, and I&amp;#x27;m sure that a lot of the raise will go towards that.&amp;quot;&lt;/p&gt;&lt;p&gt;The seed round represents a bet by NAP on what its co-founder Stefan Walter called an overdue disruption. &amp;quot;While most knowledge work has been reshaped by new technology, consulting has remained stubbornly manual,&amp;quot; Walter said. &amp;quot;AI won&amp;#x27;t replace consultants, but consultants using Ascentra might.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The startup now faces the hard work of converting pilot wins into lasting enterprise contracts&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Ascentra enters 2026 with momentum but no guarantee of success. The company must transform pilot programs at elite firms into sticky enterprise contracts — all while fending off the inevitable well-funded competitors who will flood into the space once the opportunity becomes undeniable. Its deliberately narrow focus on survey analysis provides a defensible beachhead, but expanding into adjacent workflows will require building entirely new products without sacrificing the domain expertise that Devbhandari argues is the company&amp;#x27;s core advantage.&lt;/p&gt;&lt;p&gt;Oliver Thurston, Ascentra&amp;#x27;s co-founder and chief technology officer, who previously led machine learning at Mathison AI, offered a clear-eyed assessment of the challenge. &amp;quot;Consulting workflows are uniquely complex and difficult to build products around,&amp;quot; he said in a statement. &amp;quot;It&amp;#x27;s not surprising the space hasn&amp;#x27;t changed yet. This will change though, and there&amp;#x27;s no doubt that the industry is going to look completely different in five years&amp;#x27; time.&amp;quot;&lt;/p&gt;&lt;p&gt;For now, Ascentra is placing a focused wager: that the consultants who once spent their nights formatting spreadsheets will be the ones who finally bring AI into an industry that has long resisted it. The irony is hard to miss. After years of advising Fortune 500 companies on digital transformation, consulting may finally have to take its own medicine.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/ascentra-labs-raises-usd2-million-to-help-consultants-use-ai-instead-of-all</guid><pubDate>Tue, 02 Dec 2025 14:00:00 +0000</pubDate></item><item><title>Mistral launches Mistral 3, a family of open models designed to run on laptops, drones, and edge devices (AI | VentureBeat)</title><link>https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://mistral.ai/"&gt;Mistral AI&lt;/a&gt;, Europe&amp;#x27;s most prominent artificial intelligence startup, is releasing its most ambitious product suite to date: a family of 10 open-source models designed to run everywhere from smartphones and autonomous drones to enterprise cloud systems, marking a major escalation in the company&amp;#x27;s challenge to both U.S. tech giants and surging Chinese competitors.&lt;/p&gt;&lt;p&gt;The &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral 3 family&lt;/a&gt;, launching today, includes a new flagship model called &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral Large 3&lt;/a&gt; and a suite of smaller &amp;quot;&lt;a href="https://mistral.ai/models"&gt;Ministral 3&lt;/a&gt;&amp;quot; models optimized for edge computing applications. All models will be released under the permissive Apache 2.0 license, allowing unrestricted commercial use — a sharp contrast to the closed systems offered by &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://www.google.com/"&gt;Google&lt;/a&gt;, and &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The release is a pointed bet by Mistral that the future of artificial intelligence lies not in building ever-larger proprietary systems, but in offering businesses maximum flexibility to customize and deploy AI tailored to their specific needs, often using smaller models that can run without cloud connectivity.&lt;/p&gt;&lt;p&gt;&amp;quot;The gap between closed and open source is getting smaller, because more and more people are contributing to open source, which is great,&amp;quot; Guillaume Lample, Mistral&amp;#x27;s chief scientist and co-founder, said in an exclusive interview with VentureBeat. &amp;quot;We are catching up fast.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Mistral is choosing flexibility over frontier performance in the AI race&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The strategic calculus behind &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral 3&lt;/a&gt; diverges sharply from recent model releases by industry leaders. While &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://www.google.com/"&gt;Google&lt;/a&gt;, and &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt; have focused recent launches on increasingly capable &amp;quot;agentic&amp;quot; systems — AI that can autonomously execute complex multi-step tasks — Mistral is prioritizing breadth, efficiency, and what Lample calls &amp;quot;distributed intelligence.&amp;quot;&lt;/p&gt;&lt;p&gt;Mistral &lt;a href="https://mistral.ai/models"&gt;Large 3&lt;/a&gt;, the flagship model, employs a &lt;a href="https://huggingface.co/blog/moe"&gt;Mixture of Experts&lt;/a&gt; architecture with 41 billion active parameters drawn from a total pool of 675 billion parameters. The model can process both text and images, handles context windows up to 256,000 tokens, and was trained with particular emphasis on non-English languages — a rarity among frontier AI systems.&lt;/p&gt;&lt;p&gt;&amp;quot;Most AI labs focus on their native language, but Mistral Large 3 was trained on a wide variety of languages, making advanced AI useful for billions who speak different native languages,&amp;quot; the company said in a statement reviewed ahead of the announcement.&lt;/p&gt;&lt;p&gt;But the more significant departure lies in the &lt;a href="https://mistral.ai/models"&gt;Ministral 3&lt;/a&gt; lineup: nine compact models across three sizes (14 billion, 8 billion, and 3 billion parameters) and three variants tailored for different use cases. Each variant serves a distinct purpose: base models for extensive customization, instruction-tuned models for general chat and task completion, and reasoning-optimized models for complex logic requiring step-by-step deliberation.&lt;/p&gt;&lt;p&gt;The smallest Ministral 3 models can run on devices with as little as 4 gigabytes of video memory using 4-bit quantization — making frontier AI capabilities accessible on standard laptops, smartphones, and embedded systems without requiring expensive cloud infrastructure or even internet connectivity. This approach reflects Mistral&amp;#x27;s belief that AI&amp;#x27;s next evolution will be defined not by sheer scale, but by ubiquity: models small enough to run on drones, in vehicles, in robots, and on consumer devices.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How fine-tuned small models beat expensive large models for enterprise customers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Lample&amp;#x27;s comments reveal a business model fundamentally different from that of closed-source competitors. Rather than competing primarily on benchmark performance, Mistral is targeting enterprise customers frustrated by the cost and inflexibility of proprietary systems.&lt;/p&gt;&lt;p&gt;&amp;quot;Sometimes customers say, &amp;#x27;Is there a use case where the best closed-source model isn&amp;#x27;t working?&amp;#x27; If that&amp;#x27;s the case, then they&amp;#x27;re essentially stuck,&amp;quot; Lample explained. &amp;quot;There&amp;#x27;s nothing they can do. It&amp;#x27;s the best model available, and it&amp;#x27;s not working out of the box.&amp;quot;&lt;/p&gt;&lt;p&gt;This is where Mistral&amp;#x27;s approach diverges. When a generic model fails, the company deploys engineering teams to work directly with customers, analyzing specific problems, creating synthetic training data, and fine-tuning smaller models to outperform larger general-purpose systems on narrow tasks.&lt;/p&gt;&lt;p&gt;&amp;quot;In more than 90% of cases, a small model can do the job, especially if it&amp;#x27;s fine-tuned. It doesn&amp;#x27;t have to be a model with hundreds of billions of parameters, just a 14-billion or 24-billion parameter model,&amp;quot; Lample said. &amp;quot;So it&amp;#x27;s not only much cheaper, but also faster, plus you have all the benefits: you don&amp;#x27;t need to worry about privacy, latency, reliability, and so on.&amp;quot;&lt;/p&gt;&lt;p&gt;The economic argument is compelling. Multiple enterprise customers have approached Mistral after building prototypes with expensive closed-source models, only to find deployment costs prohibitive at scale, according to Lample.&lt;/p&gt;&lt;p&gt;&amp;quot;They come back to us a couple of months later because they realize, &amp;#x27;We built this prototype, but it&amp;#x27;s way too slow and way too expensive,&amp;#x27;&amp;quot; he said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Where Mistral 3 fits in the increasingly crowded open-source AI market&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral&amp;#x27;s release comes amid fierce competition on multiple fronts. OpenAI recently released &lt;a href="https://openai.com/index/gpt-5-1-codex-max/"&gt;GPT-5.1&lt;/a&gt; with enhanced agentic capabilities. Google launched &lt;a href="https://blog.google/products/gemini/gemini-3/"&gt;Gemini 3&lt;/a&gt; with improved multimodal understanding. Anthropic released &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;Opus 4.5&lt;/a&gt; on the same day as this interview, with similar agent-focused features.&lt;/p&gt;&lt;p&gt;But Lample argues those comparisons miss the point. &amp;quot;It&amp;#x27;s a little bit behind. But I think what matters is that we are catching up fast,&amp;quot; he acknowledged regarding performance against closed models. &amp;quot;I think we are maybe playing a strategic long game.&amp;quot;&lt;/p&gt;&lt;p&gt;That long game involves a different competitive set: primarily open-source models from Chinese companies like &lt;a href="https://www.deepseek.com/"&gt;DeepSeek&lt;/a&gt; and Alibaba&amp;#x27;s &lt;a href="https://qwen.ai/home"&gt;Qwen&lt;/a&gt; series, which have made remarkable strides in recent months.&lt;/p&gt;&lt;p&gt;Mistral differentiates itself through multilingual capabilities that extend far beyond English or Chinese, multimodal integration handling both text and images in a unified model, and what the company characterizes as superior customization through easier fine-tuning.&lt;/p&gt;&lt;p&gt;&amp;quot;One key difference with the models themselves is that we focused much more on multilinguality,&amp;quot; Lample said. &amp;quot;If you look at all the top models from [Chinese competitors], they&amp;#x27;re all text-only. They have visual models as well, but as separate systems. We wanted to integrate everything into a single model.&amp;quot;&lt;/p&gt;&lt;p&gt;The multilingual emphasis aligns with Mistral&amp;#x27;s broader positioning as a European AI champion focused on digital sovereignty — the principle that organizations and nations should maintain control over their AI infrastructure and data.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Building beyond models: Mistral&amp;#x27;s full-stack enterprise AI platform strategy&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral 3&amp;#x27;s release builds on an increasingly comprehensive enterprise AI platform that extends well beyond model development. The company has assembled a full-stack offering that differentiates it from pure model providers.&lt;/p&gt;&lt;p&gt;Recent product launches include &lt;a href="https://mistral.ai/news/agents-api"&gt;Mistral Agents API&lt;/a&gt;, which combines language models with built-in connectors for code execution, web search, image generation, and persistent memory across conversations; &lt;a href="https://mistral.ai/news/magistral"&gt;Magistral&lt;/a&gt;, the company&amp;#x27;s reasoning model designed for domain-specific, transparent, and multilingual reasoning; and &lt;a href="https://mistral.ai/news/mistral-code"&gt;Mistral Code&lt;/a&gt;, an AI-powered coding assistant bundling models, an in-IDE assistant, and local deployment options with enterprise tooling.&lt;/p&gt;&lt;p&gt;The consumer-facing &lt;a href="https://mistral.ai/products/le-chat"&gt;Le Chat assistant&lt;/a&gt; has been enhanced with Deep Research mode for structured research reports, voice capabilities, and Projects for organizing conversations into context-rich folders. More recently, Le Chat gained a connector directory with 20+ enterprise integrations powered by the Model Context Protocol (MCP), spanning tools like Databricks, Snowflake, GitHub, Atlassian, Asana, and Stripe.&lt;/p&gt;&lt;p&gt;In October, Mistral unveiled &lt;a href="https://mistral.ai/products/ai-studio"&gt;AI Studio&lt;/a&gt;, a production AI platform providing observability, agent runtime, and AI registry capabilities to help enterprises track output changes, monitor usage, run evaluations, and fine-tune models using proprietary data.&lt;/p&gt;&lt;p&gt;Mistral now positions itself as a full-stack, global enterprise AI company, offering not just models but an application-building layer through AI Studio, compute infrastructure, and forward-deployed engineers to help businesses realize return on investment.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why open source AI matters for customization, transparency and sovereignty&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral&amp;#x27;s commitment to open-source development under permissive licenses is both an ideological stance and a competitive strategy in an AI landscape increasingly dominated by closed systems.&lt;/p&gt;&lt;p&gt;Lample elaborated on the practical benefits: &amp;quot;I think something that people don&amp;#x27;t realize — but our customers know this very well — is how much better any model can actually improve if you fine tune it on the task of interest. There&amp;#x27;s a huge gap between a base model and one that&amp;#x27;s fine-tuned for a specific task, and in many cases, it outperforms the closed-source model.&amp;quot;&lt;/p&gt;&lt;p&gt;The approach enables capabilities impossible with closed systems: organizations can fine-tune models on proprietary data that never leaves their infrastructure, customize architectures for specific workflows, and maintain complete transparency into how AI systems make decisions — critical for regulated industries like finance, healthcare, and defense.&lt;/p&gt;&lt;p&gt;This positioning has attracted government and public sector partnerships. The company launched &amp;quot;&lt;a href="https://mistral.ai/news/ai-for-citizens"&gt;AI for Citizens&lt;/a&gt;&amp;quot; in July 2025, an initiative to &amp;quot;help States and public institutions strategically harness AI for their people by transforming public services&amp;quot; and has secured strategic partnerships with France&amp;#x27;s army and job agency, Luxembourg&amp;#x27;s government, and various European public sector organizations.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Mistral&amp;#x27;s transatlantic AI collaboration goes beyond European borders&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While Mistral is frequently characterized as Europe&amp;#x27;s answer to OpenAI, the company views itself as a transatlantic collaboration rather than a purely European venture. The company has teams across both continents, with co-founders spending significant time with customers and partners in the United States, and these models are being trained in partnerships with U.S.-based teams and infrastructure providers.&lt;/p&gt;&lt;p&gt;This transatlantic positioning may prove strategically important as geopolitical tensions around AI development intensify. The recent ASML investment, a &lt;a href="https://www.nytimes.com/2025/09/09/business/asml-mistral-ai-chips-investment.html"&gt;€1.7 billion ($1.5 billion) funding round&lt;/a&gt; led by the Dutch semiconductor equipment manufacturer, signals deepening collaboration across the Western semiconductor and AI value chain at a moment when both Europe and the United States are seeking to reduce dependence on Chinese technology.&lt;/p&gt;&lt;p&gt;Mistral&amp;#x27;s investor base reflects this dynamic: the Series C round included participation from U.S. firms &lt;a href="https://a16z.com/"&gt;Andreessen Horowitz&lt;/a&gt;, &lt;a href="https://www.generalcatalyst.com/"&gt;General Catalyst&lt;/a&gt;, &lt;a href="https://www.lightspeedhq.com/"&gt;Lightspeed&lt;/a&gt;, and &lt;a href="https://www.indexventures.com/"&gt;Index Ventures&lt;/a&gt; alongside European investors like France&amp;#x27;s state-backed Bpifrance and global players like DST Global and Nvidia.&lt;/p&gt;&lt;p&gt;Founded in May 2023 by former Google DeepMind and Meta researchers, Mistral has raised roughly $1.05 billion (€1 billion) in funding. The company was valued at &lt;a href="https://techcrunch.com/2024/06/11/paris-based-ai-startup-mistral-ai-raises-640-million/"&gt;$6 billion&lt;/a&gt; in a June 2024 Series B, then &lt;a href="https://mistral.ai/news/mistral-ai-raises-1-7-b-to-accelerate-technological-progress-with-ai"&gt;more than doubled its valuation&lt;/a&gt; in a September Series C.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Can customization and efficiency beat raw performance in enterprise AI?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral 3 &lt;/a&gt;release crystallizes a fundamental question facing the AI industry: Will enterprises ultimately prioritize the absolute cutting-edge capabilities of proprietary systems, or will they choose open, customizable alternatives that offer greater control, lower costs, and independence from big tech platforms?&lt;/p&gt;&lt;p&gt;Mistral&amp;#x27;s answer is unambiguous. The company is betting that as AI moves from prototype to production, the factors that matter most shift dramatically. Raw benchmark scores matter less than total cost of ownership. Slight performance edges matter less than the ability to fine-tune for specific workflows. Cloud-based convenience matters less than data sovereignty and edge deployment.&lt;/p&gt;&lt;p&gt;It&amp;#x27;s a wager with significant risks. Despite Lample&amp;#x27;s optimism about closing the performance gap, Mistral&amp;#x27;s models still trail the absolute frontier. The company&amp;#x27;s revenue, while growing, reportedly remains modest relative to its nearly $14 billion valuation. And competition intensifies from both well-funded Chinese rivals making remarkable open-source progress and U.S. tech giants increasingly offering their own smaller, more efficient models.&lt;/p&gt;&lt;p&gt;But if Mistral is right — if the future of AI looks less like a handful of cloud-based oracles and more like millions of specialized systems running everywhere from factory floors to smartphones — then the company has positioned itself at the center of that transformation.&lt;/p&gt;&lt;p&gt;The release of &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral 3 &lt;/a&gt;is the most comprehensive expression yet of that vision: 10 models, spanning every size category, optimized for every deployment scenario, available to anyone who wants to build with them.&lt;/p&gt;&lt;p&gt;Whether &amp;quot;distributed intelligence&amp;quot; becomes the industry&amp;#x27;s dominant paradigm or remains a compelling alternative serving a narrower market will determine not just Mistral&amp;#x27;s fate, but the broader question of who controls the AI future — and whether that future will be open.&lt;/p&gt;&lt;p&gt;For now, the race is on. And Mistral is betting it can win not by building the biggest model, but by building everywhere else.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://mistral.ai/"&gt;Mistral AI&lt;/a&gt;, Europe&amp;#x27;s most prominent artificial intelligence startup, is releasing its most ambitious product suite to date: a family of 10 open-source models designed to run everywhere from smartphones and autonomous drones to enterprise cloud systems, marking a major escalation in the company&amp;#x27;s challenge to both U.S. tech giants and surging Chinese competitors.&lt;/p&gt;&lt;p&gt;The &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral 3 family&lt;/a&gt;, launching today, includes a new flagship model called &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral Large 3&lt;/a&gt; and a suite of smaller &amp;quot;&lt;a href="https://mistral.ai/models"&gt;Ministral 3&lt;/a&gt;&amp;quot; models optimized for edge computing applications. All models will be released under the permissive Apache 2.0 license, allowing unrestricted commercial use — a sharp contrast to the closed systems offered by &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://www.google.com/"&gt;Google&lt;/a&gt;, and &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The release is a pointed bet by Mistral that the future of artificial intelligence lies not in building ever-larger proprietary systems, but in offering businesses maximum flexibility to customize and deploy AI tailored to their specific needs, often using smaller models that can run without cloud connectivity.&lt;/p&gt;&lt;p&gt;&amp;quot;The gap between closed and open source is getting smaller, because more and more people are contributing to open source, which is great,&amp;quot; Guillaume Lample, Mistral&amp;#x27;s chief scientist and co-founder, said in an exclusive interview with VentureBeat. &amp;quot;We are catching up fast.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Mistral is choosing flexibility over frontier performance in the AI race&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The strategic calculus behind &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral 3&lt;/a&gt; diverges sharply from recent model releases by industry leaders. While &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://www.google.com/"&gt;Google&lt;/a&gt;, and &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt; have focused recent launches on increasingly capable &amp;quot;agentic&amp;quot; systems — AI that can autonomously execute complex multi-step tasks — Mistral is prioritizing breadth, efficiency, and what Lample calls &amp;quot;distributed intelligence.&amp;quot;&lt;/p&gt;&lt;p&gt;Mistral &lt;a href="https://mistral.ai/models"&gt;Large 3&lt;/a&gt;, the flagship model, employs a &lt;a href="https://huggingface.co/blog/moe"&gt;Mixture of Experts&lt;/a&gt; architecture with 41 billion active parameters drawn from a total pool of 675 billion parameters. The model can process both text and images, handles context windows up to 256,000 tokens, and was trained with particular emphasis on non-English languages — a rarity among frontier AI systems.&lt;/p&gt;&lt;p&gt;&amp;quot;Most AI labs focus on their native language, but Mistral Large 3 was trained on a wide variety of languages, making advanced AI useful for billions who speak different native languages,&amp;quot; the company said in a statement reviewed ahead of the announcement.&lt;/p&gt;&lt;p&gt;But the more significant departure lies in the &lt;a href="https://mistral.ai/models"&gt;Ministral 3&lt;/a&gt; lineup: nine compact models across three sizes (14 billion, 8 billion, and 3 billion parameters) and three variants tailored for different use cases. Each variant serves a distinct purpose: base models for extensive customization, instruction-tuned models for general chat and task completion, and reasoning-optimized models for complex logic requiring step-by-step deliberation.&lt;/p&gt;&lt;p&gt;The smallest Ministral 3 models can run on devices with as little as 4 gigabytes of video memory using 4-bit quantization — making frontier AI capabilities accessible on standard laptops, smartphones, and embedded systems without requiring expensive cloud infrastructure or even internet connectivity. This approach reflects Mistral&amp;#x27;s belief that AI&amp;#x27;s next evolution will be defined not by sheer scale, but by ubiquity: models small enough to run on drones, in vehicles, in robots, and on consumer devices.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How fine-tuned small models beat expensive large models for enterprise customers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Lample&amp;#x27;s comments reveal a business model fundamentally different from that of closed-source competitors. Rather than competing primarily on benchmark performance, Mistral is targeting enterprise customers frustrated by the cost and inflexibility of proprietary systems.&lt;/p&gt;&lt;p&gt;&amp;quot;Sometimes customers say, &amp;#x27;Is there a use case where the best closed-source model isn&amp;#x27;t working?&amp;#x27; If that&amp;#x27;s the case, then they&amp;#x27;re essentially stuck,&amp;quot; Lample explained. &amp;quot;There&amp;#x27;s nothing they can do. It&amp;#x27;s the best model available, and it&amp;#x27;s not working out of the box.&amp;quot;&lt;/p&gt;&lt;p&gt;This is where Mistral&amp;#x27;s approach diverges. When a generic model fails, the company deploys engineering teams to work directly with customers, analyzing specific problems, creating synthetic training data, and fine-tuning smaller models to outperform larger general-purpose systems on narrow tasks.&lt;/p&gt;&lt;p&gt;&amp;quot;In more than 90% of cases, a small model can do the job, especially if it&amp;#x27;s fine-tuned. It doesn&amp;#x27;t have to be a model with hundreds of billions of parameters, just a 14-billion or 24-billion parameter model,&amp;quot; Lample said. &amp;quot;So it&amp;#x27;s not only much cheaper, but also faster, plus you have all the benefits: you don&amp;#x27;t need to worry about privacy, latency, reliability, and so on.&amp;quot;&lt;/p&gt;&lt;p&gt;The economic argument is compelling. Multiple enterprise customers have approached Mistral after building prototypes with expensive closed-source models, only to find deployment costs prohibitive at scale, according to Lample.&lt;/p&gt;&lt;p&gt;&amp;quot;They come back to us a couple of months later because they realize, &amp;#x27;We built this prototype, but it&amp;#x27;s way too slow and way too expensive,&amp;#x27;&amp;quot; he said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Where Mistral 3 fits in the increasingly crowded open-source AI market&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral&amp;#x27;s release comes amid fierce competition on multiple fronts. OpenAI recently released &lt;a href="https://openai.com/index/gpt-5-1-codex-max/"&gt;GPT-5.1&lt;/a&gt; with enhanced agentic capabilities. Google launched &lt;a href="https://blog.google/products/gemini/gemini-3/"&gt;Gemini 3&lt;/a&gt; with improved multimodal understanding. Anthropic released &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;Opus 4.5&lt;/a&gt; on the same day as this interview, with similar agent-focused features.&lt;/p&gt;&lt;p&gt;But Lample argues those comparisons miss the point. &amp;quot;It&amp;#x27;s a little bit behind. But I think what matters is that we are catching up fast,&amp;quot; he acknowledged regarding performance against closed models. &amp;quot;I think we are maybe playing a strategic long game.&amp;quot;&lt;/p&gt;&lt;p&gt;That long game involves a different competitive set: primarily open-source models from Chinese companies like &lt;a href="https://www.deepseek.com/"&gt;DeepSeek&lt;/a&gt; and Alibaba&amp;#x27;s &lt;a href="https://qwen.ai/home"&gt;Qwen&lt;/a&gt; series, which have made remarkable strides in recent months.&lt;/p&gt;&lt;p&gt;Mistral differentiates itself through multilingual capabilities that extend far beyond English or Chinese, multimodal integration handling both text and images in a unified model, and what the company characterizes as superior customization through easier fine-tuning.&lt;/p&gt;&lt;p&gt;&amp;quot;One key difference with the models themselves is that we focused much more on multilinguality,&amp;quot; Lample said. &amp;quot;If you look at all the top models from [Chinese competitors], they&amp;#x27;re all text-only. They have visual models as well, but as separate systems. We wanted to integrate everything into a single model.&amp;quot;&lt;/p&gt;&lt;p&gt;The multilingual emphasis aligns with Mistral&amp;#x27;s broader positioning as a European AI champion focused on digital sovereignty — the principle that organizations and nations should maintain control over their AI infrastructure and data.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Building beyond models: Mistral&amp;#x27;s full-stack enterprise AI platform strategy&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral 3&amp;#x27;s release builds on an increasingly comprehensive enterprise AI platform that extends well beyond model development. The company has assembled a full-stack offering that differentiates it from pure model providers.&lt;/p&gt;&lt;p&gt;Recent product launches include &lt;a href="https://mistral.ai/news/agents-api"&gt;Mistral Agents API&lt;/a&gt;, which combines language models with built-in connectors for code execution, web search, image generation, and persistent memory across conversations; &lt;a href="https://mistral.ai/news/magistral"&gt;Magistral&lt;/a&gt;, the company&amp;#x27;s reasoning model designed for domain-specific, transparent, and multilingual reasoning; and &lt;a href="https://mistral.ai/news/mistral-code"&gt;Mistral Code&lt;/a&gt;, an AI-powered coding assistant bundling models, an in-IDE assistant, and local deployment options with enterprise tooling.&lt;/p&gt;&lt;p&gt;The consumer-facing &lt;a href="https://mistral.ai/products/le-chat"&gt;Le Chat assistant&lt;/a&gt; has been enhanced with Deep Research mode for structured research reports, voice capabilities, and Projects for organizing conversations into context-rich folders. More recently, Le Chat gained a connector directory with 20+ enterprise integrations powered by the Model Context Protocol (MCP), spanning tools like Databricks, Snowflake, GitHub, Atlassian, Asana, and Stripe.&lt;/p&gt;&lt;p&gt;In October, Mistral unveiled &lt;a href="https://mistral.ai/products/ai-studio"&gt;AI Studio&lt;/a&gt;, a production AI platform providing observability, agent runtime, and AI registry capabilities to help enterprises track output changes, monitor usage, run evaluations, and fine-tune models using proprietary data.&lt;/p&gt;&lt;p&gt;Mistral now positions itself as a full-stack, global enterprise AI company, offering not just models but an application-building layer through AI Studio, compute infrastructure, and forward-deployed engineers to help businesses realize return on investment.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why open source AI matters for customization, transparency and sovereignty&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral&amp;#x27;s commitment to open-source development under permissive licenses is both an ideological stance and a competitive strategy in an AI landscape increasingly dominated by closed systems.&lt;/p&gt;&lt;p&gt;Lample elaborated on the practical benefits: &amp;quot;I think something that people don&amp;#x27;t realize — but our customers know this very well — is how much better any model can actually improve if you fine tune it on the task of interest. There&amp;#x27;s a huge gap between a base model and one that&amp;#x27;s fine-tuned for a specific task, and in many cases, it outperforms the closed-source model.&amp;quot;&lt;/p&gt;&lt;p&gt;The approach enables capabilities impossible with closed systems: organizations can fine-tune models on proprietary data that never leaves their infrastructure, customize architectures for specific workflows, and maintain complete transparency into how AI systems make decisions — critical for regulated industries like finance, healthcare, and defense.&lt;/p&gt;&lt;p&gt;This positioning has attracted government and public sector partnerships. The company launched &amp;quot;&lt;a href="https://mistral.ai/news/ai-for-citizens"&gt;AI for Citizens&lt;/a&gt;&amp;quot; in July 2025, an initiative to &amp;quot;help States and public institutions strategically harness AI for their people by transforming public services&amp;quot; and has secured strategic partnerships with France&amp;#x27;s army and job agency, Luxembourg&amp;#x27;s government, and various European public sector organizations.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Mistral&amp;#x27;s transatlantic AI collaboration goes beyond European borders&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While Mistral is frequently characterized as Europe&amp;#x27;s answer to OpenAI, the company views itself as a transatlantic collaboration rather than a purely European venture. The company has teams across both continents, with co-founders spending significant time with customers and partners in the United States, and these models are being trained in partnerships with U.S.-based teams and infrastructure providers.&lt;/p&gt;&lt;p&gt;This transatlantic positioning may prove strategically important as geopolitical tensions around AI development intensify. The recent ASML investment, a &lt;a href="https://www.nytimes.com/2025/09/09/business/asml-mistral-ai-chips-investment.html"&gt;€1.7 billion ($1.5 billion) funding round&lt;/a&gt; led by the Dutch semiconductor equipment manufacturer, signals deepening collaboration across the Western semiconductor and AI value chain at a moment when both Europe and the United States are seeking to reduce dependence on Chinese technology.&lt;/p&gt;&lt;p&gt;Mistral&amp;#x27;s investor base reflects this dynamic: the Series C round included participation from U.S. firms &lt;a href="https://a16z.com/"&gt;Andreessen Horowitz&lt;/a&gt;, &lt;a href="https://www.generalcatalyst.com/"&gt;General Catalyst&lt;/a&gt;, &lt;a href="https://www.lightspeedhq.com/"&gt;Lightspeed&lt;/a&gt;, and &lt;a href="https://www.indexventures.com/"&gt;Index Ventures&lt;/a&gt; alongside European investors like France&amp;#x27;s state-backed Bpifrance and global players like DST Global and Nvidia.&lt;/p&gt;&lt;p&gt;Founded in May 2023 by former Google DeepMind and Meta researchers, Mistral has raised roughly $1.05 billion (€1 billion) in funding. The company was valued at &lt;a href="https://techcrunch.com/2024/06/11/paris-based-ai-startup-mistral-ai-raises-640-million/"&gt;$6 billion&lt;/a&gt; in a June 2024 Series B, then &lt;a href="https://mistral.ai/news/mistral-ai-raises-1-7-b-to-accelerate-technological-progress-with-ai"&gt;more than doubled its valuation&lt;/a&gt; in a September Series C.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Can customization and efficiency beat raw performance in enterprise AI?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral 3 &lt;/a&gt;release crystallizes a fundamental question facing the AI industry: Will enterprises ultimately prioritize the absolute cutting-edge capabilities of proprietary systems, or will they choose open, customizable alternatives that offer greater control, lower costs, and independence from big tech platforms?&lt;/p&gt;&lt;p&gt;Mistral&amp;#x27;s answer is unambiguous. The company is betting that as AI moves from prototype to production, the factors that matter most shift dramatically. Raw benchmark scores matter less than total cost of ownership. Slight performance edges matter less than the ability to fine-tune for specific workflows. Cloud-based convenience matters less than data sovereignty and edge deployment.&lt;/p&gt;&lt;p&gt;It&amp;#x27;s a wager with significant risks. Despite Lample&amp;#x27;s optimism about closing the performance gap, Mistral&amp;#x27;s models still trail the absolute frontier. The company&amp;#x27;s revenue, while growing, reportedly remains modest relative to its nearly $14 billion valuation. And competition intensifies from both well-funded Chinese rivals making remarkable open-source progress and U.S. tech giants increasingly offering their own smaller, more efficient models.&lt;/p&gt;&lt;p&gt;But if Mistral is right — if the future of AI looks less like a handful of cloud-based oracles and more like millions of specialized systems running everywhere from factory floors to smartphones — then the company has positioned itself at the center of that transformation.&lt;/p&gt;&lt;p&gt;The release of &lt;a href="https://mistral.ai/news/mistral-3"&gt;Mistral 3 &lt;/a&gt;is the most comprehensive expression yet of that vision: 10 models, spanning every size category, optimized for every deployment scenario, available to anyone who wants to build with them.&lt;/p&gt;&lt;p&gt;Whether &amp;quot;distributed intelligence&amp;quot; becomes the industry&amp;#x27;s dominant paradigm or remains a compelling alternative serving a narrower market will determine not just Mistral&amp;#x27;s fate, but the broader question of who controls the AI future — and whether that future will be open.&lt;/p&gt;&lt;p&gt;For now, the race is on. And Mistral is betting it can win not by building the biggest model, but by building everywhere else.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on</guid><pubDate>Tue, 02 Dec 2025 15:00:00 +0000</pubDate></item><item><title>Amazon releases an impressive new AI chip and teases an Nvidia-friendly roadmap (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/01/GettyImages-1136663877.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services, which has been building its own AI training chips for years now, just introduced a new version known as Trainium3 that comes with some impressive specs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The cloud provider, which made the announcement Tuesday at AWS re:Invent 2025, also teased the next product on its AI training product roadmap: Trainium4, which is already in the works and will be able to work with Nvidia’s chips.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS used its annual tech conference to formally launch Trainium3 UltraServer, a system powered by the company’s state-of-the art, 3 nanometer Trainium3 chip, as well as its homegrown networking tech.&amp;nbsp;As you might expect, the third-generation chip and system offer big bumps in performance for AI training and inference over the second-generation chip, according to AWS.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS says the system is more than 4x faster, with 4x more memory, not just for training, but for delivering AI apps at peak demand. Additionally, thousands of UltraServers can be linked together to provide an app with up to 1 million Trainium3 chips — 10x the previous generation. Each UltraServer can host 144 chips, according to the company.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps more importantly, AWS says the chips and systems are also 40% more energy efficient than the previous generation.&amp;nbsp;While the world races to build bigger data centers powered by astronomical gigawatts of electricity, data center giant AWS is trying to make systems that drink less, not more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It is, obviously, in AWS’s direct interests to do so. But in its classic, Amazon cost-conscious way, it promises that these systems save its AI cloud customers money, too.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS customers like Anthropic (of which Amazon is also an investor), Japan’s LLM Karakuri, SplashMusic, and Decart have already been using the third-gen chip and system and significantly cut their inference costs, Amazon said.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also presented a bit of a roadmap for the next chip, Trainium4, which is already in development. AWS promised the chip will provide another big step up in performance and support Nvidia’s NVLink Fusion high-speed chip interconnect technology.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This means the AWS Trainium4-powered systems will be able to interoperate and extend their performance with Nvidia GPUs while still using Amazon’s homegrown, lower-cost server rack technology.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting, too, that Nvidia’s CUDA (Compute Unified Device Architecture) has become the de facto standard that all the major AI apps are built to support. The Trainium4-powered systems may make it easier to woo big AI apps built with Nvidia GPUs in mind to Amazon’s cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon did not announce a timeline for Trainium4. If the company follows previous rollout timelines, we’ll likely hear more about Trainium4 at next year’s conference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual enterprise tech event here.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flags&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/01/GettyImages-1136663877.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services, which has been building its own AI training chips for years now, just introduced a new version known as Trainium3 that comes with some impressive specs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The cloud provider, which made the announcement Tuesday at AWS re:Invent 2025, also teased the next product on its AI training product roadmap: Trainium4, which is already in the works and will be able to work with Nvidia’s chips.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS used its annual tech conference to formally launch Trainium3 UltraServer, a system powered by the company’s state-of-the art, 3 nanometer Trainium3 chip, as well as its homegrown networking tech.&amp;nbsp;As you might expect, the third-generation chip and system offer big bumps in performance for AI training and inference over the second-generation chip, according to AWS.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS says the system is more than 4x faster, with 4x more memory, not just for training, but for delivering AI apps at peak demand. Additionally, thousands of UltraServers can be linked together to provide an app with up to 1 million Trainium3 chips — 10x the previous generation. Each UltraServer can host 144 chips, according to the company.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps more importantly, AWS says the chips and systems are also 40% more energy efficient than the previous generation.&amp;nbsp;While the world races to build bigger data centers powered by astronomical gigawatts of electricity, data center giant AWS is trying to make systems that drink less, not more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It is, obviously, in AWS’s direct interests to do so. But in its classic, Amazon cost-conscious way, it promises that these systems save its AI cloud customers money, too.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS customers like Anthropic (of which Amazon is also an investor), Japan’s LLM Karakuri, SplashMusic, and Decart have already been using the third-gen chip and system and significantly cut their inference costs, Amazon said.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also presented a bit of a roadmap for the next chip, Trainium4, which is already in development. AWS promised the chip will provide another big step up in performance and support Nvidia’s NVLink Fusion high-speed chip interconnect technology.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This means the AWS Trainium4-powered systems will be able to interoperate and extend their performance with Nvidia GPUs while still using Amazon’s homegrown, lower-cost server rack technology.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting, too, that Nvidia’s CUDA (Compute Unified Device Architecture) has become the de facto standard that all the major AI apps are built to support. The Trainium4-powered systems may make it easier to woo big AI apps built with Nvidia GPUs in mind to Amazon’s cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Amazon did not announce a timeline for Trainium4. If the company follows previous rollout timelines, we’ll likely hear more about Trainium4 at next year’s conference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual enterprise tech event here.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flags&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/amazon-releases-an-impressive-new-ai-chip-and-teases-a-nvidia-friendly-roadmap/</guid><pubDate>Tue, 02 Dec 2025 16:00:00 +0000</pubDate></item><item><title>NVIDIA and AWS Expand Full-Stack Partnership, Providing the Secure, High-Performance Compute Platform Vital for Future Innovation (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/nvidia-aws-lockup-corp-blog-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At AWS re:Invent, NVIDIA and Amazon Web Services expanded their strategic collaboration with new technology integrations across interconnect technology, cloud infrastructure, open models and physical AI.&lt;/p&gt;
&lt;p&gt;As part of this expansion, AWS will support NVIDIA NVLink Fusion — a platform for custom AI infrastructure — for deploying its custom-designed silicon, including next-generation Trainium4 chips for inference and agentic AI model training, Graviton CPUs for a broad range of workloads and the Nitro System virtualization infrastructure.&lt;/p&gt;
&lt;p&gt;Using NVIDIA NVLink Fusion, AWS will combine NVIDIA NVLink scale-up interconnect and the NVIDIA MGX rack architecture with AWS custom silicon to increase performance and accelerate time to market for its next-generation cloud-scale AI capabilities.&lt;/p&gt;
&lt;p&gt;AWS is designing Trainium4 to integrate with NVLink and NVIDIA MGX, the first of a multigenerational collaboration between NVIDIA and AWS for NVLink Fusion.&lt;/p&gt;
&lt;p&gt;AWS has already deployed MGX racks at scale with NVIDIA GPUs. Integrating NVLink Fusion will allow AWS to further simplify deployment and systems management across its platforms.&lt;/p&gt;
&lt;p&gt;AWS can also harness the NVLink Fusion supplier ecosystem, which provides all the components required for full rack-scale deployment, from the rack and chassis, to power-delivery and cooling systems.&lt;/p&gt;
&lt;p&gt;By supporting AWS’s Elastic Fabric Adapter and Nitro System, the NVIDIA Vera Rubin architecture on AWS will give customers robust networking choices while maintaining full compatibility with AWS’s cloud infrastructure and accelerating new AI service rollout.&lt;/p&gt;
&lt;p&gt;“GPU compute demand is skyrocketing — more compute makes smarter AI, smarter AI drives broader use and broader use creates demand for even more compute. The virtuous cycle of AI has arrived,” said Jensen Huang, founder and CEO of NVIDIA. “With NVIDIA NVLink Fusion coming to AWS Trainium4, we’re unifying our scale-up architecture with AWS’s custom silicon to build a new generation of accelerated platforms. Together, NVIDIA and AWS are creating the compute fabric for the AI industrial revolution — bringing advanced AI to every company, in every country, and accelerating the world’s path to intelligence.”&lt;/p&gt;
&lt;p&gt;“AWS and NVIDIA have worked side by side for more than 15 years, and today marks a new milestone in that journey,” said Matt Garman, CEO of AWS. “With NVIDIA, we’re advancing our large-scale AI infrastructure to deliver customers the highest performance, efficiency and scalability. The upcoming support of NVIDIA NVLink Fusion in AWS Trainium4, Graviton and the Nitro System will bring new capabilities to customers so they can innovate faster than ever before.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Convergence of Scale and Sovereignty&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AWS has expanded its accelerated computing portfolio with the NVIDIA Blackwell architecture, including NVIDIA HGX B300 and NVIDIA GB300 NVL72 GPUs, giving customers immediate access to the industry’s most advanced GPUs for training and inference. Availability of NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, designed for visual applications, on AWS is expected in the coming weeks.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&lt;/b&gt;These GPUs form part of the AWS infrastructure backbone powering AWS AI Factories, a new AI cloud offering that will provide customers around the world with the dedicated infrastructure they need to harness advanced AI services and capabilities in their own data centers, operated by AWS, while also letting customers maintain control of their data and comply with local regulations.&lt;/p&gt;
&lt;p&gt;NVIDIA and AWS are committing to deploy sovereign AI clouds globally and bring the best of AI innovation to the world. With the launch of AWS AI Factories, the companies are providing secure, sovereign AI infrastructure to deliver unprecedented computing capabilities for organizations around the world while meeting increasingly rigorous sovereign AI requirements.&lt;/p&gt;
&lt;p&gt;For public sector organizations, AWS AI Factories will transform the federal supercomputing and AI landscape. AWS AI Factories customers will be able to seamlessly integrate AWS’s industry-leading cloud infrastructure and services — known for its reliability, security and scalability — with NVIDIA Blackwell GPUs and the full-stack NVIDIA accelerated computing platform, including NVIDIA Spectrum-X Ethernet switches.&lt;/p&gt;
&lt;p&gt;The unified architecture will ensure customers can access advanced AI services and capabilities, as well as train and deploy massive models, while maintaining absolute control of proprietary data and full compliance with local regulatory frameworks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Nemotron Integration With Amazon Bedrock Expands Software Optimizations&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Beyond hardware, the partnership expands integration of NVIDIA’s software stack with the AWS AI ecosystem. NVIDIA Nemotron open models are now integrated with Amazon Bedrock, enabling customers to build generative AI applications and agents at production scale. Developers can access Nemotron Nano 2 and Nemotron Nano 2 VL to build specialized agentic AI applications that process text, code, images and video with high efficiency and accuracy.&lt;/p&gt;
&lt;p&gt;The integration makes high-performance, open NVIDIA models instantly accessible via Amazon Bedrock’s serverless platform where customers can rely on proven scalability and zero infrastructure management. Industry leaders CrowdStrike and BridgeWise are the first to use the service to deploy specialized AI agents.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Software on AWS Simplifies Developer Experience&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA and AWS are also co-engineering at the software layer to accelerate the data backbone of every enterprise. Amazon OpenSearch Service now offers serverless GPU acceleration for vector index building, powered by NVIDIA cuVS, an open-source library for GPU-accelerated vector search and data clustering. This milestone represents a fundamental shift to using GPUs for unstructured data processing, with early adopters seeing up to 10x faster vector indexing at a quarter of the cost.&lt;/p&gt;
&lt;p&gt;These dramatic gains reduce search latency, accelerate writes and unlock faster productivity for dynamic AI techniques like retrieval-augmented generation by delivering the right amount of GPU power precisely when it’s needed. AWS is the first major cloud provider to offer serverless vector indexing with NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;Production-ready AI agents require performance visibility, optimization and scalable infrastructure. By combining Strands Agents for agent development and orchestration, the NVIDIA NeMo Agent Toolkit for deep profiling and performance tuning, and Amazon Bedrock AgentCore for secure, scalable agent infrastructure, organizations can empower developers with a complete, predictable path from prototype to production.&lt;/p&gt;
&lt;p&gt;This expanded support builds on AWS’s existing integrations with NVIDIA technologies — including NVIDIA NIM microservices and frameworks like NVIDIA Riva and NVIDIA BioNeMo, as well as model development tools integrated with Amazon SageMaker and Amazon Bedrock — that enable organizations to deploy agentic AI, speech AI and scientific applications faster than ever.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Accelerating Physical AI With AWS&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Developing physical AI demands high-quality and diverse datasets for training robot models, as well as frameworks for testing and validation in simulation before real-world deployment.&lt;/p&gt;
&lt;p&gt;NVIDIA Cosmos world foundation models (WFMs) are now available as NVIDIA NIM microservices on Amazon EKS, enabling real-time robotics control and simulation workloads with seamless reliability and cloud-native efficiency. For batch-based tasks and offline workloads such as large-scale synthetic data generation, Cosmos WFMs are also available on AWS Batch as containers.&lt;/p&gt;
&lt;p&gt;Cosmos-generated world states can then be used to train and validate robots using open-source simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab.&lt;/p&gt;
&lt;p&gt;Leading robotics companies such as Agility Robotics, Agile Robots, ANYbotics, Diligent Robotics, Dyna Robotics, Field AI, Haply Robotics, Lightwheel, RIVR and Skild AI are using the NVIDIA Isaac platform with AWS for use cases ranging from collecting, storing and processing robot-generated data to training and simulation for scaling robotics development.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Sustained Collaboration&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Underscoring years of continued collaboration, NVIDIA earned the AWS Global GenAI Infrastructure and Data Partner of the Year award, which recognizes top technology partners with the Generative AI Competency that support vector embeddings, data storage and management or synthetic data generation in multiple types and formats.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about NVIDIA and AWS’s collaboration and join sessions at &lt;/i&gt;&lt;i&gt;AWS re:Invent&lt;/i&gt;&lt;i&gt;, running through Friday, Dec. 5, in Las Vegas.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/nvidia-aws-lockup-corp-blog-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At AWS re:Invent, NVIDIA and Amazon Web Services expanded their strategic collaboration with new technology integrations across interconnect technology, cloud infrastructure, open models and physical AI.&lt;/p&gt;
&lt;p&gt;As part of this expansion, AWS will support NVIDIA NVLink Fusion — a platform for custom AI infrastructure — for deploying its custom-designed silicon, including next-generation Trainium4 chips for inference and agentic AI model training, Graviton CPUs for a broad range of workloads and the Nitro System virtualization infrastructure.&lt;/p&gt;
&lt;p&gt;Using NVIDIA NVLink Fusion, AWS will combine NVIDIA NVLink scale-up interconnect and the NVIDIA MGX rack architecture with AWS custom silicon to increase performance and accelerate time to market for its next-generation cloud-scale AI capabilities.&lt;/p&gt;
&lt;p&gt;AWS is designing Trainium4 to integrate with NVLink and NVIDIA MGX, the first of a multigenerational collaboration between NVIDIA and AWS for NVLink Fusion.&lt;/p&gt;
&lt;p&gt;AWS has already deployed MGX racks at scale with NVIDIA GPUs. Integrating NVLink Fusion will allow AWS to further simplify deployment and systems management across its platforms.&lt;/p&gt;
&lt;p&gt;AWS can also harness the NVLink Fusion supplier ecosystem, which provides all the components required for full rack-scale deployment, from the rack and chassis, to power-delivery and cooling systems.&lt;/p&gt;
&lt;p&gt;By supporting AWS’s Elastic Fabric Adapter and Nitro System, the NVIDIA Vera Rubin architecture on AWS will give customers robust networking choices while maintaining full compatibility with AWS’s cloud infrastructure and accelerating new AI service rollout.&lt;/p&gt;
&lt;p&gt;“GPU compute demand is skyrocketing — more compute makes smarter AI, smarter AI drives broader use and broader use creates demand for even more compute. The virtuous cycle of AI has arrived,” said Jensen Huang, founder and CEO of NVIDIA. “With NVIDIA NVLink Fusion coming to AWS Trainium4, we’re unifying our scale-up architecture with AWS’s custom silicon to build a new generation of accelerated platforms. Together, NVIDIA and AWS are creating the compute fabric for the AI industrial revolution — bringing advanced AI to every company, in every country, and accelerating the world’s path to intelligence.”&lt;/p&gt;
&lt;p&gt;“AWS and NVIDIA have worked side by side for more than 15 years, and today marks a new milestone in that journey,” said Matt Garman, CEO of AWS. “With NVIDIA, we’re advancing our large-scale AI infrastructure to deliver customers the highest performance, efficiency and scalability. The upcoming support of NVIDIA NVLink Fusion in AWS Trainium4, Graviton and the Nitro System will bring new capabilities to customers so they can innovate faster than ever before.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Convergence of Scale and Sovereignty&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AWS has expanded its accelerated computing portfolio with the NVIDIA Blackwell architecture, including NVIDIA HGX B300 and NVIDIA GB300 NVL72 GPUs, giving customers immediate access to the industry’s most advanced GPUs for training and inference. Availability of NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, designed for visual applications, on AWS is expected in the coming weeks.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&lt;/b&gt;These GPUs form part of the AWS infrastructure backbone powering AWS AI Factories, a new AI cloud offering that will provide customers around the world with the dedicated infrastructure they need to harness advanced AI services and capabilities in their own data centers, operated by AWS, while also letting customers maintain control of their data and comply with local regulations.&lt;/p&gt;
&lt;p&gt;NVIDIA and AWS are committing to deploy sovereign AI clouds globally and bring the best of AI innovation to the world. With the launch of AWS AI Factories, the companies are providing secure, sovereign AI infrastructure to deliver unprecedented computing capabilities for organizations around the world while meeting increasingly rigorous sovereign AI requirements.&lt;/p&gt;
&lt;p&gt;For public sector organizations, AWS AI Factories will transform the federal supercomputing and AI landscape. AWS AI Factories customers will be able to seamlessly integrate AWS’s industry-leading cloud infrastructure and services — known for its reliability, security and scalability — with NVIDIA Blackwell GPUs and the full-stack NVIDIA accelerated computing platform, including NVIDIA Spectrum-X Ethernet switches.&lt;/p&gt;
&lt;p&gt;The unified architecture will ensure customers can access advanced AI services and capabilities, as well as train and deploy massive models, while maintaining absolute control of proprietary data and full compliance with local regulatory frameworks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Nemotron Integration With Amazon Bedrock Expands Software Optimizations&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Beyond hardware, the partnership expands integration of NVIDIA’s software stack with the AWS AI ecosystem. NVIDIA Nemotron open models are now integrated with Amazon Bedrock, enabling customers to build generative AI applications and agents at production scale. Developers can access Nemotron Nano 2 and Nemotron Nano 2 VL to build specialized agentic AI applications that process text, code, images and video with high efficiency and accuracy.&lt;/p&gt;
&lt;p&gt;The integration makes high-performance, open NVIDIA models instantly accessible via Amazon Bedrock’s serverless platform where customers can rely on proven scalability and zero infrastructure management. Industry leaders CrowdStrike and BridgeWise are the first to use the service to deploy specialized AI agents.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Software on AWS Simplifies Developer Experience&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA and AWS are also co-engineering at the software layer to accelerate the data backbone of every enterprise. Amazon OpenSearch Service now offers serverless GPU acceleration for vector index building, powered by NVIDIA cuVS, an open-source library for GPU-accelerated vector search and data clustering. This milestone represents a fundamental shift to using GPUs for unstructured data processing, with early adopters seeing up to 10x faster vector indexing at a quarter of the cost.&lt;/p&gt;
&lt;p&gt;These dramatic gains reduce search latency, accelerate writes and unlock faster productivity for dynamic AI techniques like retrieval-augmented generation by delivering the right amount of GPU power precisely when it’s needed. AWS is the first major cloud provider to offer serverless vector indexing with NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;Production-ready AI agents require performance visibility, optimization and scalable infrastructure. By combining Strands Agents for agent development and orchestration, the NVIDIA NeMo Agent Toolkit for deep profiling and performance tuning, and Amazon Bedrock AgentCore for secure, scalable agent infrastructure, organizations can empower developers with a complete, predictable path from prototype to production.&lt;/p&gt;
&lt;p&gt;This expanded support builds on AWS’s existing integrations with NVIDIA technologies — including NVIDIA NIM microservices and frameworks like NVIDIA Riva and NVIDIA BioNeMo, as well as model development tools integrated with Amazon SageMaker and Amazon Bedrock — that enable organizations to deploy agentic AI, speech AI and scientific applications faster than ever.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Accelerating Physical AI With AWS&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Developing physical AI demands high-quality and diverse datasets for training robot models, as well as frameworks for testing and validation in simulation before real-world deployment.&lt;/p&gt;
&lt;p&gt;NVIDIA Cosmos world foundation models (WFMs) are now available as NVIDIA NIM microservices on Amazon EKS, enabling real-time robotics control and simulation workloads with seamless reliability and cloud-native efficiency. For batch-based tasks and offline workloads such as large-scale synthetic data generation, Cosmos WFMs are also available on AWS Batch as containers.&lt;/p&gt;
&lt;p&gt;Cosmos-generated world states can then be used to train and validate robots using open-source simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab.&lt;/p&gt;
&lt;p&gt;Leading robotics companies such as Agility Robotics, Agile Robots, ANYbotics, Diligent Robotics, Dyna Robotics, Field AI, Haply Robotics, Lightwheel, RIVR and Skild AI are using the NVIDIA Isaac platform with AWS for use cases ranging from collecting, storing and processing robot-generated data to training and simulation for scaling robotics development.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Sustained Collaboration&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Underscoring years of continued collaboration, NVIDIA earned the AWS Global GenAI Infrastructure and Data Partner of the Year award, which recognizes top technology partners with the Generative AI Competency that support vector embeddings, data storage and management or synthetic data generation in multiple types and formats.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about NVIDIA and AWS’s collaboration and join sessions at &lt;/i&gt;&lt;i&gt;AWS re:Invent&lt;/i&gt;&lt;i&gt;, running through Friday, Dec. 5, in Las Vegas.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</guid><pubDate>Tue, 02 Dec 2025 16:00:27 +0000</pubDate></item><item><title>AWS re:Invent 2025: How to watch and follow along live (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/aws-reinvent-2025-how-to-watch-and-follow-along-live/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/IMG_3817.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ big annual event, re:Invent 2025, is getting into full swing in Las Vegas today. Last year’s event was largely focused on their AI efforts, including new foundation models, services tackling AI hallucinations, and new security measures. And this year is likely to follow suit, based on what Amazon has announced so far and the lineup of speakers and programming that you can explore in more detail below.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The event formally kicks off this morning, December 2, at 8 a.m. PT. You can expect editorial coverage of the keynotes, interviews with AWS execs, and a roundup of news as the week’s programming rolls out, all of which you’ll be able to find on our site or by just keeping tabs on our AWS re:Invent tag.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s become the norm for events like re:Invent to broadcast keynotes and select programming — we just did something similar for TechCrunch Disrupt 2025 — and you can check out a breadth of streams with no ticket necessary below:&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-keynote-addresses"&gt;AWS re:Invent keynote addresses&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s re:Invent features five keynotes from Tuesday through Thursday, with, of course, a clear focus on AI. You can follow along with each of those keynotes via the scheduled livestreams below. Or you can just go to Fortnite (yes, really) and watch their five keynotes live there.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ceo-matt-garman-december-2-at-8-a-m-pt"&gt;AWS CEO Matt Garman: December 2 at 8 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-vp-of-agentic-ai-swami-sivasubramanian-december-3-at-8-30-a-m-pt"&gt;AWS VP of Agentic AI Swami Sivasubramanian: December 3 at 8:30 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-vp-of-global-specialists-and-partners-aws-dr-ruba-borno-december-3-at-3-p-m-pt"&gt;VP of Global Specialists and Partners, AWS Dr. Ruba Borno: December 3 at 3 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-svp-of-utility-computing-peter-desantis-vp-of-compute-and-machine-learning-services-aws-december-4-at-9-a-m-pt"&gt;SVP of Utility Computing Peter DeSantis, VP of Compute and Machine Learning Services, AWS: December 4 at 9 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-cto-of-amazon-com-dr-werner-vogels-december-4-at-3-30-p-m-pt"&gt;CTO of Amazon.com Dr. Werner Vogels: December 4 at 3:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-partner-showcases"&gt;AWS re:Invent partner showcases&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;A series of industry-specific partner showcases will also be livestreamed for those not on the ground in Las Vegas, with a series of streams that extend throughout the run of the event.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-one-december-2-at-10-a-m-pt"&gt;AWS Security, Day One: December 2 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-one-december-2-at-2-p-m-pt"&gt;AWS AI, Day One: December 2 at 2 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-two-december-3-at-10-a-m-pt"&gt;AWS AI, Day Two: December 3 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-two-december-3-at-4-30-p-m-pt"&gt;AWS Security, Day Two: December 3 at 4:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-three-december-4-at-10-00-a-m-pt"&gt;AWS Security, Day Three: December 4 at 10:00 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-three-december-4-at-11-35-a-m-pt"&gt;AWS AI, Day Three: December 4 at 11:35 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-industries-december-4-at-1-40-p-m-pt"&gt;AWS Industries: December 4 at 1:40 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also has partnered with companies like TechCrunch to highlight a sponsored showcase of their AWS OnAir programming in particular, which kicked off Monday to preview the event and highlight some early reveals, which you can watch the archive of on Twitch:&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/IMG_3817.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ big annual event, re:Invent 2025, is getting into full swing in Las Vegas today. Last year’s event was largely focused on their AI efforts, including new foundation models, services tackling AI hallucinations, and new security measures. And this year is likely to follow suit, based on what Amazon has announced so far and the lineup of speakers and programming that you can explore in more detail below.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The event formally kicks off this morning, December 2, at 8 a.m. PT. You can expect editorial coverage of the keynotes, interviews with AWS execs, and a roundup of news as the week’s programming rolls out, all of which you’ll be able to find on our site or by just keeping tabs on our AWS re:Invent tag.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s become the norm for events like re:Invent to broadcast keynotes and select programming — we just did something similar for TechCrunch Disrupt 2025 — and you can check out a breadth of streams with no ticket necessary below:&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-keynote-addresses"&gt;AWS re:Invent keynote addresses&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s re:Invent features five keynotes from Tuesday through Thursday, with, of course, a clear focus on AI. You can follow along with each of those keynotes via the scheduled livestreams below. Or you can just go to Fortnite (yes, really) and watch their five keynotes live there.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ceo-matt-garman-december-2-at-8-a-m-pt"&gt;AWS CEO Matt Garman: December 2 at 8 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-vp-of-agentic-ai-swami-sivasubramanian-december-3-at-8-30-a-m-pt"&gt;AWS VP of Agentic AI Swami Sivasubramanian: December 3 at 8:30 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-vp-of-global-specialists-and-partners-aws-dr-ruba-borno-december-3-at-3-p-m-pt"&gt;VP of Global Specialists and Partners, AWS Dr. Ruba Borno: December 3 at 3 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-svp-of-utility-computing-peter-desantis-vp-of-compute-and-machine-learning-services-aws-december-4-at-9-a-m-pt"&gt;SVP of Utility Computing Peter DeSantis, VP of Compute and Machine Learning Services, AWS: December 4 at 9 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-cto-of-amazon-com-dr-werner-vogels-december-4-at-3-30-p-m-pt"&gt;CTO of Amazon.com Dr. Werner Vogels: December 4 at 3:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-partner-showcases"&gt;AWS re:Invent partner showcases&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;A series of industry-specific partner showcases will also be livestreamed for those not on the ground in Las Vegas, with a series of streams that extend throughout the run of the event.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-one-december-2-at-10-a-m-pt"&gt;AWS Security, Day One: December 2 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-one-december-2-at-2-p-m-pt"&gt;AWS AI, Day One: December 2 at 2 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-two-december-3-at-10-a-m-pt"&gt;AWS AI, Day Two: December 3 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-two-december-3-at-4-30-p-m-pt"&gt;AWS Security, Day Two: December 3 at 4:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-three-december-4-at-10-00-a-m-pt"&gt;AWS Security, Day Three: December 4 at 10:00 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-three-december-4-at-11-35-a-m-pt"&gt;AWS AI, Day Three: December 4 at 11:35 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-industries-december-4-at-1-40-p-m-pt"&gt;AWS Industries: December 4 at 1:40 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also has partnered with companies like TechCrunch to highlight a sponsored showcase of their AWS OnAir programming in particular, which kicked off Monday to preview the event and highlight some early reveals, which you can watch the archive of on Twitch:&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/aws-reinvent-2025-how-to-watch-and-follow-along-live/</guid><pubDate>Tue, 02 Dec 2025 16:24:51 +0000</pubDate></item><item><title>[NEW] Frontier AI research lab tackles enterprise deployment challenges (AI News)</title><link>https://www.artificialintelligence-news.com/news/frontier-ai-research-lab-tackles-enterprise-deployment-challenges/</link><description>&lt;p&gt;Thomson Reuters and Imperial College London have established a frontier AI research lab to overcome historic deployment challenges.&lt;/p&gt;&lt;p&gt;Speed and scale have defined the current AI boom. But for enterprises, the primary obstacles to deployment are different: trust, accuracy, and lineage. Addressing these barriers, Thomson Reuters and Imperial College London have announced a five-year partnership to establish a joint ‘Frontier AI Research Lab’.&lt;/p&gt;&lt;p&gt;With the involvement of both a corporate and academic leader, the initiative appears built to target the disconnect between high-level computer science and the pragmatic requirements of professional services. The lab will pursue academic research in AI, focusing on safety, reliability, and the development of frontier capabilities. It offers enterprise leaders a preview of how future systems might advance beyond generative text to perform reliable work in high-stakes environments.&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-improving-reliability-with-practical-frontier-ai-research"&gt;Improving reliability with practical frontier AI research&lt;/h3&gt;&lt;p&gt;Current Large Language Models (LLMs) often struggle with the precision required in sectors such as law, tax, and compliance. To counter this, the lab plans to train large-scale foundation models jointly. This is an opportunity typically restricted to a handful of industrial technology giants.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Researchers will experiment with data-centric machine learning and retrieval-augmented generation using Thomson Reuters’ substantial repository of content. By grounding AI models in verified and domain-specific data, the initiative aims to greatly improve the algorithms used to drive positive impact in the wider world and address challenges prior to real-world deployment.&lt;/p&gt;&lt;p&gt;Dr Jonathan Richard Schwarz, Head of AI Research at Thomson Reuters, said: “We are only beginning to understand the transformative impact this technology will have on all aspects of society.&lt;/p&gt;&lt;p&gt;“Our vision is a unique research space where foundational algorithms are developed and made available to world experts, advancing the transparency, verifiability, and trustworthiness in which these changes are driving impact in the world.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;Data provenance is the central theme here. As Dr Schwarz suggests, the value lies not merely in the model architecture but in the quality of the information it processes. The partnership creates an avenue for researchers to access high-quality data spanning complex and knowledge-intensive domains.&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-making-enterprise-ai-deployment-challenges-history"&gt;Making enterprise AI deployment challenges history&lt;/h3&gt;&lt;p&gt;The lab’s frontier AI research agenda indicates where enterprise technology is heading. Beyond simple content generation, the facility will investigate agentic AI systems, reasoning, planning, and human-in-the-loop workflows.&lt;/p&gt;&lt;p&gt;These areas are essential for organisations looking to automate multi-step processes rather than just discrete tasks. Professor Alessandra Russo, who will co-lead the lab alongside Dr Schwarz and Cambridge’s Professor Felix Steffek, believes the dedicated infrastructure will empower researchers to deliver scientific advances that have practical relevance.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“With dedicated space, a focused PhD cohort, and high-quality computing infrastructure and support, our researchers will be empowered to push the boundaries of AI and deliver scientific advances that truly matter,” Professor Russo stated.&lt;/p&gt;&lt;p&gt;“Our collaboration with Thomson Reuters anchors that work in real-world use cases, ensuring that breakthroughs translate into meaningful societal benefit. There is huge potential to unlock creative approaches to a wide range of roles and sectors, enabling AI to strengthen society, energise traditional industries, and create new roles and opportunities across the economy.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;Operations leaders should note that future AI implementations will likely require robust “reasoning” capabilities (i.e. the ability for a system to plan a series of actions and verify its own outputs) before they can be trusted with autonomous decision-making in regulated industries.&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-boosting-infrastructure-and-talent-pipelines-to-advance-frontier-ai-research"&gt;Boosting infrastructure and talent pipelines to advance frontier AI research&lt;/h3&gt;&lt;p&gt;Running these experiments requires substantial compute power, a resource often lacking in purely academic settings. The partnership addresses this by providing researchers access to Imperial’s high-performance computing cluster. This enables AI experiments at a meaningful scale to uncover any challenges that need to be overcome prior to real-world deployment.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The setup creates a feedback loop between research and practice. The lab is planned to host over a dozen PhD students who will work alongside Thomson Reuters foundational research scientists. This structure accelerates the translation of research into practice and establishes a direct pipeline for talent development and real-world validation.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Professor Mary Ryan, Vice Provost for Research and Enterprise at Imperial, commented: “This collaboration gives our researchers the space and support to explore fundamental questions about how AI can and should work for society.&lt;/p&gt;&lt;p&gt;“Progress in this area depends on rigorous science, open inquiry, and strong partnerships—ideals exemplified by the approach this lab will take.”&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-overcoming-legal-and-economic-challenges-for-successful-enterprise-ai-deployments"&gt;Overcoming legal and economic challenges for successful enterprise AI deployments&lt;/h3&gt;&lt;p&gt;The risks associated with AI are as much legal and economic as they are technical. Recognising this, the lab’s steering committee includes Professor Felix Steffek, a Professor of Law at the University of Cambridge.&lt;/p&gt;&lt;p&gt;“AI has great potential to improve access to justice,” said Professor Steffek. “However, there are significant challenges that foundational research needs to address in order to make legal AI applications safe and ethically responsible.&lt;/p&gt;&lt;p&gt;“The lab will bring together bright minds from multiple disciplines – including law, ethics, and AI – to advance the potential and address the risks of legal AI.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;The scope of research extends to the technology’s broader economic impact and the future of work. The lab aims to produce insights on how AI can energise traditional industries and create new roles across the economy.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Overall, the Frontier AI Research Lab represents a model for de-risking enterprise AI strategies and overcoming challenges that have historically held back deployments. Coupling industrial data and compute resources with academic rigour helps organisations understand the “black box” nature of these systems and overcome the challenges to ensure the success of any deployment.&lt;/p&gt;&lt;p&gt;Activities at the lab will commence upon formal launch, starting with the recruitment of the initial PhD cohort. Business leaders should track the joint publications coming out of this unit as these findings will likely serve as valuable benchmarks for evaluating the safety and efficacy of internal AI deployments.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Agentic AI autonomy grows in North American enterprises&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Thomson Reuters and Imperial College London have established a frontier AI research lab to overcome historic deployment challenges.&lt;/p&gt;&lt;p&gt;Speed and scale have defined the current AI boom. But for enterprises, the primary obstacles to deployment are different: trust, accuracy, and lineage. Addressing these barriers, Thomson Reuters and Imperial College London have announced a five-year partnership to establish a joint ‘Frontier AI Research Lab’.&lt;/p&gt;&lt;p&gt;With the involvement of both a corporate and academic leader, the initiative appears built to target the disconnect between high-level computer science and the pragmatic requirements of professional services. The lab will pursue academic research in AI, focusing on safety, reliability, and the development of frontier capabilities. It offers enterprise leaders a preview of how future systems might advance beyond generative text to perform reliable work in high-stakes environments.&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-improving-reliability-with-practical-frontier-ai-research"&gt;Improving reliability with practical frontier AI research&lt;/h3&gt;&lt;p&gt;Current Large Language Models (LLMs) often struggle with the precision required in sectors such as law, tax, and compliance. To counter this, the lab plans to train large-scale foundation models jointly. This is an opportunity typically restricted to a handful of industrial technology giants.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Researchers will experiment with data-centric machine learning and retrieval-augmented generation using Thomson Reuters’ substantial repository of content. By grounding AI models in verified and domain-specific data, the initiative aims to greatly improve the algorithms used to drive positive impact in the wider world and address challenges prior to real-world deployment.&lt;/p&gt;&lt;p&gt;Dr Jonathan Richard Schwarz, Head of AI Research at Thomson Reuters, said: “We are only beginning to understand the transformative impact this technology will have on all aspects of society.&lt;/p&gt;&lt;p&gt;“Our vision is a unique research space where foundational algorithms are developed and made available to world experts, advancing the transparency, verifiability, and trustworthiness in which these changes are driving impact in the world.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;Data provenance is the central theme here. As Dr Schwarz suggests, the value lies not merely in the model architecture but in the quality of the information it processes. The partnership creates an avenue for researchers to access high-quality data spanning complex and knowledge-intensive domains.&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-making-enterprise-ai-deployment-challenges-history"&gt;Making enterprise AI deployment challenges history&lt;/h3&gt;&lt;p&gt;The lab’s frontier AI research agenda indicates where enterprise technology is heading. Beyond simple content generation, the facility will investigate agentic AI systems, reasoning, planning, and human-in-the-loop workflows.&lt;/p&gt;&lt;p&gt;These areas are essential for organisations looking to automate multi-step processes rather than just discrete tasks. Professor Alessandra Russo, who will co-lead the lab alongside Dr Schwarz and Cambridge’s Professor Felix Steffek, believes the dedicated infrastructure will empower researchers to deliver scientific advances that have practical relevance.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“With dedicated space, a focused PhD cohort, and high-quality computing infrastructure and support, our researchers will be empowered to push the boundaries of AI and deliver scientific advances that truly matter,” Professor Russo stated.&lt;/p&gt;&lt;p&gt;“Our collaboration with Thomson Reuters anchors that work in real-world use cases, ensuring that breakthroughs translate into meaningful societal benefit. There is huge potential to unlock creative approaches to a wide range of roles and sectors, enabling AI to strengthen society, energise traditional industries, and create new roles and opportunities across the economy.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;Operations leaders should note that future AI implementations will likely require robust “reasoning” capabilities (i.e. the ability for a system to plan a series of actions and verify its own outputs) before they can be trusted with autonomous decision-making in regulated industries.&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-boosting-infrastructure-and-talent-pipelines-to-advance-frontier-ai-research"&gt;Boosting infrastructure and talent pipelines to advance frontier AI research&lt;/h3&gt;&lt;p&gt;Running these experiments requires substantial compute power, a resource often lacking in purely academic settings. The partnership addresses this by providing researchers access to Imperial’s high-performance computing cluster. This enables AI experiments at a meaningful scale to uncover any challenges that need to be overcome prior to real-world deployment.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The setup creates a feedback loop between research and practice. The lab is planned to host over a dozen PhD students who will work alongside Thomson Reuters foundational research scientists. This structure accelerates the translation of research into practice and establishes a direct pipeline for talent development and real-world validation.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Professor Mary Ryan, Vice Provost for Research and Enterprise at Imperial, commented: “This collaboration gives our researchers the space and support to explore fundamental questions about how AI can and should work for society.&lt;/p&gt;&lt;p&gt;“Progress in this area depends on rigorous science, open inquiry, and strong partnerships—ideals exemplified by the approach this lab will take.”&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-overcoming-legal-and-economic-challenges-for-successful-enterprise-ai-deployments"&gt;Overcoming legal and economic challenges for successful enterprise AI deployments&lt;/h3&gt;&lt;p&gt;The risks associated with AI are as much legal and economic as they are technical. Recognising this, the lab’s steering committee includes Professor Felix Steffek, a Professor of Law at the University of Cambridge.&lt;/p&gt;&lt;p&gt;“AI has great potential to improve access to justice,” said Professor Steffek. “However, there are significant challenges that foundational research needs to address in order to make legal AI applications safe and ethically responsible.&lt;/p&gt;&lt;p&gt;“The lab will bring together bright minds from multiple disciplines – including law, ethics, and AI – to advance the potential and address the risks of legal AI.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;The scope of research extends to the technology’s broader economic impact and the future of work. The lab aims to produce insights on how AI can energise traditional industries and create new roles across the economy.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Overall, the Frontier AI Research Lab represents a model for de-risking enterprise AI strategies and overcoming challenges that have historically held back deployments. Coupling industrial data and compute resources with academic rigour helps organisations understand the “black box” nature of these systems and overcome the challenges to ensure the success of any deployment.&lt;/p&gt;&lt;p&gt;Activities at the lab will commence upon formal launch, starting with the recruitment of the initial PhD cohort. Business leaders should track the joint publications coming out of this unit as these findings will likely serve as valuable benchmarks for evaluating the safety and efficacy of internal AI deployments.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Agentic AI autonomy grows in North American enterprises&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/frontier-ai-research-lab-tackles-enterprise-deployment-challenges/</guid><pubDate>Tue, 02 Dec 2025 16:26:07 +0000</pubDate></item><item><title>OpenAI slammed for app suggestions that looked like ads (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/openai-slammed-for-app-suggestions-that-looked-like-ads/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT’s unwelcome suggestion for a Peloton app during a conversation led to some backlash from OpenAI customers. People feared that ads had arrived, even for paid customers. OpenAI, however, clarified that the app suggestion was not an advertisement, but instead a poor attempt to integrate an app discovery feature within conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a post on X, which has since been viewed nearly 462,000 times, AI startup Hyperbolic’s co-founder, Yuchen Jin, shared a screenshot where ChatGPT seemingly suggested connecting the Peloton app in an unrelated conversation. Worse still, Jin noted he was a paid subscriber to ChatGPT’s $200 per month Pro Plan. At that price point, ads would not be expected.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The post, which was reshared and saved hundreds of times across X, received quite a bit of attention, as it seemed to indicate OpenAI was testing the insertion of ads into its paid product. Users complained that paying customers, especially, shouldn’t have to see app suggestions like this.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One person also pointed out that they couldn’t get ChatGPT to stop recommending Spotify to them, even though they were an Apple Music subscriber.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Hey, Kol. Thanks for flagging 🙏 This is not an ad (there's no financial component). It's only a suggestion to install Peloton's app. But the lack of relevancy makes it a bad/confusing experience. We're iterating on the suggestions and UX, trying to make sure they're awesome.&lt;/p&gt;— Daniel McAuley (@_dmca) December 1, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s data lead for ChatGPT, Daniel McAuley, later jumped into the thread to clarify that the Peloton placement was not an ad; it was “only a suggestion to install Peloton’s app.” He said there was “no financial component” to the appearance of the app suggestion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, he admitted that “the lack of relevancy” to the conversation made it a bad and confusing experience, and OpenAI was iterating on the suggestions and the user experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A company spokesperson also confirmed to TechCrunch that what users had spotted was one of the ways OpenAI had been “testing surfacing apps in ChatGPT conversations.” They pointed to OpenAI’s announcement in October about its new app platform, where the company noted that apps would “fit naturally” into user conversations.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“You can discover [apps] when ChatGPT suggests one at the right time, or by calling them by name. Apps respond to natural language and include interactive interfaces you can use right in the chat,” the post explained. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But that didn’t appear to be the case here, as the user claims they weren’t discussing anything related to health and fitness. Instead, as the screenshot shows, they had been chatting with the AI about a podcast featuring Elon Musk, where xAI was the topic being discussed. Inserting Peloton into this experience was unhelpful and a distraction. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yet even if the app suggestion had been relevant, users may have still viewed it as an ad, given that it’s directing people to a product from a business that isn’t free. In addition, users can’t turn off these app suggestions, which may make them feel more intrusive.  &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This user sentiment could have potential ramifications for OpenAI’s desire to replace the App Store experience, and apps that run on your phone, with integrated apps that run within ChatGPT. If users don’t want to see app suggestions, they could choose to switch to a competitor’s chatbot to avoid them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently, ChatGPT apps are available to logged-in users outside of the EU, Switzerland, and the U.K., and the integrations are still in pilot testing. OpenAI partners with a number of app makers, including Booking.com, Canva, Coursera, Figma, Expedia, Zillow, and others. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT’s unwelcome suggestion for a Peloton app during a conversation led to some backlash from OpenAI customers. People feared that ads had arrived, even for paid customers. OpenAI, however, clarified that the app suggestion was not an advertisement, but instead a poor attempt to integrate an app discovery feature within conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a post on X, which has since been viewed nearly 462,000 times, AI startup Hyperbolic’s co-founder, Yuchen Jin, shared a screenshot where ChatGPT seemingly suggested connecting the Peloton app in an unrelated conversation. Worse still, Jin noted he was a paid subscriber to ChatGPT’s $200 per month Pro Plan. At that price point, ads would not be expected.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The post, which was reshared and saved hundreds of times across X, received quite a bit of attention, as it seemed to indicate OpenAI was testing the insertion of ads into its paid product. Users complained that paying customers, especially, shouldn’t have to see app suggestions like this.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One person also pointed out that they couldn’t get ChatGPT to stop recommending Spotify to them, even though they were an Apple Music subscriber.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Hey, Kol. Thanks for flagging 🙏 This is not an ad (there's no financial component). It's only a suggestion to install Peloton's app. But the lack of relevancy makes it a bad/confusing experience. We're iterating on the suggestions and UX, trying to make sure they're awesome.&lt;/p&gt;— Daniel McAuley (@_dmca) December 1, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s data lead for ChatGPT, Daniel McAuley, later jumped into the thread to clarify that the Peloton placement was not an ad; it was “only a suggestion to install Peloton’s app.” He said there was “no financial component” to the appearance of the app suggestion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, he admitted that “the lack of relevancy” to the conversation made it a bad and confusing experience, and OpenAI was iterating on the suggestions and the user experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A company spokesperson also confirmed to TechCrunch that what users had spotted was one of the ways OpenAI had been “testing surfacing apps in ChatGPT conversations.” They pointed to OpenAI’s announcement in October about its new app platform, where the company noted that apps would “fit naturally” into user conversations.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“You can discover [apps] when ChatGPT suggests one at the right time, or by calling them by name. Apps respond to natural language and include interactive interfaces you can use right in the chat,” the post explained. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But that didn’t appear to be the case here, as the user claims they weren’t discussing anything related to health and fitness. Instead, as the screenshot shows, they had been chatting with the AI about a podcast featuring Elon Musk, where xAI was the topic being discussed. Inserting Peloton into this experience was unhelpful and a distraction. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yet even if the app suggestion had been relevant, users may have still viewed it as an ad, given that it’s directing people to a product from a business that isn’t free. In addition, users can’t turn off these app suggestions, which may make them feel more intrusive.  &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This user sentiment could have potential ramifications for OpenAI’s desire to replace the App Store experience, and apps that run on your phone, with integrated apps that run within ChatGPT. If users don’t want to see app suggestions, they could choose to switch to a competitor’s chatbot to avoid them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently, ChatGPT apps are available to logged-in users outside of the EU, Switzerland, and the U.K., and the integrations are still in pilot testing. OpenAI partners with a number of app makers, including Booking.com, Canva, Coursera, Figma, Expedia, Zillow, and others. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/openai-slammed-for-app-suggestions-that-looked-like-ads/</guid><pubDate>Tue, 02 Dec 2025 16:43:21 +0000</pubDate></item><item><title>Amazon's new AI can code for days without human help. What does that mean for software engineers? (AI | VentureBeat)</title><link>https://venturebeat.com/ai/amazons-new-ai-can-code-for-days-without-human-help-what-does-that-mean-for</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://aws.amazon.com/"&gt;&lt;u&gt;Amazon Web Services&lt;/u&gt;&lt;/a&gt; on Tuesday announced a new class of artificial intelligence systems called &amp;quot;&lt;a href="https://www.aboutamazon.com/news/aws/amazon-ai-frontier-agents-autonomous-kiro"&gt;&lt;u&gt;frontier agents&lt;/u&gt;&lt;/a&gt;&amp;quot; that can work autonomously for hours or even days without human intervention, representing one of the most ambitious attempts yet to automate the full software development lifecycle.&lt;/p&gt;&lt;p&gt;The announcement, made during AWS CEO Matt Garman&amp;#x27;s &lt;a href="https://reinvent.awsevents.com/keynotes/"&gt;&lt;u&gt;keynote address&lt;/u&gt;&lt;/a&gt; at the company&amp;#x27;s annual &lt;a href="https://reinvent.awsevents.com/"&gt;&lt;u&gt;re:Invent conference&lt;/u&gt;&lt;/a&gt;, introduces three specialized AI agents designed to act as virtual team members: Kiro autonomous agent for software development, AWS Security Agent for application security, and AWS DevOps Agent for IT operations.&lt;/p&gt;&lt;p&gt;The move signals Amazon&amp;#x27;s intent to leap ahead in the intensifying competition to build AI systems capable of performing complex, multi-step tasks that currently require teams of skilled engineers.&lt;/p&gt;&lt;p&gt;&amp;quot;We see frontier agents as a completely new class of agents,&amp;quot; said Deepak Singh, vice president of developer agents and experiences at Amazon, in an interview ahead of the announcement. &amp;quot;They&amp;#x27;re fundamentally designed to work for hours and days. You&amp;#x27;re not giving them a problem that you want finished in the next five minutes. You&amp;#x27;re giving them complex challenges that they may have to think about, try different solutions, and get to the right conclusion — and they should do that without intervention.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Amazon believes its new agents leave existing AI coding tools behind&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The frontier agents differ from existing AI coding assistants like &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt; or Amazon&amp;#x27;s own &lt;a href="https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/"&gt;&lt;u&gt;CodeWhisperer&lt;/u&gt;&lt;/a&gt; in several fundamental ways.&lt;/p&gt;&lt;p&gt;Current AI coding tools, while powerful, require engineers to drive every interaction. Developers must write prompts, provide context, and manually coordinate work across different code repositories. When switching between tasks, the AI loses context and must start fresh.&lt;/p&gt;&lt;p&gt;The new frontier agents, by contrast, maintain persistent memory across sessions and continuously learn from an organization&amp;#x27;s codebase, documentation, and team communications. They can independently determine which code repositories require changes, work on multiple files simultaneously, and coordinate complex transformations spanning dozens of microservices.&lt;/p&gt;&lt;p&gt;&amp;quot;With a current agent, you would go microservice by microservice, making changes one at a time, and each change would be a different session with no shared context,&amp;quot; Singh explained. &amp;quot;With a frontier agent, you say, &amp;#x27;I need to solve this broad problem.&amp;#x27; You point it to the right application, and it decides which repos need changes.&amp;quot;&lt;/p&gt;&lt;p&gt;The agents exhibit three defining characteristics that AWS believes set them apart: autonomy in decision-making, the ability to scale by spawning multiple agents to work on different aspects of a problem simultaneously, and the capacity to operate independently for extended periods.&lt;/p&gt;&lt;p&gt;&amp;quot;A frontier agent can decide to spin up 10 versions of itself, all working on different parts of the problem at once,&amp;quot; Singh said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How each of the three frontier agents tackles a different phase of development&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="http://g"&gt;&lt;u&gt;Kiro autonomous agent&lt;/u&gt;&lt;/a&gt; serves as a virtual developer that maintains context across coding sessions and learns from an organization&amp;#x27;s pull requests, code reviews, and technical discussions. Teams can connect it to GitHub, Jira, Slack, and internal documentation systems. The agent then acts like a teammate, accepting task assignments and working independently until it either completes the work or requires human guidance.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.aboutamazon.com/news/aws/amazon-ai-frontier-agents-autonomous-kiro"&gt;&lt;u&gt;AWS Security Agent&lt;/u&gt;&lt;/a&gt; embeds security expertise throughout the development process, automatically reviewing design documents and scanning pull requests against organizational security requirements. Perhaps most significantly, it transforms penetration testing from a weeks-long manual process into an on-demand capability that completes in hours.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.aboutamazon.com/news/aws/amazon-ai-frontier-agents-autonomous-kiro"&gt;&lt;u&gt;SmugMug&lt;/u&gt;&lt;/a&gt;, a photo hosting platform, has already deployed the security agent. &amp;quot;AWS Security Agent helped catch a business logic bug that no existing tools would have caught, exposing information improperly,&amp;quot; said Andres Ruiz, staff software engineer at the company. &amp;quot;To any other tool, this would have been invisible. But the ability for Security Agent to contextualize the information, parse the API response, and find the unexpected information there represents a leap forward in automated security testing.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://aws.amazon.com/about-aws/whats-new/2025/12/devops-agent-preview-frontier-agent-operational-excellence/"&gt;&lt;u&gt;AWS DevOps Agent&lt;/u&gt;&lt;/a&gt; functions as an always-on operations team member, responding instantly to incidents and using its accumulated knowledge to identify root causes. It connects to observability tools including Amazon CloudWatch, Datadog, Dynatrace, New Relic, and Splunk, along with runbooks and deployment pipelines.&lt;/p&gt;&lt;p&gt;Commonwealth Bank of Australia tested the DevOps agent by replicating a complex network and identity management issue that typically requires hours for experienced engineers to diagnose. The agent identified the root cause in under 15 minutes.&lt;/p&gt;&lt;p&gt;&amp;quot;AWS DevOps Agent thinks and acts like a seasoned DevOps engineer, helping our engineers build a banking infrastructure that&amp;#x27;s faster, more resilient, and designed to deliver better experiences for our customers,&amp;quot; said Jason Sandry, head of cloud services at Commonwealth Bank.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Amazon makes its case against Google and Microsoft in the AI coding wars&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The announcement arrives amid a fierce battle among technology giants to dominate the emerging market for AI-powered development tools. Google has made significant noise in recent weeks with its own &lt;a href="https://cloud.google.com/use-cases/ai-code-generation?hl=en"&gt;&lt;u&gt;AI coding capabilities&lt;/u&gt;&lt;/a&gt;, while Microsoft continues to advance &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt; and its broader AI development toolkit.&lt;/p&gt;&lt;p&gt;Singh argued that AWS holds distinct advantages rooted in the company&amp;#x27;s 20-year history operating cloud infrastructure and Amazon&amp;#x27;s own massive software engineering organization.&lt;/p&gt;&lt;p&gt;&amp;quot;AWS has been the cloud of choice for 20 years, so we have two decades of knowledge building and running it, and working with customers who&amp;#x27;ve been building and running applications on it,&amp;quot; Singh said. &amp;quot;The learnings from operating AWS, the knowledge our customers have, the experience we&amp;#x27;ve built using these tools ourselves every day to build real-world applications—all of that is embodied in these frontier agents.&amp;quot;&lt;/p&gt;&lt;p&gt;He drew a distinction between tools suitable for prototypes versus production systems. &amp;quot;There&amp;#x27;s a lot of things out there that you can use to build your prototype or your toy application. But if you want to build production applications, there&amp;#x27;s a lot of knowledge that we bring in as AWS that apply here.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The safeguards Amazon built to keep autonomous agents from going rogue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The prospect of AI systems operating autonomously for days raises immediate questions about what happens when they go off track. Singh described multiple safeguards built into the system.&lt;/p&gt;&lt;p&gt;All learnings accumulated by the agents are logged and visible, allowing engineers to understand what knowledge influences the agent&amp;#x27;s decisions. Teams can even remove specific learnings if they discover the agent has absorbed incorrect information from team communications.&lt;/p&gt;&lt;p&gt;&amp;quot;You can go in and even redact that from its knowledge like, &amp;#x27;No, we don&amp;#x27;t want you to ever use this knowledge,&amp;#x27;&amp;quot; Singh said. &amp;quot;You can look at the knowledge like it&amp;#x27;s almost—it&amp;#x27;s like looking at your neurons inside your brain. You can disconnect some.&amp;quot;&lt;/p&gt;&lt;p&gt;Engineers can also monitor agent activity in real-time and intervene when necessary, either redirecting the agent or taking over entirely. Most critically, the agents never commit code directly to production systems. That responsibility remains with human engineers.&lt;/p&gt;&lt;p&gt;&amp;quot;These agents are never going to check the code into production. That is still the human&amp;#x27;s responsibility,&amp;quot; Singh emphasized. &amp;quot;You are still, as an engineer, responsible for the code you&amp;#x27;re checking in, whether it&amp;#x27;s generated by you or by an agent working autonomously.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What frontier agents mean for the future of software engineering jobs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The announcement inevitably raises concerns about the impact on software engineering jobs. Singh pushed back against the notion that frontier agents will replace developers, framing them instead as tools that amplify human capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;Software engineering is craft. What&amp;#x27;s changing is not, &amp;#x27;Hey, agents are doing all the work.&amp;#x27; The craft of software engineering is changing—how you use agents, how do you set up your code base, how do you set up your prompts, how do you set up your rules, how do you set up your knowledge bases so that agents can be effective,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;Singh noted that senior engineers who had drifted away from hands-on coding are now writing more code than ever. &amp;quot;It&amp;#x27;s actually easier for them to become software engineers,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;He pointed to an internal example where a team completed a project in 78 days that would have taken 18 months using traditional practices. &amp;quot;Because they were able to use AI. And the thing that made it work was not just the fact that they were using AI, but how they organized and set up their practices of how they built that software were maximized around that.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Amazon plans to make AI-generated code more trustworthy over time&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Singh outlined several areas where frontier agents will evolve over the coming years. Multi-agent architectures, where systems of specialized agents coordinate to solve complex problems, represent a major frontier. So does the integration of formal verification techniques to increase confidence in AI-generated code.&lt;/p&gt;&lt;p&gt;AWS recently introduced property-based testing in Kiro, which uses automated reasoning to extract testable properties from specifications and generate thousands of test scenarios automatically.&lt;/p&gt;&lt;p&gt;&amp;quot;If you have a shopping cart application, every way an order can be canceled, and how it might be canceled, and the way refunds are handled in Germany versus the US—if you&amp;#x27;re writing a unit test, maybe two, Germany and US, but now, because you have this property-based testing approach, your agent can create a scenario for every country you operate in and test all of them automatically for you,&amp;quot; Singh explained.&lt;/p&gt;&lt;p&gt;Building trust in autonomous systems remains the central challenge. &amp;quot;Right now you still require tons of human guardrails at every step to make sure that the right thing happens. And as we get better at these techniques, you will use less and less, and you&amp;#x27;ll be able to trust the agents a lot more,&amp;quot; he said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Amazon&amp;#x27;s bigger bet on autonomous AI stretches far beyond writing code&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The frontier agents announcement arrived alongside a cascade of other news at &lt;a href="https://www.aboutamazon.com/news/aws/aws-re-invent-2025-ai-news-updates"&gt;&lt;u&gt;re:Invent 2025&lt;/u&gt;&lt;/a&gt;. AWS kicked off the conference with major announcements on agentic AI capabilities, customer service innovations, and multicloud networking.&lt;/p&gt;&lt;p&gt;Amazon expanded its Nova portfolio with &lt;a href="https://www.aboutamazon.com/news/aws/aws-agentic-ai-amazon-bedrock-nova-models"&gt;&lt;u&gt;four new models&lt;/u&gt;&lt;/a&gt; delivering industry-leading price-performance across reasoning, multimodal processing, conversational AI, code generation, and agentic tasks. Nova Forge pioneers &amp;quot;open training,&amp;quot; giving organizations access to pre-trained model checkpoints and the ability to blend proprietary data with Amazon Nova-curated datasets.&lt;/p&gt;&lt;p&gt;AWS also added &lt;a href="https://aws.amazon.com/blogs/aws/amazon-bedrock-adds-fully-managed-open-weight-models/"&gt;&lt;u&gt;18 new open weight models to Amazon Bedrock&lt;/u&gt;&lt;/a&gt;, reinforcing its commitment to offering a broad selection of fully managed models from leading AI providers. The launch includes new models from Mistral AI, Google&amp;#x27;s Gemma 3, MiniMax&amp;#x27;s M2, NVIDIA&amp;#x27;s Nemotron, and OpenAI&amp;#x27;s GPT OSS Safeguard.&lt;/p&gt;&lt;p&gt;On the infrastructure side, &lt;a href="https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-ec2-trn3-ultraservers/"&gt;&lt;u&gt;Amazon EC2 Trn3 UltraServers&lt;/u&gt;&lt;/a&gt;, powered by AWS&amp;#x27;s first 3nm AI chip, pack up to 144 Trainium3 chips into a single integrated system, delivering up to 4.4x more compute performance and 4x greater energy efficiency than the previous generation. AWS AI Factories provides enterprises and government organizations with dedicated AWS AI infrastructure deployed in their own data centers, combining NVIDIA GPUs, Trainium chips, AWS networking, and AI services like Amazon Bedrock and SageMaker AI.&lt;/p&gt;&lt;p&gt;All three frontier agents launched in preview on Tuesday. Pricing will be announced when the services reach general availability.&lt;/p&gt;&lt;p&gt;Singh made clear the company sees applications far beyond coding. &amp;quot;These are the first frontier agents we are releasing, and they&amp;#x27;re in the software development lifecycle,&amp;quot; he said. &amp;quot;The problems and use cases for frontier agents—these agents that are long running, capable of autonomy, thinking, always learning and improving—can be applied to many, many domains.&amp;quot;&lt;/p&gt;&lt;p&gt;Amazon, after all, operates satellite networks, runs robotics warehouses, and manages one of the world&amp;#x27;s largest e-commerce platforms. If autonomous agents can learn to write code on their own, the company is betting they can eventually learn to do just about anything else.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://aws.amazon.com/"&gt;&lt;u&gt;Amazon Web Services&lt;/u&gt;&lt;/a&gt; on Tuesday announced a new class of artificial intelligence systems called &amp;quot;&lt;a href="https://www.aboutamazon.com/news/aws/amazon-ai-frontier-agents-autonomous-kiro"&gt;&lt;u&gt;frontier agents&lt;/u&gt;&lt;/a&gt;&amp;quot; that can work autonomously for hours or even days without human intervention, representing one of the most ambitious attempts yet to automate the full software development lifecycle.&lt;/p&gt;&lt;p&gt;The announcement, made during AWS CEO Matt Garman&amp;#x27;s &lt;a href="https://reinvent.awsevents.com/keynotes/"&gt;&lt;u&gt;keynote address&lt;/u&gt;&lt;/a&gt; at the company&amp;#x27;s annual &lt;a href="https://reinvent.awsevents.com/"&gt;&lt;u&gt;re:Invent conference&lt;/u&gt;&lt;/a&gt;, introduces three specialized AI agents designed to act as virtual team members: Kiro autonomous agent for software development, AWS Security Agent for application security, and AWS DevOps Agent for IT operations.&lt;/p&gt;&lt;p&gt;The move signals Amazon&amp;#x27;s intent to leap ahead in the intensifying competition to build AI systems capable of performing complex, multi-step tasks that currently require teams of skilled engineers.&lt;/p&gt;&lt;p&gt;&amp;quot;We see frontier agents as a completely new class of agents,&amp;quot; said Deepak Singh, vice president of developer agents and experiences at Amazon, in an interview ahead of the announcement. &amp;quot;They&amp;#x27;re fundamentally designed to work for hours and days. You&amp;#x27;re not giving them a problem that you want finished in the next five minutes. You&amp;#x27;re giving them complex challenges that they may have to think about, try different solutions, and get to the right conclusion — and they should do that without intervention.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Amazon believes its new agents leave existing AI coding tools behind&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The frontier agents differ from existing AI coding assistants like &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt; or Amazon&amp;#x27;s own &lt;a href="https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/"&gt;&lt;u&gt;CodeWhisperer&lt;/u&gt;&lt;/a&gt; in several fundamental ways.&lt;/p&gt;&lt;p&gt;Current AI coding tools, while powerful, require engineers to drive every interaction. Developers must write prompts, provide context, and manually coordinate work across different code repositories. When switching between tasks, the AI loses context and must start fresh.&lt;/p&gt;&lt;p&gt;The new frontier agents, by contrast, maintain persistent memory across sessions and continuously learn from an organization&amp;#x27;s codebase, documentation, and team communications. They can independently determine which code repositories require changes, work on multiple files simultaneously, and coordinate complex transformations spanning dozens of microservices.&lt;/p&gt;&lt;p&gt;&amp;quot;With a current agent, you would go microservice by microservice, making changes one at a time, and each change would be a different session with no shared context,&amp;quot; Singh explained. &amp;quot;With a frontier agent, you say, &amp;#x27;I need to solve this broad problem.&amp;#x27; You point it to the right application, and it decides which repos need changes.&amp;quot;&lt;/p&gt;&lt;p&gt;The agents exhibit three defining characteristics that AWS believes set them apart: autonomy in decision-making, the ability to scale by spawning multiple agents to work on different aspects of a problem simultaneously, and the capacity to operate independently for extended periods.&lt;/p&gt;&lt;p&gt;&amp;quot;A frontier agent can decide to spin up 10 versions of itself, all working on different parts of the problem at once,&amp;quot; Singh said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How each of the three frontier agents tackles a different phase of development&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="http://g"&gt;&lt;u&gt;Kiro autonomous agent&lt;/u&gt;&lt;/a&gt; serves as a virtual developer that maintains context across coding sessions and learns from an organization&amp;#x27;s pull requests, code reviews, and technical discussions. Teams can connect it to GitHub, Jira, Slack, and internal documentation systems. The agent then acts like a teammate, accepting task assignments and working independently until it either completes the work or requires human guidance.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.aboutamazon.com/news/aws/amazon-ai-frontier-agents-autonomous-kiro"&gt;&lt;u&gt;AWS Security Agent&lt;/u&gt;&lt;/a&gt; embeds security expertise throughout the development process, automatically reviewing design documents and scanning pull requests against organizational security requirements. Perhaps most significantly, it transforms penetration testing from a weeks-long manual process into an on-demand capability that completes in hours.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.aboutamazon.com/news/aws/amazon-ai-frontier-agents-autonomous-kiro"&gt;&lt;u&gt;SmugMug&lt;/u&gt;&lt;/a&gt;, a photo hosting platform, has already deployed the security agent. &amp;quot;AWS Security Agent helped catch a business logic bug that no existing tools would have caught, exposing information improperly,&amp;quot; said Andres Ruiz, staff software engineer at the company. &amp;quot;To any other tool, this would have been invisible. But the ability for Security Agent to contextualize the information, parse the API response, and find the unexpected information there represents a leap forward in automated security testing.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://aws.amazon.com/about-aws/whats-new/2025/12/devops-agent-preview-frontier-agent-operational-excellence/"&gt;&lt;u&gt;AWS DevOps Agent&lt;/u&gt;&lt;/a&gt; functions as an always-on operations team member, responding instantly to incidents and using its accumulated knowledge to identify root causes. It connects to observability tools including Amazon CloudWatch, Datadog, Dynatrace, New Relic, and Splunk, along with runbooks and deployment pipelines.&lt;/p&gt;&lt;p&gt;Commonwealth Bank of Australia tested the DevOps agent by replicating a complex network and identity management issue that typically requires hours for experienced engineers to diagnose. The agent identified the root cause in under 15 minutes.&lt;/p&gt;&lt;p&gt;&amp;quot;AWS DevOps Agent thinks and acts like a seasoned DevOps engineer, helping our engineers build a banking infrastructure that&amp;#x27;s faster, more resilient, and designed to deliver better experiences for our customers,&amp;quot; said Jason Sandry, head of cloud services at Commonwealth Bank.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Amazon makes its case against Google and Microsoft in the AI coding wars&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The announcement arrives amid a fierce battle among technology giants to dominate the emerging market for AI-powered development tools. Google has made significant noise in recent weeks with its own &lt;a href="https://cloud.google.com/use-cases/ai-code-generation?hl=en"&gt;&lt;u&gt;AI coding capabilities&lt;/u&gt;&lt;/a&gt;, while Microsoft continues to advance &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt; and its broader AI development toolkit.&lt;/p&gt;&lt;p&gt;Singh argued that AWS holds distinct advantages rooted in the company&amp;#x27;s 20-year history operating cloud infrastructure and Amazon&amp;#x27;s own massive software engineering organization.&lt;/p&gt;&lt;p&gt;&amp;quot;AWS has been the cloud of choice for 20 years, so we have two decades of knowledge building and running it, and working with customers who&amp;#x27;ve been building and running applications on it,&amp;quot; Singh said. &amp;quot;The learnings from operating AWS, the knowledge our customers have, the experience we&amp;#x27;ve built using these tools ourselves every day to build real-world applications—all of that is embodied in these frontier agents.&amp;quot;&lt;/p&gt;&lt;p&gt;He drew a distinction between tools suitable for prototypes versus production systems. &amp;quot;There&amp;#x27;s a lot of things out there that you can use to build your prototype or your toy application. But if you want to build production applications, there&amp;#x27;s a lot of knowledge that we bring in as AWS that apply here.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The safeguards Amazon built to keep autonomous agents from going rogue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The prospect of AI systems operating autonomously for days raises immediate questions about what happens when they go off track. Singh described multiple safeguards built into the system.&lt;/p&gt;&lt;p&gt;All learnings accumulated by the agents are logged and visible, allowing engineers to understand what knowledge influences the agent&amp;#x27;s decisions. Teams can even remove specific learnings if they discover the agent has absorbed incorrect information from team communications.&lt;/p&gt;&lt;p&gt;&amp;quot;You can go in and even redact that from its knowledge like, &amp;#x27;No, we don&amp;#x27;t want you to ever use this knowledge,&amp;#x27;&amp;quot; Singh said. &amp;quot;You can look at the knowledge like it&amp;#x27;s almost—it&amp;#x27;s like looking at your neurons inside your brain. You can disconnect some.&amp;quot;&lt;/p&gt;&lt;p&gt;Engineers can also monitor agent activity in real-time and intervene when necessary, either redirecting the agent or taking over entirely. Most critically, the agents never commit code directly to production systems. That responsibility remains with human engineers.&lt;/p&gt;&lt;p&gt;&amp;quot;These agents are never going to check the code into production. That is still the human&amp;#x27;s responsibility,&amp;quot; Singh emphasized. &amp;quot;You are still, as an engineer, responsible for the code you&amp;#x27;re checking in, whether it&amp;#x27;s generated by you or by an agent working autonomously.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What frontier agents mean for the future of software engineering jobs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The announcement inevitably raises concerns about the impact on software engineering jobs. Singh pushed back against the notion that frontier agents will replace developers, framing them instead as tools that amplify human capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;Software engineering is craft. What&amp;#x27;s changing is not, &amp;#x27;Hey, agents are doing all the work.&amp;#x27; The craft of software engineering is changing—how you use agents, how do you set up your code base, how do you set up your prompts, how do you set up your rules, how do you set up your knowledge bases so that agents can be effective,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;Singh noted that senior engineers who had drifted away from hands-on coding are now writing more code than ever. &amp;quot;It&amp;#x27;s actually easier for them to become software engineers,&amp;quot; he said.&lt;/p&gt;&lt;p&gt;He pointed to an internal example where a team completed a project in 78 days that would have taken 18 months using traditional practices. &amp;quot;Because they were able to use AI. And the thing that made it work was not just the fact that they were using AI, but how they organized and set up their practices of how they built that software were maximized around that.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Amazon plans to make AI-generated code more trustworthy over time&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Singh outlined several areas where frontier agents will evolve over the coming years. Multi-agent architectures, where systems of specialized agents coordinate to solve complex problems, represent a major frontier. So does the integration of formal verification techniques to increase confidence in AI-generated code.&lt;/p&gt;&lt;p&gt;AWS recently introduced property-based testing in Kiro, which uses automated reasoning to extract testable properties from specifications and generate thousands of test scenarios automatically.&lt;/p&gt;&lt;p&gt;&amp;quot;If you have a shopping cart application, every way an order can be canceled, and how it might be canceled, and the way refunds are handled in Germany versus the US—if you&amp;#x27;re writing a unit test, maybe two, Germany and US, but now, because you have this property-based testing approach, your agent can create a scenario for every country you operate in and test all of them automatically for you,&amp;quot; Singh explained.&lt;/p&gt;&lt;p&gt;Building trust in autonomous systems remains the central challenge. &amp;quot;Right now you still require tons of human guardrails at every step to make sure that the right thing happens. And as we get better at these techniques, you will use less and less, and you&amp;#x27;ll be able to trust the agents a lot more,&amp;quot; he said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Amazon&amp;#x27;s bigger bet on autonomous AI stretches far beyond writing code&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The frontier agents announcement arrived alongside a cascade of other news at &lt;a href="https://www.aboutamazon.com/news/aws/aws-re-invent-2025-ai-news-updates"&gt;&lt;u&gt;re:Invent 2025&lt;/u&gt;&lt;/a&gt;. AWS kicked off the conference with major announcements on agentic AI capabilities, customer service innovations, and multicloud networking.&lt;/p&gt;&lt;p&gt;Amazon expanded its Nova portfolio with &lt;a href="https://www.aboutamazon.com/news/aws/aws-agentic-ai-amazon-bedrock-nova-models"&gt;&lt;u&gt;four new models&lt;/u&gt;&lt;/a&gt; delivering industry-leading price-performance across reasoning, multimodal processing, conversational AI, code generation, and agentic tasks. Nova Forge pioneers &amp;quot;open training,&amp;quot; giving organizations access to pre-trained model checkpoints and the ability to blend proprietary data with Amazon Nova-curated datasets.&lt;/p&gt;&lt;p&gt;AWS also added &lt;a href="https://aws.amazon.com/blogs/aws/amazon-bedrock-adds-fully-managed-open-weight-models/"&gt;&lt;u&gt;18 new open weight models to Amazon Bedrock&lt;/u&gt;&lt;/a&gt;, reinforcing its commitment to offering a broad selection of fully managed models from leading AI providers. The launch includes new models from Mistral AI, Google&amp;#x27;s Gemma 3, MiniMax&amp;#x27;s M2, NVIDIA&amp;#x27;s Nemotron, and OpenAI&amp;#x27;s GPT OSS Safeguard.&lt;/p&gt;&lt;p&gt;On the infrastructure side, &lt;a href="https://aws.amazon.com/about-aws/whats-new/2025/12/amazon-ec2-trn3-ultraservers/"&gt;&lt;u&gt;Amazon EC2 Trn3 UltraServers&lt;/u&gt;&lt;/a&gt;, powered by AWS&amp;#x27;s first 3nm AI chip, pack up to 144 Trainium3 chips into a single integrated system, delivering up to 4.4x more compute performance and 4x greater energy efficiency than the previous generation. AWS AI Factories provides enterprises and government organizations with dedicated AWS AI infrastructure deployed in their own data centers, combining NVIDIA GPUs, Trainium chips, AWS networking, and AI services like Amazon Bedrock and SageMaker AI.&lt;/p&gt;&lt;p&gt;All three frontier agents launched in preview on Tuesday. Pricing will be announced when the services reach general availability.&lt;/p&gt;&lt;p&gt;Singh made clear the company sees applications far beyond coding. &amp;quot;These are the first frontier agents we are releasing, and they&amp;#x27;re in the software development lifecycle,&amp;quot; he said. &amp;quot;The problems and use cases for frontier agents—these agents that are long running, capable of autonomy, thinking, always learning and improving—can be applied to many, many domains.&amp;quot;&lt;/p&gt;&lt;p&gt;Amazon, after all, operates satellite networks, runs robotics warehouses, and manages one of the world&amp;#x27;s largest e-commerce platforms. If autonomous agents can learn to write code on their own, the company is betting they can eventually learn to do just about anything else.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/amazons-new-ai-can-code-for-days-without-human-help-what-does-that-mean-for</guid><pubDate>Tue, 02 Dec 2025 17:30:00 +0000</pubDate></item><item><title>AWS launches new Nova AI models and a service that gives customers more control (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/aws-launches-new-nova-ai-models-and-a-service-that-gives-customers-more-control/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/IMG_6848.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services is rolling out a slate of new homegrown AI models and a service for enterprise customers to build their own custom versions. The cloud provider launched Nova 2, a fleet of four new AI models to its Nova model family, during AWS CEO Matt Garman’s AWS re:Invent keynote on Tuesday. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first version of AWS Nova was announced last year at the company’s annual tech conference. At the time, the company released four text-generating models and one image-generating model. This year, AWS is giving the models an upgrade and launching an accompanying service.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The momentum has been really fantastic,” Garman said during his Tuesday keynote. “Nova has been, has grown to be used by tens of thousands of customers today, everyone from marketing giants to tech leaders like Infosys or Blue Origin or Robinhood to innovative startups like NinjaTech AI and today, we’re making Nova even better.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The four new models include Nova 2 Lite, a more cost-effective reasoning model. Reasoning AI models “think” before they respond and can process text, images, and videos to generate text that’s meant for everyday tasks. Nova 2 Pro is a reasoning agent that can process text, images, videos and speech that is designed for “highly complex tasks” like coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nova 2 Sonic is a new speech-to-speech model to be used for conversational AI. Nova 2 Omni is a multimodal reasoning and generation model that can process images, text, video and speech input, and produce both text and images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the model upgrades, AWS also announced a new service called Nova Forge which allows AWS cloud customers to build their own frontier version of AWS Nova models called Novellas for $100,000 a year, according to CNBC reporting. This service allows enterprises to access pre-trained, mid-trained or post-trained models for companies to then train on their own proprietary data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Garman said this will be able to solve some of the problems that arise when enterprises try to incorporate their own data into already-trained AI models.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The more you customize models, the more you add a bunch of data in post training, these models tend to forget some of that interesting stuff that it learned earlier the core reasoning,” Garman said. “It’s a little bit like humans trying to learn new language. When you start when you’re really young, it’s actually relatively easy to pick up, but when you try to, you learn a new language later in life, it’s actually much, much harder. Model training is kind of like this too.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Companies including Reddit, Sony and Booking.com are early Nova Forge customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual enterprise tech event here.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/IMG_6848.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services is rolling out a slate of new homegrown AI models and a service for enterprise customers to build their own custom versions. The cloud provider launched Nova 2, a fleet of four new AI models to its Nova model family, during AWS CEO Matt Garman’s AWS re:Invent keynote on Tuesday. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first version of AWS Nova was announced last year at the company’s annual tech conference. At the time, the company released four text-generating models and one image-generating model. This year, AWS is giving the models an upgrade and launching an accompanying service.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The momentum has been really fantastic,” Garman said during his Tuesday keynote. “Nova has been, has grown to be used by tens of thousands of customers today, everyone from marketing giants to tech leaders like Infosys or Blue Origin or Robinhood to innovative startups like NinjaTech AI and today, we’re making Nova even better.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The four new models include Nova 2 Lite, a more cost-effective reasoning model. Reasoning AI models “think” before they respond and can process text, images, and videos to generate text that’s meant for everyday tasks. Nova 2 Pro is a reasoning agent that can process text, images, videos and speech that is designed for “highly complex tasks” like coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nova 2 Sonic is a new speech-to-speech model to be used for conversational AI. Nova 2 Omni is a multimodal reasoning and generation model that can process images, text, video and speech input, and produce both text and images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the model upgrades, AWS also announced a new service called Nova Forge which allows AWS cloud customers to build their own frontier version of AWS Nova models called Novellas for $100,000 a year, according to CNBC reporting. This service allows enterprises to access pre-trained, mid-trained or post-trained models for companies to then train on their own proprietary data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Garman said this will be able to solve some of the problems that arise when enterprises try to incorporate their own data into already-trained AI models.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The more you customize models, the more you add a bunch of data in post training, these models tend to forget some of that interesting stuff that it learned earlier the core reasoning,” Garman said. “It’s a little bit like humans trying to learn new language. When you start when you’re really young, it’s actually relatively easy to pick up, but when you try to, you learn a new language later in life, it’s actually much, much harder. Model training is kind of like this too.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Companies including Reddit, Sony and Booking.com are early Nova Forge customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Follow along with all of TechCrunch’s coverage of the annual enterprise tech event here.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/aws-launches-new-nova-ai-models-and-a-service-that-gives-customers-more-control/</guid><pubDate>Tue, 02 Dec 2025 17:54:02 +0000</pubDate></item><item><title>ChatGPT referrals to retailers’ apps increased 28% year-over-year, says report (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/chatgpt-referrals-to-retailers-apps-increased-28-year-over-year-says-report/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/01/GettyImages-1336136316.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;New data shows ChatGPT’s growing influence as a referrer to e-commerce websites, as well as how small its slice of this market is currently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a new analysis by mobile app insights provider Apptopia, ChatGPT referrals to retailer mobile apps increased 28% year-over-year over the Black Friday holiday shopping weekend, running from Thanksgiving Day on Thursday through Sunday.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, the use of ChatGPT may not be benefiting smaller retailers as much as it’s helping to further entrench the e-commerce giants Amazon and Walmart. This year, Amazon’s share of ChatGPT referrals grew to 54%, up from 40.5% in 2024. Walmart’s share, meanwhile, increased from 2.7% last year to now 14.9%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The data was collected by Apptopia’s U.S. panel, which is based on observed consumer activity on mobile devices. As it’s not first-party data, its figures are only estimates. For this analysis, the firm defined a referral session as a retail mobile app session that directly followed (within 30 seconds) a ChatGPT session.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the big jump from 2024, consumers’ use of AI chatbots to find e-commerce deals is still a small sliver of the overall referral market, Apptopia noted. Last year, ChatGPT’s referrals to e-commerce apps were only 0.64% of all ChatGPT sessions on Black Friday, and that figure only grew to 0.82% this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, a referral session was anytime ChatGPT either gave the searcher a shopping idea or the user clicked a link directly from their chat session that brought them to the retail app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apptopia isn’t the only firm digging into how AI is impacting e-commerce during the busy holiday shopping season. Adobe also reported this week that AI traffic to U.S. retail sites (measured by shoppers clicking on a link) increased by 805% year-over-year on Black Friday, and those who landed on a retail site from an AI chatbot were 38% more likely to make a purchase.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Adobe said that AI traffic to U.S. retail sites on Cyber Monday increased by 670%. In the holiday shopping season so far (November 1 to December 1), AI traffic is up 760%, it noted.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flags&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/01/GettyImages-1336136316.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;New data shows ChatGPT’s growing influence as a referrer to e-commerce websites, as well as how small its slice of this market is currently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a new analysis by mobile app insights provider Apptopia, ChatGPT referrals to retailer mobile apps increased 28% year-over-year over the Black Friday holiday shopping weekend, running from Thanksgiving Day on Thursday through Sunday.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, the use of ChatGPT may not be benefiting smaller retailers as much as it’s helping to further entrench the e-commerce giants Amazon and Walmart. This year, Amazon’s share of ChatGPT referrals grew to 54%, up from 40.5% in 2024. Walmart’s share, meanwhile, increased from 2.7% last year to now 14.9%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The data was collected by Apptopia’s U.S. panel, which is based on observed consumer activity on mobile devices. As it’s not first-party data, its figures are only estimates. For this analysis, the firm defined a referral session as a retail mobile app session that directly followed (within 30 seconds) a ChatGPT session.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the big jump from 2024, consumers’ use of AI chatbots to find e-commerce deals is still a small sliver of the overall referral market, Apptopia noted. Last year, ChatGPT’s referrals to e-commerce apps were only 0.64% of all ChatGPT sessions on Black Friday, and that figure only grew to 0.82% this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, a referral session was anytime ChatGPT either gave the searcher a shopping idea or the user clicked a link directly from their chat session that brought them to the retail app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apptopia isn’t the only firm digging into how AI is impacting e-commerce during the busy holiday shopping season. Adobe also reported this week that AI traffic to U.S. retail sites (measured by shoppers clicking on a link) increased by 805% year-over-year on Black Friday, and those who landed on a retail site from an AI chatbot were 38% more likely to make a purchase.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Adobe said that AI traffic to U.S. retail sites on Cyber Monday increased by 670%. In the holiday shopping season so far (November 1 to December 1), AI traffic is up 760%, it noted.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flags&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/chatgpt-referrals-to-retailers-apps-increased-28-year-over-year-says-report/</guid><pubDate>Tue, 02 Dec 2025 17:56:16 +0000</pubDate></item><item><title>[NEW] NVIDIA Partners With Mistral AI to Accelerate New Family of Open Models (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/mistral-frontier-open-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/genai-tech-blog-gtc25-paris-nv-mistral-1920x1080-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;span&gt;Today, &lt;/span&gt;&lt;span&gt;Mistral AI announced the Mistral 3 family of open-source multilingual, multimodal models, optimized across NVIDIA supercomputing and edge platforms. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mistral Large 3 is a mixture-of-experts (MoE) model&lt;/span&gt;&lt;span&gt; — i&lt;/span&gt;&lt;span&gt;nstead of firing up every neuron for every token, it only activates the parts of the model with the most impact. The result is efficiency that delivers scale without waste, accuracy without compromise and makes enterprise AI not just possible, but practical.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mistral AI’s new models deliver industry-leading accuracy and efficiency for enterprise AI. It will be available everywhere, from the cloud to the data center to the edge, starting Tuesday, Dec. 2.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With 41B active parameters, 675B total parameters and a large 256K context window, Mistral Large 3 delivers scalability, efficiency and adaptability for enterprise AI workloads. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;By combining NVIDIA GB200 NVL72 systems and Mistral AI’s MoE architecture&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt; enterprises can efficiently deploy and scale&amp;nbsp;massive AI models, benefiting from advanced parallelism and hardware optimizations. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;This combination makes the announcement a step toward the era of&amp;nbsp;&lt;/span&gt;&lt;span&gt;—&lt;/span&gt;&lt;span&gt;&amp;nbsp;what Mistral AI calls ‘distributed intelligence,’ bridging the gap between research breakthroughs and real-world applications.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The model’s granular MoE architecture unlocks the full performance benefits of large-scale expert parallelism by tapping into NVIDIA NVLink’s coherent memory domain and using wide expert parallelism optimizations.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;These benefits stack with accuracy-preserving, low-precision NVFP4 and NVIDIA Dynamo disaggregated inference optimizations, ensuring peak performance for large-scale training and inference.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;On the GB200 NVL72, Mistral Large 3 achieved &lt;/span&gt;&lt;span class="FindHit CommentStart SCXW137788108 BCX0"&gt;10x&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt; performance gain compared &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;with &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;the prior&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;–&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;generation NVIDIA H200.&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt; This generational gain translates into &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;a &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;better user experience, lower per-toke&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;n &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;co&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;st&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt; and higher energy efficiency.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mistral &lt;/span&gt;&lt;span&gt;AI &lt;/span&gt;&lt;span&gt;isn’t just driving state of the art for frontier large language models; it also released nine small language models that help developers run AI anywhere. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The compact Ministral 3 suite is optimized to run across NVIDIA’s edge platforms, including NVIDIA Spark, RTX PCs and laptops and NVIDIA Jetson devices.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;To deliver peak performance, NVIDIA collaborates on top AI frameworks such as &lt;/span&gt;&lt;span&gt;Llama.cpp&lt;/span&gt;&lt;span&gt; and &lt;/span&gt;&lt;span&gt;Ollama&lt;/span&gt;&lt;span&gt; to deliver peak performance across NVIDIA GPUs on the edge.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Today, developers and enthusiasts can try out the Ministral 3 suite via Llama.cpp and Ollama for fast and efficient AI on the edge.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The Mistral 3 family of models is &lt;/span&gt;&lt;span&gt;openly available, empowering researchers and developers everywhere to experiment, customize and accelerate AI innovation while democratizing access to frontier-class technologies.&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;By linking Mistral AI’s models to open-source NVIDIA NeMo tools for AI agent lifecycle development — Data Designer, Customizer, Guardrails and NeMo Agent Toolkit — enterprises can customize these models further for their own use cases, making it faster to move from prototype to production.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;And to achieve efficiency from cloud to edge, NVIDIA has optimized inference frameworks including NVIDIA TensorRT-LLM, &lt;/span&gt;&lt;span&gt;SGLang&lt;/span&gt;&lt;span&gt; and &lt;/span&gt;&lt;span&gt;v&lt;/span&gt;&lt;span&gt;LLM&lt;/span&gt;&lt;span&gt; for the Mistral 3 model family.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mistral 3 is available today on leading open-source platforms and cloud service providers. In addition, the models are expected to be deployable soon as NVIDIA NIM microservices.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Wherever AI needs to go, these models are ready.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;&lt;span&gt;See &lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;notice&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt; regarding software product information.&lt;/span&gt;&lt;/i&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/genai-tech-blog-gtc25-paris-nv-mistral-1920x1080-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;span&gt;Today, &lt;/span&gt;&lt;span&gt;Mistral AI announced the Mistral 3 family of open-source multilingual, multimodal models, optimized across NVIDIA supercomputing and edge platforms. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mistral Large 3 is a mixture-of-experts (MoE) model&lt;/span&gt;&lt;span&gt; — i&lt;/span&gt;&lt;span&gt;nstead of firing up every neuron for every token, it only activates the parts of the model with the most impact. The result is efficiency that delivers scale without waste, accuracy without compromise and makes enterprise AI not just possible, but practical.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mistral AI’s new models deliver industry-leading accuracy and efficiency for enterprise AI. It will be available everywhere, from the cloud to the data center to the edge, starting Tuesday, Dec. 2.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With 41B active parameters, 675B total parameters and a large 256K context window, Mistral Large 3 delivers scalability, efficiency and adaptability for enterprise AI workloads. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;By combining NVIDIA GB200 NVL72 systems and Mistral AI’s MoE architecture&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt; enterprises can efficiently deploy and scale&amp;nbsp;massive AI models, benefiting from advanced parallelism and hardware optimizations. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;This combination makes the announcement a step toward the era of&amp;nbsp;&lt;/span&gt;&lt;span&gt;—&lt;/span&gt;&lt;span&gt;&amp;nbsp;what Mistral AI calls ‘distributed intelligence,’ bridging the gap between research breakthroughs and real-world applications.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The model’s granular MoE architecture unlocks the full performance benefits of large-scale expert parallelism by tapping into NVIDIA NVLink’s coherent memory domain and using wide expert parallelism optimizations.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;These benefits stack with accuracy-preserving, low-precision NVFP4 and NVIDIA Dynamo disaggregated inference optimizations, ensuring peak performance for large-scale training and inference.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;On the GB200 NVL72, Mistral Large 3 achieved &lt;/span&gt;&lt;span class="FindHit CommentStart SCXW137788108 BCX0"&gt;10x&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt; performance gain compared &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;with &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;the prior&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;–&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;generation NVIDIA H200.&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt; This generational gain translates into &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;a &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;better user experience, lower per-toke&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;n &lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;co&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt;st&lt;/span&gt;&lt;span class="NormalTextRun SCXW137788108 BCX0"&gt; and higher energy efficiency.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mistral &lt;/span&gt;&lt;span&gt;AI &lt;/span&gt;&lt;span&gt;isn’t just driving state of the art for frontier large language models; it also released nine small language models that help developers run AI anywhere. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The compact Ministral 3 suite is optimized to run across NVIDIA’s edge platforms, including NVIDIA Spark, RTX PCs and laptops and NVIDIA Jetson devices.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;To deliver peak performance, NVIDIA collaborates on top AI frameworks such as &lt;/span&gt;&lt;span&gt;Llama.cpp&lt;/span&gt;&lt;span&gt; and &lt;/span&gt;&lt;span&gt;Ollama&lt;/span&gt;&lt;span&gt; to deliver peak performance across NVIDIA GPUs on the edge.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Today, developers and enthusiasts can try out the Ministral 3 suite via Llama.cpp and Ollama for fast and efficient AI on the edge.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The Mistral 3 family of models is &lt;/span&gt;&lt;span&gt;openly available, empowering researchers and developers everywhere to experiment, customize and accelerate AI innovation while democratizing access to frontier-class technologies.&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;By linking Mistral AI’s models to open-source NVIDIA NeMo tools for AI agent lifecycle development — Data Designer, Customizer, Guardrails and NeMo Agent Toolkit — enterprises can customize these models further for their own use cases, making it faster to move from prototype to production.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;And to achieve efficiency from cloud to edge, NVIDIA has optimized inference frameworks including NVIDIA TensorRT-LLM, &lt;/span&gt;&lt;span&gt;SGLang&lt;/span&gt;&lt;span&gt; and &lt;/span&gt;&lt;span&gt;v&lt;/span&gt;&lt;span&gt;LLM&lt;/span&gt;&lt;span&gt; for the Mistral 3 model family.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mistral 3 is available today on leading open-source platforms and cloud service providers. In addition, the models are expected to be deployable soon as NVIDIA NIM microservices.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Wherever AI needs to go, these models are ready.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;&lt;span&gt;See &lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;notice&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt; regarding software product information.&lt;/span&gt;&lt;/i&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/mistral-frontier-open-models/</guid><pubDate>Tue, 02 Dec 2025 18:00:30 +0000</pubDate></item><item><title>[NEW] New control system teaches soft robots the art of staying safe (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-control-system-teaches-soft-robots-art-staying-safe-1202</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-csail-Contact-Aware.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-8adfd4f6-7fff-d047-727d-cc029b78b3ad"&gt;Imagine having a continuum soft robotic arm bend around a bunch of grapes or broccoli, adjusting its grip in real time as it lifts the object. Unlike traditional rigid robots that generally aim to avoid contact with the environment as much as possible and stay far away from humans for safety reasons, this arm senses subtle forces, stretching and flexing in ways that mimic more of the compliance of a human hand. Its every motion is calculated to avoid excessive force while achieving the task efficiently. In MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Laboratory for Information and Decisions Systems (LIDS) labs, these seemingly simple movements are the culmination of complex mathematics, careful engineering, and a vision for robots that can safely interact with humans and delicate objects.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots, with their deformable bodies, promise a future where machines move more seamlessly alongside people, assist in caregiving, or handle delicate items in industrial settings. Yet that very flexibility makes them difficult to control. Small bends or twists can produce unpredictable forces, raising the risk of damage or injury. This motivates the need for safe control strategies for soft robots.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Inspired by advances in safe control and formal methods for rigid robots, we aim to adapt these ideas to soft robotics — modeling their complex behavior and embracing, rather than avoiding, contact — to enable higher-performance designs (e.g., greater payload and precision) without sacrificing safety or embodied intelligence,” says lead senior author and MIT Assistant Professor Gioele Zardini, who is a principal investigator in LIDS and the Department of Civil and Environmental Engineering, and an affiliate faculty with the Institute for Data, Systems, and Society (IDSS). “This vision is shared by recent and parallel work from other groups.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Safety first&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The team developed a new framework that blends nonlinear control theory (controlling systems that involve highly complex dynamics) with advanced physical modeling techniques and efficient real-time optimization to produce what they call “contact-aware safety.” At the heart of the approach are high-order control barrier functions (HOCBFs) and high-order control Lyapunov functions (HOCLFs). HOCBFs define safe operating boundaries, ensuring the robot doesn’t exert unsafe forces. HOCLFs guide the robot efficiently toward its task objectives, balancing safety with performance.&lt;/p&gt;&lt;p dir="ltr"&gt;“Essentially, we’re teaching the robot to know its own limits when interacting with the environment while still achieving its goals,” says MIT Department of Mechanical Engineering PhD student Kiwan Wong, the lead author of a new paper describing the framework. “The approach involves some complex derivation of soft robot dynamics, contact models, and control constraints, but the specification of control objectives and safety barriers is rather straightforward for the practitioner, and the outcomes are very tangible, as you see the robot moving smoothly, reacting to contact, and never causing unsafe situations.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Compared with traditional kinematic CBFs — where forward-invariant safe sets are hard to specify — the HOCBF framework simplifies barrier design, and its optimization formulation accounts for system dynamics (e.g., inertia), ensuring the soft robot stops early enough to avoid unsafe contact forces,” says Worcester Polytechnic Institute Assistant Professor and former CSAIL postdoc Wei Xiao.&lt;/p&gt;&lt;p dir="ltr"&gt;“Since soft robots emerged, the field has highlighted their embodied intelligence and greater inherent safety relative to rigid robots, thanks to passive material and structural compliance. Yet their “cognitive” intelligence — especially safety systems — has lagged behind that of rigid serial-link manipulators,” says co-lead author Maximilian Stölzle, a research intern at Disney Research and formerly a Delft University of Technology PhD student and visiting researcher at MIT LIDS and CSAIL. “This work helps close that gap by adapting proven algorithms to soft robots and tailoring them for safe contact and soft-continuum dynamics.”&lt;/p&gt;&lt;p dir="ltr"&gt;The LIDS and CSAIL team tested the system on a series of experiments designed to challenge the robot’s safety and adaptability. In one test, the arm pressed gently against a compliant surface, maintaining a precise force without overshooting. In another, it traced the contours of a curved object, adjusting its grip to avoid slippage. In yet another demonstration, the robot manipulated fragile items alongside a human operator, reacting in real time to unexpected nudges or shifts. “These experiments show that our framework is able to generalize to diverse tasks and objectives, and the robot can sense, adapt, and act in complex scenarios while always respecting clearly defined safety limits,” says Zardini.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots with contact-aware safety could be a real value-add in high-stakes places, of course. In health care, they could assist in surgeries, providing precise manipulation while reducing risk to patients. In industry, they might handle fragile goods without constant supervision. In domestic settings, robots could help with chores or caregiving tasks, interacting safely with children or the elderly — a key step toward making soft robots reliable partners in real-world environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots have incredible potential,” says co-lead senior author Daniela Rus, director of CSAIL and a professor in the Department of Electrical Engineering and Computer Science. “But ensuring safety and encoding motion tasks via relatively simple objectives has always been a central challenge. We wanted to create a system where the robot can remain flexible and responsive while mathematically guaranteeing it won’t exceed safe force limits.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Combining soft robot models, differentiable simulation, and control theory&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Underlying the control strategy is a differentiable implementation of something called the Piecewise Cosserat-Segment (PCS) dynamics model, which predicts how a soft robot deforms and where forces accumulate. This model allows the system to anticipate how the robot’s body will respond to actuation and complex interactions with the environment. “The aspect that I most like about this work is the blend of integration of new and old tools coming from different fields like advanced soft robot models, differentiable simulation, Lyapunov theory, convex optimization, and injury-severity–based safety constraints. All of this is nicely blended into a real-time controller fully grounded in first principles,” says co-author Cosimo Della Santina, who is an associate professor at Delft University of Technology.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Complementing this is the Differentiable Conservative Separating Axis Theorem (DCSAT), which estimates distances between the soft robot and obstacles in the environment that can be approximated with a chain of convex polygons in a differentiable manner. “Earlier differentiable distance metrics for convex polygons either couldn’t compute penetration depth — essential for estimating contact forces — or yielded non-conservative estimates that could compromise safety,” says Wong. “Instead, the DCSAT metric returns strictly conservative, and therefore safe, estimates while simultaneously allowing for fast and differentiable computation.” Together, PCS and DCSAT give the robot a predictive sense of its environment for more proactive, safe interactions.&lt;/p&gt;&lt;p dir="ltr"&gt;Looking ahead, the team plans to extend their methods to three-dimensional soft robots and explore integration with learning-based strategies. By combining contact-aware safety with adaptive learning, soft robots could handle even more complex, unpredictable environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“This is what makes our work exciting,” says Rus. “You can see the robot behaving in a human-like, careful manner, but behind that grace is a rigorous control framework ensuring it never oversteps its bounds.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots are generally safer to interact with than rigid-bodied robots by design, due to the compliance and energy-absorbing properties of their bodies,” says University of Michigan Assistant Professor Daniel Bruder, who wasn’t involved in the research. “However, as soft robots become faster, stronger, and more capable, that may no longer be enough to ensure safety. This work takes a crucial step towards ensuring soft robots can operate safely by offering a method to limit contact forces across their entire bodies.”&lt;/p&gt;&lt;p&gt;The team’s work was supported, in part, by The Hong Kong Jockey Club Scholarships, the European Union’s Horizon Europe Program, Cultuurfonds Wetenschapsbeurzen, and the Rudge (1948) and Nancy Allen Chair. Their work was published earlier this month in the Institute of Electrical and Electronics Engineers’ &lt;em&gt;Robotics and Automation Letters&lt;/em&gt;.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-csail-Contact-Aware.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-8adfd4f6-7fff-d047-727d-cc029b78b3ad"&gt;Imagine having a continuum soft robotic arm bend around a bunch of grapes or broccoli, adjusting its grip in real time as it lifts the object. Unlike traditional rigid robots that generally aim to avoid contact with the environment as much as possible and stay far away from humans for safety reasons, this arm senses subtle forces, stretching and flexing in ways that mimic more of the compliance of a human hand. Its every motion is calculated to avoid excessive force while achieving the task efficiently. In MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Laboratory for Information and Decisions Systems (LIDS) labs, these seemingly simple movements are the culmination of complex mathematics, careful engineering, and a vision for robots that can safely interact with humans and delicate objects.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots, with their deformable bodies, promise a future where machines move more seamlessly alongside people, assist in caregiving, or handle delicate items in industrial settings. Yet that very flexibility makes them difficult to control. Small bends or twists can produce unpredictable forces, raising the risk of damage or injury. This motivates the need for safe control strategies for soft robots.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Inspired by advances in safe control and formal methods for rigid robots, we aim to adapt these ideas to soft robotics — modeling their complex behavior and embracing, rather than avoiding, contact — to enable higher-performance designs (e.g., greater payload and precision) without sacrificing safety or embodied intelligence,” says lead senior author and MIT Assistant Professor Gioele Zardini, who is a principal investigator in LIDS and the Department of Civil and Environmental Engineering, and an affiliate faculty with the Institute for Data, Systems, and Society (IDSS). “This vision is shared by recent and parallel work from other groups.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Safety first&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The team developed a new framework that blends nonlinear control theory (controlling systems that involve highly complex dynamics) with advanced physical modeling techniques and efficient real-time optimization to produce what they call “contact-aware safety.” At the heart of the approach are high-order control barrier functions (HOCBFs) and high-order control Lyapunov functions (HOCLFs). HOCBFs define safe operating boundaries, ensuring the robot doesn’t exert unsafe forces. HOCLFs guide the robot efficiently toward its task objectives, balancing safety with performance.&lt;/p&gt;&lt;p dir="ltr"&gt;“Essentially, we’re teaching the robot to know its own limits when interacting with the environment while still achieving its goals,” says MIT Department of Mechanical Engineering PhD student Kiwan Wong, the lead author of a new paper describing the framework. “The approach involves some complex derivation of soft robot dynamics, contact models, and control constraints, but the specification of control objectives and safety barriers is rather straightforward for the practitioner, and the outcomes are very tangible, as you see the robot moving smoothly, reacting to contact, and never causing unsafe situations.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Compared with traditional kinematic CBFs — where forward-invariant safe sets are hard to specify — the HOCBF framework simplifies barrier design, and its optimization formulation accounts for system dynamics (e.g., inertia), ensuring the soft robot stops early enough to avoid unsafe contact forces,” says Worcester Polytechnic Institute Assistant Professor and former CSAIL postdoc Wei Xiao.&lt;/p&gt;&lt;p dir="ltr"&gt;“Since soft robots emerged, the field has highlighted their embodied intelligence and greater inherent safety relative to rigid robots, thanks to passive material and structural compliance. Yet their “cognitive” intelligence — especially safety systems — has lagged behind that of rigid serial-link manipulators,” says co-lead author Maximilian Stölzle, a research intern at Disney Research and formerly a Delft University of Technology PhD student and visiting researcher at MIT LIDS and CSAIL. “This work helps close that gap by adapting proven algorithms to soft robots and tailoring them for safe contact and soft-continuum dynamics.”&lt;/p&gt;&lt;p dir="ltr"&gt;The LIDS and CSAIL team tested the system on a series of experiments designed to challenge the robot’s safety and adaptability. In one test, the arm pressed gently against a compliant surface, maintaining a precise force without overshooting. In another, it traced the contours of a curved object, adjusting its grip to avoid slippage. In yet another demonstration, the robot manipulated fragile items alongside a human operator, reacting in real time to unexpected nudges or shifts. “These experiments show that our framework is able to generalize to diverse tasks and objectives, and the robot can sense, adapt, and act in complex scenarios while always respecting clearly defined safety limits,” says Zardini.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots with contact-aware safety could be a real value-add in high-stakes places, of course. In health care, they could assist in surgeries, providing precise manipulation while reducing risk to patients. In industry, they might handle fragile goods without constant supervision. In domestic settings, robots could help with chores or caregiving tasks, interacting safely with children or the elderly — a key step toward making soft robots reliable partners in real-world environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots have incredible potential,” says co-lead senior author Daniela Rus, director of CSAIL and a professor in the Department of Electrical Engineering and Computer Science. “But ensuring safety and encoding motion tasks via relatively simple objectives has always been a central challenge. We wanted to create a system where the robot can remain flexible and responsive while mathematically guaranteeing it won’t exceed safe force limits.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Combining soft robot models, differentiable simulation, and control theory&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Underlying the control strategy is a differentiable implementation of something called the Piecewise Cosserat-Segment (PCS) dynamics model, which predicts how a soft robot deforms and where forces accumulate. This model allows the system to anticipate how the robot’s body will respond to actuation and complex interactions with the environment. “The aspect that I most like about this work is the blend of integration of new and old tools coming from different fields like advanced soft robot models, differentiable simulation, Lyapunov theory, convex optimization, and injury-severity–based safety constraints. All of this is nicely blended into a real-time controller fully grounded in first principles,” says co-author Cosimo Della Santina, who is an associate professor at Delft University of Technology.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Complementing this is the Differentiable Conservative Separating Axis Theorem (DCSAT), which estimates distances between the soft robot and obstacles in the environment that can be approximated with a chain of convex polygons in a differentiable manner. “Earlier differentiable distance metrics for convex polygons either couldn’t compute penetration depth — essential for estimating contact forces — or yielded non-conservative estimates that could compromise safety,” says Wong. “Instead, the DCSAT metric returns strictly conservative, and therefore safe, estimates while simultaneously allowing for fast and differentiable computation.” Together, PCS and DCSAT give the robot a predictive sense of its environment for more proactive, safe interactions.&lt;/p&gt;&lt;p dir="ltr"&gt;Looking ahead, the team plans to extend their methods to three-dimensional soft robots and explore integration with learning-based strategies. By combining contact-aware safety with adaptive learning, soft robots could handle even more complex, unpredictable environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“This is what makes our work exciting,” says Rus. “You can see the robot behaving in a human-like, careful manner, but behind that grace is a rigorous control framework ensuring it never oversteps its bounds.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots are generally safer to interact with than rigid-bodied robots by design, due to the compliance and energy-absorbing properties of their bodies,” says University of Michigan Assistant Professor Daniel Bruder, who wasn’t involved in the research. “However, as soft robots become faster, stronger, and more capable, that may no longer be enough to ensure safety. This work takes a crucial step towards ensuring soft robots can operate safely by offering a method to limit contact forces across their entire bodies.”&lt;/p&gt;&lt;p&gt;The team’s work was supported, in part, by The Hong Kong Jockey Club Scholarships, the European Union’s Horizon Europe Program, Cultuurfonds Wetenschapsbeurzen, and the Rudge (1948) and Nancy Allen Chair. Their work was published earlier this month in the Institute of Electrical and Electronics Engineers’ &lt;em&gt;Robotics and Automation Letters&lt;/em&gt;.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-control-system-teaches-soft-robots-art-staying-safe-1202</guid><pubDate>Tue, 02 Dec 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] Android 16 adds AI notification summaries, new customization options, and more (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/android-16-adds-ai-notification-summaries-new-customization-options-and-more/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it is releasing a slew of Android 16 updates, along with new general Android and accessibility features. The rollout of the new Android 16 features, which are first coming to Pixel devices, marks a new chapter in how Android updates are delivered, as the company is moving from a single yearly update to more frequent releases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Android 16 is adding AI-powered notification summaries that condense long messages and group chats into quick, glanceable overviews. A new “Notification organizer” will automatically group and silence lower-priority notifications, such as promotions, news, and social alerts.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The update also brings more ways to customize devices, with users getting access to custom icon shapes, themed icons, and the option to automatically darken light apps, even those that don’t have their own native dark theme.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, there’s a new Parental Controls option within Android Settings that allows parents to set screen time limits, create downtime schedules, control app usage, and more for their children.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3071853" height="519" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-02-at-12.51.39-PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;These updates are starting to roll out with Android 16 on eligible Pixel devices starting Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also releasing several new Android features that aren’t specific to Android 16. A new beta feature called “Call Reason” allows users to flag calls to saved contacts as “urgent.” Recipients will see this on their incoming call screen and know it’s time-sensitive. If they miss the call, the “urgent” note will stay in their call history.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also launching “Expressive Captions” that display the full emotion of speech with tags like [sad] or [joyful], whether it’s a video message or a post on social media. The company says this will allow users to glean the full context of what’s being said when the sound is off.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant is making it easier to spot and exit unwanted group chats. If an unknown number invites a user to a group, they’ll get an alert that shows key information about the group. The user can then quickly choose to reply, leave the chat, or block and report the number.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Pinned tabs in Chrome now work the same way as on desktop, which means pinned pages stay saved at the front of the browser, letting users pick up where they left off.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3071855" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/2.-Expressive-Captions.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also updating Circle to Search, its feature that&amp;nbsp;allows users to&amp;nbsp;search from anywhere on their phone&amp;nbsp;by using gestures like circling, highlighting, scribbling, or tapping. Users can now analyze suspicious messages with the feature — after initiating Circle to Search, an AI Overview will appear indicating whether the message is likely a scam.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For accessibility updates, Google is enhancing its “Guided Frame” feature in the Pixel camera app. Previously, the feature has notified users about things like a face in the frame. Now, it will provide a more in-depth description, such as “one girl with a yellow T-shirt sits on the sofa and looks at the dog.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, users no longer need to physically tap their phone to start using Voice Access, which allows users to control their Android devices with voice commands. Now, users just need to say “Hey Google, start Voice Access” to begin controlling their phone hands-free.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also launching Fast Pair for hearing aids, starting with hearing aids from Demant, a Danish company that owns several major hearing aid brands, including Oticon, Sonic, and Bernafon.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it is releasing a slew of Android 16 updates, along with new general Android and accessibility features. The rollout of the new Android 16 features, which are first coming to Pixel devices, marks a new chapter in how Android updates are delivered, as the company is moving from a single yearly update to more frequent releases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Android 16 is adding AI-powered notification summaries that condense long messages and group chats into quick, glanceable overviews. A new “Notification organizer” will automatically group and silence lower-priority notifications, such as promotions, news, and social alerts.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The update also brings more ways to customize devices, with users getting access to custom icon shapes, themed icons, and the option to automatically darken light apps, even those that don’t have their own native dark theme.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, there’s a new Parental Controls option within Android Settings that allows parents to set screen time limits, create downtime schedules, control app usage, and more for their children.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3071853" height="519" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-02-at-12.51.39-PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;These updates are starting to roll out with Android 16 on eligible Pixel devices starting Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also releasing several new Android features that aren’t specific to Android 16. A new beta feature called “Call Reason” allows users to flag calls to saved contacts as “urgent.” Recipients will see this on their incoming call screen and know it’s time-sensitive. If they miss the call, the “urgent” note will stay in their call history.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also launching “Expressive Captions” that display the full emotion of speech with tags like [sad] or [joyful], whether it’s a video message or a post on social media. The company says this will allow users to glean the full context of what’s being said when the sound is off.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant is making it easier to spot and exit unwanted group chats. If an unknown number invites a user to a group, they’ll get an alert that shows key information about the group. The user can then quickly choose to reply, leave the chat, or block and report the number.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Pinned tabs in Chrome now work the same way as on desktop, which means pinned pages stay saved at the front of the browser, letting users pick up where they left off.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3071855" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/2.-Expressive-Captions.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also updating Circle to Search, its feature that&amp;nbsp;allows users to&amp;nbsp;search from anywhere on their phone&amp;nbsp;by using gestures like circling, highlighting, scribbling, or tapping. Users can now analyze suspicious messages with the feature — after initiating Circle to Search, an AI Overview will appear indicating whether the message is likely a scam.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For accessibility updates, Google is enhancing its “Guided Frame” feature in the Pixel camera app. Previously, the feature has notified users about things like a face in the frame. Now, it will provide a more in-depth description, such as “one girl with a yellow T-shirt sits on the sofa and looks at the dog.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, users no longer need to physically tap their phone to start using Voice Access, which allows users to control their Android devices with voice commands. Now, users just need to say “Hey Google, start Voice Access” to begin controlling their phone hands-free.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also launching Fast Pair for hearing aids, starting with hearing aids from Demant, a Danish company that owns several major hearing aid brands, including Oticon, Sonic, and Bernafon.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/android-16-adds-ai-notification-summaries-new-customization-options-and-more/</guid><pubDate>Tue, 02 Dec 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] Google announces second Android 16 release of 2025 is heading to Pixels (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/12/google-announces-second-android-16-release-of-2025-is-heading-to-pixels/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The update is rolling out to Pixels starting today.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Android 16 on a Pixel" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Android-16-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Android 16 on a Pixel" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Android-16-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A new Android 16 is rolling out. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is following through on its pledge to split Android versions into more frequent updates. We already had one Android 16 release this year, and now it’s time for the second. The new version is rolling out first on Google’s Pixel phones, featuring more icon customization, easier parental controls, and AI-powered notifications. Don’t be bummed if you aren’t first in line for the new Android 16—Google also has a raft of general improvements coming to the wider Android ecosystem.&lt;/p&gt;
&lt;h2&gt;Android 16, part 2&lt;/h2&gt;
&lt;p&gt;Since rolling out the first version of Android in 2008, Google has largely stuck to one major release per year. Android 16 changes things, moving from one monolithic release to two. Today’s OS update is the second part of the Android 16 era, but don’t expect major changes. As expected, the first release in June made more changes. Most of what we’ll see in the second update is geared toward Google’s Pixel phones, plus some less notable changes for developers.&lt;/p&gt;
&lt;p&gt;Google’s new AI features for notifications are probably the most important change. Android 16 will use AI for two notification tasks: summarizing and organizing. The OS will take long chat conversations and summarize the notifications with AI. Notification data is processed locally on the device and won’t be uploaded anywhere. In the notification shade, the collapsed notification line will feature a summary of the conversation rather than a snippet of one message. Expanding the notification will display the full text.&lt;/p&gt;
&lt;div style="margin-left: auto; margin-right: auto;"&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1920" id="video-2129874-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/1-Notification-summary.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;
&lt;p&gt;Google also says AI will help to reduce notification overload in Android 16, part 2. This will build on the notification grouping from the first Android 16 release by gathering lower-priority notifications and silencing them. These items will be organized into batches, like news and promotions, where they can be safely ignored until you want to take a closer look.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Material 3 Expressive came to Pixels earlier this year but not as part of the first Android 16 upgrade—Google’s relationship with Android versions is complicated these days. Regardless, Material 3 will get a bit more cohesive on Pixels following this update. Google will now apply Material theming to all icons on your device automatically, replacing legacy colored icons with theme-friendly versions. Similarly, dark mode will be supported across more apps, even if the devs haven’t added support. Google is also adding a few more icon shape options if you want to jazz up your home screen.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2129875 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Android 16 screens" class="fullwidth full" height="1920" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Android-16-2.jpg" width="2160" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;By way of functional changes, Google has added a more intuitive way of managing parental controls—you can just use the managed device directly. Parents will be able to set a PIN code for accessing features like screen time, app usage, and so on without grabbing a different device. If you want more options or control, the new on-device settings will also help you configure Google Family Link.&lt;/p&gt;
&lt;h2&gt;Android for all&lt;/h2&gt;
&lt;p&gt;No Pixel? No problem. Google has also bundled up a collection of app and system updates that will begin rolling out today for all supported Android devices.&lt;/p&gt;
&lt;p&gt;Chrome for Android is getting an update with tab pinning, mirroring a feature that has been in the desktop version since time immemorial. The Google Messages app is also taking care of some low-hanging fruit. When you’re invited to a group chat by a new number, the app will display group information and a one-tap option to leave and report the chat as spam.&lt;/p&gt;
&lt;p&gt;Google’s official dialer app comes on Pixels, but it’s also in the Play Store for anyone to download. If you and your contacts use Google Dialer, you’ll soon be able to place calls with a “reason.” You can flag a call as “Urgent” to indicate to the recipient that they shouldn’t send you to voicemail. The urgent label will also remain in the call history if they miss the call.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2129877 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="928" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/new-android-dec.jpg" width="909" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google also says it’s adding yet another AI-powered feature to stop scams. Circle to Search, which is available on most modern Android phones, lets you highlight anything you come across to check for scams. It’s unclear how accurate it will be, though. This feature plugs into AI Overviews to assess the risk and provide suggestions.&lt;/p&gt;
&lt;p&gt;Android devices are also getting a raft of new accessibility options. Those who use a mouse with AutoClick can now set custom hover times. You’ll also be able to launch TalkBack voice control in Gboard with a two-finger tap. Voice Access, which lets you control the phone UI by voice, is also getting easier to use. Rather than tapping things to launch it, you’ll be able to tell the phone’s Gemini assistant by voice to “start Voice Access.”&lt;/p&gt;
&lt;p&gt;Gemini is also making an appearance in Google’s Guided Frame camera feature, but this one is only for Pixels. Guided Frame helps those with low or no vision take photos by offering voice descriptions of what’s in the frame. With the new update, Guided Frame will use Gemini to summarize the shot’s content. This should allow for more detailed descriptions and hopefully not too many hallucinations.&lt;/p&gt;
&lt;p&gt;The widely available Android feature updates will roll out over the coming weeks. Pixel owners should begin getting update notifications for the new Android 16 build over a similar timeframe. There will also be manual update files on Google’s developer site. Non-Pixel phones will get the new Android 16 whenever OEMs get around to it, but there may not be much overlap with the features Google has announced for Pixels.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The update is rolling out to Pixels starting today.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Android 16 on a Pixel" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Android-16-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Android 16 on a Pixel" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Android-16-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A new Android 16 is rolling out. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is following through on its pledge to split Android versions into more frequent updates. We already had one Android 16 release this year, and now it’s time for the second. The new version is rolling out first on Google’s Pixel phones, featuring more icon customization, easier parental controls, and AI-powered notifications. Don’t be bummed if you aren’t first in line for the new Android 16—Google also has a raft of general improvements coming to the wider Android ecosystem.&lt;/p&gt;
&lt;h2&gt;Android 16, part 2&lt;/h2&gt;
&lt;p&gt;Since rolling out the first version of Android in 2008, Google has largely stuck to one major release per year. Android 16 changes things, moving from one monolithic release to two. Today’s OS update is the second part of the Android 16 era, but don’t expect major changes. As expected, the first release in June made more changes. Most of what we’ll see in the second update is geared toward Google’s Pixel phones, plus some less notable changes for developers.&lt;/p&gt;
&lt;p&gt;Google’s new AI features for notifications are probably the most important change. Android 16 will use AI for two notification tasks: summarizing and organizing. The OS will take long chat conversations and summarize the notifications with AI. Notification data is processed locally on the device and won’t be uploaded anywhere. In the notification shade, the collapsed notification line will feature a summary of the conversation rather than a snippet of one message. Expanding the notification will display the full text.&lt;/p&gt;
&lt;div style="margin-left: auto; margin-right: auto;"&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1920" id="video-2129874-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/1-Notification-summary.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;
&lt;p&gt;Google also says AI will help to reduce notification overload in Android 16, part 2. This will build on the notification grouping from the first Android 16 release by gathering lower-priority notifications and silencing them. These items will be organized into batches, like news and promotions, where they can be safely ignored until you want to take a closer look.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Material 3 Expressive came to Pixels earlier this year but not as part of the first Android 16 upgrade—Google’s relationship with Android versions is complicated these days. Regardless, Material 3 will get a bit more cohesive on Pixels following this update. Google will now apply Material theming to all icons on your device automatically, replacing legacy colored icons with theme-friendly versions. Similarly, dark mode will be supported across more apps, even if the devs haven’t added support. Google is also adding a few more icon shape options if you want to jazz up your home screen.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2129875 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Android 16 screens" class="fullwidth full" height="1920" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Android-16-2.jpg" width="2160" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;By way of functional changes, Google has added a more intuitive way of managing parental controls—you can just use the managed device directly. Parents will be able to set a PIN code for accessing features like screen time, app usage, and so on without grabbing a different device. If you want more options or control, the new on-device settings will also help you configure Google Family Link.&lt;/p&gt;
&lt;h2&gt;Android for all&lt;/h2&gt;
&lt;p&gt;No Pixel? No problem. Google has also bundled up a collection of app and system updates that will begin rolling out today for all supported Android devices.&lt;/p&gt;
&lt;p&gt;Chrome for Android is getting an update with tab pinning, mirroring a feature that has been in the desktop version since time immemorial. The Google Messages app is also taking care of some low-hanging fruit. When you’re invited to a group chat by a new number, the app will display group information and a one-tap option to leave and report the chat as spam.&lt;/p&gt;
&lt;p&gt;Google’s official dialer app comes on Pixels, but it’s also in the Play Store for anyone to download. If you and your contacts use Google Dialer, you’ll soon be able to place calls with a “reason.” You can flag a call as “Urgent” to indicate to the recipient that they shouldn’t send you to voicemail. The urgent label will also remain in the call history if they miss the call.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2129877 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="928" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/new-android-dec.jpg" width="909" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google also says it’s adding yet another AI-powered feature to stop scams. Circle to Search, which is available on most modern Android phones, lets you highlight anything you come across to check for scams. It’s unclear how accurate it will be, though. This feature plugs into AI Overviews to assess the risk and provide suggestions.&lt;/p&gt;
&lt;p&gt;Android devices are also getting a raft of new accessibility options. Those who use a mouse with AutoClick can now set custom hover times. You’ll also be able to launch TalkBack voice control in Gboard with a two-finger tap. Voice Access, which lets you control the phone UI by voice, is also getting easier to use. Rather than tapping things to launch it, you’ll be able to tell the phone’s Gemini assistant by voice to “start Voice Access.”&lt;/p&gt;
&lt;p&gt;Gemini is also making an appearance in Google’s Guided Frame camera feature, but this one is only for Pixels. Guided Frame helps those with low or no vision take photos by offering voice descriptions of what’s in the frame. With the new update, Guided Frame will use Gemini to summarize the shot’s content. This should allow for more detailed descriptions and hopefully not too many hallucinations.&lt;/p&gt;
&lt;p&gt;The widely available Android feature updates will roll out over the coming weeks. Pixel owners should begin getting update notifications for the new Android 16 build over a similar timeframe. There will also be manual update files on Google’s developer site. Non-Pixel phones will get the new Android 16 whenever OEMs get around to it, but there may not be much overlap with the features Google has announced for Pixels.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/12/google-announces-second-android-16-release-of-2025-is-heading-to-pixels/</guid><pubDate>Tue, 02 Dec 2025 19:11:47 +0000</pubDate></item><item><title>[NEW] Amazon previews 3 AI agents, including ‘Kiro’ that can code on its own for days (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/amazon-previews-3-ai-agents-including-kiro-that-can-code-on-its-own-for-days/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/02/GettyImages-1065679054.jpg?resize=1200,849" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services on Tuesday announced three new AI agents it calls “frontier agents,” including one designed to learn how you like to work and then operate on its own for days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each of these agents handle different tasks such as writing code, security processes like code reviews, and automating DevOps tasks such as preventing incidents when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perhaps the biggest and most interesting claim by AWS is its promise that the frontier agent called “Kiro autonomous agent” can work on its own for days at a time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kiro is a software coding agent based on AWS’s existing AI coding tool Kiro, which was announced in July. While that existing tool could be used for vibe coding (which is really just prototyping), it was intended to produce operational code, or software that would be pushed live. To make reliable code, the AI must follow a company’s software-coding specifications. Kiro does that through a concept called “spec-driven development.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Kiro codes, it has the human instruct, confirm, or correct its assumptions, thereby creating specifications. The Kiro autonomous agent watches how the team works in various tools by scanning existing code, among other training means. And then, AWS says, it can work independently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You simply assign a complex task from the backlog and it independently figures out how to get that work done,” AWS CEO Matt Garman promised when introducing the new product during his keynote at AWS re:Invent on Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It actually learns how you like to work, and it continues to deepen its understanding of your code and your products and the standards that your team follows over time,” he said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says Kiro maintains “persistent context across sessions.” In other words, it doesn’t run out of memory and forget what it was supposed to do.  It can therefore be handed tasks and work on its own for hours or days, Amazon promises, with minimal human intervention. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Garman described a task like updating a bit of critical code used by 15 bits of corporate software. Instead of assigning and verifying each update, Kiro can be assigned to fix all 15 in one prompt. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To complete the automation of coding tasks, the cloud provider developed AWS Security Agent, an agent that works independently to identify security problems as code is written, tests it after the fact, and then offers suggested fixes. The DevOps Agent rounds out the trio, automatically testing the new code for performance issues, or compatibility with other software, hardware, or cloud settings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To be sure, Amazon’s agents aren’t the first to claim long work windows. For instance OpenAI said last month that GPT‑5.1-Codex-Max, its agentic coding model, is designed for long runs, too, up to 24 hours.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also not totally clear that the biggest hurdle to agentic adoption is the context window (aka the ability to work continuously without stalling out). LLMs still have hallucination and accuracy issues that turn developers into “babysitters,” they say.  So developers often want to assign short tasks and verify quickly before moving on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, before agents can become like co-workers, context windows must grow bigger. Amazon’s tech is another big step in that direction. &lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/02/GettyImages-1065679054.jpg?resize=1200,849" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services on Tuesday announced three new AI agents it calls “frontier agents,” including one designed to learn how you like to work and then operate on its own for days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each of these agents handle different tasks such as writing code, security processes like code reviews, and automating DevOps tasks such as preventing incidents when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perhaps the biggest and most interesting claim by AWS is its promise that the frontier agent called “Kiro autonomous agent” can work on its own for days at a time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kiro is a software coding agent based on AWS’s existing AI coding tool Kiro, which was announced in July. While that existing tool could be used for vibe coding (which is really just prototyping), it was intended to produce operational code, or software that would be pushed live. To make reliable code, the AI must follow a company’s software-coding specifications. Kiro does that through a concept called “spec-driven development.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Kiro codes, it has the human instruct, confirm, or correct its assumptions, thereby creating specifications. The Kiro autonomous agent watches how the team works in various tools by scanning existing code, among other training means. And then, AWS says, it can work independently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You simply assign a complex task from the backlog and it independently figures out how to get that work done,” AWS CEO Matt Garman promised when introducing the new product during his keynote at AWS re:Invent on Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It actually learns how you like to work, and it continues to deepen its understanding of your code and your products and the standards that your team follows over time,” he said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says Kiro maintains “persistent context across sessions.” In other words, it doesn’t run out of memory and forget what it was supposed to do.  It can therefore be handed tasks and work on its own for hours or days, Amazon promises, with minimal human intervention. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Garman described a task like updating a bit of critical code used by 15 bits of corporate software. Instead of assigning and verifying each update, Kiro can be assigned to fix all 15 in one prompt. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To complete the automation of coding tasks, the cloud provider developed AWS Security Agent, an agent that works independently to identify security problems as code is written, tests it after the fact, and then offers suggested fixes. The DevOps Agent rounds out the trio, automatically testing the new code for performance issues, or compatibility with other software, hardware, or cloud settings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To be sure, Amazon’s agents aren’t the first to claim long work windows. For instance OpenAI said last month that GPT‑5.1-Codex-Max, its agentic coding model, is designed for long runs, too, up to 24 hours.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also not totally clear that the biggest hurdle to agentic adoption is the context window (aka the ability to work continuously without stalling out). LLMs still have hallucination and accuracy issues that turn developers into “babysitters,” they say.  So developers often want to assign short tasks and verify quickly before moving on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, before agents can become like co-workers, context windows must grow bigger. Amazon’s tech is another big step in that direction. &lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/amazon-previews-3-ai-agents-including-kiro-that-can-code-on-its-own-for-days/</guid><pubDate>Tue, 02 Dec 2025 22:18:01 +0000</pubDate></item><item><title>[NEW] Google tests merging AI Overviews with AI Mode (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/google-tests-merging-ai-overviews-with-ai-mode/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/google-ai-mode-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As OpenAI goes into “Code Red” over competitive pressures, Google announced it has begun testing a new feature that merges its AI Overviews with AI Mode in Search. That means that users who are provided with the now common AI-generated snapshot of key information on a topic or question above their search results can choose to go deeper by asking follow-up questions in a conversational interface.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google calls this conversational feature AI Mode. It launched to U.S. users this May, and to global users this August, allowing for back-and-forth chats with Google’s Gemini AI, in an experience similar to ChatGPT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, accessing the experience so far has required you to think ahead about what type of question you were preparing to search for. If it were a more traditional search query, or one where you could expect to get a quick answer, you’d likely stick with typing into the search box as usual.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if you expected to ask more questions or explore a topic in more detail, you’d have to click over to the AI Mode tab to start chatting with the AI instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google now wants to test whether or not it makes sense to differentiate the two experiences. After all, the process of information seeking can often lead to a desire to learn more. You may have thought you were starting a simple query, only to find yourself delving deeper into the topic.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;(1/2) Today we’re starting to test a new way to seamlessly go deeper in AI Mode directly from the Search results page on mobile, globally.&lt;/p&gt;&lt;p&gt;This brings us closer to our vision for Search: just ask whatever’s on your mind – no matter how long or complex – and find exactly what you… pic.twitter.com/mcCS7oT2FI&lt;/p&gt;— Robby Stein (@rmstein) December 1, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new test, announced on Monday, Google says users will be able to “seamlessly go deeper” in AI Mode directly from the Search results page. While the test is rolling out to users globally, it’s only available on mobile devices for the time being. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout comes alongside a push inside Google’s AI rival, OpenAI, which is now delaying other products to focus on improving the chatbox experience. Thanks in part to the release of Gemini’s Nano Banana image model and other Gemini improvements, Gemini has grown to over 650 million monthly users as of November. Merging the conversational mode with AI Overviews, which has 2 billion monthly users, could give Gemini an edge in consumer adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notes VP of Product for Google Search Robby Stein, in a post on X, “You shouldn’t have to think about where or how to ask your question.” Instead, he explained, users will continue to get an AI Overview as a helpful starting point, but will then be able to ask conversational follow-up questions in AI Mode from the same screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This brings us closer to our vision for Search: just ask whatever’s on your mind – no matter how long or complex – and find exactly what you need,” Stein wrote. &lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/google-ai-mode-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As OpenAI goes into “Code Red” over competitive pressures, Google announced it has begun testing a new feature that merges its AI Overviews with AI Mode in Search. That means that users who are provided with the now common AI-generated snapshot of key information on a topic or question above their search results can choose to go deeper by asking follow-up questions in a conversational interface.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google calls this conversational feature AI Mode. It launched to U.S. users this May, and to global users this August, allowing for back-and-forth chats with Google’s Gemini AI, in an experience similar to ChatGPT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, accessing the experience so far has required you to think ahead about what type of question you were preparing to search for. If it were a more traditional search query, or one where you could expect to get a quick answer, you’d likely stick with typing into the search box as usual.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if you expected to ask more questions or explore a topic in more detail, you’d have to click over to the AI Mode tab to start chatting with the AI instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google now wants to test whether or not it makes sense to differentiate the two experiences. After all, the process of information seeking can often lead to a desire to learn more. You may have thought you were starting a simple query, only to find yourself delving deeper into the topic.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;(1/2) Today we’re starting to test a new way to seamlessly go deeper in AI Mode directly from the Search results page on mobile, globally.&lt;/p&gt;&lt;p&gt;This brings us closer to our vision for Search: just ask whatever’s on your mind – no matter how long or complex – and find exactly what you… pic.twitter.com/mcCS7oT2FI&lt;/p&gt;— Robby Stein (@rmstein) December 1, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new test, announced on Monday, Google says users will be able to “seamlessly go deeper” in AI Mode directly from the Search results page. While the test is rolling out to users globally, it’s only available on mobile devices for the time being. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout comes alongside a push inside Google’s AI rival, OpenAI, which is now delaying other products to focus on improving the chatbox experience. Thanks in part to the release of Gemini’s Nano Banana image model and other Gemini improvements, Gemini has grown to over 650 million monthly users as of November. Merging the conversational mode with AI Overviews, which has 2 billion monthly users, could give Gemini an edge in consumer adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notes VP of Product for Google Search Robby Stein, in a post on X, “You shouldn’t have to think about where or how to ask your question.” Instead, he explained, users will continue to get an AI Overview as a helpful starting point, but will then be able to ask conversational follow-up questions in AI Mode from the same screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This brings us closer to our vision for Search: just ask whatever’s on your mind – no matter how long or complex – and find exactly what you need,” Stein wrote. &lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/google-tests-merging-ai-overviews-with-ai-mode/</guid><pubDate>Tue, 02 Dec 2025 22:26:34 +0000</pubDate></item><item><title>[NEW] OpenAI CEO declares “code red” as Gemini gains 200 million users in 3 months (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/openai-ceo-declares-code-red-as-gemini-gains-200-million-users-in-3-months/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Three years after Google sounded alarm bells over ChatGPT, the tables have turned.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/aaltman_crop-640x360.jpg" width="640" /&gt;
                  &lt;img alt="OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/aaltman_crop-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anadolu via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The shoe is most certainly on the other foot. On Monday, OpenAI CEO Sam Altman reportedly declared a “code red” at the company to improve ChatGPT, delaying advertising plans and other products in the process, &amp;nbsp;The Information reported based on a leaked internal memo. The move follows Google’s release of its Gemini 3 model last month, which has outperformed ChatGPT on some industry benchmark tests and sparked high-profile praise on social media.&lt;/p&gt;
&lt;p&gt;In the memo, Altman wrote, “We are at a critical time for ChatGPT.” The company will push back work on advertising integration, AI agents for health and shopping, and a personal assistant feature called Pulse. Altman encouraged temporary team transfers and established daily calls for employees responsible for enhancing the chatbot.&lt;/p&gt;
&lt;p&gt;The directive creates an odd symmetry with events from December 2022, when Google management declared its own “code red” internal emergency after ChatGPT launched and rapidly gained in popularity. At the time, Google CEO Sundar Pichai reassigned teams across the company to develop AI prototypes and products to compete with OpenAI’s chatbot. Now, three years later, the AI industry is in a very different place.&lt;/p&gt;
&lt;p&gt;Google released Gemini 3 in mid-November, and the model quickly topped the LMArena leaderboard, a crowdsourced vibemarking site that allows users to compare two AI models and select the one with outputs that please them most. The launch has been accompanied by measured praise from some and bombastic hype from others. Salesforce CEO Marc Benioff wrote Sunday on X that he was switching to Gemini 3 after using ChatGPT daily for three years. “I’m not going back,” Benioff wrote. “The leap is insane.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In addition to buzz about Gemini on social media, Google is quickly catching up to ChatGPT in user numbers. ChatGPT has more than 800 million weekly users, according to OpenAI, while Google’s Gemini app has grown from 450 million monthly active users in July to 650 million in October, according to Business Insider.&lt;/p&gt;
&lt;h2&gt;Financial stakes run high&lt;/h2&gt;
&lt;p&gt;Not everyone views OpenAI’s “code red” as a genuine alarm. Reuters columnist Robert Cyran wrote on Tuesday that OpenAI’s announcement added “to the impression that OpenAI is trying to do too much at once with technology that still requires a great deal of development and funding.” On the same day Altman’s memo circulated, OpenAI announced an ownership stake in a Thrive Capital venture and a collaboration with Accenture. “The only thing bigger than the company’s attention deficit is its appetite for capital,” Cyran wrote.&lt;/p&gt;
&lt;p&gt;In fact, OpenAI faces an unusual competitive disadvantage: Unlike Google, which subsidizes its AI ventures through search advertising revenue, OpenAI does not turn a profit and relies on fundraising to survive. According to The Information, the company, now valued at around $500 billion, has committed more than $1 trillion in financial obligations to cloud computing providers and chipmakers that supply the computing power needed to train and run its AI models.&lt;/p&gt;
&lt;p&gt;But the tech industry never stands still, and things can change quickly. Altman’s memo also reportedly stated that OpenAI plans to release a new simulated reasoning model next week that may beat Gemini 3 in internal evaluations. In AI, the back-and-forth cycle of one-upmanship is expected to continue as long as the dollars keep flowing.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Three years after Google sounded alarm bells over ChatGPT, the tables have turned.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/aaltman_crop-640x360.jpg" width="640" /&gt;
                  &lt;img alt="OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/aaltman_crop-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anadolu via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The shoe is most certainly on the other foot. On Monday, OpenAI CEO Sam Altman reportedly declared a “code red” at the company to improve ChatGPT, delaying advertising plans and other products in the process, &amp;nbsp;The Information reported based on a leaked internal memo. The move follows Google’s release of its Gemini 3 model last month, which has outperformed ChatGPT on some industry benchmark tests and sparked high-profile praise on social media.&lt;/p&gt;
&lt;p&gt;In the memo, Altman wrote, “We are at a critical time for ChatGPT.” The company will push back work on advertising integration, AI agents for health and shopping, and a personal assistant feature called Pulse. Altman encouraged temporary team transfers and established daily calls for employees responsible for enhancing the chatbot.&lt;/p&gt;
&lt;p&gt;The directive creates an odd symmetry with events from December 2022, when Google management declared its own “code red” internal emergency after ChatGPT launched and rapidly gained in popularity. At the time, Google CEO Sundar Pichai reassigned teams across the company to develop AI prototypes and products to compete with OpenAI’s chatbot. Now, three years later, the AI industry is in a very different place.&lt;/p&gt;
&lt;p&gt;Google released Gemini 3 in mid-November, and the model quickly topped the LMArena leaderboard, a crowdsourced vibemarking site that allows users to compare two AI models and select the one with outputs that please them most. The launch has been accompanied by measured praise from some and bombastic hype from others. Salesforce CEO Marc Benioff wrote Sunday on X that he was switching to Gemini 3 after using ChatGPT daily for three years. “I’m not going back,” Benioff wrote. “The leap is insane.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In addition to buzz about Gemini on social media, Google is quickly catching up to ChatGPT in user numbers. ChatGPT has more than 800 million weekly users, according to OpenAI, while Google’s Gemini app has grown from 450 million monthly active users in July to 650 million in October, according to Business Insider.&lt;/p&gt;
&lt;h2&gt;Financial stakes run high&lt;/h2&gt;
&lt;p&gt;Not everyone views OpenAI’s “code red” as a genuine alarm. Reuters columnist Robert Cyran wrote on Tuesday that OpenAI’s announcement added “to the impression that OpenAI is trying to do too much at once with technology that still requires a great deal of development and funding.” On the same day Altman’s memo circulated, OpenAI announced an ownership stake in a Thrive Capital venture and a collaboration with Accenture. “The only thing bigger than the company’s attention deficit is its appetite for capital,” Cyran wrote.&lt;/p&gt;
&lt;p&gt;In fact, OpenAI faces an unusual competitive disadvantage: Unlike Google, which subsidizes its AI ventures through search advertising revenue, OpenAI does not turn a profit and relies on fundraising to survive. According to The Information, the company, now valued at around $500 billion, has committed more than $1 trillion in financial obligations to cloud computing providers and chipmakers that supply the computing power needed to train and run its AI models.&lt;/p&gt;
&lt;p&gt;But the tech industry never stands still, and things can change quickly. Altman’s memo also reportedly stated that OpenAI plans to release a new simulated reasoning model next week that may beat Gemini 3 in internal evaluations. In AI, the back-and-forth cycle of one-upmanship is expected to continue as long as the dollars keep flowing.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/openai-ceo-declares-code-red-as-gemini-gains-200-million-users-in-3-months/</guid><pubDate>Tue, 02 Dec 2025 22:42:09 +0000</pubDate></item><item><title>[NEW] Amazon challenges competitors with on-premises Nvidia ‘AI Factories’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/amazon-challenges-competitors-with-on-premises-nvidia-ai-factories/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Amazon-AWS-AI-Factory.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon announced a new product Tuesday called “AI Factories” that allows big corporations and governments to run its AI systems in their own data centers. Or as AWS puts it: Customers supply the power and the data center, and AWS plunks in the AI system, manages it, and can tie it into other AWS cloud services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea is to cater to companies and governments concerned with data sovereignty, or absolute control over their data so it can’t wind up in a competitor’s or foreign adversary’s hands. An on-prem AI Factory means not sending their data to a model maker and not even sharing the hardware.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If that product name sounds familiar, it should. That’s what Nvidia calls its hardware systems that are chock-full of tools needed to run AI, from its GPU chips to its networking tech. This AWS AI Factory is, in fact, a collaboration with Nvidia, both companies say. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, the AWS Factory will use a combination of AWS and Nvidia technology. Companies that deploy these systems can opt for Nvidia’s latest Blackwell GPUs or Amazon’s new Trainium3 chip. It uses AWS’ homegrown networking, storage, databases, and security and can tap into Amazon Bedrock — the AI model selection and management service, and AWS SageMaker AI, the model building and training tool.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Interestingly, AWS is far from the only giant cloud provider installing Nvidia AI Factories. In October, Microsoft showed off its first of many-to-come AI Factories rolling out into its global data centers to run OpenAI workloads. Microsoft didn’t announce at the time that these extreme machines would be available for private clouds. Instead, Microsoft highlighted how it was leaning on a host of Nvidia AI Factory data center tech to build and connect its new “AI Superfactories,” aka new state-of-the-art data centers being built in Wisconsin and Georgia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Microsoft also outlined the data centers and cloud services that would be built in local countries to address the data sovereignty issue. To be fair, its options also include “Azure Local,” Microsoft’s own managed hardware that could be installed on customer sites.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, it is a bit ironic that AI is causing the biggest cloud providers to invest so heavily in corporate private data centers and hybrid clouds like it’s 2009 all over again.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Amazon-AWS-AI-Factory.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon announced a new product Tuesday called “AI Factories” that allows big corporations and governments to run its AI systems in their own data centers. Or as AWS puts it: Customers supply the power and the data center, and AWS plunks in the AI system, manages it, and can tie it into other AWS cloud services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea is to cater to companies and governments concerned with data sovereignty, or absolute control over their data so it can’t wind up in a competitor’s or foreign adversary’s hands. An on-prem AI Factory means not sending their data to a model maker and not even sharing the hardware.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If that product name sounds familiar, it should. That’s what Nvidia calls its hardware systems that are chock-full of tools needed to run AI, from its GPU chips to its networking tech. This AWS AI Factory is, in fact, a collaboration with Nvidia, both companies say. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, the AWS Factory will use a combination of AWS and Nvidia technology. Companies that deploy these systems can opt for Nvidia’s latest Blackwell GPUs or Amazon’s new Trainium3 chip. It uses AWS’ homegrown networking, storage, databases, and security and can tap into Amazon Bedrock — the AI model selection and management service, and AWS SageMaker AI, the model building and training tool.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Interestingly, AWS is far from the only giant cloud provider installing Nvidia AI Factories. In October, Microsoft showed off its first of many-to-come AI Factories rolling out into its global data centers to run OpenAI workloads. Microsoft didn’t announce at the time that these extreme machines would be available for private clouds. Instead, Microsoft highlighted how it was leaning on a host of Nvidia AI Factory data center tech to build and connect its new “AI Superfactories,” aka new state-of-the-art data centers being built in Wisconsin and Georgia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Microsoft also outlined the data centers and cloud services that would be built in local countries to address the data sovereignty issue. To be fair, its options also include “Azure Local,” Microsoft’s own managed hardware that could be installed on customer sites.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, it is a bit ironic that AI is causing the biggest cloud providers to invest so heavily in corporate private data centers and hybrid clouds like it’s 2009 all over again.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/amazon-challenges-competitors-with-on-premises-nvidia-ai-factories/</guid><pubDate>Wed, 03 Dec 2025 00:43:37 +0000</pubDate></item><item><title>[NEW] All the biggest news from AWS’ big tech show re:Invent 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/all-the-biggest-news-from-aws-big-tech-show-reinvent-2025/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2179195367.jpg?resize=1200,746" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ annual tech conference AWS re:Invent has wrapped up its first official day of programming and has already delivered an endless stream of product news. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The unsurprising theme is AI for the enterprise, although this year it’s all about upgrades that give its customers greater control to customize AI agents — including one that AWS claims can learn from you and then work independently for days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS re:Invent 2025, which runs through December 5, started with a keynote from AWS CEO Matt Garman, who leaned into the idea that AI agents can unlock the “true value” of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI assistants are starting to give way to AI agents that can perform tasks and automate on your behalf,” he said during the December 2 keynote. “This is where we’re starting to see material business returns from your AI investments.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AI agent news promises to be a persistent presence throughout AWS re:Invent 2025, there were other announcements, too. Here is a roundup of the announcements that got our attention. TechCrunch will continue to update this article through the end of AWS re:Invent, so be sure to check back.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-training-chip-and-nvidia-compatibility"&gt;An AI training chip and Nvidia compatibility&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS introduced a new version of its AI training chip called Trainium3 along with an AI system called UltraServer that runs it. The TL;DR: This upgraded chip comes with some impressive specs, including a promise of up to 4x performance gains for both AI training and inference while lowering energy use by 40%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also provided a teaser. The company already has Trainium4 in development, which will be able to work with Nvidia’s chips.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-expanded-agentcore-capabilities"&gt;Expanded AgentCore capabilities&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS announced new features in its AgentCore AI agent building platform. One feature of note is Policy in AgentCore, which gives developers the ability to more easily set boundaries for AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also announced that agents will now be able to log and remember things about their users. Plus it announced that it will help its customers evaluate agents through 13 prebuilt evaluation systems.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-nonstop-ai-agent-worker-bee"&gt;A nonstop AI agent worker bee&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS announced three new AI agents (there is that term again) called “Frontier agents,” including one called “Kiro autonomous agent” that writes code and is designed to learn how a team likes to work so it can operate largely on its own for hours or days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another of these new agents handles security processes like code reviews, and the third does DevOps tasks such&amp;nbsp;as preventing incidents&amp;nbsp;when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-new-nova-models-and-services"&gt;New Nova models and services&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS is rolling out four new AI models within its Nova AI model family — three of which are text generating and one that can create text and images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also announced a new service called Nova Forge that allows AWS cloud customers to access pre-trained, mid-trained, or post-trained models that they can then top off by training on their own proprietary data. AWS’s big pitch is flexibility and customization.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-lyft-s-argument-for-ai-agents"&gt;Lyft’s argument for AI agents&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The ride-hailing company was among many AWS customers that piped up during the event to share their success stories and evidence of how products affected their business. Lyft is using Anthropic’s Claude model via Amazon Bedrock to create an AI agent that handles driver and rider questions and issues. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said this AI agent has reduced average resolution time by 87%. Lyft also said it has seen a 70% increase in driver usage of the AI agent this year. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-factory-for-the-private-data-center"&gt;An AI Factory for the private data center&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon also announced “AI Factories” that allow big corporations and governments to run AWS AI systems in their own data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system was designed in partnership with Nvidia and includes both Nvidia’s tech and AWS’s. While companies that use it can stock it with Nvidia GPUs, they can also opt for Amazon’s newest homegrown AI chip, the Trainium3. The system is Amazon’s way of addressing data sovereignty, or the need of governments and many companies to control their data and not share it, even to use AI.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2179195367.jpg?resize=1200,746" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ annual tech conference AWS re:Invent has wrapped up its first official day of programming and has already delivered an endless stream of product news. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The unsurprising theme is AI for the enterprise, although this year it’s all about upgrades that give its customers greater control to customize AI agents — including one that AWS claims can learn from you and then work independently for days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS re:Invent 2025, which runs through December 5, started with a keynote from AWS CEO Matt Garman, who leaned into the idea that AI agents can unlock the “true value” of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI assistants are starting to give way to AI agents that can perform tasks and automate on your behalf,” he said during the December 2 keynote. “This is where we’re starting to see material business returns from your AI investments.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AI agent news promises to be a persistent presence throughout AWS re:Invent 2025, there were other announcements, too. Here is a roundup of the announcements that got our attention. TechCrunch will continue to update this article through the end of AWS re:Invent, so be sure to check back.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-training-chip-and-nvidia-compatibility"&gt;An AI training chip and Nvidia compatibility&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS introduced a new version of its AI training chip called Trainium3 along with an AI system called UltraServer that runs it. The TL;DR: This upgraded chip comes with some impressive specs, including a promise of up to 4x performance gains for both AI training and inference while lowering energy use by 40%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also provided a teaser. The company already has Trainium4 in development, which will be able to work with Nvidia’s chips.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-expanded-agentcore-capabilities"&gt;Expanded AgentCore capabilities&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS announced new features in its AgentCore AI agent building platform. One feature of note is Policy in AgentCore, which gives developers the ability to more easily set boundaries for AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also announced that agents will now be able to log and remember things about their users. Plus it announced that it will help its customers evaluate agents through 13 prebuilt evaluation systems.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-nonstop-ai-agent-worker-bee"&gt;A nonstop AI agent worker bee&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS announced three new AI agents (there is that term again) called “Frontier agents,” including one called “Kiro autonomous agent” that writes code and is designed to learn how a team likes to work so it can operate largely on its own for hours or days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another of these new agents handles security processes like code reviews, and the third does DevOps tasks such&amp;nbsp;as preventing incidents&amp;nbsp;when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-new-nova-models-and-services"&gt;New Nova models and services&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS is rolling out four new AI models within its Nova AI model family — three of which are text generating and one that can create text and images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also announced a new service called Nova Forge that allows AWS cloud customers to access pre-trained, mid-trained, or post-trained models that they can then top off by training on their own proprietary data. AWS’s big pitch is flexibility and customization.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-lyft-s-argument-for-ai-agents"&gt;Lyft’s argument for AI agents&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The ride-hailing company was among many AWS customers that piped up during the event to share their success stories and evidence of how products affected their business. Lyft is using Anthropic’s Claude model via Amazon Bedrock to create an AI agent that handles driver and rider questions and issues. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said this AI agent has reduced average resolution time by 87%. Lyft also said it has seen a 70% increase in driver usage of the AI agent this year. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-factory-for-the-private-data-center"&gt;An AI Factory for the private data center&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon also announced “AI Factories” that allow big corporations and governments to run AWS AI systems in their own data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system was designed in partnership with Nvidia and includes both Nvidia’s tech and AWS’s. While companies that use it can stock it with Nvidia GPUs, they can also opt for Amazon’s newest homegrown AI chip, the Trainium3. The system is Amazon’s way of addressing data sovereignty, or the need of governments and many companies to control their data and not share it, even to use AI.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/all-the-biggest-news-from-aws-big-tech-show-reinvent-2025/</guid><pubDate>Wed, 03 Dec 2025 01:06:25 +0000</pubDate></item></channel></rss>