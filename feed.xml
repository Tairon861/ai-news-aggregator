<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 29 Aug 2025 06:31:09 +0000</lastBuildDate><item><title> ()</title><link>https://www.wired.com/feed/category/artificial-intelligence/rss</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://www.wired.com/feed/category/artificial-intelligence/rss</guid></item><item><title>AI or not, Will Smith’s crowd video is fresh cringe (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/28/ai-or-not-will-smiths-crowd-video-is-fresh-cringe-2/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Will Smith posted a video on social media that shows oceans of fans cheering him on during his recent European tour.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“My favorite part of the tour is seeing you all up close,” the caption says. “Thank you for seeing me too.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In these thousands-deep crowds, some fans are holding up signs espousing their love for Smith, with one even saying that his music helped them survive cancer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the video gives off an odd aura — it looks believably real at first glance, until you look closer and find digitally mangled faces, nonsensical finger placements, and oddly augmented features across the series of clips.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-9-16 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The video looks strange enough that fans responded with accusations that the crowd footage was created using AI. It’s bad news for Smith, who’s already suffered reputational damage after “the slap.” If he were using AI to make his concerts look more impressive, or even spinning up stories of fans using his music to cope with cancer treatment, that would be pretty indefensible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These fans aren’t fake, though — or at least, that’s our best guess. (There’s not a reliable way to determine whether content was created using AI, which has made the current online landscape a nightmare of misinformation.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As tech blogger Andy Baio pointed out, Will Smith has posted photos and videos throughout his tour that show some of the same fans and signs depicted in the questionable video.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;There’s nothing about these older posts that indicates that the photos and videos are synthetic, yet when they’re depicted in this new video, they look like they’ve been generated using AI. It seems like Smith’s team has collaged real footage with AI-generated videos that use real crowd photos as source images, which makes the video even more difficult to interpret.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040744" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-28-at-11.22.36AM-1.jpg?w=518" width="518" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;The images on the left are taken from an allegedly AI-generated video on Will Smith’s social media, whereas the images on the right were uploaded previously.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Will Smith on Instagram &lt;span class="screen-reader-text"&gt;(opens in a new window)&lt;/span&gt;&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;But social media audiences will not take the time to scroll through past Will Smith posts, find evidence that a fan really did listen to his music during cancer treatment, and give him the benefit of the doubt. What fans will take away from the post is that Smith is posting fake videos of his fans, which is deeply cringe, even if the reality is a bit less egregious.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s bad timing for Smith, too, that YouTube had recently begun testing a feature that would use “traditional machine learning technology to unblur, denoise, and improve clarity” on some Shorts posts — these edits made Smith’s YouTube Short look even more fake than the videos on other platforms.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;YouTube’s creator liaison Rene Ritchie has since shared that the platform will soon allow creators to opt out of this feature, which has proven unpopular thus far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You could make the argument that Will Smith has not duped his fans — that his team simply used AI to generate footage from photographs to create a more visually gripping social media post and that this practice could be compared to other forms of video editing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fans don’t see it this way, though. The public is more resistant to generative AI technology than existing creative tools, like autotune or Photoshop. But even in those cases, many fans remain turned off by artists who rely on these tools in ways that feel untruthful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If a fan buys tickets to see a pop star, but it turns out that his recordings only sound good because his terrible voice has been autotuned, then they’d feel duped. It’s like photographing a model to advertise a facial moisturizer, only to edit acne off the model’s face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once an artist breaks their audience’s trust, it’s hard to win it back — even if you’re the Fresh Prince of Bel-Air.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Will Smith posted a video on social media that shows oceans of fans cheering him on during his recent European tour.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“My favorite part of the tour is seeing you all up close,” the caption says. “Thank you for seeing me too.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In these thousands-deep crowds, some fans are holding up signs espousing their love for Smith, with one even saying that his music helped them survive cancer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the video gives off an odd aura — it looks believably real at first glance, until you look closer and find digitally mangled faces, nonsensical finger placements, and oddly augmented features across the series of clips.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-9-16 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The video looks strange enough that fans responded with accusations that the crowd footage was created using AI. It’s bad news for Smith, who’s already suffered reputational damage after “the slap.” If he were using AI to make his concerts look more impressive, or even spinning up stories of fans using his music to cope with cancer treatment, that would be pretty indefensible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These fans aren’t fake, though — or at least, that’s our best guess. (There’s not a reliable way to determine whether content was created using AI, which has made the current online landscape a nightmare of misinformation.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As tech blogger Andy Baio pointed out, Will Smith has posted photos and videos throughout his tour that show some of the same fans and signs depicted in the questionable video.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;There’s nothing about these older posts that indicates that the photos and videos are synthetic, yet when they’re depicted in this new video, they look like they’ve been generated using AI. It seems like Smith’s team has collaged real footage with AI-generated videos that use real crowd photos as source images, which makes the video even more difficult to interpret.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3040744" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-28-at-11.22.36AM-1.jpg?w=518" width="518" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;The images on the left are taken from an allegedly AI-generated video on Will Smith’s social media, whereas the images on the right were uploaded previously.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Will Smith on Instagram &lt;span class="screen-reader-text"&gt;(opens in a new window)&lt;/span&gt;&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;But social media audiences will not take the time to scroll through past Will Smith posts, find evidence that a fan really did listen to his music during cancer treatment, and give him the benefit of the doubt. What fans will take away from the post is that Smith is posting fake videos of his fans, which is deeply cringe, even if the reality is a bit less egregious.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s bad timing for Smith, too, that YouTube had recently begun testing a feature that would use “traditional machine learning technology to unblur, denoise, and improve clarity” on some Shorts posts — these edits made Smith’s YouTube Short look even more fake than the videos on other platforms.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;YouTube’s creator liaison Rene Ritchie has since shared that the platform will soon allow creators to opt out of this feature, which has proven unpopular thus far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You could make the argument that Will Smith has not duped his fans — that his team simply used AI to generate footage from photographs to create a more visually gripping social media post and that this practice could be compared to other forms of video editing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fans don’t see it this way, though. The public is more resistant to generative AI technology than existing creative tools, like autotune or Photoshop. But even in those cases, many fans remain turned off by artists who rely on these tools in ways that feel untruthful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If a fan buys tickets to see a pop star, but it turns out that his recordings only sound good because his terrible voice has been autotuned, then they’d feel duped. It’s like photographing a model to advertise a facial moisturizer, only to edit acne off the model’s face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once an artist breaks their audience’s trust, it’s hard to win it back — even if you’re the Fresh Prince of Bel-Air.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/28/ai-or-not-will-smiths-crowd-video-is-fresh-cringe-2/</guid><pubDate>Thu, 28 Aug 2025 20:02:52 +0000</pubDate></item><item><title>Anthropic users face a new choice – opt out or share your chats for AI training (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/28/anthropic-users-face-a-new-choice-opt-out-or-share-your-data-for-ai-training/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is making some big changes to how it handles user data, requiring all Claude users to decide by September 28 whether they want their conversations used to train AI models. While the company directed us to its blog post on the policy changes when asked about what prompted the move, we’ve formed some theories of our own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But first, what’s changing: Previously, Anthropic didn’t use consumer chat data for model training. Now, the company wants to train its AI systems on user conversations and coding sessions, and it said it’s extending data retention to five years for those who don’t opt out.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That is a massive update. Previously, users of Anthropic’s consumer products were told that their prompts and conversation outputs would be automatically deleted from Anthropic’s back end within 30 days “unless legally or policy‑required to keep them longer” or their input was flagged as violating its policies, in which case a user’s inputs and outputs might be retained for up to two years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By consumer, we mean the new policies apply to Claude Free, Pro, and Max users, including those using Claude Code. Business customers using Claude Gov, Claude for Work, Claude for Education, or API access will be unaffected, which is how OpenAI similarly protects enterprise customers from data training policies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So why is this happening? In that post about the update, Anthropic frames the changes around user choice, saying that by not opting out, users will “help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations.” Users will “also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, help us help you. But the full truth is probably a little less selfless.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like every other large language model company, Anthropic needs data more than it needs people to have fuzzy feelings about its brand. Training AI models requires vast amounts of high-quality conversational data, and accessing millions of Claude interactions should provide exactly the kind of real-world content that can improve Anthropic’s competitive positioning against rivals like OpenAI and Google.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond the competitive pressures of AI development, the changes would also seem to reflect broader industry shifts in data policies, as companies like Anthropic and OpenAI face increasing scrutiny over their data retention practices. OpenAI, for instance, is currently fighting a court order that forces the company to retain all consumer ChatGPT conversations indefinitely, including deleted chats, because of a lawsuit filed by The New York Times and other publishers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In June, OpenAI COO Brad Lightcap called this “a sweeping and unnecessary demand” that “fundamentally conflicts with the privacy commitments we have made to our users.” The court order affects ChatGPT Free, Plus, Pro, and Team users, though enterprise customers and those with Zero Data Retention agreements are still protected.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s alarming is how much confusion all of these changing usage policies are creating for users, many of whom remain oblivious to them.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In fairness, everything is moving quickly now, so as the tech changes, privacy policies are bound to change. But many of these changes are fairly sweeping and mentioned only fleetingly amid the companies’ other news. (You wouldn’t think Tuesday’s policy changes for Anthropic users were very big news based on where the company placed this update on its press page.)&lt;/p&gt;

&lt;figure class="wp-block-image alignfull size-large"&gt;&lt;img alt="alt" class="wp-image-3040915" height="417" src="https://techcrunch.com/wp-content/uploads/2025/08/Claude-screenshot.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Anthropic&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;But many users don’t realize the guidelines to which they’ve agreed have changed because the design practically guarantees it. Most ChatGPT users keep clicking on “delete” toggles that aren’t technically deleting anything. Meanwhile, Anthropic’s implementation of its new policy follows a familiar pattern.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;How so? New users will choose their preference during signup, but existing users face a pop-up with “Updates to Consumer Terms and Policies” in large text and a prominent black “Accept” button with a much tinier toggle switch for training permissions below in smaller print — and automatically set to “On.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As observed earlier today by The Verge, the design raises concerns that users might quickly click “Accept” without noticing they’re agreeing to data sharing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, the stakes for user awareness couldn’t be higher. Privacy experts have long warned that the complexity surrounding AI makes meaningful user consent nearly unattainable. Under the Biden administration, the Federal Trade Commission even stepped in, warning that AI companies risk enforcement action if they engage in “surreptitiously changing its terms of service or privacy policy, or burying a disclosure behind hyperlinks, in legalese, or in fine print.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether the commission — now operating with just three of its five commissioners — still has its eye on these practices today is an open question, one we’ve put directly to the FTC.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is making some big changes to how it handles user data, requiring all Claude users to decide by September 28 whether they want their conversations used to train AI models. While the company directed us to its blog post on the policy changes when asked about what prompted the move, we’ve formed some theories of our own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But first, what’s changing: Previously, Anthropic didn’t use consumer chat data for model training. Now, the company wants to train its AI systems on user conversations and coding sessions, and it said it’s extending data retention to five years for those who don’t opt out.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That is a massive update. Previously, users of Anthropic’s consumer products were told that their prompts and conversation outputs would be automatically deleted from Anthropic’s back end within 30 days “unless legally or policy‑required to keep them longer” or their input was flagged as violating its policies, in which case a user’s inputs and outputs might be retained for up to two years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By consumer, we mean the new policies apply to Claude Free, Pro, and Max users, including those using Claude Code. Business customers using Claude Gov, Claude for Work, Claude for Education, or API access will be unaffected, which is how OpenAI similarly protects enterprise customers from data training policies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So why is this happening? In that post about the update, Anthropic frames the changes around user choice, saying that by not opting out, users will “help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations.” Users will “also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, help us help you. But the full truth is probably a little less selfless.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like every other large language model company, Anthropic needs data more than it needs people to have fuzzy feelings about its brand. Training AI models requires vast amounts of high-quality conversational data, and accessing millions of Claude interactions should provide exactly the kind of real-world content that can improve Anthropic’s competitive positioning against rivals like OpenAI and Google.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond the competitive pressures of AI development, the changes would also seem to reflect broader industry shifts in data policies, as companies like Anthropic and OpenAI face increasing scrutiny over their data retention practices. OpenAI, for instance, is currently fighting a court order that forces the company to retain all consumer ChatGPT conversations indefinitely, including deleted chats, because of a lawsuit filed by The New York Times and other publishers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In June, OpenAI COO Brad Lightcap called this “a sweeping and unnecessary demand” that “fundamentally conflicts with the privacy commitments we have made to our users.” The court order affects ChatGPT Free, Plus, Pro, and Team users, though enterprise customers and those with Zero Data Retention agreements are still protected.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s alarming is how much confusion all of these changing usage policies are creating for users, many of whom remain oblivious to them.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In fairness, everything is moving quickly now, so as the tech changes, privacy policies are bound to change. But many of these changes are fairly sweeping and mentioned only fleetingly amid the companies’ other news. (You wouldn’t think Tuesday’s policy changes for Anthropic users were very big news based on where the company placed this update on its press page.)&lt;/p&gt;

&lt;figure class="wp-block-image alignfull size-large"&gt;&lt;img alt="alt" class="wp-image-3040915" height="417" src="https://techcrunch.com/wp-content/uploads/2025/08/Claude-screenshot.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Anthropic&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;But many users don’t realize the guidelines to which they’ve agreed have changed because the design practically guarantees it. Most ChatGPT users keep clicking on “delete” toggles that aren’t technically deleting anything. Meanwhile, Anthropic’s implementation of its new policy follows a familiar pattern.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;How so? New users will choose their preference during signup, but existing users face a pop-up with “Updates to Consumer Terms and Policies” in large text and a prominent black “Accept” button with a much tinier toggle switch for training permissions below in smaller print — and automatically set to “On.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As observed earlier today by The Verge, the design raises concerns that users might quickly click “Accept” without noticing they’re agreeing to data sharing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, the stakes for user awareness couldn’t be higher. Privacy experts have long warned that the complexity surrounding AI makes meaningful user consent nearly unattainable. Under the Biden administration, the Federal Trade Commission even stepped in, warning that AI companies risk enforcement action if they engage in “surreptitiously changing its terms of service or privacy policy, or burying a disclosure behind hyperlinks, in legalese, or in fine print.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether the commission — now operating with just three of its five commissioners — still has its eye on these practices today is an open question, one we’ve put directly to the FTC.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/28/anthropic-users-face-a-new-choice-opt-out-or-share-your-data-for-ai-training/</guid><pubDate>Thu, 28 Aug 2025 20:43:12 +0000</pubDate></item><item><title>Forget data labeling: Tencent’s R-Zero shows how LLMs can train themselves (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/forget-data-labeling-tencents-r-zero-shows-how-llms-can-train-themselves/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A new training framework &lt;span&gt;developed by researchers at&amp;nbsp;Tencent AI Lab&amp;nbsp;and&amp;nbsp;Washington University in St. Louis&amp;nbsp;enables large language models (LLMs) to improve themselves without requiring&amp;nbsp;&lt;/span&gt;any human-labeled data. The technique, called R-Zero, uses reinforcement learning to generate its own training data from scratch, addressing one of the main bottlenecks in creating self-evolving AI systems. R-Zero works by having two independent models co-evolve by interacting with and challenging each other.&lt;/p&gt;&lt;p&gt;Experiments show that R-Zero substantially improves reasoning capabilities across different LLMs, which could lower the complexity and costs of training advanced AI. For enterprises, this approach could accelerate the development of specialized models for complex reasoning tasks without the massive expense of curating labeled datasets.&lt;/p&gt;&lt;p&gt;The idea behind self-evolving LLMs is to create AI systems that can autonomously generate, refine, and learn from their own experiences. This offers a scalable path toward more intelligent and capable AI. However, a major challenge is that training these models requires large volumes of high-quality tasks and labels, which act as supervision signals for the AI to learn from.&lt;/p&gt;&lt;p&gt;Relying on human annotators to create this data is not only costly and slow but also creates a fundamental bottleneck. It effectively limits an AI’s potential capabilities to what humans can teach it. To address this, researchers have developed label-free methods that derive reward signals directly from a model’s own outputs, for example, by measuring its confidence in an answer. While these methods eliminate the need for explicit labels, they still rely on a pre-existing set of tasks, thereby limiting their applicability in truly self-evolving scenarios.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Other approaches involve having models generate their own tasks to learn from. However, in domains like open-ended reasoning, where there is no simple way to check for correctness (such as a code executor), ensuring the quality of this self-generated data is a significant hurdle.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-r-zero-works"&gt;How R-Zero works&lt;/h2&gt;



&lt;p&gt;R-Zero is a framework designed to train reasoning LLMs that can evolve from zero external data. The process begins with a single base model, which is split into two roles: a “Challenger” and a “Solver.” These two models are optimized independently but evolve together through a continuous cycle of interaction.&lt;/p&gt;



&lt;p&gt;The Challenger’s goal is to create new tasks that are just at the threshold of the Solver’s current abilities, neither too easy nor impossible. The Solver, in turn, is rewarded for solving these increasingly complex tasks. In written comments to VentureBeat, Chengsong Huang, co-author of the paper and a doctoral student at Washington University in St. Louis, explained that this dynamic is crucial because generating high-quality questions is often more complicated than finding the answers.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016157" height="280" src="https://venturebeat.com/wp-content/uploads/2025/08/image_78dc45.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;“What we found in a practical setting is that the biggest challenge is not generating the answers… but rather generating high-quality, novel, and progressively more difficult questions,” Huang said. “We believe that good teachers are far rarer than good students. The co-evolutionary dynamic automates the creation of this ‘teacher,’ ensuring a steady and dynamic curriculum that pushes the Solver’s capabilities far beyond what a static, pre-existing dataset could achieve.”&lt;/p&gt;



&lt;p&gt;Once the Challenger generates enough questions, they are filtered for diversity and compiled into a training dataset. In the Solver’s training phase, it is fine-tuned on these challenging questions. The “correct” answer for each question is determined by a majority vote from the Solver’s own previous attempts.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This entire process repeats, creating a self-improving loop that operates without any human intervention, allowing the two models to push each other to become progressively more capable across each iteration.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-r-zero-in-action"&gt;R-Zero in action&lt;/h2&gt;



&lt;p&gt;The researchers tested R-Zero on several open-source LLMs, including models from the Qwen3 and OctoThinker families. They first trained the models on math problems and then tested whether the learned reasoning skills could generalize to other complex, general-domain benchmarks like MMLU-Pro (multi-language understanding and reasoning tasks) and SuperGPQA (science and reasoning tasks).&lt;/p&gt;



&lt;p&gt;The results showed that R-Zero is a highly effective, model-agnostic framework. For instance, it boosted the Qwen3-4B-Base model’s score by +6.49 on average across math reasoning benchmarks. The training process consistently and substantially improved performance, with gains accumulating over several iterations. The larger Qwen3-8B-Base model saw its average math score climb by +5.51 points after three iterations.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016158" height="564" src="https://venturebeat.com/wp-content/uploads/2025/08/image_f72f0b.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;A key finding was the immediate performance leap after the first iteration, which validated the effectiveness of the Challenger’s role in creating a high-quality learning curriculum. “This confirms that the intelligent curriculum generated by the RL-trained Challenger is significantly more effective than that of a non-trained generator,” the researchers write in their paper.&lt;/p&gt;



&lt;p&gt;Notably, the skills learned from math problems were effectively transferred to general reasoning tasks, thereby enhancing the models’ underlying capabilities. For example, the same Qwen3-4B-Base model showed an improvement of +7.54 on general-domain reasoning benchmarks. Another interesting finding is that R-Zero can serve as a decisive pre-training step. Models first improved by R-Zero achieved even higher performance when later fine-tuned on traditional labeled data, suggesting the framework acts as a performance amplifier.&lt;/p&gt;



&lt;p&gt;For enterprises, the “from zero data” approach could be a game-changer, especially in niche domains where high-quality data is scarce or non-existent. Huang highlights that R-Zero’s main advantage is its ability to sidestep the most expensive and time-consuming part of AI development: data curation.&lt;/p&gt;



&lt;p&gt;“Our approach entirely bypasses the fundamental bottleneck of having to find, label, and curate high-quality datasets,” he said. “This is not just about a cost-saving measure; it’s a pathway toward creating AI that can surpass human capabilities, because it is no longer limited by the scope of human knowledge or data.”&lt;/p&gt;



&lt;p&gt;However, the co-evolutionary process also revealed a critical challenge. As the Challenger successfully generates progressively more difficult problems, the Solver’s ability to produce reliable “correct” answers via majority vote begins to decline. The researchers found that the true accuracy of these self-generated labels dropped from 79% in the first iteration to 63% by the third&lt;span&gt;, compared to a strong oracle LLM such as&amp;nbsp;GPT -4&lt;/span&gt;. This decline in data quality is a key trade-off and a potential bottleneck for the system’s long-term performance.&lt;/p&gt;



&lt;p&gt;Huang acknowledged that this is a fundamental problem for the self-evolving paradigm. “Our work is a proof of concept that demonstrates the potential of this approach, but we acknowledge that maintaining stable, long-term improvement without plateauing is a significant hurdle,” he said. “Solving this problem will be a crucial next step for the entire research community.”&lt;/p&gt;



&lt;p&gt;The researchers also highlight a key limitation of the framework: the current mechanism is best suited for domains like math where correctness can be objectively determined. So, how could this powerful paradigm be extended to more subjective enterprise tasks like generating marketing copy or summarizing reports?&lt;/p&gt;



&lt;p&gt;Huang suggests a potential path forward involves adding a third, co-evolving AI agent to the mix: a “Verifier” or “Critic.”&lt;/p&gt;



&lt;p&gt;“Instead of evaluating for a simple ‘correct’ answer, this Verifier would be trained to evaluate the quality of the Solver’s output based on more nuanced criteria,” he explained. “The co-evolutionary dynamic would then involve the Challenger creating the prompt, the Solver generating the response, and the Verifier providing a quality signal, with all three models improving together.”&lt;/p&gt;



&lt;p&gt;While this remains a direction for future research, it points toward a future where fully autonomous AI systems can master not just objective logic, but subjective reasoning as well.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A new training framework &lt;span&gt;developed by researchers at&amp;nbsp;Tencent AI Lab&amp;nbsp;and&amp;nbsp;Washington University in St. Louis&amp;nbsp;enables large language models (LLMs) to improve themselves without requiring&amp;nbsp;&lt;/span&gt;any human-labeled data. The technique, called R-Zero, uses reinforcement learning to generate its own training data from scratch, addressing one of the main bottlenecks in creating self-evolving AI systems. R-Zero works by having two independent models co-evolve by interacting with and challenging each other.&lt;/p&gt;&lt;p&gt;Experiments show that R-Zero substantially improves reasoning capabilities across different LLMs, which could lower the complexity and costs of training advanced AI. For enterprises, this approach could accelerate the development of specialized models for complex reasoning tasks without the massive expense of curating labeled datasets.&lt;/p&gt;&lt;p&gt;The idea behind self-evolving LLMs is to create AI systems that can autonomously generate, refine, and learn from their own experiences. This offers a scalable path toward more intelligent and capable AI. However, a major challenge is that training these models requires large volumes of high-quality tasks and labels, which act as supervision signals for the AI to learn from.&lt;/p&gt;&lt;p&gt;Relying on human annotators to create this data is not only costly and slow but also creates a fundamental bottleneck. It effectively limits an AI’s potential capabilities to what humans can teach it. To address this, researchers have developed label-free methods that derive reward signals directly from a model’s own outputs, for example, by measuring its confidence in an answer. While these methods eliminate the need for explicit labels, they still rely on a pre-existing set of tasks, thereby limiting their applicability in truly self-evolving scenarios.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Other approaches involve having models generate their own tasks to learn from. However, in domains like open-ended reasoning, where there is no simple way to check for correctness (such as a code executor), ensuring the quality of this self-generated data is a significant hurdle.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-r-zero-works"&gt;How R-Zero works&lt;/h2&gt;



&lt;p&gt;R-Zero is a framework designed to train reasoning LLMs that can evolve from zero external data. The process begins with a single base model, which is split into two roles: a “Challenger” and a “Solver.” These two models are optimized independently but evolve together through a continuous cycle of interaction.&lt;/p&gt;



&lt;p&gt;The Challenger’s goal is to create new tasks that are just at the threshold of the Solver’s current abilities, neither too easy nor impossible. The Solver, in turn, is rewarded for solving these increasingly complex tasks. In written comments to VentureBeat, Chengsong Huang, co-author of the paper and a doctoral student at Washington University in St. Louis, explained that this dynamic is crucial because generating high-quality questions is often more complicated than finding the answers.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016157" height="280" src="https://venturebeat.com/wp-content/uploads/2025/08/image_78dc45.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;“What we found in a practical setting is that the biggest challenge is not generating the answers… but rather generating high-quality, novel, and progressively more difficult questions,” Huang said. “We believe that good teachers are far rarer than good students. The co-evolutionary dynamic automates the creation of this ‘teacher,’ ensuring a steady and dynamic curriculum that pushes the Solver’s capabilities far beyond what a static, pre-existing dataset could achieve.”&lt;/p&gt;



&lt;p&gt;Once the Challenger generates enough questions, they are filtered for diversity and compiled into a training dataset. In the Solver’s training phase, it is fine-tuned on these challenging questions. The “correct” answer for each question is determined by a majority vote from the Solver’s own previous attempts.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This entire process repeats, creating a self-improving loop that operates without any human intervention, allowing the two models to push each other to become progressively more capable across each iteration.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-r-zero-in-action"&gt;R-Zero in action&lt;/h2&gt;



&lt;p&gt;The researchers tested R-Zero on several open-source LLMs, including models from the Qwen3 and OctoThinker families. They first trained the models on math problems and then tested whether the learned reasoning skills could generalize to other complex, general-domain benchmarks like MMLU-Pro (multi-language understanding and reasoning tasks) and SuperGPQA (science and reasoning tasks).&lt;/p&gt;



&lt;p&gt;The results showed that R-Zero is a highly effective, model-agnostic framework. For instance, it boosted the Qwen3-4B-Base model’s score by +6.49 on average across math reasoning benchmarks. The training process consistently and substantially improved performance, with gains accumulating over several iterations. The larger Qwen3-8B-Base model saw its average math score climb by +5.51 points after three iterations.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016158" height="564" src="https://venturebeat.com/wp-content/uploads/2025/08/image_f72f0b.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;A key finding was the immediate performance leap after the first iteration, which validated the effectiveness of the Challenger’s role in creating a high-quality learning curriculum. “This confirms that the intelligent curriculum generated by the RL-trained Challenger is significantly more effective than that of a non-trained generator,” the researchers write in their paper.&lt;/p&gt;



&lt;p&gt;Notably, the skills learned from math problems were effectively transferred to general reasoning tasks, thereby enhancing the models’ underlying capabilities. For example, the same Qwen3-4B-Base model showed an improvement of +7.54 on general-domain reasoning benchmarks. Another interesting finding is that R-Zero can serve as a decisive pre-training step. Models first improved by R-Zero achieved even higher performance when later fine-tuned on traditional labeled data, suggesting the framework acts as a performance amplifier.&lt;/p&gt;



&lt;p&gt;For enterprises, the “from zero data” approach could be a game-changer, especially in niche domains where high-quality data is scarce or non-existent. Huang highlights that R-Zero’s main advantage is its ability to sidestep the most expensive and time-consuming part of AI development: data curation.&lt;/p&gt;



&lt;p&gt;“Our approach entirely bypasses the fundamental bottleneck of having to find, label, and curate high-quality datasets,” he said. “This is not just about a cost-saving measure; it’s a pathway toward creating AI that can surpass human capabilities, because it is no longer limited by the scope of human knowledge or data.”&lt;/p&gt;



&lt;p&gt;However, the co-evolutionary process also revealed a critical challenge. As the Challenger successfully generates progressively more difficult problems, the Solver’s ability to produce reliable “correct” answers via majority vote begins to decline. The researchers found that the true accuracy of these self-generated labels dropped from 79% in the first iteration to 63% by the third&lt;span&gt;, compared to a strong oracle LLM such as&amp;nbsp;GPT -4&lt;/span&gt;. This decline in data quality is a key trade-off and a potential bottleneck for the system’s long-term performance.&lt;/p&gt;



&lt;p&gt;Huang acknowledged that this is a fundamental problem for the self-evolving paradigm. “Our work is a proof of concept that demonstrates the potential of this approach, but we acknowledge that maintaining stable, long-term improvement without plateauing is a significant hurdle,” he said. “Solving this problem will be a crucial next step for the entire research community.”&lt;/p&gt;



&lt;p&gt;The researchers also highlight a key limitation of the framework: the current mechanism is best suited for domains like math where correctness can be objectively determined. So, how could this powerful paradigm be extended to more subjective enterprise tasks like generating marketing copy or summarizing reports?&lt;/p&gt;



&lt;p&gt;Huang suggests a potential path forward involves adding a third, co-evolving AI agent to the mix: a “Verifier” or “Critic.”&lt;/p&gt;



&lt;p&gt;“Instead of evaluating for a simple ‘correct’ answer, this Verifier would be trained to evaluate the quality of the Solver’s output based on more nuanced criteria,” he explained. “The co-evolutionary dynamic would then involve the Challenger creating the prompt, the Solver generating the response, and the Verifier providing a quality signal, with all three models improving together.”&lt;/p&gt;



&lt;p&gt;While this remains a direction for future research, it points toward a future where fully autonomous AI systems can master not just objective logic, but subjective reasoning as well.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/forget-data-labeling-tencents-r-zero-shows-how-llms-can-train-themselves/</guid><pubDate>Thu, 28 Aug 2025 21:07:08 +0000</pubDate></item><item><title>Nvidia’s $46.7B Q2 proves the platform, but its next fight is ASIC economics on inference (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/nvidias-strong-q2-results-cant-mask-the-asic-challenge-in-their-future/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Nvidia reported $46.7 billion in revenue for fiscal Q2 2026 in their earnings announcement and call yesterday, with data center revenue hitting $41.1 billion, up 56% year over year. The company also released guidance for Q3, predicting a $54 billion quarter.&lt;/p&gt;&lt;p&gt;Behind these confirmed earnings call numbers lies a more complex story of how custom application-specific integrated circuits (ASICs) are gaining ground in key Nvidia segments and will challenge their growth in the quarters to come.&lt;/p&gt;&lt;p&gt;Bank of America’s Vivek Arya asked Nvidia’s president and CEO, Jensen Huang, if he saw any scenario where ASICs could take market share from Nvidia GPUs. ASICs continue to gain ground on performance and cost advantages over Nvidia, Broadcom projects 55% to 60% AI revenue growth next year.&lt;/p&gt;&lt;p&gt;Huang pushed back hard on the earnings call. He emphasized that building AI infrastructure is “really hard” and most ASIC projects fail to reach production. That’s a fair point, but they have a competitor in Broadcom, which is seeing its AI revenue steadily ramp up, approaching a $20 billion annual run rate. Further underscoring the growing competitive fragmentation of the market is how Google, Meta and Microsoft all deploy custom silicon at scale. The market has spoken.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-asics-are-redefining-the-competitive-landscape-in-real-time"&gt;&lt;strong&gt;ASICs are redefining the competitive landscape in real-time&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Nvidia is more than capable of competing with new ASIC providers. Where they’re running into headwinds is how effectively ASIC competitors are positioning the combination of their use cases, performance claims and cost positions. They’re also looking to differentiate themselves in terms of the level of ecosystem lock-in they require, with Broadcom leading in this competitive dimension.&lt;/p&gt;



&lt;p&gt;The following table compares Nvidia Blackwell with its primary competitors. Real-world results vary significantly depending on specific workloads and deployment configurations:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Metric&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Nvidia Blackwell&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Google TPU v5e/v6&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;AWS Trainium/Inferentia2&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Intel Gaudi2/3&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Broadcom Jericho3-AI&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Primary Use Cases&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Training, inference, generative AI&lt;/td&gt;&lt;td&gt;Hyperscale training &amp;amp; inference&lt;/td&gt;&lt;td&gt;AWS-focused training &amp;amp; inference&lt;/td&gt;&lt;td&gt;Training, inference, hybrid-cloud deployments&lt;/td&gt;&lt;td&gt;AI cluster networking&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Performance Claims&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Up to 50x improvement over Hopper*&lt;/td&gt;&lt;td&gt;67% improvement TPU v6 vs v5*&lt;/td&gt;&lt;td&gt;Comparable GPU performance at lower power*&lt;/td&gt;&lt;td&gt;2-4x price-performance vs prior gen*&lt;/td&gt;&lt;td&gt;InfiniBand parity on Ethernet*&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Cost Position&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Premium pricing, comprehensive ecosystem&lt;/td&gt;&lt;td&gt;Significant savings vs GPUs per Google*&lt;/td&gt;&lt;td&gt;Aggressive pricing per AWS marketing*&lt;/td&gt;&lt;td&gt;Budget alternative positioning*&lt;/td&gt;&lt;td&gt;Lower networking TCO per vendor*&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Ecosystem Lock-In&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Moderate (CUDA, proprietary)&lt;/td&gt;&lt;td&gt;High (Google Cloud, TensorFlow/JAX)&lt;/td&gt;&lt;td&gt;High (AWS, proprietary Neuron SDK)&lt;/td&gt;&lt;td&gt;Moderate (supports open stack)&lt;/td&gt;&lt;td&gt;Low (Ethernet-based standards)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Universal (cloud, OEM)&lt;/td&gt;&lt;td&gt;Google Cloud-exclusive&lt;/td&gt;&lt;td&gt;AWS-exclusive&lt;/td&gt;&lt;td&gt;Multiple cloud and on-premise&lt;/td&gt;&lt;td&gt;Broadcom direct, OEM integrators&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Strategic Appeal&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Proven scale, broad support&lt;/td&gt;&lt;td&gt;Cloud workload optimization&lt;/td&gt;&lt;td&gt;AWS integration advantages&lt;/td&gt;&lt;td&gt;Multi-cloud flexibility&lt;/td&gt;&lt;td&gt;Simplified networking&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Market Position&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Leadership with margin pressure&lt;/td&gt;&lt;td&gt;Growing in specific workloads&lt;/td&gt;&lt;td&gt;Expanding within AWS&lt;/td&gt;&lt;td&gt;Emerging alternative&lt;/td&gt;&lt;td&gt;Infrastructure enabler&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;&lt;em&gt;*Performance-per-watt improvements and cost savings depend on specific workload characteristics, model types, deployment configurations and vendor testing assumptions. Actual results vary significantly by use case.&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-hyperscalers-continue-building-their-own-paths"&gt;&lt;strong&gt;Hyperscalers continue building their own paths&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Every major cloud provider has adopted custom silicon to gain the performance, cost, ecosystem scale and extensive DevOps advantages of defining an ASIC from the ground up. Google operates TPU v6 in production through its partnership with Broadcom. Meta built MTIA chips specifically for ranking and recommendations. Microsoft develops Project Maia for sustainable AI workloads.&lt;/p&gt;



&lt;p&gt;Amazon Web Services encourages customers to use Trainium for training and Inferentia for inference.&lt;/p&gt;



&lt;p&gt;Add to that the fact that ByteDance runs TikTok recommendations on custom silicon despite geopolitical tensions. That’s billions of inference requests running on ASICs daily, not GPUs.&lt;/p&gt;



&lt;p&gt;CFO Colette Kress acknowledged the competitive reality during the call. She referenced China revenue, saying it had dropped to a low single-digit percentage of data center revenue. Current Q3 guidance excludes H20 shipments to China completely. While Huang’s statements about China’s extensive opportunities tried to steer the earnings call in a positive direction, it was clear that equity analysts weren’t buying all of it.&lt;/p&gt;



&lt;p&gt;The general tone and perspective is that export controls create ongoing uncertainty for Nvidia in a market that arguably represents its second most significant growth opportunity. Huang said that 50% of all AI researchers are in China and he is fully committed to serving that market. &amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-nvidia-s-platform-advantage-is-one-of-their-greatest-strengths"&gt;&lt;strong&gt;Nvidia’s platform advantage is one of their greatest strengths&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Huang made a valid case for Nvidia’s integrated approach during the earnings call. Building modern AI requires six different chip types working together, he argued, and that complexity creates barriers competitors struggle to match. Nvidia doesn’t just ship GPUs anymore, he emphasized multiple times on the earnings call. The company delivers a complete AI infrastructure that scales globally, he emphatically stated, returning to AI infrastructure as a core message of the earnings call, citing it six times. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;The platform’s ubiquity makes it a default configuration supported by nearly every DevOps cycle of cloud hyperscalers. Nvidia runs across AWS, Azure and Google Cloud. PyTorch and TensorFlow also optimize for CUDA by default. When Meta drops a new Llama model or Google updates Gemini, they target Nvidia hardware first because that’s where millions of developers already work. The ecosystem creates its own gravity.&lt;/p&gt;



&lt;p&gt;The networking business validates the AI infrastructure strategy. Revenue hit $7.3 billion in Q2, up 98% year over year. NVLink connects GPUs at speeds traditional networking can’t touch. Huang revealed the real economics during the call: Nvidia captures about 35% of a typical gigawatt AI factory’s budget.&lt;/p&gt;



&lt;p&gt;“Out of a gigawatt AI factory, which can go anywhere from 50 to, you know, plus or minus 10%, let’s say, to $60 billion, we represent about 35% plus or minus of that. … And of course, what you get for that is not a GPU. … we’ve really transitioned to become an AI infrastructure company,” Huang said.&lt;/p&gt;



&lt;p&gt;That’s not just selling chips. that’s&amp;nbsp;owning the architecture&amp;nbsp;and capturing a significant portion of the entire AI build-out, powered by leading-edge networking and compute platforms like NVLink rack-scale systems and Spectrum X Ethernet.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-market-dynamics-are-shifting-quickly-as-nvidia-continues-reporting-strong-results"&gt;&lt;strong&gt;Market dynamics are shifting quickly as Nvidia continues reporting strong results&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Nvidia’s revenue growth decelerated from triple digits to 56% year over year. While that’s still impressive, it’s clear the trajectory of the company’s growth is changing. Competition is starting to have an effect on their growth, with this quarter seeing the most noticeable impact. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In particular,&amp;nbsp;China’s strategic role&amp;nbsp;in the global AI race drew pointed attention from analysts. As Joe Moore of Morgan Stanley probed late in the call, Huang estimated the&amp;nbsp;2025 China AI infrastructure opportunity at $50 billion. He communicated both optimism about the scale (“the second largest computing market in the world,” with “about 50% of the world’s AI researchers”) and realism about regulatory friction.&lt;/p&gt;



&lt;p&gt;A third pivotal force shaping Nvidia’s trajectory is the expanding complexity and cost of AI infrastructure itself. As hyperscalers and long-standing Nvidia clients invest billions in next-generation build-outs, the networking demands, compute and energy efficiency have intensified.&lt;/p&gt;



&lt;p&gt;Huang’s comments highlighted how “orders of magnitude speed up” from new platforms like Blackwell and innovations in NVLink, InfiniBand, and Spectrum XGS networking redefine the economic returns for customers’ data center capital. Meanwhile, supply chain pressures and the need for constant technological reinvention mean Nvidia must maintain a relentless pace and adaptability to remain entrenched as the preferred architecture provider.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-nvidia-s-path-forward-is-clear"&gt;&lt;strong&gt;Nvidia’s path forward is clear&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Nvidia issuing guidance for Q3 of $54 billion sends the signal that the core part of their DNA is as strong as ever. Continually improving Blackwell while developing Rubin architecture is evidence that their ability to innovate is as strong as ever.&lt;/p&gt;



&lt;p&gt;The question is whether a new type of innovative challenge they’re facing is one they can take on and win with the same level of development intensity they’ve shown in the past. VentureBeat expects Broadcom to continue aggressively pursuing new hyperscaler partnerships and strengthen its roadmap for specific optimizations aimed at inference workloads. Every ASIC competitor will take the competitive intensity they have to a new level, looking to get design wins that create a higher switching costs as well.&lt;/p&gt;



&lt;p&gt;Huang closed the earnings call, acknowledging the stakes: “A new industrial revolution has started. The AI race is on.” That race includes serious competitors Nvidia dismissed just two years ago. Broadcom, Google, Amazon and others invest billions in custom silicon. They’re not experimenting anymore. They’re shipping at scale.&lt;/p&gt;



&lt;p&gt;Nvidia faces its strongest competition since CUDA’s dominance began. The company’s $46.7 billion quarter proves its strength. However, custom silicon’s momentum suggests that the game has changed. The next chapter will test whether Nvidia’s platform advantages outweigh ASIC economics. VentureBeat expects technology buyers to follow the path of fund managers, betting on both Nvidia to sustain its lucrative customer base and ASIC competitors to secure design wins as intensifying competition drives greater market fragmentation.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Nvidia reported $46.7 billion in revenue for fiscal Q2 2026 in their earnings announcement and call yesterday, with data center revenue hitting $41.1 billion, up 56% year over year. The company also released guidance for Q3, predicting a $54 billion quarter.&lt;/p&gt;&lt;p&gt;Behind these confirmed earnings call numbers lies a more complex story of how custom application-specific integrated circuits (ASICs) are gaining ground in key Nvidia segments and will challenge their growth in the quarters to come.&lt;/p&gt;&lt;p&gt;Bank of America’s Vivek Arya asked Nvidia’s president and CEO, Jensen Huang, if he saw any scenario where ASICs could take market share from Nvidia GPUs. ASICs continue to gain ground on performance and cost advantages over Nvidia, Broadcom projects 55% to 60% AI revenue growth next year.&lt;/p&gt;&lt;p&gt;Huang pushed back hard on the earnings call. He emphasized that building AI infrastructure is “really hard” and most ASIC projects fail to reach production. That’s a fair point, but they have a competitor in Broadcom, which is seeing its AI revenue steadily ramp up, approaching a $20 billion annual run rate. Further underscoring the growing competitive fragmentation of the market is how Google, Meta and Microsoft all deploy custom silicon at scale. The market has spoken.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-asics-are-redefining-the-competitive-landscape-in-real-time"&gt;&lt;strong&gt;ASICs are redefining the competitive landscape in real-time&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Nvidia is more than capable of competing with new ASIC providers. Where they’re running into headwinds is how effectively ASIC competitors are positioning the combination of their use cases, performance claims and cost positions. They’re also looking to differentiate themselves in terms of the level of ecosystem lock-in they require, with Broadcom leading in this competitive dimension.&lt;/p&gt;



&lt;p&gt;The following table compares Nvidia Blackwell with its primary competitors. Real-world results vary significantly depending on specific workloads and deployment configurations:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Metric&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Nvidia Blackwell&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Google TPU v5e/v6&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;AWS Trainium/Inferentia2&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Intel Gaudi2/3&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Broadcom Jericho3-AI&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Primary Use Cases&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Training, inference, generative AI&lt;/td&gt;&lt;td&gt;Hyperscale training &amp;amp; inference&lt;/td&gt;&lt;td&gt;AWS-focused training &amp;amp; inference&lt;/td&gt;&lt;td&gt;Training, inference, hybrid-cloud deployments&lt;/td&gt;&lt;td&gt;AI cluster networking&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Performance Claims&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Up to 50x improvement over Hopper*&lt;/td&gt;&lt;td&gt;67% improvement TPU v6 vs v5*&lt;/td&gt;&lt;td&gt;Comparable GPU performance at lower power*&lt;/td&gt;&lt;td&gt;2-4x price-performance vs prior gen*&lt;/td&gt;&lt;td&gt;InfiniBand parity on Ethernet*&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Cost Position&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Premium pricing, comprehensive ecosystem&lt;/td&gt;&lt;td&gt;Significant savings vs GPUs per Google*&lt;/td&gt;&lt;td&gt;Aggressive pricing per AWS marketing*&lt;/td&gt;&lt;td&gt;Budget alternative positioning*&lt;/td&gt;&lt;td&gt;Lower networking TCO per vendor*&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Ecosystem Lock-In&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Moderate (CUDA, proprietary)&lt;/td&gt;&lt;td&gt;High (Google Cloud, TensorFlow/JAX)&lt;/td&gt;&lt;td&gt;High (AWS, proprietary Neuron SDK)&lt;/td&gt;&lt;td&gt;Moderate (supports open stack)&lt;/td&gt;&lt;td&gt;Low (Ethernet-based standards)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Universal (cloud, OEM)&lt;/td&gt;&lt;td&gt;Google Cloud-exclusive&lt;/td&gt;&lt;td&gt;AWS-exclusive&lt;/td&gt;&lt;td&gt;Multiple cloud and on-premise&lt;/td&gt;&lt;td&gt;Broadcom direct, OEM integrators&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Strategic Appeal&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Proven scale, broad support&lt;/td&gt;&lt;td&gt;Cloud workload optimization&lt;/td&gt;&lt;td&gt;AWS integration advantages&lt;/td&gt;&lt;td&gt;Multi-cloud flexibility&lt;/td&gt;&lt;td&gt;Simplified networking&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Market Position&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Leadership with margin pressure&lt;/td&gt;&lt;td&gt;Growing in specific workloads&lt;/td&gt;&lt;td&gt;Expanding within AWS&lt;/td&gt;&lt;td&gt;Emerging alternative&lt;/td&gt;&lt;td&gt;Infrastructure enabler&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;&lt;em&gt;*Performance-per-watt improvements and cost savings depend on specific workload characteristics, model types, deployment configurations and vendor testing assumptions. Actual results vary significantly by use case.&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-hyperscalers-continue-building-their-own-paths"&gt;&lt;strong&gt;Hyperscalers continue building their own paths&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Every major cloud provider has adopted custom silicon to gain the performance, cost, ecosystem scale and extensive DevOps advantages of defining an ASIC from the ground up. Google operates TPU v6 in production through its partnership with Broadcom. Meta built MTIA chips specifically for ranking and recommendations. Microsoft develops Project Maia for sustainable AI workloads.&lt;/p&gt;



&lt;p&gt;Amazon Web Services encourages customers to use Trainium for training and Inferentia for inference.&lt;/p&gt;



&lt;p&gt;Add to that the fact that ByteDance runs TikTok recommendations on custom silicon despite geopolitical tensions. That’s billions of inference requests running on ASICs daily, not GPUs.&lt;/p&gt;



&lt;p&gt;CFO Colette Kress acknowledged the competitive reality during the call. She referenced China revenue, saying it had dropped to a low single-digit percentage of data center revenue. Current Q3 guidance excludes H20 shipments to China completely. While Huang’s statements about China’s extensive opportunities tried to steer the earnings call in a positive direction, it was clear that equity analysts weren’t buying all of it.&lt;/p&gt;



&lt;p&gt;The general tone and perspective is that export controls create ongoing uncertainty for Nvidia in a market that arguably represents its second most significant growth opportunity. Huang said that 50% of all AI researchers are in China and he is fully committed to serving that market. &amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-nvidia-s-platform-advantage-is-one-of-their-greatest-strengths"&gt;&lt;strong&gt;Nvidia’s platform advantage is one of their greatest strengths&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Huang made a valid case for Nvidia’s integrated approach during the earnings call. Building modern AI requires six different chip types working together, he argued, and that complexity creates barriers competitors struggle to match. Nvidia doesn’t just ship GPUs anymore, he emphasized multiple times on the earnings call. The company delivers a complete AI infrastructure that scales globally, he emphatically stated, returning to AI infrastructure as a core message of the earnings call, citing it six times. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;The platform’s ubiquity makes it a default configuration supported by nearly every DevOps cycle of cloud hyperscalers. Nvidia runs across AWS, Azure and Google Cloud. PyTorch and TensorFlow also optimize for CUDA by default. When Meta drops a new Llama model or Google updates Gemini, they target Nvidia hardware first because that’s where millions of developers already work. The ecosystem creates its own gravity.&lt;/p&gt;



&lt;p&gt;The networking business validates the AI infrastructure strategy. Revenue hit $7.3 billion in Q2, up 98% year over year. NVLink connects GPUs at speeds traditional networking can’t touch. Huang revealed the real economics during the call: Nvidia captures about 35% of a typical gigawatt AI factory’s budget.&lt;/p&gt;



&lt;p&gt;“Out of a gigawatt AI factory, which can go anywhere from 50 to, you know, plus or minus 10%, let’s say, to $60 billion, we represent about 35% plus or minus of that. … And of course, what you get for that is not a GPU. … we’ve really transitioned to become an AI infrastructure company,” Huang said.&lt;/p&gt;



&lt;p&gt;That’s not just selling chips. that’s&amp;nbsp;owning the architecture&amp;nbsp;and capturing a significant portion of the entire AI build-out, powered by leading-edge networking and compute platforms like NVLink rack-scale systems and Spectrum X Ethernet.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-market-dynamics-are-shifting-quickly-as-nvidia-continues-reporting-strong-results"&gt;&lt;strong&gt;Market dynamics are shifting quickly as Nvidia continues reporting strong results&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Nvidia’s revenue growth decelerated from triple digits to 56% year over year. While that’s still impressive, it’s clear the trajectory of the company’s growth is changing. Competition is starting to have an effect on their growth, with this quarter seeing the most noticeable impact. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In particular,&amp;nbsp;China’s strategic role&amp;nbsp;in the global AI race drew pointed attention from analysts. As Joe Moore of Morgan Stanley probed late in the call, Huang estimated the&amp;nbsp;2025 China AI infrastructure opportunity at $50 billion. He communicated both optimism about the scale (“the second largest computing market in the world,” with “about 50% of the world’s AI researchers”) and realism about regulatory friction.&lt;/p&gt;



&lt;p&gt;A third pivotal force shaping Nvidia’s trajectory is the expanding complexity and cost of AI infrastructure itself. As hyperscalers and long-standing Nvidia clients invest billions in next-generation build-outs, the networking demands, compute and energy efficiency have intensified.&lt;/p&gt;



&lt;p&gt;Huang’s comments highlighted how “orders of magnitude speed up” from new platforms like Blackwell and innovations in NVLink, InfiniBand, and Spectrum XGS networking redefine the economic returns for customers’ data center capital. Meanwhile, supply chain pressures and the need for constant technological reinvention mean Nvidia must maintain a relentless pace and adaptability to remain entrenched as the preferred architecture provider.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-nvidia-s-path-forward-is-clear"&gt;&lt;strong&gt;Nvidia’s path forward is clear&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Nvidia issuing guidance for Q3 of $54 billion sends the signal that the core part of their DNA is as strong as ever. Continually improving Blackwell while developing Rubin architecture is evidence that their ability to innovate is as strong as ever.&lt;/p&gt;



&lt;p&gt;The question is whether a new type of innovative challenge they’re facing is one they can take on and win with the same level of development intensity they’ve shown in the past. VentureBeat expects Broadcom to continue aggressively pursuing new hyperscaler partnerships and strengthen its roadmap for specific optimizations aimed at inference workloads. Every ASIC competitor will take the competitive intensity they have to a new level, looking to get design wins that create a higher switching costs as well.&lt;/p&gt;



&lt;p&gt;Huang closed the earnings call, acknowledging the stakes: “A new industrial revolution has started. The AI race is on.” That race includes serious competitors Nvidia dismissed just two years ago. Broadcom, Google, Amazon and others invest billions in custom silicon. They’re not experimenting anymore. They’re shipping at scale.&lt;/p&gt;



&lt;p&gt;Nvidia faces its strongest competition since CUDA’s dominance began. The company’s $46.7 billion quarter proves its strength. However, custom silicon’s momentum suggests that the game has changed. The next chapter will test whether Nvidia’s platform advantages outweigh ASIC economics. VentureBeat expects technology buyers to follow the path of fund managers, betting on both Nvidia to sustain its lucrative customer base and ASIC competitors to secure design wins as intensifying competition drives greater market fragmentation.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/nvidias-strong-q2-results-cant-mask-the-asic-challenge-in-their-future/</guid><pubDate>Thu, 28 Aug 2025 21:09:54 +0000</pubDate></item><item><title>Nous Research drops Hermes 4 AI models that outperform ChatGPT without content restrictions (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/nous-research-drops-hermes-4-ai-models-that-outperform-chatgpt-without-content-restrictions/</link><description>&lt;p&gt;Nous Research, a secretive artificial intelligence startup that has emerged as a leading voice in the open-source AI movement, quietly released Hermes 4 on Monday, a family of large language models that the company claims can match the performance of leading proprietary systems while offering unprecedented user control and minimal content restrictions.&lt;/p&gt;&lt;p&gt;The release represents a significant escalation in the battle between open-source AI advocates and major technology companies over who should control access to advanced artificial intelligence capabilities. Unlike models from OpenAI, Google, or Anthropic, Hermes 4 is designed to respond to nearly any request without the safety guardrails that have become standard in commercial AI systems.&lt;/p&gt;&lt;p&gt;“Hermes 4 builds on our legacy of user-aligned models with expanded test-time compute capabilities,” Nous Research announced on X (formerly Twitter). “Special attention was given to making the models creative and interesting to interact with, unencumbered by censorship, and neutrally aligned while maintaining state of the art level math, coding, and reasoning performance for open weight models.”&lt;/p&gt;&lt;p&gt;Hermes 4 introduces what Nous Research calls “hybrid reasoning,” allowing users to toggle between fast responses and deeper, step-by-step thinking processes. When activated, the models generate their internal reasoning within special &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tags before providing a final answer — similar to OpenAI’s o1 reasoning models but with full transparency into the AI’s thought process.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The technical achievement is substantial. In testing, Hermes 4’s largest 405-billion parameter model scored 96.3% on the MATH-500 benchmark in reasoning mode and 81.9% on the challenging AIME’24 mathematics competition — performance that rivals or exceeds many proprietary systems costing millions more to develop.&lt;/p&gt;



&lt;p&gt;“The challenge is making thinking traces useful and verifiable without runaway reasoning,” noted AI researcher Rohan Paul on X, highlighting one of the technical breakthroughs in the release.&lt;/p&gt;



&lt;p&gt;Perhaps most notably, Hermes 4 achieved the highest score among all tested models on “RefusalBench,” a new benchmark Nous Research created to measure how often AI systems refuse to answer questions. The model scored 57.1% in reasoning mode, significantly outperforming GPT-4o (17.67%) and Claude Sonnet 4 (17%).&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-3016215" height="562" src="https://venturebeat.com/wp-content/uploads/2025/08/GzS-zJWa4AEonw_.png" width="572" /&gt;&lt;figcaption class="wp-element-caption"&gt;Hermes 4 models from Nous Research answered significantly more questions than competing AI systems on RefusalBench, a test measuring how often models refuse to respond to user requests. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-inside-dataforge-and-atropos-the-breakthrough-training-systems-behind-hermes-4-s-capabilities"&gt;Inside DataForge and Atropos: The breakthrough training systems behind Hermes 4’s capabilities&lt;/h2&gt;



&lt;p&gt;Behind Hermes 4’s capabilities lies a sophisticated training infrastructure that Nous Research has developed over several years. The models were trained using two novel systems: DataForge, a graph-based synthetic data generator, and Atropos, an open-source reinforcement learning framework.&lt;/p&gt;



&lt;p&gt;DataForge creates training data through what the company describes as “random walks” through directed graphs, transforming simple pre-training data into complex instruction-following examples. The system can, for instance, take a Wikipedia article and transform it into a rap song, then generate questions and answers based on that transformation.&lt;/p&gt;



&lt;p&gt;Atropos, meanwhile, operates like hundreds of specialized training environments where AI models practice specific skills—mathematics, coding, tool use, and creative writing—receiving feedback only when they produce correct solutions. This “rejection sampling” approach ensures that only verified, high-quality responses make it into the training data.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Atropos is Nous' Reinforcement Learning framework&lt;/p&gt;&lt;p&gt;Atropos is an open source reinforcement learning environment by Nous that has hundreds of “gyms” (like math, coding, games, tool‑use, vision) to train and evaluate LLM trajectories via scalable, async RL loops.&lt;/p&gt;&lt;p&gt;In other words… pic.twitter.com/fjxaQKClEZ&lt;/p&gt;— Tommy (@Shaughnessy119) August 26, 2025&lt;/blockquote&gt; 



&lt;p&gt;“Nous used these environments to generate the dataset for Hermes 4!” explained Tommy Shaughnessy, a venture capitalist at Delphi Ventures who has invested in Nous Research. “All in the dataset contains 3.5 million reasoning samples and 1.6 million non-reasoning samples! Hermes was trained on RL data, not just static datasets of question and answer!”&lt;/p&gt;



&lt;p&gt;The training process required 192 Nvidia B200 GPUs and 71,616 GPU hours for the largest model — a significant but not unprecedented computational investment that demonstrates how specialized techniques can compete with the massive scale of tech giants.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-nous-research-believes-ai-safety-guardrails-are-annoying-as-hell-and-hurt-innovation"&gt;Why Nous Research believes AI safety guardrails are ‘annoying as hell’ and hurt innovation&lt;/h2&gt;



&lt;p&gt;Nous Research has built its reputation on a philosophy that puts user control above corporate content policies. The company’s models are designed to be “steerable,” meaning they can be fine-tuned or prompted to behave in specific ways without the rigid safety constraints that characterize commercial AI systems.&lt;/p&gt;



&lt;p&gt;“Hermes 4 is not shackled by disclaimers, rules and being overly cautious which is annoying as hell and hurts innovation and usability,” wrote Shaughnessy in a detailed thread analyzing the release. “If its open source but refuses all requests its pointless. Not an issue with Hermes 4.”&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Hermes 4 is not shackled by disclaimers, rules and being overly cautious which is annoying as hell and hurts innovation and usability.&lt;/p&gt;&lt;p&gt;Hermes 4 70B is at the complete opposite of the spectrum vs OpenAI's open source model. It's also ~4x more open vs ChatGPT 4o!&lt;/p&gt;&lt;p&gt;If its open… pic.twitter.com/q5RpX1oOzo&lt;/p&gt;— Tommy (@Shaughnessy119) August 26, 2025&lt;/blockquote&gt; 



&lt;p&gt;This approach has made Nous Research popular among AI researchers and developers who want maximum flexibility, but it also places the company at the center of ongoing debates about AI safety and content moderation. While the models can theoretically be used for harmful purposes, Nous Research argues that transparency and user control are preferable to corporate gatekeeping.&lt;/p&gt;



&lt;p&gt;The company’s technical report, released alongside the models, provides unprecedented detail about the training process, evaluation results, and even the actual text outputs from benchmark tests. “We believe this report sets a new standard for transparency in benchmarking,” the company stated.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-a-small-startup-with-192-gpus-is-competing-against-big-tech-s-billion-dollar-ai-budgets"&gt;How a small startup with 192 GPUs is competing against Big Tech’s billion-dollar AI budgets&lt;/h2&gt;



&lt;p&gt;Hermes 4‘s release comes at a pivotal moment in the AI industry. While major technology companies have poured billions into developing increasingly powerful AI systems, a growing open-source movement argues that these capabilities should not be controlled by a handful of corporations.&lt;/p&gt;



&lt;p&gt;Recent months have seen significant advances in open-source AI, with models like Meta’s Llama 3.1, DeepSeek’s R1, and Alibaba’s Qwen series achieving performance that rivals proprietary systems. Hermes 4 represents another step in this progression, particularly in the area of reasoning—long considered a strength of closed systems like OpenAI’s o1.&lt;/p&gt;



&lt;p&gt;“First up, Nous is a startup with dozens of extremely talented people,” noted Shaughnessy. “They do not have the $100b+ annual capex spend of a hyperscaler nor 1,000’s of employees and despite that they continue to put out innovative models and research at an insane pace.”&lt;/p&gt;



&lt;p&gt;The startup, which raised $65 million in funding earlier this year led by Paradigm, has also been developing Psyche Network, a distributed training system that aims to coordinate AI training across internet-connected computers using blockchain technology.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-technical-fix-that-stopped-hermes-4-from-thinking-in-endless-loops"&gt;The technical fix that stopped Hermes 4 from thinking in endless loops&lt;/h2&gt;



&lt;p&gt;One of Hermes 4‘s most significant technical contributions addresses a problem plaguing reasoning models: overly long thinking processes. The researchers found that their smaller 14-billion parameter model would reach maximum context length 60% of the time when reasoning, essentially getting stuck in endless loops of thinking.&lt;/p&gt;



&lt;p&gt;Their solution involved a second training stage that teaches models to stop reasoning at exactly 30,000 tokens, reducing overlong generation by 65-79% while maintaining most of the reasoning performance. This “length control” technique could prove valuable for the broader AI research community.&lt;/p&gt;



&lt;p&gt;“Smaller models (&amp;lt;14B) tend to overthink when distilled, but larger models don’t,” observed AI researcher Muyu He on X, highlighting insights from the technical report.&lt;/p&gt;



&lt;p&gt;However, Hermes 4 still faces limitations common to open-source models. Despite impressive benchmark performance, the models require significant computational resources to run and may not match the ease of use or reliability of commercial AI services for many applications.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-where-to-try-hermes-4-and-what-it-costs-compared-to-chatgpt-and-claude"&gt;Where to try Hermes 4 and what it costs compared to ChatGPT and Claude&lt;/h2&gt;



&lt;p&gt;Nous Research has made Hermes 4 available through multiple channels, reflecting the open-source philosophy. The model weights are freely downloadable on Hugging Face, while the company also offers API access through its revamped chat interface and partnerships with inference providers like Chutes, Nebius, and Luminal.&lt;/p&gt;



&lt;p&gt;“You can try Hermes 4 in the new, revamped Nous Chat UI,” the company announced, highlighting features like parallel interactions and a memory system.&lt;/p&gt;



&lt;p&gt;For enterprise users and researchers, the models represent a potentially attractive alternative to paying for API access to proprietary systems, especially for applications requiring high levels of customization or handling of sensitive content.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-bigger-picture-what-hermes-4-means-for-the-future-of-ai-development"&gt;The bigger picture: What Hermes 4 means for the future of AI development&lt;/h2&gt;



&lt;p&gt;The release of Hermes 4 represents more than just another AI model launch — it’s a statement about who should control the future of artificial intelligence. In an industry increasingly dominated by a handful of tech giants with virtually unlimited resources, Nous Research has demonstrated that innovation can still come from unexpected places.&lt;/p&gt;



&lt;p&gt;The company’s approach raises fundamental questions about the trade-offs between safety and capability, between corporate control and user freedom. While major technology companies argue that careful content moderation and safety guardrails are essential for responsible AI deployment, Nous Research contends that transparency and user agency are more important than corporate-imposed restrictions.&lt;/p&gt;



&lt;p&gt;Whether this philosophy will ultimately prove beneficial or problematic remains to be seen. But one thing is certain: Hermes 4 has shown that the future of AI won’t be determined solely by the companies with the deepest pockets.&lt;/p&gt;



&lt;p&gt;In a field where yesterday’s impossibilities become tomorrow’s commodities, Nous Research just proved that the only thing more dangerous than an AI that says no might be one that’s willing to say yes.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;Nous Research, a secretive artificial intelligence startup that has emerged as a leading voice in the open-source AI movement, quietly released Hermes 4 on Monday, a family of large language models that the company claims can match the performance of leading proprietary systems while offering unprecedented user control and minimal content restrictions.&lt;/p&gt;&lt;p&gt;The release represents a significant escalation in the battle between open-source AI advocates and major technology companies over who should control access to advanced artificial intelligence capabilities. Unlike models from OpenAI, Google, or Anthropic, Hermes 4 is designed to respond to nearly any request without the safety guardrails that have become standard in commercial AI systems.&lt;/p&gt;&lt;p&gt;“Hermes 4 builds on our legacy of user-aligned models with expanded test-time compute capabilities,” Nous Research announced on X (formerly Twitter). “Special attention was given to making the models creative and interesting to interact with, unencumbered by censorship, and neutrally aligned while maintaining state of the art level math, coding, and reasoning performance for open weight models.”&lt;/p&gt;&lt;p&gt;Hermes 4 introduces what Nous Research calls “hybrid reasoning,” allowing users to toggle between fast responses and deeper, step-by-step thinking processes. When activated, the models generate their internal reasoning within special &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tags before providing a final answer — similar to OpenAI’s o1 reasoning models but with full transparency into the AI’s thought process.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The technical achievement is substantial. In testing, Hermes 4’s largest 405-billion parameter model scored 96.3% on the MATH-500 benchmark in reasoning mode and 81.9% on the challenging AIME’24 mathematics competition — performance that rivals or exceeds many proprietary systems costing millions more to develop.&lt;/p&gt;



&lt;p&gt;“The challenge is making thinking traces useful and verifiable without runaway reasoning,” noted AI researcher Rohan Paul on X, highlighting one of the technical breakthroughs in the release.&lt;/p&gt;



&lt;p&gt;Perhaps most notably, Hermes 4 achieved the highest score among all tested models on “RefusalBench,” a new benchmark Nous Research created to measure how often AI systems refuse to answer questions. The model scored 57.1% in reasoning mode, significantly outperforming GPT-4o (17.67%) and Claude Sonnet 4 (17%).&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-3016215" height="562" src="https://venturebeat.com/wp-content/uploads/2025/08/GzS-zJWa4AEonw_.png" width="572" /&gt;&lt;figcaption class="wp-element-caption"&gt;Hermes 4 models from Nous Research answered significantly more questions than competing AI systems on RefusalBench, a test measuring how often models refuse to respond to user requests. (Credit: Nous Research)&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-inside-dataforge-and-atropos-the-breakthrough-training-systems-behind-hermes-4-s-capabilities"&gt;Inside DataForge and Atropos: The breakthrough training systems behind Hermes 4’s capabilities&lt;/h2&gt;



&lt;p&gt;Behind Hermes 4’s capabilities lies a sophisticated training infrastructure that Nous Research has developed over several years. The models were trained using two novel systems: DataForge, a graph-based synthetic data generator, and Atropos, an open-source reinforcement learning framework.&lt;/p&gt;



&lt;p&gt;DataForge creates training data through what the company describes as “random walks” through directed graphs, transforming simple pre-training data into complex instruction-following examples. The system can, for instance, take a Wikipedia article and transform it into a rap song, then generate questions and answers based on that transformation.&lt;/p&gt;



&lt;p&gt;Atropos, meanwhile, operates like hundreds of specialized training environments where AI models practice specific skills—mathematics, coding, tool use, and creative writing—receiving feedback only when they produce correct solutions. This “rejection sampling” approach ensures that only verified, high-quality responses make it into the training data.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Atropos is Nous' Reinforcement Learning framework&lt;/p&gt;&lt;p&gt;Atropos is an open source reinforcement learning environment by Nous that has hundreds of “gyms” (like math, coding, games, tool‑use, vision) to train and evaluate LLM trajectories via scalable, async RL loops.&lt;/p&gt;&lt;p&gt;In other words… pic.twitter.com/fjxaQKClEZ&lt;/p&gt;— Tommy (@Shaughnessy119) August 26, 2025&lt;/blockquote&gt; 



&lt;p&gt;“Nous used these environments to generate the dataset for Hermes 4!” explained Tommy Shaughnessy, a venture capitalist at Delphi Ventures who has invested in Nous Research. “All in the dataset contains 3.5 million reasoning samples and 1.6 million non-reasoning samples! Hermes was trained on RL data, not just static datasets of question and answer!”&lt;/p&gt;



&lt;p&gt;The training process required 192 Nvidia B200 GPUs and 71,616 GPU hours for the largest model — a significant but not unprecedented computational investment that demonstrates how specialized techniques can compete with the massive scale of tech giants.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-nous-research-believes-ai-safety-guardrails-are-annoying-as-hell-and-hurt-innovation"&gt;Why Nous Research believes AI safety guardrails are ‘annoying as hell’ and hurt innovation&lt;/h2&gt;



&lt;p&gt;Nous Research has built its reputation on a philosophy that puts user control above corporate content policies. The company’s models are designed to be “steerable,” meaning they can be fine-tuned or prompted to behave in specific ways without the rigid safety constraints that characterize commercial AI systems.&lt;/p&gt;



&lt;p&gt;“Hermes 4 is not shackled by disclaimers, rules and being overly cautious which is annoying as hell and hurts innovation and usability,” wrote Shaughnessy in a detailed thread analyzing the release. “If its open source but refuses all requests its pointless. Not an issue with Hermes 4.”&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Hermes 4 is not shackled by disclaimers, rules and being overly cautious which is annoying as hell and hurts innovation and usability.&lt;/p&gt;&lt;p&gt;Hermes 4 70B is at the complete opposite of the spectrum vs OpenAI's open source model. It's also ~4x more open vs ChatGPT 4o!&lt;/p&gt;&lt;p&gt;If its open… pic.twitter.com/q5RpX1oOzo&lt;/p&gt;— Tommy (@Shaughnessy119) August 26, 2025&lt;/blockquote&gt; 



&lt;p&gt;This approach has made Nous Research popular among AI researchers and developers who want maximum flexibility, but it also places the company at the center of ongoing debates about AI safety and content moderation. While the models can theoretically be used for harmful purposes, Nous Research argues that transparency and user control are preferable to corporate gatekeeping.&lt;/p&gt;



&lt;p&gt;The company’s technical report, released alongside the models, provides unprecedented detail about the training process, evaluation results, and even the actual text outputs from benchmark tests. “We believe this report sets a new standard for transparency in benchmarking,” the company stated.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-a-small-startup-with-192-gpus-is-competing-against-big-tech-s-billion-dollar-ai-budgets"&gt;How a small startup with 192 GPUs is competing against Big Tech’s billion-dollar AI budgets&lt;/h2&gt;



&lt;p&gt;Hermes 4‘s release comes at a pivotal moment in the AI industry. While major technology companies have poured billions into developing increasingly powerful AI systems, a growing open-source movement argues that these capabilities should not be controlled by a handful of corporations.&lt;/p&gt;



&lt;p&gt;Recent months have seen significant advances in open-source AI, with models like Meta’s Llama 3.1, DeepSeek’s R1, and Alibaba’s Qwen series achieving performance that rivals proprietary systems. Hermes 4 represents another step in this progression, particularly in the area of reasoning—long considered a strength of closed systems like OpenAI’s o1.&lt;/p&gt;



&lt;p&gt;“First up, Nous is a startup with dozens of extremely talented people,” noted Shaughnessy. “They do not have the $100b+ annual capex spend of a hyperscaler nor 1,000’s of employees and despite that they continue to put out innovative models and research at an insane pace.”&lt;/p&gt;



&lt;p&gt;The startup, which raised $65 million in funding earlier this year led by Paradigm, has also been developing Psyche Network, a distributed training system that aims to coordinate AI training across internet-connected computers using blockchain technology.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-technical-fix-that-stopped-hermes-4-from-thinking-in-endless-loops"&gt;The technical fix that stopped Hermes 4 from thinking in endless loops&lt;/h2&gt;



&lt;p&gt;One of Hermes 4‘s most significant technical contributions addresses a problem plaguing reasoning models: overly long thinking processes. The researchers found that their smaller 14-billion parameter model would reach maximum context length 60% of the time when reasoning, essentially getting stuck in endless loops of thinking.&lt;/p&gt;



&lt;p&gt;Their solution involved a second training stage that teaches models to stop reasoning at exactly 30,000 tokens, reducing overlong generation by 65-79% while maintaining most of the reasoning performance. This “length control” technique could prove valuable for the broader AI research community.&lt;/p&gt;



&lt;p&gt;“Smaller models (&amp;lt;14B) tend to overthink when distilled, but larger models don’t,” observed AI researcher Muyu He on X, highlighting insights from the technical report.&lt;/p&gt;



&lt;p&gt;However, Hermes 4 still faces limitations common to open-source models. Despite impressive benchmark performance, the models require significant computational resources to run and may not match the ease of use or reliability of commercial AI services for many applications.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-where-to-try-hermes-4-and-what-it-costs-compared-to-chatgpt-and-claude"&gt;Where to try Hermes 4 and what it costs compared to ChatGPT and Claude&lt;/h2&gt;



&lt;p&gt;Nous Research has made Hermes 4 available through multiple channels, reflecting the open-source philosophy. The model weights are freely downloadable on Hugging Face, while the company also offers API access through its revamped chat interface and partnerships with inference providers like Chutes, Nebius, and Luminal.&lt;/p&gt;



&lt;p&gt;“You can try Hermes 4 in the new, revamped Nous Chat UI,” the company announced, highlighting features like parallel interactions and a memory system.&lt;/p&gt;



&lt;p&gt;For enterprise users and researchers, the models represent a potentially attractive alternative to paying for API access to proprietary systems, especially for applications requiring high levels of customization or handling of sensitive content.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-bigger-picture-what-hermes-4-means-for-the-future-of-ai-development"&gt;The bigger picture: What Hermes 4 means for the future of AI development&lt;/h2&gt;



&lt;p&gt;The release of Hermes 4 represents more than just another AI model launch — it’s a statement about who should control the future of artificial intelligence. In an industry increasingly dominated by a handful of tech giants with virtually unlimited resources, Nous Research has demonstrated that innovation can still come from unexpected places.&lt;/p&gt;



&lt;p&gt;The company’s approach raises fundamental questions about the trade-offs between safety and capability, between corporate control and user freedom. While major technology companies argue that careful content moderation and safety guardrails are essential for responsible AI deployment, Nous Research contends that transparency and user agency are more important than corporate-imposed restrictions.&lt;/p&gt;



&lt;p&gt;Whether this philosophy will ultimately prove beneficial or problematic remains to be seen. But one thing is certain: Hermes 4 has shown that the future of AI won’t be determined solely by the companies with the deepest pockets.&lt;/p&gt;



&lt;p&gt;In a field where yesterday’s impossibilities become tomorrow’s commodities, Nous Research just proved that the only thing more dangerous than an AI that says no might be one that’s willing to say yes.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/nous-research-drops-hermes-4-ai-models-that-outperform-chatgpt-without-content-restrictions/</guid><pubDate>Thu, 28 Aug 2025 21:46:07 +0000</pubDate></item><item><title>Trump administration’s deal is structured to prevent Intel from selling foundry unit (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/28/trump-administrations-deal-is-structured-to-prevent-intel-from-selling-foundry-unit/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/08/newsroom-robert-noyce-bldg-2.jpg.rendition.intel_.web_.1920.1080.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration seems intent on controlling Intel’s ability to make key business decisions around its floundering foundry business unit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to reporting from the Financial Times, at a Deutsche Bank conference on Thursday, Intel’s CFO David Zinsner shared new details about the company’s recent deal with the Trump administration, which gave the U.S. government a 10% equity stake.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal was structured in a way to penalize Intel if it spins out its foundry business unit, which makes custom chips for outside customers, within the next few years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week’s deal included a five-year warrant that would allow the U.S. government to take an additional 5% of Intel, at $20 a share, if the company held less than 51% equity in its foundry business. Zinsner said he expects that warrant to expire.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think from the government’s perspective, they were aligned with that; they didn’t want to see us take the business and spin it off or sell it to somebody,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zinsner added that the company received $5.7 billion in cash on Wednesday, as a result of last week’s deal, according to Reuters. (That cash comes from the remaining grants previously awarded, but not yet paid, to Intel under the U.S. CHIPS and Science Act.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;White House press secretary Karoline Leavitt told reporters today that the deal was still being ironed out.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Intel declined to comment on the deal beyond Zinsner’s remarks. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This deal structure is clearly a testament to the Trump administration’s desire to bring more chip manufacturing to the United States as many players in the industry turn to Taiwan Semiconductor Manufacturing Company’s offshore manufacturing instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But this warrant also forces Intel to keep a business unit that is losing money. Intel Foundry reported an operating income loss of $3.1 billion during the second quarter and has been a source of strife for the semiconductor business.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There have been calls from analysts, board members, and investors alike to spin out the struggling foundry unit, which looked like it might actually happen last fall, before Intel Foundry’s architect, former CEO Pat Gelsinger, retired suddenly in December.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/08/newsroom-robert-noyce-bldg-2.jpg.rendition.intel_.web_.1920.1080.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration seems intent on controlling Intel’s ability to make key business decisions around its floundering foundry business unit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to reporting from the Financial Times, at a Deutsche Bank conference on Thursday, Intel’s CFO David Zinsner shared new details about the company’s recent deal with the Trump administration, which gave the U.S. government a 10% equity stake.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal was structured in a way to penalize Intel if it spins out its foundry business unit, which makes custom chips for outside customers, within the next few years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week’s deal included a five-year warrant that would allow the U.S. government to take an additional 5% of Intel, at $20 a share, if the company held less than 51% equity in its foundry business. Zinsner said he expects that warrant to expire.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think from the government’s perspective, they were aligned with that; they didn’t want to see us take the business and spin it off or sell it to somebody,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zinsner added that the company received $5.7 billion in cash on Wednesday, as a result of last week’s deal, according to Reuters. (That cash comes from the remaining grants previously awarded, but not yet paid, to Intel under the U.S. CHIPS and Science Act.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;White House press secretary Karoline Leavitt told reporters today that the deal was still being ironed out.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Intel declined to comment on the deal beyond Zinsner’s remarks. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This deal structure is clearly a testament to the Trump administration’s desire to bring more chip manufacturing to the United States as many players in the industry turn to Taiwan Semiconductor Manufacturing Company’s offshore manufacturing instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But this warrant also forces Intel to keep a business unit that is losing money. Intel Foundry reported an operating income loss of $3.1 billion during the second quarter and has been a source of strife for the semiconductor business.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There have been calls from analysts, board members, and investors alike to spin out the struggling foundry unit, which looked like it might actually happen last fall, before Intel Foundry’s architect, former CEO Pat Gelsinger, retired suddenly in December.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/28/trump-administrations-deal-is-structured-to-prevent-intel-from-selling-foundry-unit/</guid><pubDate>Thu, 28 Aug 2025 21:56:27 +0000</pubDate></item><item><title>In crowded voice AI market, OpenAI bets on instruction-following and expressive speech to win enterprise adoption (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/in-crowded-voice-ai-market-openai-bets-on-instruction-following-and-expressive-speech-to-win-enterprise-adoption/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;OpenAI adds to an increasingly competitive AI voice market for enterprises with its new model, gpt-realtime, that follows complex instructions and with voices “that sound more natural and expressive.”&lt;/p&gt;



&lt;p&gt;As voice AI continues to grow, and customers find use cases such as customer service calls or real-time translation, the market for realistic-sounding AI voices that also offer enterprise-grade security is heating up. OpenAI claims its new model provides a more human-like voice, but it still needs to compete against companies like ElevenLabs.&lt;/p&gt;



&lt;p&gt;The model will be available on the Realtime API, which the company also made generally available. Along with the gpt-realtime model, OpenAI also released new voices on the API, which it calls Cedar and Marin, and updated its other voices to work with the latest model.&lt;/p&gt;



&lt;p&gt;OpenAI said in a livestream that it worked with its customers who are building voice applications to train gpt-realtime and “carefully aligned the model to evals that are built on real-world scenarios like customer support and academic tutoring.”&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p&gt;The company touted the model’s ability to create emotive, natural-sounding voices that also align with how developers build with the technology.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-speech-to-speech-models"&gt;Speech-to-speech models&lt;/h2&gt;



&lt;p&gt;The model operates within a speech-to-speech framework, enabling it to understand spoken prompts and respond vocally. Speech-to-speech models are ideally suited for real-time responses, where a person, typically a customer, interacts with an application.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For example, a customer wants to return some products and calls a customer service platform. They could be talking to an AI voice assistant that responds to questions and requests as if they were speaking with a human.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In a livestream, OpenAI customers T-Mobile showcased an AI voice-powered agent that helps people find new phones. Another customer, the real estate search platform Zillow, showcased an agent who helps someone narrow down a neighborhood to find the perfect place.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;OpenAI said gpt-realtime is its “most advanced, production-ready voice model.” Like its other voice models, it can switch languages mid-sentence. However, OpenAI researchers noted gpt-realtime can follow more complex instructions like “speak emphatically in a French accent.”&lt;/p&gt;



&lt;p&gt;But gpt-realtime faces competition from other models that many brands already use. ElevenLabs released Conversation AI 2.0 in May. Soundhound partners with fast food franchises for an AI voice drive-thru. Emphatic AI startup Hume has launched its EVI 3 model, which allows users to generate AI versions of their own voice.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As enterprises discover various use cases for voice AI, even more general model providers that offer multimodal LLMs are making a case for themselves. Mistral released its new Voxtral model, stating it would work well with real-time translation. Google is enhancing its audio capabilities and gaining popularity with an audio feature on NotebookLM that converts research notes into a podcast.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-better-instruction-following"&gt;Better instruction following&lt;/h2&gt;



&lt;p&gt;OpenAI said gpt-realtime is smarter and understands native audio better, including the ability to catch non-verbal cues like laughs or sighs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Benchmarking using the Big Bench Audio eval showed the model scoring 82.8% in accuracy, compared to its previous model, which scored 65.6%. OpenAI did not provide numbers testing gpt-realtime against models from its competitors.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-3016219" height="465" src="https://venturebeat.com/wp-content/uploads/2025/08/image_abee53.png" width="777" /&gt;&lt;/figure&gt;



&lt;p&gt;OpenAI focused on improving the model’s instruction-following capabilities, ensuring the model would adhere to directions more effectively. The new model achieves a score of 30.5% on the MultiChallenge audio benchmark. The engineers also beefed up function calling so gpt-realtime can access the correct tools.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-realtime-api-updates"&gt;Realtime API updates&lt;/h2&gt;



&lt;p&gt;To support the new model and enhance how enterprises integrate real-time AI capabilities into their applications, OpenAI has added several new features to the Realtime API.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It can now support MCP and recognize image inputs, allowing it to inform users about what it sees in real-time. This is a feature Google heavily emphasized during its Project Astra presentation last year.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The Realtime API can also handle Session Initiation Protocol (SIP). SIP connects apps to phones like a public phone network or desk phones, opening up more contact center use cases. Users can also save and reuse prompts on the API.&lt;/p&gt;



&lt;p&gt;So far, people are impressed with the model, although these are still initial tests of a model that was recently released.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Tbh, the MCP and SIP features are the real story here, not just another model. &lt;/p&gt;&lt;p&gt;The ability to connect to external tools and systems seamlessly is what will finally move these models from being impressive demos to being integrated into actual workflows. &lt;/p&gt;&lt;p&gt;The real time aspect…&lt;/p&gt;— JK (@_junaidkhalid1) August 28, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Testing out gpt-realtime&lt;/p&gt;&lt;p&gt;Initial review:&lt;br /&gt;– Noticable audio improvement&lt;br /&gt;– It's a stickler for the instructions (very good)&lt;br /&gt;– Feels fast pic.twitter.com/LtyCs0QLXV&lt;/p&gt;— Jake Colling (@JacobColling) August 28, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Well, GPT-realtime got a livestream not because most users are interested, but for strategic business reasons&lt;/p&gt;&lt;p&gt;Call centers are a major target for LLM providers and the first company to reach a real breakthrough will get massive revenue&lt;/p&gt;— AnKo (@anko_979) August 28, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Pros &amp;amp; Cons from @OpenAI real-time update from someone building in AI audio:&lt;/p&gt;&lt;p&gt;Pro: Better function calling, more emotion, 20% cheaper, better control, image is cool but won't use&lt;/p&gt;&lt;p&gt;Con: no custom voices (creative experience MUST HAVE), still *expensive* vs TTS-LLM-STT pipelines&lt;/p&gt;— Gavin Purcell (@gavinpurcell) August 28, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;OpenAI reduced prices for gpt-realtime by 20% to $32 per million audio input tokens and $64 for audio output tokens.&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;OpenAI adds to an increasingly competitive AI voice market for enterprises with its new model, gpt-realtime, that follows complex instructions and with voices “that sound more natural and expressive.”&lt;/p&gt;



&lt;p&gt;As voice AI continues to grow, and customers find use cases such as customer service calls or real-time translation, the market for realistic-sounding AI voices that also offer enterprise-grade security is heating up. OpenAI claims its new model provides a more human-like voice, but it still needs to compete against companies like ElevenLabs.&lt;/p&gt;



&lt;p&gt;The model will be available on the Realtime API, which the company also made generally available. Along with the gpt-realtime model, OpenAI also released new voices on the API, which it calls Cedar and Marin, and updated its other voices to work with the latest model.&lt;/p&gt;



&lt;p&gt;OpenAI said in a livestream that it worked with its customers who are building voice applications to train gpt-realtime and “carefully aligned the model to evals that are built on real-world scenarios like customer support and academic tutoring.”&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p&gt;The company touted the model’s ability to create emotive, natural-sounding voices that also align with how developers build with the technology.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-speech-to-speech-models"&gt;Speech-to-speech models&lt;/h2&gt;



&lt;p&gt;The model operates within a speech-to-speech framework, enabling it to understand spoken prompts and respond vocally. Speech-to-speech models are ideally suited for real-time responses, where a person, typically a customer, interacts with an application.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For example, a customer wants to return some products and calls a customer service platform. They could be talking to an AI voice assistant that responds to questions and requests as if they were speaking with a human.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In a livestream, OpenAI customers T-Mobile showcased an AI voice-powered agent that helps people find new phones. Another customer, the real estate search platform Zillow, showcased an agent who helps someone narrow down a neighborhood to find the perfect place.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;OpenAI said gpt-realtime is its “most advanced, production-ready voice model.” Like its other voice models, it can switch languages mid-sentence. However, OpenAI researchers noted gpt-realtime can follow more complex instructions like “speak emphatically in a French accent.”&lt;/p&gt;



&lt;p&gt;But gpt-realtime faces competition from other models that many brands already use. ElevenLabs released Conversation AI 2.0 in May. Soundhound partners with fast food franchises for an AI voice drive-thru. Emphatic AI startup Hume has launched its EVI 3 model, which allows users to generate AI versions of their own voice.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As enterprises discover various use cases for voice AI, even more general model providers that offer multimodal LLMs are making a case for themselves. Mistral released its new Voxtral model, stating it would work well with real-time translation. Google is enhancing its audio capabilities and gaining popularity with an audio feature on NotebookLM that converts research notes into a podcast.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-better-instruction-following"&gt;Better instruction following&lt;/h2&gt;



&lt;p&gt;OpenAI said gpt-realtime is smarter and understands native audio better, including the ability to catch non-verbal cues like laughs or sighs.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Benchmarking using the Big Bench Audio eval showed the model scoring 82.8% in accuracy, compared to its previous model, which scored 65.6%. OpenAI did not provide numbers testing gpt-realtime against models from its competitors.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-3016219" height="465" src="https://venturebeat.com/wp-content/uploads/2025/08/image_abee53.png" width="777" /&gt;&lt;/figure&gt;



&lt;p&gt;OpenAI focused on improving the model’s instruction-following capabilities, ensuring the model would adhere to directions more effectively. The new model achieves a score of 30.5% on the MultiChallenge audio benchmark. The engineers also beefed up function calling so gpt-realtime can access the correct tools.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-realtime-api-updates"&gt;Realtime API updates&lt;/h2&gt;



&lt;p&gt;To support the new model and enhance how enterprises integrate real-time AI capabilities into their applications, OpenAI has added several new features to the Realtime API.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It can now support MCP and recognize image inputs, allowing it to inform users about what it sees in real-time. This is a feature Google heavily emphasized during its Project Astra presentation last year.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The Realtime API can also handle Session Initiation Protocol (SIP). SIP connects apps to phones like a public phone network or desk phones, opening up more contact center use cases. Users can also save and reuse prompts on the API.&lt;/p&gt;



&lt;p&gt;So far, people are impressed with the model, although these are still initial tests of a model that was recently released.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Tbh, the MCP and SIP features are the real story here, not just another model. &lt;/p&gt;&lt;p&gt;The ability to connect to external tools and systems seamlessly is what will finally move these models from being impressive demos to being integrated into actual workflows. &lt;/p&gt;&lt;p&gt;The real time aspect…&lt;/p&gt;— JK (@_junaidkhalid1) August 28, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Testing out gpt-realtime&lt;/p&gt;&lt;p&gt;Initial review:&lt;br /&gt;– Noticable audio improvement&lt;br /&gt;– It's a stickler for the instructions (very good)&lt;br /&gt;– Feels fast pic.twitter.com/LtyCs0QLXV&lt;/p&gt;— Jake Colling (@JacobColling) August 28, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Well, GPT-realtime got a livestream not because most users are interested, but for strategic business reasons&lt;/p&gt;&lt;p&gt;Call centers are a major target for LLM providers and the first company to reach a real breakthrough will get massive revenue&lt;/p&gt;— AnKo (@anko_979) August 28, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Pros &amp;amp; Cons from @OpenAI real-time update from someone building in AI audio:&lt;/p&gt;&lt;p&gt;Pro: Better function calling, more emotion, 20% cheaper, better control, image is cool but won't use&lt;/p&gt;&lt;p&gt;Con: no custom voices (creative experience MUST HAVE), still *expensive* vs TTS-LLM-STT pipelines&lt;/p&gt;— Gavin Purcell (@gavinpurcell) August 28, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;OpenAI reduced prices for gpt-realtime by 20% to $32 per million audio input tokens and $64 for audio output tokens.&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/in-crowded-voice-ai-market-openai-bets-on-instruction-following-and-expressive-speech-to-win-enterprise-adoption/</guid><pubDate>Thu, 28 Aug 2025 23:26:47 +0000</pubDate></item></channel></rss>