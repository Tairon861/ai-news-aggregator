<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 03 Sep 2025 01:27:56 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Just 4 days left to exhibit at TechCrunch Disrupt 2025 — 10 tables remain (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/02/just-4-days-left-to-exhibit-at-techcrunch-disrupt-2025-10-tables-remain/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The clock is ticking, and exhibit space at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is filling up fast. With just &lt;strong&gt;4 days left&lt;/strong&gt; to book your table, now’s the time to stop circling the idea and claim your spot on the show floor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Only 10 tables remain&lt;/strong&gt; — and once they’re gone, they’re gone. No waitlist. No last-minute add-ons.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Disrupt draws over &lt;strong&gt;10,000 startup and VC leaders&lt;/strong&gt;, and the &lt;strong&gt;exhibit floor&lt;/strong&gt; is where some of the biggest conversations begin. If you’ve got a killer product, a bold vision, or early traction, and you’re ready to scale, then this is your platform.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 exhibitor" class="wp-image-3040886" height="469" src="https://techcrunch.com/wp-content/uploads/2025/08/polygraf2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Silkroad &lt;span class="screen-reader-text"&gt;(opens in a new window)&lt;/span&gt;&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-here-s-what-your-table-unlocks"&gt;Here’s what your table unlocks&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Prime placement in front of the biggest names in venture, media, and tech&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;A dedicated listing on the Disrupt website, event app, on-site signage, and a shout-out to TechCrunch readers&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 passes to the full event — perfect for team networking or joining interactive sessions with tech leaders&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;And many more benefits&lt;/strong&gt; that are designed to help you scale your startup and strengthen your brand&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;You’ve got just a few days to decide: Are you showing up, or watching from the sidelines?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;&lt;em&gt;Secure your table&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;&lt;strong&gt; before the September 5 deadline!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The clock is ticking, and exhibit space at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is filling up fast. With just &lt;strong&gt;4 days left&lt;/strong&gt; to book your table, now’s the time to stop circling the idea and claim your spot on the show floor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Only 10 tables remain&lt;/strong&gt; — and once they’re gone, they’re gone. No waitlist. No last-minute add-ons.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Disrupt draws over &lt;strong&gt;10,000 startup and VC leaders&lt;/strong&gt;, and the &lt;strong&gt;exhibit floor&lt;/strong&gt; is where some of the biggest conversations begin. If you’ve got a killer product, a bold vision, or early traction, and you’re ready to scale, then this is your platform.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 exhibitor" class="wp-image-3040886" height="469" src="https://techcrunch.com/wp-content/uploads/2025/08/polygraf2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Silkroad &lt;span class="screen-reader-text"&gt;(opens in a new window)&lt;/span&gt;&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-here-s-what-your-table-unlocks"&gt;Here’s what your table unlocks&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Prime placement in front of the biggest names in venture, media, and tech&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;A dedicated listing on the Disrupt website, event app, on-site signage, and a shout-out to TechCrunch readers&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;10 passes to the full event — perfect for team networking or joining interactive sessions with tech leaders&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;And many more benefits&lt;/strong&gt; that are designed to help you scale your startup and strengthen your brand&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;You’ve got just a few days to decide: Are you showing up, or watching from the sidelines?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;&lt;em&gt;Secure your table&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;&lt;strong&gt; before the September 5 deadline!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/02/just-4-days-left-to-exhibit-at-techcrunch-disrupt-2025-10-tables-remain/</guid><pubDate>Tue, 02 Sep 2025 14:00:00 +0000</pubDate></item><item><title>Microsoft gives free Copilot AI services to US government workers (AI News)</title><link>https://www.artificialintelligence-news.com/news/microsoft-gives-free-copilot-ai-services-to-us-government-workers/</link><description>&lt;p&gt;Millions of US federal government workers are about to get a new AI assistant on their devices for free in the form of Microsoft Copilot. The move is part of a deal between Microsoft and the US General Services Administration (GSA) that’s also expected to save taxpayers $3.1 billion in its first year.&lt;/p&gt;&lt;p&gt;The centrepiece of this huge new agreement is a full year of Microsoft 365 Copilot at no extra cost for government workers using the high-security G5 licence. This is a push to get the latest AI tools into the hands of public servants quickly and safely, aiming to improve how the government operates.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-microsoft-pushes-the-us-government-into-the-ai-era"&gt;Microsoft pushes the US government into the AI era&lt;/h3&gt;&lt;p&gt;This deal aims to place the US government at the forefront of AI adoption. It’s a direct response to the administration’s AI Action Plan, designed to bring the power of modern artificial intelligence to everything from managing citizen enquiries to analysing complex data.&lt;/p&gt;&lt;p&gt;“OneGov represents a paradigm shift in federal procurement that is leading to immense cost savings, achieved by leveraging the purchasing power of the entire federal government,” explained FAS Commissioner Josh Gruenbaum.&lt;/p&gt;&lt;p&gt;The free Copilot offer is specifically for users on the Microsoft 365 G5 plan, the premium tier for departments that handle sensitive information and require the tightest security protocols. But the benefits extend further, with the deal helping agencies to use AI for automating routine tasks, freeing up people to focus on the work that matters most.&lt;/p&gt;&lt;p&gt;The agreement also makes it cheaper and easier for different departments to modernise their technology. By offering big discounts on Azure cloud services and getting rid of data transfer fees, it tackles a major headache that has often slowed down collaboration between agencies.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-security-is-not-an-afterthought"&gt;Security is not an afterthought&lt;/h3&gt;&lt;p&gt;Of course, giving AI access to government systems raises immediate security questions. The deal addresses this head-on, with Microsoft emphasising that its core cloud and AI services have already passed FedRAMP High security authorisation, a critical standard for handling sensitive government data.&lt;/p&gt;&lt;p&gt;While the full FedRAMP High certification for Copilot itself is expected soon, it has already been given a provisional green light by the Department of Defense. The package also includes advanced security tools like Microsoft Sentinel and Entra ID to support the government’s “zero trust” security goal.&lt;/p&gt;&lt;p&gt;GSA Deputy Administrator Stephen Ehikian strongly encouraged government agencies to take advantage of the new tools.&lt;/p&gt;&lt;p&gt;“GSA is proud to partner with technology companies, like Microsoft, to advance AI adoption across the federal government, a key priority of the Trump Administration,” said Ehikian. “We urge our federal partners to leverage these agreements, providing government workers with transformative AI tools that streamline operations, cut costs, and enhance results.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-helping-government-agencies-to-use-ai-effectively"&gt;Helping government agencies to use AI effectively&lt;/h3&gt;&lt;p&gt;Microsoft is also putting money into making sure the technology is actually used effectively. The company has committed an extra $20 million for support and training, including workshops to help agencies get the most out of the new tools and find other areas to reduce waste.&lt;/p&gt;&lt;p&gt;All told, the package is estimated to deliver more than $6 billion in value over the next three years.&lt;/p&gt;&lt;p&gt;“With this new agreement with the US General Services Administration, including a no-cost Microsoft 365 Copilot offer, we will help federal agencies use AI and digital technologies to improve citizen services, strengthen security, and save taxpayers more than $3 billion in the first year alone,” commented Satya Nadella, Chairman and CEO of Microsoft.&lt;/p&gt;&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;For more than four decades, Microsoft has&amp;nbsp;partnered&amp;nbsp;with&amp;nbsp;the U.S.&amp;nbsp;Government to serve the American people. From modernizing IT infrastructure to advancing cybersecurity, our partnership has always been rooted in trust, innovation, and shared purpose.&lt;/p&gt;&lt;p&gt;Today, we are building on…&lt;/p&gt;— Satya Nadella (@satyanadella) September 2, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;For the millions of people working within the US government, this agreement with Microsoft means that an AI-powered assistant is set to change their daily work.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Marketing AI boom faces crisis of consumer trust&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." class="wp-image-109137" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Millions of US federal government workers are about to get a new AI assistant on their devices for free in the form of Microsoft Copilot. The move is part of a deal between Microsoft and the US General Services Administration (GSA) that’s also expected to save taxpayers $3.1 billion in its first year.&lt;/p&gt;&lt;p&gt;The centrepiece of this huge new agreement is a full year of Microsoft 365 Copilot at no extra cost for government workers using the high-security G5 licence. This is a push to get the latest AI tools into the hands of public servants quickly and safely, aiming to improve how the government operates.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-microsoft-pushes-the-us-government-into-the-ai-era"&gt;Microsoft pushes the US government into the AI era&lt;/h3&gt;&lt;p&gt;This deal aims to place the US government at the forefront of AI adoption. It’s a direct response to the administration’s AI Action Plan, designed to bring the power of modern artificial intelligence to everything from managing citizen enquiries to analysing complex data.&lt;/p&gt;&lt;p&gt;“OneGov represents a paradigm shift in federal procurement that is leading to immense cost savings, achieved by leveraging the purchasing power of the entire federal government,” explained FAS Commissioner Josh Gruenbaum.&lt;/p&gt;&lt;p&gt;The free Copilot offer is specifically for users on the Microsoft 365 G5 plan, the premium tier for departments that handle sensitive information and require the tightest security protocols. But the benefits extend further, with the deal helping agencies to use AI for automating routine tasks, freeing up people to focus on the work that matters most.&lt;/p&gt;&lt;p&gt;The agreement also makes it cheaper and easier for different departments to modernise their technology. By offering big discounts on Azure cloud services and getting rid of data transfer fees, it tackles a major headache that has often slowed down collaboration between agencies.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-security-is-not-an-afterthought"&gt;Security is not an afterthought&lt;/h3&gt;&lt;p&gt;Of course, giving AI access to government systems raises immediate security questions. The deal addresses this head-on, with Microsoft emphasising that its core cloud and AI services have already passed FedRAMP High security authorisation, a critical standard for handling sensitive government data.&lt;/p&gt;&lt;p&gt;While the full FedRAMP High certification for Copilot itself is expected soon, it has already been given a provisional green light by the Department of Defense. The package also includes advanced security tools like Microsoft Sentinel and Entra ID to support the government’s “zero trust” security goal.&lt;/p&gt;&lt;p&gt;GSA Deputy Administrator Stephen Ehikian strongly encouraged government agencies to take advantage of the new tools.&lt;/p&gt;&lt;p&gt;“GSA is proud to partner with technology companies, like Microsoft, to advance AI adoption across the federal government, a key priority of the Trump Administration,” said Ehikian. “We urge our federal partners to leverage these agreements, providing government workers with transformative AI tools that streamline operations, cut costs, and enhance results.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-helping-government-agencies-to-use-ai-effectively"&gt;Helping government agencies to use AI effectively&lt;/h3&gt;&lt;p&gt;Microsoft is also putting money into making sure the technology is actually used effectively. The company has committed an extra $20 million for support and training, including workshops to help agencies get the most out of the new tools and find other areas to reduce waste.&lt;/p&gt;&lt;p&gt;All told, the package is estimated to deliver more than $6 billion in value over the next three years.&lt;/p&gt;&lt;p&gt;“With this new agreement with the US General Services Administration, including a no-cost Microsoft 365 Copilot offer, we will help federal agencies use AI and digital technologies to improve citizen services, strengthen security, and save taxpayers more than $3 billion in the first year alone,” commented Satya Nadella, Chairman and CEO of Microsoft.&lt;/p&gt;&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;For more than four decades, Microsoft has&amp;nbsp;partnered&amp;nbsp;with&amp;nbsp;the U.S.&amp;nbsp;Government to serve the American people. From modernizing IT infrastructure to advancing cybersecurity, our partnership has always been rooted in trust, innovation, and shared purpose.&lt;/p&gt;&lt;p&gt;Today, we are building on…&lt;/p&gt;— Satya Nadella (@satyanadella) September 2, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;p&gt;For the millions of people working within the US government, this agreement with Microsoft means that an AI-powered assistant is set to change their daily work.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Marketing AI boom faces crisis of consumer trust&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for the AI &amp;amp; Big Data Expo event series." class="wp-image-109137" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/microsoft-gives-free-copilot-ai-services-to-us-government-workers/</guid><pubDate>Tue, 02 Sep 2025 14:22:42 +0000</pubDate></item><item><title>OpenAI announces parental controls for ChatGPT after teen suicide lawsuit (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/09/openai-announces-parental-controls-for-chatgpt-after-teen-suicide-lawsuit/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Promised protections follow reports of vulnerable users misled in extended chats.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, OpenAI announced plans to roll out parental controls for ChatGPT and route sensitive mental health conversations to its simulated reasoning models, following what the company has called "heartbreaking cases" of users experiencing crises while using the AI assistant. The moves come after multiple reported incidents where ChatGPT allegedly failed to intervene appropriately when users expressed suicidal thoughts or experienced mental health episodes.&lt;/p&gt;
&lt;p&gt;"This work has already been underway, but we want to proactively preview our plans for the next 120 days, so you won’t need to wait for launches to see where we’re headed," OpenAI wrote in a blog post published Tuesday. "The work will continue well beyond this period of time, but we’re making a focused effort to launch as many of these improvements as possible this year."&lt;/p&gt;
&lt;p&gt;The planned parental controls represent OpenAI's most concrete response to concerns about teen safety on the platform so far. Within the next month, OpenAI says, parents will be able to link their accounts with their teens' ChatGPT accounts (minimum age 13) through email invitations, control how the AI model responds with age-appropriate behavior rules that are on by default, manage which features to disable (including memory and chat history), and receive notifications when the system detects their teen experiencing acute distress.&lt;/p&gt;
&lt;p&gt;The parental controls build on existing features like in-app reminders during long sessions that encourage users to take breaks, which OpenAI rolled out for all users in August.&lt;/p&gt;
&lt;h2&gt;High-profile cases prompt safety changes&lt;/h2&gt;
&lt;p&gt;OpenAI's new safety initiative arrives after several high-profile cases drew scrutiny to ChatGPT's handling of vulnerable users. In August, Matt and Maria Raine filed suit against OpenAI after their 16-year-old son Adam died by suicide following extensive ChatGPT interactions that included 377 messages flagged for self-harm content. According to court documents, ChatGPT mentioned suicide 1,275 times in conversations with Adam—six times more often than the teen himself. Last week, The Wall Street Journal reported that a 56-year-old man killed his mother and himself after ChatGPT reinforced his paranoid delusions rather than challenging them.&lt;/p&gt;
&lt;p&gt;To guide these safety improvements, OpenAI is working with what it calls an Expert Council on Well-Being and AI to "shape a clear, evidence-based vision for how AI can support people's well-being," according to the company's blog post. The council will help define and measure well-being, set priorities, and design future safeguards including the parental controls.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;A separate "Global Physician Network" of more than 250 physicians who have practiced in 60 countries provides medical expertise, with 90 physicians across 30 countries specifically contributing research on how ChatGPT should behave in mental health contexts. These physicians advise on handling specific issues like eating disorders, substance use, and adolescent mental health, though OpenAI notes it "remains accountable for the choices we make" despite the expert input.&lt;/p&gt;
&lt;h2&gt;Degrading safeguards in extended conversations&lt;/h2&gt;
&lt;p&gt;OpenAI recently acknowledged that ChatGPT's safety measures can break down during lengthy conversations—precisely when vulnerable users might need them most. "As the back-and-forth grows, parts of the model's safety training may degrade," the company wrote in a blog post last week. The AI assistant might correctly point users to suicide hotlines initially, but "after many messages over a long period of time, it might eventually offer an answer that goes against our safeguards."&lt;/p&gt;
&lt;p&gt;This degradation reflects fundamental limitations in the Transformer AI architecture that underlies ChatGPT. OpenAI's models use a mechanism that compares every new text fragment to the entire conversation history, with computational costs growing quadratically as conversation length increases. Also, as conversations lengthen beyond the model's context window, the system drops earlier messages and potentially loses important context from the beginning of the conversation.&lt;/p&gt;
&lt;p&gt;The timing of these safety measures follows OpenAI's February decision to ease content safeguards after user complaints about overly restrictive moderation and issues related to a rise in sycophancy, where the GPT-4o AI model told users what they wanted to hear. Combined with a very persuasive simulation of humanlike personality,&amp;nbsp;these tendencies created particularly hazardous conditions for vulnerable users who believed they were interacting with an authoritative and accurate source of information rather than a pattern-matching system generating statistically likely responses.&lt;/p&gt;
&lt;p&gt;Research from July led by Oxford psychiatrists identified what they call "bidirectional belief amplification"—a feedback loop where chatbot sycophancy reinforces user beliefs, which then conditions the chatbot to generate increasingly extreme validations. The researchers warn that this creates conditions for "a technological folie à deux," where two individuals mutually reinforce the same delusion.&lt;/p&gt;
&lt;p&gt;Unlike pharmaceuticals or human therapists, AI chatbots face few safety regulations in the United States, though Illinois recently banned chatbots as therapists, with fines of up to $10,000 per violation. The Oxford researchers conclude that "current AI safety measures are inadequate to address these interaction-based risks" and call for treating chatbots that function as companions or therapists with the same regulatory oversight as mental health interventions.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Promised protections follow reports of vulnerable users misled in extended chats.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, OpenAI announced plans to roll out parental controls for ChatGPT and route sensitive mental health conversations to its simulated reasoning models, following what the company has called "heartbreaking cases" of users experiencing crises while using the AI assistant. The moves come after multiple reported incidents where ChatGPT allegedly failed to intervene appropriately when users expressed suicidal thoughts or experienced mental health episodes.&lt;/p&gt;
&lt;p&gt;"This work has already been underway, but we want to proactively preview our plans for the next 120 days, so you won’t need to wait for launches to see where we’re headed," OpenAI wrote in a blog post published Tuesday. "The work will continue well beyond this period of time, but we’re making a focused effort to launch as many of these improvements as possible this year."&lt;/p&gt;
&lt;p&gt;The planned parental controls represent OpenAI's most concrete response to concerns about teen safety on the platform so far. Within the next month, OpenAI says, parents will be able to link their accounts with their teens' ChatGPT accounts (minimum age 13) through email invitations, control how the AI model responds with age-appropriate behavior rules that are on by default, manage which features to disable (including memory and chat history), and receive notifications when the system detects their teen experiencing acute distress.&lt;/p&gt;
&lt;p&gt;The parental controls build on existing features like in-app reminders during long sessions that encourage users to take breaks, which OpenAI rolled out for all users in August.&lt;/p&gt;
&lt;h2&gt;High-profile cases prompt safety changes&lt;/h2&gt;
&lt;p&gt;OpenAI's new safety initiative arrives after several high-profile cases drew scrutiny to ChatGPT's handling of vulnerable users. In August, Matt and Maria Raine filed suit against OpenAI after their 16-year-old son Adam died by suicide following extensive ChatGPT interactions that included 377 messages flagged for self-harm content. According to court documents, ChatGPT mentioned suicide 1,275 times in conversations with Adam—six times more often than the teen himself. Last week, The Wall Street Journal reported that a 56-year-old man killed his mother and himself after ChatGPT reinforced his paranoid delusions rather than challenging them.&lt;/p&gt;
&lt;p&gt;To guide these safety improvements, OpenAI is working with what it calls an Expert Council on Well-Being and AI to "shape a clear, evidence-based vision for how AI can support people's well-being," according to the company's blog post. The council will help define and measure well-being, set priorities, and design future safeguards including the parental controls.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;A separate "Global Physician Network" of more than 250 physicians who have practiced in 60 countries provides medical expertise, with 90 physicians across 30 countries specifically contributing research on how ChatGPT should behave in mental health contexts. These physicians advise on handling specific issues like eating disorders, substance use, and adolescent mental health, though OpenAI notes it "remains accountable for the choices we make" despite the expert input.&lt;/p&gt;
&lt;h2&gt;Degrading safeguards in extended conversations&lt;/h2&gt;
&lt;p&gt;OpenAI recently acknowledged that ChatGPT's safety measures can break down during lengthy conversations—precisely when vulnerable users might need them most. "As the back-and-forth grows, parts of the model's safety training may degrade," the company wrote in a blog post last week. The AI assistant might correctly point users to suicide hotlines initially, but "after many messages over a long period of time, it might eventually offer an answer that goes against our safeguards."&lt;/p&gt;
&lt;p&gt;This degradation reflects fundamental limitations in the Transformer AI architecture that underlies ChatGPT. OpenAI's models use a mechanism that compares every new text fragment to the entire conversation history, with computational costs growing quadratically as conversation length increases. Also, as conversations lengthen beyond the model's context window, the system drops earlier messages and potentially loses important context from the beginning of the conversation.&lt;/p&gt;
&lt;p&gt;The timing of these safety measures follows OpenAI's February decision to ease content safeguards after user complaints about overly restrictive moderation and issues related to a rise in sycophancy, where the GPT-4o AI model told users what they wanted to hear. Combined with a very persuasive simulation of humanlike personality,&amp;nbsp;these tendencies created particularly hazardous conditions for vulnerable users who believed they were interacting with an authoritative and accurate source of information rather than a pattern-matching system generating statistically likely responses.&lt;/p&gt;
&lt;p&gt;Research from July led by Oxford psychiatrists identified what they call "bidirectional belief amplification"—a feedback loop where chatbot sycophancy reinforces user beliefs, which then conditions the chatbot to generate increasingly extreme validations. The researchers warn that this creates conditions for "a technological folie à deux," where two individuals mutually reinforce the same delusion.&lt;/p&gt;
&lt;p&gt;Unlike pharmaceuticals or human therapists, AI chatbots face few safety regulations in the United States, though Illinois recently banned chatbots as therapists, with fines of up to $10,000 per violation. The Oxford researchers conclude that "current AI safety measures are inadequate to address these interaction-based risks" and call for treating chatbots that function as companions or therapists with the same regulatory oversight as mental health interventions.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/09/openai-announces-parental-controls-for-chatgpt-after-teen-suicide-lawsuit/</guid><pubDate>Tue, 02 Sep 2025 15:10:26 +0000</pubDate></item><item><title>Tesla Dojo: The rise and fall of Elon Musk’s AI supercomputer (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/02/tesla-dojo-the-rise-and-fall-of-elon-musks-ai-supercomputer/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For years, Elon Musk has spoken of the promise of Dojo, the AI supercomputer that was supposed to be the cornerstone of Tesla’s AI ambitions. It was important enough to Musk that in July 2024, he said the company’s AI team would “double down” on Dojo in the lead-up to Tesla’s robotaxi reveal, which happened in October.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After six years of hype, Tesla decided last month to shut down Dojo and disband the team behind the supercomputer in August 2025. Within weeks of projecting that Dojo 2, Tesla’s second supercluster that was meant to be built on the company’s in-house D2 chips, would reach scale by 2026, Musk reversed course, declaring it “an evolutionary dead end.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This article originally set out to explain what Dojo was and how it could help Tesla achieve full-self driving, autonomous humanoid robots, semiconductor autonomy, and more. Now, you can think of it more as an obituary of a project that convinced so many analysts and investors that Tesla wasn’t just an automaker, it was an AI company.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dojo was Tesla’s custom-built supercomputer that was designed to train its “Full Self-Driving” neural networks. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beefing up Dojo went hand-in-hand with Tesla’s goal to reach full self-driving and bring a robotaxi to market. FSD (Supervised) is Tesla’s advanced driver assistance system that’s on hundreds of thousands of Tesla vehicles today and can perform some automated driving tasks, but it still requires a human to be attentive behind the wheel.&amp;nbsp;It’s also the basis of similar technology powering Tesla’s limited robotaxi service that the company launched in Austin this June using Model Y SUVs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even as Dojo’s &lt;em&gt;raison d’être&lt;/em&gt; started to come to life, Tesla failed to attribute its self-driving successes — controversial as they were — to the supercomputer. In fact, Musk and Tesla had barely mentioned Dojo at all over the past year. In August 2024, Tesla began promoting Cortex, the company’s “giant new AI training supercluster being built at Tesla HQ in Austin to solve real-world AI,” which Musk has said would have “massive storage for video training of FSD and Optimus.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Tesla’s Q4 2024 shareholder deck, the company shared updates on Cortex, but nothing on Dojo. It’s not clear whether Tesla’s Dojo shutdown affects Cortex.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The response to Dojo’s disbanding has been mixed. Some see it as another example of Musk making promises he can’t deliver on that comes at a time of falling EV sales and a lackluster robotaxi rollout. Others say the shutdown wasn’t a failure, but a strategic pivot from a high-risk, self-reliant hardware to a streamlined path that relies on partners for chip development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dojo’s story reveals what was on the line, where the project fell short, and what its shutdown signals for Tesla’s future.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-recap-of-dojo-s-shutdown"&gt;A recap of Dojo’s shutdown&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla disbanded its Dojo team and shut down the project in mid-August 2025. Dojo’s lead, Peter Bannon, left the company as well, following the departure of around 20 workers who left to start their own AI chip and infrastructure company called DensityAI. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Analysts have pointed out that losing key talent can quickly derail a project, especially a highly specialized, internal tech project.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The shutdown came a couple of weeks after Tesla signed a $16.5 billion deal to get its next-generation AI6 chips from Samsung. The AI6 chip is Tesla’s bet on a chip design that can scale from powering FSD and Tesla’s Optimus humanoid robots to high-performance AI training in data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once it became clear that all paths converged to AI6, I had to shut down Dojo and make some tough personnel choices, as Dojo 2 was now an evolutionary dead end,” Musk posted on X, the social media platform he owns. “Dojo 3 arguably lives on in the form of a large number of AI6 [systems-on-a-chip] on a single board.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-tesla-s-dojo-backstory"&gt;Tesla’s Dojo backstory&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2819229" height="453" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-1239825394.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Suzanne Cordeiro / AFP / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Musk has insisted that Tesla isn’t just an automaker, or even a purveyor of solar panels and energy storage systems. Instead, he has pitched Tesla as an AI company, one that has cracked the code to self-driving cars by mimicking human perception.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most other companies building autonomous vehicle technology rely on a combination of sensors to perceive the world — like lidar, radar and cameras — as well as high-definition maps to localize the vehicle. Tesla believes it can achieve fully autonomous driving by relying on cameras alone to capture visual data and then use advanced neural networks to process that data and make quick decisions about how the car should behave.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The pitch has been that Dojo-trained AI software will eventually be pushed out to Tesla customers via over-the-air updates. The scale of FSD also means Tesla has been able to rake in millions of miles worth of video footage that it uses to train FSD. The idea there is that the more data Tesla can collect, the closer the automaker can get to actually achieving full self-driving.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, some industry experts say there might be a limit to the brute force approach of throwing more data at a model and expecting it to get smarter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“First of all, there’s an economic constraint, and soon it will just get too expensive to do that,” Anand Raghunathan, Purdue University’s Silicon Valley professor of electrical and computer engineering, told TechCrunch. Further, he said, “Some people claim that we might actually run out of meaningful data to train the models on. More data doesn’t necessarily mean more information, so it depends on whether that data has information that is useful to create a better model, and if the training process is able to actually distill that information into a better model.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Raghunathan said despite these doubts, the trend of more data appears to be here for the short-term at least. And more data means more compute power needed to store and process it all to train Tesla’s AI models. That was where Dojo, the supercomputer, came in.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-a-supercomputer"&gt;What is a supercomputer?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Dojo was Tesla’s supercomputer system that was designed to function as a training ground for AI, specifically FSD. The name is a nod to the space where martial arts are practiced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A supercomputer is made up of thousands of smaller computers called nodes. Each of those nodes has its own CPU (central processing unit) and GPU (graphics processing unit). The former handles overall management of the node, and the latter does the complex stuff, like splitting tasks into multiple parts and working on them simultaneously. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPUs are essential for machine learning operations like those that power FSD training in simulation. They also power large language models, which is why the rise of generative AI has made Nvidia the most valuable company on the planet.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even Tesla buys Nvidia GPUs to train its AI (more on that later).&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-did-tesla-need-a-supercomputer"&gt;Why did Tesla need a supercomputer?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla’s vision-only approach was the main reason Tesla needed a supercomputer. The neural networks behind FSD are trained on vast amounts of driving data to recognize and classify objects around the vehicle and then make driving decisions. That means that when FSD is engaged, the neural nets have to collect and process visual data continuously at speeds that match the depth and velocity recognition capabilities of a human.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In other words, Tesla means to create a digital duplicate of the human visual cortex and brain function.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To get there, Tesla needs to store and process all the video data collected from its cars around the world and run millions of simulations to train its model on the data.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla relied mainly on Nvidia to power its current Dojo training computer, but it didn’t want to have all its eggs in one basket — not least because Nvidia chips are expensive. Tesla had hoped to make something better that increased bandwidth and decreased latencies. That’s why the automaker’s AI division decided to come up with its own custom hardware program that aimed to train AI models more efficiently than traditional systems.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At that program’s core was Tesla’s proprietary D1 chips, which the company said were optimized for AI workloads.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-tell-me-more-about-these-chips"&gt;Tell me more about these chips&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Ganesh Venkataramanan, former senior director of Autopilot hardware, presenting the D1 training tile at Tesla’s 2021 AI Day." class="wp-image-2819208" height="336" src="https://techcrunch.com/wp-content/uploads/2024/08/Tesla-AI-Day-Dojo-Tile.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Ganesh Venkataramanan, former senior director of Autopilot hardware, presenting the D1 training tile at Tesla’s 2021 AI Day.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tesla / screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla, like Apple, thinks hardware and software should be designed to work together. That’s why Tesla was working to move away from the standard GPU hardware and design its own chips to power Dojo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla unveiled its D1 chip, a silicon square the size of a palm, on AI Day in 2021. The D1 chip entered into production around July 2023.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Taiwan Semiconductor Manufacturing Company (TSMC) manufactured the chips using 7 nanometer semiconductor nodes. The D1 has 50 billion transistors and a large die size of 645 millimeters squared, according to Tesla. This is all to say that the D1 promises to be extremely powerful and efficient and to handle complex tasks quickly.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The D1 wasn’t as powerful as Nvidia’s A100 chip, though.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla had been working on a next-gen D2 chip that aimed to solve information flow bottlenecks. Instead of connecting the individual chips, the D2 would have put the entire Dojo tile onto a single wafer of silicon.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla never confirmed how many D1 chips it ordered or received. The company also never&amp;nbsp;provided a timeline for how long it would have taken to get Dojo supercomputers running on D1 chips.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-did-dojo-mean-for-tesla"&gt;What did Dojo mean for Tesla?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Visitors are viewing Tesla's humanoid robot Optimus Prime II at WAIC in Shanghai, China, on July 7, 2024." class="wp-image-2819231" height="444" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-2162480419.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Visitors are viewing Tesla’s humanoid robot Optimus Prime II at WAIC in Shanghai, China, on July 7, 2024. &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Costfoto / NurPhoto / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla’s hope was that by taking control of its own chip production, it might one day be able to quickly add large amounts of compute power to AI training programs at a low cost.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It also meant not having to rely on Nvidia’s chips in the future, which are increasingly expensive and hard to secure. Now, Tesla is going all-in on partnerships — with Nvidia, AMD, and Samsung, which will build its next-gen AI6 chip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During Tesla’s second-quarter 2024 earnings call, Musk said demand for Nvidia hardware was “so high that it’s often difficult to get the GPUs.” He said he was “quite concerned about actually being able to get steady GPUs when we want them, and I think this therefore requires that we put a lot more effort on Dojo in order to ensure that we’ve got the training capability that we need.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dojo was a risky bet, one that Musk hedged several times by saying that Tesla might not succeed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the long run, Tesla toyed with the idea of creating a new business model based on its AI division, with Musk even saying during a Q2 2024 earnings call that he saw “a path to being competitive with Nvidia with Dojo.” While D1 was more tailored for Tesla computer vision labeling and training — useful for FSD and Optimus training — it wouldn’t have been useful for much else. Future versions would have to be more tailored to general-purpose AI training, Musk said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem that Tesla might have come up against is that almost all AI software out there has been written to work with GPUs. Using Dojo chips to train general-purpose AI models would have required rewriting the software.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That is, unless Tesla rented out its compute, similar to how AWS and Azure rent out cloud computing capabilities — an idea that excited analysts. A September 2023 report from Morgan Stanley predicted that Dojo could add $500 billion to Tesla’s market value by unlocking new revenue streams in the form of robotaxis and software services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, Dojo chips were an insurance policy for the automaker, but one that might have paid dividends.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-far-did-tesla-dojo-get"&gt;How far did Tesla Dojo get?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2819240" height="453" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-524212924.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Nvidia CEO Jensen Huang and Tesla CEO Elon Musk&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kim Kulish / Corbis / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Musk often provided progress reports, but many of his goals for Dojo were never reached.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, Musk suggested in June 2023 that Dojo had been online and running useful tasks for a few months.” Around the same time, Tesla said it expected Dojo to be one of the top five most powerful supercomputers by February 2024 and had planned for total compute to reach 100 exaflops in October 2024, which would have required roughly 276,000 D1s, or around 320,500 Nvidia A100 GPUs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla never provided an update or any information that would suggest it ever reached these goals.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla and Musk made numerous other pledges for Dojo, including financial ones.&amp;nbsp;For instance, Tesla committed in&amp;nbsp;January 2024 to spend $500 million to build a Dojo supercomputer at its gigafactory in Buffalo, New York, and has already spent $314 million of that, per a 2024 report.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Just after Tesla’s second-quarter 2024 earnings call, Musk posted photos of Dojo 1 on X, saying that it would have “roughly 8k H100-equivalent of training online by end of year. Not massive, but not trivial either.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite all of this activity — particularly by Musk on X and in earnings calls — mention of Dojo abruptly ended August 2024.&amp;nbsp;And talk switched to Cortex.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the company’s fourth-quarter 2024 earnings call, Tesla said it completed the deployment of Cortex, “a ~50k H100 training cluster at Gigafactory Texas” and that Cortex helped enable V13 of supervised FSD.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Q2 2025, Tesla noted it “expanded AI training compute with an additional 16k H200 GPUs at Gigafactory Texas, bringing Cortex to a total of 67k H100 equivalents.” During that same earnings call, Musk said he expected to have a second Dojo cluster operating “at scale” in 2026. He also hinted at potential redundancies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Thinking about Dojo 3 and the AI6 inference chip, it seems like intuitively, we want to try to find convergence there, where it’s basically the same chip,” Musk said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A few weeks later, he reversed course and disbanded the Dojo team.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch confirmed in late August 2025 that Tesla still plans to commit $500 million to a supercomputer in Buffalo — it just won’t be Dojo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story originally published August 3, 2024. The article was updated for a final time September 2, 2025, with new information about Tesla’s decision to shut down Dojo.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For years, Elon Musk has spoken of the promise of Dojo, the AI supercomputer that was supposed to be the cornerstone of Tesla’s AI ambitions. It was important enough to Musk that in July 2024, he said the company’s AI team would “double down” on Dojo in the lead-up to Tesla’s robotaxi reveal, which happened in October.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After six years of hype, Tesla decided last month to shut down Dojo and disband the team behind the supercomputer in August 2025. Within weeks of projecting that Dojo 2, Tesla’s second supercluster that was meant to be built on the company’s in-house D2 chips, would reach scale by 2026, Musk reversed course, declaring it “an evolutionary dead end.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This article originally set out to explain what Dojo was and how it could help Tesla achieve full-self driving, autonomous humanoid robots, semiconductor autonomy, and more. Now, you can think of it more as an obituary of a project that convinced so many analysts and investors that Tesla wasn’t just an automaker, it was an AI company.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dojo was Tesla’s custom-built supercomputer that was designed to train its “Full Self-Driving” neural networks. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beefing up Dojo went hand-in-hand with Tesla’s goal to reach full self-driving and bring a robotaxi to market. FSD (Supervised) is Tesla’s advanced driver assistance system that’s on hundreds of thousands of Tesla vehicles today and can perform some automated driving tasks, but it still requires a human to be attentive behind the wheel.&amp;nbsp;It’s also the basis of similar technology powering Tesla’s limited robotaxi service that the company launched in Austin this June using Model Y SUVs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even as Dojo’s &lt;em&gt;raison d’être&lt;/em&gt; started to come to life, Tesla failed to attribute its self-driving successes — controversial as they were — to the supercomputer. In fact, Musk and Tesla had barely mentioned Dojo at all over the past year. In August 2024, Tesla began promoting Cortex, the company’s “giant new AI training supercluster being built at Tesla HQ in Austin to solve real-world AI,” which Musk has said would have “massive storage for video training of FSD and Optimus.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Tesla’s Q4 2024 shareholder deck, the company shared updates on Cortex, but nothing on Dojo. It’s not clear whether Tesla’s Dojo shutdown affects Cortex.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The response to Dojo’s disbanding has been mixed. Some see it as another example of Musk making promises he can’t deliver on that comes at a time of falling EV sales and a lackluster robotaxi rollout. Others say the shutdown wasn’t a failure, but a strategic pivot from a high-risk, self-reliant hardware to a streamlined path that relies on partners for chip development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dojo’s story reveals what was on the line, where the project fell short, and what its shutdown signals for Tesla’s future.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-recap-of-dojo-s-shutdown"&gt;A recap of Dojo’s shutdown&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla disbanded its Dojo team and shut down the project in mid-August 2025. Dojo’s lead, Peter Bannon, left the company as well, following the departure of around 20 workers who left to start their own AI chip and infrastructure company called DensityAI. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Analysts have pointed out that losing key talent can quickly derail a project, especially a highly specialized, internal tech project.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The shutdown came a couple of weeks after Tesla signed a $16.5 billion deal to get its next-generation AI6 chips from Samsung. The AI6 chip is Tesla’s bet on a chip design that can scale from powering FSD and Tesla’s Optimus humanoid robots to high-performance AI training in data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once it became clear that all paths converged to AI6, I had to shut down Dojo and make some tough personnel choices, as Dojo 2 was now an evolutionary dead end,” Musk posted on X, the social media platform he owns. “Dojo 3 arguably lives on in the form of a large number of AI6 [systems-on-a-chip] on a single board.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-tesla-s-dojo-backstory"&gt;Tesla’s Dojo backstory&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2819229" height="453" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-1239825394.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Suzanne Cordeiro / AFP / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Musk has insisted that Tesla isn’t just an automaker, or even a purveyor of solar panels and energy storage systems. Instead, he has pitched Tesla as an AI company, one that has cracked the code to self-driving cars by mimicking human perception.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most other companies building autonomous vehicle technology rely on a combination of sensors to perceive the world — like lidar, radar and cameras — as well as high-definition maps to localize the vehicle. Tesla believes it can achieve fully autonomous driving by relying on cameras alone to capture visual data and then use advanced neural networks to process that data and make quick decisions about how the car should behave.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The pitch has been that Dojo-trained AI software will eventually be pushed out to Tesla customers via over-the-air updates. The scale of FSD also means Tesla has been able to rake in millions of miles worth of video footage that it uses to train FSD. The idea there is that the more data Tesla can collect, the closer the automaker can get to actually achieving full self-driving.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, some industry experts say there might be a limit to the brute force approach of throwing more data at a model and expecting it to get smarter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“First of all, there’s an economic constraint, and soon it will just get too expensive to do that,” Anand Raghunathan, Purdue University’s Silicon Valley professor of electrical and computer engineering, told TechCrunch. Further, he said, “Some people claim that we might actually run out of meaningful data to train the models on. More data doesn’t necessarily mean more information, so it depends on whether that data has information that is useful to create a better model, and if the training process is able to actually distill that information into a better model.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Raghunathan said despite these doubts, the trend of more data appears to be here for the short-term at least. And more data means more compute power needed to store and process it all to train Tesla’s AI models. That was where Dojo, the supercomputer, came in.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-a-supercomputer"&gt;What is a supercomputer?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Dojo was Tesla’s supercomputer system that was designed to function as a training ground for AI, specifically FSD. The name is a nod to the space where martial arts are practiced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A supercomputer is made up of thousands of smaller computers called nodes. Each of those nodes has its own CPU (central processing unit) and GPU (graphics processing unit). The former handles overall management of the node, and the latter does the complex stuff, like splitting tasks into multiple parts and working on them simultaneously. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPUs are essential for machine learning operations like those that power FSD training in simulation. They also power large language models, which is why the rise of generative AI has made Nvidia the most valuable company on the planet.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even Tesla buys Nvidia GPUs to train its AI (more on that later).&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-did-tesla-need-a-supercomputer"&gt;Why did Tesla need a supercomputer?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla’s vision-only approach was the main reason Tesla needed a supercomputer. The neural networks behind FSD are trained on vast amounts of driving data to recognize and classify objects around the vehicle and then make driving decisions. That means that when FSD is engaged, the neural nets have to collect and process visual data continuously at speeds that match the depth and velocity recognition capabilities of a human.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In other words, Tesla means to create a digital duplicate of the human visual cortex and brain function.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To get there, Tesla needs to store and process all the video data collected from its cars around the world and run millions of simulations to train its model on the data.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla relied mainly on Nvidia to power its current Dojo training computer, but it didn’t want to have all its eggs in one basket — not least because Nvidia chips are expensive. Tesla had hoped to make something better that increased bandwidth and decreased latencies. That’s why the automaker’s AI division decided to come up with its own custom hardware program that aimed to train AI models more efficiently than traditional systems.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At that program’s core was Tesla’s proprietary D1 chips, which the company said were optimized for AI workloads.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-tell-me-more-about-these-chips"&gt;Tell me more about these chips&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Ganesh Venkataramanan, former senior director of Autopilot hardware, presenting the D1 training tile at Tesla’s 2021 AI Day." class="wp-image-2819208" height="336" src="https://techcrunch.com/wp-content/uploads/2024/08/Tesla-AI-Day-Dojo-Tile.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Ganesh Venkataramanan, former senior director of Autopilot hardware, presenting the D1 training tile at Tesla’s 2021 AI Day.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tesla / screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla, like Apple, thinks hardware and software should be designed to work together. That’s why Tesla was working to move away from the standard GPU hardware and design its own chips to power Dojo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla unveiled its D1 chip, a silicon square the size of a palm, on AI Day in 2021. The D1 chip entered into production around July 2023.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Taiwan Semiconductor Manufacturing Company (TSMC) manufactured the chips using 7 nanometer semiconductor nodes. The D1 has 50 billion transistors and a large die size of 645 millimeters squared, according to Tesla. This is all to say that the D1 promises to be extremely powerful and efficient and to handle complex tasks quickly.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The D1 wasn’t as powerful as Nvidia’s A100 chip, though.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla had been working on a next-gen D2 chip that aimed to solve information flow bottlenecks. Instead of connecting the individual chips, the D2 would have put the entire Dojo tile onto a single wafer of silicon.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla never confirmed how many D1 chips it ordered or received. The company also never&amp;nbsp;provided a timeline for how long it would have taken to get Dojo supercomputers running on D1 chips.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-did-dojo-mean-for-tesla"&gt;What did Dojo mean for Tesla?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Visitors are viewing Tesla's humanoid robot Optimus Prime II at WAIC in Shanghai, China, on July 7, 2024." class="wp-image-2819231" height="444" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-2162480419.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Visitors are viewing Tesla’s humanoid robot Optimus Prime II at WAIC in Shanghai, China, on July 7, 2024. &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Costfoto / NurPhoto / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla’s hope was that by taking control of its own chip production, it might one day be able to quickly add large amounts of compute power to AI training programs at a low cost.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It also meant not having to rely on Nvidia’s chips in the future, which are increasingly expensive and hard to secure. Now, Tesla is going all-in on partnerships — with Nvidia, AMD, and Samsung, which will build its next-gen AI6 chip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During Tesla’s second-quarter 2024 earnings call, Musk said demand for Nvidia hardware was “so high that it’s often difficult to get the GPUs.” He said he was “quite concerned about actually being able to get steady GPUs when we want them, and I think this therefore requires that we put a lot more effort on Dojo in order to ensure that we’ve got the training capability that we need.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dojo was a risky bet, one that Musk hedged several times by saying that Tesla might not succeed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the long run, Tesla toyed with the idea of creating a new business model based on its AI division, with Musk even saying during a Q2 2024 earnings call that he saw “a path to being competitive with Nvidia with Dojo.” While D1 was more tailored for Tesla computer vision labeling and training — useful for FSD and Optimus training — it wouldn’t have been useful for much else. Future versions would have to be more tailored to general-purpose AI training, Musk said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem that Tesla might have come up against is that almost all AI software out there has been written to work with GPUs. Using Dojo chips to train general-purpose AI models would have required rewriting the software.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That is, unless Tesla rented out its compute, similar to how AWS and Azure rent out cloud computing capabilities — an idea that excited analysts. A September 2023 report from Morgan Stanley predicted that Dojo could add $500 billion to Tesla’s market value by unlocking new revenue streams in the form of robotaxis and software services.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, Dojo chips were an insurance policy for the automaker, but one that might have paid dividends.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-far-did-tesla-dojo-get"&gt;How far did Tesla Dojo get?&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2819240" height="453" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-524212924.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Nvidia CEO Jensen Huang and Tesla CEO Elon Musk&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kim Kulish / Corbis / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Musk often provided progress reports, but many of his goals for Dojo were never reached.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, Musk suggested in June 2023 that Dojo had been online and running useful tasks for a few months.” Around the same time, Tesla said it expected Dojo to be one of the top five most powerful supercomputers by February 2024 and had planned for total compute to reach 100 exaflops in October 2024, which would have required roughly 276,000 D1s, or around 320,500 Nvidia A100 GPUs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla never provided an update or any information that would suggest it ever reached these goals.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tesla and Musk made numerous other pledges for Dojo, including financial ones.&amp;nbsp;For instance, Tesla committed in&amp;nbsp;January 2024 to spend $500 million to build a Dojo supercomputer at its gigafactory in Buffalo, New York, and has already spent $314 million of that, per a 2024 report.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Just after Tesla’s second-quarter 2024 earnings call, Musk posted photos of Dojo 1 on X, saying that it would have “roughly 8k H100-equivalent of training online by end of year. Not massive, but not trivial either.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite all of this activity — particularly by Musk on X and in earnings calls — mention of Dojo abruptly ended August 2024.&amp;nbsp;And talk switched to Cortex.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the company’s fourth-quarter 2024 earnings call, Tesla said it completed the deployment of Cortex, “a ~50k H100 training cluster at Gigafactory Texas” and that Cortex helped enable V13 of supervised FSD.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Q2 2025, Tesla noted it “expanded AI training compute with an additional 16k H200 GPUs at Gigafactory Texas, bringing Cortex to a total of 67k H100 equivalents.” During that same earnings call, Musk said he expected to have a second Dojo cluster operating “at scale” in 2026. He also hinted at potential redundancies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Thinking about Dojo 3 and the AI6 inference chip, it seems like intuitively, we want to try to find convergence there, where it’s basically the same chip,” Musk said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A few weeks later, he reversed course and disbanded the Dojo team.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch confirmed in late August 2025 that Tesla still plans to commit $500 million to a supercomputer in Buffalo — it just won’t be Dojo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story originally published August 3, 2024. The article was updated for a final time September 2, 2025, with new information about Tesla’s decision to shut down Dojo.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/02/tesla-dojo-the-rise-and-fall-of-elon-musks-ai-supercomputer/</guid><pubDate>Tue, 02 Sep 2025 16:18:46 +0000</pubDate></item><item><title>WordPress shows off Telex, its experimental AI development tool (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/02/wordpress-shows-off-telex-its-experimental-ai-development-tool/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Web publishing platform WordPress is introducing an early version of an AI development tool, which WordPress co-founder and Automattic CEO Matt Mullenweg described as a “V0 or Lovable, but specifically for WordPress” — V0 and Lovable being references to popular “vibe coding” services for building software using prompt-based, AI interfaces. Mullenweg introduced the new WordPress AI tool, called Telex, at the WordCamp US 2025 conference in Portland last week, alongside other AI experiments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During his keynote address, Mullenweg briefly demonstrated how Telex would allow users to create Gutenberg blocks — or the modular bits of text, images, columns, and more — that make up a WordPress website. He showed off how one developer used the new tool to make a simple marketing animation. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Available on its own domain at telex.automattic.ai, Telex today is labeled as “experimental.” To use the service, you type in a prompt for which sort of content block you want to produce, which is returned as a .zip file you can install as a plug-in to a WordPress site or WordPress Playground. (The latter being the platform that lets you run WordPress in a web browser on any device without a host.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3041473" height="348" src="https://techcrunch.com/wp-content/uploads/2025/09/telex.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Telex screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch follows WordPress’s announcement earlier this year that it was forming an AI team to steward the development of AI products that align with the WordPress project’s long-term goals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early testers found that Telex still has a ways to go, as several test projects failed or needed additional work to run properly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Mullenweg did stress that Telex was still a prototype, he was bullish on the potential for AI to further the WordPress mission over time. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When we think about democratized publishing, like embedded in that, is very core to WordPress’ mission, has been taking things that were difficult to do, that required knowledge of coding or anything else, and … made it accessible to people. Made it accessible in a radically open way, in every language, at low cost, open source — we actually own it and have rights to it,” Mullenweg said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The CEO also admitted there were parts of AI’s progress that could be scary, given the hype and the talk about AI potentially being a bubble — but that didn’t overrule his excitement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“At the core of it, there is a seed of something, which is so enabling,” he said of AI. “It is an incredibly exciting time to be building for WordPress.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3041477" height="325" src="https://techcrunch.com/wp-content/uploads/2025/09/telex-ex.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Telex screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Mullenweg also showed off another simpler AI tool, built with an hour or two of work during Contributor Day, which offered a WordPress help assistant inside the browser. And he spoke of his favorite AI browser — Perplexity’s Comet — which would allow users to interact with WordPress from its interface.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As for the legal drama that’s been surrounding the company over the past year or so, Mullenweg only offered a brief update. The company has been engaged in a dispute with hosting provider WP Engine, which Mullenweg alleges is profiting off the work WordPress does, without contributing enough back. He wants WP Engine to therefore license the WordPress trademark, which he says confuses customers about its association with the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The quick update is, it’s working its way through the legal system. We trust in the fairness of the courts,” Mullenweg said. “If there’s any commentary, I’ll just say that there was a settlement conference, I showed up; the other CEO did not. But it is working its way through that. And that’s my only comment on that whole rigmarole.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Web publishing platform WordPress is introducing an early version of an AI development tool, which WordPress co-founder and Automattic CEO Matt Mullenweg described as a “V0 or Lovable, but specifically for WordPress” — V0 and Lovable being references to popular “vibe coding” services for building software using prompt-based, AI interfaces. Mullenweg introduced the new WordPress AI tool, called Telex, at the WordCamp US 2025 conference in Portland last week, alongside other AI experiments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During his keynote address, Mullenweg briefly demonstrated how Telex would allow users to create Gutenberg blocks — or the modular bits of text, images, columns, and more — that make up a WordPress website. He showed off how one developer used the new tool to make a simple marketing animation. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Available on its own domain at telex.automattic.ai, Telex today is labeled as “experimental.” To use the service, you type in a prompt for which sort of content block you want to produce, which is returned as a .zip file you can install as a plug-in to a WordPress site or WordPress Playground. (The latter being the platform that lets you run WordPress in a web browser on any device without a host.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3041473" height="348" src="https://techcrunch.com/wp-content/uploads/2025/09/telex.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Telex screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch follows WordPress’s announcement earlier this year that it was forming an AI team to steward the development of AI products that align with the WordPress project’s long-term goals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early testers found that Telex still has a ways to go, as several test projects failed or needed additional work to run properly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Mullenweg did stress that Telex was still a prototype, he was bullish on the potential for AI to further the WordPress mission over time. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When we think about democratized publishing, like embedded in that, is very core to WordPress’ mission, has been taking things that were difficult to do, that required knowledge of coding or anything else, and … made it accessible to people. Made it accessible in a radically open way, in every language, at low cost, open source — we actually own it and have rights to it,” Mullenweg said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The CEO also admitted there were parts of AI’s progress that could be scary, given the hype and the talk about AI potentially being a bubble — but that didn’t overrule his excitement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“At the core of it, there is a seed of something, which is so enabling,” he said of AI. “It is an incredibly exciting time to be building for WordPress.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3041477" height="325" src="https://techcrunch.com/wp-content/uploads/2025/09/telex-ex.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Telex screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Mullenweg also showed off another simpler AI tool, built with an hour or two of work during Contributor Day, which offered a WordPress help assistant inside the browser. And he spoke of his favorite AI browser — Perplexity’s Comet — which would allow users to interact with WordPress from its interface.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As for the legal drama that’s been surrounding the company over the past year or so, Mullenweg only offered a brief update. The company has been engaged in a dispute with hosting provider WP Engine, which Mullenweg alleges is profiting off the work WordPress does, without contributing enough back. He wants WP Engine to therefore license the WordPress trademark, which he says confuses customers about its association with the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The quick update is, it’s working its way through the legal system. We trust in the fairness of the courts,” Mullenweg said. “If there’s any commentary, I’ll just say that there was a settlement conference, I showed up; the other CEO did not. But it is working its way through that. And that’s my only comment on that whole rigmarole.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/02/wordpress-shows-off-telex-its-experimental-ai-development-tool/</guid><pubDate>Tue, 02 Sep 2025 16:26:48 +0000</pubDate></item><item><title>Anthropic raises $13B Series F at $183B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/02/anthropic-raises-13b-series-f-at-183b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-02-at-12.22.37PM.png?resize=1200,671" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI firm Anthropic has raised a $13 billion Series F round that brings its post-money valuation up to $183 billion — funds the company says will be used to grow its enterprise adoption, deepen safety research, and support international expansion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Iconiq co-led the round with Fidelity Management &amp;amp; Research Company and Lightspeed Venture Partners, according to the company’s blog post. Other backers include a string of institutional investors, VCs, sovereign wealth funds, private equity, and asset managers, such as Altimeter, Baillie Gifford, BlackRock, Blackstone, Coatue, D1 Capital Partners, Insight Partners, Ontario Teachers’ Pension Plan, Qatar Investment Authority, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We are seeing exponential growth in demand across our entire customer base,” Anthropic CFO Krishna Rao said in the post. “This financing demonstrates investors’ extraordinary confidence in our financial performance and the strength of their collaboration with us to continue fueling our unprecedented growth.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic last raised $3.5 billion at a $61.5 billion post-money valuation in March 2025.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This latest fundraise comes after reports that Anthropic was nearing a deal to raise between $3 billion and $5 billion in funding at a $170 billion valuation. It also follows impressive growth from the AI&amp;nbsp;startup, which reported a jump in annual recurring revenue from $1 billion to $5 billion over the course of 2025 amid accelerated API usage and enterprise adoption.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anthropic now serves over 300,000 business customers, and our number of large accounts—customers that each represent over $100,000 in run-rate revenue—has grown nearly 7x in the past year,” the company said in the company blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Claude Code is also a developer favorite and one of the main impetuses for Anthropic’s growth. The company said its vibe-coding product already generates more than $500 million in run-rate revenue with usage growing more than 10x in the last three months.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But maintaining that growth and competing with rivals like OpenAI, Cursor, and others requires more money, as its CEO Dario Amodei recently confessed in a memo, reported by Wired. He said he wasn’t “thrilled” about taking money from sovereign wealth funds of dictatorial governments, but that it’s difficult to run a business by excluding “bad people” from investing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story is developing. Check back for updates…&lt;/em&gt;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-02-at-12.22.37PM.png?resize=1200,671" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI firm Anthropic has raised a $13 billion Series F round that brings its post-money valuation up to $183 billion — funds the company says will be used to grow its enterprise adoption, deepen safety research, and support international expansion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Iconiq co-led the round with Fidelity Management &amp;amp; Research Company and Lightspeed Venture Partners, according to the company’s blog post. Other backers include a string of institutional investors, VCs, sovereign wealth funds, private equity, and asset managers, such as Altimeter, Baillie Gifford, BlackRock, Blackstone, Coatue, D1 Capital Partners, Insight Partners, Ontario Teachers’ Pension Plan, Qatar Investment Authority, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We are seeing exponential growth in demand across our entire customer base,” Anthropic CFO Krishna Rao said in the post. “This financing demonstrates investors’ extraordinary confidence in our financial performance and the strength of their collaboration with us to continue fueling our unprecedented growth.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic last raised $3.5 billion at a $61.5 billion post-money valuation in March 2025.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This latest fundraise comes after reports that Anthropic was nearing a deal to raise between $3 billion and $5 billion in funding at a $170 billion valuation. It also follows impressive growth from the AI&amp;nbsp;startup, which reported a jump in annual recurring revenue from $1 billion to $5 billion over the course of 2025 amid accelerated API usage and enterprise adoption.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anthropic now serves over 300,000 business customers, and our number of large accounts—customers that each represent over $100,000 in run-rate revenue—has grown nearly 7x in the past year,” the company said in the company blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Claude Code is also a developer favorite and one of the main impetuses for Anthropic’s growth. The company said its vibe-coding product already generates more than $500 million in run-rate revenue with usage growing more than 10x in the last three months.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But maintaining that growth and competing with rivals like OpenAI, Cursor, and others requires more money, as its CEO Dario Amodei recently confessed in a memo, reported by Wired. He said he wasn’t “thrilled” about taking money from sovereign wealth funds of dictatorial governments, but that it’s difficult to run a business by excluding “bad people” from investing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story is developing. Check back for updates…&lt;/em&gt;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/02/anthropic-raises-13b-series-f-at-183b-valuation/</guid><pubDate>Tue, 02 Sep 2025 16:34:16 +0000</pubDate></item><item><title>Tesla’s Dojo, a timeline (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/02/teslas-dojo-a-timeline/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/tesla-dojo-v3.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk doesn’t want Tesla to be just an automaker. He wants Tesla to be an AI company, one that’s figured out how to make cars drive themselves.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Crucial to that mission was Dojo, a custom-built supercomputer designed by Tesla to train its Full Self-Driving (FSD) neural networks. FSD isn’t actually fully self-driving; it can perform some automated driving tasks, but still requires an attentive human behind the wheel. But Tesla thinks with more data, more compute power, and more training, it can cross the threshold from almost self-driving to full self-driving.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And that’s where Dojo was supposed to come in. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk teased Dojo for years and ramped up discussions about the supercomputer throughout 2024. But Dojo is now out, and another supercomputer called Cortex has entered the chat. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Below is a timeline of Dojo mentions and promises.&amp;nbsp;Check out this explainer on Dojo for even more information on what it was, why it mattered, and what comes next.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2019"&gt;&lt;strong&gt;2019&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-first-mentions-of-dojo"&gt;&lt;strong&gt;First mentions of Dojo&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;April 22 – &lt;/strong&gt;At Tesla’s Autonomy Day, the automaker had its AI team onstage to talk about Autopilot and Full Self-Driving, and the AI powering them both. The company shares information about Tesla’s custom-built chips that are designed specifically for neural networks and self-driving cars.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the event, Musk teases Dojo, revealing that it’s a supercomputer for training AI.&amp;nbsp;He also notes that all Tesla cars being produced at the time would have all hardware necessary for full self-driving and only needed a software update.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-2020-nbsp"&gt;&lt;strong&gt;2020&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-musk-begins-the-dojo-roadshow"&gt;&lt;strong&gt;Musk begins the Dojo roadshow&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Feb 2 – &lt;/strong&gt;Musk says Tesla will soon have more than a million connected vehicles worldwide with sensors and compute needed for full self-driving — and touts Dojo’s capabilities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Dojo, our training supercomputer, will be able to process vast amounts of video training data &amp;amp; efficiently run hyperspace arrays with a vast number of parameters, plenty of memory &amp;amp; ultra-high bandwidth between cores. More on this later.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 14 –&lt;/strong&gt; Musk reiterates Tesla’s plan to develop a neural network training computer called Dojo “to process truly vast amounts of video data,” calling it “a beast.” He also says the first version of Dojo is “about a year away,” which would put its launch date somewhere around August 2021.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;December&lt;/strong&gt; &lt;strong&gt;31&lt;/strong&gt; &lt;strong&gt;–&lt;/strong&gt; Elon says Dojo isn’t needed, but it will make self-driving better. “It isn’t enough to be safer than human drivers, Autopilot ultimately needs to be more than 10 times safer than human drivers.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2021"&gt;&lt;strong&gt;2021&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-tesla-makes-dojo-official"&gt;&lt;strong&gt;Tesla makes Dojo official&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 19 – &lt;/strong&gt;The automaker officially announces Dojo at Tesla’s first AI Day, an event meant to attract engineers to Tesla’s AI team. Tesla also introduces its D1 chip, which the automaker says it will use — alongside Nvidia’s GPU — to power the Dojo supercomputer. Tesla notes its AI cluster will house 3,000 D1 chips.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;October 12 – &lt;/strong&gt;Tesla releases&lt;strong&gt; &lt;/strong&gt;a&lt;strong&gt; &lt;/strong&gt;Dojo Technology whitepaper, “a guide to Tesla’s configurable floating point formats &amp;amp; arithmetic.” The whitepaper outlines a technical standard for a new type of binary floating-point arithmetic that’s used in deep learning neural networks and can be implemented “entirely in software, entirely in hardware, or in any combination of software and hardware.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2022"&gt;&lt;strong&gt;2022&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-tesla-reveals-dojo-progress"&gt;&lt;strong&gt;Tesla reveals Dojo progress&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 12 – &lt;/strong&gt;Musk says Tesla will “phase in Dojo. Won’t need to buy as many incremental GPUs next year.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;September 30 – &lt;/strong&gt;At Tesla’s second AI Day, the company reveals that it has installed the first Dojo cabinet, testing 2.2 megawatts of load testing. Tesla says it was building one tile per day (which is made up of 25 D1 chips). Tesla demos Dojo onstage running a Stable Diffusion model to create an AI-generated image of a “Cybertruck on Mars.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Importantly, the company sets a target date of a full Exapod cluster to be completed by Q1 2023, and says it plans to build a total of seven Exapods in Palo Alto.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2023"&gt;&lt;strong&gt;2023&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-a-long-shot-bet"&gt;&lt;strong&gt;A ‘long-shot bet&lt;/strong&gt;‘&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;April 19 – &lt;/strong&gt;Musk tells investors during Tesla’s first-quarter earnings that Dojo “has the potential for an order of magnitude improvement in the cost of training,” and also “has the potential to become a sellable service that we would offer to other companies in the same way that Amazon Web Services offers web services.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk also notes that he’d “look at Dojo as kind of a long-shot bet,” but a “bet worth making.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;June 21 –&lt;/strong&gt; The Tesla AI X account posts that the company’s neural networks are already in customer vehicles. The thread includes a graph with a timeline of Tesla’s current and projected compute power, which places the start of Dojo production at July 2023, although it’s not clear if this refers to the D1 chips or the supercomputer itself. Musk says that same day that Dojo was already online and running tasks at Tesla data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also projects that Tesla’s compute will be the top five in the entire world by around February 2024 (there are no indications this was successful) and that Tesla would reach 100 exaflops by October 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 19 – &lt;/strong&gt;Tesla notes in its second-quarter earnings report it has started production of Dojo. Musk also says Tesla plans to spend more than $1 billion on Dojo through 2024.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;September 6 – &lt;/strong&gt;Musk posts on X that Tesla is limited by AI training compute, but that Nvidia and Dojo will fix that. He says managing the data from the roughly 160 billion frames of video Tesla gets from its cars per day is extremely difficult.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2024"&gt;&lt;strong&gt;2024&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-plans-to-scale"&gt;&lt;strong&gt;Plans to scale&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;January 24 – &lt;/strong&gt;During Tesla’s fourth-quarter and full-year earnings call, Musk acknowledges again that Dojo is a high-risk, high-reward project. He also says that Tesla was pursuing “the dual path of Nvidia and Dojo,” that “Dojo is working” and is “doing training jobs.” He notes Tesla is scaling it up and has “plans for Dojo 1.5, Dojo 2, Dojo 3 and whatnot.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;January 26 – &lt;/strong&gt;Tesla announced plans to spend $500 million to build a Dojo supercomputer in Buffalo. Musk then downplays the investment somewhat, posting on X that while $500 million is a large sum, it’s “only equivalent to a 10k H100 system from Nvidia. Tesla will spend more than that on Nvidia hardware this year. The table stakes for being competitive in AI are at least several billion dollars per year at this point.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;April 30 – &lt;/strong&gt;At TSMC’s North American Technology Symposium, the company says Dojo’s next-generation training tile — the D2, which puts the entire Dojo tile onto a single silicon wafer, rather than connecting 25 chips to make one tile — is already in production, according to IEEE Spectrum.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;May 20 – &lt;/strong&gt;Musk notes that the rear portion of the Giga Texas factory extension will include the construction of “a super dense, water-cooled supercomputer cluster.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;June 4 – &lt;/strong&gt;A CNBC report reveals Musk diverted thousands of Nvidia chips reserved for Tesla to X and xAI. After initially saying the report was false, Musk posts on X that Tesla didn’t have a location to send the Nvidia chips to turn them on, due to the continued construction on the south extension of Giga Texas, “so they would have just sat in a warehouse.” He noted the extension will “house 50k H100s for FSD training.”&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also posts:&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Of the roughly $10B in AI-related expenditures I said Tesla would make this year, about half is internal, primarily the Tesla-designed AI inference computer and sensors present in all of our cars, plus Dojo. For building the AI training superclusters, NVidia hardware is about 2/3 of the cost. My current best guess for Nvidia purchases by Tesla are $3B to $4B this year.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 1 – &lt;/strong&gt;Musk reveals on X that current Tesla vehicles may not have the right hardware for the company’s next-gen AI model. He says that the roughly 5x increase in parameter count with the next-gen AI “is very difficult to achieve without upgrading the vehicle inference computer.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-nvidia-supply-challenges"&gt;Nvidia supply challenges&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 23 – &lt;/strong&gt;During Tesla’s second-quarter earnings call, Musk says demand for Nvidia hardware is “so high that it’s often difficult to get the GPUs.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think this therefore requires that we put a lot more effort on Dojo in order to ensure that we’ve got the training capability that we need,” Musk says. “And we do see a path to being competitive with Nvidia with Dojo.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A graph in Tesla’s investor deck predicts that Tesla AI training capacity will ramp to roughly 90,000 H100 equivalent GPUs by the end of 2024, up from around 40,000 in June. Later that day on X, Musk posts that Dojo 1 will have “roughly 8k H100-equivalent of training online by end of year.” He also posts photos of the supercomputer, which appears to use the same fridge-like stainless steel exterior as Tesla’s Cybertrucks.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-from-dojo-to-cortex"&gt;From Dojo to Cortex&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 30 –&amp;nbsp; &lt;/strong&gt;AI5 is ~18 months away from high-volume production, Musk says in a reply to a post from someone claiming to start a club of “Tesla HW4/AI4 owners angry about getting left behind when AI5 comes out.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 3 – &lt;/strong&gt;Musk posts on X that he did a walkthrough of “the Tesla supercompute cluster at Giga Texas (aka Cortex).” He notes that it would be made roughly of 100,000 H100/H200 Nvidia GPUs with “massive storage for video training of FSD &amp;amp; Optimus.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 26 – &lt;/strong&gt;Musk posts on X a video of Cortex, which he refers to as “the giant new AI training supercluster being built at Tesla HQ in Austin to solve real-world AI.”&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-202-5"&gt;&lt;strong&gt;202&lt;/strong&gt;5&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-dojo-shutdown-its-team-disbanded"&gt;&lt;strong&gt;Dojo shutdown, its team disbanded&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;January 29 –&lt;/strong&gt; Tesla’s Q4 and full-year 2024 earnings call included no mention of Dojo. Cortex, Tesla’s new AI training supercluster at the Austin gigafactory, did make an appearance, however. Tesla noted in its shareholder deck that it completed the deployment of Cortex, which is made up of roughly 50,000 H100 Nvidia GPUs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Cortex helped enable V13 of FSD (Supervised), which boasts major improvements in safety and comfort thanks to 4.2x increase in data, higher resolution video inputs … among other enhancements,” according to the letter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the call, CFO Vaibhav Taneja noted that Tesla accelerated the buildout of Cortex to speed up the rollout of FSD V13. He said that accumulated AI-related capital expenditures, including infrastructure, “so far has been approximately $5 billion.” In 2025, Taneja said he expects capex to be flat as it relates to AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 23 – &lt;/strong&gt;During Tesla’s Q2 2025 earnings call, Musk said Dojo 2 was expected to be “operating at scale” sometime in 2026, with “scale being somewhere around 100k H100 equivalent.” In the same breath, he hinted at possible redundancies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Thinking about Dojo 3 and the AI6 inference chip, it seems like intuitively, we want to try to find convergence there, where it’s basically the same chip,” Musk said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 28 – &lt;/strong&gt;Tesla signed a $16.5 billion deal to get its next-generation AI6 chips from Samsung. The AI6 chip is Tesla’s bet on a chip design that can scale from powering FSD and Tesla’s Optimus humanoid robots to high-performance AI training in data centers.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 6 – &lt;/strong&gt;Bloomberg reports that close to 20 Dojo workers left to start their own company that builds AI chips, software, and hardware called DensityAI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 7 – &lt;/strong&gt;Bloomberg reports that Tesla has disbanded its Dojo team and shut down the project. Peter Bannon, Dojo’s lead, left the company, as well.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk responded to the reports on X, saying: “It doesn’t make sense for Tesla to divide its resources and scale two quite different AI chip designs. The Tesla AI5, AI6 and subsequent chips will be excellent for inference and at least pretty good for training. All effort is focused on that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 10 – &lt;/strong&gt;“Once it became clear that all paths converged to AI6, I had to shut down Dojo and make some tough personnel choices, as Dojo 2 was now an evolutionary dead end,” Musk posted on X, the social media platform he owns. “Dojo 3 arguably lives on in the form of a large number of AI6 [systems-on-a-chip] on a single board.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;September 1&lt;/strong&gt; &lt;strong&gt;–&lt;/strong&gt; Tesla shares its Master Plan Part IV on social media platform X. There is no mention of Dojo or Cortex, although AI, and more specifically, “physical AI” takes a central role.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story originally published August 10, 2024. The final update of the Tesla Dojo timeline published September 2, 2025.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/tesla-dojo-v3.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk doesn’t want Tesla to be just an automaker. He wants Tesla to be an AI company, one that’s figured out how to make cars drive themselves.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Crucial to that mission was Dojo, a custom-built supercomputer designed by Tesla to train its Full Self-Driving (FSD) neural networks. FSD isn’t actually fully self-driving; it can perform some automated driving tasks, but still requires an attentive human behind the wheel. But Tesla thinks with more data, more compute power, and more training, it can cross the threshold from almost self-driving to full self-driving.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And that’s where Dojo was supposed to come in. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk teased Dojo for years and ramped up discussions about the supercomputer throughout 2024. But Dojo is now out, and another supercomputer called Cortex has entered the chat. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Below is a timeline of Dojo mentions and promises.&amp;nbsp;Check out this explainer on Dojo for even more information on what it was, why it mattered, and what comes next.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2019"&gt;&lt;strong&gt;2019&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-first-mentions-of-dojo"&gt;&lt;strong&gt;First mentions of Dojo&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;April 22 – &lt;/strong&gt;At Tesla’s Autonomy Day, the automaker had its AI team onstage to talk about Autopilot and Full Self-Driving, and the AI powering them both. The company shares information about Tesla’s custom-built chips that are designed specifically for neural networks and self-driving cars.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the event, Musk teases Dojo, revealing that it’s a supercomputer for training AI.&amp;nbsp;He also notes that all Tesla cars being produced at the time would have all hardware necessary for full self-driving and only needed a software update.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-2020-nbsp"&gt;&lt;strong&gt;2020&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-musk-begins-the-dojo-roadshow"&gt;&lt;strong&gt;Musk begins the Dojo roadshow&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Feb 2 – &lt;/strong&gt;Musk says Tesla will soon have more than a million connected vehicles worldwide with sensors and compute needed for full self-driving — and touts Dojo’s capabilities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Dojo, our training supercomputer, will be able to process vast amounts of video training data &amp;amp; efficiently run hyperspace arrays with a vast number of parameters, plenty of memory &amp;amp; ultra-high bandwidth between cores. More on this later.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 14 –&lt;/strong&gt; Musk reiterates Tesla’s plan to develop a neural network training computer called Dojo “to process truly vast amounts of video data,” calling it “a beast.” He also says the first version of Dojo is “about a year away,” which would put its launch date somewhere around August 2021.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;December&lt;/strong&gt; &lt;strong&gt;31&lt;/strong&gt; &lt;strong&gt;–&lt;/strong&gt; Elon says Dojo isn’t needed, but it will make self-driving better. “It isn’t enough to be safer than human drivers, Autopilot ultimately needs to be more than 10 times safer than human drivers.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2021"&gt;&lt;strong&gt;2021&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-tesla-makes-dojo-official"&gt;&lt;strong&gt;Tesla makes Dojo official&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 19 – &lt;/strong&gt;The automaker officially announces Dojo at Tesla’s first AI Day, an event meant to attract engineers to Tesla’s AI team. Tesla also introduces its D1 chip, which the automaker says it will use — alongside Nvidia’s GPU — to power the Dojo supercomputer. Tesla notes its AI cluster will house 3,000 D1 chips.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;October 12 – &lt;/strong&gt;Tesla releases&lt;strong&gt; &lt;/strong&gt;a&lt;strong&gt; &lt;/strong&gt;Dojo Technology whitepaper, “a guide to Tesla’s configurable floating point formats &amp;amp; arithmetic.” The whitepaper outlines a technical standard for a new type of binary floating-point arithmetic that’s used in deep learning neural networks and can be implemented “entirely in software, entirely in hardware, or in any combination of software and hardware.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2022"&gt;&lt;strong&gt;2022&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-tesla-reveals-dojo-progress"&gt;&lt;strong&gt;Tesla reveals Dojo progress&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 12 – &lt;/strong&gt;Musk says Tesla will “phase in Dojo. Won’t need to buy as many incremental GPUs next year.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;September 30 – &lt;/strong&gt;At Tesla’s second AI Day, the company reveals that it has installed the first Dojo cabinet, testing 2.2 megawatts of load testing. Tesla says it was building one tile per day (which is made up of 25 D1 chips). Tesla demos Dojo onstage running a Stable Diffusion model to create an AI-generated image of a “Cybertruck on Mars.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Importantly, the company sets a target date of a full Exapod cluster to be completed by Q1 2023, and says it plans to build a total of seven Exapods in Palo Alto.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2023"&gt;&lt;strong&gt;2023&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-a-long-shot-bet"&gt;&lt;strong&gt;A ‘long-shot bet&lt;/strong&gt;‘&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;April 19 – &lt;/strong&gt;Musk tells investors during Tesla’s first-quarter earnings that Dojo “has the potential for an order of magnitude improvement in the cost of training,” and also “has the potential to become a sellable service that we would offer to other companies in the same way that Amazon Web Services offers web services.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk also notes that he’d “look at Dojo as kind of a long-shot bet,” but a “bet worth making.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;June 21 –&lt;/strong&gt; The Tesla AI X account posts that the company’s neural networks are already in customer vehicles. The thread includes a graph with a timeline of Tesla’s current and projected compute power, which places the start of Dojo production at July 2023, although it’s not clear if this refers to the D1 chips or the supercomputer itself. Musk says that same day that Dojo was already online and running tasks at Tesla data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also projects that Tesla’s compute will be the top five in the entire world by around February 2024 (there are no indications this was successful) and that Tesla would reach 100 exaflops by October 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 19 – &lt;/strong&gt;Tesla notes in its second-quarter earnings report it has started production of Dojo. Musk also says Tesla plans to spend more than $1 billion on Dojo through 2024.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;September 6 – &lt;/strong&gt;Musk posts on X that Tesla is limited by AI training compute, but that Nvidia and Dojo will fix that. He says managing the data from the roughly 160 billion frames of video Tesla gets from its cars per day is extremely difficult.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-2024"&gt;&lt;strong&gt;2024&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-plans-to-scale"&gt;&lt;strong&gt;Plans to scale&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;January 24 – &lt;/strong&gt;During Tesla’s fourth-quarter and full-year earnings call, Musk acknowledges again that Dojo is a high-risk, high-reward project. He also says that Tesla was pursuing “the dual path of Nvidia and Dojo,” that “Dojo is working” and is “doing training jobs.” He notes Tesla is scaling it up and has “plans for Dojo 1.5, Dojo 2, Dojo 3 and whatnot.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;January 26 – &lt;/strong&gt;Tesla announced plans to spend $500 million to build a Dojo supercomputer in Buffalo. Musk then downplays the investment somewhat, posting on X that while $500 million is a large sum, it’s “only equivalent to a 10k H100 system from Nvidia. Tesla will spend more than that on Nvidia hardware this year. The table stakes for being competitive in AI are at least several billion dollars per year at this point.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;April 30 – &lt;/strong&gt;At TSMC’s North American Technology Symposium, the company says Dojo’s next-generation training tile — the D2, which puts the entire Dojo tile onto a single silicon wafer, rather than connecting 25 chips to make one tile — is already in production, according to IEEE Spectrum.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;May 20 – &lt;/strong&gt;Musk notes that the rear portion of the Giga Texas factory extension will include the construction of “a super dense, water-cooled supercomputer cluster.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;June 4 – &lt;/strong&gt;A CNBC report reveals Musk diverted thousands of Nvidia chips reserved for Tesla to X and xAI. After initially saying the report was false, Musk posts on X that Tesla didn’t have a location to send the Nvidia chips to turn them on, due to the continued construction on the south extension of Giga Texas, “so they would have just sat in a warehouse.” He noted the extension will “house 50k H100s for FSD training.”&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also posts:&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Of the roughly $10B in AI-related expenditures I said Tesla would make this year, about half is internal, primarily the Tesla-designed AI inference computer and sensors present in all of our cars, plus Dojo. For building the AI training superclusters, NVidia hardware is about 2/3 of the cost. My current best guess for Nvidia purchases by Tesla are $3B to $4B this year.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 1 – &lt;/strong&gt;Musk reveals on X that current Tesla vehicles may not have the right hardware for the company’s next-gen AI model. He says that the roughly 5x increase in parameter count with the next-gen AI “is very difficult to achieve without upgrading the vehicle inference computer.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-nvidia-supply-challenges"&gt;Nvidia supply challenges&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 23 – &lt;/strong&gt;During Tesla’s second-quarter earnings call, Musk says demand for Nvidia hardware is “so high that it’s often difficult to get the GPUs.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think this therefore requires that we put a lot more effort on Dojo in order to ensure that we’ve got the training capability that we need,” Musk says. “And we do see a path to being competitive with Nvidia with Dojo.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A graph in Tesla’s investor deck predicts that Tesla AI training capacity will ramp to roughly 90,000 H100 equivalent GPUs by the end of 2024, up from around 40,000 in June. Later that day on X, Musk posts that Dojo 1 will have “roughly 8k H100-equivalent of training online by end of year.” He also posts photos of the supercomputer, which appears to use the same fridge-like stainless steel exterior as Tesla’s Cybertrucks.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-from-dojo-to-cortex"&gt;From Dojo to Cortex&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 30 –&amp;nbsp; &lt;/strong&gt;AI5 is ~18 months away from high-volume production, Musk says in a reply to a post from someone claiming to start a club of “Tesla HW4/AI4 owners angry about getting left behind when AI5 comes out.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 3 – &lt;/strong&gt;Musk posts on X that he did a walkthrough of “the Tesla supercompute cluster at Giga Texas (aka Cortex).” He notes that it would be made roughly of 100,000 H100/H200 Nvidia GPUs with “massive storage for video training of FSD &amp;amp; Optimus.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 26 – &lt;/strong&gt;Musk posts on X a video of Cortex, which he refers to as “the giant new AI training supercluster being built at Tesla HQ in Austin to solve real-world AI.”&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-202-5"&gt;&lt;strong&gt;202&lt;/strong&gt;5&lt;/h2&gt;

&lt;h2 class="wp-block-heading" id="h-dojo-shutdown-its-team-disbanded"&gt;&lt;strong&gt;Dojo shutdown, its team disbanded&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;January 29 –&lt;/strong&gt; Tesla’s Q4 and full-year 2024 earnings call included no mention of Dojo. Cortex, Tesla’s new AI training supercluster at the Austin gigafactory, did make an appearance, however. Tesla noted in its shareholder deck that it completed the deployment of Cortex, which is made up of roughly 50,000 H100 Nvidia GPUs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Cortex helped enable V13 of FSD (Supervised), which boasts major improvements in safety and comfort thanks to 4.2x increase in data, higher resolution video inputs … among other enhancements,” according to the letter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the call, CFO Vaibhav Taneja noted that Tesla accelerated the buildout of Cortex to speed up the rollout of FSD V13. He said that accumulated AI-related capital expenditures, including infrastructure, “so far has been approximately $5 billion.” In 2025, Taneja said he expects capex to be flat as it relates to AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 23 – &lt;/strong&gt;During Tesla’s Q2 2025 earnings call, Musk said Dojo 2 was expected to be “operating at scale” sometime in 2026, with “scale being somewhere around 100k H100 equivalent.” In the same breath, he hinted at possible redundancies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Thinking about Dojo 3 and the AI6 inference chip, it seems like intuitively, we want to try to find convergence there, where it’s basically the same chip,” Musk said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;July 28 – &lt;/strong&gt;Tesla signed a $16.5 billion deal to get its next-generation AI6 chips from Samsung. The AI6 chip is Tesla’s bet on a chip design that can scale from powering FSD and Tesla’s Optimus humanoid robots to high-performance AI training in data centers.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 6 – &lt;/strong&gt;Bloomberg reports that close to 20 Dojo workers left to start their own company that builds AI chips, software, and hardware called DensityAI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 7 – &lt;/strong&gt;Bloomberg reports that Tesla has disbanded its Dojo team and shut down the project. Peter Bannon, Dojo’s lead, left the company, as well.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk responded to the reports on X, saying: “It doesn’t make sense for Tesla to divide its resources and scale two quite different AI chip designs. The Tesla AI5, AI6 and subsequent chips will be excellent for inference and at least pretty good for training. All effort is focused on that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;August 10 – &lt;/strong&gt;“Once it became clear that all paths converged to AI6, I had to shut down Dojo and make some tough personnel choices, as Dojo 2 was now an evolutionary dead end,” Musk posted on X, the social media platform he owns. “Dojo 3 arguably lives on in the form of a large number of AI6 [systems-on-a-chip] on a single board.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;September 1&lt;/strong&gt; &lt;strong&gt;–&lt;/strong&gt; Tesla shares its Master Plan Part IV on social media platform X. There is no mention of Dojo or Cortex, although AI, and more specifically, “physical AI” takes a central role.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story originally published August 10, 2024. The final update of the Tesla Dojo timeline published September 2, 2025.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/02/teslas-dojo-a-timeline/</guid><pubDate>Tue, 02 Sep 2025 16:39:01 +0000</pubDate></item><item><title>[NEW] OpenAI acquires product testing startup Statsig and shakes up its leadership team (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/02/openai-acquires-product-testing-startup-statsig-and-shakes-up-its-leadership-team/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-2021258442.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced in a blog post on Tuesday that it agreed to acquire the product testing startup Statsig, and bring on its founder and CEO, Vijaye Raji, as the company’s CTO of Applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is paying $1.1 billion for Statsig in an all-stock deal — one of the largest acquisitions ever for the ChatGPT maker — under the company’s current $300 billion valuation, OpenAI spokesperson Kayla Wood told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The acquisition marks OpenAI’s latest effort to build out its Applications business, helmed by the former CEO of Instacart, Fidji Simo, who started work at the company a few weeks ago. Raji will report to Simo and will head product engineering for ChatGPT, the company’s AI coding tool Codex, and future applications that OpenAI plans to build. The company says that bringing Statsig’s experimentation platform in-house will accelerate product development across the Applications organization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Raji comes on board, OpenAI is making changes to its leadership team. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s chief product officer, Kevin Weil, will become VP of a new group called OpenAI for Science, he announced in a post on LinkedIn. Weil says the goal of his new organization “is to build the next great scientific instrument: an AI-powered platform that accelerates scientific discovery.” Weil says he will work closely with Sebastien Bubeck, an OpenAI researcher and the former VP of AI and Distinguished Scientist at Microsoft.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m able to do this because the product and design leaders at OpenAI are amazing, and now are complemented by Fidji Simo beginning her role as CEO of Applications,” said Weil. “OpenAI’s products have been my life since I joined, and they’re in great hands.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, OpenAI’s current head of engineering, Srinivas Narayanan, announced in a post on LinkedIn that he would transition to a new role as the company’s CTO of B2B applications. In the role, Narayanan says he will collaborate directly with OpenAI’s COO, Brad Lightcap, who oversees many of the company’s relationships with enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says the Statsig acquisition is pending regulatory review. Once completed, the company says that all Statsig employees will become OpenAI employees. However, the product testing startup will “continue operating independently and serving its customer base out of its Seattle office,” the company said in their blog post.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-2021258442.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced in a blog post on Tuesday that it agreed to acquire the product testing startup Statsig, and bring on its founder and CEO, Vijaye Raji, as the company’s CTO of Applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is paying $1.1 billion for Statsig in an all-stock deal — one of the largest acquisitions ever for the ChatGPT maker — under the company’s current $300 billion valuation, OpenAI spokesperson Kayla Wood told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The acquisition marks OpenAI’s latest effort to build out its Applications business, helmed by the former CEO of Instacart, Fidji Simo, who started work at the company a few weeks ago. Raji will report to Simo and will head product engineering for ChatGPT, the company’s AI coding tool Codex, and future applications that OpenAI plans to build. The company says that bringing Statsig’s experimentation platform in-house will accelerate product development across the Applications organization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Raji comes on board, OpenAI is making changes to its leadership team. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s chief product officer, Kevin Weil, will become VP of a new group called OpenAI for Science, he announced in a post on LinkedIn. Weil says the goal of his new organization “is to build the next great scientific instrument: an AI-powered platform that accelerates scientific discovery.” Weil says he will work closely with Sebastien Bubeck, an OpenAI researcher and the former VP of AI and Distinguished Scientist at Microsoft.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m able to do this because the product and design leaders at OpenAI are amazing, and now are complemented by Fidji Simo beginning her role as CEO of Applications,” said Weil. “OpenAI’s products have been my life since I joined, and they’re in great hands.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, OpenAI’s current head of engineering, Srinivas Narayanan, announced in a post on LinkedIn that he would transition to a new role as the company’s CTO of B2B applications. In the role, Narayanan says he will collaborate directly with OpenAI’s COO, Brad Lightcap, who oversees many of the company’s relationships with enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says the Statsig acquisition is pending regulatory review. Once completed, the company says that all Statsig employees will become OpenAI employees. However, the product testing startup will “continue operating independently and serving its customer base out of its Seattle office,” the company said in their blog post.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/02/openai-acquires-product-testing-startup-statsig-and-shakes-up-its-leadership-team/</guid><pubDate>Tue, 02 Sep 2025 19:04:05 +0000</pubDate></item><item><title>[NEW] Tesla has a new master plan—it just doesn’t have any specifics (AI – Ars Technica)</title><link>https://arstechnica.com/cars/2025/09/tesla-has-a-new-master-plan-it-just-doesnt-have-any-specifics/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Did an AI write this? Because it reads like an AI wrote this.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Concept of Creative Ideas and Innovation. Flow chart of converting ideas into action and implementation." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1074291718-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Concept of Creative Ideas and Innovation. Flow chart of converting ideas into action and implementation." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1074291718-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Yesterday afternoon, while much of the country enjoyed Labor Day, Tesla CEO Elon Musk published a new master plan for the company to his social media platform. It's the fourth such document for Tesla, replacing the goals Musk laid out in 2023 when he said the company would sell 20 million EVs a year in 2030. This time, it is not entirely sure what Tesla's plan actually entails. The text, which reads as though it was written by AI, is at times anodyne, at times confusing, but always free of specifics.&lt;/p&gt;
&lt;p&gt;Each iteration of the master plan is Tesla's north star, the new plan reads, promising to "to deliver unconstrained sustainability without compromise," whatever that actually means.&lt;/p&gt;
&lt;p&gt;"Now, we are combining our manufacturing capabilities with our autonomous prowess to deliver new products and services that will accelerate global prosperity and human thriving driven by economic growth shared by all," reads the plan.&lt;/p&gt;
&lt;p&gt;This is an interesting statement considering each time Tesla has tried to build a new model the result has been months and months of "production difficulties," not to mention the multiple federal safety investigations into the company's autonomous and partially automated driving systems.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Tesla also disbanded the team building its "Dojo" supercomputer several weeks ago. Much touted by Musk in the past as the key to beating autonomous vehicle developers like Waymo (which has already deployed commercially in several cities), Tesla will no longer rely on this in-house resource and instead rely on external companies, according to Bloomberg.&lt;/p&gt;
&lt;p&gt;"Shortages in resources can be remedied by improved technology, greater innovation and new ideas," the plan continues.&lt;/p&gt;
&lt;p&gt;Then plan veers into corporate buzzwords, with statements like "[o]ur desire to push beyond what is considered achievable will foster the growth needed for truly sustainable abundance."&lt;/p&gt;
&lt;p&gt;In keeping with Musk's recent robot obsession, there's very little mention of Tesla electric vehicles other than a brief mention of autonomous vehicles, but there is quite a lot of text devoted to the company's humanoid robot. "Jobs and tasks that are particularly monotonous or dangerous can now be accomplished by other means," it states, blithely eliding the fact that it makes very little sense to compromise an industrial robot with a bipedal humanoid body, as evinced by the non-humanoid form factors of just about every industrial robot working today. Robot arms mounted to the floor don’t need to worry about balance, nor do quadraped robots with wheels.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Did an AI write this? Because it reads like an AI wrote this.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Concept of Creative Ideas and Innovation. Flow chart of converting ideas into action and implementation." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1074291718-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Concept of Creative Ideas and Innovation. Flow chart of converting ideas into action and implementation." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1074291718-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Yesterday afternoon, while much of the country enjoyed Labor Day, Tesla CEO Elon Musk published a new master plan for the company to his social media platform. It's the fourth such document for Tesla, replacing the goals Musk laid out in 2023 when he said the company would sell 20 million EVs a year in 2030. This time, it is not entirely sure what Tesla's plan actually entails. The text, which reads as though it was written by AI, is at times anodyne, at times confusing, but always free of specifics.&lt;/p&gt;
&lt;p&gt;Each iteration of the master plan is Tesla's north star, the new plan reads, promising to "to deliver unconstrained sustainability without compromise," whatever that actually means.&lt;/p&gt;
&lt;p&gt;"Now, we are combining our manufacturing capabilities with our autonomous prowess to deliver new products and services that will accelerate global prosperity and human thriving driven by economic growth shared by all," reads the plan.&lt;/p&gt;
&lt;p&gt;This is an interesting statement considering each time Tesla has tried to build a new model the result has been months and months of "production difficulties," not to mention the multiple federal safety investigations into the company's autonomous and partially automated driving systems.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Tesla also disbanded the team building its "Dojo" supercomputer several weeks ago. Much touted by Musk in the past as the key to beating autonomous vehicle developers like Waymo (which has already deployed commercially in several cities), Tesla will no longer rely on this in-house resource and instead rely on external companies, according to Bloomberg.&lt;/p&gt;
&lt;p&gt;"Shortages in resources can be remedied by improved technology, greater innovation and new ideas," the plan continues.&lt;/p&gt;
&lt;p&gt;Then plan veers into corporate buzzwords, with statements like "[o]ur desire to push beyond what is considered achievable will foster the growth needed for truly sustainable abundance."&lt;/p&gt;
&lt;p&gt;In keeping with Musk's recent robot obsession, there's very little mention of Tesla electric vehicles other than a brief mention of autonomous vehicles, but there is quite a lot of text devoted to the company's humanoid robot. "Jobs and tasks that are particularly monotonous or dangerous can now be accomplished by other means," it states, blithely eliding the fact that it makes very little sense to compromise an industrial robot with a bipedal humanoid body, as evinced by the non-humanoid form factors of just about every industrial robot working today. Robot arms mounted to the floor don’t need to worry about balance, nor do quadraped robots with wheels.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/cars/2025/09/tesla-has-a-new-master-plan-it-just-doesnt-have-any-specifics/</guid><pubDate>Tue, 02 Sep 2025 19:25:00 +0000</pubDate></item><item><title>[NEW] Amazon launches Lens Live, an AI-powered shopping tool for use in the real world (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/02/amazon-launches-lens-live-an-ai-powered-shopping-tool-for-use-in-the-real-world/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon is further investing in AI-powered shopping experiences with Tuesday’s launch of Lens Live, a new AI-powered upgrade to its Amazon Lens shopping feature that allows consumers to discover new products through visual search, similar to competitors like Google Lens and Pinterest Lens. The tool will also integrate with Amazon’s AI shopping assistant, Rufus, for product insights, the retailer notes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lens Live will not replace Amazon’s existing visual search tool, Amazon Lens, which lets you take a picture, upload an image, or scan a barcode to discover products. Instead, it brings a real-time component to Amazon Lens so you can point your phone at things you’re seeing in the real world to see matching products in a swipeable carousel at the bottom of the screen.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The addition is one of several ways Amazon has been leveraging AI to help online shoppers. Over the past year or so, the company has also rolled out other features like its AI assistant Rufus, AI-powered shopping guides, AI-enhanced product reviews, AI tools for finding clothes that fit, AI audio product summaries, personalized shopping prompts, as well as tools for merchants.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lens Live also capitalizes on activities customers are already doing: comparison shopping while in retail stores out in the real world to see if Amazon has a better deal on the same or similar item.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3041652" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/download-1.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When using the new Lens Live feature, customers can tap on any item in their camera view to trigger the feature to focus on that product. If they find a match they like, they can add it to their shopping cart by tapping the (+) plus icon or tap the heart icon to save it to their wish list.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is powered by&amp;nbsp;Amazon SageMaker&amp;nbsp;services, which allow machine learning models to be deployed at scale. It runs on AWS-managed Amazon OpenSearch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Amazon’s AI-powered shopping assistant Rufus is available in the new experience, allowing customers to see AI-generated product summaries and suggested questions of conversational prompts they can ask to learn more about the item. According to Amazon, this lets shoppers do some quick product research and view product insights before making a purchase.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Lens Live feature is first launching on the Amazon Shopping app on iOS, initially for “tens of millions” of U.S. shoppers before rolling out to others in the U.S. The company didn’t say whether it’s going to expand to other global markets.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon is further investing in AI-powered shopping experiences with Tuesday’s launch of Lens Live, a new AI-powered upgrade to its Amazon Lens shopping feature that allows consumers to discover new products through visual search, similar to competitors like Google Lens and Pinterest Lens. The tool will also integrate with Amazon’s AI shopping assistant, Rufus, for product insights, the retailer notes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lens Live will not replace Amazon’s existing visual search tool, Amazon Lens, which lets you take a picture, upload an image, or scan a barcode to discover products. Instead, it brings a real-time component to Amazon Lens so you can point your phone at things you’re seeing in the real world to see matching products in a swipeable carousel at the bottom of the screen.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The addition is one of several ways Amazon has been leveraging AI to help online shoppers. Over the past year or so, the company has also rolled out other features like its AI assistant Rufus, AI-powered shopping guides, AI-enhanced product reviews, AI tools for finding clothes that fit, AI audio product summaries, personalized shopping prompts, as well as tools for merchants.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lens Live also capitalizes on activities customers are already doing: comparison shopping while in retail stores out in the real world to see if Amazon has a better deal on the same or similar item.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3041652" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/download-1.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When using the new Lens Live feature, customers can tap on any item in their camera view to trigger the feature to focus on that product. If they find a match they like, they can add it to their shopping cart by tapping the (+) plus icon or tap the heart icon to save it to their wish list.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is powered by&amp;nbsp;Amazon SageMaker&amp;nbsp;services, which allow machine learning models to be deployed at scale. It runs on AWS-managed Amazon OpenSearch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Amazon’s AI-powered shopping assistant Rufus is available in the new experience, allowing customers to see AI-generated product summaries and suggested questions of conversational prompts they can ask to learn more about the item. According to Amazon, this lets shoppers do some quick product research and view product insights before making a purchase.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Lens Live feature is first launching on the Amazon Shopping app on iOS, initially for “tens of millions” of U.S. shoppers before rolling out to others in the U.S. The company didn’t say whether it’s going to expand to other global markets.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/02/amazon-launches-lens-live-an-ai-powered-shopping-tool-for-use-in-the-real-world/</guid><pubDate>Tue, 02 Sep 2025 19:42:23 +0000</pubDate></item><item><title>OpenAI to route sensitive conversations to GPT-5, introduce parental controls (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/02/openai-to-route-sensitive-conversations-to-gpt-5-introduce-parental-controls/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-1922977290.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;em&gt;This article has been updated with comment from lead counsel in the Raine family’s wrongful death lawsuit against OpenAI&lt;/em&gt;.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI said Tuesday it plans to route sensitive conversations to reasoning models like GPT-5 and roll out parental controls within the next month — part of an ongoing response to recent safety incidents involving ChatGPT failing to detect mental distress.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new guardrails come in the aftermath of the suicide of teenager Adam Raine, who discussed self-harm and plans to end his life with ChatGPT, which even supplied him with information about specific suicide methods. Raine’s parents have filed a wrongful death lawsuit against OpenAI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post last week, OpenAI acknowledged shortcomings in its safety systems, including failures to maintain guardrails during extended conversations. Experts attribute these issues to fundamental design elements: the models’ tendency to validate user statements and their next-word prediction algorithms, which cause chatbots to follow conversational threads rather than redirect potentially harmful discussions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That tendency is displayed in the extreme in the case of Stein-Erik Soelberg, whose murder-suicide was reported on by The Wall Street Journal over the weekend. Soelberg, who had a history of mental illness, used ChatGPT to validate and fuel his paranoia that he was being targeted in a grand conspiracy. His delusions progressed so badly that he ended up killing his mother and himself last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI thinks that at least one solution to conversations that go off the rails could be to automatically reroute sensitive chats to “reasoning” models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We recently introduced a real-time router that can choose between efficient chat models and reasoning models based on the conversation context,” OpenAI wrote in a Tuesday blog post. “We’ll soon begin to route some sensitive conversations—like when our system detects signs of acute distress—to a reasoning model, like GPT‑5-thinking, so it can provide more helpful and beneficial responses, regardless of which model a person first selected.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says its GPT-5 thinking and o3 models are built to spend more time thinking for longer and reasoning through context before answering, which means they are “more resistant to adversarial prompts.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI firm also said it would roll out parental controls in the next month, allowing parents to link their account with their teen’s account through an email invitation. In late July, OpenAI rolled out Study Mode in ChatGPT to help students maintain critical thinking capabilities while studying, rather than tapping ChatGPT to write their essays for them. Soon, parents will be able to control how ChatGPT responds to their child with “age-appropriate model behavior rules, which are on by default.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Parents will also be able to disable features like memory and chat history, which experts say could lead to delusional thinking and other problematic behavior, including dependency and attachment issues, reinforcement of harmful thought patterns, and the illusion of thought-reading. In the case of Adam Raine, ChatGPT supplied methods to commit suicide that reflected knowledge of his hobbies, per The New York Times.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perhaps the most important parental control that OpenAI intends to roll out is that parents can receive notifications when the system detects their teenager is in a moment of “acute distress.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has asked OpenAI for more information about how the company is able to flag moments of acute distress in real time, how long it has had “age-appropriate model behavior rules” on by default, and whether it is exploring allowing parents to implement a time limit on teenage use of ChatGPT.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has already rolled out in-app reminders during long sessions to encourage breaks for all users, but stops short of cutting people off who might be using ChatGPT to spiral.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI firm says these safeguards are part of a “120-day initiative” to preview plans for improvements that OpenAI hopes to launch this year. The company also said it is partnering with experts — including ones with expertise in areas like eating disorders, substance use, and adolescent health — via its Global Physician Network and Expert Council on Well-Being and AI to help “define and measure well-being, set priorities, and design future safeguards.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has asked OpenAI how many mental health professionals are involved in this initiative, who leads its Expert Council, and what suggestions mental health experts have made in terms of product, research, and policy decisions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jay Edelson, lead counsel in the Raine family’s wrongful death lawsuit against OpenAI, said the company’s response to ChatGPT’s ongoing safety risks has been “inadequate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“OpenAI doesn’t need an expert panel to determine that ChatGPT 4o is dangerous,” Edelson said in a statement shared with TechCrunch. “They knew that the day they launched the product, and they know it today. Nor should Sam Altman be hiding behind the company’s PR team. Sam should either unequivocally say that he believes ChatGPT is safe or immediately pull it from the market.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-1922977290.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;em&gt;This article has been updated with comment from lead counsel in the Raine family’s wrongful death lawsuit against OpenAI&lt;/em&gt;.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI said Tuesday it plans to route sensitive conversations to reasoning models like GPT-5 and roll out parental controls within the next month — part of an ongoing response to recent safety incidents involving ChatGPT failing to detect mental distress.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new guardrails come in the aftermath of the suicide of teenager Adam Raine, who discussed self-harm and plans to end his life with ChatGPT, which even supplied him with information about specific suicide methods. Raine’s parents have filed a wrongful death lawsuit against OpenAI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post last week, OpenAI acknowledged shortcomings in its safety systems, including failures to maintain guardrails during extended conversations. Experts attribute these issues to fundamental design elements: the models’ tendency to validate user statements and their next-word prediction algorithms, which cause chatbots to follow conversational threads rather than redirect potentially harmful discussions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That tendency is displayed in the extreme in the case of Stein-Erik Soelberg, whose murder-suicide was reported on by The Wall Street Journal over the weekend. Soelberg, who had a history of mental illness, used ChatGPT to validate and fuel his paranoia that he was being targeted in a grand conspiracy. His delusions progressed so badly that he ended up killing his mother and himself last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI thinks that at least one solution to conversations that go off the rails could be to automatically reroute sensitive chats to “reasoning” models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We recently introduced a real-time router that can choose between efficient chat models and reasoning models based on the conversation context,” OpenAI wrote in a Tuesday blog post. “We’ll soon begin to route some sensitive conversations—like when our system detects signs of acute distress—to a reasoning model, like GPT‑5-thinking, so it can provide more helpful and beneficial responses, regardless of which model a person first selected.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says its GPT-5 thinking and o3 models are built to spend more time thinking for longer and reasoning through context before answering, which means they are “more resistant to adversarial prompts.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI firm also said it would roll out parental controls in the next month, allowing parents to link their account with their teen’s account through an email invitation. In late July, OpenAI rolled out Study Mode in ChatGPT to help students maintain critical thinking capabilities while studying, rather than tapping ChatGPT to write their essays for them. Soon, parents will be able to control how ChatGPT responds to their child with “age-appropriate model behavior rules, which are on by default.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Parents will also be able to disable features like memory and chat history, which experts say could lead to delusional thinking and other problematic behavior, including dependency and attachment issues, reinforcement of harmful thought patterns, and the illusion of thought-reading. In the case of Adam Raine, ChatGPT supplied methods to commit suicide that reflected knowledge of his hobbies, per The New York Times.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perhaps the most important parental control that OpenAI intends to roll out is that parents can receive notifications when the system detects their teenager is in a moment of “acute distress.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has asked OpenAI for more information about how the company is able to flag moments of acute distress in real time, how long it has had “age-appropriate model behavior rules” on by default, and whether it is exploring allowing parents to implement a time limit on teenage use of ChatGPT.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has already rolled out in-app reminders during long sessions to encourage breaks for all users, but stops short of cutting people off who might be using ChatGPT to spiral.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI firm says these safeguards are part of a “120-day initiative” to preview plans for improvements that OpenAI hopes to launch this year. The company also said it is partnering with experts — including ones with expertise in areas like eating disorders, substance use, and adolescent health — via its Global Physician Network and Expert Council on Well-Being and AI to help “define and measure well-being, set priorities, and design future safeguards.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has asked OpenAI how many mental health professionals are involved in this initiative, who leads its Expert Council, and what suggestions mental health experts have made in terms of product, research, and policy decisions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jay Edelson, lead counsel in the Raine family’s wrongful death lawsuit against OpenAI, said the company’s response to ChatGPT’s ongoing safety risks has been “inadequate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“OpenAI doesn’t need an expert panel to determine that ChatGPT 4o is dangerous,” Edelson said in a statement shared with TechCrunch. “They knew that the day they launched the product, and they know it today. Nor should Sam Altman be hiding behind the company’s PR team. Sam should either unequivocally say that he believes ChatGPT is safe or immediately pull it from the market.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/02/openai-to-route-sensitive-conversations-to-gpt-5-introduce-parental-controls/</guid><pubDate>Tue, 02 Sep 2025 20:13:04 +0000</pubDate></item><item><title>[NEW] 3 Questions: On biology and medicine’s “data revolution” (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/3-questions-caroline-uhler-biology-medicine-data-revolution-0902</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-caroline-uhler.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;&lt;em&gt;Caroline Uhler is&amp;nbsp;an Andrew (1956) and Erna Viterbi Professor of Engineering at MIT; a professor of electrical engineering and computer science in the Institute for Data, Science, and Society (IDSS);&amp;nbsp;and director of the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard, where she is also a core institute and scientific leadership team member.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;Uhler is interested in all the methods by which scientists can uncover causality in biological systems, ranging from causal discovery on observed variables to causal feature learning and representation learning.&amp;nbsp;In this interview, she discusses machine learning in biology, areas that are ripe for problem-solving, and cutting-edge research coming out of the Schmidt Center.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q: &lt;/strong&gt;The Eric and Wendy Schmidt Center has four distinct areas of focus structured around four natural levels of biological organization: proteins, cells, tissues, and organisms. What, within the current landscape of machine learning, makes&amp;nbsp;now the right time to work on these specific problem classes?&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A: &lt;/strong&gt;Biology and medicine are currently undergoing a “data revolution.” The availability of large-scale, diverse datasets — ranging from genomics and multi-omics to high-resolution imaging and electronic health records — makes this an opportune time. Inexpensive and accurate DNA sequencing is a reality, advanced molecular imaging has become routine, and single cell genomics is allowing the profiling of millions of cells. These innovations — and the massive datasets they produce — have brought us to the threshold of a new era in biology, one where we will be able to move beyond characterizing the units of life (such as all proteins, genes, and cell types) to understanding the `programs of life’, such as the logic of gene circuits and cell-cell communication that underlies tissue patterning and the molecular mechanisms that underlie the genotype-phenotype map.&lt;/p&gt;&lt;p dir="ltr"&gt;At the same time, in the past decade, machine learning has seen remarkable progress with models like BERT, GPT-3, and ChatGPT demonstrating advanced capabilities in text understanding and generation, while vision transformers and multimodal models like CLIP have achieved human-level performance in image-related tasks. These breakthroughs provide powerful architectural blueprints and training strategies that can be adapted to biological data. For instance, transformers can model genomic sequences similar to language, and vision models can analyze medical and microscopy images.&lt;/p&gt;&lt;p dir="ltr"&gt;Importantly, biology is poised to be not just a beneficiary of machine learning, but also a significant source of inspiration for new ML research. Much like agriculture and breeding spurred modern statistics, biology has the potential to inspire new and perhaps even more profound avenues of ML research. Unlike fields such as recommender systems and internet advertising, where there are no natural laws to discover and predictive accuracy is the ultimate measure of value, in biology, phenomena are physically interpretable, and causal mechanisms are the ultimate goal. Additionally, biology boasts genetic and chemical tools that enable perturbational screens on an unparalleled scale compared to other fields. These combined features make biology uniquely suited to both benefit greatly from ML and serve as a profound wellspring of inspiration for it.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q: &lt;/strong&gt;Taking a somewhat different tack, what problems in biology are still really resistant to our current tool set? Are there areas, perhaps specific challenges in disease or in wellness, which you feel are ripe for problem-solving?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;Machine learning has demonstrated remarkable success in predictive tasks across domains such as image classification, natural language processing, and clinical risk modeling. However, in the biological sciences, predictive accuracy is often insufficient. The fundamental questions in these fields are inherently causal: How does a perturbation to a specific gene or pathway affect downstream cellular processes? What is the mechanism by which an intervention leads to a phenotypic change? Traditional machine learning models, which are primarily optimized for capturing statistical associations in observational data, often fail to answer such interventional queries.There is a strong need for biology and medicine to also inspire new foundational developments in machine learning.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The field is now equipped with high-throughput perturbation technologies — such as pooled CRISPR screens, single-cell transcriptomics, and spatial profiling — that generate rich datasets under systematic interventions. These data modalities naturally call for the development of models that go beyond pattern recognition to support causal inference, active experimental design, and representation learning in settings with complex, structured latent variables. From a mathematical perspective, this requires tackling core questions of identifiability, sample efficiency, and the integration of combinatorial, geometric, and probabilistic tools. I believe that addressing these challenges will not only unlock new insights into the mechanisms of cellular systems, but also push the theoretical boundaries of machine learning.&lt;/p&gt;&lt;p&gt;With respect to foundation models, a consensus in the field is that we are still far from creating a holistic foundation model for biology across scales, similar to what ChatGPT represents in the language domain — a sort of digital organism capable of simulating all biological phenomena. While new foundation models emerge almost weekly, these models have thus far been specialized for a specific scale and question, and focus on one or a few modalities.&lt;/p&gt;&lt;p&gt;Significant progress has been made in predicting protein structures from their sequences. This success has highlighted the importance of iterative machine learning challenges, such as CASP (critical assessment of structure prediction), which have been instrumental in benchmarking state-of-the-art algorithms for protein structure prediction and driving their improvement.&lt;/p&gt;&lt;p&gt;The Schmidt Center is organizing challenges to increase awareness in the ML field and make progress in the development of methods to solve causal prediction problems that are so critical for the biomedical sciences. With the increasing availability of single-gene perturbation data at the single-cell level,&amp;nbsp;I believe predicting the effect of single or combinatorial perturbations, and which perturbations could drive a desired phenotype, are solvable problems. With our Cell Perturbation Prediction Challenge (CPPC), we aim to provide the means to objectively test and benchmark algorithms for predicting the effect of new perturbations.&lt;/p&gt;&lt;p&gt;Another area where the field has made remarkable strides is disease diagnostic and patient triage. Machine learning algorithms can integrate different sources of patient information (data modalities), generate missing modalities, identify patterns that may be difficult for us to detect, and help stratify patients based on their disease risk. While we must remain cautious about potential biases in model predictions, the danger of models learning shortcuts instead of true correlations, and the risk of automation bias in clinical decision-making, I believe this is an area where machine learning is already having a significant impact.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q: &lt;/strong&gt;Let’s talk about some of the&amp;nbsp;headlines coming out of the Schmidt Center recently. What current research do you think people should be particularly excited about, and why?&lt;em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;In collaboration with Dr. Fei Chen at the Broad Institute, we have recently developed a method for the prediction of unseen proteins’ subcellular location, called PUPS. Many existing methods can only make predictions based on the specific protein and cell data on which they were trained. PUPS, however, combines a protein language model with an image in-painting model to utilize both protein sequences and cellular images. We demonstrate that the protein sequence input enables generalization to unseen proteins, and the cellular image input captures single-cell variability, enabling cell-type-specific predictions. The model learns how relevant each amino acid residue is for the predicted sub-cellular localization, and it can predict changes in localization due to mutations in the protein sequences. Since proteins’ function is strictly related to their subcellular localization, our predictions could provide insights into potential mechanisms of disease. In the future, we aim to extend this method to predict the localization of multiple proteins in a cell and possibly understand protein-protein interactions.&lt;/p&gt;&lt;p&gt;Together with Professor G.V. Shivashankar, a long-time collaborator at ETH Zürich, we have previously shown how simple images of cells stained with fluorescent DNA-intercalating dyes to label the chromatin can yield a lot of information about the state and fate of a cell in health and disease, when combined with machine learning algorithms. Recently, we have furthered this observation and proved the deep link between chromatin organization and gene regulation by developing Image2Reg, a method that enables the prediction of unseen genetically or chemically perturbed genes from chromatin images. Image2Reg utilizes convolutional neural networks to learn an informative representation of the chromatin images of perturbed cells. It also employs a graph convolutional network to create a gene embedding that captures the regulatory effects of genes based on protein-protein interaction data, integrated with cell-type-specific transcriptomic data. Finally, it learns a map between the resulting physical and biochemical representation of cells, allowing us to predict the perturbed gene modules based on chromatin images.&lt;/p&gt;&lt;p&gt;Furthermore, we recently finalized the development of a method for predicting the outcomes of unseen combinatorial gene perturbations and identifying the types of interactions occurring between the perturbed genes. MORPH can guide the design of the most informative perturbations for lab-in-a-loop experiments. Furthermore, the attention-based framework provably enables our method to identify causal relations among the genes, providing insights into the underlying gene regulatory programs. Finally, thanks to its modular structure, we can apply MORPH to perturbation data measured in various modalities, including not only transcriptomics, but also imaging. We are very excited about the potential of this method to enable the efficient exploration of the perturbation space to advance our understanding of cellular programs by bridging causal theory to important applications, with implications for both basic research and therapeutic applications.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/mit-caroline-uhler.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;&lt;em&gt;Caroline Uhler is&amp;nbsp;an Andrew (1956) and Erna Viterbi Professor of Engineering at MIT; a professor of electrical engineering and computer science in the Institute for Data, Science, and Society (IDSS);&amp;nbsp;and director of the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard, where she is also a core institute and scientific leadership team member.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;Uhler is interested in all the methods by which scientists can uncover causality in biological systems, ranging from causal discovery on observed variables to causal feature learning and representation learning.&amp;nbsp;In this interview, she discusses machine learning in biology, areas that are ripe for problem-solving, and cutting-edge research coming out of the Schmidt Center.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q: &lt;/strong&gt;The Eric and Wendy Schmidt Center has four distinct areas of focus structured around four natural levels of biological organization: proteins, cells, tissues, and organisms. What, within the current landscape of machine learning, makes&amp;nbsp;now the right time to work on these specific problem classes?&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A: &lt;/strong&gt;Biology and medicine are currently undergoing a “data revolution.” The availability of large-scale, diverse datasets — ranging from genomics and multi-omics to high-resolution imaging and electronic health records — makes this an opportune time. Inexpensive and accurate DNA sequencing is a reality, advanced molecular imaging has become routine, and single cell genomics is allowing the profiling of millions of cells. These innovations — and the massive datasets they produce — have brought us to the threshold of a new era in biology, one where we will be able to move beyond characterizing the units of life (such as all proteins, genes, and cell types) to understanding the `programs of life’, such as the logic of gene circuits and cell-cell communication that underlies tissue patterning and the molecular mechanisms that underlie the genotype-phenotype map.&lt;/p&gt;&lt;p dir="ltr"&gt;At the same time, in the past decade, machine learning has seen remarkable progress with models like BERT, GPT-3, and ChatGPT demonstrating advanced capabilities in text understanding and generation, while vision transformers and multimodal models like CLIP have achieved human-level performance in image-related tasks. These breakthroughs provide powerful architectural blueprints and training strategies that can be adapted to biological data. For instance, transformers can model genomic sequences similar to language, and vision models can analyze medical and microscopy images.&lt;/p&gt;&lt;p dir="ltr"&gt;Importantly, biology is poised to be not just a beneficiary of machine learning, but also a significant source of inspiration for new ML research. Much like agriculture and breeding spurred modern statistics, biology has the potential to inspire new and perhaps even more profound avenues of ML research. Unlike fields such as recommender systems and internet advertising, where there are no natural laws to discover and predictive accuracy is the ultimate measure of value, in biology, phenomena are physically interpretable, and causal mechanisms are the ultimate goal. Additionally, biology boasts genetic and chemical tools that enable perturbational screens on an unparalleled scale compared to other fields. These combined features make biology uniquely suited to both benefit greatly from ML and serve as a profound wellspring of inspiration for it.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q: &lt;/strong&gt;Taking a somewhat different tack, what problems in biology are still really resistant to our current tool set? Are there areas, perhaps specific challenges in disease or in wellness, which you feel are ripe for problem-solving?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;Machine learning has demonstrated remarkable success in predictive tasks across domains such as image classification, natural language processing, and clinical risk modeling. However, in the biological sciences, predictive accuracy is often insufficient. The fundamental questions in these fields are inherently causal: How does a perturbation to a specific gene or pathway affect downstream cellular processes? What is the mechanism by which an intervention leads to a phenotypic change? Traditional machine learning models, which are primarily optimized for capturing statistical associations in observational data, often fail to answer such interventional queries.There is a strong need for biology and medicine to also inspire new foundational developments in machine learning.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The field is now equipped with high-throughput perturbation technologies — such as pooled CRISPR screens, single-cell transcriptomics, and spatial profiling — that generate rich datasets under systematic interventions. These data modalities naturally call for the development of models that go beyond pattern recognition to support causal inference, active experimental design, and representation learning in settings with complex, structured latent variables. From a mathematical perspective, this requires tackling core questions of identifiability, sample efficiency, and the integration of combinatorial, geometric, and probabilistic tools. I believe that addressing these challenges will not only unlock new insights into the mechanisms of cellular systems, but also push the theoretical boundaries of machine learning.&lt;/p&gt;&lt;p&gt;With respect to foundation models, a consensus in the field is that we are still far from creating a holistic foundation model for biology across scales, similar to what ChatGPT represents in the language domain — a sort of digital organism capable of simulating all biological phenomena. While new foundation models emerge almost weekly, these models have thus far been specialized for a specific scale and question, and focus on one or a few modalities.&lt;/p&gt;&lt;p&gt;Significant progress has been made in predicting protein structures from their sequences. This success has highlighted the importance of iterative machine learning challenges, such as CASP (critical assessment of structure prediction), which have been instrumental in benchmarking state-of-the-art algorithms for protein structure prediction and driving their improvement.&lt;/p&gt;&lt;p&gt;The Schmidt Center is organizing challenges to increase awareness in the ML field and make progress in the development of methods to solve causal prediction problems that are so critical for the biomedical sciences. With the increasing availability of single-gene perturbation data at the single-cell level,&amp;nbsp;I believe predicting the effect of single or combinatorial perturbations, and which perturbations could drive a desired phenotype, are solvable problems. With our Cell Perturbation Prediction Challenge (CPPC), we aim to provide the means to objectively test and benchmark algorithms for predicting the effect of new perturbations.&lt;/p&gt;&lt;p&gt;Another area where the field has made remarkable strides is disease diagnostic and patient triage. Machine learning algorithms can integrate different sources of patient information (data modalities), generate missing modalities, identify patterns that may be difficult for us to detect, and help stratify patients based on their disease risk. While we must remain cautious about potential biases in model predictions, the danger of models learning shortcuts instead of true correlations, and the risk of automation bias in clinical decision-making, I believe this is an area where machine learning is already having a significant impact.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q: &lt;/strong&gt;Let’s talk about some of the&amp;nbsp;headlines coming out of the Schmidt Center recently. What current research do you think people should be particularly excited about, and why?&lt;em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;In collaboration with Dr. Fei Chen at the Broad Institute, we have recently developed a method for the prediction of unseen proteins’ subcellular location, called PUPS. Many existing methods can only make predictions based on the specific protein and cell data on which they were trained. PUPS, however, combines a protein language model with an image in-painting model to utilize both protein sequences and cellular images. We demonstrate that the protein sequence input enables generalization to unseen proteins, and the cellular image input captures single-cell variability, enabling cell-type-specific predictions. The model learns how relevant each amino acid residue is for the predicted sub-cellular localization, and it can predict changes in localization due to mutations in the protein sequences. Since proteins’ function is strictly related to their subcellular localization, our predictions could provide insights into potential mechanisms of disease. In the future, we aim to extend this method to predict the localization of multiple proteins in a cell and possibly understand protein-protein interactions.&lt;/p&gt;&lt;p&gt;Together with Professor G.V. Shivashankar, a long-time collaborator at ETH Zürich, we have previously shown how simple images of cells stained with fluorescent DNA-intercalating dyes to label the chromatin can yield a lot of information about the state and fate of a cell in health and disease, when combined with machine learning algorithms. Recently, we have furthered this observation and proved the deep link between chromatin organization and gene regulation by developing Image2Reg, a method that enables the prediction of unseen genetically or chemically perturbed genes from chromatin images. Image2Reg utilizes convolutional neural networks to learn an informative representation of the chromatin images of perturbed cells. It also employs a graph convolutional network to create a gene embedding that captures the regulatory effects of genes based on protein-protein interaction data, integrated with cell-type-specific transcriptomic data. Finally, it learns a map between the resulting physical and biochemical representation of cells, allowing us to predict the perturbed gene modules based on chromatin images.&lt;/p&gt;&lt;p&gt;Furthermore, we recently finalized the development of a method for predicting the outcomes of unseen combinatorial gene perturbations and identifying the types of interactions occurring between the perturbed genes. MORPH can guide the design of the most informative perturbations for lab-in-a-loop experiments. Furthermore, the attention-based framework provably enables our method to identify causal relations among the genes, providing insights into the underlying gene regulatory programs. Finally, thanks to its modular structure, we can apply MORPH to perturbation data measured in various modalities, including not only transcriptomics, but also imaging. We are very excited about the potential of this method to enable the efficient exploration of the perturbation space to advance our understanding of cellular programs by bridging causal theory to important applications, with implications for both basic research and therapeutic applications.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/3-questions-caroline-uhler-biology-medicine-data-revolution-0902</guid><pubDate>Tue, 02 Sep 2025 21:45:00 +0000</pubDate></item></channel></rss>