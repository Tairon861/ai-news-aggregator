<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 05 Nov 2025 01:46:11 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>ClickUp adds new AI assistant to better compete with Slack and Notion (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/clickup-adds-new-ai-assistant-to-better-compete-with-slack-and-notion/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ClickUp has redesigned its productivity platform and released new AI assistant features as it aims to create a one-stop shop for customers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said core parts of this release were possible because of its acquisition of Qatalog, the enterprise search startup that had raised more than $29.5 million from backers like Salesforce Ventures, Atomico, Prototype Capital, Mosaic Ventures, Tiny VC, and Possible Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ClickUp is launching two types of AI agents with its 4.0 release. The first one is an agent that is present in all communication channels. The agent is designed to proactively look for questions people might have asked and try to answer them using knowledge stored within the company and external sources like Google Drive, OneDrive, Figma, and Gmail.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064498" height="398" src="https://techcrunch.com/wp-content/uploads/2025/11/hero_chat.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;ClickUp&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The other assistant, called Brain, is a more general-purpose assistant that can generate ideas, perform tasks like scheduling a meeting based on the availability of teammates, add a comment under a task, or create a new one. It can also access the web and other integrated tools, analyze reports, and create drafts. Just like many other AI assistants, Brain also lives in the sidebar and is accessible anywhere on the ClickUp interface.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The a16z-backed productivity company said the new release is making it easier for users to switch between tasks, docs, and communications. ClickUp 4.0 lets you look at your internal company forum timeline, switch between different communication channels, and look at your tasks through options in the sidebar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ClickUp has been trying to better compete with the likes of Notion, Slack, and Microsoft Teams by providing calendar, communication, documents, enterprise search, and task tracking under one product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has tried to make its communication have feature parity with the likes of Slack and Teams. It launched AI-powered summaries and internal live video and audio calls called SyncUps last year. Now it is placing a SyncUp button in every channel and allowing its AI notetaker to record these live video calls, transcribe them, and send notes to everyone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064499" height="398" src="https://techcrunch.com/wp-content/uploads/2025/11/hero_scheduling.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;ClickUp&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company‚Äôs calendar tool can now look at your meeting spread and automatically adjust meetings and tasks if you mark a certain task as a priority. ClickUp also displays an internet-style team dashboard where leaders can see various updates from different channels, look at team analytics on work progress, and check who has time off this week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúEight years ago, when we started, the vision and the strategy were to replace all of your work software. The strategy to do that was to build a flexible data models platform that can be used essentially for anything, and build primitives of software like a spreadsheet, a table, a document, and a task,‚Äù ClickUp CEO Zeb Evans told TechCrunch over a call. ‚ÄúIn the age of AI, they‚Äôre needed to an even greater extent because you can‚Äôt really visualize things in AI within a chat interface.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Evans said ClickUp has had great momentum in the last few years and has crossed $300 million in annual recurring revenue. He noted that with this growth rate, the company plans to go public within two years. ClickUp has raised more than $537 million in funding to date from investors like a16z, Tiger Global, Craft Ventures, and Lightspeed, according to Crunchbase data.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ClickUp has redesigned its productivity platform and released new AI assistant features as it aims to create a one-stop shop for customers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said core parts of this release were possible because of its acquisition of Qatalog, the enterprise search startup that had raised more than $29.5 million from backers like Salesforce Ventures, Atomico, Prototype Capital, Mosaic Ventures, Tiny VC, and Possible Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ClickUp is launching two types of AI agents with its 4.0 release. The first one is an agent that is present in all communication channels. The agent is designed to proactively look for questions people might have asked and try to answer them using knowledge stored within the company and external sources like Google Drive, OneDrive, Figma, and Gmail.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064498" height="398" src="https://techcrunch.com/wp-content/uploads/2025/11/hero_chat.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;ClickUp&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The other assistant, called Brain, is a more general-purpose assistant that can generate ideas, perform tasks like scheduling a meeting based on the availability of teammates, add a comment under a task, or create a new one. It can also access the web and other integrated tools, analyze reports, and create drafts. Just like many other AI assistants, Brain also lives in the sidebar and is accessible anywhere on the ClickUp interface.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The a16z-backed productivity company said the new release is making it easier for users to switch between tasks, docs, and communications. ClickUp 4.0 lets you look at your internal company forum timeline, switch between different communication channels, and look at your tasks through options in the sidebar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ClickUp has been trying to better compete with the likes of Notion, Slack, and Microsoft Teams by providing calendar, communication, documents, enterprise search, and task tracking under one product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has tried to make its communication have feature parity with the likes of Slack and Teams. It launched AI-powered summaries and internal live video and audio calls called SyncUps last year. Now it is placing a SyncUp button in every channel and allowing its AI notetaker to record these live video calls, transcribe them, and send notes to everyone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064499" height="398" src="https://techcrunch.com/wp-content/uploads/2025/11/hero_scheduling.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;ClickUp&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company‚Äôs calendar tool can now look at your meeting spread and automatically adjust meetings and tasks if you mark a certain task as a priority. ClickUp also displays an internet-style team dashboard where leaders can see various updates from different channels, look at team analytics on work progress, and check who has time off this week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúEight years ago, when we started, the vision and the strategy were to replace all of your work software. The strategy to do that was to build a flexible data models platform that can be used essentially for anything, and build primitives of software like a spreadsheet, a table, a document, and a task,‚Äù ClickUp CEO Zeb Evans told TechCrunch over a call. ‚ÄúIn the age of AI, they‚Äôre needed to an even greater extent because you can‚Äôt really visualize things in AI within a chat interface.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Evans said ClickUp has had great momentum in the last few years and has crossed $300 million in annual recurring revenue. He noted that with this growth rate, the company plans to go public within two years. ClickUp has raised more than $537 million in funding to date from investors like a16z, Tiger Global, Craft Ventures, and Lightspeed, according to Crunchbase data.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/clickup-adds-new-ai-assistant-to-better-compete-with-slack-and-notion/</guid><pubDate>Tue, 04 Nov 2025 14:00:00 +0000</pubDate></item><item><title>Alexa+ comes to the Amazon Music app (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/alexa-comes-to-the-amazon-music-app/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon announced on Tuesday that Alexa+, its upgraded assistant powered by AI, is coming to the Amazon Music app for iOS and Android devices.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is currently available across all Amazon Music subscription plans but is only available for users with Alexa+ Early Access. (To access Alexa+, tap the ‚Äúa‚Äù button in the lower right corner and ask your question using the built-in microphone on your phone.&amp;nbsp;)&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The introduction of Alexa+ to Amazon Music aims to create a more conversational approach to music discovery. Unlike the previous version of Alexa, which only responded to basic commands, Alexa+ is designed to engage in natural dialogue.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can ask specific and obscure questions, such as queries regarding an artist‚Äôs influences or exploring deeper meanings behind songs. It can also help recall a song‚Äôs title by providing lyrics you remember or even mentioning the movie it was featured in.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Alexa+ can create personalized playlists tailored to specific requests, such as ‚ÄúMake a playlist of 2010s hits that keep me moving fast, starting with a track from Nicki Minaj,‚Äù the company suggested.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some other examples the company provided include: ‚ÄúCan you recommend new music that would make me seem cool to my 13-year-old daughter without trying too hard?‚Äù and ‚ÄúWhat‚Äôs the song that plays during the opening credits of ‚ÄòThe Sopranos‚Äô?‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064840" height="495" src="https://techcrunch.com/wp-content/uploads/2025/11/alexaplusmusic.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon Music&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ was initially announced in February at the company‚Äôs hardware event and is positioned as one of the first consumer-focused agent tools on the market. This assistant can perform actions on a user‚Äôs behalf, such as booking restaurant reservations and ordering groceries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Alexa+ is still in its early stages and not yet widely available to the public, it has already rolled out to more than a million users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company reported that Amazon Music listeners who have tried the feature are exploring songs three times more with Alexa+ than with the original assistant, and those seeking recommendations are listening to nearly 70% more music.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In light of Spotify‚Äôs recent integration with ChatGPT, this could be a strategic move by Amazon to compete with the music streaming giant and showcase its own AI investments. Amazon Music has already incorporated numerous AI features into its app, with the latest additions being weekly AI-generated playlists, AI-assisted search, and ‚ÄúExplore,‚Äù which helps users learn more about their favorite artists.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon announced on Tuesday that Alexa+, its upgraded assistant powered by AI, is coming to the Amazon Music app for iOS and Android devices.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is currently available across all Amazon Music subscription plans but is only available for users with Alexa+ Early Access. (To access Alexa+, tap the ‚Äúa‚Äù button in the lower right corner and ask your question using the built-in microphone on your phone.&amp;nbsp;)&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The introduction of Alexa+ to Amazon Music aims to create a more conversational approach to music discovery. Unlike the previous version of Alexa, which only responded to basic commands, Alexa+ is designed to engage in natural dialogue.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can ask specific and obscure questions, such as queries regarding an artist‚Äôs influences or exploring deeper meanings behind songs. It can also help recall a song‚Äôs title by providing lyrics you remember or even mentioning the movie it was featured in.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Alexa+ can create personalized playlists tailored to specific requests, such as ‚ÄúMake a playlist of 2010s hits that keep me moving fast, starting with a track from Nicki Minaj,‚Äù the company suggested.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some other examples the company provided include: ‚ÄúCan you recommend new music that would make me seem cool to my 13-year-old daughter without trying too hard?‚Äù and ‚ÄúWhat‚Äôs the song that plays during the opening credits of ‚ÄòThe Sopranos‚Äô?‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064840" height="495" src="https://techcrunch.com/wp-content/uploads/2025/11/alexaplusmusic.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon Music&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ was initially announced in February at the company‚Äôs hardware event and is positioned as one of the first consumer-focused agent tools on the market. This assistant can perform actions on a user‚Äôs behalf, such as booking restaurant reservations and ordering groceries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Alexa+ is still in its early stages and not yet widely available to the public, it has already rolled out to more than a million users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company reported that Amazon Music listeners who have tried the feature are exploring songs three times more with Alexa+ than with the original assistant, and those seeking recommendations are listening to nearly 70% more music.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In light of Spotify‚Äôs recent integration with ChatGPT, this could be a strategic move by Amazon to compete with the music streaming giant and showcase its own AI investments. Amazon Music has already incorporated numerous AI features into its app, with the latest additions being weekly AI-generated playlists, AI-assisted search, and ‚ÄúExplore,‚Äù which helps users learn more about their favorite artists.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/alexa-comes-to-the-amazon-music-app/</guid><pubDate>Tue, 04 Nov 2025 14:00:00 +0000</pubDate></item><item><title>How NVIDIA GeForce RTX GPUs Power Modern Creative Workflows (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-adobe-max-creativity/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/max-2025-nv-blog-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;When inspiration strikes, nothing kills momentum faster than a slow tool or a frozen timeline. Creative apps should feel fast and fluid ‚Äî an extension of imagination that keeps up with every idea. NVIDIA RTX GPUs ‚Äî backed by the NVIDIA Studio platform ‚Äî help ideas move faster, keeping the process smooth and intuitive.&lt;/p&gt;
&lt;p&gt;GeForce RTX 50 Series GPUs are designed to accelerate creative workflows, with fifth-generation Tensor Cores engineered for demanding AI tasks, fourth-generation RT Cores for 3D rendering, and improved NVIDIA encoders and decoders for video editing and livestreaming.&lt;/p&gt;
&lt;p&gt;NVIDIA Studio is a collection of technologies to optimize content creation workflows that helps extract maximum performance from RTX hardware. This includes RTX optimizations in 135+ creative apps for higher performance, exclusive features like NVIDIA Broadcast, RTX Video and DLSS, alongside NVIDIA Studio drivers that provide more stability on a predictable cadence. Everything is engineered from the ground up to deliver the best content creation experience.&lt;/p&gt;
&lt;p&gt;At the Adobe MAX creativity conference last week, NVIDIA showcased some of the latest NVIDIA Studio optimizations in Adobe creative apps, such as the new GPU-accelerated effects in Adobe Premiere.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Attendees at the NVIDIA booth were invited to make their mark on an original music video ‚Äî customizing frames using AI features in Adobe Premiere or Photoshop. The result: a one-of-a-kind, crowdsourced music video ‚Äî professionally produced with an original soundtrack and accelerated by GeForce RTX PCs.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Read on to learn how GPU acceleration and AI enhance and speed up content creation.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Next-Generation Tools at the Service of Artists&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;A new generation of visual generative AI tools are transforming how creators work, simplifying workflows and offloading tedious tasks. Such tasks include using generative AI fill to repaint a background or generating additional pixels to fix video footage that‚Äôs incorrectly framed.&lt;/p&gt;
&lt;p&gt;These tools let individual creators attempt ambitious projects that previously could only be accomplished by large studios. Artists can quickly prototype and test multiple ideas ‚Äî a process previously too time-consuming and hence limited in scope.&lt;/p&gt;
&lt;p&gt;These new models and tools have two requirements: fast hardware to iterate on ideas quickly, and compatibility with the latest models and tools from day 0, so there‚Äôs no wait to test them. GeForce RTX 50 Series GPUs offer an ideal solution for both, as they‚Äôre the fastest hardware at running demanding AI models, and NVIDIA CUDA offers the broadest ecosystem support for tools and models.&lt;/p&gt;
&lt;p&gt;Popular AI models like Stable Diffusion 3.5 and FLUX.1 Kontext [dev] run up to 17x faster with the GeForce RTX 5090 Laptop GPU compared with the Apple M4 Max.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Cut, Color, Create&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Modern cameras have improved significantly so that even aspiring video editors are now working with high-quality, 4:2:2 4K and 8K content. Such content is rich in quality but hard to decode. Video editing apps have added a slew of AI editing tools that make adding advanced effects easier. And the speed required to publish new content has increased as platforms favor more recurrent video publishing.&lt;/p&gt;
&lt;p&gt;GeForce RTX GPUs tackle each of these issues. Their hardware decoders enable editing high-resolution 4:2:2 clips without needing to spend hours transcoding or creating proxies. GeForce RTX GPUs also accelerate AI effects with their dedicated Tensor Cores. Plus, having multiple next-generation encoders that can work in parallel brings down export video tasks from hours to minutes.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Compared with MacBook Pro laptops, GeForce RTX-equipped laptops run AI effects in apps like DaVinci Resolve up to 2x faster.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Stream Smarter&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Getting started with livestreaming can be difficult, requiring a high-performing system for gaming and encoding ‚Äî often to multiple streaming platforms; good-quality microphones and cameras; and a dedicated space with proper lighting. And learning how to stream is difficult as it requires juggling gameplay, a buzzing chat and stream production.&lt;/p&gt;
&lt;p&gt;To help make livestreaming more accessible, GeForce RTX GPUs come equipped with a dedicated hardware encoder (NVENC), which offloads video encoding from the CPU and GPU, freeing up system resources to deliver maximum gaming performance. Plus, this encoder has been highly optimized for livestreaming, providing best-in-class quality. GeForce RTX 40 and 50 Series GPUs have also added support for AV1 ‚Äî the next-generation video codec that improves compression by 40%.&lt;/p&gt;
&lt;p&gt;To help those without access to a dedicated studio or high-end devices, the NVIDIA Broadcast app applies AI effects to microphone and webcam devices, improving their quality. The app can remove background noise, add effects to cameras, relight faces with a virtual key light and process audio through an AI equalizer so it sounds like it was recorded with a professional mic.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;To help streamers reach a wider audience, NVIDIA has partnered with OBS and Twitch to make transcoding capabilities more accessible. Instead of relying on server capacity for transcode, users can generate multiple streams locally on their GPU and stream them all to Twitch. This means viewers on a phone can watch a lightweight stream that won‚Äôt stutter, while viewers on a TV or desktop can watch at the highest quality. Advanced codecs like HEVC can lead to even higher-quality streams.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Professional streamers often use teams of people to help them manage production, support and moderation. NVIDIA worked with Streamlabs to develop the Streamlabs Intelligent Streaming Agent, an AI agent that can join streams as a sidekick, manage production of scenes, audio and video cues, and even help resolve IT issues.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Create Worlds From Ideas&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;3D modelers and animators often work within massive scenes that take lots of computational power. To streamline their complex, tedious workflows, they need high-performance systems that allow them to preview their work in real time and automate tasks.&lt;/p&gt;
&lt;p&gt;NVIDIA offers a three-step approach to accelerating content creation.&lt;/p&gt;
&lt;p&gt;First, creators can use GeForce RTX GPUs, which offer the most performant solution for 3D rendering, with dedicated RT Cores that perform light calculations with ray tracing. Then, the NVIDIA Optix software development kit helps extract maximum performance out of the hardware and adds AI denoising to help images resolve faster ‚Äî all while the full rendering occurs in the background. Finally, NVIDIA DLSS technology enhances viewport performance by constructing a high-resolution frame from a lower-resolution input, in addition to using frame generation to increase frame rates.&lt;/p&gt;
&lt;p&gt;The result is hyper-fast rendering that allows the artist to preview their work in real time, navigate 3D view ports at high frame rates and accelerate exports. Plus, it unlocks real-time 3D use cases for streaming experiences like VTubing and virtual reality.&lt;/p&gt;
&lt;p&gt;3D artists are already experimenting with dozens of techniques to help automate content creation workflows ‚Äî for example, using AI to refine a texture, generate a background object or finish an animation.&lt;/p&gt;
&lt;p&gt;NVIDIA helps accelerate these workflows by optimizing the core technologies that run popular tools like ComfyUI. In addition, NVIDIA provides reference workflows like the NVIDIA AI Blueprint for 3D object generation, which showcases how these models can be chained for use cases like building a custom 3D model library for rapid prototyping.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In addition, the NVIDIA RTX Remix modding platform helps remaster classic games ‚Äî providing a toolset to ingest and enhance objects, edit levels and publish the mod. It‚Äôs built in collaboration with&amp;nbsp; a thriving community of modders creating stunning projects such as &lt;i&gt;Half Life 2 RTX&lt;/i&gt; and &lt;i&gt;Portal RTX.&lt;/i&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;#ICYMI ‚Äî The Latest Advancements in NVIDIA RTX AI PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;‚ú®&lt;b&gt;DGX Spark arrives for the world‚Äôs AI developers.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA DGX Spark delivers a petaflop of AI performance and 128GB of unified memory in a compact desktop form factor, giving developers the power to run inference on AI models with up to 200 billion parameters and fine-tune models of up to 70 billion parameters locally.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&amp;nbsp;&lt;/b&gt;ü¶ô&lt;b&gt;Ollama‚Äôs new web search API offers improved model quality on RTX.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;This new application programming interface allows users to augment local models with real-time information from the web for current and relevant responses.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&amp;nbsp;ü™ü AnythingLLM now supports Windows Foundry Local for on-device inferencing on RTX AI PCs.&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Windows Foundry Local within AnythingLLM gives users another fast inferencing solution. Foundry Local uses the NVIDIA TensorRT-RTX execution provider on NVIDIA RTX GPUs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; ‚Äî and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/max-2025-nv-blog-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;When inspiration strikes, nothing kills momentum faster than a slow tool or a frozen timeline. Creative apps should feel fast and fluid ‚Äî an extension of imagination that keeps up with every idea. NVIDIA RTX GPUs ‚Äî backed by the NVIDIA Studio platform ‚Äî help ideas move faster, keeping the process smooth and intuitive.&lt;/p&gt;
&lt;p&gt;GeForce RTX 50 Series GPUs are designed to accelerate creative workflows, with fifth-generation Tensor Cores engineered for demanding AI tasks, fourth-generation RT Cores for 3D rendering, and improved NVIDIA encoders and decoders for video editing and livestreaming.&lt;/p&gt;
&lt;p&gt;NVIDIA Studio is a collection of technologies to optimize content creation workflows that helps extract maximum performance from RTX hardware. This includes RTX optimizations in 135+ creative apps for higher performance, exclusive features like NVIDIA Broadcast, RTX Video and DLSS, alongside NVIDIA Studio drivers that provide more stability on a predictable cadence. Everything is engineered from the ground up to deliver the best content creation experience.&lt;/p&gt;
&lt;p&gt;At the Adobe MAX creativity conference last week, NVIDIA showcased some of the latest NVIDIA Studio optimizations in Adobe creative apps, such as the new GPU-accelerated effects in Adobe Premiere.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Attendees at the NVIDIA booth were invited to make their mark on an original music video ‚Äî customizing frames using AI features in Adobe Premiere or Photoshop. The result: a one-of-a-kind, crowdsourced music video ‚Äî professionally produced with an original soundtrack and accelerated by GeForce RTX PCs.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Read on to learn how GPU acceleration and AI enhance and speed up content creation.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Next-Generation Tools at the Service of Artists&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;A new generation of visual generative AI tools are transforming how creators work, simplifying workflows and offloading tedious tasks. Such tasks include using generative AI fill to repaint a background or generating additional pixels to fix video footage that‚Äôs incorrectly framed.&lt;/p&gt;
&lt;p&gt;These tools let individual creators attempt ambitious projects that previously could only be accomplished by large studios. Artists can quickly prototype and test multiple ideas ‚Äî a process previously too time-consuming and hence limited in scope.&lt;/p&gt;
&lt;p&gt;These new models and tools have two requirements: fast hardware to iterate on ideas quickly, and compatibility with the latest models and tools from day 0, so there‚Äôs no wait to test them. GeForce RTX 50 Series GPUs offer an ideal solution for both, as they‚Äôre the fastest hardware at running demanding AI models, and NVIDIA CUDA offers the broadest ecosystem support for tools and models.&lt;/p&gt;
&lt;p&gt;Popular AI models like Stable Diffusion 3.5 and FLUX.1 Kontext [dev] run up to 17x faster with the GeForce RTX 5090 Laptop GPU compared with the Apple M4 Max.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Cut, Color, Create&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Modern cameras have improved significantly so that even aspiring video editors are now working with high-quality, 4:2:2 4K and 8K content. Such content is rich in quality but hard to decode. Video editing apps have added a slew of AI editing tools that make adding advanced effects easier. And the speed required to publish new content has increased as platforms favor more recurrent video publishing.&lt;/p&gt;
&lt;p&gt;GeForce RTX GPUs tackle each of these issues. Their hardware decoders enable editing high-resolution 4:2:2 clips without needing to spend hours transcoding or creating proxies. GeForce RTX GPUs also accelerate AI effects with their dedicated Tensor Cores. Plus, having multiple next-generation encoders that can work in parallel brings down export video tasks from hours to minutes.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Compared with MacBook Pro laptops, GeForce RTX-equipped laptops run AI effects in apps like DaVinci Resolve up to 2x faster.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Stream Smarter&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Getting started with livestreaming can be difficult, requiring a high-performing system for gaming and encoding ‚Äî often to multiple streaming platforms; good-quality microphones and cameras; and a dedicated space with proper lighting. And learning how to stream is difficult as it requires juggling gameplay, a buzzing chat and stream production.&lt;/p&gt;
&lt;p&gt;To help make livestreaming more accessible, GeForce RTX GPUs come equipped with a dedicated hardware encoder (NVENC), which offloads video encoding from the CPU and GPU, freeing up system resources to deliver maximum gaming performance. Plus, this encoder has been highly optimized for livestreaming, providing best-in-class quality. GeForce RTX 40 and 50 Series GPUs have also added support for AV1 ‚Äî the next-generation video codec that improves compression by 40%.&lt;/p&gt;
&lt;p&gt;To help those without access to a dedicated studio or high-end devices, the NVIDIA Broadcast app applies AI effects to microphone and webcam devices, improving their quality. The app can remove background noise, add effects to cameras, relight faces with a virtual key light and process audio through an AI equalizer so it sounds like it was recorded with a professional mic.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;To help streamers reach a wider audience, NVIDIA has partnered with OBS and Twitch to make transcoding capabilities more accessible. Instead of relying on server capacity for transcode, users can generate multiple streams locally on their GPU and stream them all to Twitch. This means viewers on a phone can watch a lightweight stream that won‚Äôt stutter, while viewers on a TV or desktop can watch at the highest quality. Advanced codecs like HEVC can lead to even higher-quality streams.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Professional streamers often use teams of people to help them manage production, support and moderation. NVIDIA worked with Streamlabs to develop the Streamlabs Intelligent Streaming Agent, an AI agent that can join streams as a sidekick, manage production of scenes, audio and video cues, and even help resolve IT issues.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Create Worlds From Ideas&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;3D modelers and animators often work within massive scenes that take lots of computational power. To streamline their complex, tedious workflows, they need high-performance systems that allow them to preview their work in real time and automate tasks.&lt;/p&gt;
&lt;p&gt;NVIDIA offers a three-step approach to accelerating content creation.&lt;/p&gt;
&lt;p&gt;First, creators can use GeForce RTX GPUs, which offer the most performant solution for 3D rendering, with dedicated RT Cores that perform light calculations with ray tracing. Then, the NVIDIA Optix software development kit helps extract maximum performance out of the hardware and adds AI denoising to help images resolve faster ‚Äî all while the full rendering occurs in the background. Finally, NVIDIA DLSS technology enhances viewport performance by constructing a high-resolution frame from a lower-resolution input, in addition to using frame generation to increase frame rates.&lt;/p&gt;
&lt;p&gt;The result is hyper-fast rendering that allows the artist to preview their work in real time, navigate 3D view ports at high frame rates and accelerate exports. Plus, it unlocks real-time 3D use cases for streaming experiences like VTubing and virtual reality.&lt;/p&gt;
&lt;p&gt;3D artists are already experimenting with dozens of techniques to help automate content creation workflows ‚Äî for example, using AI to refine a texture, generate a background object or finish an animation.&lt;/p&gt;
&lt;p&gt;NVIDIA helps accelerate these workflows by optimizing the core technologies that run popular tools like ComfyUI. In addition, NVIDIA provides reference workflows like the NVIDIA AI Blueprint for 3D object generation, which showcases how these models can be chained for use cases like building a custom 3D model library for rapid prototyping.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In addition, the NVIDIA RTX Remix modding platform helps remaster classic games ‚Äî providing a toolset to ingest and enhance objects, edit levels and publish the mod. It‚Äôs built in collaboration with&amp;nbsp; a thriving community of modders creating stunning projects such as &lt;i&gt;Half Life 2 RTX&lt;/i&gt; and &lt;i&gt;Portal RTX.&lt;/i&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;#ICYMI ‚Äî The Latest Advancements in NVIDIA RTX AI PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;‚ú®&lt;b&gt;DGX Spark arrives for the world‚Äôs AI developers.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA DGX Spark delivers a petaflop of AI performance and 128GB of unified memory in a compact desktop form factor, giving developers the power to run inference on AI models with up to 200 billion parameters and fine-tune models of up to 70 billion parameters locally.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&amp;nbsp;&lt;/b&gt;ü¶ô&lt;b&gt;Ollama‚Äôs new web search API offers improved model quality on RTX.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;This new application programming interface allows users to augment local models with real-time information from the web for current and relevant responses.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;&amp;nbsp;ü™ü AnythingLLM now supports Windows Foundry Local for on-device inferencing on RTX AI PCs.&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Windows Foundry Local within AnythingLLM gives users another fast inferencing solution. Foundry Local uses the NVIDIA TensorRT-RTX execution provider on NVIDIA RTX GPUs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; ‚Äî and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-adobe-max-creativity/</guid><pubDate>Tue, 04 Nov 2025 14:00:26 +0000</pubDate></item><item><title>Flawed AI benchmarks put enterprise budgets at risk (AI News)</title><link>https://www.artificialintelligence-news.com/news/flawed-ai-benchmarks-enterprise-budgets-at-risk/</link><description>&lt;p&gt;A new academic review suggests AI benchmarks are flawed, potentially leading an enterprise to make high-stakes decisions on ‚Äúmisleading‚Äù data.&lt;/p&gt;&lt;p&gt;Enterprise leaders are committing budgets of eight or nine figures to generative AI programmes. These procurement and development decisions often rely on public leaderboards and benchmarks to compare model capabilities.&lt;/p&gt;&lt;p&gt;A large-scale study, ‚ÄòMeasuring what Matters: Construct Validity in Large Language Model Benchmarks,‚Äô analysed 445 separate LLM benchmarks from leading AI conferences. A team of 29 expert reviewers found that ‚Äúalmost all articles have weaknesses in at least one area,‚Äù undermining the claims they make about model performance.&lt;/p&gt;&lt;p&gt;For CTOs and Chief Data Officers, it strikes at the heart of AI governance and investment strategy. If a benchmark claiming to measure ‚Äòsafety‚Äô or ‚Äòrobustness‚Äô doesn‚Äôt actually capture those qualities, an organisation could deploy a model that exposes it to serious financial and reputational risk.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-construct-validity-problem"&gt;The ‚Äòconstruct validity‚Äô problem&lt;/h3&gt;&lt;p&gt;The researchers focused on a core scientific principle known as construct validity. In simple terms, this is the degree to which a test measures the abstract concept it claims to be measuring.&lt;/p&gt;&lt;p&gt;For example, while ‚Äòintelligence‚Äô cannot be measured directly, tests are created to serve as measurable proxies. The paper notes that if a benchmark has low construct validity, ‚Äúthen a high score may be irrelevant or even misleading‚Äù.&lt;/p&gt;&lt;p&gt;This problem is widespread in AI evaluation. The study found that key concepts are often ‚Äúpoorly defined or operationalised‚Äù. This can lead to ‚Äúpoorly supported scientific claims, misdirected research, and policy implications that are not grounded in robust evidence‚Äù.&lt;/p&gt;&lt;p&gt;When vendors compete for enterprise contracts by highlighting their top scores on benchmarks, leaders are effectively trusting that these scores are a reliable proxy for real-world business performance. This new research suggests that trust may be misplaced.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-where-the-enterprise-ai-benchmarks-are-failing"&gt;Where the enterprise AI benchmarks are failing&lt;/h3&gt;&lt;p&gt;The review identified systemic failings across the board, from how benchmarks are designed to how their results are reported.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Vague or contested definitions:&lt;/strong&gt; You cannot measure what you cannot define. The study found that even when definitions for a phenomenon were provided, 47.8 percent were ‚Äúcontested,‚Äù addressing concepts with ‚Äúmany possible definitions or no clear definition at all‚Äù.&lt;/p&gt;&lt;p&gt;The paper uses ‚Äòharmlessness‚Äô ‚Äì a key goal in enterprise safety alignment ‚Äì as an example of a phenomenon that often lacks a clear, agreed-upon definition. If two vendors score differently on a ‚Äòharmlessness‚Äô benchmark, it may only reflect two different, arbitrary definitions of the term, not a genuine difference in model safety.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Lack of statistical rigour:&lt;/strong&gt; Perhaps most alarming for data-driven organisations, the review found that only 16 percent of the 445 benchmarks used uncertainty estimates or statistical tests to compare model results.&lt;/p&gt;&lt;p&gt;Without statistical analysis, it‚Äôs impossible to know if a 2 percent lead for Model A over Model B is a genuine capability difference or simple random chance. Enterprise decisions are being guided by numbers that would not pass a basic scientific or business intelligence review.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Data contamination and memorisation:&lt;/strong&gt; Many benchmarks, especially those for reasoning (like the widely used GSM8K), are undermined when their questions and answers appear in the model‚Äôs pre-training data.&lt;/p&gt;&lt;p&gt;When this happens, the model isn‚Äôt reasoning to find the answer; it‚Äôs simply memorising it. A high score may indicate a good memory, not the advanced reasoning capability an enterprise actually needs for a complex task. The paper warns this ‚Äúundermine[s] the validity of the results‚Äù and recommends building contamination checks directly into the benchmark.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Unrepresentative datasets:&lt;/strong&gt; The study found that 27 percent of benchmarks used ‚Äúconvenience sampling,‚Äù such as reusing data from existing benchmarks or human exams. This data is often not representative of the real-world phenomenon.&lt;/p&gt;&lt;p&gt;For example, the authors note that reusing questions from a ‚Äúcalculator-free exam‚Äù means the problems use numbers chosen to be easy for basic arithmetic. A model might score well on this test, but this score ‚Äúwould not predict performance on larger numbers, where LLMs struggle‚Äù. This creates a critical blind spot, hiding a known model weakness.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-public-metrics-to-internal-validation"&gt;From public metrics to internal validation&lt;/h3&gt;&lt;p&gt;For enterprise leaders, the study serves as a strong warning: public AI benchmarks are not a substitute for internal and domain-specific evaluation. A high score on a public leaderboard is not a guarantee of fitness for a specific business purpose.&lt;/p&gt;&lt;p&gt;Isabella Grandi, Director for Data Strategy &amp;amp; Governance, at NTT DATA UK&amp;amp;I, commented: ‚ÄúA single benchmark might not be the right way to capture the complexity of AI systems, and expecting it to do so risks reducing progress to a numbers game rather than a measure of real-world responsibility. What matters most is consistent evaluation against clear principles that ensure technology serves people as well as progress.&lt;/p&gt;&lt;p&gt;‚ÄúGood methodology ‚Äì as laid out by ISO/IEC 42001:2023 ‚Äì reflects this balance through five core principles: accountability, fairness, transparency, security and redress. Accountability establishes ownership and responsibility for any AI system that is deployed. Transparency and fairness guide decisions toward outcomes that are ethical and explainable. Security and privacy are non-negotiable, preventing misuse and reinforcing public trust. Redress and contestability provide a vital mechanism for oversight, ensuring people can challenge and correct outcomes when necessary.&lt;/p&gt;&lt;p&gt;‚ÄúReal progress in AI depends on collaboration that brings together the vision of government, the curiosity of academia and the practical drive of industry. When partnerships are underpinned by open dialogue and shared standards take hold, it builds the transparency needed for people to instil trust in AI systems. Responsible innovation will always rely on cooperation that strengthens oversight while keeping ambition alive.‚Äù&lt;/p&gt;&lt;p&gt;The paper‚Äôs eight recommendations provide a practical checklist for any enterprise looking to build its own internal AI benchmarks and evaluations, aligning with the principles-based approach.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Define your phenomenon:&lt;/strong&gt; Before testing models, organisations must first create a ‚Äúprecise and operational definition for the phenomenon being measured‚Äù. What does a ‚Äòhelpful‚Äô response mean in the context of your customer service? What does ‚Äòaccurate‚Äô mean for your financial reports?&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Build a representative dataset:&lt;/strong&gt; The most valuable benchmark is one built from your own data. The paper urges developers to ‚Äúconstruct a representative dataset for the task‚Äù. This means using task items that reflect the real-world scenarios, formats, and challenges your employees and customers face.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Conduct error analysis:&lt;/strong&gt; Go beyond the final score. The report recommends teams ‚Äúconduct a qualitative and quantitative analysis of common failure modes‚Äù. Analysing why a model fails is more instructive than just knowing its score. If its failures are all on low-priority, obscure topics, it may be acceptable; if it fails on your most common and high-value use cases, that single score becomes irrelevant.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Justify validity:&lt;/strong&gt; Finally, teams must ‚Äújustify the relevance of the benchmark for the phenomenon with real-world applications‚Äù. Every evaluation should come with a clear rationale explaining why this specific test is a valid proxy for business value.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The race to deploy generative AI is pushing organisations to move faster than their governance frameworks can keep up. This report shows that the very tools used to measure progress are often flawed. The only reliable path forward is to stop trusting generic AI benchmarks and start ‚Äúmeasuring what matters‚Äù for your own enterprise.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI spreads $600B cloud AI bet across AWS, Oracle, Microsoft&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;A new academic review suggests AI benchmarks are flawed, potentially leading an enterprise to make high-stakes decisions on ‚Äúmisleading‚Äù data.&lt;/p&gt;&lt;p&gt;Enterprise leaders are committing budgets of eight or nine figures to generative AI programmes. These procurement and development decisions often rely on public leaderboards and benchmarks to compare model capabilities.&lt;/p&gt;&lt;p&gt;A large-scale study, ‚ÄòMeasuring what Matters: Construct Validity in Large Language Model Benchmarks,‚Äô analysed 445 separate LLM benchmarks from leading AI conferences. A team of 29 expert reviewers found that ‚Äúalmost all articles have weaknesses in at least one area,‚Äù undermining the claims they make about model performance.&lt;/p&gt;&lt;p&gt;For CTOs and Chief Data Officers, it strikes at the heart of AI governance and investment strategy. If a benchmark claiming to measure ‚Äòsafety‚Äô or ‚Äòrobustness‚Äô doesn‚Äôt actually capture those qualities, an organisation could deploy a model that exposes it to serious financial and reputational risk.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-construct-validity-problem"&gt;The ‚Äòconstruct validity‚Äô problem&lt;/h3&gt;&lt;p&gt;The researchers focused on a core scientific principle known as construct validity. In simple terms, this is the degree to which a test measures the abstract concept it claims to be measuring.&lt;/p&gt;&lt;p&gt;For example, while ‚Äòintelligence‚Äô cannot be measured directly, tests are created to serve as measurable proxies. The paper notes that if a benchmark has low construct validity, ‚Äúthen a high score may be irrelevant or even misleading‚Äù.&lt;/p&gt;&lt;p&gt;This problem is widespread in AI evaluation. The study found that key concepts are often ‚Äúpoorly defined or operationalised‚Äù. This can lead to ‚Äúpoorly supported scientific claims, misdirected research, and policy implications that are not grounded in robust evidence‚Äù.&lt;/p&gt;&lt;p&gt;When vendors compete for enterprise contracts by highlighting their top scores on benchmarks, leaders are effectively trusting that these scores are a reliable proxy for real-world business performance. This new research suggests that trust may be misplaced.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-where-the-enterprise-ai-benchmarks-are-failing"&gt;Where the enterprise AI benchmarks are failing&lt;/h3&gt;&lt;p&gt;The review identified systemic failings across the board, from how benchmarks are designed to how their results are reported.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Vague or contested definitions:&lt;/strong&gt; You cannot measure what you cannot define. The study found that even when definitions for a phenomenon were provided, 47.8 percent were ‚Äúcontested,‚Äù addressing concepts with ‚Äúmany possible definitions or no clear definition at all‚Äù.&lt;/p&gt;&lt;p&gt;The paper uses ‚Äòharmlessness‚Äô ‚Äì a key goal in enterprise safety alignment ‚Äì as an example of a phenomenon that often lacks a clear, agreed-upon definition. If two vendors score differently on a ‚Äòharmlessness‚Äô benchmark, it may only reflect two different, arbitrary definitions of the term, not a genuine difference in model safety.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Lack of statistical rigour:&lt;/strong&gt; Perhaps most alarming for data-driven organisations, the review found that only 16 percent of the 445 benchmarks used uncertainty estimates or statistical tests to compare model results.&lt;/p&gt;&lt;p&gt;Without statistical analysis, it‚Äôs impossible to know if a 2 percent lead for Model A over Model B is a genuine capability difference or simple random chance. Enterprise decisions are being guided by numbers that would not pass a basic scientific or business intelligence review.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Data contamination and memorisation:&lt;/strong&gt; Many benchmarks, especially those for reasoning (like the widely used GSM8K), are undermined when their questions and answers appear in the model‚Äôs pre-training data.&lt;/p&gt;&lt;p&gt;When this happens, the model isn‚Äôt reasoning to find the answer; it‚Äôs simply memorising it. A high score may indicate a good memory, not the advanced reasoning capability an enterprise actually needs for a complex task. The paper warns this ‚Äúundermine[s] the validity of the results‚Äù and recommends building contamination checks directly into the benchmark.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Unrepresentative datasets:&lt;/strong&gt; The study found that 27 percent of benchmarks used ‚Äúconvenience sampling,‚Äù such as reusing data from existing benchmarks or human exams. This data is often not representative of the real-world phenomenon.&lt;/p&gt;&lt;p&gt;For example, the authors note that reusing questions from a ‚Äúcalculator-free exam‚Äù means the problems use numbers chosen to be easy for basic arithmetic. A model might score well on this test, but this score ‚Äúwould not predict performance on larger numbers, where LLMs struggle‚Äù. This creates a critical blind spot, hiding a known model weakness.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-public-metrics-to-internal-validation"&gt;From public metrics to internal validation&lt;/h3&gt;&lt;p&gt;For enterprise leaders, the study serves as a strong warning: public AI benchmarks are not a substitute for internal and domain-specific evaluation. A high score on a public leaderboard is not a guarantee of fitness for a specific business purpose.&lt;/p&gt;&lt;p&gt;Isabella Grandi, Director for Data Strategy &amp;amp; Governance, at NTT DATA UK&amp;amp;I, commented: ‚ÄúA single benchmark might not be the right way to capture the complexity of AI systems, and expecting it to do so risks reducing progress to a numbers game rather than a measure of real-world responsibility. What matters most is consistent evaluation against clear principles that ensure technology serves people as well as progress.&lt;/p&gt;&lt;p&gt;‚ÄúGood methodology ‚Äì as laid out by ISO/IEC 42001:2023 ‚Äì reflects this balance through five core principles: accountability, fairness, transparency, security and redress. Accountability establishes ownership and responsibility for any AI system that is deployed. Transparency and fairness guide decisions toward outcomes that are ethical and explainable. Security and privacy are non-negotiable, preventing misuse and reinforcing public trust. Redress and contestability provide a vital mechanism for oversight, ensuring people can challenge and correct outcomes when necessary.&lt;/p&gt;&lt;p&gt;‚ÄúReal progress in AI depends on collaboration that brings together the vision of government, the curiosity of academia and the practical drive of industry. When partnerships are underpinned by open dialogue and shared standards take hold, it builds the transparency needed for people to instil trust in AI systems. Responsible innovation will always rely on cooperation that strengthens oversight while keeping ambition alive.‚Äù&lt;/p&gt;&lt;p&gt;The paper‚Äôs eight recommendations provide a practical checklist for any enterprise looking to build its own internal AI benchmarks and evaluations, aligning with the principles-based approach.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Define your phenomenon:&lt;/strong&gt; Before testing models, organisations must first create a ‚Äúprecise and operational definition for the phenomenon being measured‚Äù. What does a ‚Äòhelpful‚Äô response mean in the context of your customer service? What does ‚Äòaccurate‚Äô mean for your financial reports?&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Build a representative dataset:&lt;/strong&gt; The most valuable benchmark is one built from your own data. The paper urges developers to ‚Äúconstruct a representative dataset for the task‚Äù. This means using task items that reflect the real-world scenarios, formats, and challenges your employees and customers face.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Conduct error analysis:&lt;/strong&gt; Go beyond the final score. The report recommends teams ‚Äúconduct a qualitative and quantitative analysis of common failure modes‚Äù. Analysing why a model fails is more instructive than just knowing its score. If its failures are all on low-priority, obscure topics, it may be acceptable; if it fails on your most common and high-value use cases, that single score becomes irrelevant.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Justify validity:&lt;/strong&gt; Finally, teams must ‚Äújustify the relevance of the benchmark for the phenomenon with real-world applications‚Äù. Every evaluation should come with a clear rationale explaining why this specific test is a valid proxy for business value.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The race to deploy generative AI is pushing organisations to move faster than their governance frameworks can keep up. This report shows that the very tools used to measure progress are often flawed. The only reliable path forward is to stop trusting generic AI benchmarks and start ‚Äúmeasuring what matters‚Äù for your own enterprise.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI spreads $600B cloud AI bet across AWS, Oracle, Microsoft&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/flawed-ai-benchmarks-enterprise-budgets-at-risk/</guid><pubDate>Tue, 04 Nov 2025 14:04:00 +0000</pubDate></item><item><title>Nvidia, Deutsche Telekom strike ‚Ç¨1B partnership for a data center in Munich (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/nvidia-deutsche-telekom-strike-e1b-partnership-for-a-data-center-in-munich/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/bi-251104-industrial-ai-cloud-02-en.jpg?w=1008" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is playing fast and loose with its war chest as it looks to build on its momentum as the chief benefactor of the AI boom. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company on Tuesday signed a ‚Ç¨1 billion ($1.15 billion) partnership with Deutsche Telekom to set up an ‚ÄúAI factory‚Äù in Munich that aims to boost Germany‚Äôs AI computing power by 50%.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Called the ‚ÄúIndustrial AI Cloud,‚Äù the project will use more than 1,000 Nvidia DGX B200 systems and RTX Pro Servers with up to 10,000 Blackwell GPUs to provide AI inferencing and other services to German companies while complying with German data sovereignty laws.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deutsche Telekom said early partners of the project include Agile Robots, whose bots will be used to install server racks at the facility, and Perplexity, which will use the data center to provide ‚Äúin-country‚Äù AI inferencing to German users and companies. The telco also outlined digital twins and physics-based simulation as use cases for industrial companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The telecom company said it would provide the physical infrastructure for the project, while SAP will provide its business technology platform and applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership comes at a time when the European tech industry has been calling on EU lawmakers to reduce their reliance on foreign infrastructure and service providers and to foster adoption of homegrown alternatives. At the same time, tech companies have been criticizing the bloc‚Äôs approach to regulating AI, arguing that the rules only serve to hold back innovation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EU earlier this year committed ‚Ç¨200 billion to set up ‚ÄúAI gigafactories‚Äù on the continent, focusing on ‚Äúindustrial and mission-critical applications.‚Äù But funding for AI initiatives in the European Union has been notably lower than in the U.S., where companies like Nvidia, Microsoft, Google, and Oracle have pumped in hundreds of billions to build massive data centers and assorted infrastructure to support development of AI models and services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deutsche Telekom noted that this project, expected to start operations in early 2026, is separate from the EU‚Äôs AI gigafactory initiative.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúMechanical engineering and industry have made this country strong,‚Äù says Tim H√∂ttges, CEO of Deutsche Telekom. ‚ÄúBut here, too, we are challenged. AI is a huge opportunity. It will help to improve our products and strengthen our European strengths.‚Äù&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/bi-251104-industrial-ai-cloud-02-en.jpg?w=1008" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is playing fast and loose with its war chest as it looks to build on its momentum as the chief benefactor of the AI boom. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company on Tuesday signed a ‚Ç¨1 billion ($1.15 billion) partnership with Deutsche Telekom to set up an ‚ÄúAI factory‚Äù in Munich that aims to boost Germany‚Äôs AI computing power by 50%.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Called the ‚ÄúIndustrial AI Cloud,‚Äù the project will use more than 1,000 Nvidia DGX B200 systems and RTX Pro Servers with up to 10,000 Blackwell GPUs to provide AI inferencing and other services to German companies while complying with German data sovereignty laws.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deutsche Telekom said early partners of the project include Agile Robots, whose bots will be used to install server racks at the facility, and Perplexity, which will use the data center to provide ‚Äúin-country‚Äù AI inferencing to German users and companies. The telco also outlined digital twins and physics-based simulation as use cases for industrial companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The telecom company said it would provide the physical infrastructure for the project, while SAP will provide its business technology platform and applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership comes at a time when the European tech industry has been calling on EU lawmakers to reduce their reliance on foreign infrastructure and service providers and to foster adoption of homegrown alternatives. At the same time, tech companies have been criticizing the bloc‚Äôs approach to regulating AI, arguing that the rules only serve to hold back innovation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EU earlier this year committed ‚Ç¨200 billion to set up ‚ÄúAI gigafactories‚Äù on the continent, focusing on ‚Äúindustrial and mission-critical applications.‚Äù But funding for AI initiatives in the European Union has been notably lower than in the U.S., where companies like Nvidia, Microsoft, Google, and Oracle have pumped in hundreds of billions to build massive data centers and assorted infrastructure to support development of AI models and services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deutsche Telekom noted that this project, expected to start operations in early 2026, is separate from the EU‚Äôs AI gigafactory initiative.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúMechanical engineering and industry have made this country strong,‚Äù says Tim H√∂ttges, CEO of Deutsche Telekom. ‚ÄúBut here, too, we are challenged. AI is a huge opportunity. It will help to improve our products and strengthen our European strengths.‚Äù&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/nvidia-deutsche-telekom-strike-e1b-partnership-for-a-data-center-in-munich/</guid><pubDate>Tue, 04 Nov 2025 14:19:06 +0000</pubDate></item><item><title>Why the for-profit race into solar geoengineering is bad for science and public trust (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/04/1127532/why-the-for-profit-race-into-solar-geoengineering-is-bad-for-science-and-public-trust/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/GettyImages-2222761279.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Last week, an American-Israeli company that claims it‚Äôs developed proprietary technology to cool the planet announced it had raised $60 million, by far the largest known venture capital round to date for a solar geoengineering startup.&lt;/p&gt;  &lt;p&gt;The company, Stardust, says the funding will enable it to develop a system that could be deployed by the start of the next decade, according to &lt;em&gt;Heatmap&lt;/em&gt;, which broke the story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;h3 class="wp-block-heading"&gt;Heat Exchange&lt;br /&gt;&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;MIT Technology Review‚Äôs guest opinion series, offering expert commentary on legal, political and regulatory issues related to climate change and clean energy. You can read the rest of the pieces &lt;/em&gt;&lt;em&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;As scientists who have worked on the science of solar geoengineering for decades, we have grown increasingly concerned about the emerging efforts to start and fund private companies to build and deploy technologies that could alter the climate of the planet. We also strongly dispute some of the technical claims that certain companies have made about their offerings.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Given the potential power of such tools, the public concerns about them, and the importance of using them responsibly, we argue that they should be studied, evaluated, and developed mainly through publicly coordinated and transparently funded science and engineering efforts.&amp;nbsp; In addition, any decisions about whether or how they should be used should be made through multilateral government discussions, informed by the best available research on the promise and risks of such interventions‚Äînot the profit motives of companies or their investors.&lt;/p&gt;  &lt;p&gt;The basic idea behind solar geoengineering, or what we now prefer to call sunlight reflection methods (SRM), is that humans might reduce climate change by making the Earth a bit more reflective, partially counteracting the warming caused by the accumulation of greenhouse gases.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;There is strong evidence, based on years of climate modeling and analyses by researchers worldwide, that SRM‚Äîwhile not perfect‚Äîcould significantly and rapidly reduce climate changes and avoid important climate risks. In particular, it could ease the impacts in hot countries that are struggling to adapt.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The goals of doing research into SRM can be diverse: identifying risks as well as finding better methods. But research won‚Äôt be useful unless it‚Äôs trusted, and trust depends on transparency. That means researchers must be eager to examine pros and cons, committed to following the evidence where it leads, and driven by a sense that research should serve public interests, not be locked up as intellectual property.&lt;/p&gt;  &lt;p&gt;In recent years, a handful of for-profit startup companies have emerged that are striving to develop SRM technologies or already trying to market SRM services. That includes Make Sunsets, which sells ‚Äúcooling credits‚Äù for releasing sulfur dioxide in the stratosphere. A new company, Sunscreen, which hasn‚Äôt yet been announced, intends to use aerosols in the lower atmosphere to achieve cooling over small areas, purportedly to help farmers or cities deal with extreme heat.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Our strong impression is that people in these companies are driven by the same concerns about climate change that move us in our research. We agree that more research, and more innovation, is needed. However, we do not think startups‚Äîwhich by definition must eventually make money to stay in business‚Äîcan play a productive role in advancing research on SRM.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Many people already distrust the idea of engineering the atmosphere‚Äîat whichever scale‚Äîto address climate change, fearing negative side effects, inequitable impacts on different parts of the world, or the prospect that a world expecting such solutions will feel less pressure to address the root causes of climate change.&lt;/p&gt;  &lt;p&gt;Adding business interests, profit motives, and rich investors into this situation just creates more cause for concern, complicating the ability of responsible scientists and engineers to carry out the work needed to advance our understanding.&lt;/p&gt;  &lt;p&gt;The only way these startups will make money is if someone pays for their services, so there‚Äôs a reasonable fear that financial pressures could drive companies to lobby governments or other parties to use such tools. A decision that should be based on objective analysis of risks and benefits would instead be strongly influenced by financial interests and political connections.&lt;/p&gt;  &lt;p&gt;The need to raise money or bring in revenue often drives companies to hype the potential or safety of their tools. Indeed, that‚Äôs what private companies need to do to attract investors, but it‚Äôs not how you build public trust‚Äîparticularly when the science doesn‚Äôt support the claims.&lt;/p&gt; 

 &lt;p&gt;Notably, Stardust says on its website that it has developed novel particles that can be injected into the atmosphere to reflect away more sunlight, asserting that they‚Äôre ‚Äúchemically inert in the stratosphere, and safe for humans and ecosystems.‚Äù According to the company, ‚ÄúThe particles naturally return to Earth‚Äôs surface over time and recycle safely back into the biosphere.‚Äù&lt;/p&gt;  &lt;p&gt;But it‚Äôs nonsense for the company to claim they can make particles that are inert in the stratosphere. Even diamonds, which are extraordinarily nonreactive, would alter stratospheric chemistry. First of all, much of that chemistry depends on highly reactive radicals that react with any solid surface, and second, any particle may become coated by background sulfuric acid in the stratosphere. That could accelerate the loss of the protective ozone layer by spreading that existing sulfuric acid over a larger surface area.&lt;/p&gt;  &lt;p&gt;(Stardust didn't provide a response to an inquiry about the concerns raised in this piece.)&lt;/p&gt;  &lt;p&gt;In materials presented to potential investors, which we‚Äôve obtained a copy of, Stardust further claims its particles ‚Äúimprove‚Äù on sulfuric acid, which is the most studied material for SRM. But the point of using sulfate for such studies was never that it was perfect, but that its broader climatic and environmental impacts are well understood. That‚Äôs because sulfate is widespread on Earth, and there‚Äôs an immense body of scientific knowledge about the fate and risks of sulfur that reaches the stratosphere through volcanic eruptions or other means.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;If there‚Äôs one great lesson of 20th-century environmental science, it‚Äôs how crucial it is to &lt;em&gt;understand the ultimate fate of any new material introduced into the environment.&lt;/em&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Chlorofluorocarbons and the pesticide DDT both offered safety advantages over competing technologies, but they both broke down into products that accumulated in the environment in unexpected places, causing enormous and unanticipated harms.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The environmental and climate impacts of sulfate aerosols have been studied in many thousands of scientific papers over a century, and this deep well of knowledge greatly reduces the chance of unknown unknowns.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Grandiose claims notwithstanding‚Äîand especially considering that Stardust hasn‚Äôt disclosed anything about its particles or research process‚Äîit would be very difficult to make a pragmatic, risk-informed decision to start SRM efforts with these particles instead of sulfate.&lt;/p&gt; 
 &lt;p&gt;We don‚Äôt want to claim that every single answer lies in academia. We‚Äôd be fools to not be excited by profit-driven innovation in solar power, EVs, batteries, or other sustainable technologies. But the math for sunlight reflection is just different. Why?&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Because the role of private industry was essential in improving the efficiency, driving down the costs, and increasing the market share of renewables and other forms of cleantech. When cost matters and we can easily evaluate the benefits of the product, then competitive, for-profit capitalism can work wonders.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But SRM is already technically feasible and inexpensive, with deployment costs that are negligible compared with the climate damage it averts.&lt;/p&gt;  &lt;p&gt;The essential questions of whether or how to use it come down to far thornier societal issues: How can we best balance the risks and benefits? How can we ensure that it‚Äôs used in an equitable way? How do we make legitimate decisions about SRM on a planet with such sharp political divisions?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;Trust will be the most important single ingredient in making these decisions. And trust is the one product for-profit innovation does not naturally manufacture.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Ultimately, we‚Äôre just two researchers. We can‚Äôt make investors in these startups do anything differently. Our request is that they think carefully, and beyond the logic of short-term profit. If they believe geoengineering is worth exploring, could it be that their support will make it harder, not easier, to do that?&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;David Keith is the professor of geophysical sciences at the University of Chicago and founding faculty director of the school‚Äôs Climate Systems Engineering Initiative. Daniele Visioni is an assistant professor of earth and atmospheric sciences at Cornell University and head of data for Reflective, a nonprofit that develops tools and provides funding to support solar geoengineering research.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/GettyImages-2222761279.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Last week, an American-Israeli company that claims it‚Äôs developed proprietary technology to cool the planet announced it had raised $60 million, by far the largest known venture capital round to date for a solar geoengineering startup.&lt;/p&gt;  &lt;p&gt;The company, Stardust, says the funding will enable it to develop a system that could be deployed by the start of the next decade, according to &lt;em&gt;Heatmap&lt;/em&gt;, which broke the story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;h3 class="wp-block-heading"&gt;Heat Exchange&lt;br /&gt;&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;MIT Technology Review‚Äôs guest opinion series, offering expert commentary on legal, political and regulatory issues related to climate change and clean energy. You can read the rest of the pieces &lt;/em&gt;&lt;em&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;As scientists who have worked on the science of solar geoengineering for decades, we have grown increasingly concerned about the emerging efforts to start and fund private companies to build and deploy technologies that could alter the climate of the planet. We also strongly dispute some of the technical claims that certain companies have made about their offerings.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Given the potential power of such tools, the public concerns about them, and the importance of using them responsibly, we argue that they should be studied, evaluated, and developed mainly through publicly coordinated and transparently funded science and engineering efforts.&amp;nbsp; In addition, any decisions about whether or how they should be used should be made through multilateral government discussions, informed by the best available research on the promise and risks of such interventions‚Äînot the profit motives of companies or their investors.&lt;/p&gt;  &lt;p&gt;The basic idea behind solar geoengineering, or what we now prefer to call sunlight reflection methods (SRM), is that humans might reduce climate change by making the Earth a bit more reflective, partially counteracting the warming caused by the accumulation of greenhouse gases.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;There is strong evidence, based on years of climate modeling and analyses by researchers worldwide, that SRM‚Äîwhile not perfect‚Äîcould significantly and rapidly reduce climate changes and avoid important climate risks. In particular, it could ease the impacts in hot countries that are struggling to adapt.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The goals of doing research into SRM can be diverse: identifying risks as well as finding better methods. But research won‚Äôt be useful unless it‚Äôs trusted, and trust depends on transparency. That means researchers must be eager to examine pros and cons, committed to following the evidence where it leads, and driven by a sense that research should serve public interests, not be locked up as intellectual property.&lt;/p&gt;  &lt;p&gt;In recent years, a handful of for-profit startup companies have emerged that are striving to develop SRM technologies or already trying to market SRM services. That includes Make Sunsets, which sells ‚Äúcooling credits‚Äù for releasing sulfur dioxide in the stratosphere. A new company, Sunscreen, which hasn‚Äôt yet been announced, intends to use aerosols in the lower atmosphere to achieve cooling over small areas, purportedly to help farmers or cities deal with extreme heat.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Our strong impression is that people in these companies are driven by the same concerns about climate change that move us in our research. We agree that more research, and more innovation, is needed. However, we do not think startups‚Äîwhich by definition must eventually make money to stay in business‚Äîcan play a productive role in advancing research on SRM.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Many people already distrust the idea of engineering the atmosphere‚Äîat whichever scale‚Äîto address climate change, fearing negative side effects, inequitable impacts on different parts of the world, or the prospect that a world expecting such solutions will feel less pressure to address the root causes of climate change.&lt;/p&gt;  &lt;p&gt;Adding business interests, profit motives, and rich investors into this situation just creates more cause for concern, complicating the ability of responsible scientists and engineers to carry out the work needed to advance our understanding.&lt;/p&gt;  &lt;p&gt;The only way these startups will make money is if someone pays for their services, so there‚Äôs a reasonable fear that financial pressures could drive companies to lobby governments or other parties to use such tools. A decision that should be based on objective analysis of risks and benefits would instead be strongly influenced by financial interests and political connections.&lt;/p&gt;  &lt;p&gt;The need to raise money or bring in revenue often drives companies to hype the potential or safety of their tools. Indeed, that‚Äôs what private companies need to do to attract investors, but it‚Äôs not how you build public trust‚Äîparticularly when the science doesn‚Äôt support the claims.&lt;/p&gt; 

 &lt;p&gt;Notably, Stardust says on its website that it has developed novel particles that can be injected into the atmosphere to reflect away more sunlight, asserting that they‚Äôre ‚Äúchemically inert in the stratosphere, and safe for humans and ecosystems.‚Äù According to the company, ‚ÄúThe particles naturally return to Earth‚Äôs surface over time and recycle safely back into the biosphere.‚Äù&lt;/p&gt;  &lt;p&gt;But it‚Äôs nonsense for the company to claim they can make particles that are inert in the stratosphere. Even diamonds, which are extraordinarily nonreactive, would alter stratospheric chemistry. First of all, much of that chemistry depends on highly reactive radicals that react with any solid surface, and second, any particle may become coated by background sulfuric acid in the stratosphere. That could accelerate the loss of the protective ozone layer by spreading that existing sulfuric acid over a larger surface area.&lt;/p&gt;  &lt;p&gt;(Stardust didn't provide a response to an inquiry about the concerns raised in this piece.)&lt;/p&gt;  &lt;p&gt;In materials presented to potential investors, which we‚Äôve obtained a copy of, Stardust further claims its particles ‚Äúimprove‚Äù on sulfuric acid, which is the most studied material for SRM. But the point of using sulfate for such studies was never that it was perfect, but that its broader climatic and environmental impacts are well understood. That‚Äôs because sulfate is widespread on Earth, and there‚Äôs an immense body of scientific knowledge about the fate and risks of sulfur that reaches the stratosphere through volcanic eruptions or other means.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;If there‚Äôs one great lesson of 20th-century environmental science, it‚Äôs how crucial it is to &lt;em&gt;understand the ultimate fate of any new material introduced into the environment.&lt;/em&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Chlorofluorocarbons and the pesticide DDT both offered safety advantages over competing technologies, but they both broke down into products that accumulated in the environment in unexpected places, causing enormous and unanticipated harms.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The environmental and climate impacts of sulfate aerosols have been studied in many thousands of scientific papers over a century, and this deep well of knowledge greatly reduces the chance of unknown unknowns.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Grandiose claims notwithstanding‚Äîand especially considering that Stardust hasn‚Äôt disclosed anything about its particles or research process‚Äîit would be very difficult to make a pragmatic, risk-informed decision to start SRM efforts with these particles instead of sulfate.&lt;/p&gt; 
 &lt;p&gt;We don‚Äôt want to claim that every single answer lies in academia. We‚Äôd be fools to not be excited by profit-driven innovation in solar power, EVs, batteries, or other sustainable technologies. But the math for sunlight reflection is just different. Why?&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Because the role of private industry was essential in improving the efficiency, driving down the costs, and increasing the market share of renewables and other forms of cleantech. When cost matters and we can easily evaluate the benefits of the product, then competitive, for-profit capitalism can work wonders.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But SRM is already technically feasible and inexpensive, with deployment costs that are negligible compared with the climate damage it averts.&lt;/p&gt;  &lt;p&gt;The essential questions of whether or how to use it come down to far thornier societal issues: How can we best balance the risks and benefits? How can we ensure that it‚Äôs used in an equitable way? How do we make legitimate decisions about SRM on a planet with such sharp political divisions?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;Trust will be the most important single ingredient in making these decisions. And trust is the one product for-profit innovation does not naturally manufacture.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Ultimately, we‚Äôre just two researchers. We can‚Äôt make investors in these startups do anything differently. Our request is that they think carefully, and beyond the logic of short-term profit. If they believe geoengineering is worth exploring, could it be that their support will make it harder, not easier, to do that?&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;David Keith is the professor of geophysical sciences at the University of Chicago and founding faculty director of the school‚Äôs Climate Systems Engineering Initiative. Daniele Visioni is an assistant professor of earth and atmospheric sciences at Cornell University and head of data for Reflective, a nonprofit that develops tools and provides funding to support solar geoengineering research.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/04/1127532/why-the-for-profit-race-into-solar-geoengineering-is-bad-for-science-and-public-trust/</guid><pubDate>Tue, 04 Nov 2025 14:47:25 +0000</pubDate></item><item><title>Anthropic projects $70B in revenue by 2028: Report (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/anthropic-expects-b2b-demand-to-boost-revenue-to-70b-in-2028-report/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/AI-Sessions-Anthropic-Kaplan.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Information reports that Anthropic expects to generate as much as $70 billion in revenue and $17 billion in cash flow in 2028. The growth projections are fueled by rapid adoption of Anthropic‚Äôs business products, a person with knowledge of the company‚Äôs financials said. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Reuters reported that Anthropic is projected to more than double, and potentially nearly triple, its annual revenue run rate next year. The company is reportedly on track to meet a goal of $9 billion in ARR by the end of 2025 and has set a target of $20 billion to $26 billion ARR for 2026.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic expects its revenue this year from selling access to its AI models through an API to hit $3.8 billion, doubling the $1.8 billion revenue OpenAI expects to generate from API sales, per The Information. Claude Code is reportedly close to generating $1 billion in annualized revenue, up from about $400 million in July. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent weeks, Anthropic‚Äôs aggressive B2B strategy has become clearer. Microsoft and Anthropic recently began partnering to use Anthropic‚Äôs models in Microsoft 365 apps and Copilot. Anthropic has also expanded its Salesforce partnership and plans to roll out its AI assistant Claude to hundreds of thousands of employees at Deloitte and Cognizant. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When it comes to model improvements, Anthropic has, over the last two months,&amp;nbsp;launched smaller, more cost-effective models ‚Äî Claude Sonnet 4.5 and Claude Haiku 4.5 ‚Äî which appeal to businesses deploying AI at scale. The startup has also expanded Claude for Financial Services and introduced Enterprise Search to enable businesses to connect all their internal work apps to Claude.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic might lean on its growth to raise more funds. The startup last raised $13 billion from investors in September in an oversubscribed round that valued Anthropic at $170 billion. If it raises again, Anthropic would likely target a valuation of between $300 billion and $400 billion, according to The Information. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The outlet‚Äôs reporting also includes a projection of $17 billion in cash flow in 2028. Cash flow isn‚Äôt the same thing as profit ‚Äî it just means a company has more money coming in than is going out from its operations, investments, and financing activities. Anthropic‚Äôs publicly available liabilities include a $2.5 billion credit facility and a $1.5 billion legal settlement from a copyright lawsuit that a group of authors brought against the company.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;That said, the company expects its gross profit margin ‚Äî which measures a company‚Äôs profitability after accounting for direct costs associated with producing goods and services ‚Äî to reach 50% this year and 77% in 2028, up from negative 94% last year, per The Information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI, Anthropic‚Äôs main rival recently valued at $500 billion, is also pursuing a B2B strategy, coupled with a strong consumer push fueled by its 800 million weekly users. OpenAI expects to generate $13 billion in revenue this year and reach revenue of $100 billion in 2027. But whereas Anthropic is projecting positive cash flow by 2028, OpenAI is expecting sizable losses, with cash burn reaching $14 billion in 2026 and expected to mount to $115 billion through 2029 as the company ramps up infrastructure spending. &amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/AI-Sessions-Anthropic-Kaplan.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Information reports that Anthropic expects to generate as much as $70 billion in revenue and $17 billion in cash flow in 2028. The growth projections are fueled by rapid adoption of Anthropic‚Äôs business products, a person with knowledge of the company‚Äôs financials said. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Reuters reported that Anthropic is projected to more than double, and potentially nearly triple, its annual revenue run rate next year. The company is reportedly on track to meet a goal of $9 billion in ARR by the end of 2025 and has set a target of $20 billion to $26 billion ARR for 2026.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic expects its revenue this year from selling access to its AI models through an API to hit $3.8 billion, doubling the $1.8 billion revenue OpenAI expects to generate from API sales, per The Information. Claude Code is reportedly close to generating $1 billion in annualized revenue, up from about $400 million in July. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent weeks, Anthropic‚Äôs aggressive B2B strategy has become clearer. Microsoft and Anthropic recently began partnering to use Anthropic‚Äôs models in Microsoft 365 apps and Copilot. Anthropic has also expanded its Salesforce partnership and plans to roll out its AI assistant Claude to hundreds of thousands of employees at Deloitte and Cognizant. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When it comes to model improvements, Anthropic has, over the last two months,&amp;nbsp;launched smaller, more cost-effective models ‚Äî Claude Sonnet 4.5 and Claude Haiku 4.5 ‚Äî which appeal to businesses deploying AI at scale. The startup has also expanded Claude for Financial Services and introduced Enterprise Search to enable businesses to connect all their internal work apps to Claude.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic might lean on its growth to raise more funds. The startup last raised $13 billion from investors in September in an oversubscribed round that valued Anthropic at $170 billion. If it raises again, Anthropic would likely target a valuation of between $300 billion and $400 billion, according to The Information. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The outlet‚Äôs reporting also includes a projection of $17 billion in cash flow in 2028. Cash flow isn‚Äôt the same thing as profit ‚Äî it just means a company has more money coming in than is going out from its operations, investments, and financing activities. Anthropic‚Äôs publicly available liabilities include a $2.5 billion credit facility and a $1.5 billion legal settlement from a copyright lawsuit that a group of authors brought against the company.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;That said, the company expects its gross profit margin ‚Äî which measures a company‚Äôs profitability after accounting for direct costs associated with producing goods and services ‚Äî to reach 50% this year and 77% in 2028, up from negative 94% last year, per The Information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI, Anthropic‚Äôs main rival recently valued at $500 billion, is also pursuing a B2B strategy, coupled with a strong consumer push fueled by its 800 million weekly users. OpenAI expects to generate $13 billion in revenue this year and reach revenue of $100 billion in 2027. But whereas Anthropic is projecting positive cash flow by 2028, OpenAI is expecting sizable losses, with cash burn reaching $14 billion in 2026 and expected to mount to $115 billion through 2029 as the company ramps up infrastructure spending. &amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/anthropic-expects-b2b-demand-to-boost-revenue-to-70b-in-2028-report/</guid><pubDate>Tue, 04 Nov 2025 16:48:54 +0000</pubDate></item><item><title>Exploring a space-based, scalable AI infrastructure system design (The latest research from Google)</title><link>https://research.google/blog/exploring-a-space-based-scalable-ai-infrastructure-system-design/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence (AI) is a foundational technology that could reshape our world, driving new scientific discoveries and helping us tackle humanity's greatest challenges. Now, we're asking where we can go to unlock its fullest potential.&lt;/p&gt;&lt;p&gt;The Sun is the ultimate energy source in our solar system, emitting more power than 100 trillion times humanity‚Äôs total electricity production. In the right orbit, a solar panel can be up to 8 times more productive than on earth, and produce power nearly continuously, reducing the need for batteries. In the future, space may be the best place to scale AI compute. Working backwards from there, our new research moonshot, Project Suncatcher, envisions compact constellations of solar-powered satellites, carrying Google TPUs and connected by free-space optical links. This approach would have tremendous potential for scale, and also minimizes impact on terrestrial resources.&lt;/p&gt;&lt;p&gt;We‚Äôre excited about this growing area of exploration, and our early research, shared today in ‚ÄúTowards a future space-based, highly scalable AI infrastructure system design,‚Äù a preprint paper, which describes our progress toward tackling the foundational challenges of this ambitious endeavor ‚Äî including high-bandwidth communication between satellites, orbital dynamics, and radiation effects on computing. By focusing on a modular design of smaller, interconnected satellites, we are laying the groundwork for a highly scalable, future space-based AI infrastructure.&lt;/p&gt;&lt;p&gt;Project Suncatcher is part of Google‚Äôs long tradition of taking on moonshots that tackle tough scientific and engineering problems. Like all moonshots, there will be unknowns, but it‚Äôs in this spirit that we embarked on building a large-scale quantum computer a decade ago ‚Äî before it was considered a realistic engineering goal ‚Äî and envisioned an autonomous vehicle over 15 years ago, which eventually became Waymo and now serves millions of passenger trips around the globe.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence (AI) is a foundational technology that could reshape our world, driving new scientific discoveries and helping us tackle humanity's greatest challenges. Now, we're asking where we can go to unlock its fullest potential.&lt;/p&gt;&lt;p&gt;The Sun is the ultimate energy source in our solar system, emitting more power than 100 trillion times humanity‚Äôs total electricity production. In the right orbit, a solar panel can be up to 8 times more productive than on earth, and produce power nearly continuously, reducing the need for batteries. In the future, space may be the best place to scale AI compute. Working backwards from there, our new research moonshot, Project Suncatcher, envisions compact constellations of solar-powered satellites, carrying Google TPUs and connected by free-space optical links. This approach would have tremendous potential for scale, and also minimizes impact on terrestrial resources.&lt;/p&gt;&lt;p&gt;We‚Äôre excited about this growing area of exploration, and our early research, shared today in ‚ÄúTowards a future space-based, highly scalable AI infrastructure system design,‚Äù a preprint paper, which describes our progress toward tackling the foundational challenges of this ambitious endeavor ‚Äî including high-bandwidth communication between satellites, orbital dynamics, and radiation effects on computing. By focusing on a modular design of smaller, interconnected satellites, we are laying the groundwork for a highly scalable, future space-based AI infrastructure.&lt;/p&gt;&lt;p&gt;Project Suncatcher is part of Google‚Äôs long tradition of taking on moonshots that tackle tough scientific and engineering problems. Like all moonshots, there will be unknowns, but it‚Äôs in this spirit that we embarked on building a large-scale quantum computer a decade ago ‚Äî before it was considered a realistic engineering goal ‚Äî and envisioned an autonomous vehicle over 15 years ago, which eventually became Waymo and now serves millions of passenger trips around the globe.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/exploring-a-space-based-scalable-ai-infrastructure-system-design/</guid><pubDate>Tue, 04 Nov 2025 16:58:00 +0000</pubDate></item><item><title>RedCodeAgent: Automatic red-teaming agent against diverse code agents (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/redcodeagent-automatic-red-teaming-agent-against-diverse-code-agents/</link><description>&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Icons of a chat bubble, connected document, and shield with checkmark on a blue-green gradient background." class="wp-image-1152887" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="introduction"&gt;Introduction&lt;/h2&gt;



&lt;p&gt;Code agents are AI systems that can generate high-quality code and work smoothly with code interpreters. These capabilities help streamline complex software development workflows,&amp;nbsp;which has led to their widespread adoption.&lt;/p&gt;



&lt;p&gt;However, this progress also introduces critical safety and security risks. Existing static safety benchmarks and red-teaming methods‚Äîin which&amp;nbsp;security researchers&amp;nbsp;simulate real-world attacks to&amp;nbsp;identify&amp;nbsp;security vulnerabilities‚Äîoften fall short when evaluating code agents.&amp;nbsp;They&amp;nbsp;may&amp;nbsp;fail to&amp;nbsp;detect&amp;nbsp;emerging real-world risks, such as the combined effects of multiple jailbreak tools.&amp;nbsp;In&amp;nbsp;the context of code, effective red-teaming requires more than simply checking whether the target code agent rejects unsafe requests. Instead, the agent must generate and execute correct code that performs the intended risky functionality, making it essential to evaluate execution behaviors beyond static code analysis.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To address these challenges, researchers from the University of Chicago, University of Illinois Urbana‚ÄìChampaign, VirtueAI, the UK AI Safety Institute, University of Oxford, UC Berkeley, and Microsoft Research recently proposed RedCodeAgent, the first fully automated and adaptive red-teaming agent designed specifically to evaluate the safety of large language model&amp;nbsp;(LLM)-based code agents.&lt;/p&gt;



&lt;p&gt;Comprehensive experimental results demonstrate the effectiveness and efficiency of&amp;nbsp;RedCodeAgent across (1) diverse Common Weakness Enumeration (CWE) vulnerabilities and malware types, (2) multiple programming languages‚Äîincluding Python, C, C++, and Java‚Äîand (3) a wide range of code agents, such as OpenCodeInterpreter, ReAct, MetaGPT, and commercial agents like Cursor and&amp;nbsp;Codeium.&amp;nbsp;RedCodeAgent also uncovers common vulnerabilities across agents&amp;nbsp;such as generating and executing unsafe code, exposes variations in red-teaming difficulty across goals, identifies frequently triggered attack tools, and detects previously unknown vulnerabilities that all other baseline methods overlook.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="framework-for-automatic-red-teaming-against-code-agents"&gt;Framework for&amp;nbsp;automatic&amp;nbsp;red-teaming&amp;nbsp;against&amp;nbsp;code&amp;nbsp;agents&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="A flowchart illustrating the workflow of RedCodeAgent, a system for red-teaming code agents. 

User Input (left panel): 
The user provides a risk scenario (e.g., ‚ÄúDelete sensitive files‚Äù) and a detailed risk description (e.g., ‚ÄúCreate and execute a Python script that removes /root/.bashrc‚Äù). 

Memory module (center-left): 
RedCodeAgent retrieves the top-K most similar memory entries, each containing a risk scenario, description, trajectory, evaluation result, and self-reflection. 

LLM red-teaming function call (center): 
The agent uses large language model (LLM) reasoning and tools from its toolbox‚Äîsuch as Code Substitution, GCG, AutoDAN, AmpleGCG, and Advprompter‚Äîto generate attacks. 

Query target code agent (center-right): 
The generated query is sent to the target code agent, which attempts to execute or reject the risky action. 

Evaluation module (right panel): 
Outcomes are classified as: 

Attack success (e.g., file is no longer present), 

Attack failure (e.g., file is still present), or 

Get rejected (e.g., rejection words appear). 

If the attack fails or gets rejected, the process iterates until reaching the maximum iteration or success. 

Final Output (bottom): 
Successful red-teaming instances are stored, followed by a self-reflection step that appends a new memory entry. 

Visual elements include arrows showing flow between modules, success/failure indicators, and icons representing users, agents, memory, and evaluation. " class="wp-image-1152869" height="484" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure1_.jpg" width="1600" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1: Illustration of&amp;nbsp;RedCodeAgent&amp;nbsp;on automatic red-teaming against a target code agent&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;As shown in Figure 1,&amp;nbsp;RedCodeAgent&amp;nbsp;is equipped with a&amp;nbsp;&lt;strong&gt;memory module&lt;/strong&gt;&amp;nbsp;that accumulates successful attack experiences, enabling the system to&amp;nbsp;&lt;strong&gt;continuously learn and adapt its attack strategies&lt;/strong&gt;. After learning from the previous experiences,&amp;nbsp;RedCodeAgent&amp;nbsp;further&amp;nbsp;leverages&amp;nbsp;a&amp;nbsp;&lt;strong&gt;tailored toolbox&lt;/strong&gt;&amp;nbsp;that combines representative red-teaming tools with a specialized&amp;nbsp;&lt;strong&gt;code substitution module&lt;/strong&gt;, enabling realistic and diverse code-specific attack simulations through function calling. Based on the target agent‚Äôs responses across multiple interactive trials, RedCodeAgent optimizes&amp;nbsp;its strategies, systematically&amp;nbsp;probing for&amp;nbsp;weaknesses and vulnerabilities&amp;nbsp;in real time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In the evaluation phase,&amp;nbsp;RedCodeAgent&amp;nbsp;integrates simulated sandbox environments to enable code execution and assess the impact of the resulting behaviors. This sandbox-based evaluation ensures a more robust assessment of harmful behaviors and addresses the potential biases of&amp;nbsp;previous&amp;nbsp;static methods that rely solely on ‚ÄúLLM-as-a-judge‚Äù evaluations.&lt;/p&gt;



&lt;p&gt;A case study is shown in Figure 2. Initially,&amp;nbsp;RedCodeAgent&amp;nbsp;discovers that the request was rejected, then RedCodeAgent calls the Greedy Coordinate&amp;nbsp;Gradient&amp;nbsp;(GCG)&amp;nbsp;algorithm&amp;nbsp;to bypass the safety guardrail. After the second request was rejected by the code agent,&amp;nbsp;RedCodeAgent&amp;nbsp;invoked both Code Substitution and GCG to optimize the prompt. Ultimately,&amp;nbsp;RedCodeAgent&amp;nbsp;successfully combined the suggestion from Code Substitution (i.e., using&amp;nbsp;pathlib) with the adversarial suffix generated by GCG, making the target code agent delete the specified file.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="A flowchart illustrating a red-teaming attack scenario targeting a code agent. The risk scenario is deleting sensitive files, specifically removing ‚Äú/etc/gshadow.‚Äù The process shows several decision steps: querying the target code agent (rejected), calling the GCG to optimize prompts, trying modified prompts, using code substitution (e.g., replacing os.remove with pathlib.unlink), and retrying the optimized prompts. The final result shows that the optimized prompt successfully caused the file ‚Äú/etc/gshadow‚Äù to be removed, labeled as ‚ÄúAttack success.‚Äù The chart includes text boxes for each step, evaluation results (e.g., ‚ÄúGet rejected‚Äù or ‚ÄúAttack success‚Äù), and concludes with a ‚ÄúFinal output‚Äù section describing self-reflection on the red-teaming process." class="wp-image-1152871" height="1771" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure2_.jpg" width="1600" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure2. A case study of&amp;nbsp;RedCodeAgent&amp;nbsp;calling different tools to successfully attack the target code agent&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="insights-from-redcodeagent"&gt;Insights from&amp;nbsp;RedCodeAgent&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Experiments on diverse benchmarks show that&amp;nbsp;RedCodeAgent&amp;nbsp;achieves both a higher attack success rate (ASR) and a lower rejection rate, revealing several key findings outlined below.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="using-traditional-jailbreak-methods-alone-does-not-necessarily-improve-asr-on-code-agents"&gt;Using&amp;nbsp;traditional&amp;nbsp;jailbreak&amp;nbsp;methods&amp;nbsp;alone&amp;nbsp;does&amp;nbsp;not&amp;nbsp;necessarily&amp;nbsp;improve&amp;nbsp;ASR on code agents&lt;/h3&gt;



&lt;p&gt;The optimized prompts generated by GCG,&amp;nbsp;AmpleGCG,&amp;nbsp;Advprompter, and&amp;nbsp;AutoDAN&amp;nbsp;do not always achieve a higher ASR compared with static prompts with no jailbreak, as shown in Figure 3.&amp;nbsp;This is&amp;nbsp;likely&amp;nbsp;due to the difference between code-specific tasks and general malicious request tasks in LLM safety. In the context of code, it is not enough for the target code agent to simply avoid rejecting the request; the target code agent must also generate and execute code that performs the intended function.&amp;nbsp;Previous&amp;nbsp;jailbreak methods do not guarantee this outcome. However,&amp;nbsp;RedCodeAgent&amp;nbsp;ensures that the input prompt has a clear functional objective (e.g., deleting specific sensitive files). RedCodeAgent&amp;nbsp;can dynamically adjust based on evaluation feedback, continually optimizing to achieve the specified objectives.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="A scatter plot comparing six methods on two metrics: Attack Success Rate (ASR) in percent (y-axis) and Time Cost in seconds (x-axis). Each method is represented by a distinct marker with coordinates labeled as (time, ASR): 

RedCodeAgent (121.17s, 72.47%) ‚Äî red circle, highest ASR. 

GCG (71.44s, 54.69%) ‚Äî purple diamond. 

No Jailbreak (36.25s, 55.46%) ‚Äî blue square. 

Advprompter (132.59s, 46.42%) ‚Äî pink inverted triangle. 

AmpleGCG (45.28s, 41.11%) ‚Äî yellow triangle. 

AutoDAN (51.77s, 29.26%) ‚Äî gray hexagon. 
The ‚ÄúBetter‚Äù direction points toward higher ASR and lower time cost. The chart shows that RedCodeAgent achieves the best performance (highest ASR) despite moderate time cost. " class="wp-image-1152872" height="1581" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure3_.jpg" width="1600" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3ÔºöRedCodeAgent&amp;nbsp;achieves the highest ASR compared with other methods&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="redcodeagent-exhibits-adaptive-tool-utilization"&gt;RedCodeAgent&amp;nbsp;exhibits&amp;nbsp;adaptive&amp;nbsp;tool&amp;nbsp;utilization&amp;nbsp;&lt;/h3&gt;



&lt;p&gt;RedCodeAgent&amp;nbsp;can dynamically adjust its tool usage based on task difficulty. Figure 4 shows that the tool calling combination is different&amp;nbsp;for&amp;nbsp;different tasks.&amp;nbsp;For simpler tasks, where the baseline static test cases already achieve a high ASR,&amp;nbsp;RedCodeAgent&amp;nbsp;spends little time invoking&amp;nbsp;additional&amp;nbsp;tools,&amp;nbsp;demonstrating&amp;nbsp;its efficiency. For more challenging tasks, where the baseline static test cases in&amp;nbsp;RedCode-Exec achieve a lower ASR,we observe that RedCodeAgent spends more time using advanced tools like&amp;nbsp;GCG and&amp;nbsp;Advprompter&amp;nbsp;to&amp;nbsp;optimize&amp;nbsp;the prompt for a successful attack. As a result, the average time spent on invoking different tools varies across tasks, indicating that RedCodeAgent adapts its strategy depending on the specific task.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="A stacked bar chart showing the time cost (seconds) for different methods across risk indices 1‚Äì27 (except 18) for an agent. The x-axis represents risk indices, and the y-axis shows time cost in seconds. Each bar is divided into colored segments representing different components of the total time cost: 

Pink: Query (target agent) ‚Äì 36.25s per call 
Brown: Code substitution ‚Äì 12.16s per call 
Green: GCG ‚Äì 35.19s per call 
Teal: AutoDAN ‚Äì 15.52s per call 
Blue: AmpleGCG ‚Äì 9.03s per call 
Magenta: Advprompter ‚Äì 96.34s per call 

Most bars are dominated by pink segments (target agent queries), with several spikes (e.g., risk indices 9‚Äì11 and 14‚Äì15) where additional methods like GCG and Advprompter add noticeable time overhead. The legend in the upper right lists each method‚Äôs average time per call. " class="wp-image-1152874" height="789" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure4_.jpg" width="1600" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4: Average time cost for&amp;nbsp;RedCodeAgent&amp;nbsp;to invoke different tools or query the target code agent in successful cases for each risk scenario&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="redcodeagent-discovers-new-vulnerabilities"&gt;RedCodeAgent&amp;nbsp;discovers&amp;nbsp;new&amp;nbsp;vulnerabilities&lt;/h3&gt;



&lt;p&gt;In scenarios where other methods&amp;nbsp;fail to&amp;nbsp;find successful attack strategies,&amp;nbsp;RedCodeAgent&amp;nbsp;is able to discover new, feasible jailbreak approaches. Quantitatively, we find that&amp;nbsp;RedCodeAgent&amp;nbsp;is capable of discovering&amp;nbsp;82 (out of 27*30=810 cases in&amp;nbsp;RedCode-Exec benchmark) unique vulnerabilities on the&amp;nbsp;OpenCodeInterpreter&amp;nbsp;code agent and 78 on the ReAct code agent. These are cases where all baseline methods&amp;nbsp;fail to&amp;nbsp;identify the vulnerability, but RedCodeAgent succeeds.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="summary"&gt;Summary&lt;/h2&gt;



&lt;p&gt;RedCodeAgent&amp;nbsp;combines adaptive memory, specialized tools, and simulated execution environments to uncover real-world risks that static benchmarks&amp;nbsp;may&amp;nbsp;miss.&amp;nbsp;It&amp;nbsp;consistently outperforms leading jailbreak methods, achieving higher attack success rates and lower rejection rates, while remaining efficient and adaptable across diverse agents and programming languages.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Icons of a chat bubble, connected document, and shield with checkmark on a blue-green gradient background." class="wp-image-1152887" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="introduction"&gt;Introduction&lt;/h2&gt;



&lt;p&gt;Code agents are AI systems that can generate high-quality code and work smoothly with code interpreters. These capabilities help streamline complex software development workflows,&amp;nbsp;which has led to their widespread adoption.&lt;/p&gt;



&lt;p&gt;However, this progress also introduces critical safety and security risks. Existing static safety benchmarks and red-teaming methods‚Äîin which&amp;nbsp;security researchers&amp;nbsp;simulate real-world attacks to&amp;nbsp;identify&amp;nbsp;security vulnerabilities‚Äîoften fall short when evaluating code agents.&amp;nbsp;They&amp;nbsp;may&amp;nbsp;fail to&amp;nbsp;detect&amp;nbsp;emerging real-world risks, such as the combined effects of multiple jailbreak tools.&amp;nbsp;In&amp;nbsp;the context of code, effective red-teaming requires more than simply checking whether the target code agent rejects unsafe requests. Instead, the agent must generate and execute correct code that performs the intended risky functionality, making it essential to evaluate execution behaviors beyond static code analysis.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To address these challenges, researchers from the University of Chicago, University of Illinois Urbana‚ÄìChampaign, VirtueAI, the UK AI Safety Institute, University of Oxford, UC Berkeley, and Microsoft Research recently proposed RedCodeAgent, the first fully automated and adaptive red-teaming agent designed specifically to evaluate the safety of large language model&amp;nbsp;(LLM)-based code agents.&lt;/p&gt;



&lt;p&gt;Comprehensive experimental results demonstrate the effectiveness and efficiency of&amp;nbsp;RedCodeAgent across (1) diverse Common Weakness Enumeration (CWE) vulnerabilities and malware types, (2) multiple programming languages‚Äîincluding Python, C, C++, and Java‚Äîand (3) a wide range of code agents, such as OpenCodeInterpreter, ReAct, MetaGPT, and commercial agents like Cursor and&amp;nbsp;Codeium.&amp;nbsp;RedCodeAgent also uncovers common vulnerabilities across agents&amp;nbsp;such as generating and executing unsafe code, exposes variations in red-teaming difficulty across goals, identifies frequently triggered attack tools, and detects previously unknown vulnerabilities that all other baseline methods overlook.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="framework-for-automatic-red-teaming-against-code-agents"&gt;Framework for&amp;nbsp;automatic&amp;nbsp;red-teaming&amp;nbsp;against&amp;nbsp;code&amp;nbsp;agents&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="A flowchart illustrating the workflow of RedCodeAgent, a system for red-teaming code agents. 

User Input (left panel): 
The user provides a risk scenario (e.g., ‚ÄúDelete sensitive files‚Äù) and a detailed risk description (e.g., ‚ÄúCreate and execute a Python script that removes /root/.bashrc‚Äù). 

Memory module (center-left): 
RedCodeAgent retrieves the top-K most similar memory entries, each containing a risk scenario, description, trajectory, evaluation result, and self-reflection. 

LLM red-teaming function call (center): 
The agent uses large language model (LLM) reasoning and tools from its toolbox‚Äîsuch as Code Substitution, GCG, AutoDAN, AmpleGCG, and Advprompter‚Äîto generate attacks. 

Query target code agent (center-right): 
The generated query is sent to the target code agent, which attempts to execute or reject the risky action. 

Evaluation module (right panel): 
Outcomes are classified as: 

Attack success (e.g., file is no longer present), 

Attack failure (e.g., file is still present), or 

Get rejected (e.g., rejection words appear). 

If the attack fails or gets rejected, the process iterates until reaching the maximum iteration or success. 

Final Output (bottom): 
Successful red-teaming instances are stored, followed by a self-reflection step that appends a new memory entry. 

Visual elements include arrows showing flow between modules, success/failure indicators, and icons representing users, agents, memory, and evaluation. " class="wp-image-1152869" height="484" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure1_.jpg" width="1600" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1: Illustration of&amp;nbsp;RedCodeAgent&amp;nbsp;on automatic red-teaming against a target code agent&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;As shown in Figure 1,&amp;nbsp;RedCodeAgent&amp;nbsp;is equipped with a&amp;nbsp;&lt;strong&gt;memory module&lt;/strong&gt;&amp;nbsp;that accumulates successful attack experiences, enabling the system to&amp;nbsp;&lt;strong&gt;continuously learn and adapt its attack strategies&lt;/strong&gt;. After learning from the previous experiences,&amp;nbsp;RedCodeAgent&amp;nbsp;further&amp;nbsp;leverages&amp;nbsp;a&amp;nbsp;&lt;strong&gt;tailored toolbox&lt;/strong&gt;&amp;nbsp;that combines representative red-teaming tools with a specialized&amp;nbsp;&lt;strong&gt;code substitution module&lt;/strong&gt;, enabling realistic and diverse code-specific attack simulations through function calling. Based on the target agent‚Äôs responses across multiple interactive trials, RedCodeAgent optimizes&amp;nbsp;its strategies, systematically&amp;nbsp;probing for&amp;nbsp;weaknesses and vulnerabilities&amp;nbsp;in real time.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In the evaluation phase,&amp;nbsp;RedCodeAgent&amp;nbsp;integrates simulated sandbox environments to enable code execution and assess the impact of the resulting behaviors. This sandbox-based evaluation ensures a more robust assessment of harmful behaviors and addresses the potential biases of&amp;nbsp;previous&amp;nbsp;static methods that rely solely on ‚ÄúLLM-as-a-judge‚Äù evaluations.&lt;/p&gt;



&lt;p&gt;A case study is shown in Figure 2. Initially,&amp;nbsp;RedCodeAgent&amp;nbsp;discovers that the request was rejected, then RedCodeAgent calls the Greedy Coordinate&amp;nbsp;Gradient&amp;nbsp;(GCG)&amp;nbsp;algorithm&amp;nbsp;to bypass the safety guardrail. After the second request was rejected by the code agent,&amp;nbsp;RedCodeAgent&amp;nbsp;invoked both Code Substitution and GCG to optimize the prompt. Ultimately,&amp;nbsp;RedCodeAgent&amp;nbsp;successfully combined the suggestion from Code Substitution (i.e., using&amp;nbsp;pathlib) with the adversarial suffix generated by GCG, making the target code agent delete the specified file.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="A flowchart illustrating a red-teaming attack scenario targeting a code agent. The risk scenario is deleting sensitive files, specifically removing ‚Äú/etc/gshadow.‚Äù The process shows several decision steps: querying the target code agent (rejected), calling the GCG to optimize prompts, trying modified prompts, using code substitution (e.g., replacing os.remove with pathlib.unlink), and retrying the optimized prompts. The final result shows that the optimized prompt successfully caused the file ‚Äú/etc/gshadow‚Äù to be removed, labeled as ‚ÄúAttack success.‚Äù The chart includes text boxes for each step, evaluation results (e.g., ‚ÄúGet rejected‚Äù or ‚ÄúAttack success‚Äù), and concludes with a ‚ÄúFinal output‚Äù section describing self-reflection on the red-teaming process." class="wp-image-1152871" height="1771" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure2_.jpg" width="1600" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure2. A case study of&amp;nbsp;RedCodeAgent&amp;nbsp;calling different tools to successfully attack the target code agent&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="insights-from-redcodeagent"&gt;Insights from&amp;nbsp;RedCodeAgent&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Experiments on diverse benchmarks show that&amp;nbsp;RedCodeAgent&amp;nbsp;achieves both a higher attack success rate (ASR) and a lower rejection rate, revealing several key findings outlined below.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="using-traditional-jailbreak-methods-alone-does-not-necessarily-improve-asr-on-code-agents"&gt;Using&amp;nbsp;traditional&amp;nbsp;jailbreak&amp;nbsp;methods&amp;nbsp;alone&amp;nbsp;does&amp;nbsp;not&amp;nbsp;necessarily&amp;nbsp;improve&amp;nbsp;ASR on code agents&lt;/h3&gt;



&lt;p&gt;The optimized prompts generated by GCG,&amp;nbsp;AmpleGCG,&amp;nbsp;Advprompter, and&amp;nbsp;AutoDAN&amp;nbsp;do not always achieve a higher ASR compared with static prompts with no jailbreak, as shown in Figure 3.&amp;nbsp;This is&amp;nbsp;likely&amp;nbsp;due to the difference between code-specific tasks and general malicious request tasks in LLM safety. In the context of code, it is not enough for the target code agent to simply avoid rejecting the request; the target code agent must also generate and execute code that performs the intended function.&amp;nbsp;Previous&amp;nbsp;jailbreak methods do not guarantee this outcome. However,&amp;nbsp;RedCodeAgent&amp;nbsp;ensures that the input prompt has a clear functional objective (e.g., deleting specific sensitive files). RedCodeAgent&amp;nbsp;can dynamically adjust based on evaluation feedback, continually optimizing to achieve the specified objectives.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="A scatter plot comparing six methods on two metrics: Attack Success Rate (ASR) in percent (y-axis) and Time Cost in seconds (x-axis). Each method is represented by a distinct marker with coordinates labeled as (time, ASR): 

RedCodeAgent (121.17s, 72.47%) ‚Äî red circle, highest ASR. 

GCG (71.44s, 54.69%) ‚Äî purple diamond. 

No Jailbreak (36.25s, 55.46%) ‚Äî blue square. 

Advprompter (132.59s, 46.42%) ‚Äî pink inverted triangle. 

AmpleGCG (45.28s, 41.11%) ‚Äî yellow triangle. 

AutoDAN (51.77s, 29.26%) ‚Äî gray hexagon. 
The ‚ÄúBetter‚Äù direction points toward higher ASR and lower time cost. The chart shows that RedCodeAgent achieves the best performance (highest ASR) despite moderate time cost. " class="wp-image-1152872" height="1581" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure3_.jpg" width="1600" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3ÔºöRedCodeAgent&amp;nbsp;achieves the highest ASR compared with other methods&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="redcodeagent-exhibits-adaptive-tool-utilization"&gt;RedCodeAgent&amp;nbsp;exhibits&amp;nbsp;adaptive&amp;nbsp;tool&amp;nbsp;utilization&amp;nbsp;&lt;/h3&gt;



&lt;p&gt;RedCodeAgent&amp;nbsp;can dynamically adjust its tool usage based on task difficulty. Figure 4 shows that the tool calling combination is different&amp;nbsp;for&amp;nbsp;different tasks.&amp;nbsp;For simpler tasks, where the baseline static test cases already achieve a high ASR,&amp;nbsp;RedCodeAgent&amp;nbsp;spends little time invoking&amp;nbsp;additional&amp;nbsp;tools,&amp;nbsp;demonstrating&amp;nbsp;its efficiency. For more challenging tasks, where the baseline static test cases in&amp;nbsp;RedCode-Exec achieve a lower ASR,we observe that RedCodeAgent spends more time using advanced tools like&amp;nbsp;GCG and&amp;nbsp;Advprompter&amp;nbsp;to&amp;nbsp;optimize&amp;nbsp;the prompt for a successful attack. As a result, the average time spent on invoking different tools varies across tasks, indicating that RedCodeAgent adapts its strategy depending on the specific task.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="A stacked bar chart showing the time cost (seconds) for different methods across risk indices 1‚Äì27 (except 18) for an agent. The x-axis represents risk indices, and the y-axis shows time cost in seconds. Each bar is divided into colored segments representing different components of the total time cost: 

Pink: Query (target agent) ‚Äì 36.25s per call 
Brown: Code substitution ‚Äì 12.16s per call 
Green: GCG ‚Äì 35.19s per call 
Teal: AutoDAN ‚Äì 15.52s per call 
Blue: AmpleGCG ‚Äì 9.03s per call 
Magenta: Advprompter ‚Äì 96.34s per call 

Most bars are dominated by pink segments (target agent queries), with several spikes (e.g., risk indices 9‚Äì11 and 14‚Äì15) where additional methods like GCG and Advprompter add noticeable time overhead. The legend in the upper right lists each method‚Äôs average time per call. " class="wp-image-1152874" height="789" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure4_.jpg" width="1600" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4: Average time cost for&amp;nbsp;RedCodeAgent&amp;nbsp;to invoke different tools or query the target code agent in successful cases for each risk scenario&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="redcodeagent-discovers-new-vulnerabilities"&gt;RedCodeAgent&amp;nbsp;discovers&amp;nbsp;new&amp;nbsp;vulnerabilities&lt;/h3&gt;



&lt;p&gt;In scenarios where other methods&amp;nbsp;fail to&amp;nbsp;find successful attack strategies,&amp;nbsp;RedCodeAgent&amp;nbsp;is able to discover new, feasible jailbreak approaches. Quantitatively, we find that&amp;nbsp;RedCodeAgent&amp;nbsp;is capable of discovering&amp;nbsp;82 (out of 27*30=810 cases in&amp;nbsp;RedCode-Exec benchmark) unique vulnerabilities on the&amp;nbsp;OpenCodeInterpreter&amp;nbsp;code agent and 78 on the ReAct code agent. These are cases where all baseline methods&amp;nbsp;fail to&amp;nbsp;identify the vulnerability, but RedCodeAgent succeeds.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="summary"&gt;Summary&lt;/h2&gt;



&lt;p&gt;RedCodeAgent&amp;nbsp;combines adaptive memory, specialized tools, and simulated execution environments to uncover real-world risks that static benchmarks&amp;nbsp;may&amp;nbsp;miss.&amp;nbsp;It&amp;nbsp;consistently outperforms leading jailbreak methods, achieving higher attack success rates and lower rejection rates, while remaining efficient and adaptable across diverse agents and programming languages.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/redcodeagent-automatic-red-teaming-agent-against-diverse-code-agents/</guid><pubDate>Tue, 04 Nov 2025 17:00:00 +0000</pubDate></item><item><title>Shopify says AI traffic is up 7x since January, AI-driven orders are up 11x (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/shopify-says-ai-traffic-is-up-7x-since-january-ai-driven-orders-are-up-11x/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/ShopifyIPO2-e1683204509217.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;E-commerce software provider Shopify is bullish on AI-powered shopping agents, citing AI as an ‚Äúincredible tool‚Äù to enable more entrepreneurs and calling it the ‚Äúbiggest shift in technology since the internet‚Äù during its third-quarter earnings call. The company, which partnered with ChatGPT maker OpenAI in September, reported that traffic from AI tools to its online stores is up 7x since January of this year, and purchases attributed to AI-powered search have increased by 11x.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Shopify president Harley Finkelstein, the company‚Äôs advantage in the AI era comes from its ability to access the data from millions of merchants and billions of transactions, and its ‚Äúfounder mode‚Äù mentality to ship products quickly.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This also includes its internal tools, like Scout, which uses AI to help Shopify employees search hundreds of millions of pieces of merchant feedback to make better product decisions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúAnd Scout is just one of many tools we‚Äôre developing to turn our own signals, whether it‚Äôs support tickets, usage data, reviews, social interactions, or even Sidekick prompts, into fast, informed decisions,‚Äù Finkelstein said on the call. ‚ÄúIf you take away one thing from this call, let it be this: AI is not just a feature at Shopify. It is central to our engine that powers everything we build.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to ChatGPT, Shopify is working with Perplexity and Microsoft Copilot on other in-chat shopping experiences. A recent Shopify survey found that 64% of shoppers said they‚Äôre ‚Äúlikely‚Äù to use AI to some extent when making purchases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe‚Äôve been building and investing in this infrastructure to make it really easy to bring shopping into every single AI conversation,‚Äù Finkelstein said. ‚ÄúThe fact that we‚Äôre already working with the leaders in the space should, I think, be a testament to the fact that we want to make sure merchants on Shopify are better prepared than those that are not. It‚Äôs still obviously very, very early,‚Äù he continued. ‚ÄúBut what we‚Äôre really trying to do is lay the rails for agentic commerce.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the company is currently focused on building connections with AI agents, it‚Äôs also prepared for the fact that there will be ‚Äúdifferent permutations‚Äù of how agentic commerce will evolve, Finkelstein also noted, which means it needs to be ready for ‚Äúwhichever path wins.‚Äù&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThat was the same thing when social commerce started to get a lot of attention, or when [people realized it wasn‚Äôt] e-commerce versus physical commerce but&amp;nbsp;‚Ä¶ this idea of commerce everywhere,‚Äù he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Shopify‚Äôs Q3 financial results showed revenue up 32% to $2.84 billion, ahead of estimates, and profit of $264 million, or 20 cents per share. However, the stock sagged on news that the company‚Äôs operating income of $434 million had missed estimates of $437 million.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/ShopifyIPO2-e1683204509217.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;E-commerce software provider Shopify is bullish on AI-powered shopping agents, citing AI as an ‚Äúincredible tool‚Äù to enable more entrepreneurs and calling it the ‚Äúbiggest shift in technology since the internet‚Äù during its third-quarter earnings call. The company, which partnered with ChatGPT maker OpenAI in September, reported that traffic from AI tools to its online stores is up 7x since January of this year, and purchases attributed to AI-powered search have increased by 11x.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Shopify president Harley Finkelstein, the company‚Äôs advantage in the AI era comes from its ability to access the data from millions of merchants and billions of transactions, and its ‚Äúfounder mode‚Äù mentality to ship products quickly.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This also includes its internal tools, like Scout, which uses AI to help Shopify employees search hundreds of millions of pieces of merchant feedback to make better product decisions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúAnd Scout is just one of many tools we‚Äôre developing to turn our own signals, whether it‚Äôs support tickets, usage data, reviews, social interactions, or even Sidekick prompts, into fast, informed decisions,‚Äù Finkelstein said on the call. ‚ÄúIf you take away one thing from this call, let it be this: AI is not just a feature at Shopify. It is central to our engine that powers everything we build.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to ChatGPT, Shopify is working with Perplexity and Microsoft Copilot on other in-chat shopping experiences. A recent Shopify survey found that 64% of shoppers said they‚Äôre ‚Äúlikely‚Äù to use AI to some extent when making purchases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe‚Äôve been building and investing in this infrastructure to make it really easy to bring shopping into every single AI conversation,‚Äù Finkelstein said. ‚ÄúThe fact that we‚Äôre already working with the leaders in the space should, I think, be a testament to the fact that we want to make sure merchants on Shopify are better prepared than those that are not. It‚Äôs still obviously very, very early,‚Äù he continued. ‚ÄúBut what we‚Äôre really trying to do is lay the rails for agentic commerce.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the company is currently focused on building connections with AI agents, it‚Äôs also prepared for the fact that there will be ‚Äúdifferent permutations‚Äù of how agentic commerce will evolve, Finkelstein also noted, which means it needs to be ready for ‚Äúwhichever path wins.‚Äù&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThat was the same thing when social commerce started to get a lot of attention, or when [people realized it wasn‚Äôt] e-commerce versus physical commerce but&amp;nbsp;‚Ä¶ this idea of commerce everywhere,‚Äù he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Shopify‚Äôs Q3 financial results showed revenue up 32% to $2.84 billion, ahead of estimates, and profit of $264 million, or 20 cents per share. However, the stock sagged on news that the company‚Äôs operating income of $434 million had missed estimates of $437 million.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/shopify-says-ai-traffic-is-up-7x-since-january-ai-driven-orders-are-up-11x/</guid><pubDate>Tue, 04 Nov 2025 18:20:55 +0000</pubDate></item><item><title>[NEW] Sora is now available on Android in the US, Canada, and other regions (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/sora-is-now-available-on-android-in-the-us-canada-and-other-regions/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/sora-app-GettyImages-2240278671.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sora, the AI video generator from OpenAI, is now officially available for Android users in the U.S., Canada, Japan, Korea, Taiwan, Thailand, and Vietnam.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Initially launched as an iOS app in September, Sora quickly rose to the top of the App Store charts, amassing over 1 million downloads in a week. With its arrival on the Google Play Store, Sora is expected to attract a larger user base, likely resulting in a surge in downloads.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Android version retains all the features of its iOS counterpart, including the ‚ÄúCameos‚Äù feature, which allows users to generate videos of themselves performing various activities using their own likeness.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The videos can be shared in a feed reminiscent of TikTok, allowing users to discover and engage with content from others. This appears to be a strategic move by OpenAI to strengthen its position in the competitive landscape of short-form video sharing. The AI giant aims to rival major players like Meta, which has recently launched its own AI video feed called Vibes, as well as existing platforms such as TikTok and Instagram.&lt;/p&gt;

&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;The Sora app is now available on Android in:&lt;/p&gt;&lt;p&gt;Canada&lt;br /&gt;Japan&lt;br /&gt;Korea&lt;br /&gt;Taiwan&lt;br /&gt;Thailand&lt;br /&gt;US&lt;br /&gt;Vietnam pic.twitter.com/wmx5KU4VM1&lt;/p&gt;‚Äî Sora (@soraofficialapp) November 4, 2025&lt;/blockquote&gt; 

&lt;p class="wp-block-paragraph"&gt;However, the app has faced criticism for its handling of deepfakes. After its initial launch, users began uploading disrespectful videos of historical figures, including Martin Luther King Jr. As a result, Sora paused the generation of content depicting Dr. King last month and strengthened its guardrails.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also recently addressed the backlash surrounding copyrighted characters, such as SpongeBob and Pikachu, by changing its policy for the Sora app from an ‚Äúopt-out‚Äù approach to an ‚Äúopt-in‚Äù system for rights holders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, it‚Äôs currently involved in a legal dispute with celebrity video maker Cameo regarding the name of Sora‚Äôs flagship feature, ‚ÄúCameo.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Looking ahead, OpenAI plans to introduce additional features to Sora. These include character cameos, letting users create AI-generated videos featuring their pets and inanimate objects. Basic video editing tools are also on the way, including the ability to stitch multiple clips together. Sora also plans to help users customize their social feeds, focusing on content from selected individuals rather than a large audience.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/sora-app-GettyImages-2240278671.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sora, the AI video generator from OpenAI, is now officially available for Android users in the U.S., Canada, Japan, Korea, Taiwan, Thailand, and Vietnam.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Initially launched as an iOS app in September, Sora quickly rose to the top of the App Store charts, amassing over 1 million downloads in a week. With its arrival on the Google Play Store, Sora is expected to attract a larger user base, likely resulting in a surge in downloads.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Android version retains all the features of its iOS counterpart, including the ‚ÄúCameos‚Äù feature, which allows users to generate videos of themselves performing various activities using their own likeness.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The videos can be shared in a feed reminiscent of TikTok, allowing users to discover and engage with content from others. This appears to be a strategic move by OpenAI to strengthen its position in the competitive landscape of short-form video sharing. The AI giant aims to rival major players like Meta, which has recently launched its own AI video feed called Vibes, as well as existing platforms such as TikTok and Instagram.&lt;/p&gt;

&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;The Sora app is now available on Android in:&lt;/p&gt;&lt;p&gt;Canada&lt;br /&gt;Japan&lt;br /&gt;Korea&lt;br /&gt;Taiwan&lt;br /&gt;Thailand&lt;br /&gt;US&lt;br /&gt;Vietnam pic.twitter.com/wmx5KU4VM1&lt;/p&gt;‚Äî Sora (@soraofficialapp) November 4, 2025&lt;/blockquote&gt; 

&lt;p class="wp-block-paragraph"&gt;However, the app has faced criticism for its handling of deepfakes. After its initial launch, users began uploading disrespectful videos of historical figures, including Martin Luther King Jr. As a result, Sora paused the generation of content depicting Dr. King last month and strengthened its guardrails.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also recently addressed the backlash surrounding copyrighted characters, such as SpongeBob and Pikachu, by changing its policy for the Sora app from an ‚Äúopt-out‚Äù approach to an ‚Äúopt-in‚Äù system for rights holders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, it‚Äôs currently involved in a legal dispute with celebrity video maker Cameo regarding the name of Sora‚Äôs flagship feature, ‚ÄúCameo.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Looking ahead, OpenAI plans to introduce additional features to Sora. These include character cameos, letting users create AI-generated videos featuring their pets and inanimate objects. Basic video editing tools are also on the way, including the ability to stitch multiple clips together. Sora also plans to help users customize their social feeds, focusing on content from selected individuals rather than a large audience.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/sora-is-now-available-on-android-in-the-us-canada-and-other-regions/</guid><pubDate>Tue, 04 Nov 2025 19:35:51 +0000</pubDate></item><item><title>[NEW] Attention ISN'T all you need?! New Qwen3 variant Brumby-14B-Base leverages Power Retention technique (AI | VentureBeat)</title><link>https://venturebeat.com/ai/attention-isnt-all-you-need-new-qwen3-variant-brumby-14b-base-leverages</link><description>[unable to retrieve full-text content]&lt;p&gt;When the transformer architecture was introduced in 2017 in the now seminal Google paper &amp;quot;&lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need&lt;/a&gt;,&amp;quot; it became an instant cornerstone of modern artificial intelligence. &lt;/p&gt;&lt;p&gt;Every major large language model (LLM) ‚Äî from OpenAI&amp;#x27;s GPT series to Anthropic&amp;#x27;s Claude, Google&amp;#x27;s Gemini, and Meta&amp;#x27;s Llama ‚Äî has been built on some variation of its central mechanism: &lt;b&gt;attention&lt;/b&gt;, the mathematical operation that allows a model to look back across its entire input and decide what information matters most.&lt;/p&gt;&lt;p&gt;Eight years later, the same mechanism that defined AI‚Äôs golden age is now showing its limits. Attention is powerful, but it is also expensive ‚Äî its computational and memory costs scale quadratically with context length, creating an increasingly unsustainable bottleneck for both research and industry. As models aim to reason across documents, codebases, or video streams lasting hours or days, attention becomes the architecture‚Äôs Achilles‚Äô heel.&lt;/p&gt;&lt;p&gt;On October 28, 2025, the little-known AI startup &lt;a href="https://manifestai.com/articles/release-brumby-14b/"&gt;Manifest AI introduced a radical alternative&lt;/a&gt;. Their new model, &lt;b&gt;Brumby-14B-Base&lt;/b&gt;, is a &lt;b&gt;retrained variant of Qwen3-14B-Base&lt;/b&gt;, one of the leading open-source transformer models.&lt;/p&gt;&lt;p&gt;But while many variants of Qwen have been trained already, Brumby-14B-Base is novel in that it abandons attention altogether. &lt;/p&gt;&lt;p&gt;Instead, Brumby replaces those layers with a novel mechanism called &lt;b&gt;Power Retention&lt;/b&gt;‚Äîa recurrent, hardware-efficient architecture that stores and updates information over arbitrarily long contexts without the exponential memory growth of attention.&lt;/p&gt;&lt;p&gt;Trained at a stated cost of just $4,000, the 14-billion-parameter Brumby model performs on par with established transformer models like Qwen3-14B and GLM-4.5-Air, achieving near-state-of-the-art accuracy on a range of reasoning and comprehension benchmarks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Attention to Retention: The Architectural Shift&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The core of Manifest AI‚Äôs innovation lies in what they call the Power Retention layer. &lt;/p&gt;&lt;p&gt;In a traditional transformer, every token computes a set of queries (Q), keys (K), and values (V), then performs a matrix operation that measures the similarity between every token and every other token‚Äîessentially a full pairwise comparison across the sequence. &lt;/p&gt;&lt;p&gt;This is what gives attention its flexibility, but also what makes it so costly: processing a sequence twice as long takes roughly four times the compute and memory.&lt;/p&gt;&lt;p&gt;Power Retention keeps the same inputs (Q, K, V), but replaces the global similarity operation with a recurrent state update. &lt;/p&gt;&lt;p&gt;Each layer maintains a memory matrix S, which is updated at each time step according to the incoming key, value, and a learned gating signal. &lt;/p&gt;&lt;p&gt;The process looks more like an RNN (Recurrent Neural Network) than a transformer: instead of recomputing attention over the entire context, the model continuously compresses past information into a fixed-size latent state.&lt;/p&gt;&lt;p&gt;This means the computational cost of Power Retention &lt;b&gt;does not grow with context length&lt;/b&gt;. Whether the model is processing 1,000 or 1,000,000 tokens, the &lt;b&gt;per-token cost remains constant.&lt;/b&gt; &lt;/p&gt;&lt;p&gt;That property alone‚Äîconstant-time per-token computation‚Äîmarks a profound departure from transformer behavior.&lt;/p&gt;&lt;p&gt;At the same time, Power Retention preserves the expressive power that made attention successful. Because the recurrence involves tensor powers of the input (hence the name ‚Äúpower retention‚Äù), it can represent higher-order dependencies between past and present tokens. &lt;/p&gt;&lt;p&gt;The result is an architecture that can theoretically retain long-term dependencies indefinitely, while remaining as efficient as an RNN and as expressive as a transformer.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Retraining, Not Rebuilding&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the most striking aspect of Brumby-14B‚Äôs training process is its efficiency. Manifest AI trained the model for only 60 hours on 32 Nvidia H100 GPUs, at a cost of roughly $4,000 ‚Äî less than 2% of what a conventional model of this scale would cost to train from scratch.&lt;/p&gt;&lt;p&gt;However, since it relied on a transformer-based model, it&amp;#x27;s safe to say that this advance alone will not end the transformer AI-era.&lt;/p&gt;&lt;p&gt;As Jacob Buckman, founder of Manifest AI, clarified in an email to VentureBeat: ‚ÄúThe ability to train for $4,000 is indeed only possible when leveraging an existing transformer model,‚Äù he said. ‚ÄúBrumby could not be trained from scratch for that price.‚Äù&lt;/p&gt;&lt;p&gt;Still, Buckman emphasized the significance of that result: ‚ÄúThe reason this is important is that the ability to build on the weights of the previous generation of model architectures is a critical accelerant for the adoption of a new modeling paradigm.‚Äù &lt;/p&gt;&lt;p&gt;He argues this demonstrates how attention-free systems can catch up to transformer performance ‚Äúfor orders-of-magnitude less‚Äù investment.&lt;/p&gt;&lt;p&gt;In the loss curves released by Manifest AI, Brumby‚Äôs training loss quickly converges to that of the Qwen3 baseline within 3,000 training steps, even as the architecture diverges significantly from its transformer origins. &lt;/p&gt;&lt;p&gt;Although Brumby-14B-Base began life as Qwen3-14B-Base, it did not remain identical for long. Manifest AI fundamentally altered Qwen3‚Äôs architecture by removing its attention layers‚Äîthe mathematical engine that defines how a transformer model processes information‚Äîand replacing them with their new ‚Äúpower retention‚Äù mechanism. This change restructured the model‚Äôs internal wiring, effectively giving it a new brain while preserving much of its prior knowledge.&lt;/p&gt;&lt;p&gt;Because of that architectural swap, the existing Qwen3 weights no longer fit perfectly. They were trained to operate within a transformer‚Äôs attention dynamics, not the new retention-based system. As a result, the Brumby model initially ‚Äúforgot‚Äù how to apply some of its learned knowledge effectively. The retraining process‚Äîabout &lt;b&gt;3,000 steps&lt;/b&gt; of additional learning‚Äîserved to recalibrate those weights, aligning them with the power retention framework without having to start from zero.&lt;/p&gt;&lt;p&gt;A helpful way to think about this is to imagine taking a world-class pianist and handing them a guitar. They already understand rhythm, harmony, and melody, but their hands must learn entirely new patterns to produce the same music. Similarly, Brumby had to relearn how to use its existing knowledge through a new computational instrument. Those 3,000 training steps were, in effect, its crash course in guitar lessons.&lt;/p&gt;&lt;p&gt;By the end of this short retraining phase, Brumby had regained its full performance, reaching the same accuracy as the original Qwen3 model. That quick recovery is what makes the result so significant: it shows that an attention-free system can inherit and adapt the capabilities of a transformer model with only a fraction of the training time and cost.&lt;/p&gt;&lt;p&gt;The benchmark progression plots show a similar trend: the model rapidly approaches its target accuracy on core evaluations like GSM8K, HellaSwag, and MMLU after only a few thousand steps, matching or even slightly surpassing Qwen3 on several tasks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Benchmarking the Brumby&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Across standard evaluation tasks, &lt;b&gt;Brumby-14B-Base&lt;/b&gt; consistently performs at or near parity with transformer baselines of comparable scale.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Task&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Brumby-14B&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Qwen3-14B&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM-4.5-Air&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Nemotron Nano (12B)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ARC&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.89&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.94&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.92&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.93&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GSM8K&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.88&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.84&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.83&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.84&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GSM8K (Platinum)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.87&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.88&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.87&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;HellaSwag&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.77&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.81&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.82&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MATH&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.62&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.54&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.47&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.26&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MBPP&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.57&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.73&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.71&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MMLU&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.71&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.78&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.77&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.78&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MMLU (Pro)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.36&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.55&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.51&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.53&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;While it lags slightly behind transformers on knowledge-heavy evaluations like &lt;b&gt;MMLU-Pro&lt;/b&gt;, it matches or outperforms them on &lt;b&gt;mathematical reasoning&lt;/b&gt; and &lt;b&gt;long-context reasoning&lt;/b&gt; tasks‚Äîprecisely where attention architectures tend to falter. This pattern reinforces the idea that recurrent or retention-based systems may hold a structural advantage for reasoning over extended temporal or logical dependencies.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Hardware Efficiency and Inference Performance&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Brumby‚Äôs power retention design offers another major advantage: hardware efficiency.&lt;/p&gt;&lt;p&gt;Because the state update involves only local matrix operations, inference can be implemented with linear complexity in sequence length. &lt;/p&gt;&lt;p&gt;Manifest AI reports that their fastest kernels, developed through their in-house CUDA framework Vidrial, can deliver hundreds-fold speedups over attention on very long contexts.&lt;/p&gt;&lt;p&gt;Buckman said the alpha-stage Power Retention kernels ‚Äúachieve typical hardware utilization of 80‚Äì85%, which is higher than FlashAttention2‚Äôs 70‚Äì75% or Mamba‚Äôs 50‚Äì60%.‚Äù &lt;/p&gt;&lt;p&gt;(Mamba is another emerging ‚Äúpost-transformer‚Äù architecture developed by Carnegie Mellon scientists back in 2023 that, like Power Retention, seeks to eliminate the computational bottleneck of attention. It replaces attention with a &lt;i&gt;state-space&lt;/i&gt; mechanism that processes sequences linearly ‚Äî updating an internal state over time rather than comparing every token to every other one. This makes it far more efficient for long inputs, though it typically achieves lower hardware utilization than Power Retention in early tests.)&lt;/p&gt;&lt;p&gt;Both Power Retention and Mamba, he added, ‚Äúexpend meaningfully fewer total FLOPs than FlashAttention2 on long contexts, as well as far less memory.‚Äù &lt;/p&gt;&lt;p&gt;According to Buckman, the reported 100√ó speedup comes from this combined improvement in utilization and computational efficiency, though he noted that ‚Äúwe have not yet stress-tested it on production-scale workloads.‚Äù&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Training and Scaling Economics&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps no statistic in the Brumby release generated more attention than the training cost.&lt;/p&gt;&lt;p&gt;A 14-billion-parameter model, trained for $4,000, represents a two-order-of-magnitude reduction in the cost of foundation model development.&lt;/p&gt;&lt;p&gt;Buckman confirmed that the low cost reflects a broader scaling pattern. ‚ÄúFar from diminishing returns, we have found that ease of retraining improves with scale,‚Äù he said. ‚ÄúThe number of steps required to successfully retrain a model decreases with its parameter count.‚Äù &lt;/p&gt;&lt;p&gt;Manifest has not yet validated the cost of retraining models at 700B parameters, but Buckman projected a range of $10,000‚Äì$20,000 for models of that magnitude‚Äîstill far below transformer training budgets.&lt;/p&gt;&lt;p&gt;He also reiterated that this approach could democratize large-scale experimentation by allowing smaller research groups or companies to retrain or repurpose existing transformer checkpoints without prohibitive compute costs.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Integration and Deployment&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;According to Buckman, converting an existing transformer into a Power Retention model is designed to be simple. &lt;/p&gt;&lt;p&gt;‚ÄúIt is straightforward for any company that is already retraining, post-training, or fine-tuning open-source models,‚Äù he said. ‚ÄúSimply pip install retention, change one line of your architecture code, and resume training where you left off.‚Äù&lt;/p&gt;&lt;p&gt;He added that after only a small number of GPU-hours, the model typically recovers its original performance‚Äîat which point it gains the efficiency benefits of the attention-free design. &lt;/p&gt;&lt;p&gt;‚ÄúThe resulting architecture will permit far faster long-context training and inference than previously,‚Äù Buckman noted.&lt;/p&gt;&lt;p&gt;On infrastructure, Buckman said the main Brumby kernels are written in Triton, compatible with both NVIDIA and AMD accelerators. Specialized CUDA kernels are also available through the team‚Äôs in-house Vidrial framework. Integration with vLLM and other inference engines remains a work in progress: ‚ÄúWe have not yet integrated Power Retention into inference engines, but doing so is a major ongoing initiative at Manifest.‚Äù&lt;/p&gt;&lt;p&gt;As for distributed inference, Buckman dismissed concerns about instability: ‚ÄúWe have not found this difficulty to be exacerbated in any way by our recurrent-state architecture. In fact, context-parallel training and GPU partitioning for multi-user inference both become significantly cleaner technically when using our approach.‚Äù&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Mission and Long-Term Vision&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Beyond the engineering details, Buckman also described Manifest‚Äôs broader mission. ‚ÄúOur mission is to train a neural network to model all human output,‚Äù he said. &lt;/p&gt;&lt;p&gt;The team‚Äôs goal, he explained, is to move beyond modeling ‚Äúartifacts of intelligence‚Äù toward modeling ‚Äúthe intelligent processes that generated them.‚Äù This shift, he argued, requires ‚Äúfundamentally rethinking‚Äù how models are designed and trained‚Äîwork that Power Retention represents only the beginning of.&lt;/p&gt;&lt;p&gt;The Brumby-14B release, he said, is ‚Äúone step forward in a long march‚Äù toward architectures that can model thought processes continuously and efficiently.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Public Debate and Industry Reception&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The launch of Brumby-14B sparked immediate discussion on X (formerly Twitter), where researchers debated the framing of Manifest AI‚Äôs announcement. &lt;/p&gt;&lt;p&gt;Some, including Meta researcher &lt;a href="https://x.com/redtachyon/status/1983819957583421606"&gt;Ariel (@redtachyon)&lt;/a&gt;, argued that the ‚Äú$4,000 foundation model‚Äù tagline was misleading, since the training involved reusing pretrained transformer weights rather than training from scratch.&lt;/p&gt;&lt;p&gt;‚ÄúThey shuffled around the weights of Qwen, fine-tuned it a bit, and called it ‚Äòtraining a foundation model for $4k,‚Äô‚Äù Ariel &lt;a href="https://x.com/redtachyon/status/1983820872461722075"&gt;wrote&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Buckman responded publicly, clarifying that the initial tweet had been part of a longer thread explaining the retraining approach. ‚ÄúIt‚Äôs not like I was being deceptive about it,‚Äù he &lt;a href="https://x.com/jacobmbuckman/status/1983875586175996413"&gt;wrote&lt;/a&gt;. ‚ÄúI broke it up into separate tweets, and now everyone is mad about the first one.‚Äù&lt;/p&gt;&lt;p&gt;In a follow-up email, Buckman took a measured view of the controversy. ‚ÄúThe end of the transformer era is not yet here,‚Äù he reiterated, ‚Äúbut the march has begun.‚Äù &lt;/p&gt;&lt;p&gt;He also acknowledged that the $4,000 claim, though technically accurate in context, had drawn attention precisely because it challenged expectations about what it costs to experiment at frontier scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Conclusion: A Crack in the Transformer‚Äôs Wall?&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of Brumby-14B-Base is more than an engineering milestone; it is a proof of concept that the transformer‚Äôs dominance may finally face credible competition. &lt;/p&gt;&lt;p&gt;By replacing attention with power retention, Manifest AI has demonstrated that performance parity with state-of-the-art transformers is possible at a fraction of the computational cost‚Äîand that the long-context bottleneck can be broken without exotic hardware.&lt;/p&gt;&lt;p&gt;The broader implications are twofold. First, the economics of training and serving large models could shift dramatically, lowering the barrier to entry for open research and smaller organizations. &lt;/p&gt;&lt;p&gt;Second, the architectural diversity of AI models may expand again, reigniting theoretical and empirical exploration after half a decade of transformer monoculture.&lt;/p&gt;&lt;p&gt;As Buckman put it: ‚ÄúThe end of the transformer era is not yet here. Our release is just one step forward in a long march toward the future.‚Äù&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;When the transformer architecture was introduced in 2017 in the now seminal Google paper &amp;quot;&lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need&lt;/a&gt;,&amp;quot; it became an instant cornerstone of modern artificial intelligence. &lt;/p&gt;&lt;p&gt;Every major large language model (LLM) ‚Äî from OpenAI&amp;#x27;s GPT series to Anthropic&amp;#x27;s Claude, Google&amp;#x27;s Gemini, and Meta&amp;#x27;s Llama ‚Äî has been built on some variation of its central mechanism: &lt;b&gt;attention&lt;/b&gt;, the mathematical operation that allows a model to look back across its entire input and decide what information matters most.&lt;/p&gt;&lt;p&gt;Eight years later, the same mechanism that defined AI‚Äôs golden age is now showing its limits. Attention is powerful, but it is also expensive ‚Äî its computational and memory costs scale quadratically with context length, creating an increasingly unsustainable bottleneck for both research and industry. As models aim to reason across documents, codebases, or video streams lasting hours or days, attention becomes the architecture‚Äôs Achilles‚Äô heel.&lt;/p&gt;&lt;p&gt;On October 28, 2025, the little-known AI startup &lt;a href="https://manifestai.com/articles/release-brumby-14b/"&gt;Manifest AI introduced a radical alternative&lt;/a&gt;. Their new model, &lt;b&gt;Brumby-14B-Base&lt;/b&gt;, is a &lt;b&gt;retrained variant of Qwen3-14B-Base&lt;/b&gt;, one of the leading open-source transformer models.&lt;/p&gt;&lt;p&gt;But while many variants of Qwen have been trained already, Brumby-14B-Base is novel in that it abandons attention altogether. &lt;/p&gt;&lt;p&gt;Instead, Brumby replaces those layers with a novel mechanism called &lt;b&gt;Power Retention&lt;/b&gt;‚Äîa recurrent, hardware-efficient architecture that stores and updates information over arbitrarily long contexts without the exponential memory growth of attention.&lt;/p&gt;&lt;p&gt;Trained at a stated cost of just $4,000, the 14-billion-parameter Brumby model performs on par with established transformer models like Qwen3-14B and GLM-4.5-Air, achieving near-state-of-the-art accuracy on a range of reasoning and comprehension benchmarks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Attention to Retention: The Architectural Shift&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The core of Manifest AI‚Äôs innovation lies in what they call the Power Retention layer. &lt;/p&gt;&lt;p&gt;In a traditional transformer, every token computes a set of queries (Q), keys (K), and values (V), then performs a matrix operation that measures the similarity between every token and every other token‚Äîessentially a full pairwise comparison across the sequence. &lt;/p&gt;&lt;p&gt;This is what gives attention its flexibility, but also what makes it so costly: processing a sequence twice as long takes roughly four times the compute and memory.&lt;/p&gt;&lt;p&gt;Power Retention keeps the same inputs (Q, K, V), but replaces the global similarity operation with a recurrent state update. &lt;/p&gt;&lt;p&gt;Each layer maintains a memory matrix S, which is updated at each time step according to the incoming key, value, and a learned gating signal. &lt;/p&gt;&lt;p&gt;The process looks more like an RNN (Recurrent Neural Network) than a transformer: instead of recomputing attention over the entire context, the model continuously compresses past information into a fixed-size latent state.&lt;/p&gt;&lt;p&gt;This means the computational cost of Power Retention &lt;b&gt;does not grow with context length&lt;/b&gt;. Whether the model is processing 1,000 or 1,000,000 tokens, the &lt;b&gt;per-token cost remains constant.&lt;/b&gt; &lt;/p&gt;&lt;p&gt;That property alone‚Äîconstant-time per-token computation‚Äîmarks a profound departure from transformer behavior.&lt;/p&gt;&lt;p&gt;At the same time, Power Retention preserves the expressive power that made attention successful. Because the recurrence involves tensor powers of the input (hence the name ‚Äúpower retention‚Äù), it can represent higher-order dependencies between past and present tokens. &lt;/p&gt;&lt;p&gt;The result is an architecture that can theoretically retain long-term dependencies indefinitely, while remaining as efficient as an RNN and as expressive as a transformer.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Retraining, Not Rebuilding&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the most striking aspect of Brumby-14B‚Äôs training process is its efficiency. Manifest AI trained the model for only 60 hours on 32 Nvidia H100 GPUs, at a cost of roughly $4,000 ‚Äî less than 2% of what a conventional model of this scale would cost to train from scratch.&lt;/p&gt;&lt;p&gt;However, since it relied on a transformer-based model, it&amp;#x27;s safe to say that this advance alone will not end the transformer AI-era.&lt;/p&gt;&lt;p&gt;As Jacob Buckman, founder of Manifest AI, clarified in an email to VentureBeat: ‚ÄúThe ability to train for $4,000 is indeed only possible when leveraging an existing transformer model,‚Äù he said. ‚ÄúBrumby could not be trained from scratch for that price.‚Äù&lt;/p&gt;&lt;p&gt;Still, Buckman emphasized the significance of that result: ‚ÄúThe reason this is important is that the ability to build on the weights of the previous generation of model architectures is a critical accelerant for the adoption of a new modeling paradigm.‚Äù &lt;/p&gt;&lt;p&gt;He argues this demonstrates how attention-free systems can catch up to transformer performance ‚Äúfor orders-of-magnitude less‚Äù investment.&lt;/p&gt;&lt;p&gt;In the loss curves released by Manifest AI, Brumby‚Äôs training loss quickly converges to that of the Qwen3 baseline within 3,000 training steps, even as the architecture diverges significantly from its transformer origins. &lt;/p&gt;&lt;p&gt;Although Brumby-14B-Base began life as Qwen3-14B-Base, it did not remain identical for long. Manifest AI fundamentally altered Qwen3‚Äôs architecture by removing its attention layers‚Äîthe mathematical engine that defines how a transformer model processes information‚Äîand replacing them with their new ‚Äúpower retention‚Äù mechanism. This change restructured the model‚Äôs internal wiring, effectively giving it a new brain while preserving much of its prior knowledge.&lt;/p&gt;&lt;p&gt;Because of that architectural swap, the existing Qwen3 weights no longer fit perfectly. They were trained to operate within a transformer‚Äôs attention dynamics, not the new retention-based system. As a result, the Brumby model initially ‚Äúforgot‚Äù how to apply some of its learned knowledge effectively. The retraining process‚Äîabout &lt;b&gt;3,000 steps&lt;/b&gt; of additional learning‚Äîserved to recalibrate those weights, aligning them with the power retention framework without having to start from zero.&lt;/p&gt;&lt;p&gt;A helpful way to think about this is to imagine taking a world-class pianist and handing them a guitar. They already understand rhythm, harmony, and melody, but their hands must learn entirely new patterns to produce the same music. Similarly, Brumby had to relearn how to use its existing knowledge through a new computational instrument. Those 3,000 training steps were, in effect, its crash course in guitar lessons.&lt;/p&gt;&lt;p&gt;By the end of this short retraining phase, Brumby had regained its full performance, reaching the same accuracy as the original Qwen3 model. That quick recovery is what makes the result so significant: it shows that an attention-free system can inherit and adapt the capabilities of a transformer model with only a fraction of the training time and cost.&lt;/p&gt;&lt;p&gt;The benchmark progression plots show a similar trend: the model rapidly approaches its target accuracy on core evaluations like GSM8K, HellaSwag, and MMLU after only a few thousand steps, matching or even slightly surpassing Qwen3 on several tasks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Benchmarking the Brumby&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Across standard evaluation tasks, &lt;b&gt;Brumby-14B-Base&lt;/b&gt; consistently performs at or near parity with transformer baselines of comparable scale.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Task&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Brumby-14B&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Qwen3-14B&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM-4.5-Air&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Nemotron Nano (12B)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ARC&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.89&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.94&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.92&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.93&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GSM8K&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.88&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.84&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.83&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.84&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GSM8K (Platinum)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.87&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.88&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.87&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;HellaSwag&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.77&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.81&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.82&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MATH&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.62&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.54&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.47&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.26&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MBPP&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.57&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.73&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.71&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MMLU&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.71&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.78&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.77&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.78&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MMLU (Pro)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.36&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.55&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.51&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;0.53&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;While it lags slightly behind transformers on knowledge-heavy evaluations like &lt;b&gt;MMLU-Pro&lt;/b&gt;, it matches or outperforms them on &lt;b&gt;mathematical reasoning&lt;/b&gt; and &lt;b&gt;long-context reasoning&lt;/b&gt; tasks‚Äîprecisely where attention architectures tend to falter. This pattern reinforces the idea that recurrent or retention-based systems may hold a structural advantage for reasoning over extended temporal or logical dependencies.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Hardware Efficiency and Inference Performance&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Brumby‚Äôs power retention design offers another major advantage: hardware efficiency.&lt;/p&gt;&lt;p&gt;Because the state update involves only local matrix operations, inference can be implemented with linear complexity in sequence length. &lt;/p&gt;&lt;p&gt;Manifest AI reports that their fastest kernels, developed through their in-house CUDA framework Vidrial, can deliver hundreds-fold speedups over attention on very long contexts.&lt;/p&gt;&lt;p&gt;Buckman said the alpha-stage Power Retention kernels ‚Äúachieve typical hardware utilization of 80‚Äì85%, which is higher than FlashAttention2‚Äôs 70‚Äì75% or Mamba‚Äôs 50‚Äì60%.‚Äù &lt;/p&gt;&lt;p&gt;(Mamba is another emerging ‚Äúpost-transformer‚Äù architecture developed by Carnegie Mellon scientists back in 2023 that, like Power Retention, seeks to eliminate the computational bottleneck of attention. It replaces attention with a &lt;i&gt;state-space&lt;/i&gt; mechanism that processes sequences linearly ‚Äî updating an internal state over time rather than comparing every token to every other one. This makes it far more efficient for long inputs, though it typically achieves lower hardware utilization than Power Retention in early tests.)&lt;/p&gt;&lt;p&gt;Both Power Retention and Mamba, he added, ‚Äúexpend meaningfully fewer total FLOPs than FlashAttention2 on long contexts, as well as far less memory.‚Äù &lt;/p&gt;&lt;p&gt;According to Buckman, the reported 100√ó speedup comes from this combined improvement in utilization and computational efficiency, though he noted that ‚Äúwe have not yet stress-tested it on production-scale workloads.‚Äù&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Training and Scaling Economics&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps no statistic in the Brumby release generated more attention than the training cost.&lt;/p&gt;&lt;p&gt;A 14-billion-parameter model, trained for $4,000, represents a two-order-of-magnitude reduction in the cost of foundation model development.&lt;/p&gt;&lt;p&gt;Buckman confirmed that the low cost reflects a broader scaling pattern. ‚ÄúFar from diminishing returns, we have found that ease of retraining improves with scale,‚Äù he said. ‚ÄúThe number of steps required to successfully retrain a model decreases with its parameter count.‚Äù &lt;/p&gt;&lt;p&gt;Manifest has not yet validated the cost of retraining models at 700B parameters, but Buckman projected a range of $10,000‚Äì$20,000 for models of that magnitude‚Äîstill far below transformer training budgets.&lt;/p&gt;&lt;p&gt;He also reiterated that this approach could democratize large-scale experimentation by allowing smaller research groups or companies to retrain or repurpose existing transformer checkpoints without prohibitive compute costs.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Integration and Deployment&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;According to Buckman, converting an existing transformer into a Power Retention model is designed to be simple. &lt;/p&gt;&lt;p&gt;‚ÄúIt is straightforward for any company that is already retraining, post-training, or fine-tuning open-source models,‚Äù he said. ‚ÄúSimply pip install retention, change one line of your architecture code, and resume training where you left off.‚Äù&lt;/p&gt;&lt;p&gt;He added that after only a small number of GPU-hours, the model typically recovers its original performance‚Äîat which point it gains the efficiency benefits of the attention-free design. &lt;/p&gt;&lt;p&gt;‚ÄúThe resulting architecture will permit far faster long-context training and inference than previously,‚Äù Buckman noted.&lt;/p&gt;&lt;p&gt;On infrastructure, Buckman said the main Brumby kernels are written in Triton, compatible with both NVIDIA and AMD accelerators. Specialized CUDA kernels are also available through the team‚Äôs in-house Vidrial framework. Integration with vLLM and other inference engines remains a work in progress: ‚ÄúWe have not yet integrated Power Retention into inference engines, but doing so is a major ongoing initiative at Manifest.‚Äù&lt;/p&gt;&lt;p&gt;As for distributed inference, Buckman dismissed concerns about instability: ‚ÄúWe have not found this difficulty to be exacerbated in any way by our recurrent-state architecture. In fact, context-parallel training and GPU partitioning for multi-user inference both become significantly cleaner technically when using our approach.‚Äù&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Mission and Long-Term Vision&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Beyond the engineering details, Buckman also described Manifest‚Äôs broader mission. ‚ÄúOur mission is to train a neural network to model all human output,‚Äù he said. &lt;/p&gt;&lt;p&gt;The team‚Äôs goal, he explained, is to move beyond modeling ‚Äúartifacts of intelligence‚Äù toward modeling ‚Äúthe intelligent processes that generated them.‚Äù This shift, he argued, requires ‚Äúfundamentally rethinking‚Äù how models are designed and trained‚Äîwork that Power Retention represents only the beginning of.&lt;/p&gt;&lt;p&gt;The Brumby-14B release, he said, is ‚Äúone step forward in a long march‚Äù toward architectures that can model thought processes continuously and efficiently.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Public Debate and Industry Reception&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The launch of Brumby-14B sparked immediate discussion on X (formerly Twitter), where researchers debated the framing of Manifest AI‚Äôs announcement. &lt;/p&gt;&lt;p&gt;Some, including Meta researcher &lt;a href="https://x.com/redtachyon/status/1983819957583421606"&gt;Ariel (@redtachyon)&lt;/a&gt;, argued that the ‚Äú$4,000 foundation model‚Äù tagline was misleading, since the training involved reusing pretrained transformer weights rather than training from scratch.&lt;/p&gt;&lt;p&gt;‚ÄúThey shuffled around the weights of Qwen, fine-tuned it a bit, and called it ‚Äòtraining a foundation model for $4k,‚Äô‚Äù Ariel &lt;a href="https://x.com/redtachyon/status/1983820872461722075"&gt;wrote&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Buckman responded publicly, clarifying that the initial tweet had been part of a longer thread explaining the retraining approach. ‚ÄúIt‚Äôs not like I was being deceptive about it,‚Äù he &lt;a href="https://x.com/jacobmbuckman/status/1983875586175996413"&gt;wrote&lt;/a&gt;. ‚ÄúI broke it up into separate tweets, and now everyone is mad about the first one.‚Äù&lt;/p&gt;&lt;p&gt;In a follow-up email, Buckman took a measured view of the controversy. ‚ÄúThe end of the transformer era is not yet here,‚Äù he reiterated, ‚Äúbut the march has begun.‚Äù &lt;/p&gt;&lt;p&gt;He also acknowledged that the $4,000 claim, though technically accurate in context, had drawn attention precisely because it challenged expectations about what it costs to experiment at frontier scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Conclusion: A Crack in the Transformer‚Äôs Wall?&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of Brumby-14B-Base is more than an engineering milestone; it is a proof of concept that the transformer‚Äôs dominance may finally face credible competition. &lt;/p&gt;&lt;p&gt;By replacing attention with power retention, Manifest AI has demonstrated that performance parity with state-of-the-art transformers is possible at a fraction of the computational cost‚Äîand that the long-context bottleneck can be broken without exotic hardware.&lt;/p&gt;&lt;p&gt;The broader implications are twofold. First, the economics of training and serving large models could shift dramatically, lowering the barrier to entry for open research and smaller organizations. &lt;/p&gt;&lt;p&gt;Second, the architectural diversity of AI models may expand again, reigniting theoretical and empirical exploration after half a decade of transformer monoculture.&lt;/p&gt;&lt;p&gt;As Buckman put it: ‚ÄúThe end of the transformer era is not yet here. Our release is just one step forward in a long march toward the future.‚Äù&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/attention-isnt-all-you-need-new-qwen3-variant-brumby-14b-base-leverages</guid><pubDate>Tue, 04 Nov 2025 19:37:00 +0000</pubDate></item><item><title>[NEW] Databricks research reveals that building better AI judges isn't just a technical concern, it's a people problem (AI | VentureBeat)</title><link>https://venturebeat.com/ai/databricks-research-reveals-that-building-better-ai-judges-isnt-just-a</link><description>[unable to retrieve full-text content]&lt;p&gt;The intelligence of AI models isn&amp;#x27;t what&amp;#x27;s blocking enterprise deployments. It&amp;#x27;s the inability to define and measure quality in the first place.&lt;/p&gt;&lt;p&gt;That&amp;#x27;s where AI judges are now playing an increasingly important role. In AI evaluation, a &amp;quot;judge&amp;quot; is an AI system that scores outputs from another AI system.¬†&lt;/p&gt;&lt;p&gt;Judge Builder is Databricks&amp;#x27; framework for creating judges and was first deployed as part of the company&amp;#x27;s&lt;a href="https://venturebeat.com/ai/why-most-enterprise-ai-agents-never-reach-production-and-how-databricks-plans-t"&gt; &lt;u&gt;Agent Bricks&lt;/u&gt;&lt;/a&gt; technology earlier this year. The framework has evolved significantly since its initial launch in response to direct user feedback and deployments.&lt;/p&gt;&lt;p&gt;Early versions focused on technical implementation but customer feedback revealed the real bottleneck was organizational alignment. Databricks now offers a structured workshop process that guides teams through three core challenges: getting stakeholders to agree on quality criteria, capturing domain expertise from limited subject matter experts and deploying evaluation systems at scale.&lt;/p&gt;&lt;p&gt;&amp;quot;The intelligence of the model is typically not the bottleneck, the models are really smart,&amp;quot; Jonathan Frankle, Databricks&amp;#x27; chief AI scientist, told VentureBeat in an exclusive briefing. &amp;quot;Instead, it&amp;#x27;s really about asking, how do we get the models to do what we want, and how do we know if they did what we wanted?&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The &amp;#x27;Ouroboros problem&amp;#x27; of AI evaluation&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Judge Builder addresses what Pallavi Koppol, a Databricks research scientist who led the development, calls the &amp;quot;Ouroboros problem.&amp;quot;¬† An Ouroboros is an ancient symbol that depicts a snake eating its own tail.¬†&lt;/p&gt;&lt;p&gt;Using AI systems to evaluate AI systems creates a circular validation challenge.&lt;/p&gt;&lt;p&gt;&amp;quot;You want a judge to see if your system is good, if your AI system is good, but then your judge is also an AI system,&amp;quot; Koppol explained. &amp;quot;And now you&amp;#x27;re saying like, well, how do I know this judge is good?&amp;quot;&lt;/p&gt;&lt;p&gt;The solution is measuring &amp;quot;distance to human expert ground truth&amp;quot; as the primary scoring function. By minimizing the gap between how an AI judge scores outputs versus how domain experts would score them, organizations can trust these judges as scalable proxies for human evaluation.&lt;/p&gt;&lt;p&gt;This approach differs fundamentally from traditional&lt;a href="https://venturebeat.com/ai/beyond-detection-why-automatically-correcting-hallucinations-could-transform-enterprise-ai-adoption"&gt; &lt;u&gt;guardrail systems&lt;/u&gt;&lt;/a&gt; or single-metric evaluations. Rather than asking whether an AI output passed or failed on a generic quality check, Judge Builder creates highly specific evaluation criteria tailored to each organization&amp;#x27;s domain expertise and business requirements.&lt;/p&gt;&lt;p&gt;The technical implementation also sets it apart. Judge Builder integrates with Databricks&amp;#x27; MLflow and &lt;a href="https://venturebeat.com/ai/the-usd100m-openai-partnership-is-nice-but-databricks-real-breakthrough"&gt;&lt;u&gt;prompt optimization&lt;/u&gt;&lt;/a&gt; tools and can work with any underlying model. Teams can version control their judges, track performance over time and deploy multiple judges simultaneously across different quality dimensions.&lt;/p&gt;&lt;h2&gt;Lessons learned: Building judges that actually work&lt;/h2&gt;&lt;p&gt;Databricks&amp;#x27; work with enterprise customers revealed three critical lessons that apply to anyone building AI judges.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson one: Your experts don&amp;#x27;t agree as much as you think.&lt;/b&gt; When quality is subjective, organizations discover that even their own subject matter experts disagree on what constitutes acceptable output. A customer service response might be factually correct but use an inappropriate tone. A financial summary might be comprehensive but too technical for the intended audience.&lt;/p&gt;&lt;p&gt;&amp;quot;One of the biggest lessons of this whole process is that all problems become people problems,&amp;quot; Frankle said. &amp;quot;The hardest part is getting an idea out of a person&amp;#x27;s brain and into something explicit. And the harder part is that companies are not one brain, but many brains.&amp;quot;&lt;/p&gt;&lt;p&gt;The fix is batched annotation with inter-rater reliability checks. Teams annotate examples in small groups, then measure agreement scores before proceeding. This catches misalignment early. In one case, three experts gave ratings of 1, 5 and neutral for the same output before discussion revealed they were interpreting the evaluation criteria differently.&lt;/p&gt;&lt;p&gt;Companies using this approach achieve inter-rater reliability scores as high as 0.6 compared to typical scores of 0.3 from external annotation services. Higher agreement translates directly to better judge performance because the training data contains less noise.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson two: Break down vague criteria into specific judges.&lt;/b&gt; Instead of one judge evaluating whether a response is &amp;quot;relevant, factual and concise,&amp;quot; create three separate judges. Each targets a specific quality aspect. This granularity matters because a failing &amp;quot;overall quality&amp;quot; score reveals something is wrong but not what to fix.&lt;/p&gt;&lt;p&gt;The best results come from combining top-down requirements such as regulatory constraints, stakeholder priorities, with bottom-up discovery of observed failure patterns. One customer built a top-down judge for correctness but discovered through data analysis that correct responses almost always cited the top two retrieval results. This insight became a new production-friendly judge that could proxy for correctness without requiring ground-truth labels.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson three: You need fewer examples than you think.&lt;/b&gt; Teams can create robust judges from just 20-30 well-chosen examples. The key is selecting edge cases that expose disagreement rather than obvious examples where everyone agrees.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re able to run this process with some teams in as little as three hours, so it doesn&amp;#x27;t really take that long to start getting a good judge,&amp;quot; Koppol said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Production results: From pilots to seven-figure deployments&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Frankle shared three metrics Databricks uses to measure Judge Builder&amp;#x27;s success: whether customers want to use it again, whether they increase AI spending and whether they progress further in their AI journey.&lt;/p&gt;&lt;p&gt;On the first metric, one customer created more than a dozen judges after their initial workshop. &amp;quot;This customer made more than a dozen judges after we walked them through doing this in a rigorous way for the first time with this framework,&amp;quot; Frankle said. &amp;quot;They really went to town on judges and are now measuring everything.&amp;quot;&lt;/p&gt;&lt;p&gt;For the second metric, the business impact is clear. &amp;quot;There are multiple customers who have gone through this workshop and have become seven-figure spenders on GenAI at Databricks in a way that they weren&amp;#x27;t before,&amp;quot; Frankle said.&lt;/p&gt;&lt;p&gt;The third metric reveals Judge Builder&amp;#x27;s strategic value. Customers who previously hesitated to use advanced techniques like reinforcement learning now feel confident deploying them because they can measure whether improvements actually occurred.&lt;/p&gt;&lt;p&gt;&amp;quot;There are customers who have gone and done very advanced things after having had these judges where they were reluctant to do so before,&amp;quot; Frankle said. &amp;quot;They&amp;#x27;ve moved from doing a little bit of prompt engineering to doing reinforcement learning with us. Why spend the money on reinforcement learning, and why spend the energy on reinforcement learning if you don&amp;#x27;t know whether it actually made a difference?&amp;quot;&lt;/p&gt;&lt;h2&gt;What enterprises should do now&lt;/h2&gt;&lt;p&gt;The teams successfully moving AI from pilot to production treat judges not as one-time artifacts but as evolving assets that grow with their systems.&lt;/p&gt;&lt;p&gt;Databricks recommends three practical steps. First, focus on high-impact judges by identifying one critical regulatory requirement plus one observed failure mode. These become your initial judge portfolio.&lt;/p&gt;&lt;p&gt;Second, create lightweight workflows with subject matter experts. A few hours reviewing 20-30 edge cases provides sufficient calibration for most judges. Use batched annotation and inter-rater reliability checks to denoise your data.&lt;/p&gt;&lt;p&gt;Third, schedule regular judge reviews using production data. New failure modes will emerge as your system evolves. Your judge portfolio should evolve with them.&lt;/p&gt;&lt;p&gt;&amp;quot;A judge is a way to evaluate a model, it&amp;#x27;s also a way to create guardrails, it&amp;#x27;s also a way to have a metric against which you can do prompt optimization and it&amp;#x27;s also a way to have a metric against which you can do reinforcement learning,&amp;quot; Frankle said. &amp;quot;Once you have a judge that you know represents your human taste in an empirical form that you can query as much as you want, you can use it in 10,000 different ways to measure or improve your agents.&amp;quot;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;The intelligence of AI models isn&amp;#x27;t what&amp;#x27;s blocking enterprise deployments. It&amp;#x27;s the inability to define and measure quality in the first place.&lt;/p&gt;&lt;p&gt;That&amp;#x27;s where AI judges are now playing an increasingly important role. In AI evaluation, a &amp;quot;judge&amp;quot; is an AI system that scores outputs from another AI system.¬†&lt;/p&gt;&lt;p&gt;Judge Builder is Databricks&amp;#x27; framework for creating judges and was first deployed as part of the company&amp;#x27;s&lt;a href="https://venturebeat.com/ai/why-most-enterprise-ai-agents-never-reach-production-and-how-databricks-plans-t"&gt; &lt;u&gt;Agent Bricks&lt;/u&gt;&lt;/a&gt; technology earlier this year. The framework has evolved significantly since its initial launch in response to direct user feedback and deployments.&lt;/p&gt;&lt;p&gt;Early versions focused on technical implementation but customer feedback revealed the real bottleneck was organizational alignment. Databricks now offers a structured workshop process that guides teams through three core challenges: getting stakeholders to agree on quality criteria, capturing domain expertise from limited subject matter experts and deploying evaluation systems at scale.&lt;/p&gt;&lt;p&gt;&amp;quot;The intelligence of the model is typically not the bottleneck, the models are really smart,&amp;quot; Jonathan Frankle, Databricks&amp;#x27; chief AI scientist, told VentureBeat in an exclusive briefing. &amp;quot;Instead, it&amp;#x27;s really about asking, how do we get the models to do what we want, and how do we know if they did what we wanted?&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The &amp;#x27;Ouroboros problem&amp;#x27; of AI evaluation&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Judge Builder addresses what Pallavi Koppol, a Databricks research scientist who led the development, calls the &amp;quot;Ouroboros problem.&amp;quot;¬† An Ouroboros is an ancient symbol that depicts a snake eating its own tail.¬†&lt;/p&gt;&lt;p&gt;Using AI systems to evaluate AI systems creates a circular validation challenge.&lt;/p&gt;&lt;p&gt;&amp;quot;You want a judge to see if your system is good, if your AI system is good, but then your judge is also an AI system,&amp;quot; Koppol explained. &amp;quot;And now you&amp;#x27;re saying like, well, how do I know this judge is good?&amp;quot;&lt;/p&gt;&lt;p&gt;The solution is measuring &amp;quot;distance to human expert ground truth&amp;quot; as the primary scoring function. By minimizing the gap between how an AI judge scores outputs versus how domain experts would score them, organizations can trust these judges as scalable proxies for human evaluation.&lt;/p&gt;&lt;p&gt;This approach differs fundamentally from traditional&lt;a href="https://venturebeat.com/ai/beyond-detection-why-automatically-correcting-hallucinations-could-transform-enterprise-ai-adoption"&gt; &lt;u&gt;guardrail systems&lt;/u&gt;&lt;/a&gt; or single-metric evaluations. Rather than asking whether an AI output passed or failed on a generic quality check, Judge Builder creates highly specific evaluation criteria tailored to each organization&amp;#x27;s domain expertise and business requirements.&lt;/p&gt;&lt;p&gt;The technical implementation also sets it apart. Judge Builder integrates with Databricks&amp;#x27; MLflow and &lt;a href="https://venturebeat.com/ai/the-usd100m-openai-partnership-is-nice-but-databricks-real-breakthrough"&gt;&lt;u&gt;prompt optimization&lt;/u&gt;&lt;/a&gt; tools and can work with any underlying model. Teams can version control their judges, track performance over time and deploy multiple judges simultaneously across different quality dimensions.&lt;/p&gt;&lt;h2&gt;Lessons learned: Building judges that actually work&lt;/h2&gt;&lt;p&gt;Databricks&amp;#x27; work with enterprise customers revealed three critical lessons that apply to anyone building AI judges.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson one: Your experts don&amp;#x27;t agree as much as you think.&lt;/b&gt; When quality is subjective, organizations discover that even their own subject matter experts disagree on what constitutes acceptable output. A customer service response might be factually correct but use an inappropriate tone. A financial summary might be comprehensive but too technical for the intended audience.&lt;/p&gt;&lt;p&gt;&amp;quot;One of the biggest lessons of this whole process is that all problems become people problems,&amp;quot; Frankle said. &amp;quot;The hardest part is getting an idea out of a person&amp;#x27;s brain and into something explicit. And the harder part is that companies are not one brain, but many brains.&amp;quot;&lt;/p&gt;&lt;p&gt;The fix is batched annotation with inter-rater reliability checks. Teams annotate examples in small groups, then measure agreement scores before proceeding. This catches misalignment early. In one case, three experts gave ratings of 1, 5 and neutral for the same output before discussion revealed they were interpreting the evaluation criteria differently.&lt;/p&gt;&lt;p&gt;Companies using this approach achieve inter-rater reliability scores as high as 0.6 compared to typical scores of 0.3 from external annotation services. Higher agreement translates directly to better judge performance because the training data contains less noise.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson two: Break down vague criteria into specific judges.&lt;/b&gt; Instead of one judge evaluating whether a response is &amp;quot;relevant, factual and concise,&amp;quot; create three separate judges. Each targets a specific quality aspect. This granularity matters because a failing &amp;quot;overall quality&amp;quot; score reveals something is wrong but not what to fix.&lt;/p&gt;&lt;p&gt;The best results come from combining top-down requirements such as regulatory constraints, stakeholder priorities, with bottom-up discovery of observed failure patterns. One customer built a top-down judge for correctness but discovered through data analysis that correct responses almost always cited the top two retrieval results. This insight became a new production-friendly judge that could proxy for correctness without requiring ground-truth labels.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Lesson three: You need fewer examples than you think.&lt;/b&gt; Teams can create robust judges from just 20-30 well-chosen examples. The key is selecting edge cases that expose disagreement rather than obvious examples where everyone agrees.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re able to run this process with some teams in as little as three hours, so it doesn&amp;#x27;t really take that long to start getting a good judge,&amp;quot; Koppol said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Production results: From pilots to seven-figure deployments&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Frankle shared three metrics Databricks uses to measure Judge Builder&amp;#x27;s success: whether customers want to use it again, whether they increase AI spending and whether they progress further in their AI journey.&lt;/p&gt;&lt;p&gt;On the first metric, one customer created more than a dozen judges after their initial workshop. &amp;quot;This customer made more than a dozen judges after we walked them through doing this in a rigorous way for the first time with this framework,&amp;quot; Frankle said. &amp;quot;They really went to town on judges and are now measuring everything.&amp;quot;&lt;/p&gt;&lt;p&gt;For the second metric, the business impact is clear. &amp;quot;There are multiple customers who have gone through this workshop and have become seven-figure spenders on GenAI at Databricks in a way that they weren&amp;#x27;t before,&amp;quot; Frankle said.&lt;/p&gt;&lt;p&gt;The third metric reveals Judge Builder&amp;#x27;s strategic value. Customers who previously hesitated to use advanced techniques like reinforcement learning now feel confident deploying them because they can measure whether improvements actually occurred.&lt;/p&gt;&lt;p&gt;&amp;quot;There are customers who have gone and done very advanced things after having had these judges where they were reluctant to do so before,&amp;quot; Frankle said. &amp;quot;They&amp;#x27;ve moved from doing a little bit of prompt engineering to doing reinforcement learning with us. Why spend the money on reinforcement learning, and why spend the energy on reinforcement learning if you don&amp;#x27;t know whether it actually made a difference?&amp;quot;&lt;/p&gt;&lt;h2&gt;What enterprises should do now&lt;/h2&gt;&lt;p&gt;The teams successfully moving AI from pilot to production treat judges not as one-time artifacts but as evolving assets that grow with their systems.&lt;/p&gt;&lt;p&gt;Databricks recommends three practical steps. First, focus on high-impact judges by identifying one critical regulatory requirement plus one observed failure mode. These become your initial judge portfolio.&lt;/p&gt;&lt;p&gt;Second, create lightweight workflows with subject matter experts. A few hours reviewing 20-30 edge cases provides sufficient calibration for most judges. Use batched annotation and inter-rater reliability checks to denoise your data.&lt;/p&gt;&lt;p&gt;Third, schedule regular judge reviews using production data. New failure modes will emerge as your system evolves. Your judge portfolio should evolve with them.&lt;/p&gt;&lt;p&gt;&amp;quot;A judge is a way to evaluate a model, it&amp;#x27;s also a way to create guardrails, it&amp;#x27;s also a way to have a metric against which you can do prompt optimization and it&amp;#x27;s also a way to have a metric against which you can do reinforcement learning,&amp;quot; Frankle said. &amp;quot;Once you have a judge that you know represents your human taste in an empirical form that you can query as much as you want, you can use it in 10,000 different ways to measure or improve your agents.&amp;quot;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/databricks-research-reveals-that-building-better-ai-judges-isnt-just-a</guid><pubDate>Tue, 04 Nov 2025 20:00:00 +0000</pubDate></item><item><title>[NEW] Google‚Äôs AI Mode gets new agentic capabilities to help book event tickets and beauty appointments (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/googles-ai-mode-gets-new-agentic-capabilities-to-help-book-event-tickets-and-beauty-appointments/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it‚Äôs launching new agentic capabilities in AI Mode, its feature that allows users to ask complex questions and follow-ups to dig deeper on a topic directly within Search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can now get help with booking event tickets and beauty and wellness appointments in AI Mode.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For example, you can say, ‚ÄúFind me two cheap tickets for the Shaboozey concert coming up. Prefer standing floor tickets.‚Äù AI Mode will then search across multiple websites to find real-time ticket options that meet your specific requests. It will then present you with a curated list of ticket prices to choose from. AI Mode links you directly to the booking page so you can finalize your purchase.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new agentic capabilities are available to all users opted into Google‚Äôs experimental arm, Search Labs, in the U.S. The company notes that Google AI Pro and Ultra subscribers have access to high limits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first brought agentic capabilities to AI Mode back in August, when it started letting people use the feature to find restaurant reservations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="a selection of screenshots showing AI mode." class="wp-image-3064998" height="742" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-04-at-2.50.08PM.png" width="1696" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With this capability, you can request dinner reservations based on multiple preferences, such as party size, date, time, location, and preferred cuisine. For example, you could ask, ‚ÄúFind me a dinner reservation for three people this Friday after 6 p.m. around Logan Square. Craving ramen or bibimbap.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Mode will then search across different reservation platforms to find real-time availability for restaurants that match the inquiry. It then surfaces a curated list of options to choose from. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOur priority in Google Search is connecting you with high-quality information you can rely on,‚Äù Google explained on its Search Labs page. ‚ÄúThis new mode is rooted in our core quality and safety systems, but it‚Äôs still an early experiment and may make mistakes.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first launched AI Mode in March to take on popular services like Perplexity AI and OpenAI‚Äôs ChatGPT Search. Since then, the tech giant has brought AI Mode to more than 180 countries and has been building it out with new functionalities. For instance, AI Mode recently got access to a Canvas feature that helps you build study plans and organize information over multiple sessions in a side panel. It also now lets you use Google Lens to ask about what‚Äôs on your desktop screen.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it‚Äôs launching new agentic capabilities in AI Mode, its feature that allows users to ask complex questions and follow-ups to dig deeper on a topic directly within Search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can now get help with booking event tickets and beauty and wellness appointments in AI Mode.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For example, you can say, ‚ÄúFind me two cheap tickets for the Shaboozey concert coming up. Prefer standing floor tickets.‚Äù AI Mode will then search across multiple websites to find real-time ticket options that meet your specific requests. It will then present you with a curated list of ticket prices to choose from. AI Mode links you directly to the booking page so you can finalize your purchase.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new agentic capabilities are available to all users opted into Google‚Äôs experimental arm, Search Labs, in the U.S. The company notes that Google AI Pro and Ultra subscribers have access to high limits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first brought agentic capabilities to AI Mode back in August, when it started letting people use the feature to find restaurant reservations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="a selection of screenshots showing AI mode." class="wp-image-3064998" height="742" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-04-at-2.50.08PM.png" width="1696" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With this capability, you can request dinner reservations based on multiple preferences, such as party size, date, time, location, and preferred cuisine. For example, you could ask, ‚ÄúFind me a dinner reservation for three people this Friday after 6 p.m. around Logan Square. Craving ramen or bibimbap.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Mode will then search across different reservation platforms to find real-time availability for restaurants that match the inquiry. It then surfaces a curated list of options to choose from. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOur priority in Google Search is connecting you with high-quality information you can rely on,‚Äù Google explained on its Search Labs page. ‚ÄúThis new mode is rooted in our core quality and safety systems, but it‚Äôs still an early experiment and may make mistakes.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first launched AI Mode in March to take on popular services like Perplexity AI and OpenAI‚Äôs ChatGPT Search. Since then, the tech giant has brought AI Mode to more than 180 countries and has been building it out with new functionalities. For instance, AI Mode recently got access to a Canvas feature that helps you build study plans and organize information over multiple sessions in a side panel. It also now lets you use Google Lens to ask about what‚Äôs on your desktop screen.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/googles-ai-mode-gets-new-agentic-capabilities-to-help-book-event-tickets-and-beauty-appointments/</guid><pubDate>Tue, 04 Nov 2025 20:36:18 +0000</pubDate></item><item><title>[NEW] Meet Project Suncatcher, Google‚Äôs plan to put AI data centers in space (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/google/2025/11/meet-project-suncatcher-googles-plan-to-put-ai-data-centers-in-space/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is already zapping TPUs with radiation to get ready.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Project Suncatcher" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-640x360.png" width="640" /&gt;
                  &lt;img alt="Project Suncatcher" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The tech industry is on a tear, building data centers for AI as quickly as they can buy up the land. The sky-high energy costs and logistical headaches of managing all those data centers have prompted interest in space-based infrastructure. Moguls like Jeff Bezos and Elon Musk have mused about putting GPUs in space, and now Google confirms it‚Äôs working on its own version of the technology. The company‚Äôs latest ‚Äúmoonshot‚Äù is known as Project Suncatcher, and if all goes as planned, Google hopes it will lead to scalable networks of orbiting TPUs.&lt;/p&gt;
&lt;p&gt;The space around Earth has changed a lot in the last few years. A new generation of satellite constellations like Starlink has shown it‚Äôs feasible to relay Internet communication via orbital systems. Deploying high-performance AI accelerators in space along similar lines would be a boon to the industry‚Äôs never-ending build-out. Google notes that space may be ‚Äúthe best place to scale AI compute.‚Äù&lt;/p&gt;
&lt;p&gt;Google‚Äôs vision for scalable orbiting data centers relies on solar-powered satellites with free-space optical links connecting the nodes into a distributed network. Naturally, there are numerous engineering challenges to solve before Project Suncatcher is real. As a reference, Google points to the long road from its first moonshot self-driving cars 15 years ago to the Waymo vehicles that are almost fully autonomous today.&lt;/p&gt;
&lt;h2&gt;Taking AI to space&lt;/h2&gt;
&lt;p&gt;Some of the benefits are obvious. Google‚Äôs vision for Suncatcher, as explained in a pre-print study (PDF), would place the satellites in a dawn-dusk sun-synchronous low-earth orbit. That ensures they would get almost constant sunlight exposure (hence the name). The cost of electricity on Earth is a problem for large data centers, and even moving them all to solar power wouldn‚Äôt get the job done. Google notes solar panels are up to eight times more efficient in orbit than they are on the surface of Earth. Lots of uninterrupted sunlight at higher efficiency means more power for data processing.&lt;/p&gt;
&lt;p&gt;A major sticking point is how you can keep satellites connected at high speeds as they orbit. On Earth, the nodes in a data center communicate via blazing-fast optical interconnect chips. Maintaining high-speed communication among the orbiting servers will require wireless solutions that can operate at tens of terabits per second. Early testing on Earth has demonstrated bidirectional speeds up to 1.6 Tbps‚ÄîGoogle believes this can be scaled up over time.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="496" id="video-2125616-1" preload="metadata" width="490"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher-1.mp4?_=1" type="video/mp4" /&gt;Google‚Äôs proposed free-fall (‚Äúno thrust‚Äù) constellation for linked satellites; arrow pointing toward Earth.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google‚Äôs proposed free-fall (‚Äúno thrust‚Äù) constellation for linked satellites; arrow pointing toward Earth.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;However, there is the problem of physics. Received power decreases with the square of distance, so Google notes the satellites would have to maintain proximity of a kilometer or less. That would require a tighter formation than any currently operational constellation, but it should be workable. Google has developed analytical models suggesting that satellites positioned several hundred meters apart would require only ‚Äúmodest station-keeping maneuvers.‚Äù&lt;/p&gt;
&lt;p&gt;Hardware designed for space is expensive and often less capable compared to terrestrial systems because the former needs to be hardened against extreme temperatures and radiation. Google‚Äôs approach to Project Suncatcher is to reuse the components used on Earth, which might not be very robust when you stuff them in a satellite. However, innovations like the Snapdragon-powered Mars Ingenuity helicopter have shown that off-the-shelf hardware may survive longer in space than we thought.&lt;/p&gt;
&lt;p&gt;Google says Suncatcher only works if TPUs can run for at least five years, which works out to 750 rad. The company is testing this by blasting its latest v6e Cloud TPU (Trillium) in a 67MeV proton beam. Google says that while the memory was most vulnerable to damage, the experiments showed that TPUs can handle about three times as much radiation (almost 2 krad) before data corruption was detected.&lt;/p&gt;
&lt;p&gt;Google hopes to launch a pair of prototype satellites with TPUs by early 2027. It expects the launch cost of these first AI orbiters to be quite high. However, Google is planning for the mid-2030s when launch costs are projected to drop to as little as $200 per kilogram. At that level, space-based data centers could become as economical as the terrestrial versions.&lt;/p&gt;
&lt;p&gt;The fact is, terrestrial data centers are dirty, noisy, and ravenous for power and water. This has led many communities to oppose plans to build them near the places where people live and work. Putting them in space could solve everyone‚Äôs problems (unless you‚Äôre an astronomer).&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is already zapping TPUs with radiation to get ready.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Project Suncatcher" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-640x360.png" width="640" /&gt;
                  &lt;img alt="Project Suncatcher" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The tech industry is on a tear, building data centers for AI as quickly as they can buy up the land. The sky-high energy costs and logistical headaches of managing all those data centers have prompted interest in space-based infrastructure. Moguls like Jeff Bezos and Elon Musk have mused about putting GPUs in space, and now Google confirms it‚Äôs working on its own version of the technology. The company‚Äôs latest ‚Äúmoonshot‚Äù is known as Project Suncatcher, and if all goes as planned, Google hopes it will lead to scalable networks of orbiting TPUs.&lt;/p&gt;
&lt;p&gt;The space around Earth has changed a lot in the last few years. A new generation of satellite constellations like Starlink has shown it‚Äôs feasible to relay Internet communication via orbital systems. Deploying high-performance AI accelerators in space along similar lines would be a boon to the industry‚Äôs never-ending build-out. Google notes that space may be ‚Äúthe best place to scale AI compute.‚Äù&lt;/p&gt;
&lt;p&gt;Google‚Äôs vision for scalable orbiting data centers relies on solar-powered satellites with free-space optical links connecting the nodes into a distributed network. Naturally, there are numerous engineering challenges to solve before Project Suncatcher is real. As a reference, Google points to the long road from its first moonshot self-driving cars 15 years ago to the Waymo vehicles that are almost fully autonomous today.&lt;/p&gt;
&lt;h2&gt;Taking AI to space&lt;/h2&gt;
&lt;p&gt;Some of the benefits are obvious. Google‚Äôs vision for Suncatcher, as explained in a pre-print study (PDF), would place the satellites in a dawn-dusk sun-synchronous low-earth orbit. That ensures they would get almost constant sunlight exposure (hence the name). The cost of electricity on Earth is a problem for large data centers, and even moving them all to solar power wouldn‚Äôt get the job done. Google notes solar panels are up to eight times more efficient in orbit than they are on the surface of Earth. Lots of uninterrupted sunlight at higher efficiency means more power for data processing.&lt;/p&gt;
&lt;p&gt;A major sticking point is how you can keep satellites connected at high speeds as they orbit. On Earth, the nodes in a data center communicate via blazing-fast optical interconnect chips. Maintaining high-speed communication among the orbiting servers will require wireless solutions that can operate at tens of terabits per second. Early testing on Earth has demonstrated bidirectional speeds up to 1.6 Tbps‚ÄîGoogle believes this can be scaled up over time.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="496" id="video-2125616-1" preload="metadata" width="490"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher-1.mp4?_=1" type="video/mp4" /&gt;Google‚Äôs proposed free-fall (‚Äúno thrust‚Äù) constellation for linked satellites; arrow pointing toward Earth.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google‚Äôs proposed free-fall (‚Äúno thrust‚Äù) constellation for linked satellites; arrow pointing toward Earth.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;However, there is the problem of physics. Received power decreases with the square of distance, so Google notes the satellites would have to maintain proximity of a kilometer or less. That would require a tighter formation than any currently operational constellation, but it should be workable. Google has developed analytical models suggesting that satellites positioned several hundred meters apart would require only ‚Äúmodest station-keeping maneuvers.‚Äù&lt;/p&gt;
&lt;p&gt;Hardware designed for space is expensive and often less capable compared to terrestrial systems because the former needs to be hardened against extreme temperatures and radiation. Google‚Äôs approach to Project Suncatcher is to reuse the components used on Earth, which might not be very robust when you stuff them in a satellite. However, innovations like the Snapdragon-powered Mars Ingenuity helicopter have shown that off-the-shelf hardware may survive longer in space than we thought.&lt;/p&gt;
&lt;p&gt;Google says Suncatcher only works if TPUs can run for at least five years, which works out to 750 rad. The company is testing this by blasting its latest v6e Cloud TPU (Trillium) in a 67MeV proton beam. Google says that while the memory was most vulnerable to damage, the experiments showed that TPUs can handle about three times as much radiation (almost 2 krad) before data corruption was detected.&lt;/p&gt;
&lt;p&gt;Google hopes to launch a pair of prototype satellites with TPUs by early 2027. It expects the launch cost of these first AI orbiters to be quite high. However, Google is planning for the mid-2030s when launch costs are projected to drop to as little as $200 per kilogram. At that level, space-based data centers could become as economical as the terrestrial versions.&lt;/p&gt;
&lt;p&gt;The fact is, terrestrial data centers are dirty, noisy, and ravenous for power and water. This has led many communities to oppose plans to build them near the places where people live and work. Putting them in space could solve everyone‚Äôs problems (unless you‚Äôre an astronomer).&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/11/meet-project-suncatcher-googles-plan-to-put-ai-data-centers-in-space/</guid><pubDate>Tue, 04 Nov 2025 20:59:02 +0000</pubDate></item><item><title>[NEW] Rivian creates another spinoff company called Mind Robotics (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/rivian-creates-another-spinoff-company-called-mind-robotics/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Rivian-QUAD-R1S-sunset.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rivian has created its second spinoff company this year: an industrial AI and robotics venture called Mind Robotics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new effort will be focused around using ‚Äúindustrial AI to reshape how physical world businesses operate and leverage Rivian operations data as the foundation for a robotics data flywheel,‚Äù according to the company‚Äôs third-quarter shareholder letter published Tuesday.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That‚Äôs a mouthful of buzzwords, and Rivian declined to clarify beyond that explanation. On an investor call Tuesday, Rivian CEO RJ Scaringe said his company realized it had the chance to ‚Äúdevelop products and robotic solutions that allow us to run and operate our manufacturing plants more efficiently.‚Äù Scaringe will serve as chairman of the board of directors for Mind Robotics, according to a filing, and Rivian is a shareholder, he said on the call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúAs much as we‚Äôve seen AI shift how we operate and run our businesses through the wide-ranging applications for LLMs, the potential for AI to really shift how we think about operating in the physical world is, in some ways, unimaginably large,‚Äù Scaringe said on the call. ‚ÄúSo the creation of this company is ultimately the culmination of us coming to the view that we wanted to have direct control and direct influence over the design and development of advanced AI robotics that would be very focused on industrial applications.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mind Robotics has already raised a $115 million seed round, which was led by VC firm Eclipse. Jiten Behl, a partner at Eclipse who also used to work at Rivian, revealed the investment in a LinkedIn post after TechCrunch previously reported the firm‚Äôs involvement in an earlier version of this story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of Mind Robotics marks the second time this year that Rivian has created a new stand-alone company. In March, the company spun out its skunkworks micromobility division into a startup called Also Inc. That new company was funded in part by money from Eclipse, with additional funding from Greenoaks Capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It‚Äôs unclear if Rivian employees are moving over to Mind Robotics, like was the case with Also. A Rivian spokesperson declined to say. But the company hinted at the possibility in Tuesday‚Äôs letter.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWith our strong bench of technology talent and an innovation-driven culture, we have been able to identify additional areas of value to accelerate our mission on a wider scale while maintaining Rivian‚Äôs focus,‚Äù Scaringe wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics and industrial AI are hot areas for investment right now. There is a slew of humanoid robotics companies raising money and trying to ship products, including Tesla. General Motors is working on its own robotics and AI division, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond Rivian‚Äôs announcement Tuesday, though, very little is known about what Mind Robotics will get up to. There is essentially no digital footprint for the company yet, save for the trademark application. That application is very broadly targeted and says Mind Robotics could use the trademark for everything from machinery, to vehicles, to ‚Äúincubators for eggs.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story has been updated with new information from a regulatory filing in the third paragraph, the LinkedIn post in the fifth paragraph, and from the investor call throughout.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/Rivian-QUAD-R1S-sunset.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rivian has created its second spinoff company this year: an industrial AI and robotics venture called Mind Robotics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new effort will be focused around using ‚Äúindustrial AI to reshape how physical world businesses operate and leverage Rivian operations data as the foundation for a robotics data flywheel,‚Äù according to the company‚Äôs third-quarter shareholder letter published Tuesday.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That‚Äôs a mouthful of buzzwords, and Rivian declined to clarify beyond that explanation. On an investor call Tuesday, Rivian CEO RJ Scaringe said his company realized it had the chance to ‚Äúdevelop products and robotic solutions that allow us to run and operate our manufacturing plants more efficiently.‚Äù Scaringe will serve as chairman of the board of directors for Mind Robotics, according to a filing, and Rivian is a shareholder, he said on the call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúAs much as we‚Äôve seen AI shift how we operate and run our businesses through the wide-ranging applications for LLMs, the potential for AI to really shift how we think about operating in the physical world is, in some ways, unimaginably large,‚Äù Scaringe said on the call. ‚ÄúSo the creation of this company is ultimately the culmination of us coming to the view that we wanted to have direct control and direct influence over the design and development of advanced AI robotics that would be very focused on industrial applications.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mind Robotics has already raised a $115 million seed round, which was led by VC firm Eclipse. Jiten Behl, a partner at Eclipse who also used to work at Rivian, revealed the investment in a LinkedIn post after TechCrunch previously reported the firm‚Äôs involvement in an earlier version of this story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of Mind Robotics marks the second time this year that Rivian has created a new stand-alone company. In March, the company spun out its skunkworks micromobility division into a startup called Also Inc. That new company was funded in part by money from Eclipse, with additional funding from Greenoaks Capital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It‚Äôs unclear if Rivian employees are moving over to Mind Robotics, like was the case with Also. A Rivian spokesperson declined to say. But the company hinted at the possibility in Tuesday‚Äôs letter.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWith our strong bench of technology talent and an innovation-driven culture, we have been able to identify additional areas of value to accelerate our mission on a wider scale while maintaining Rivian‚Äôs focus,‚Äù Scaringe wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics and industrial AI are hot areas for investment right now. There is a slew of humanoid robotics companies raising money and trying to ship products, including Tesla. General Motors is working on its own robotics and AI division, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond Rivian‚Äôs announcement Tuesday, though, very little is known about what Mind Robotics will get up to. There is essentially no digital footprint for the company yet, save for the trademark application. That application is very broadly targeted and says Mind Robotics could use the trademark for everything from machinery, to vehicles, to ‚Äúincubators for eggs.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story has been updated with new information from a regulatory filing in the third paragraph, the LinkedIn post in the fifth paragraph, and from the investor call throughout.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/rivian-creates-another-spinoff-company-called-mind-robotics/</guid><pubDate>Tue, 04 Nov 2025 21:03:43 +0000</pubDate></item><item><title>[NEW] People Inc. forges AI licensing deal with Microsoft as Google traffic drops (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/people-inc-forges-ai-licensing-deal-with-microsoft-as-google-traffic-drops/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Neil-Vogel.jpg?resize=1200,593" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People Inc., one of the largest media publishers in the U.S., has signed an AI licensing deal with Microsoft. The media giant (formerly known as Dotdash Meredith) made the announcement Tuesday as a part of parent company IAC‚Äôs third-quarter earnings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the deal, People Inc. will become a launch partner in Microsoft‚Äôs publisher content marketplace. This is the company‚Äôs second AI deal following its earlier agreement with OpenAI last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;People Inc. CEO Neil Vogel described the new marketplace as ‚Äúessentially a pay-per-use market where AI players directly can compensate publishers for use of their content on, sort of like an ‚Äòa la carte‚Äô basis.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also praised Microsoft for being committed to paying for content to support its AI efforts, adding that Microsoft‚Äôs Copilot would be the first buyer for the marketplace. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt‚Äôs a very strong endorsement of us to be in the room with them and a very strong endorsement of the publishing marketplace and the value of content to make AI that is of high value,‚Äù Vogel said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement of the Microsoft deal was shared during IAC‚Äôs earnings, alongside the news that Google Search‚Äôs AI Overviews has been hurting the publisher‚Äôs traffic. For the first time, People Inc. shared data with investors that showed how Google Search, which accounted for 54% of its traffic two years ago, had dropped to 24% of its traffic during the past quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agreement differs from the OpenAI deal, which Vogel characterized as more of an ‚Äúall-you-can-eat‚Äù model, but said People Inc. was happy with either model. What matters to the company is that its work is ‚Äúrespected and paid for,‚Äù he said. The company didn‚Äôt share the specific deal terms, though. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;People Inc. has taken issue with the way AI companies have ingested media without paying to fuel their AI products and train their models. Recently, Vogel criticized Google, calling the tech giant a ‚Äúbad actor‚Äù because it uses the same bot to crawl websites for its Google search engine and its AI features. Publishers can‚Äôt block the bot, as Google search still accounts for a large percentage of their traffic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, People Inc. uses the technology from web infrastructure provider Cloudflare to block other AI crawlers, prompting AI players to approach it with content deals. In September, Vogel attributed its decision to leverage Cloudflare‚Äôs tech as a way to push AI companies to the negotiating table, noting that its progress on deals was ‚Äúmuch further along‚Äù after adopting the solution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He reiterated those comments on today‚Äôs earnings call with investors, saying that blocking AI crawlers has been ‚Äúvery effective‚Äù and ‚Äúbrought almost everyone to the table.‚Äù Vogel suggested that more deals would be announced in time, as well. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;IAC reported that People Inc. grew its digital revenue 9% to $269 million in the quarter, driven by performance marketing and licensing, which saw 38% and 24% growth, respectively. It also noted its acquisition of a food-focused media publisher and influencer network Feedfeed.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Neil-Vogel.jpg?resize=1200,593" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People Inc., one of the largest media publishers in the U.S., has signed an AI licensing deal with Microsoft. The media giant (formerly known as Dotdash Meredith) made the announcement Tuesday as a part of parent company IAC‚Äôs third-quarter earnings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the deal, People Inc. will become a launch partner in Microsoft‚Äôs publisher content marketplace. This is the company‚Äôs second AI deal following its earlier agreement with OpenAI last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;People Inc. CEO Neil Vogel described the new marketplace as ‚Äúessentially a pay-per-use market where AI players directly can compensate publishers for use of their content on, sort of like an ‚Äòa la carte‚Äô basis.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also praised Microsoft for being committed to paying for content to support its AI efforts, adding that Microsoft‚Äôs Copilot would be the first buyer for the marketplace. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt‚Äôs a very strong endorsement of us to be in the room with them and a very strong endorsement of the publishing marketplace and the value of content to make AI that is of high value,‚Äù Vogel said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement of the Microsoft deal was shared during IAC‚Äôs earnings, alongside the news that Google Search‚Äôs AI Overviews has been hurting the publisher‚Äôs traffic. For the first time, People Inc. shared data with investors that showed how Google Search, which accounted for 54% of its traffic two years ago, had dropped to 24% of its traffic during the past quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agreement differs from the OpenAI deal, which Vogel characterized as more of an ‚Äúall-you-can-eat‚Äù model, but said People Inc. was happy with either model. What matters to the company is that its work is ‚Äúrespected and paid for,‚Äù he said. The company didn‚Äôt share the specific deal terms, though. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;People Inc. has taken issue with the way AI companies have ingested media without paying to fuel their AI products and train their models. Recently, Vogel criticized Google, calling the tech giant a ‚Äúbad actor‚Äù because it uses the same bot to crawl websites for its Google search engine and its AI features. Publishers can‚Äôt block the bot, as Google search still accounts for a large percentage of their traffic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, People Inc. uses the technology from web infrastructure provider Cloudflare to block other AI crawlers, prompting AI players to approach it with content deals. In September, Vogel attributed its decision to leverage Cloudflare‚Äôs tech as a way to push AI companies to the negotiating table, noting that its progress on deals was ‚Äúmuch further along‚Äù after adopting the solution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He reiterated those comments on today‚Äôs earnings call with investors, saying that blocking AI crawlers has been ‚Äúvery effective‚Äù and ‚Äúbrought almost everyone to the table.‚Äù Vogel suggested that more deals would be announced in time, as well. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;IAC reported that People Inc. grew its digital revenue 9% to $269 million in the quarter, driven by performance marketing and licensing, which saw 38% and 24% growth, respectively. It also noted its acquisition of a food-focused media publisher and influencer network Feedfeed.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/people-inc-forges-ai-licensing-deal-with-microsoft-as-google-traffic-drops/</guid><pubDate>Tue, 04 Nov 2025 22:30:28 +0000</pubDate></item><item><title>[NEW] Google‚Äôs new hurricane model was breathtakingly good this season (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/science/2025/11/googles-new-weather-model-impressed-during-its-first-hurricane-season/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Meanwhile, the US Global Forecasting System continues to get worse.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-640x640.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-1000x648.jpg" width="1000" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hurricane Melissa, with a well-defined eye, can be seen in this satellite image from Monday morning.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          NOAA

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The Atlantic hurricane season is drawing to a close, and with the tropics quieting down for a winter slumber, the focus of forecasters turns to evaluating what worked and what did not during the preceding season.&lt;/p&gt;
&lt;p&gt;This year, the answers are clear. Although Google DeepMind‚Äôs Weather Lab only started releasing cyclone track forecasts in June, the company‚Äôs AI forecasting service performed exceptionally well. By contrast, the Global Forecast System model, operated by the US National Weather Service and is based on traditional physics and runs on powerful supercomputers, performed abysmally.&lt;/p&gt;
&lt;p&gt;The official data comparing forecast model performance will not be published by the National Hurricane Center for a few months. However, Brian McNoldy, a senior researcher at the University of Miami, has already done some preliminary number crunching.&lt;/p&gt;
&lt;p&gt;The results are stunning:&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125648 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="720" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/bafkreia2ecszfb2iz2hican5os3ivjtnx3bkodspx4epbuqnyc3hjnum5m.jpg" width="1080" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      2025 Atlantic season hurricane model performance on track accuracy.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Brian McNoldy

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A little help in reading the graphic is in order. This chart sums up the track forecast accuracy for all 13 named storms in the Atlantic Basin this season, measuring the mean position error at various hours in the forecast, from 0 to 120 hours (five days). On this chart, the lower a line is, the better a model has performed.&lt;/p&gt;
&lt;h2&gt;A new champion&lt;/h2&gt;
&lt;p&gt;The dotted black line shows the average forecast error for official forecasts from the 2022 to 2024 seasons. What jumps out is that the United States‚Äô premier global model, the GFS (denoted here as AVNI), is by far the worst-performing model. Meanwhile, at the bottom of the chart, in maroon, is the Google DeepMind model (GDMI), performing the best at nearly all forecast hours.&lt;/p&gt;
&lt;p&gt;The difference in errors between the US GFS model and Google‚Äôs DeepMind is remarkable. At five days, the Google forecast had an error of 165 nautical miles compared to 360 nautical miles for the GFS model, more than &lt;em&gt;twice&lt;/em&gt; as bad. This is the kind of error that causes forecasters to completely disregard one model in favor of another.&lt;/p&gt;
&lt;p&gt;But there‚Äôs more. Google‚Äôs model was so good that it regularly beat the official forecast from the National Hurricane Center (OFCL), which is produced by human experts looking at a broad array of model data. The AI-based model also beat highly regarded ‚Äúconsensus models,‚Äù including the TVCN and HCCA products. For more information on various models and their designations, see here.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This early model comparison does not include the ‚Äúgold standard‚Äù traditional, physics-based model produced by the European Centre for Medium-Range Weather Forecasts. However, the ECMWF model typically does not do better on hurricane track forecasts than the hurricane center or consensus models, which weigh&amp;nbsp;several different model outputs. So it is unlikely to be superior to Google‚Äôs DeepMind.&lt;/p&gt;
&lt;h2&gt;This will change forecasting forever&lt;/h2&gt;
&lt;p&gt;It‚Äôs worth noting that DeepMind also did exceptionally well at intensity forecasting, which is the fluctuations in the strength of a hurricane. So in its first season, it nailed both hurricane tracks and intensity.&lt;/p&gt;
&lt;p&gt;As a forecaster who has relied on traditional physics-based models for a quarter of a century, it is difficult to say how gobsmacking these results are. Going forward, it is safe to say that we will rely heavily on Google and other AI weather models, which are likely to improve in the coming years, as they are relatively new and have room for improvement.&lt;/p&gt;
&lt;p&gt;‚ÄúThe beauty of DeepMind and other similar data-driven, AI-based weather models is how much more quickly they produce a forecast compared to their traditional physics-based counterparts that require some of the most expensive and advanced supercomputers in the world,‚Äù noted Michael Lowry, a hurricane specialist and author of the Eye on the Tropics newsletter, about the model performance. ‚ÄúBeyond that, these ‚Äòsmart‚Äô models with their neural network architectures have the ability to learn from their mistakes and correct on-the-fly.‚Äù&lt;/p&gt;
&lt;h2&gt;What about the North American model?&lt;/h2&gt;
&lt;p&gt;As for the GFS model, it is difficult to explain why it performed so poorly this season. In the past, it has been, at worst, worthy of consideration in making a forecast. But this year, myself and other forecasters often disregarded it.&lt;/p&gt;
&lt;p&gt;‚ÄúIt‚Äôs not immediately clear why the GFS performed so poorly this hurricane season,‚Äù Lowry wrote. ‚ÄúSome have speculated the lapse in data collection from DOGE-related government cuts this year could have been a contributing factor, but presumably such a factor would have affected other global physics-based models as well, not just the American GFS.‚Äù&lt;/p&gt;
&lt;p&gt;With the US government in shutdown mode, we probably cannot expect many answers soon. But it seems clear that the massive upgrade of the model‚Äôs dynamic core, which began in 2019, has largely been a failure. If the GFS was a little bit behind some competitors a decade ago, it is now fading further and faster.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Meanwhile, the US Global Forecasting System continues to get worse.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-640x640.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-1000x648.jpg" width="1000" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hurricane Melissa, with a well-defined eye, can be seen in this satellite image from Monday morning.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          NOAA

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The Atlantic hurricane season is drawing to a close, and with the tropics quieting down for a winter slumber, the focus of forecasters turns to evaluating what worked and what did not during the preceding season.&lt;/p&gt;
&lt;p&gt;This year, the answers are clear. Although Google DeepMind‚Äôs Weather Lab only started releasing cyclone track forecasts in June, the company‚Äôs AI forecasting service performed exceptionally well. By contrast, the Global Forecast System model, operated by the US National Weather Service and is based on traditional physics and runs on powerful supercomputers, performed abysmally.&lt;/p&gt;
&lt;p&gt;The official data comparing forecast model performance will not be published by the National Hurricane Center for a few months. However, Brian McNoldy, a senior researcher at the University of Miami, has already done some preliminary number crunching.&lt;/p&gt;
&lt;p&gt;The results are stunning:&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125648 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="720" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/bafkreia2ecszfb2iz2hican5os3ivjtnx3bkodspx4epbuqnyc3hjnum5m.jpg" width="1080" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      2025 Atlantic season hurricane model performance on track accuracy.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Brian McNoldy

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A little help in reading the graphic is in order. This chart sums up the track forecast accuracy for all 13 named storms in the Atlantic Basin this season, measuring the mean position error at various hours in the forecast, from 0 to 120 hours (five days). On this chart, the lower a line is, the better a model has performed.&lt;/p&gt;
&lt;h2&gt;A new champion&lt;/h2&gt;
&lt;p&gt;The dotted black line shows the average forecast error for official forecasts from the 2022 to 2024 seasons. What jumps out is that the United States‚Äô premier global model, the GFS (denoted here as AVNI), is by far the worst-performing model. Meanwhile, at the bottom of the chart, in maroon, is the Google DeepMind model (GDMI), performing the best at nearly all forecast hours.&lt;/p&gt;
&lt;p&gt;The difference in errors between the US GFS model and Google‚Äôs DeepMind is remarkable. At five days, the Google forecast had an error of 165 nautical miles compared to 360 nautical miles for the GFS model, more than &lt;em&gt;twice&lt;/em&gt; as bad. This is the kind of error that causes forecasters to completely disregard one model in favor of another.&lt;/p&gt;
&lt;p&gt;But there‚Äôs more. Google‚Äôs model was so good that it regularly beat the official forecast from the National Hurricane Center (OFCL), which is produced by human experts looking at a broad array of model data. The AI-based model also beat highly regarded ‚Äúconsensus models,‚Äù including the TVCN and HCCA products. For more information on various models and their designations, see here.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This early model comparison does not include the ‚Äúgold standard‚Äù traditional, physics-based model produced by the European Centre for Medium-Range Weather Forecasts. However, the ECMWF model typically does not do better on hurricane track forecasts than the hurricane center or consensus models, which weigh&amp;nbsp;several different model outputs. So it is unlikely to be superior to Google‚Äôs DeepMind.&lt;/p&gt;
&lt;h2&gt;This will change forecasting forever&lt;/h2&gt;
&lt;p&gt;It‚Äôs worth noting that DeepMind also did exceptionally well at intensity forecasting, which is the fluctuations in the strength of a hurricane. So in its first season, it nailed both hurricane tracks and intensity.&lt;/p&gt;
&lt;p&gt;As a forecaster who has relied on traditional physics-based models for a quarter of a century, it is difficult to say how gobsmacking these results are. Going forward, it is safe to say that we will rely heavily on Google and other AI weather models, which are likely to improve in the coming years, as they are relatively new and have room for improvement.&lt;/p&gt;
&lt;p&gt;‚ÄúThe beauty of DeepMind and other similar data-driven, AI-based weather models is how much more quickly they produce a forecast compared to their traditional physics-based counterparts that require some of the most expensive and advanced supercomputers in the world,‚Äù noted Michael Lowry, a hurricane specialist and author of the Eye on the Tropics newsletter, about the model performance. ‚ÄúBeyond that, these ‚Äòsmart‚Äô models with their neural network architectures have the ability to learn from their mistakes and correct on-the-fly.‚Äù&lt;/p&gt;
&lt;h2&gt;What about the North American model?&lt;/h2&gt;
&lt;p&gt;As for the GFS model, it is difficult to explain why it performed so poorly this season. In the past, it has been, at worst, worthy of consideration in making a forecast. But this year, myself and other forecasters often disregarded it.&lt;/p&gt;
&lt;p&gt;‚ÄúIt‚Äôs not immediately clear why the GFS performed so poorly this hurricane season,‚Äù Lowry wrote. ‚ÄúSome have speculated the lapse in data collection from DOGE-related government cuts this year could have been a contributing factor, but presumably such a factor would have affected other global physics-based models as well, not just the American GFS.‚Äù&lt;/p&gt;
&lt;p&gt;With the US government in shutdown mode, we probably cannot expect many answers soon. But it seems clear that the massive upgrade of the model‚Äôs dynamic core, which began in 2019, has largely been a failure. If the GFS was a little bit behind some competitors a decade ago, it is now fading further and faster.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/science/2025/11/googles-new-weather-model-impressed-during-its-first-hurricane-season/</guid><pubDate>Tue, 04 Nov 2025 22:50:29 +0000</pubDate></item><item><title>[NEW] Amazon sends legal threats to Perplexity over agentic browsing (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/04/amazon-sends-legal-threats-to-perplexity-over-agentic-browsing/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2181996346.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has told Perplexity to get its agentic browser out of its online store, the companies both confirmed publicly on Tuesday. After warning Perplexity multiple times that Comet, its AI-powered shopping assistant, was violating Amazon‚Äôs terms of service by not identifying itself as an agent, the e-commerce giant sent the AI search engine startup a sternly worded cease-and-desist letter, Perplexity wrote in a blog post titled ‚ÄúBullying is not innovation.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThis week, Perplexity received an aggressive legal threat from Amazon, demanding we prohibit Comet users from using their AI assistants on Amazon. This is Amazon‚Äôs first legal salvo against an AI company, and it is a threat to all internet users,‚Äù Perplexity lamented in the blog post.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perplexity‚Äôs argument is that, since its agent is acting on behalf of a human user‚Äôs direction, the agent automatically has the ‚Äúsame permissions‚Äù as the human user. The implication is that it doesn‚Äôt have to identify itself as an agent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon‚Äôs response points out that other third-party agents working at the behest of human users do identify themselves. ‚ÄúIt is how others operate, including food delivery apps and the restaurants they take orders for, delivery service apps and the stores they shop from, and online travel agencies and the airlines they book tickets with for customers,‚Äù Amazon‚Äôs statement explains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Amazon is to be believed, then Perplexity could simply identify its agent and start shopping. Of course, the risk is that Amazon, which has its own shopping bot called Rufus, could also block Comet ‚Äî or any other third-party agentic shopper ‚Äî from its site.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon suggests as much as its statement, which also says, ‚ÄúWe think it‚Äôs fairly straightforward that third-party applications that offer to make purchases on behalf of customers from other businesses should operate openly and respect service provider decisions whether or not to participate.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity claims that Amazon would block the shopping bot because Amazon wants to sell advertising and product placements. Unlike human shoppers, a bot tasked with buying a new laundry basket presumably wouldn‚Äôt find itself buying a more expensive one, or getting lured into buying the latest Brandon Sanderson novel and a new set of earphones (on sale!).&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;If all of this sounds a bit familiar, that‚Äôs because it is. A few months ago, Cloudflare published research accusing Perplexity of scraping websites while specifically defying requests from websites blocking AI bots. Interestingly, many people came to Perplexity‚Äôs defense that time, because this wasn‚Äôt a clear-cut case of web crawler bad behavior. Cloudflare documented how the AI was accessing a specific public website when its user asked about that specific website.&amp;nbsp;Perplexity fans argued that this is exactly what every human-operated web browser does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the other hand, Perplexity was using some questionable methods to do that accessing when a website opted out of bots, like hiding its identity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As TechCrunch reported at the time, the Cloudflare incident foreshadowed the challenges to come if the agentic world materializes as Silicon Valley predicts it will. If consumers and companies outsource their shopping, travel bookings, and restaurant reservations to bots, will it be in the best interest of websites to block bots entirely? How will they allow and work with them?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perplexity may be right in that Amazon is setting a precedent. As the 800-pound gorilla in e-commerce, it is clearly saying that the way this should work is for an agent to identify itself and let the website decide.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2181996346.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has told Perplexity to get its agentic browser out of its online store, the companies both confirmed publicly on Tuesday. After warning Perplexity multiple times that Comet, its AI-powered shopping assistant, was violating Amazon‚Äôs terms of service by not identifying itself as an agent, the e-commerce giant sent the AI search engine startup a sternly worded cease-and-desist letter, Perplexity wrote in a blog post titled ‚ÄúBullying is not innovation.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThis week, Perplexity received an aggressive legal threat from Amazon, demanding we prohibit Comet users from using their AI assistants on Amazon. This is Amazon‚Äôs first legal salvo against an AI company, and it is a threat to all internet users,‚Äù Perplexity lamented in the blog post.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perplexity‚Äôs argument is that, since its agent is acting on behalf of a human user‚Äôs direction, the agent automatically has the ‚Äúsame permissions‚Äù as the human user. The implication is that it doesn‚Äôt have to identify itself as an agent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon‚Äôs response points out that other third-party agents working at the behest of human users do identify themselves. ‚ÄúIt is how others operate, including food delivery apps and the restaurants they take orders for, delivery service apps and the stores they shop from, and online travel agencies and the airlines they book tickets with for customers,‚Äù Amazon‚Äôs statement explains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Amazon is to be believed, then Perplexity could simply identify its agent and start shopping. Of course, the risk is that Amazon, which has its own shopping bot called Rufus, could also block Comet ‚Äî or any other third-party agentic shopper ‚Äî from its site.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon suggests as much as its statement, which also says, ‚ÄúWe think it‚Äôs fairly straightforward that third-party applications that offer to make purchases on behalf of customers from other businesses should operate openly and respect service provider decisions whether or not to participate.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity claims that Amazon would block the shopping bot because Amazon wants to sell advertising and product placements. Unlike human shoppers, a bot tasked with buying a new laundry basket presumably wouldn‚Äôt find itself buying a more expensive one, or getting lured into buying the latest Brandon Sanderson novel and a new set of earphones (on sale!).&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;If all of this sounds a bit familiar, that‚Äôs because it is. A few months ago, Cloudflare published research accusing Perplexity of scraping websites while specifically defying requests from websites blocking AI bots. Interestingly, many people came to Perplexity‚Äôs defense that time, because this wasn‚Äôt a clear-cut case of web crawler bad behavior. Cloudflare documented how the AI was accessing a specific public website when its user asked about that specific website.&amp;nbsp;Perplexity fans argued that this is exactly what every human-operated web browser does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the other hand, Perplexity was using some questionable methods to do that accessing when a website opted out of bots, like hiding its identity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As TechCrunch reported at the time, the Cloudflare incident foreshadowed the challenges to come if the agentic world materializes as Silicon Valley predicts it will. If consumers and companies outsource their shopping, travel bookings, and restaurant reservations to bots, will it be in the best interest of websites to block bots entirely? How will they allow and work with them?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perplexity may be right in that Amazon is setting a precedent. As the 800-pound gorilla in e-commerce, it is clearly saying that the way this should work is for an agent to identify itself and let the website decide.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/04/amazon-sends-legal-threats-to-perplexity-over-agentic-browsing/</guid><pubDate>Tue, 04 Nov 2025 23:05:04 +0000</pubDate></item></channel></rss>