<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 20 Nov 2025 06:32:40 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>OpenAI debuts GPT‑5.1-Codex-Max coding model and it already completed a 24-hour task internally (AI | VentureBeat)</title><link>https://venturebeat.com/ai/openai-debuts-gpt-5-1-codex-max-coding-model-and-it-already-completed-a-24</link><description>[unable to retrieve full-text content]&lt;p&gt;OpenAI has &lt;a href="https://openai.com/index/gpt-5-1-codex-max/"&gt;&lt;b&gt;introduced GPT‑5.1-Codex-Max&lt;/b&gt;&lt;/a&gt;, a new frontier agentic coding model now available in its Codex developer environment. The release marks a significant step forward in AI-assisted software engineering, offering improved long-horizon reasoning, efficiency, and real-time interactive capabilities. GPT‑5.1-Codex-Max will now replace GPT‑5.1-Codex as the default model across Codex-integrated surfaces.&lt;/p&gt;&lt;p&gt;The new model is designed to serve as a persistent, high-context software development agent, capable of managing complex refactors, debugging workflows, and project-scale tasks across multiple context windows.&lt;/p&gt;&lt;p&gt;It comes on the heels of&lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt; Google releasing its powerful new Gemini 3 Pro model&lt;/a&gt; yesterday, yet still outperforms or matches it on key coding benchmarks: &lt;/p&gt;&lt;p&gt;On &lt;b&gt;SWE-Bench Verified&lt;/b&gt;, &lt;b&gt;GPT‑5.1-Codex-Max achieved 77.9% accuracy&lt;/b&gt; at extra-high reasoning effort, edging past Gemini 3 Pro’s 76.2%. &lt;/p&gt;&lt;p&gt;It also led on &lt;b&gt;Terminal-Bench 2.0, with 58.1% accuracy versus Gemini’s 54.2%, &lt;/b&gt;and matched Gemini’s score of 2,439 on LiveCodeBench Pro, a competitive coding Elo benchmark.&lt;/p&gt;&lt;p&gt;When measured against Gemini 3 Pro’s most advanced configuration — its Deep Thinking model — Codex-Max holds a slight edge in agentic coding benchmarks, as well. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance Benchmarks: Incremental Gains Across Key Tasks&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;GPT‑5.1-Codex-Max demonstrates measurable improvements over GPT‑5.1-Codex across a range of standard software engineering benchmarks. &lt;/p&gt;&lt;p&gt;On SWE-Lancer IC SWE, it achieved 79.9% accuracy, a significant increase from GPT‑5.1-Codex’s 66.3%. In SWE-Bench Verified (n=500), it reached 77.9% accuracy at extra-high reasoning effort, outperforming GPT‑5.1-Codex’s 73.7%.&lt;/p&gt;&lt;p&gt;Performance on Terminal Bench 2.0 (n=89) showed more modest improvements, with GPT‑5.1-Codex-Max achieving 58.1% accuracy compared to 52.8% for GPT‑5.1-Codex. &lt;/p&gt;&lt;p&gt;All evaluations were run with compaction and extra-high reasoning effort enabled.&lt;/p&gt;&lt;p&gt;These results indicate that the new model offers a higher ceiling on both benchmarked correctness and real-world usability under extended reasoning loads.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical Architecture: Long-Horizon Reasoning via Compaction&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;A major architectural improvement in GPT‑5.1-Codex-Max is its ability to reason effectively over extended input-output sessions using a mechanism called &lt;b&gt;compaction&lt;/b&gt;. &lt;/p&gt;&lt;p&gt;This enables the model to retain key contextual information while discarding irrelevant details as it nears its context window limit — effectively allowing for continuous work across millions of tokens without performance degradation.&lt;/p&gt;&lt;p&gt;The model has been internally observed to complete tasks lasting more than 24 hours, including multi-step refactors, test-driven iteration, and autonomous debugging.&lt;/p&gt;&lt;p&gt;Compaction also improves token efficiency. At medium reasoning effort, GPT‑5.1-Codex-Max used approximately 30% fewer thinking tokens than GPT‑5.1-Codex for comparable or better accuracy, which has implications for both cost and latency.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Platform Integration and Use Cases&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;GPT‑5.1-Codex-Max is currently available across multiple Codex-based environments, which refer to OpenAI’s own integrated tools and interfaces built specifically for code-focused AI agents. These include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Codex CLI&lt;/b&gt;, OpenAI’s official command-line tool (@openai/codex), where GPT‑5.1-Codex-Max is already live.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;IDE extensions&lt;/b&gt;, likely developed or maintained by OpenAI, though no specific third-party IDE integrations were named.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Interactive coding environments&lt;/b&gt;, such as those used to demonstrate frontend simulation apps like CartPole or Snell’s Law Explorer.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Internal code review tooling&lt;/b&gt;, used by OpenAI’s engineering teams.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For now, GPT‑5.1-Codex-Max is not yet available via public API, though OpenAI states this is coming soon. Users who wish to work with the model in terminal environments today can do so by installing and using the Codex CLI.&lt;/p&gt;&lt;p&gt;It is not currently confirmed whether or how the model will integrate into third-party IDEs unless they are built on top of the CLI or future API.&lt;/p&gt;&lt;p&gt;The model is capable of interacting with live tools and simulations. Examples shown in the release include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;An interactive CartPole policy gradient simulator, which visualizes reinforcement learning training and activations.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Snell’s Law optics explorer, supporting dynamic ray tracing across refractive indices.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These interfaces exemplify the model’s ability to reason in real time while maintaining an interactive development session — effectively bridging computation, visualization, and implementation within a single loop.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Cybersecurity and Safety Constraints&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While GPT‑5.1-Codex-Max does not meet OpenAI’s “High” capability threshold for cybersecurity under its Preparedness Framework, it is currently the most capable cybersecurity model OpenAI has deployed. It supports use cases such as automated vulnerability detection and remediation, but with strict sandboxing and disabled network access by default.&lt;/p&gt;&lt;p&gt;OpenAI reports no increase in scaled malicious use but has introduced enhanced monitoring systems, including activity routing and disruption mechanisms for suspicious behavior. Codex remains isolated to a local workspace unless developers opt-in to broader access, mitigating risks like prompt injection from untrusted content.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deployment Context and Developer Usage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;GPT‑5.1-Codex-Max is currently available to users on &lt;b&gt;ChatGPT Plus, Pro, Business, Edu, and Enterprise&lt;/b&gt; plans. It will also become the new default in Codex-based environments, replacing GPT‑5.1-Codex, which was a more general-purpose model.&lt;/p&gt;&lt;p&gt;OpenAI states that 95% of its internal engineers use Codex weekly, and since adoption, these engineers have shipped ~70% more pull requests on average — highlighting the tool’s impact on internal development velocity.&lt;/p&gt;&lt;p&gt;Despite its autonomy and persistence, OpenAI stresses that Codex-Max should be treated as a coding assistant, not a replacement for human review. The model produces terminal logs, test citations, and tool call outputs to support transparency in generated code.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Outlook&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;GPT‑5.1-Codex-Max represents a significant evolution in OpenAI’s strategy toward agentic development tools, offering greater reasoning depth, token efficiency, and interactive capabilities across software engineering tasks. By extending its context management and compaction strategies, the model is positioned to handle tasks at the scale of full repositories, rather than individual files or snippets.&lt;/p&gt;&lt;p&gt;With continued emphasis on agentic workflows, secure sandboxes, and real-world evaluation metrics, Codex-Max sets the stage for the next generation of AI-assisted programming environments — while underscoring the importance of oversight in increasingly autonomous systems.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;OpenAI has &lt;a href="https://openai.com/index/gpt-5-1-codex-max/"&gt;&lt;b&gt;introduced GPT‑5.1-Codex-Max&lt;/b&gt;&lt;/a&gt;, a new frontier agentic coding model now available in its Codex developer environment. The release marks a significant step forward in AI-assisted software engineering, offering improved long-horizon reasoning, efficiency, and real-time interactive capabilities. GPT‑5.1-Codex-Max will now replace GPT‑5.1-Codex as the default model across Codex-integrated surfaces.&lt;/p&gt;&lt;p&gt;The new model is designed to serve as a persistent, high-context software development agent, capable of managing complex refactors, debugging workflows, and project-scale tasks across multiple context windows.&lt;/p&gt;&lt;p&gt;It comes on the heels of&lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt; Google releasing its powerful new Gemini 3 Pro model&lt;/a&gt; yesterday, yet still outperforms or matches it on key coding benchmarks: &lt;/p&gt;&lt;p&gt;On &lt;b&gt;SWE-Bench Verified&lt;/b&gt;, &lt;b&gt;GPT‑5.1-Codex-Max achieved 77.9% accuracy&lt;/b&gt; at extra-high reasoning effort, edging past Gemini 3 Pro’s 76.2%. &lt;/p&gt;&lt;p&gt;It also led on &lt;b&gt;Terminal-Bench 2.0, with 58.1% accuracy versus Gemini’s 54.2%, &lt;/b&gt;and matched Gemini’s score of 2,439 on LiveCodeBench Pro, a competitive coding Elo benchmark.&lt;/p&gt;&lt;p&gt;When measured against Gemini 3 Pro’s most advanced configuration — its Deep Thinking model — Codex-Max holds a slight edge in agentic coding benchmarks, as well. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance Benchmarks: Incremental Gains Across Key Tasks&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;GPT‑5.1-Codex-Max demonstrates measurable improvements over GPT‑5.1-Codex across a range of standard software engineering benchmarks. &lt;/p&gt;&lt;p&gt;On SWE-Lancer IC SWE, it achieved 79.9% accuracy, a significant increase from GPT‑5.1-Codex’s 66.3%. In SWE-Bench Verified (n=500), it reached 77.9% accuracy at extra-high reasoning effort, outperforming GPT‑5.1-Codex’s 73.7%.&lt;/p&gt;&lt;p&gt;Performance on Terminal Bench 2.0 (n=89) showed more modest improvements, with GPT‑5.1-Codex-Max achieving 58.1% accuracy compared to 52.8% for GPT‑5.1-Codex. &lt;/p&gt;&lt;p&gt;All evaluations were run with compaction and extra-high reasoning effort enabled.&lt;/p&gt;&lt;p&gt;These results indicate that the new model offers a higher ceiling on both benchmarked correctness and real-world usability under extended reasoning loads.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical Architecture: Long-Horizon Reasoning via Compaction&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;A major architectural improvement in GPT‑5.1-Codex-Max is its ability to reason effectively over extended input-output sessions using a mechanism called &lt;b&gt;compaction&lt;/b&gt;. &lt;/p&gt;&lt;p&gt;This enables the model to retain key contextual information while discarding irrelevant details as it nears its context window limit — effectively allowing for continuous work across millions of tokens without performance degradation.&lt;/p&gt;&lt;p&gt;The model has been internally observed to complete tasks lasting more than 24 hours, including multi-step refactors, test-driven iteration, and autonomous debugging.&lt;/p&gt;&lt;p&gt;Compaction also improves token efficiency. At medium reasoning effort, GPT‑5.1-Codex-Max used approximately 30% fewer thinking tokens than GPT‑5.1-Codex for comparable or better accuracy, which has implications for both cost and latency.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Platform Integration and Use Cases&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;GPT‑5.1-Codex-Max is currently available across multiple Codex-based environments, which refer to OpenAI’s own integrated tools and interfaces built specifically for code-focused AI agents. These include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Codex CLI&lt;/b&gt;, OpenAI’s official command-line tool (@openai/codex), where GPT‑5.1-Codex-Max is already live.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;IDE extensions&lt;/b&gt;, likely developed or maintained by OpenAI, though no specific third-party IDE integrations were named.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Interactive coding environments&lt;/b&gt;, such as those used to demonstrate frontend simulation apps like CartPole or Snell’s Law Explorer.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Internal code review tooling&lt;/b&gt;, used by OpenAI’s engineering teams.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For now, GPT‑5.1-Codex-Max is not yet available via public API, though OpenAI states this is coming soon. Users who wish to work with the model in terminal environments today can do so by installing and using the Codex CLI.&lt;/p&gt;&lt;p&gt;It is not currently confirmed whether or how the model will integrate into third-party IDEs unless they are built on top of the CLI or future API.&lt;/p&gt;&lt;p&gt;The model is capable of interacting with live tools and simulations. Examples shown in the release include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;An interactive CartPole policy gradient simulator, which visualizes reinforcement learning training and activations.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Snell’s Law optics explorer, supporting dynamic ray tracing across refractive indices.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These interfaces exemplify the model’s ability to reason in real time while maintaining an interactive development session — effectively bridging computation, visualization, and implementation within a single loop.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Cybersecurity and Safety Constraints&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While GPT‑5.1-Codex-Max does not meet OpenAI’s “High” capability threshold for cybersecurity under its Preparedness Framework, it is currently the most capable cybersecurity model OpenAI has deployed. It supports use cases such as automated vulnerability detection and remediation, but with strict sandboxing and disabled network access by default.&lt;/p&gt;&lt;p&gt;OpenAI reports no increase in scaled malicious use but has introduced enhanced monitoring systems, including activity routing and disruption mechanisms for suspicious behavior. Codex remains isolated to a local workspace unless developers opt-in to broader access, mitigating risks like prompt injection from untrusted content.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deployment Context and Developer Usage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;GPT‑5.1-Codex-Max is currently available to users on &lt;b&gt;ChatGPT Plus, Pro, Business, Edu, and Enterprise&lt;/b&gt; plans. It will also become the new default in Codex-based environments, replacing GPT‑5.1-Codex, which was a more general-purpose model.&lt;/p&gt;&lt;p&gt;OpenAI states that 95% of its internal engineers use Codex weekly, and since adoption, these engineers have shipped ~70% more pull requests on average — highlighting the tool’s impact on internal development velocity.&lt;/p&gt;&lt;p&gt;Despite its autonomy and persistence, OpenAI stresses that Codex-Max should be treated as a coding assistant, not a replacement for human review. The model produces terminal logs, test citations, and tool call outputs to support transparency in generated code.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Outlook&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;GPT‑5.1-Codex-Max represents a significant evolution in OpenAI’s strategy toward agentic development tools, offering greater reasoning depth, token efficiency, and interactive capabilities across software engineering tasks. By extending its context management and compaction strategies, the model is positioned to handle tasks at the scale of full repositories, rather than individual files or snippets.&lt;/p&gt;&lt;p&gt;With continued emphasis on agentic workflows, secure sandboxes, and real-world evaluation metrics, Codex-Max sets the stage for the next generation of AI-assisted programming environments — while underscoring the importance of oversight in increasingly autonomous systems.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-debuts-gpt-5-1-codex-max-coding-model-and-it-already-completed-a-24</guid><pubDate>Wed, 19 Nov 2025 19:26:00 +0000</pubDate></item><item><title>Warner Music settles copyright lawsuit with Udio, signs deal for AI music platform (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/19/warner-music-settles-copyright-lawsuit-with-udio-signs-deal-for-ai-music-platform/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2155987987.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Warner Music Group (WMG) has settled a copyright infringement case with AI music startup Udio, the label announced on Wednesday. The two have also entered into a licensing deal for an AI music creation service that’s set to launch in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In its press release, WMG said that the “next-generation music creation, listening, and discovery platform” will be powered by generative AI models trained on licensed and authorized music.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company says the platform will create “new revenue streams for artists and songwriters, while ensuring their work remains protected.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The subscription service will allow users to make remixes, covers, and new songs using the voices of artists and compositions of songwriters who choose to participate. Warner Music Group says the platform will ensure artists and songwriters are credited and compensated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re unwaveringly committed to the protection of the rights of our artists and songwriters, and Udio has taken meaningful steps to ensure that the music on its service will be authorized and licensed,” said WMG CEO Robert Kyncl in the press release. “This collaboration aligns with our broader efforts to responsibly unlock AI’s potential — fueling new creative and commercial possibilities while continuing to deliver innovative experiences for fans.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Artists signed to WMG include Lady Gaga, Coldplay, The Weeknd, Sabrina Carpenter, and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This partnership is a crucial step towards realizing a future in which technology amplifies creativity and unlocks new opportunities for artists and songwriters,” said Udio co-founder and CEO Andrew Sanchez in the press release. “Our new platform will enable experiences where fans can create alongside their favorite artists and make extraordinary music in an environment that offers artists control and connection.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The settlement marks a significant shift in the music industry’s approach to AI. Warner Music Group, Universal Music Group, and Sony Music Entertainment sued Udio and rival AI music platform Suno last year for copyright infringement. Both platforms allow users to generate songs using AI-powered text prompts. Universal Music Group and Sony Music Entertainment are also reportedly in talks to license their work to Udio and Suno.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a sign of investor confidence in AI music technology, Suno announced earlier on Wednesday that it has raised a $250 million Series C round at a $2.45 billion post-money valuation. The round was led by Menlo Ventures with participation from Nvidia’s venture arm NVentures, as well as Hallwood Media, Lightspeed, and Matrix.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2155987987.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Warner Music Group (WMG) has settled a copyright infringement case with AI music startup Udio, the label announced on Wednesday. The two have also entered into a licensing deal for an AI music creation service that’s set to launch in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In its press release, WMG said that the “next-generation music creation, listening, and discovery platform” will be powered by generative AI models trained on licensed and authorized music.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company says the platform will create “new revenue streams for artists and songwriters, while ensuring their work remains protected.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The subscription service will allow users to make remixes, covers, and new songs using the voices of artists and compositions of songwriters who choose to participate. Warner Music Group says the platform will ensure artists and songwriters are credited and compensated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re unwaveringly committed to the protection of the rights of our artists and songwriters, and Udio has taken meaningful steps to ensure that the music on its service will be authorized and licensed,” said WMG CEO Robert Kyncl in the press release. “This collaboration aligns with our broader efforts to responsibly unlock AI’s potential — fueling new creative and commercial possibilities while continuing to deliver innovative experiences for fans.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Artists signed to WMG include Lady Gaga, Coldplay, The Weeknd, Sabrina Carpenter, and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This partnership is a crucial step towards realizing a future in which technology amplifies creativity and unlocks new opportunities for artists and songwriters,” said Udio co-founder and CEO Andrew Sanchez in the press release. “Our new platform will enable experiences where fans can create alongside their favorite artists and make extraordinary music in an environment that offers artists control and connection.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The settlement marks a significant shift in the music industry’s approach to AI. Warner Music Group, Universal Music Group, and Sony Music Entertainment sued Udio and rival AI music platform Suno last year for copyright infringement. Both platforms allow users to generate songs using AI-powered text prompts. Universal Music Group and Sony Music Entertainment are also reportedly in talks to license their work to Udio and Suno.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a sign of investor confidence in AI music technology, Suno announced earlier on Wednesday that it has raised a $250 million Series C round at a $2.45 billion post-money valuation. The round was led by Menlo Ventures with participation from Nvidia’s venture arm NVentures, as well as Hallwood Media, Lightspeed, and Matrix.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/19/warner-music-settles-copyright-lawsuit-with-udio-signs-deal-for-ai-music-platform/</guid><pubDate>Wed, 19 Nov 2025 19:57:09 +0000</pubDate></item><item><title>Critics scoff after Microsoft warns AI feature can infect machines and pilfer data (AI – Ars Technica)</title><link>https://arstechnica.com/security/2025/11/critics-scoff-after-microsoft-warns-ai-feature-can-infect-machines-and-pilfer-data/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Integration of Copilot Actions into Windows is off by default, but for how long?
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/microsoft-copilot-windows-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/microsoft-copilot-windows-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Photographer: Chona Kasinger/Bloomberg via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Microsoft’s warning on Tuesday that an experimental AI agent integrated into Windows can infect devices and pilfer sensitive user data has set off a familiar response from security-minded critics: Why is Big Tech so intent on pushing new features before their dangerous behaviors can be fully understood and contained?&lt;/p&gt;
&lt;p&gt;As reported Tuesday, Microsoft introduced Copilot Actions, a new set of “experimental agentic features” that, when enabled, perform “everyday tasks like organizing files, scheduling meetings, or sending emails,” and provide “an active digital collaborator that can carry out complex tasks for you to enhance efficiency and productivity.”&lt;/p&gt;
&lt;h2&gt;Hallucinations and prompt injections apply&lt;/h2&gt;
&lt;p&gt;The fanfare, however, came with a significant caveat. Microsoft recommended users enable Copilot Actions only “if you understand the security implications outlined.”&lt;/p&gt;
&lt;p&gt;The admonition is based on known defects inherent in most large language models, including Copilot, as researchers have repeatedly demonstrated.&lt;/p&gt;
&lt;p&gt;One common defect of LLMs causes them to provide factually erroneous and illogical answers, sometimes even to the most basic questions. This propensity for hallucinations, as the behavior has come to be called, means users can’t trust the output of Copilot, Gemini, Claude, or any other AI assistant and instead must independently confirm it.&lt;/p&gt;
&lt;p&gt;Another common LLM landmine is the prompt injection, a class of bug that allows hackers to plant malicious instructions in websites, resumes, and emails. LLMs are programmed to follow directions so eagerly that they are unable to discern those in valid user prompts from those contained in untrusted, third-party content created by attackers. As a result, the LLMs give the attackers the same deference as users.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Both flaws can be exploited in attacks that exfiltrate sensitive data, run malicious code, and steal cryptocurrency. So far, these vulnerabilities have proved impossible for developers to prevent and, in many cases, can only be fixed using bug-specific workarounds developed once a vulnerability has been discovered.&lt;/p&gt;
&lt;p&gt;That, in turn, led to this whopper of a disclosure in Microsoft’s post from Tuesday:&lt;/p&gt;
&lt;p&gt;“As these capabilities are introduced, AI models still face functional limitations in terms of how they behave and occasionally may hallucinate and produce unexpected outputs,” Microsoft said. “Additionally, agentic AI applications introduce novel security risks, such as cross-prompt injection (XPIA), where malicious content embedded in UI elements or documents can override agent instructions, leading to unintended actions like data exfiltration or malware installation.”&lt;/p&gt;
&lt;p&gt;Microsoft indicated that only experienced users should enable Copilot Actions, which is currently available only in beta versions of Windows. The company, however, didn’t describe what type of training or experience such users should have or what actions they should take to prevent their devices from being compromised. I asked Microsoft to provide these details, and the company declined.&lt;/p&gt;
&lt;h2&gt;Like “macros on Marvel superhero crack”&lt;/h2&gt;
&lt;p&gt;Some security experts questioned the value of the warnings in Tuesday’s post, comparing them to warnings Microsoft has provided for decades about the danger of using macros in Office apps. Despite the long-standing advice, macros have remained among the lowest-hanging fruit for hackers out to surreptitiously install malware on Windows machines. One reason for this is that Microsoft has made macros so central to productivity that many users can’t do without them.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Microsoft saying ‘don’t enable macros, they’re dangerous’… has never worked well,” independent researcher Kevin Beaumont said. “This is macros on Marvel superhero crack.”&lt;/p&gt;
&lt;p&gt;Beaumont, who is regularly hired to respond to major Windows network compromises inside enterprises, also questioned whether Microsoft will provide a means for admins to adequately restrict Copilot Actions on end-user machines or to identify machines in a network that have the feature turned on.&lt;/p&gt;
&lt;p&gt;A Microsoft spokesperson said IT admins will be able to enable or disable an agent workspace at both account and device levels, using Intune or other MDM (Mobile Device Management) apps.&lt;/p&gt;
&lt;p&gt;Critics voiced other concerns, including the difficulty for even experienced users to detect exploitation attacks targeting the AI agents they’re using.&lt;/p&gt;
&lt;p&gt;“I don’t see how users are going to prevent anything of the sort they are referring to, beyond not surfing the web I guess,” researcher Guillaume Rossolini said.&lt;/p&gt;
&lt;p&gt;Microsoft has stressed that Copilot Actions is an experimental feature that’s turned off by default. That design was likely chosen to limit its access to users with the experience required to understand its risks. Critics, however, noted that previous experimental features—Copilot, for instance—regularly become default capabilities for all users over time. Once that’s done, users who don’t trust the feature are often required to invest time developing unsupported ways to remove the features.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Sound but lofty goals&lt;/h2&gt;
&lt;p&gt;Most of Tuesday’s post focused on Microsoft’s overall strategy for securing agentic features in Windows. Goals for such features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-repudiation, meaning all actions and behaviors must be “observable and distinguishable from those taken by a user”&lt;/li&gt;
&lt;li&gt;Agents must preserve confidentiality when they collect, aggregate, or otherwise utilize user data&lt;/li&gt;
&lt;li&gt;Agents must receive user approval when accessing user data or taking actions&lt;/li&gt;
&lt;/ul&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The goals are sound, but ultimately they depend on users reading the dialog windows that warn of the risks and require careful approval before proceeding. That, in turn, diminishes the value of the protection for many users.&lt;/p&gt;
&lt;p&gt;“The usual caveat applies to such mechanisms that rely on users clicking through a permission prompt,” Earlence Fernandes, a University of California, San Diego professor specializing in AI security, told Ars. “Sometimes those users don’t fully understand what is going on, or they might just get habituated and click ‘yes’ all the time. At which point, the security boundary is not really a boundary.”&lt;/p&gt;
&lt;p&gt;As demonstrated by the rash of “ClickFix” attacks, many users can be tricked into following extremely dangerous instructions. While more experienced users (including a fair number of Ars commenters) blame the victims falling for such scams, these incidents are inevitable for a host of reasons. In some cases, even careful users are fatigued or under emotional distress and slip up as a result. Other users simply lack the knowledge to make informed decisions.&lt;/p&gt;
&lt;p&gt;Microsoft’s warning, one critic said, amounts to little more than a CYA (short for cover your ass), a legal maneuver that attempts to shield a party from liability.&lt;/p&gt;
&lt;p&gt;“Microsoft (like the rest of the industry) has no idea how to stop prompt injection or hallucinations, which makes it fundamentally unfit for almost anything serious,” critic Reed Mideke said. “The solution? Shift liability to the user. Just like every LLM chatbot has a ‘oh by the way, if you use this for anything important be sure to verify the answers” disclaimer, never mind that you wouldn’t need the chatbot in the first place if you knew the answer.”&lt;/p&gt;
&lt;p&gt;As Mideke indicated, most of the criticisms extend to AI offerings other companies—including Apple, Google, and Meta—are integrating into their products. Frequently, these integrations begin as optional features and eventually become default capabilities whether users want them or not.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Integration of Copilot Actions into Windows is off by default, but for how long?
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/microsoft-copilot-windows-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/microsoft-copilot-windows-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Photographer: Chona Kasinger/Bloomberg via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Microsoft’s warning on Tuesday that an experimental AI agent integrated into Windows can infect devices and pilfer sensitive user data has set off a familiar response from security-minded critics: Why is Big Tech so intent on pushing new features before their dangerous behaviors can be fully understood and contained?&lt;/p&gt;
&lt;p&gt;As reported Tuesday, Microsoft introduced Copilot Actions, a new set of “experimental agentic features” that, when enabled, perform “everyday tasks like organizing files, scheduling meetings, or sending emails,” and provide “an active digital collaborator that can carry out complex tasks for you to enhance efficiency and productivity.”&lt;/p&gt;
&lt;h2&gt;Hallucinations and prompt injections apply&lt;/h2&gt;
&lt;p&gt;The fanfare, however, came with a significant caveat. Microsoft recommended users enable Copilot Actions only “if you understand the security implications outlined.”&lt;/p&gt;
&lt;p&gt;The admonition is based on known defects inherent in most large language models, including Copilot, as researchers have repeatedly demonstrated.&lt;/p&gt;
&lt;p&gt;One common defect of LLMs causes them to provide factually erroneous and illogical answers, sometimes even to the most basic questions. This propensity for hallucinations, as the behavior has come to be called, means users can’t trust the output of Copilot, Gemini, Claude, or any other AI assistant and instead must independently confirm it.&lt;/p&gt;
&lt;p&gt;Another common LLM landmine is the prompt injection, a class of bug that allows hackers to plant malicious instructions in websites, resumes, and emails. LLMs are programmed to follow directions so eagerly that they are unable to discern those in valid user prompts from those contained in untrusted, third-party content created by attackers. As a result, the LLMs give the attackers the same deference as users.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Both flaws can be exploited in attacks that exfiltrate sensitive data, run malicious code, and steal cryptocurrency. So far, these vulnerabilities have proved impossible for developers to prevent and, in many cases, can only be fixed using bug-specific workarounds developed once a vulnerability has been discovered.&lt;/p&gt;
&lt;p&gt;That, in turn, led to this whopper of a disclosure in Microsoft’s post from Tuesday:&lt;/p&gt;
&lt;p&gt;“As these capabilities are introduced, AI models still face functional limitations in terms of how they behave and occasionally may hallucinate and produce unexpected outputs,” Microsoft said. “Additionally, agentic AI applications introduce novel security risks, such as cross-prompt injection (XPIA), where malicious content embedded in UI elements or documents can override agent instructions, leading to unintended actions like data exfiltration or malware installation.”&lt;/p&gt;
&lt;p&gt;Microsoft indicated that only experienced users should enable Copilot Actions, which is currently available only in beta versions of Windows. The company, however, didn’t describe what type of training or experience such users should have or what actions they should take to prevent their devices from being compromised. I asked Microsoft to provide these details, and the company declined.&lt;/p&gt;
&lt;h2&gt;Like “macros on Marvel superhero crack”&lt;/h2&gt;
&lt;p&gt;Some security experts questioned the value of the warnings in Tuesday’s post, comparing them to warnings Microsoft has provided for decades about the danger of using macros in Office apps. Despite the long-standing advice, macros have remained among the lowest-hanging fruit for hackers out to surreptitiously install malware on Windows machines. One reason for this is that Microsoft has made macros so central to productivity that many users can’t do without them.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Microsoft saying ‘don’t enable macros, they’re dangerous’… has never worked well,” independent researcher Kevin Beaumont said. “This is macros on Marvel superhero crack.”&lt;/p&gt;
&lt;p&gt;Beaumont, who is regularly hired to respond to major Windows network compromises inside enterprises, also questioned whether Microsoft will provide a means for admins to adequately restrict Copilot Actions on end-user machines or to identify machines in a network that have the feature turned on.&lt;/p&gt;
&lt;p&gt;A Microsoft spokesperson said IT admins will be able to enable or disable an agent workspace at both account and device levels, using Intune or other MDM (Mobile Device Management) apps.&lt;/p&gt;
&lt;p&gt;Critics voiced other concerns, including the difficulty for even experienced users to detect exploitation attacks targeting the AI agents they’re using.&lt;/p&gt;
&lt;p&gt;“I don’t see how users are going to prevent anything of the sort they are referring to, beyond not surfing the web I guess,” researcher Guillaume Rossolini said.&lt;/p&gt;
&lt;p&gt;Microsoft has stressed that Copilot Actions is an experimental feature that’s turned off by default. That design was likely chosen to limit its access to users with the experience required to understand its risks. Critics, however, noted that previous experimental features—Copilot, for instance—regularly become default capabilities for all users over time. Once that’s done, users who don’t trust the feature are often required to invest time developing unsupported ways to remove the features.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Sound but lofty goals&lt;/h2&gt;
&lt;p&gt;Most of Tuesday’s post focused on Microsoft’s overall strategy for securing agentic features in Windows. Goals for such features include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-repudiation, meaning all actions and behaviors must be “observable and distinguishable from those taken by a user”&lt;/li&gt;
&lt;li&gt;Agents must preserve confidentiality when they collect, aggregate, or otherwise utilize user data&lt;/li&gt;
&lt;li&gt;Agents must receive user approval when accessing user data or taking actions&lt;/li&gt;
&lt;/ul&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The goals are sound, but ultimately they depend on users reading the dialog windows that warn of the risks and require careful approval before proceeding. That, in turn, diminishes the value of the protection for many users.&lt;/p&gt;
&lt;p&gt;“The usual caveat applies to such mechanisms that rely on users clicking through a permission prompt,” Earlence Fernandes, a University of California, San Diego professor specializing in AI security, told Ars. “Sometimes those users don’t fully understand what is going on, or they might just get habituated and click ‘yes’ all the time. At which point, the security boundary is not really a boundary.”&lt;/p&gt;
&lt;p&gt;As demonstrated by the rash of “ClickFix” attacks, many users can be tricked into following extremely dangerous instructions. While more experienced users (including a fair number of Ars commenters) blame the victims falling for such scams, these incidents are inevitable for a host of reasons. In some cases, even careful users are fatigued or under emotional distress and slip up as a result. Other users simply lack the knowledge to make informed decisions.&lt;/p&gt;
&lt;p&gt;Microsoft’s warning, one critic said, amounts to little more than a CYA (short for cover your ass), a legal maneuver that attempts to shield a party from liability.&lt;/p&gt;
&lt;p&gt;“Microsoft (like the rest of the industry) has no idea how to stop prompt injection or hallucinations, which makes it fundamentally unfit for almost anything serious,” critic Reed Mideke said. “The solution? Shift liability to the user. Just like every LLM chatbot has a ‘oh by the way, if you use this for anything important be sure to verify the answers” disclaimer, never mind that you wouldn’t need the chatbot in the first place if you knew the answer.”&lt;/p&gt;
&lt;p&gt;As Mideke indicated, most of the criticisms extend to AI offerings other companies—including Apple, Google, and Meta—are integrating into their products. Frequently, these integrations begin as optional features and eventually become default capabilities whether users want them or not.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/security/2025/11/critics-scoff-after-microsoft-warns-ai-feature-can-infect-machines-and-pilfer-data/</guid><pubDate>Wed, 19 Nov 2025 20:25:46 +0000</pubDate></item><item><title>Function Health raises $298M Series B at $2.5B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/19/function-health-closes-298m-series-b-at-a-2-5b-valuation-launches-medical-intelligence/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1132225622-e1709210197672.jpg?resize=1200,812" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;From electronic health records and blood tests to the stream of data from wearable devices, the amount of health information people generate is accelerating rapidly. Yet, many users struggle to connect this trove of data in a meaningful way and actually use it to improve their health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Function Health, which offers a regular lab testing service to help people track their health, wants to change that by consolidating health data and making it usable for its customers by connecting that data to an AI model. To further that effort, the company recently raised $298 million in a Series B round led by Redpoint Ventures at a valuation of $2.5 billion. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The funding round also saw participation from a16z; Aglaé Ventures; Alumni Ventures; NBA athletes Allen Crabbe, Blake Griffin, and Taylor Griffin; Battery Ventures; Nat Friedman and Daniel Gross’ investment firm, NFDG; and Roku founder Anthony Wood. The round brings the company’s total capital raised to $350 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the funding, Function unveiled Medical Intelligence Lab, an effort to build a “medical intelligence” generative AI model that can be used to provide personalized health insights based on users’ data, content, and research. The company said the model is trained by doctors. For its customers, the company is offering an AI chatbot that can answer questions based on their health data and can tap their previous lab results, doctor’s notes, and scans to provide tailored guidance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It is not good enough to be in a world where AI exists and not be applying it to your health,” Jonathan Swerdlin, CEO and co-founder of Function, told TechCrunch. “You should be able to manage your biology. The objective of Function Health is to apply the best available technology to human health.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Swerdlin noted the platform meets HIPAA standards, fully encrypts user data, and never sells personal information. “Your data and your identity are never for sale. Every bit of your information is fully encrypted and protected. We are committed to keeping you, and your data, safe.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Function’s chief medical scientist, Dr. Dan Sodickson, and its co-founder and chief medical officer Dr. Mark Hyman, are together leading development of MI Lab and its team of doctors, researchers, and engineers.&amp;nbsp;The MI model is trained by doctors and they stay involved in the process, Swerdlin said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;While the space has many players, Function sets itself apart from competitors like Superpower, Neko Health, and InsideTracker thanks to its device-agnostic approach, Swerdlin said, adding that the platform integrates lab testing, diagnostics, and clinical insights to offer more than a typical AI coach or wellness app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Function has 75 locations in the U.S. and members “can get lab tests done at 2,000 Quest locations”, he added. Function says it has completed more than 50 million lab tests since 2023.&lt;/p&gt;









&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated to reflect that Function members can access tests at 2,000 quest locations.&lt;/em&gt; &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1132225622-e1709210197672.jpg?resize=1200,812" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;From electronic health records and blood tests to the stream of data from wearable devices, the amount of health information people generate is accelerating rapidly. Yet, many users struggle to connect this trove of data in a meaningful way and actually use it to improve their health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Function Health, which offers a regular lab testing service to help people track their health, wants to change that by consolidating health data and making it usable for its customers by connecting that data to an AI model. To further that effort, the company recently raised $298 million in a Series B round led by Redpoint Ventures at a valuation of $2.5 billion. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The funding round also saw participation from a16z; Aglaé Ventures; Alumni Ventures; NBA athletes Allen Crabbe, Blake Griffin, and Taylor Griffin; Battery Ventures; Nat Friedman and Daniel Gross’ investment firm, NFDG; and Roku founder Anthony Wood. The round brings the company’s total capital raised to $350 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the funding, Function unveiled Medical Intelligence Lab, an effort to build a “medical intelligence” generative AI model that can be used to provide personalized health insights based on users’ data, content, and research. The company said the model is trained by doctors. For its customers, the company is offering an AI chatbot that can answer questions based on their health data and can tap their previous lab results, doctor’s notes, and scans to provide tailored guidance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It is not good enough to be in a world where AI exists and not be applying it to your health,” Jonathan Swerdlin, CEO and co-founder of Function, told TechCrunch. “You should be able to manage your biology. The objective of Function Health is to apply the best available technology to human health.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Swerdlin noted the platform meets HIPAA standards, fully encrypts user data, and never sells personal information. “Your data and your identity are never for sale. Every bit of your information is fully encrypted and protected. We are committed to keeping you, and your data, safe.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Function’s chief medical scientist, Dr. Dan Sodickson, and its co-founder and chief medical officer Dr. Mark Hyman, are together leading development of MI Lab and its team of doctors, researchers, and engineers.&amp;nbsp;The MI model is trained by doctors and they stay involved in the process, Swerdlin said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;While the space has many players, Function sets itself apart from competitors like Superpower, Neko Health, and InsideTracker thanks to its device-agnostic approach, Swerdlin said, adding that the platform integrates lab testing, diagnostics, and clinical insights to offer more than a typical AI coach or wellness app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Function has 75 locations in the U.S. and members “can get lab tests done at 2,000 Quest locations”, he added. Function says it has completed more than 50 million lab tests since 2023.&lt;/p&gt;









&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated to reflect that Function members can access tests at 2,000 quest locations.&lt;/em&gt; &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/19/function-health-closes-298m-series-b-at-a-2-5b-valuation-launches-medical-intelligence/</guid><pubDate>Wed, 19 Nov 2025 20:30:00 +0000</pubDate></item><item><title>VC Jennifer Neundorfer explains how founders can stand out in a crowded AI market (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/19/vc-jennifer-neundorfer-explains-how-founders-can-stand-out-in-a-crowded-ai-market/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54886289739_6fa8cfb36d_o.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;January Ventures co-founder Jennifer Neundorfer stopped by the Equity podcast during TechCrunch Disrupt to chat about fundraising in this very AI-driven market.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founders and investors alike are obsessed with AI, and even Neundorfer said her firm is looking at ways to use AI to make their work more efficient, such as helping to do due diligence on the market and competition. As for companies being built, she has a preference for the founders looking to create something entirely new.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Where I tend to get excited is when I see someone who is using AI to do something that&amp;nbsp;isn’t&amp;nbsp;10x better. It’s actually to&amp;nbsp;create a whole new experience or workflow or behavior,” she said. “That’s what we’re looking at. Less of the incremental changes and more&amp;nbsp;totally new&amp;nbsp;behaviors.”&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;This is getting harder for founders because fatigue, she said, has hit as more AI ideas start to sound the same.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Where&amp;nbsp;I think founders&amp;nbsp;are breaking through is when they can communicate to investors why what they’re doing is&amp;nbsp;really different&amp;nbsp;than the other dozens of startups that are doing that and why they are the team to go after that,” she said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether we are in the so-called AI bubble or not, Neundorfer says a market correction is probably coming, and a lot of the companies getting windfalls of investor money now might not survive. The winners will navigate this moment building “truly category-defining companies,” capturing where the tech is going next. “Founders who can stay ahead of that curve, build at the edge of what’s possible today, and build for what’s coming,” she listed. “Founders who&amp;nbsp;are able to&amp;nbsp;really read the market and understand&amp;nbsp;what it is&amp;nbsp;their&amp;nbsp;customer wants&amp;nbsp;versus just building what is possible. Those are the founders that will have an advantage.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Elsewhere on the pod, she spoke about her life before venture, where she worked at YouTube and 21st Century Fox.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“So much of what I did was meet with people who had great technology,” she&amp;nbsp;recalled of&amp;nbsp;her time at 21st Century Fox. Meeting and talking technology with people was the part of the job that gave her the most joy and helped her realize how much she would probably love working with early-stage founders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the learning curve was steep when she decided to transition to investing. In the early days, she said she would constantly check in with founders and give detailed input on their companies.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That’s&amp;nbsp;appropriate&amp;nbsp;for some cases, but it’s really about the relationship with the founder, supporting not only weighing in on the business, but supporting them as a person,” she said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now&amp;nbsp;she’s&amp;nbsp;comfortable in the job. She serves as a mentor for various organizations, such as Techstars, and has&amp;nbsp;made more than 50 investments at January Ventures, according to PitchBook,&amp;nbsp;nabbing&amp;nbsp;some exits in the meantime.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Throughout the conversation, Neundorfer spoke about the changing venture market, funding levels for minorities and women, and about venture markets outside of San Francisco that are seeing success. Her biggest advice to diverse founders right now actually goes for many founders building in this climate: ignore the noise and focus on building a good company.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anything else becomes something they can’t control, and the worry isn’t worth it.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54886289739_6fa8cfb36d_o.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;January Ventures co-founder Jennifer Neundorfer stopped by the Equity podcast during TechCrunch Disrupt to chat about fundraising in this very AI-driven market.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founders and investors alike are obsessed with AI, and even Neundorfer said her firm is looking at ways to use AI to make their work more efficient, such as helping to do due diligence on the market and competition. As for companies being built, she has a preference for the founders looking to create something entirely new.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Where I tend to get excited is when I see someone who is using AI to do something that&amp;nbsp;isn’t&amp;nbsp;10x better. It’s actually to&amp;nbsp;create a whole new experience or workflow or behavior,” she said. “That’s what we’re looking at. Less of the incremental changes and more&amp;nbsp;totally new&amp;nbsp;behaviors.”&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;This is getting harder for founders because fatigue, she said, has hit as more AI ideas start to sound the same.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Where&amp;nbsp;I think founders&amp;nbsp;are breaking through is when they can communicate to investors why what they’re doing is&amp;nbsp;really different&amp;nbsp;than the other dozens of startups that are doing that and why they are the team to go after that,” she said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether we are in the so-called AI bubble or not, Neundorfer says a market correction is probably coming, and a lot of the companies getting windfalls of investor money now might not survive. The winners will navigate this moment building “truly category-defining companies,” capturing where the tech is going next. “Founders who can stay ahead of that curve, build at the edge of what’s possible today, and build for what’s coming,” she listed. “Founders who&amp;nbsp;are able to&amp;nbsp;really read the market and understand&amp;nbsp;what it is&amp;nbsp;their&amp;nbsp;customer wants&amp;nbsp;versus just building what is possible. Those are the founders that will have an advantage.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Elsewhere on the pod, she spoke about her life before venture, where she worked at YouTube and 21st Century Fox.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“So much of what I did was meet with people who had great technology,” she&amp;nbsp;recalled of&amp;nbsp;her time at 21st Century Fox. Meeting and talking technology with people was the part of the job that gave her the most joy and helped her realize how much she would probably love working with early-stage founders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the learning curve was steep when she decided to transition to investing. In the early days, she said she would constantly check in with founders and give detailed input on their companies.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That’s&amp;nbsp;appropriate&amp;nbsp;for some cases, but it’s really about the relationship with the founder, supporting not only weighing in on the business, but supporting them as a person,” she said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now&amp;nbsp;she’s&amp;nbsp;comfortable in the job. She serves as a mentor for various organizations, such as Techstars, and has&amp;nbsp;made more than 50 investments at January Ventures, according to PitchBook,&amp;nbsp;nabbing&amp;nbsp;some exits in the meantime.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Throughout the conversation, Neundorfer spoke about the changing venture market, funding levels for minorities and women, and about venture markets outside of San Francisco that are seeing success. Her biggest advice to diverse founders right now actually goes for many founders building in this climate: ignore the noise and focus on building a good company.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anything else becomes something they can’t control, and the worry isn’t worth it.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/19/vc-jennifer-neundorfer-explains-how-founders-can-stand-out-in-a-crowded-ai-market/</guid><pubDate>Wed, 19 Nov 2025 20:30:00 +0000</pubDate></item><item><title>The cost of thinking (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/cost-of-thinking-1119</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-mcgovern-costofthinking.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Large language models (LLMs) like ChatGPT can write an essay or plan a menu almost instantly. But until recently, it was also easy to stump them. The models, which rely on language patterns to respond to users’ queries, often failed at math problems and were not good at complex reasoning. Suddenly, however, they’ve gotten a lot better at these things.&lt;/p&gt;&lt;p&gt;A new generation of LLMs known as reasoning models are being trained to solve complex problems. Like humans, they need some time to think through problems like these — and remarkably, scientists at MIT’s McGovern Institute for Brain Research have found that the kinds of problems that require the most processing from reasoning models are the very same problems that people need take their time with. In other words, they report today in the journal &lt;em&gt;PNAS&lt;/em&gt;, the “cost of thinking” for a reasoning model is similar to the cost of thinking for a human.&lt;/p&gt;&lt;p&gt;The researchers, who were led by Evelina Fedorenko, an associate professor of brain and cognitive sciences and an investigator at the McGovern Institute, conclude that in at least one important way, reasoning models have a human-like approach to thinking. That, they note, is not by design. “People who build these models don’t care if they do it like humans. They just want a system that will robustly perform under all sorts of conditions and produce correct responses,” Fedorenko says. “The fact that there’s some convergence is really quite striking.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Reasoning models&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Like many forms of artificial intelligence, the new reasoning models are artificial neural networks: computational tools that learn how to process information when they are given data and a problem to solve. Artificial neural networks have been very successful at many of the tasks that the brain’s own neural networks do well — and in some cases, neuroscientists have discovered that those that perform best do share certain aspects of information processing in the brain. Still, some scientists argued that artificial intelligence was not ready to take on more sophisticated aspects of human intelligence.&lt;/p&gt;&lt;p&gt;“Up until recently, I was among the people saying, ‘These models are really good at things like perception and language, but it’s still going to be a long ways off until we have neural network models that can do reasoning,” Fedorenko says. “Then these large reasoning models emerged and they seem to do much better at a lot of these thinking tasks, like solving math problems and writing pieces of computer code.”&lt;/p&gt;&lt;p&gt;Andrea Gregor de Varda, a K. Lisa Yang ICoN Center Fellow and a postdoc in Fedorenko’s lab, explains that reasoning models work out problems step by step. “At some point, people realized that models needed to have more space to perform the actual computations that are needed to solve complex problems,” he says. “The performance started becoming way, way stronger if you let the models break down the problems into parts.”&lt;/p&gt;&lt;p&gt;To encourage models to work through complex problems in steps that lead to correct solutions, engineers can use reinforcement learning. During their training, the models are rewarded for correct answers and penalized for wrong ones. “The models explore the problem space themselves,” de Varda says. “The actions that lead to positive rewards are reinforced, so that they produce correct solutions more often.”&lt;/p&gt;&lt;p&gt;Models trained in this way are much more likely than their predecessors to arrive at the same answers a human would when they are given a reasoning task. Their stepwise problem-solving does mean reasoning models can take a bit longer to find an answer than the LLMs that came before — but since they’re getting right answers where the previous models would have failed, their responses are worth the wait.&lt;/p&gt;&lt;p&gt;The models’ need to take some time to work through complex problems already hints at a parallel to human thinking: if you demand that a person solve a hard problem instantaneously, they’d probably fail, too. De Varda wanted to examine this relationship more systematically. So he gave reasoning models and human volunteers the same set of problems, and tracked not just whether they got the answers right, but also how much time or effort it took them to get there.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Time versus tokens&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This meant measuring how long it took people to respond to each question, down to the millisecond. For the models, Varda used a different metric. It didn’t make sense to measure processing time, since this is more dependent on computer hardware than the effort the model puts into solving a problem. So instead, he tracked tokens, which are part of a model’s internal chain of thought. “They produce tokens that are not meant for the user to see and work on, but just to have some track of the internal computation that they’re doing,” de Varda explains. “It’s as if they were talking to themselves.”&lt;/p&gt;&lt;p&gt;Both humans and reasoning models were asked to solve seven different types of problems, like numeric arithmetic and intuitive reasoning. For each problem class, they were given many problems. The harder a given problem was, the longer it took people to solve it — and the longer it took people to solve a problem, the more tokens a reasoning model generated as it came to its own solution.&lt;/p&gt;&lt;p&gt;Likewise, the classes of problems that humans took longest to solve were the same classes of problems that required the most tokens for the models: arithmetic problems were the least demanding, whereas a group of problems called the “ARC challenge,” where pairs of colored grids represent a transformation that must be inferred and then applied to a new object, were the most costly for both people and models.&lt;/p&gt;&lt;p&gt;De Varda and Fedorenko say the striking match in the costs of thinking demonstrates one way in which reasoning models are thinking like humans. That doesn’t mean the models are recreating human intelligence, though. The researchers still want to know whether the models use similar representations of information to the human brain, and how those representations are transformed into solutions to problems. They’re also curious whether the models will be able to handle problems that require world knowledge that is not spelled out in the texts that are used for model training.&lt;/p&gt;&lt;p&gt;The researchers point out that even though reasoning models generate internal monologues as they solve problems, they are not necessarily using language to think. “If you look at the output that these models produce while reasoning, it often contains errors or some nonsensical bits, even if the model ultimately arrives at a correct answer. So the actual internal computations likely take place in an abstract, non-linguistic representation space, similar to how humans don’t use language to think,” he says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-mcgovern-costofthinking.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Large language models (LLMs) like ChatGPT can write an essay or plan a menu almost instantly. But until recently, it was also easy to stump them. The models, which rely on language patterns to respond to users’ queries, often failed at math problems and were not good at complex reasoning. Suddenly, however, they’ve gotten a lot better at these things.&lt;/p&gt;&lt;p&gt;A new generation of LLMs known as reasoning models are being trained to solve complex problems. Like humans, they need some time to think through problems like these — and remarkably, scientists at MIT’s McGovern Institute for Brain Research have found that the kinds of problems that require the most processing from reasoning models are the very same problems that people need take their time with. In other words, they report today in the journal &lt;em&gt;PNAS&lt;/em&gt;, the “cost of thinking” for a reasoning model is similar to the cost of thinking for a human.&lt;/p&gt;&lt;p&gt;The researchers, who were led by Evelina Fedorenko, an associate professor of brain and cognitive sciences and an investigator at the McGovern Institute, conclude that in at least one important way, reasoning models have a human-like approach to thinking. That, they note, is not by design. “People who build these models don’t care if they do it like humans. They just want a system that will robustly perform under all sorts of conditions and produce correct responses,” Fedorenko says. “The fact that there’s some convergence is really quite striking.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Reasoning models&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Like many forms of artificial intelligence, the new reasoning models are artificial neural networks: computational tools that learn how to process information when they are given data and a problem to solve. Artificial neural networks have been very successful at many of the tasks that the brain’s own neural networks do well — and in some cases, neuroscientists have discovered that those that perform best do share certain aspects of information processing in the brain. Still, some scientists argued that artificial intelligence was not ready to take on more sophisticated aspects of human intelligence.&lt;/p&gt;&lt;p&gt;“Up until recently, I was among the people saying, ‘These models are really good at things like perception and language, but it’s still going to be a long ways off until we have neural network models that can do reasoning,” Fedorenko says. “Then these large reasoning models emerged and they seem to do much better at a lot of these thinking tasks, like solving math problems and writing pieces of computer code.”&lt;/p&gt;&lt;p&gt;Andrea Gregor de Varda, a K. Lisa Yang ICoN Center Fellow and a postdoc in Fedorenko’s lab, explains that reasoning models work out problems step by step. “At some point, people realized that models needed to have more space to perform the actual computations that are needed to solve complex problems,” he says. “The performance started becoming way, way stronger if you let the models break down the problems into parts.”&lt;/p&gt;&lt;p&gt;To encourage models to work through complex problems in steps that lead to correct solutions, engineers can use reinforcement learning. During their training, the models are rewarded for correct answers and penalized for wrong ones. “The models explore the problem space themselves,” de Varda says. “The actions that lead to positive rewards are reinforced, so that they produce correct solutions more often.”&lt;/p&gt;&lt;p&gt;Models trained in this way are much more likely than their predecessors to arrive at the same answers a human would when they are given a reasoning task. Their stepwise problem-solving does mean reasoning models can take a bit longer to find an answer than the LLMs that came before — but since they’re getting right answers where the previous models would have failed, their responses are worth the wait.&lt;/p&gt;&lt;p&gt;The models’ need to take some time to work through complex problems already hints at a parallel to human thinking: if you demand that a person solve a hard problem instantaneously, they’d probably fail, too. De Varda wanted to examine this relationship more systematically. So he gave reasoning models and human volunteers the same set of problems, and tracked not just whether they got the answers right, but also how much time or effort it took them to get there.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Time versus tokens&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This meant measuring how long it took people to respond to each question, down to the millisecond. For the models, Varda used a different metric. It didn’t make sense to measure processing time, since this is more dependent on computer hardware than the effort the model puts into solving a problem. So instead, he tracked tokens, which are part of a model’s internal chain of thought. “They produce tokens that are not meant for the user to see and work on, but just to have some track of the internal computation that they’re doing,” de Varda explains. “It’s as if they were talking to themselves.”&lt;/p&gt;&lt;p&gt;Both humans and reasoning models were asked to solve seven different types of problems, like numeric arithmetic and intuitive reasoning. For each problem class, they were given many problems. The harder a given problem was, the longer it took people to solve it — and the longer it took people to solve a problem, the more tokens a reasoning model generated as it came to its own solution.&lt;/p&gt;&lt;p&gt;Likewise, the classes of problems that humans took longest to solve were the same classes of problems that required the most tokens for the models: arithmetic problems were the least demanding, whereas a group of problems called the “ARC challenge,” where pairs of colored grids represent a transformation that must be inferred and then applied to a new object, were the most costly for both people and models.&lt;/p&gt;&lt;p&gt;De Varda and Fedorenko say the striking match in the costs of thinking demonstrates one way in which reasoning models are thinking like humans. That doesn’t mean the models are recreating human intelligence, though. The researchers still want to know whether the models use similar representations of information to the human brain, and how those representations are transformed into solutions to problems. They’re also curious whether the models will be able to handle problems that require world knowledge that is not spelled out in the texts that are used for model training.&lt;/p&gt;&lt;p&gt;The researchers point out that even though reasoning models generate internal monologues as they solve problems, they are not necessarily using language to think. “If you look at the output that these models produce while reasoning, it often contains errors or some nonsensical bits, even if the model ultimately arrives at a correct answer. So the actual internal computations likely take place in an abstract, non-linguistic representation space, similar to how humans don’t use language to think,” he says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/cost-of-thinking-1119</guid><pubDate>Wed, 19 Nov 2025 21:45:00 +0000</pubDate></item><item><title>Nvidia’s record $57B revenue and upbeat forecast quiets AI bubble talk (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/19/nvidias-record-57b-revenue-and-upbeat-forecast-quiets-ai-bubble-talk/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205761844.jpg?resize=1200,846" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia founder and CEO Jensen Huang struck a bullish tone in the company’s third-quarter earnings. And based on the company’s results, there may be reason to.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia reported revenue of $57 billion in the third quarter, 62% higher compared to the same quarter last year. The company’s net income on a GAAP basis was $32 billion, 65% higher year-over-year. Both revenue and profit results beat Wall Street expectations.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The revenue picture shows a company booming thanks largely to its data center business. Revenue generated by Nvidia’s data center business was a record $51.2 billion, up 25% from the previous quarter and up 66% from a year ago. The remaining $5.8 billion in revenue came from Nvidia’s gaming business with $4.2 billion, followed by sales in professional visualization and automotive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s CFO Colette Kress noted in a statement to shareholders its data center business has been fueled by an acceleration of computing, powerful AI models, and agentic applications. During the company’s Q3 call, Kress said in this past quarter, the company announced AI factory and infrastructure projects amounting to an aggregate of 5 million GPUs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This demand spans every market, CSPs, sovereigns, modern builders enterprises and super computing centers, and includes multiple landmark build outs,” Kress said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Blackwell Ultra, a GPU unveiled in March and available in several configurations, has been particularly strong and is now the leader within the company. Previous versions of the Blackwell architecture also saw continued strong demand, according to the company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Huang said sales of its Blackwell GPU chips “are off the charts.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Blackwell sales are off the charts, and cloud GPUs are sold out,” Huang said in the company’s Q3 earnings statement. “Compute demand keeps accelerating and compounding across training and inference — each growing exponentially. We’ve entered the virtuous cycle of AI. The AI ecosystem is scaling fast — with more new foundation model makers, more AI startups, across more industries, and in more countries. AI is going everywhere, doing everything, all at once.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kress did note that the company’s shipments of H20, a data center GPU designed for generative AI and high-performance computing, were 50 million, a disappointing result due to its inability to sell to China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Sizable purchase orders never materialized in the quarter due to geopolitical issues and the increasingly competitive market in China,,” Kress noted on the earnings call. “While we were disappointed in the current state that prevents us from shipping more competitive data center compute products to China, we are committed to continued engagement with the U.S. and China governments, and will continue to advocate for America’s ability to compete around the world.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Importantly, Nvidia is forecasting more growth with a projected revenue of $65 billion in the fourth quarter, helping push its share price up more than 4% in after-hours trading. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot, at least in Huang’s view: forget about the bubble, there is only growth. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s been a lot of talk about an AI bubble,” Jensen said during the company’s earnings call. “From our vantage point, we see something very different.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205761844.jpg?resize=1200,846" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia founder and CEO Jensen Huang struck a bullish tone in the company’s third-quarter earnings. And based on the company’s results, there may be reason to.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia reported revenue of $57 billion in the third quarter, 62% higher compared to the same quarter last year. The company’s net income on a GAAP basis was $32 billion, 65% higher year-over-year. Both revenue and profit results beat Wall Street expectations.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The revenue picture shows a company booming thanks largely to its data center business. Revenue generated by Nvidia’s data center business was a record $51.2 billion, up 25% from the previous quarter and up 66% from a year ago. The remaining $5.8 billion in revenue came from Nvidia’s gaming business with $4.2 billion, followed by sales in professional visualization and automotive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s CFO Colette Kress noted in a statement to shareholders its data center business has been fueled by an acceleration of computing, powerful AI models, and agentic applications. During the company’s Q3 call, Kress said in this past quarter, the company announced AI factory and infrastructure projects amounting to an aggregate of 5 million GPUs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This demand spans every market, CSPs, sovereigns, modern builders enterprises and super computing centers, and includes multiple landmark build outs,” Kress said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Blackwell Ultra, a GPU unveiled in March and available in several configurations, has been particularly strong and is now the leader within the company. Previous versions of the Blackwell architecture also saw continued strong demand, according to the company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Huang said sales of its Blackwell GPU chips “are off the charts.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Blackwell sales are off the charts, and cloud GPUs are sold out,” Huang said in the company’s Q3 earnings statement. “Compute demand keeps accelerating and compounding across training and inference — each growing exponentially. We’ve entered the virtuous cycle of AI. The AI ecosystem is scaling fast — with more new foundation model makers, more AI startups, across more industries, and in more countries. AI is going everywhere, doing everything, all at once.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kress did note that the company’s shipments of H20, a data center GPU designed for generative AI and high-performance computing, were 50 million, a disappointing result due to its inability to sell to China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Sizable purchase orders never materialized in the quarter due to geopolitical issues and the increasingly competitive market in China,,” Kress noted on the earnings call. “While we were disappointed in the current state that prevents us from shipping more competitive data center compute products to China, we are committed to continued engagement with the U.S. and China governments, and will continue to advocate for America’s ability to compete around the world.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Importantly, Nvidia is forecasting more growth with a projected revenue of $65 billion in the fourth quarter, helping push its share price up more than 4% in after-hours trading. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot, at least in Huang’s view: forget about the bubble, there is only growth. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s been a lot of talk about an AI bubble,” Jensen said during the company’s earnings call. “From our vantage point, we see something very different.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/19/nvidias-record-57b-revenue-and-upbeat-forecast-quiets-ai-bubble-talk/</guid><pubDate>Wed, 19 Nov 2025 22:17:45 +0000</pubDate></item><item><title>“We’re in an LLM bubble,” Hugging Face CEO says—but not an AI one (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/were-in-an-llm-bubble-hugging-face-ceo-says-but-not-an-ai-one/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The risks of AI investment in manufacturing and other areas are less clear.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A man in a baseball hat talks and gesticulates on stage" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/1763499309592-640x360.webp" width="640" /&gt;
                  &lt;img alt="A man in a baseball hat talks and gesticulates on stage" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/1763499309592-1152x648.webp" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hugging Face CEO Clem Delangue speaking at an Axios event this week.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Axios

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;There’s been a lot of talk of an AI bubble lately, especially with regards to circular funding involving companies like OpenAI and Anthropic—but Clem Delangue, CEO of machine learning resources hub Hugging Face, has made the case that the bubble is specific to large language models, which is just one application of AI.&lt;/p&gt;
&lt;p&gt;“I think we’re in an LLM bubble, and I think the LLM bubble might be bursting next year,” he said at an Axios event this week, as quoted in a TechCrunch article. “But ‘LLM’ is just a subset of AI when it comes to applying AI to biology, chemistry, image, audio, [and] video. I think we’re at the beginning of it, and we’ll see much more in the next few years.”&lt;/p&gt;
&lt;p&gt;At Ars, we’ve written at length in recent days about the fears around AI investment. But to Delangue’s point, almost all of those discussions are about companies whose chief product is large language models, or the data centers meant to drive those—specifically, those focused on general-purpose chatbots that are meant to be everything for everybody.&lt;/p&gt;
&lt;p&gt;That’s exactly the sort of application Delangue is bearish on. “I think all the attention, all the focus, all the money, is concentrated into this idea that you can build one model through a bunch of compute and that is going to solve all problems for all companies and all people,” he said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Instead, he imagines the eventual outcome to be “a multiplicity of models that are more customized, specialized, and that are going to solve different problems.”&lt;/p&gt;
&lt;p&gt;It’s of course important to note that his company is focused on being a GitHub-like repo for exactly those sorts of specialized models, including both big models put out there by companies like OpenAI and Meta (gpt-oss and Llama 3.2, for example) and fine-tuned variants that developers have adapted to specific needs or smaller models developed by researchers. That’s essentially what Hugging Face is about.&lt;/p&gt;
&lt;p&gt;So yes, it’s natural that Delangue would say that. However, he’s not alone. In one example, research firm Gartner predicted in April that “the variety of tasks in business workflows and the need for greater accuracy are driving the shift towards specialized models fine-tuned on specific functions or domain data.”&lt;/p&gt;
&lt;p&gt;Regardless of which way LLM-based applications go, investment in other applications of AI-by-the-current-definition is only just getting started. Earlier this week, it was revealed that former Amazon CEO Jeff Bezos will be co-CEO of a new AI startup focused on applications of machine learning in engineering and manufacturing—and that startup has launched with over $6 billion in funding.&lt;/p&gt;
&lt;p&gt;That, too, could be a bubble. But despite that some of Delangue’s statements on the AI bubble discourse are clearly meant to prop up Hugging Face, there’s a helpful reminder in there: The overbroad term “AI” is a lot bigger than just large language models, and we’re still in the early days of seeing where these methodologies will lead us.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The risks of AI investment in manufacturing and other areas are less clear.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A man in a baseball hat talks and gesticulates on stage" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/1763499309592-640x360.webp" width="640" /&gt;
                  &lt;img alt="A man in a baseball hat talks and gesticulates on stage" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/1763499309592-1152x648.webp" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hugging Face CEO Clem Delangue speaking at an Axios event this week.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Axios

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;There’s been a lot of talk of an AI bubble lately, especially with regards to circular funding involving companies like OpenAI and Anthropic—but Clem Delangue, CEO of machine learning resources hub Hugging Face, has made the case that the bubble is specific to large language models, which is just one application of AI.&lt;/p&gt;
&lt;p&gt;“I think we’re in an LLM bubble, and I think the LLM bubble might be bursting next year,” he said at an Axios event this week, as quoted in a TechCrunch article. “But ‘LLM’ is just a subset of AI when it comes to applying AI to biology, chemistry, image, audio, [and] video. I think we’re at the beginning of it, and we’ll see much more in the next few years.”&lt;/p&gt;
&lt;p&gt;At Ars, we’ve written at length in recent days about the fears around AI investment. But to Delangue’s point, almost all of those discussions are about companies whose chief product is large language models, or the data centers meant to drive those—specifically, those focused on general-purpose chatbots that are meant to be everything for everybody.&lt;/p&gt;
&lt;p&gt;That’s exactly the sort of application Delangue is bearish on. “I think all the attention, all the focus, all the money, is concentrated into this idea that you can build one model through a bunch of compute and that is going to solve all problems for all companies and all people,” he said.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Instead, he imagines the eventual outcome to be “a multiplicity of models that are more customized, specialized, and that are going to solve different problems.”&lt;/p&gt;
&lt;p&gt;It’s of course important to note that his company is focused on being a GitHub-like repo for exactly those sorts of specialized models, including both big models put out there by companies like OpenAI and Meta (gpt-oss and Llama 3.2, for example) and fine-tuned variants that developers have adapted to specific needs or smaller models developed by researchers. That’s essentially what Hugging Face is about.&lt;/p&gt;
&lt;p&gt;So yes, it’s natural that Delangue would say that. However, he’s not alone. In one example, research firm Gartner predicted in April that “the variety of tasks in business workflows and the need for greater accuracy are driving the shift towards specialized models fine-tuned on specific functions or domain data.”&lt;/p&gt;
&lt;p&gt;Regardless of which way LLM-based applications go, investment in other applications of AI-by-the-current-definition is only just getting started. Earlier this week, it was revealed that former Amazon CEO Jeff Bezos will be co-CEO of a new AI startup focused on applications of machine learning in engineering and manufacturing—and that startup has launched with over $6 billion in funding.&lt;/p&gt;
&lt;p&gt;That, too, could be a bubble. But despite that some of Delangue’s statements on the AI bubble discourse are clearly meant to prop up Hugging Face, there’s a helpful reminder in there: The overbroad term “AI” is a lot bigger than just large language models, and we’re still in the early days of seeing where these methodologies will lead us.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/were-in-an-llm-bubble-hugging-face-ceo-says-but-not-an-ai-one/</guid><pubDate>Wed, 19 Nov 2025 22:57:23 +0000</pubDate></item></channel></rss>