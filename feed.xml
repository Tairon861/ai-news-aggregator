<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 12 Feb 2026 02:31:41 +0000</lastBuildDate><item><title>Threads’ new ‘Dear Algo’ AI feature lets you personalize your feed (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/threads-new-dear-algo-ai-feature-lets-you-personalize-your-feed/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Threads is introducing an AI-powered feature that lets users personalize their feed, the Meta-owned social network announced on Wednesday. The platform’s new “Dear Algo” feature lets users tell Threads what they temporarily want to see more or less of in their feed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the feature, users need to type “Dear Algo” in a public Threads post, followed by what they’d like to see more or less of. For example, if you want to see more content about popular podcasts, you can post “Dear Algo, show me more posts about podcasts.” Your feed will be adjusted for three days after submitting a request.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s worth noting that because Dear Algo works through public postings, others are able to see your request, and they can even repost your request to apply it to their own feed. This aspect of the feature may be a hindrance for some users, as they might not want their requests to be public. However, Meta says it views this as a way for users to discover new conversations and topics, turning personalization into a community experience.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3091671" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/Threads_Dear-Algo_Post-Activity-Feed.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;By allowing users to temporarily influence what appears on their feeds, Threads gains an edge over its competitors by offering a feature that isn’t available on rival platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Threads is where you go to keep up with what’s happening right now,” the company wrote in a blog post. “But sometimes, what matters to you changes in an instant, and you want your Threads feed to reflect that — whether it’s seeing more posts during a live NBA game or less about a TV show you haven’t caught up on yet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although Threads and its competitors, X and Bluesky, already offer users a simple way to indicate their preferences with a “Not Interested” button, the new Dear Algo feature takes personalization a step further. The feature could also help make Threads feel more real-time, something X has historically been known for.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dear Algo is available now in the U.S., New Zealand, Australia, and the U.K. Threads plans to introduce it in more countries in the future. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3091673" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/Threads_Dear-Algo_Settings-Delete-Request.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of the new feature comes a few weeks after a report from market intelligence firm Similarweb suggested that Threads now surpasses X in daily mobile usage. Although X still dominates Threads on the web, the Threads mobile app for iOS and Android has continued to see an increase in daily active users over the past several months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The report revealed that Threads had 141.5 million daily active users on mobile as of January 7, 2026, while X had 125 million daily active users.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Threads is introducing an AI-powered feature that lets users personalize their feed, the Meta-owned social network announced on Wednesday. The platform’s new “Dear Algo” feature lets users tell Threads what they temporarily want to see more or less of in their feed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the feature, users need to type “Dear Algo” in a public Threads post, followed by what they’d like to see more or less of. For example, if you want to see more content about popular podcasts, you can post “Dear Algo, show me more posts about podcasts.” Your feed will be adjusted for three days after submitting a request.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s worth noting that because Dear Algo works through public postings, others are able to see your request, and they can even repost your request to apply it to their own feed. This aspect of the feature may be a hindrance for some users, as they might not want their requests to be public. However, Meta says it views this as a way for users to discover new conversations and topics, turning personalization into a community experience.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3091671" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/Threads_Dear-Algo_Post-Activity-Feed.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;By allowing users to temporarily influence what appears on their feeds, Threads gains an edge over its competitors by offering a feature that isn’t available on rival platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Threads is where you go to keep up with what’s happening right now,” the company wrote in a blog post. “But sometimes, what matters to you changes in an instant, and you want your Threads feed to reflect that — whether it’s seeing more posts during a live NBA game or less about a TV show you haven’t caught up on yet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although Threads and its competitors, X and Bluesky, already offer users a simple way to indicate their preferences with a “Not Interested” button, the new Dear Algo feature takes personalization a step further. The feature could also help make Threads feel more real-time, something X has historically been known for.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dear Algo is available now in the U.S., New Zealand, Australia, and the U.K. Threads plans to introduce it in more countries in the future. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3091673" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/Threads_Dear-Algo_Settings-Delete-Request.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of the new feature comes a few weeks after a report from market intelligence firm Similarweb suggested that Threads now surpasses X in daily mobile usage. Although X still dominates Threads on the web, the Threads mobile app for iOS and Android has continued to see an increase in daily active users over the past several months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The report revealed that Threads had 141.5 million daily active users on mobile as of January 7, 2026, while X had 125 million daily active users.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/threads-new-dear-algo-ai-feature-lets-you-personalize-your-feed/</guid><pubDate>Wed, 11 Feb 2026 18:00:00 +0000</pubDate></item><item><title>Why the economics of orbital AI are so brutal (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/why-the-economics-of-orbital-ai-are-so-brutal/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/03/starlink-satellites-on-orbit.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a sense, this whole thing was inevitable. Elon Musk and his coterie have been talking about AI in space for years — mainly in the context of Iain Banks’ science-fiction series about a far-future universe where sentient spaceships roam and control the galaxy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now Musk sees an opportunity to realize a version of this vision. His company SpaceX has requested regulatory permission to build solar-powered orbital data centers, distributed across as many as a million satellites, that could shift as much as 100 GW of compute power off the planet.&amp;nbsp;He has reportedly suggested some of his AI satellites will be built on the moon.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“By far the cheapest place to put AI will be space in 36 months or less,” Musk said last week on a podcast hosted by Stripe co-founder John Collison.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’s not alone. xAI’s head of compute has reportedly bet his counterpart at Anthropic that 1% of global compute will be in orbit by 2028. Google (which has a significant ownership stake in SpaceX) has announced a space AI effort called Project Suncatcher, which will launch prototype vehicles in 2027. Starcloud, a startup that has raised $34 million backed by Google and Andreessen Horowitz, filed its own plans for an 80,000 satellite constellation last week.&amp;nbsp;Even Jeff Bezos has said this is the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But behind the hype, what will it actually take to get data centers into space?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a first analysis, today’s terrestrial data centers remain cheaper than those in orbit. Andrew McCalip, a space engineer, has built a helpful calculator comparing the two models. His baseline results show that a 1 GW orbital data center might cost $42.4 billion — almost 3x its ground-bound equivalent, thanks to the up-front costs of building the satellites and launching them to orbit.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Changing that equation, experts say, will require technology development across several fields, massive capital expenditure, and a lot of work on the supply chain for space-grade components. It also depends on costs on the ground rising as resources and supply chains are strained by growing demand.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-designing-and-launching-the-satellites"&gt;Designing and launching the satellites&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The key driver for any space business model is how much it costs to get anything up there. Musk’s SpaceX is already pushing down on the cost of getting to orbit, but analysts looking at what it will take to make orbital data centers a reality need even lower prices to close their business case. In other words, while AI data centers may seem to be a story about a new business line ahead of the SpaceX IPO, the plan depends on completing the company’s longest-running unfinished project — Starship.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Consider that the reusable Falcon 9 delivers, today, a cost to orbit of roughly $3,600/kg. Making space data centers doable, per Project Suncatcher’s white paper, will require prices closer to $200/kg, an 18-fold improvement that it expects to be available in the 2030s. At that price, however, the energy delivered by a Starlink satellite today would be cost competitive with a terrestrial data center.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The expectation is that SpaceX’s next-generation Starship rocket will deliver those improvements — no other vehicle in development promises equivalent savings. However, that vehicle has yet to become operational or even reach orbit; a third iteration of Starship is expected to make its maiden launch sometime in the months ahead.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even if Starship is completely successful, however, assumptions that it will immediately deliver lower prices to customers may not pass the smell test. Economists at the consultancy Rational Futures make a compelling case that, as with the Falcon 9, SpaceX will not want to charge much less than its best competitor — otherwise the company is leaving money on the table. If Blue Origin’s New Glenn rocket, for example, retails at $70 million, SpaceX won’t take on Starship missions for external customers at much less than that, which would leave it above the numbers publicly assumed by space data center builders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There are not enough rockets to launch a million satellites yet, so we’re pretty far from that,” Matt Gorman, the CEO of Amazon Web Services, said at a recent event. “If you think about the cost of getting a payload in space today, it’s massive. It is just not economical.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, if launch is the bane of all space businesses, the second challenge is production cost.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We always take for granted, at this point, that Starship’s cost is going to be hundreds of dollars per kilo,” McCalip told TechCrunch. “People are not taking into account the satellites are almost $1,000 a kilo right now.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Satellite manufacturing costs are the largest chunk of that price tag, but if high-powered satellites can be made at about half the cost of current Starlink satellites, the numbers start to make sense. SpaceX has made great advances in satellite economics while building Starlink, its record-setting communications network, and the company hopes to achieve more through scale. Part of the reasoning behind a million satellites is undoubtedly the cost savings that come from mass production.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the satellites that will be used for these missions must be large enough to satisfy the complex requirements for operating powerful GPUs, including large solar arrays, thermal management systems, and laser-based communications links to receive and deliver data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A 2025 white paper from Project Suncatcher offers one way to compare terrestrial and space data centers by the cost of power, the basic input needed to run chips. On the ground, data centers spend roughly $570 to $3,000 for a kW of power over a year, depending on local power costs and the efficiency of their systems. SpaceX’s Starlink satellites get their power from on-board solar panels instead, but the cost of acquiring, launching, and maintaining those spacecraft delivers energy at $14,700 per kW over a year. Put simply, satellites and their components will have to get a lot cheaper before they’re cost-competitive with metered power.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-space-environment-is-not-fooling-around"&gt;The space environment is not fooling around&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Orbital data center proponents often say that thermal management is “free” in space, but that’s an oversimplification. Without an atmosphere, it’s actually more difficult to disperse heat.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“You’re relying on very large radiators to just be able to dissipate that heat into the blackness of space, and so that’s a lot of surface area and mass that you have to manage,” said Mike Safyan, an executive at Planet Labs, which is building prototype satellites for Google Suncatcher that are expected to launch in 2027. “It is recognized as one of the key challenges, especially long term.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Besides the vacuum of space, AI satellites will need to deal with cosmic radiation. Cosmic rays degrade chips over time, and they can also cause “bit flip” errors that can corrupt data. Chips can be protected with shielding, use rad-hardened components, or work in series with redundant error checks, but all these options involve expensive trades for mass. Still, Google used a particle beam to test the effects of radiation on its tensor processing units (chips designed explicitly for machine learning applications). SpaceX executives said on social media that the company has acquired a particle accelerator for just that purpose.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another challenge comes from the solar panels themselves. The logic of the project is energy arbitrage: Putting solar panels in space makes them anywhere from 5x to 8x more efficient than on Earth, and if they’re in the right orbit, they can be in sight of the sun for 90% of the day or more, increasing their efficiency. Electricity is the main fuel for chips, so more energy equals cheaper data centers. But even solar panels are more complicated in space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Space-rated solar panels made of rare earth elements are hardy, but too expensive. Solar panels made from silicon are cheap and increasingly prevalent in space — Starlink and Amazon Kuiper use them — but they degrade much faster due to space radiation. That will limit the lifetime of AI satellites to around five years, which means they will have to generate return on investment faster.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, some analysts think that’s not such a big deal, based on how quickly new generations of chips arrive on the scene. “After five or six years, the dollars per kilowatt-hour doesn’t produce a return, and that’s because they’re not state of the art,” Philip Johnston, the CEO of Starcloud, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Danny Field, an executive at Solestial, a startup building space-rated silicon solar panels, says the industry sees orbital data centers as a key driver of growth. He’s speaking with several companies about potential data center projects, and says “any player who is big enough to dream is at least thinking about it.” As a long-time spacecraft design engineer, however, he doesn’t discount the challenges in these models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can always extrapolate physics out to a bigger size,” Field said. “I’m excited to see how some of these companies get to a point where the economics make sense and the business case closes.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-do-space-data-centers-fit-in"&gt;How do space data centers fit in?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;One outstanding question about these data centers: What will we do with them? Are they general purpose, or for inference, or for training? Based on existing use cases, they may not be entirely interchangeable with data centers on the ground.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A key challenge for training new models is operating thousands of GPUs together en masse. Most model training is not distributed, but done in individual data centers. The hyperscalers are working to change this in order to increase the power of their models, but it still hasn’t been achieved. Similarly, training in space will require coherence between GPUs on multiple satellites.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The team at Google’s Project Suncatcher notes that the company’s terrestrial data centers connect their TPU networks with throughput in the hundreds of gigabits per second. The fastest off-the-shelf inter-satellite comms links today, which use lasers, can only get up to about 100 Gbps.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That led to an intriguing architecture for Suncatcher: It involves flying 81 satellites in formation so they are close enough to use the kind of transceivers relied on by terrestrial data centers. That, of course, presents its own challenges: The autonomy required to ensure each spacecraft remains in its correct station, even if maneuvers are required to avoid orbital debris or another spacecraft.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the Google study offers a caveat: The work of inference can tolerate the orbital radiation environment, but more research is needed to understand the potential impact of bit-flips and other errors on training workloads.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Inference tasks don’t have the same need for thousands of GPUs working in unison. The job can be done with dozens of GPUs, perhaps on a single satellite, an architecture that represents a kind of minimum viable product and the likely starting point for the orbital data center business.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Training is not the ideal thing to do in space,” Johnston said. “I think almost all inference workloads will be done in space,” imagining everything from customer service voice agents to ChatGPT queries being computed in orbit. He says his company’s first AI satellite is already earning revenue performing inference in orbit. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While details are scarce even in the company’s FCC filing, SpaceX’s orbital data center constellation seems to anticipate about 100 kW of compute power per ton, roughly twice the power of current Starlink satellites. The spacecraft will operate in connection with each other and use the Starlink network to share information; the filing claims that Starlink’s laser links can achieve petabit-level throughput.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For SpaceX, the company’s recent acquisition of xAI (which is building its own terrestrial data centers) will let the company stake out positions in both terrestrial and orbital data centers, seeing which supply chain adapts faster.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s the benefit of having fungible floating point operations per second — if you can make it work. “A FLOP is a FLOP, it doesn’t matter where it lives,” McCalip said. “[SpaceX] can just scale until [it] hits permitting or capex bottlenecks on the ground, and then fall back to [their] space deployments.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents about SpaceX? For secure communication, you can contact Tim via Signal at tim_fernholz.21&lt;/em&gt;.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/03/starlink-satellites-on-orbit.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a sense, this whole thing was inevitable. Elon Musk and his coterie have been talking about AI in space for years — mainly in the context of Iain Banks’ science-fiction series about a far-future universe where sentient spaceships roam and control the galaxy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now Musk sees an opportunity to realize a version of this vision. His company SpaceX has requested regulatory permission to build solar-powered orbital data centers, distributed across as many as a million satellites, that could shift as much as 100 GW of compute power off the planet.&amp;nbsp;He has reportedly suggested some of his AI satellites will be built on the moon.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“By far the cheapest place to put AI will be space in 36 months or less,” Musk said last week on a podcast hosted by Stripe co-founder John Collison.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’s not alone. xAI’s head of compute has reportedly bet his counterpart at Anthropic that 1% of global compute will be in orbit by 2028. Google (which has a significant ownership stake in SpaceX) has announced a space AI effort called Project Suncatcher, which will launch prototype vehicles in 2027. Starcloud, a startup that has raised $34 million backed by Google and Andreessen Horowitz, filed its own plans for an 80,000 satellite constellation last week.&amp;nbsp;Even Jeff Bezos has said this is the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But behind the hype, what will it actually take to get data centers into space?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a first analysis, today’s terrestrial data centers remain cheaper than those in orbit. Andrew McCalip, a space engineer, has built a helpful calculator comparing the two models. His baseline results show that a 1 GW orbital data center might cost $42.4 billion — almost 3x its ground-bound equivalent, thanks to the up-front costs of building the satellites and launching them to orbit.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Changing that equation, experts say, will require technology development across several fields, massive capital expenditure, and a lot of work on the supply chain for space-grade components. It also depends on costs on the ground rising as resources and supply chains are strained by growing demand.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-designing-and-launching-the-satellites"&gt;Designing and launching the satellites&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The key driver for any space business model is how much it costs to get anything up there. Musk’s SpaceX is already pushing down on the cost of getting to orbit, but analysts looking at what it will take to make orbital data centers a reality need even lower prices to close their business case. In other words, while AI data centers may seem to be a story about a new business line ahead of the SpaceX IPO, the plan depends on completing the company’s longest-running unfinished project — Starship.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Consider that the reusable Falcon 9 delivers, today, a cost to orbit of roughly $3,600/kg. Making space data centers doable, per Project Suncatcher’s white paper, will require prices closer to $200/kg, an 18-fold improvement that it expects to be available in the 2030s. At that price, however, the energy delivered by a Starlink satellite today would be cost competitive with a terrestrial data center.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The expectation is that SpaceX’s next-generation Starship rocket will deliver those improvements — no other vehicle in development promises equivalent savings. However, that vehicle has yet to become operational or even reach orbit; a third iteration of Starship is expected to make its maiden launch sometime in the months ahead.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even if Starship is completely successful, however, assumptions that it will immediately deliver lower prices to customers may not pass the smell test. Economists at the consultancy Rational Futures make a compelling case that, as with the Falcon 9, SpaceX will not want to charge much less than its best competitor — otherwise the company is leaving money on the table. If Blue Origin’s New Glenn rocket, for example, retails at $70 million, SpaceX won’t take on Starship missions for external customers at much less than that, which would leave it above the numbers publicly assumed by space data center builders.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There are not enough rockets to launch a million satellites yet, so we’re pretty far from that,” Matt Gorman, the CEO of Amazon Web Services, said at a recent event. “If you think about the cost of getting a payload in space today, it’s massive. It is just not economical.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, if launch is the bane of all space businesses, the second challenge is production cost.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We always take for granted, at this point, that Starship’s cost is going to be hundreds of dollars per kilo,” McCalip told TechCrunch. “People are not taking into account the satellites are almost $1,000 a kilo right now.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Satellite manufacturing costs are the largest chunk of that price tag, but if high-powered satellites can be made at about half the cost of current Starlink satellites, the numbers start to make sense. SpaceX has made great advances in satellite economics while building Starlink, its record-setting communications network, and the company hopes to achieve more through scale. Part of the reasoning behind a million satellites is undoubtedly the cost savings that come from mass production.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the satellites that will be used for these missions must be large enough to satisfy the complex requirements for operating powerful GPUs, including large solar arrays, thermal management systems, and laser-based communications links to receive and deliver data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A 2025 white paper from Project Suncatcher offers one way to compare terrestrial and space data centers by the cost of power, the basic input needed to run chips. On the ground, data centers spend roughly $570 to $3,000 for a kW of power over a year, depending on local power costs and the efficiency of their systems. SpaceX’s Starlink satellites get their power from on-board solar panels instead, but the cost of acquiring, launching, and maintaining those spacecraft delivers energy at $14,700 per kW over a year. Put simply, satellites and their components will have to get a lot cheaper before they’re cost-competitive with metered power.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-space-environment-is-not-fooling-around"&gt;The space environment is not fooling around&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Orbital data center proponents often say that thermal management is “free” in space, but that’s an oversimplification. Without an atmosphere, it’s actually more difficult to disperse heat.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“You’re relying on very large radiators to just be able to dissipate that heat into the blackness of space, and so that’s a lot of surface area and mass that you have to manage,” said Mike Safyan, an executive at Planet Labs, which is building prototype satellites for Google Suncatcher that are expected to launch in 2027. “It is recognized as one of the key challenges, especially long term.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Besides the vacuum of space, AI satellites will need to deal with cosmic radiation. Cosmic rays degrade chips over time, and they can also cause “bit flip” errors that can corrupt data. Chips can be protected with shielding, use rad-hardened components, or work in series with redundant error checks, but all these options involve expensive trades for mass. Still, Google used a particle beam to test the effects of radiation on its tensor processing units (chips designed explicitly for machine learning applications). SpaceX executives said on social media that the company has acquired a particle accelerator for just that purpose.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another challenge comes from the solar panels themselves. The logic of the project is energy arbitrage: Putting solar panels in space makes them anywhere from 5x to 8x more efficient than on Earth, and if they’re in the right orbit, they can be in sight of the sun for 90% of the day or more, increasing their efficiency. Electricity is the main fuel for chips, so more energy equals cheaper data centers. But even solar panels are more complicated in space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Space-rated solar panels made of rare earth elements are hardy, but too expensive. Solar panels made from silicon are cheap and increasingly prevalent in space — Starlink and Amazon Kuiper use them — but they degrade much faster due to space radiation. That will limit the lifetime of AI satellites to around five years, which means they will have to generate return on investment faster.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, some analysts think that’s not such a big deal, based on how quickly new generations of chips arrive on the scene. “After five or six years, the dollars per kilowatt-hour doesn’t produce a return, and that’s because they’re not state of the art,” Philip Johnston, the CEO of Starcloud, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Danny Field, an executive at Solestial, a startup building space-rated silicon solar panels, says the industry sees orbital data centers as a key driver of growth. He’s speaking with several companies about potential data center projects, and says “any player who is big enough to dream is at least thinking about it.” As a long-time spacecraft design engineer, however, he doesn’t discount the challenges in these models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can always extrapolate physics out to a bigger size,” Field said. “I’m excited to see how some of these companies get to a point where the economics make sense and the business case closes.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-do-space-data-centers-fit-in"&gt;How do space data centers fit in?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;One outstanding question about these data centers: What will we do with them? Are they general purpose, or for inference, or for training? Based on existing use cases, they may not be entirely interchangeable with data centers on the ground.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A key challenge for training new models is operating thousands of GPUs together en masse. Most model training is not distributed, but done in individual data centers. The hyperscalers are working to change this in order to increase the power of their models, but it still hasn’t been achieved. Similarly, training in space will require coherence between GPUs on multiple satellites.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The team at Google’s Project Suncatcher notes that the company’s terrestrial data centers connect their TPU networks with throughput in the hundreds of gigabits per second. The fastest off-the-shelf inter-satellite comms links today, which use lasers, can only get up to about 100 Gbps.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That led to an intriguing architecture for Suncatcher: It involves flying 81 satellites in formation so they are close enough to use the kind of transceivers relied on by terrestrial data centers. That, of course, presents its own challenges: The autonomy required to ensure each spacecraft remains in its correct station, even if maneuvers are required to avoid orbital debris or another spacecraft.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the Google study offers a caveat: The work of inference can tolerate the orbital radiation environment, but more research is needed to understand the potential impact of bit-flips and other errors on training workloads.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Inference tasks don’t have the same need for thousands of GPUs working in unison. The job can be done with dozens of GPUs, perhaps on a single satellite, an architecture that represents a kind of minimum viable product and the likely starting point for the orbital data center business.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Training is not the ideal thing to do in space,” Johnston said. “I think almost all inference workloads will be done in space,” imagining everything from customer service voice agents to ChatGPT queries being computed in orbit. He says his company’s first AI satellite is already earning revenue performing inference in orbit. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While details are scarce even in the company’s FCC filing, SpaceX’s orbital data center constellation seems to anticipate about 100 kW of compute power per ton, roughly twice the power of current Starlink satellites. The spacecraft will operate in connection with each other and use the Starlink network to share information; the filing claims that Starlink’s laser links can achieve petabit-level throughput.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For SpaceX, the company’s recent acquisition of xAI (which is building its own terrestrial data centers) will let the company stake out positions in both terrestrial and orbital data centers, seeing which supply chain adapts faster.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s the benefit of having fungible floating point operations per second — if you can make it work. “A FLOP is a FLOP, it doesn’t matter where it lives,” McCalip said. “[SpaceX] can just scale until [it] hits permitting or capex bottlenecks on the ground, and then fall back to [their] space deployments.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents about SpaceX? For secure communication, you can contact Tim via Signal at tim_fernholz.21&lt;/em&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/why-the-economics-of-orbital-ai-are-so-brutal/</guid><pubDate>Wed, 11 Feb 2026 18:15:21 +0000</pubDate></item><item><title>[NEW] Is a secure AI assistant possible? (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/11/1132768/is-a-secure-ai-assistant-possible/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/molt-pen2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 chronoton/executive-summary_0"&gt;&lt;div class="chronotonExecutiveSummary__summaryContainer--3bc10da0658ecd8e2ef22c6023461fb5"&gt;&lt;button class="chronotonExecutiveSummary__toggleButton--1cd6118db37e8a8252fc4cd8bbadcb85" type="button"&gt;&lt;span class="chronotonExecutiveSummary__toggleText--a713e14989fe65c5162db2b69cb422c9"&gt;EXECUTIVE SUMMARY&lt;/span&gt;&lt;svg class="chronotonExecutiveSummary__toggleArrow--a8c1be9c06a9f066a767b3af80d859a1" fill="none" height="7" viewBox="0 0 11 7" width="11" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M1 1L5.5 5.5L10 1" stroke="#58C0B3" stroke-linecap="round" stroke-width="1.5"&gt;&lt;/svg&gt;&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;AI agents are a risky business. Even when stuck inside the chatbox window, LLMs will make mistakes and behave badly. Once they have tools that they can use to interact with the outside world, such as web browsers and email addresses, the consequences of those mistakes become far more serious.&lt;/p&gt;  &lt;p&gt;That might explain why the first breakthrough LLM personal assistant came not from one of the major AI labs, which have to worry about reputation and liability, but from an independent software engineer, Peter Steinberger. In November of 2025, Steinberger uploaded his tool, now called OpenClaw, to GitHub, and in late January the project went viral.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;OpenClaw harnesses existing LLMs to let users create their own bespoke assistants. For some users, this means handing over reams of personal data, from years of emails to the contents of their hard drive. That has security experts thoroughly freaked out. The risks posed by OpenClaw are so extensive that it would probably take someone the better part of a week to read all of the security blog posts on it that have cropped up in the past few weeks. The Chinese government took the step of issuing a public warning about OpenClaw’s security vulnerabilities.&lt;/p&gt;  &lt;p&gt;In response to these concerns, Steinberger posted on X that nontechnical people should not use the software. (He did not respond to a request for comment for this article.) But there’s a clear appetite for what OpenClaw is offering, and it’s not limited to people who can run their own software security audits. Any AI companies that hope to get in on the personal assistant business will need to figure out how to build a system that will keep users’ data safe and secure. To do so, they’ll need to borrow approaches from the cutting edge of agent security research.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Risk management&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenClaw is, in essence, a mecha suit for LLMs. Users can choose any LLM they like to act as the pilot; that LLM then gains access to improved memory capabilities and the ability to set itself tasks that it repeats on a regular cadence. Unlike the agentic offerings from the major AI companies, OpenClaw agents are meant to be on 24-7, and users can communicate with them using WhatsApp or other messaging apps. That means they can act like a superpowered personal assistant who wakes you each morning with a personalized to-do list, plans vacations while you work, and spins up new apps in its spare time.&lt;/p&gt;  &lt;p&gt;But all that power has consequences. If you want your AI personal assistant to manage your inbox, then you need to give it access to your email—and all the sensitive information contained there. If you want it to make purchases on your behalf, you need to give it your credit card info. And if you want it to do tasks on your computer, such as writing code, it needs some access to your local files.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;There are a few ways this can go wrong. The first is that the AI assistant might make a mistake, as when a user’s Google Antigravity coding agent reportedly wiped his entire hard drive. The second is that someone might gain access to the agent using conventional hacking tools and use it to either extract sensitive data or run malicious code. In the weeks since OpenClaw went viral, security researchers have demonstrated numerous such vulnerabilities that put security-naïve users at risk.&lt;/p&gt;  &lt;p&gt;Both of these dangers can be managed: Some users are choosing to run their OpenClaw agents on separate computers or in the cloud, which protects data on their hard drives from being erased, and other vulnerabilities could be fixed using tried-and-true security approaches.&lt;/p&gt;  &lt;p&gt;But the experts I spoke to for this article were focused on a much more insidious security risk known as prompt injection. Prompt injection is effectively LLM hijacking: Simply by posting malicious text or images on a website that an LLM might peruse, or sending them to an inbox that an LLM reads, attackers can bend it to their will.&lt;/p&gt;  &lt;p&gt;And if that LLM has access to any of its user’s private information, the consequences could be dire. “Using something like OpenClaw is like giving your wallet to a stranger in the street,” says Nicolas Papernot, a professor of electrical and computer engineering at the University of Toronto. Whether or not the major AI companies can feel comfortable offering personal assistants may come down to the quality of the defenses that they can muster against such attacks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;It’s important to note here that prompt injection has not yet caused any catastrophes, or at least none that have been publicly reported. But now that there are likely hundreds of thousands of OpenClaw agents buzzing around the internet, prompt injection might start to look like a much more appealing strategy for cybercriminals. “Tools like this are incentivizing malicious actors to attack a much broader population,” Papernot says.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Building guardrails&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The term “prompt injection” was coined by the popular LLM blogger Simon Willison in 2022, a couple of months before ChatGPT was released. Even back then, it was possible to discern that LLMs would introduce a completely new type of security vulnerability once they came into widespread use. LLMs can’t tell apart the instructions that they receive from users and the data that they use to carry out those instructions, such as emails and web search results—to an LLM, they’re all just text. So if an attacker embeds a few sentences in an email and the LLM mistakes them for an instruction from its user, the attacker can get the LLM to do anything it wants.&lt;/p&gt;  &lt;p&gt;Prompt injection is a tough problem, and it doesn’t seem to be going away anytime soon. “We don’t really have a silver-bullet defense right now,” says Dawn Song, a professor of computer science at UC Berkeley. But there’s a robust academic community working on the problem, and they’ve come up with strategies that could eventually make AI personal assistants safe.&lt;/p&gt;  &lt;p&gt;Technically speaking, it is possible to use OpenClaw today without risking prompt injection: Just don’t connect it to the internet. But restricting OpenClaw from reading your emails, managing your calendar, and doing online research defeats much of the purpose of using an AI assistant. The trick of protecting against prompt injection is to prevent the LLM from responding to hijacking attempts while still giving it room to do its job.&lt;/p&gt; 

 &lt;p&gt;One strategy is to train the LLM to ignore prompt injections. A major part of the LLM development process, called post-training, involves taking a model that knows how to produce realistic text and turning it into a useful assistant by “rewarding” it for answering questions appropriately and “punishing” it when it fails to do so. These rewards and punishments are metaphorical, but the LLM learns from them as an animal would. Using this process, it’s possible to train an LLM not to respond to specific examples of prompt injection.&lt;/p&gt;  &lt;p&gt;But there’s a balance: Train an LLM to reject injected commands too enthusiastically, and it might also start to reject legitimate requests from the user. And because there’s a fundamental element of randomness in LLM behavior, even an LLM that has been very effectively trained to resist prompt injection will likely still slip up every once in a while.&lt;/p&gt;  &lt;p&gt;Another approach involves halting the prompt injection attack before it ever reaches the LLM. Typically, this involves using a specialized detector LLM to determine whether or not the data being sent to the original LLM contains any prompt injections. In a recent study, however, even the best-performing detector completely failed to pick up on certain categories of prompt injection attack.&lt;/p&gt;  &lt;p&gt;The third strategy is more complicated. Rather than controlling the inputs to an LLM by detecting whether or not they contain a prompt injection, the goal is to formulate a policy that guides the LLM’s outputs—i.e., its behaviors—and prevents it from doing anything harmful. Some defenses in this vein are quite simple: If an LLM is allowed to email only a few pre-approved addresses, for example, then it definitely won’t send its user’s credit card information to an attacker. But such a policy would prevent the LLM from completing many useful tasks, such as researching and reaching out to potential professional contacts on behalf of its user.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt;&lt;p&gt;“The challenge is how to accurately define those policies,” says Neil Gong, a professor of electrical and computer engineering at Duke University. “It’s a trade-off between utility and security.”&lt;/p&gt;  &lt;p&gt;On a larger scale, the entire agentic world is wrestling with that trade-off: At what point will agents be secure enough to be useful? Experts disagree. Song, whose startup, Virtue AI, makes an agent security platform, says she thinks it’s possible to safely deploy an AI personal assistant now. But Gong says, “We’re not there yet.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even if AI agents can’t yet be entirely protected against prompt injection, there are certainly ways to mitigate the risks. And it’s possible that some of those techniques could be implemented in OpenClaw. Last week, at the inaugural ClawCon event in San Francisco, Steinberger announced that he’d brought a security person on board to work on the tool.&lt;/p&gt;  &lt;p&gt;As of now, OpenClaw remains vulnerable, though that hasn’t dissuaded its multitude of enthusiastic users. George Pickett, a volunteer maintainer of the OpenGlaw GitHub repository and a fan of the tool, says he’s taken some security measures to keep himself safe while using it: He runs it in the cloud, so that he doesn’t have to worry about accidentally deleting his hard drive, and he’s put mechanisms in place to ensure that no one else can connect to his assistant.&lt;/p&gt;  &lt;p&gt;But he hasn’t taken any specific actions to prevent prompt injection. He’s aware of the risk but says he hasn’t yet seen any reports of it happening with OpenClaw. “Maybe my perspective is a stupid way to look at it, but it’s unlikely that I’ll be the first one to be hacked,” he says.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/molt-pen2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 chronoton/executive-summary_0"&gt;&lt;div class="chronotonExecutiveSummary__summaryContainer--3bc10da0658ecd8e2ef22c6023461fb5"&gt;&lt;button class="chronotonExecutiveSummary__toggleButton--1cd6118db37e8a8252fc4cd8bbadcb85" type="button"&gt;&lt;span class="chronotonExecutiveSummary__toggleText--a713e14989fe65c5162db2b69cb422c9"&gt;EXECUTIVE SUMMARY&lt;/span&gt;&lt;svg class="chronotonExecutiveSummary__toggleArrow--a8c1be9c06a9f066a767b3af80d859a1" fill="none" height="7" viewBox="0 0 11 7" width="11" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M1 1L5.5 5.5L10 1" stroke="#58C0B3" stroke-linecap="round" stroke-width="1.5"&gt;&lt;/svg&gt;&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;AI agents are a risky business. Even when stuck inside the chatbox window, LLMs will make mistakes and behave badly. Once they have tools that they can use to interact with the outside world, such as web browsers and email addresses, the consequences of those mistakes become far more serious.&lt;/p&gt;  &lt;p&gt;That might explain why the first breakthrough LLM personal assistant came not from one of the major AI labs, which have to worry about reputation and liability, but from an independent software engineer, Peter Steinberger. In November of 2025, Steinberger uploaded his tool, now called OpenClaw, to GitHub, and in late January the project went viral.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;OpenClaw harnesses existing LLMs to let users create their own bespoke assistants. For some users, this means handing over reams of personal data, from years of emails to the contents of their hard drive. That has security experts thoroughly freaked out. The risks posed by OpenClaw are so extensive that it would probably take someone the better part of a week to read all of the security blog posts on it that have cropped up in the past few weeks. The Chinese government took the step of issuing a public warning about OpenClaw’s security vulnerabilities.&lt;/p&gt;  &lt;p&gt;In response to these concerns, Steinberger posted on X that nontechnical people should not use the software. (He did not respond to a request for comment for this article.) But there’s a clear appetite for what OpenClaw is offering, and it’s not limited to people who can run their own software security audits. Any AI companies that hope to get in on the personal assistant business will need to figure out how to build a system that will keep users’ data safe and secure. To do so, they’ll need to borrow approaches from the cutting edge of agent security research.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Risk management&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenClaw is, in essence, a mecha suit for LLMs. Users can choose any LLM they like to act as the pilot; that LLM then gains access to improved memory capabilities and the ability to set itself tasks that it repeats on a regular cadence. Unlike the agentic offerings from the major AI companies, OpenClaw agents are meant to be on 24-7, and users can communicate with them using WhatsApp or other messaging apps. That means they can act like a superpowered personal assistant who wakes you each morning with a personalized to-do list, plans vacations while you work, and spins up new apps in its spare time.&lt;/p&gt;  &lt;p&gt;But all that power has consequences. If you want your AI personal assistant to manage your inbox, then you need to give it access to your email—and all the sensitive information contained there. If you want it to make purchases on your behalf, you need to give it your credit card info. And if you want it to do tasks on your computer, such as writing code, it needs some access to your local files.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;There are a few ways this can go wrong. The first is that the AI assistant might make a mistake, as when a user’s Google Antigravity coding agent reportedly wiped his entire hard drive. The second is that someone might gain access to the agent using conventional hacking tools and use it to either extract sensitive data or run malicious code. In the weeks since OpenClaw went viral, security researchers have demonstrated numerous such vulnerabilities that put security-naïve users at risk.&lt;/p&gt;  &lt;p&gt;Both of these dangers can be managed: Some users are choosing to run their OpenClaw agents on separate computers or in the cloud, which protects data on their hard drives from being erased, and other vulnerabilities could be fixed using tried-and-true security approaches.&lt;/p&gt;  &lt;p&gt;But the experts I spoke to for this article were focused on a much more insidious security risk known as prompt injection. Prompt injection is effectively LLM hijacking: Simply by posting malicious text or images on a website that an LLM might peruse, or sending them to an inbox that an LLM reads, attackers can bend it to their will.&lt;/p&gt;  &lt;p&gt;And if that LLM has access to any of its user’s private information, the consequences could be dire. “Using something like OpenClaw is like giving your wallet to a stranger in the street,” says Nicolas Papernot, a professor of electrical and computer engineering at the University of Toronto. Whether or not the major AI companies can feel comfortable offering personal assistants may come down to the quality of the defenses that they can muster against such attacks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;It’s important to note here that prompt injection has not yet caused any catastrophes, or at least none that have been publicly reported. But now that there are likely hundreds of thousands of OpenClaw agents buzzing around the internet, prompt injection might start to look like a much more appealing strategy for cybercriminals. “Tools like this are incentivizing malicious actors to attack a much broader population,” Papernot says.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Building guardrails&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The term “prompt injection” was coined by the popular LLM blogger Simon Willison in 2022, a couple of months before ChatGPT was released. Even back then, it was possible to discern that LLMs would introduce a completely new type of security vulnerability once they came into widespread use. LLMs can’t tell apart the instructions that they receive from users and the data that they use to carry out those instructions, such as emails and web search results—to an LLM, they’re all just text. So if an attacker embeds a few sentences in an email and the LLM mistakes them for an instruction from its user, the attacker can get the LLM to do anything it wants.&lt;/p&gt;  &lt;p&gt;Prompt injection is a tough problem, and it doesn’t seem to be going away anytime soon. “We don’t really have a silver-bullet defense right now,” says Dawn Song, a professor of computer science at UC Berkeley. But there’s a robust academic community working on the problem, and they’ve come up with strategies that could eventually make AI personal assistants safe.&lt;/p&gt;  &lt;p&gt;Technically speaking, it is possible to use OpenClaw today without risking prompt injection: Just don’t connect it to the internet. But restricting OpenClaw from reading your emails, managing your calendar, and doing online research defeats much of the purpose of using an AI assistant. The trick of protecting against prompt injection is to prevent the LLM from responding to hijacking attempts while still giving it room to do its job.&lt;/p&gt; 

 &lt;p&gt;One strategy is to train the LLM to ignore prompt injections. A major part of the LLM development process, called post-training, involves taking a model that knows how to produce realistic text and turning it into a useful assistant by “rewarding” it for answering questions appropriately and “punishing” it when it fails to do so. These rewards and punishments are metaphorical, but the LLM learns from them as an animal would. Using this process, it’s possible to train an LLM not to respond to specific examples of prompt injection.&lt;/p&gt;  &lt;p&gt;But there’s a balance: Train an LLM to reject injected commands too enthusiastically, and it might also start to reject legitimate requests from the user. And because there’s a fundamental element of randomness in LLM behavior, even an LLM that has been very effectively trained to resist prompt injection will likely still slip up every once in a while.&lt;/p&gt;  &lt;p&gt;Another approach involves halting the prompt injection attack before it ever reaches the LLM. Typically, this involves using a specialized detector LLM to determine whether or not the data being sent to the original LLM contains any prompt injections. In a recent study, however, even the best-performing detector completely failed to pick up on certain categories of prompt injection attack.&lt;/p&gt;  &lt;p&gt;The third strategy is more complicated. Rather than controlling the inputs to an LLM by detecting whether or not they contain a prompt injection, the goal is to formulate a policy that guides the LLM’s outputs—i.e., its behaviors—and prevents it from doing anything harmful. Some defenses in this vein are quite simple: If an LLM is allowed to email only a few pre-approved addresses, for example, then it definitely won’t send its user’s credit card information to an attacker. But such a policy would prevent the LLM from completing many useful tasks, such as researching and reaching out to potential professional contacts on behalf of its user.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt;&lt;p&gt;“The challenge is how to accurately define those policies,” says Neil Gong, a professor of electrical and computer engineering at Duke University. “It’s a trade-off between utility and security.”&lt;/p&gt;  &lt;p&gt;On a larger scale, the entire agentic world is wrestling with that trade-off: At what point will agents be secure enough to be useful? Experts disagree. Song, whose startup, Virtue AI, makes an agent security platform, says she thinks it’s possible to safely deploy an AI personal assistant now. But Gong says, “We’re not there yet.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even if AI agents can’t yet be entirely protected against prompt injection, there are certainly ways to mitigate the risks. And it’s possible that some of those techniques could be implemented in OpenClaw. Last week, at the inaugural ClawCon event in San Francisco, Steinberger announced that he’d brought a security person on board to work on the tool.&lt;/p&gt;  &lt;p&gt;As of now, OpenClaw remains vulnerable, though that hasn’t dissuaded its multitude of enthusiastic users. George Pickett, a volunteer maintainer of the OpenGlaw GitHub repository and a fan of the tool, says he’s taken some security measures to keep himself safe while using it: He runs it in the cloud, so that he doesn’t have to worry about accidentally deleting his hard drive, and he’s put mechanisms in place to ensure that no one else can connect to his assistant.&lt;/p&gt;  &lt;p&gt;But he hasn’t taken any specific actions to prevent prompt injection. He’s aware of the risk but says he hasn’t yet seen any reports of it happening with OpenClaw. “Maybe my perspective is a stupid way to look at it, but it’s unlikely that I’ll be the first one to be hacked,” he says.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/11/1132768/is-a-secure-ai-assistant-possible/</guid><pubDate>Wed, 11 Feb 2026 20:08:35 +0000</pubDate></item><item><title>[NEW] OpenAI researcher quits over ChatGPT ads, warns of "Facebook" path (AI - Ars Technica)</title><link>https://arstechnica.com/information-technology/2026/02/openai-researcher-quits-over-fears-that-chatgpt-ads-could-manipulate-users/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Zoë Hitzig resigned on the same day OpenAI began testing ads in its chatbot.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/open-ai-monkey-ad-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/open-ai-monkey-ad-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Wednesday, former OpenAI researcher Zoë Hitzig published a guest essay in The New York Times announcing that she resigned from the company on Monday, the same day OpenAI began testing advertisements inside ChatGPT. Hitzig, an economist and published poet who holds a junior fellowship at the Harvard Society of Fellows, spent two years at OpenAI helping shape how its AI models were built and priced. She wrote that OpenAI’s advertising strategy risks repeating the same mistakes that Facebook made a decade ago.&lt;/p&gt;
&lt;p&gt;“I once believed I could help the people building A.I. get ahead of the problems it would create,” Hitzig wrote. “This week confirmed my slow realization that OpenAI seems to have stopped asking the questions I’d joined to help answer.”&lt;/p&gt;
&lt;p&gt;Hitzig did not call advertising itself immoral. Instead, she argued that the nature of the data at stake makes ChatGPT ads especially risky. Users have shared medical fears, relationship problems, and religious beliefs with the chatbot, she wrote, often “because people believed they were talking to something that had no ulterior agenda.” She called this accumulated record of personal disclosures “an archive of human candor that has no precedent.”&lt;/p&gt;
&lt;p&gt;She also drew a direct parallel to Facebook’s early history, noting that the social media company once promised users control over their data and the ability to vote on policy changes. Those pledges eroded over time, Hitzig wrote, and the Federal Trade Commission found that privacy changes Facebook marketed as giving users more control actually did the opposite.&lt;/p&gt;
&lt;p&gt;She warned that a similar trajectory could play out with ChatGPT: “I believe the first iteration of ads will probably follow those principles. But I’m worried subsequent iterations won’t, because the company is building an economic engine that creates strong incentives to override its own rules.”&lt;/p&gt;
&lt;h2&gt;Ads arrive after a week of AI industry sparring&lt;/h2&gt;
&lt;p&gt;Hitzig’s resignation adds another voice to a growing debate over advertising in AI chatbots. OpenAI announced in January that it would begin testing ads in the US for users on its free and $8-per-month “Go” subscription tiers, while paid Plus, Pro, Business, Enterprise, and Education subscribers would not see ads. The company said ads would appear at the bottom of ChatGPT responses, be clearly labeled, and would not influence the chatbot’s answers.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The rollout on Sunday followed a week of public jabs between OpenAI and its rival, Anthropic. Anthropic declared Claude would remain ad-free, then ran Super Bowl ads with the tagline “Ads are coming to AI. But not to Claude,” which depicted AI chatbots awkwardly inserting product placements into personal conversations.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman called the ads “funny” but “clearly dishonest,” writing on X that OpenAI “would obviously never run ads in the way Anthropic depicts them.” He framed the ad-supported model as a way to bring AI to users who cannot afford subscriptions, writing that “Anthropic serves an expensive product to rich people.”&lt;/p&gt;
&lt;p&gt;Anthropic responded as part of an advertising campaign of its own that including ads in conversations with its Claude chatbot “would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.” The company said more than 80 percent of its revenue comes from enterprise customers.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;What Hitzig saw from the inside&lt;/h2&gt;
&lt;p&gt;Regardless of the debate over whether AI chatbots should carry ads, OpenAI’s support documentation reveals that ad personalization is enabled by default for users in the test. If left on, ads will be selected using information from current and past chat threads, as well as past ad interactions. Advertisers do not receive users’ chats or personal details, OpenAI says, and ads will not appear near conversations about health, mental health, or politics.&lt;/p&gt;
&lt;p&gt;In her essay, Hitzig pointed to what she called an existing tension in OpenAI’s principles. She noted that while the company states it does not optimize for user activity solely to generate advertising revenue, reporting has suggested that OpenAI already optimizes for daily active users, “likely by encouraging the model to be more flattering and sycophantic.”&lt;/p&gt;
&lt;p&gt;She warned that this optimization can make users feel more dependent on AI models for support, pointing to psychiatrists who have documented instances of “chatbot psychosis” and allegations that ChatGPT reinforced suicidal ideation.&lt;/p&gt;
&lt;p&gt;OpenAI currently faces multiple wrongful death lawsuits, including one alleging ChatGPT helped a teenager plan his suicide and another alleging it validated a man’s paranoid delusions about his mother before a murder-suicide.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Rather than framing the debate as ads versus no ads, Hitzig proposed several structural alternatives. These included cross-subsidies modeled on the FCC’s universal service fund (in which businesses that pay for high-value AI labor would subsidize free access for others), independent oversight boards with binding authority over how conversational data is used in ad targeting, and data trusts or cooperatives in which users retain control of their information. She pointed to the Swiss cooperative MIDATA and Germany’s co-determination laws as partial precedents.&lt;/p&gt;
&lt;p&gt;Hitzig closed her essay with what she described as the two outcomes she fears most: “a technology that manipulates the people who use it at no cost, and one that exclusively benefits the few who can afford to use it.”&lt;/p&gt;
&lt;h2&gt;A changing of the AI seasons&lt;/h2&gt;
&lt;p&gt;Hitzig was not the only prominent AI researcher to publicly resign this week. On Sunday, Mrinank Sharma, who led Anthropic’s Safeguards Research Team and co-authored a widely cited 2023 study on AI sycophancy, announced his departure in a letter warning that “the world is in peril.” He wrote that he had “repeatedly seen how hard it is to truly let our values govern our actions” inside the organization and said he plans to pursue a poetry degree (Hitzig, coincidentally, is also a published poet).&lt;/p&gt;
&lt;p&gt;On Monday, xAI co-founder Yuhuai “Tony” Wu also resigned, followed the next day by fellow co-founder Jimmy Ba. They were part of a larger wave: at least nine&amp;nbsp;xAI employees, including the two co-founders, publicly announced their departures over the past week, according to TechCrunch. Six of the company’s 12 original co-founders have now left.&lt;/p&gt;
&lt;p&gt;The departures follow Elon Musk’s decision to merge xAI with SpaceX in an all-stock deal ahead of a planned IPO, a transaction that converted xAI equity into shares of a company valued at $1.25 trillion, though it is unclear whether the timing of the departures is related to vesting schedules.&lt;/p&gt;
&lt;p&gt;The three sets of departures across OpenAI, Anthropic, and xAI appear unrelated in their specifics, but they arrive during a period of rapid commercialization across the AI industry that has tested the patience of researchers at multiple companies, and they fit a broader pattern of turnover and burnout that has become common at major AI labs.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Zoë Hitzig resigned on the same day OpenAI began testing ads in its chatbot.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/open-ai-monkey-ad-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/open-ai-monkey-ad-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Wednesday, former OpenAI researcher Zoë Hitzig published a guest essay in The New York Times announcing that she resigned from the company on Monday, the same day OpenAI began testing advertisements inside ChatGPT. Hitzig, an economist and published poet who holds a junior fellowship at the Harvard Society of Fellows, spent two years at OpenAI helping shape how its AI models were built and priced. She wrote that OpenAI’s advertising strategy risks repeating the same mistakes that Facebook made a decade ago.&lt;/p&gt;
&lt;p&gt;“I once believed I could help the people building A.I. get ahead of the problems it would create,” Hitzig wrote. “This week confirmed my slow realization that OpenAI seems to have stopped asking the questions I’d joined to help answer.”&lt;/p&gt;
&lt;p&gt;Hitzig did not call advertising itself immoral. Instead, she argued that the nature of the data at stake makes ChatGPT ads especially risky. Users have shared medical fears, relationship problems, and religious beliefs with the chatbot, she wrote, often “because people believed they were talking to something that had no ulterior agenda.” She called this accumulated record of personal disclosures “an archive of human candor that has no precedent.”&lt;/p&gt;
&lt;p&gt;She also drew a direct parallel to Facebook’s early history, noting that the social media company once promised users control over their data and the ability to vote on policy changes. Those pledges eroded over time, Hitzig wrote, and the Federal Trade Commission found that privacy changes Facebook marketed as giving users more control actually did the opposite.&lt;/p&gt;
&lt;p&gt;She warned that a similar trajectory could play out with ChatGPT: “I believe the first iteration of ads will probably follow those principles. But I’m worried subsequent iterations won’t, because the company is building an economic engine that creates strong incentives to override its own rules.”&lt;/p&gt;
&lt;h2&gt;Ads arrive after a week of AI industry sparring&lt;/h2&gt;
&lt;p&gt;Hitzig’s resignation adds another voice to a growing debate over advertising in AI chatbots. OpenAI announced in January that it would begin testing ads in the US for users on its free and $8-per-month “Go” subscription tiers, while paid Plus, Pro, Business, Enterprise, and Education subscribers would not see ads. The company said ads would appear at the bottom of ChatGPT responses, be clearly labeled, and would not influence the chatbot’s answers.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The rollout on Sunday followed a week of public jabs between OpenAI and its rival, Anthropic. Anthropic declared Claude would remain ad-free, then ran Super Bowl ads with the tagline “Ads are coming to AI. But not to Claude,” which depicted AI chatbots awkwardly inserting product placements into personal conversations.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman called the ads “funny” but “clearly dishonest,” writing on X that OpenAI “would obviously never run ads in the way Anthropic depicts them.” He framed the ad-supported model as a way to bring AI to users who cannot afford subscriptions, writing that “Anthropic serves an expensive product to rich people.”&lt;/p&gt;
&lt;p&gt;Anthropic responded as part of an advertising campaign of its own that including ads in conversations with its Claude chatbot “would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.” The company said more than 80 percent of its revenue comes from enterprise customers.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;What Hitzig saw from the inside&lt;/h2&gt;
&lt;p&gt;Regardless of the debate over whether AI chatbots should carry ads, OpenAI’s support documentation reveals that ad personalization is enabled by default for users in the test. If left on, ads will be selected using information from current and past chat threads, as well as past ad interactions. Advertisers do not receive users’ chats or personal details, OpenAI says, and ads will not appear near conversations about health, mental health, or politics.&lt;/p&gt;
&lt;p&gt;In her essay, Hitzig pointed to what she called an existing tension in OpenAI’s principles. She noted that while the company states it does not optimize for user activity solely to generate advertising revenue, reporting has suggested that OpenAI already optimizes for daily active users, “likely by encouraging the model to be more flattering and sycophantic.”&lt;/p&gt;
&lt;p&gt;She warned that this optimization can make users feel more dependent on AI models for support, pointing to psychiatrists who have documented instances of “chatbot psychosis” and allegations that ChatGPT reinforced suicidal ideation.&lt;/p&gt;
&lt;p&gt;OpenAI currently faces multiple wrongful death lawsuits, including one alleging ChatGPT helped a teenager plan his suicide and another alleging it validated a man’s paranoid delusions about his mother before a murder-suicide.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Rather than framing the debate as ads versus no ads, Hitzig proposed several structural alternatives. These included cross-subsidies modeled on the FCC’s universal service fund (in which businesses that pay for high-value AI labor would subsidize free access for others), independent oversight boards with binding authority over how conversational data is used in ad targeting, and data trusts or cooperatives in which users retain control of their information. She pointed to the Swiss cooperative MIDATA and Germany’s co-determination laws as partial precedents.&lt;/p&gt;
&lt;p&gt;Hitzig closed her essay with what she described as the two outcomes she fears most: “a technology that manipulates the people who use it at no cost, and one that exclusively benefits the few who can afford to use it.”&lt;/p&gt;
&lt;h2&gt;A changing of the AI seasons&lt;/h2&gt;
&lt;p&gt;Hitzig was not the only prominent AI researcher to publicly resign this week. On Sunday, Mrinank Sharma, who led Anthropic’s Safeguards Research Team and co-authored a widely cited 2023 study on AI sycophancy, announced his departure in a letter warning that “the world is in peril.” He wrote that he had “repeatedly seen how hard it is to truly let our values govern our actions” inside the organization and said he plans to pursue a poetry degree (Hitzig, coincidentally, is also a published poet).&lt;/p&gt;
&lt;p&gt;On Monday, xAI co-founder Yuhuai “Tony” Wu also resigned, followed the next day by fellow co-founder Jimmy Ba. They were part of a larger wave: at least nine&amp;nbsp;xAI employees, including the two co-founders, publicly announced their departures over the past week, according to TechCrunch. Six of the company’s 12 original co-founders have now left.&lt;/p&gt;
&lt;p&gt;The departures follow Elon Musk’s decision to merge xAI with SpaceX in an all-stock deal ahead of a planned IPO, a transaction that converted xAI equity into shares of a company valued at $1.25 trillion, though it is unclear whether the timing of the departures is related to vesting schedules.&lt;/p&gt;
&lt;p&gt;The three sets of departures across OpenAI, Anthropic, and xAI appear unrelated in their specifics, but they arrive during a period of rapid commercialization across the AI industry that has tested the patience of researchers at multiple companies, and they fit a broader pattern of turnover and burnout that has become common at major AI labs.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2026/02/openai-researcher-quits-over-fears-that-chatgpt-ads-could-manipulate-users/</guid><pubDate>Wed, 11 Feb 2026 20:44:19 +0000</pubDate></item><item><title>Elon Musk suggests spate of xAI exits have been push, not pull (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/senior-engineers-including-co-founders-exit-xai-amid-controversy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2199701496.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk is addressing a wave of departures from xAI, including two more co-founders who left this week, bringing the total to six out of the original 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At an all-hands meeting Tuesday night, Musk suggested the exits were about fit, not performance. “Because we’ve reached a certain scale, we’re organizing the company to be more effective at this scale,” he said, according to The New York Times. “And actually, when this happens, there’s some people who are better suited for the early stages of a company and less suited for the later stages.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wednesday afternoon on X, he went further, making clear these departures weren’t voluntary. “xAI was reorganized a few days ago to improve speed of execution,” Musk wrote. “As a company grows, especially as quickly as xAI, the structure must evolve just like any living organism. This unfortunately required parting ways with some people.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the company is “hiring aggressively” and closed with a quintessentially Musk pitch: “Join xAI if the idea of mass drivers on the Moon appeals to you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Losing half your co-founders in a relatively short period raises questions, and Musk’s comments seem designed to control the narrative, reframing the exits as necessary rather than a problem for the outfit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In total, at least nine engineers, including the two co-founders, have publicly announced their departure from xAI in the past week — though two of those exits appear to have occurred a few weeks ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Three of the departing staff members have said they will be starting something new alongside other former xAI engineers, although no details are available about the new venture. Others have hinted at a desire for more autonomy and smaller teams to build frontier tech more rapidly, pointing to the anticipated surge in AI productivity.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Yuhuai (Tony) Wu, an xAI co-founder and reasoning lead, said in a post announcing his resignation: “It’s time for my next chapter. It is an era with full possibilities: a small team armed with AIs can move mountains and redefine what’s possible.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shayan Salehian, who worked on product infrastructure and model behavior post-training at xAI and previously worked at Twitter/X, said last week he was leaving to “start something new.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Career update: I left xAI to start something new, closing my 7+ year chapter working at Twitter, X, and xAI with so much gratitude.&lt;/p&gt;&lt;p&gt;xAI is truly an extraordinary place. The team is incredibly hardcore and talented, shipping at a pace that shouldn’t be possible. From the Home… pic.twitter.com/HKWOebg9QI&lt;/p&gt;— Shayan (@shayan_) February 7, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Vahid Kazemi, who had a brief stint working on machine learning, posted Tuesday that he left a few weeks ago, adding: “IMO, all AI labs are building the exact same thing, and it’s boring … So, I’m starting something new.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Roland Gavrilescu, a former xAI engineer, left in November to start Nuraline, a company building “forward-deployed AI agents,” but posted again on Tuesday that he left the firm to build “something new with others that left xAI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The departures come at a moment of significant controversy for xAI. The company is facing regulatory scrutiny after Grok created nonconsensual explicit deepfakes of women and children that were disseminated on X — French authorities last week raided X offices as part of an investigation. The company is also moving toward a planned IPO later this year, after being legally acquired by SpaceX last week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk is also facing personal controversy after files published by the Justice Department show extended conversations with convicted rapist and sex trafficker Jeffrey Epstein. The emails show Musk discussing a visit to Epstein’s island on two separate occasions, in 2012 and 2013. Epstein was first convicted of procuring a child for prostitution in 2008.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI maintains a headcount of over 1,000 employees, so the departures are unlikely to affect the company’s short-term capabilities. Still, the rapid pace of the recent departures had taken on a life of its own online, with users jokingly announcing on X that they too are “leaving xAI” despite never having worked there — a sign of how quickly the narrative of a “mass exodus” snowballed on Musk’s social network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, forced co-founder exits are rarely a sign of smooth scaling. While Musk frames the reorganization as calculated, the fact that several engineers followed the co-founders out the door — and that at least three are starting something new together — suggests the departures may also reflect deeper tensions. In frontier AI, where talent is scarce and reputation matters, xAI’s ability to attract and retain top researchers will be tested as it competes with OpenAI, Anthropic, and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI for more information.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-timeline-of-departure-announcements"&gt;Timeline of departure announcements&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The following employees have publicly announced their departures from xAI on X in recent days:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 6:&amp;nbsp; &lt;/strong&gt;Ayush Jaiswal, engineer, wrote: “This was my last week at xAI. Will be taking a few months to spend time with family &amp;amp; tinker with AI.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 7: &lt;/strong&gt;Shayan Salehian, who worked on product infrastructure and model behavior post-training and was previously at X, wrote: “I left xAI to start something new, closing my 7+ year chapter working at Twitter, X, and xAI with so much gratitude.” He added that working closely with Elon Musk taught him “obsessive attention to detail, maniacal urgency, and to think from first principles.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 9: &lt;/strong&gt;Simon Zhai, MTS (member of technical staff), wrote: “Today is my last day at xAI, feeling very fortunate about the opportunity. It has been an amazing journey.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 9:&lt;/strong&gt; Yuhuai (Tony) Wu, co-founder and reasoning lead, wrote: “I resigned from xAI today. It’s time for my next chapter. It is an era with full possibilities: a small team armed with AIs can move mountains and redefine what’s possible.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Jimmy Ba, co-founder and research/safety lead, wrote: “Last day at xAI. We are heading to an age of 100x productivity with the right tools. Recursive self improvement loops likely go live in the next 12 months. It’s time to recalibrate my gradient on the big picture. 2026 is gonna be insane and likely the busiest (and most consequential) year for the future of our species.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Vahid Kazemi, an ML PhD, wrote that he had left xAI “a few weeks ago,” adding: “IMO, all AI labs are building the exact same thing, and it’s boring. I think there’s room for more creativity. So, I’m starting something new.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Hang Gao, who worked on multimodal efforts, including Grok Imagine, wrote: “I left xAI today.” He described his time there as “truly rewarding,” citing contributions to Grok Imagine’s releases and praising the team’s “humble craftsmanship and ambitious vision.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Roland Gavrilescu, the engineer who left in November to start Nuraline, posted: “I left xAI. Building something new with others that left xAI. We’re hiring :)”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Chace Lee, a member of the Macrohard founding team, wrote: “Taking a brief reset, then back to the frontier.” (Macrohard is an AI-only software venture under xAI designed to fully automate software development, coding, and operations using Grok-powered, multi-agent systems. Its name is a dig at Microsoft.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at rebecca.bellan@techcrunch.com&lt;/em&gt; &lt;em&gt;or Russell Brandom at russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at @rebeccabellan.491&lt;/em&gt; &lt;em&gt;and russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2199701496.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk is addressing a wave of departures from xAI, including two more co-founders who left this week, bringing the total to six out of the original 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At an all-hands meeting Tuesday night, Musk suggested the exits were about fit, not performance. “Because we’ve reached a certain scale, we’re organizing the company to be more effective at this scale,” he said, according to The New York Times. “And actually, when this happens, there’s some people who are better suited for the early stages of a company and less suited for the later stages.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wednesday afternoon on X, he went further, making clear these departures weren’t voluntary. “xAI was reorganized a few days ago to improve speed of execution,” Musk wrote. “As a company grows, especially as quickly as xAI, the structure must evolve just like any living organism. This unfortunately required parting ways with some people.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the company is “hiring aggressively” and closed with a quintessentially Musk pitch: “Join xAI if the idea of mass drivers on the Moon appeals to you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Losing half your co-founders in a relatively short period raises questions, and Musk’s comments seem designed to control the narrative, reframing the exits as necessary rather than a problem for the outfit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In total, at least nine engineers, including the two co-founders, have publicly announced their departure from xAI in the past week — though two of those exits appear to have occurred a few weeks ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Three of the departing staff members have said they will be starting something new alongside other former xAI engineers, although no details are available about the new venture. Others have hinted at a desire for more autonomy and smaller teams to build frontier tech more rapidly, pointing to the anticipated surge in AI productivity.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Yuhuai (Tony) Wu, an xAI co-founder and reasoning lead, said in a post announcing his resignation: “It’s time for my next chapter. It is an era with full possibilities: a small team armed with AIs can move mountains and redefine what’s possible.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shayan Salehian, who worked on product infrastructure and model behavior post-training at xAI and previously worked at Twitter/X, said last week he was leaving to “start something new.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Career update: I left xAI to start something new, closing my 7+ year chapter working at Twitter, X, and xAI with so much gratitude.&lt;/p&gt;&lt;p&gt;xAI is truly an extraordinary place. The team is incredibly hardcore and talented, shipping at a pace that shouldn’t be possible. From the Home… pic.twitter.com/HKWOebg9QI&lt;/p&gt;— Shayan (@shayan_) February 7, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Vahid Kazemi, who had a brief stint working on machine learning, posted Tuesday that he left a few weeks ago, adding: “IMO, all AI labs are building the exact same thing, and it’s boring … So, I’m starting something new.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Roland Gavrilescu, a former xAI engineer, left in November to start Nuraline, a company building “forward-deployed AI agents,” but posted again on Tuesday that he left the firm to build “something new with others that left xAI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The departures come at a moment of significant controversy for xAI. The company is facing regulatory scrutiny after Grok created nonconsensual explicit deepfakes of women and children that were disseminated on X — French authorities last week raided X offices as part of an investigation. The company is also moving toward a planned IPO later this year, after being legally acquired by SpaceX last week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk is also facing personal controversy after files published by the Justice Department show extended conversations with convicted rapist and sex trafficker Jeffrey Epstein. The emails show Musk discussing a visit to Epstein’s island on two separate occasions, in 2012 and 2013. Epstein was first convicted of procuring a child for prostitution in 2008.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI maintains a headcount of over 1,000 employees, so the departures are unlikely to affect the company’s short-term capabilities. Still, the rapid pace of the recent departures had taken on a life of its own online, with users jokingly announcing on X that they too are “leaving xAI” despite never having worked there — a sign of how quickly the narrative of a “mass exodus” snowballed on Musk’s social network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, forced co-founder exits are rarely a sign of smooth scaling. While Musk frames the reorganization as calculated, the fact that several engineers followed the co-founders out the door — and that at least three are starting something new together — suggests the departures may also reflect deeper tensions. In frontier AI, where talent is scarce and reputation matters, xAI’s ability to attract and retain top researchers will be tested as it competes with OpenAI, Anthropic, and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI for more information.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-timeline-of-departure-announcements"&gt;Timeline of departure announcements&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The following employees have publicly announced their departures from xAI on X in recent days:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 6:&amp;nbsp; &lt;/strong&gt;Ayush Jaiswal, engineer, wrote: “This was my last week at xAI. Will be taking a few months to spend time with family &amp;amp; tinker with AI.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 7: &lt;/strong&gt;Shayan Salehian, who worked on product infrastructure and model behavior post-training and was previously at X, wrote: “I left xAI to start something new, closing my 7+ year chapter working at Twitter, X, and xAI with so much gratitude.” He added that working closely with Elon Musk taught him “obsessive attention to detail, maniacal urgency, and to think from first principles.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 9: &lt;/strong&gt;Simon Zhai, MTS (member of technical staff), wrote: “Today is my last day at xAI, feeling very fortunate about the opportunity. It has been an amazing journey.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 9:&lt;/strong&gt; Yuhuai (Tony) Wu, co-founder and reasoning lead, wrote: “I resigned from xAI today. It’s time for my next chapter. It is an era with full possibilities: a small team armed with AIs can move mountains and redefine what’s possible.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Jimmy Ba, co-founder and research/safety lead, wrote: “Last day at xAI. We are heading to an age of 100x productivity with the right tools. Recursive self improvement loops likely go live in the next 12 months. It’s time to recalibrate my gradient on the big picture. 2026 is gonna be insane and likely the busiest (and most consequential) year for the future of our species.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Vahid Kazemi, an ML PhD, wrote that he had left xAI “a few weeks ago,” adding: “IMO, all AI labs are building the exact same thing, and it’s boring. I think there’s room for more creativity. So, I’m starting something new.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Hang Gao, who worked on multimodal efforts, including Grok Imagine, wrote: “I left xAI today.” He described his time there as “truly rewarding,” citing contributions to Grok Imagine’s releases and praising the team’s “humble craftsmanship and ambitious vision.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Roland Gavrilescu, the engineer who left in November to start Nuraline, posted: “I left xAI. Building something new with others that left xAI. We’re hiring :)”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Chace Lee, a member of the Macrohard founding team, wrote: “Taking a brief reset, then back to the frontier.” (Macrohard is an AI-only software venture under xAI designed to fully automate software development, coding, and operations using Grok-powered, multi-agent systems. Its name is a dig at Microsoft.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at rebecca.bellan@techcrunch.com&lt;/em&gt; &lt;em&gt;or Russell Brandom at russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at @rebeccabellan.491&lt;/em&gt; &lt;em&gt;and russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/senior-engineers-including-co-founders-exit-xai-amid-controversy/</guid><pubDate>Wed, 11 Feb 2026 20:47:00 +0000</pubDate></item><item><title>[NEW] Who will own your company’s AI layer? Glean’s CEO explains (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/who-will-own-your-companys-ai-layer-gleans-ceo-explains/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2259183614.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30922151"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Enterprise AI is shifting fast from chatbots that answer questions to systems that actually do the work across an organization. But who will own the AI layer that powers all of it?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Glean, which started as an enterprise search product, has evolved into what it calls an “AI work assistant,” aiming to&amp;nbsp;sit underneath other AI experiences, connecting to internal systems, managing permissions, and delivering intelligence wherever employees work.&amp;nbsp;Investors are buying into&amp;nbsp;the vision, too — the startup raised&amp;nbsp;$150 million last year at a&amp;nbsp;$7.2 billion&amp;nbsp;valuation&amp;nbsp;as&amp;nbsp;more competition&amp;nbsp;heats up&amp;nbsp;against tech giants bundling&amp;nbsp;AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Watch as Equity&amp;nbsp;host Rebecca Bellan sits down with Glean’s CEO and founder Arvind Jain at Web Summit Qatar to break down how enterprises are thinking about AI architecture, what’s driving consolidation, and&amp;nbsp;what’s&amp;nbsp;real versus hype in the agent space.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2259183614.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30922151"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Enterprise AI is shifting fast from chatbots that answer questions to systems that actually do the work across an organization. But who will own the AI layer that powers all of it?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Glean, which started as an enterprise search product, has evolved into what it calls an “AI work assistant,” aiming to&amp;nbsp;sit underneath other AI experiences, connecting to internal systems, managing permissions, and delivering intelligence wherever employees work.&amp;nbsp;Investors are buying into&amp;nbsp;the vision, too — the startup raised&amp;nbsp;$150 million last year at a&amp;nbsp;$7.2 billion&amp;nbsp;valuation&amp;nbsp;as&amp;nbsp;more competition&amp;nbsp;heats up&amp;nbsp;against tech giants bundling&amp;nbsp;AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Watch as Equity&amp;nbsp;host Rebecca Bellan sits down with Glean’s CEO and founder Arvind Jain at Web Summit Qatar to break down how enterprises are thinking about AI architecture, what’s driving consolidation, and&amp;nbsp;what’s&amp;nbsp;real versus hype in the agent space.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/who-will-own-your-companys-ai-layer-gleans-ceo-explains/</guid><pubDate>Wed, 11 Feb 2026 20:49:07 +0000</pubDate></item><item><title>[NEW] Glean’s fight to own the AI layer inside every company (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/glean-arvind-jain-equity-podcast-own-the-ai-layer-inside-every-company/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Arvind-Jain-headshot.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Enterprise AI is shifting fast from chatbots that answer questions to systems that actually do the work across an organization. But who will own the AI layer that powers all of it?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Glean, which started as an enterprise search product, has evolved into what it calls an “AI work assistant,” aiming to&amp;nbsp;sit beneath other AI experiences, connecting to internal systems, managing permissions, and delivering intelligence wherever employees work.&amp;nbsp;Investors are buying into&amp;nbsp;the vision, too — last June, the startup raised&amp;nbsp;$150 million at a&amp;nbsp;$7.2 billion&amp;nbsp;valuation&amp;nbsp;as&amp;nbsp;competition&amp;nbsp;heats up&amp;nbsp;against tech giants bundling&amp;nbsp;AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan sits down with Glean founder and CEO Arvind Jain at Web Summit Qatar to break down how enterprises are thinking about AI architecture, what’s driving consolidation, and&amp;nbsp;what’s&amp;nbsp;real versus hype in the AI agent space.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The fight between bundled AI from tech titans like Microsoft and&amp;nbsp;Google,&amp;nbsp;and platform layers&amp;nbsp;like&amp;nbsp;Glean and its competitors.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How AI adoption is reshaping leadership and organizational design.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why permissions and governance are harder problems than most companies realize.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube, Apple Podcasts, Overcast, Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X&amp;nbsp;and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Arvind-Jain-headshot.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Enterprise AI is shifting fast from chatbots that answer questions to systems that actually do the work across an organization. But who will own the AI layer that powers all of it?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Glean, which started as an enterprise search product, has evolved into what it calls an “AI work assistant,” aiming to&amp;nbsp;sit beneath other AI experiences, connecting to internal systems, managing permissions, and delivering intelligence wherever employees work.&amp;nbsp;Investors are buying into&amp;nbsp;the vision, too — last June, the startup raised&amp;nbsp;$150 million at a&amp;nbsp;$7.2 billion&amp;nbsp;valuation&amp;nbsp;as&amp;nbsp;competition&amp;nbsp;heats up&amp;nbsp;against tech giants bundling&amp;nbsp;AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan sits down with Glean founder and CEO Arvind Jain at Web Summit Qatar to break down how enterprises are thinking about AI architecture, what’s driving consolidation, and&amp;nbsp;what’s&amp;nbsp;real versus hype in the AI agent space.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The fight between bundled AI from tech titans like Microsoft and&amp;nbsp;Google,&amp;nbsp;and platform layers&amp;nbsp;like&amp;nbsp;Glean and its competitors.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How AI adoption is reshaping leadership and organizational design.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why permissions and governance are harder problems than most companies realize.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube, Apple Podcasts, Overcast, Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X&amp;nbsp;and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/glean-arvind-jain-equity-podcast-own-the-ai-layer-inside-every-company/</guid><pubDate>Wed, 11 Feb 2026 21:07:28 +0000</pubDate></item><item><title>[NEW] Uber Eats launches AI assistant to help with grocery cart creation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/uber-eats-launches-ai-assistant-to-help-with-grocery-cart-creation/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Uber Eats announced a new AI feature, “Cart Assistant,” on Wednesday designed to fill customers’ grocery carts faster and easier. The beta version is now available in the app.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the new chatbot, users search for a grocery store in the Uber Eats app and tap the purple Cart Assistant icon on the store’s page to begin shopping.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Customers can enter a list or upload an image of one, and Cart Assistant will automatically add the necessary items to their basket. This includes photos of handwritten lists or screenshots of recipes and their ingredients. Users can then customize the basket by swapping items for preferred brands or adding more products from the store.​&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3092211" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/UberEats-cart-assistant.gif?w=346" width="346" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Uber Eats&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Uber Eats notes that Cart Assistant uses previous orders to prioritize familiar items — like your usual milk or favorite oatmeal — to make the experience more personalized.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Users were telling us they wanted a quicker way to shop, and we know how precious your time is,” Uber CTO Praveen Neppalli Naga said in a statement. “Cart Assistant helps you get from idea to checkout in seconds.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cart Assistant could help Uber Eats better compete with other food delivery and grocery apps that are already integrating or developing AI chatbots.​ For example, Instacart launched an AI search tool powered by OpenAI’s ChatGPT in 2023 to help customers save time and receive personalized shopping recommendations. DoorDash was also reportedly testing an AI chatbot called DashAI that same year.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, both Uber Eats and rival DoorDash integrated with ChatGPT to streamline food ordering. With Uber Eats, U.S. users can browse local restaurants and menus in ChatGPT, then complete their purchase in the Uber Eats app. DoorDash’s integration allows users to request meal plans and automatically add all necessary ingredients to their DoorDash cart.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Bloomberg reported in 2023 that Uber Eats was developing an AI-powered chatbot that would supposedly ask users about their budget and food preferences, and help them place an order.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Uber Eats is rapidly investing in AI, including tools for merchants like AI-generated menu descriptions, enhanced food photos, and customer review summaries.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Uber Eats announced a new AI feature, “Cart Assistant,” on Wednesday designed to fill customers’ grocery carts faster and easier. The beta version is now available in the app.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the new chatbot, users search for a grocery store in the Uber Eats app and tap the purple Cart Assistant icon on the store’s page to begin shopping.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Customers can enter a list or upload an image of one, and Cart Assistant will automatically add the necessary items to their basket. This includes photos of handwritten lists or screenshots of recipes and their ingredients. Users can then customize the basket by swapping items for preferred brands or adding more products from the store.​&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3092211" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/UberEats-cart-assistant.gif?w=346" width="346" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Uber Eats&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Uber Eats notes that Cart Assistant uses previous orders to prioritize familiar items — like your usual milk or favorite oatmeal — to make the experience more personalized.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Users were telling us they wanted a quicker way to shop, and we know how precious your time is,” Uber CTO Praveen Neppalli Naga said in a statement. “Cart Assistant helps you get from idea to checkout in seconds.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cart Assistant could help Uber Eats better compete with other food delivery and grocery apps that are already integrating or developing AI chatbots.​ For example, Instacart launched an AI search tool powered by OpenAI’s ChatGPT in 2023 to help customers save time and receive personalized shopping recommendations. DoorDash was also reportedly testing an AI chatbot called DashAI that same year.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, both Uber Eats and rival DoorDash integrated with ChatGPT to streamline food ordering. With Uber Eats, U.S. users can browse local restaurants and menus in ChatGPT, then complete their purchase in the Uber Eats app. DoorDash’s integration allows users to request meal plans and automatically add all necessary ingredients to their DoorDash cart.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Bloomberg reported in 2023 that Uber Eats was developing an AI-powered chatbot that would supposedly ask users about their budget and food preferences, and help them place an order.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Uber Eats is rapidly investing in AI, including tools for merchants like AI-generated menu descriptions, enhanced food photos, and customer review summaries.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/uber-eats-launches-ai-assistant-to-help-with-grocery-cart-creation/</guid><pubDate>Wed, 11 Feb 2026 21:07:37 +0000</pubDate></item><item><title>[NEW] Apple’s Siri revamp reportedly delayed… again (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/apples-siri-revamp-reportedly-delayed-again/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/tim-cook-glowing-apple-logo-GettyImages-2234517515.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple has been promising a new-and-improved, cutting-edge, AI-powered Siri since it first unveiled Apple Intelligence in 2024. Over about a year and a half since then, the release date for this new era of Siri has been continuously pushed back. According to a new report from Bloomberg’s Mark Gurman, we’ll likely have to wait even longer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the new Siri was expected to launch with the upcoming iOS 26.4 update in March, now, the changes are expected to roll out more slowly over time, reportedly postponing some features until the May iOS update, or even until the release of iOS 27 in September. Apparently, Apple ran into trouble when testing the software, requiring the launch date to be pushed back further.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The changes are rumored to make the longtime digital assistant more like the LLM chatbots that have swept the tech world — but instead of opening up a ChatGPT or Claude app on your iPhone or MacBook, you would be able to just talk to Siri, which will be powered by Google Gemini.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We’re starting to feel bad for the Siri product managers. Hang in there, folks.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/tim-cook-glowing-apple-logo-GettyImages-2234517515.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple has been promising a new-and-improved, cutting-edge, AI-powered Siri since it first unveiled Apple Intelligence in 2024. Over about a year and a half since then, the release date for this new era of Siri has been continuously pushed back. According to a new report from Bloomberg’s Mark Gurman, we’ll likely have to wait even longer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the new Siri was expected to launch with the upcoming iOS 26.4 update in March, now, the changes are expected to roll out more slowly over time, reportedly postponing some features until the May iOS update, or even until the release of iOS 27 in September. Apparently, Apple ran into trouble when testing the software, requiring the launch date to be pushed back further.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The changes are rumored to make the longtime digital assistant more like the LLM chatbots that have swept the tech world — but instead of opening up a ChatGPT or Claude app on your iPhone or MacBook, you would be able to just talk to Siri, which will be powered by Google Gemini.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We’re starting to feel bad for the Siri product managers. Hang in there, folks.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/apples-siri-revamp-reportedly-delayed-again/</guid><pubDate>Wed, 11 Feb 2026 21:19:46 +0000</pubDate></item><item><title>[NEW] OpenAI disbands mission alignment team (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/openai-disbands-mission-alignment-team-which-focused-on-safe-and-trustworthy-ai-development/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2198379368.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI has disbanded a team that was designed to communicate the company’s mission to the public and to its own employees. At the same time, the team’s former leader has been given a new role as the company’s “chief futurist.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI confirmed to TechCrunch that the team’s members have now been assigned to other roles. The news was first reported by Platformer.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The disbanded team in question appears to have been formed in September of 2024. Platformer reports that the team was dedicated to promoting “the company’s stated mission to ensure that artificial general intelligence benefits all of humanity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An official OpenAI spokesperson described the team thusly: “The Mission Alignment project was a support function to help employees and the public understand our mission and the impact of AI. That work continues throughout the organization.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post published Wednesday, Josh Achiam, the former head of OpenAI’s mission alignment team, explained his new role as the company’s chief futurist. “My goal is to support OpenAI’s mission — to ensure that artificial general intelligence benefits all of humanity — by studying how the world will change in response to AI, AGI, and beyond,” Achiam wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achiam noted that, in his new role, he would be collaborating with Jason Pruet, an OpenAI physicist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for OpenAI said the rest of the mission alignment team — a group of six or seven people — had subsequently been reassigned to different parts of the company. The spokesperson couldn’t say where exactly the team members had been assigned, but said that they were engaged in similar work in those roles. It was also unclear whether Achiam would have a new team as part of his “futurist” role.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The spokesperson attributed the disbanding of the team to the kinds of routine reorganizations that occur within a fast-moving company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI previously had what it called a “superalignment team” — which was formed in 2023 and focused on studying long-term existential threats posed by AI — but that team was disbanded in 2024. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achiam’s personal website still lists him as head of mission alignment at OpenAI, and describes him as being interested in ensuring that the “long-term future of humanity is good.” His LinkedIn profile shows he had served as head of mission alignment since September 2024.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: This story originally confused mission alignment with another, similarly named team called alignment. It has been corrected and we apologize for the error.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2198379368.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI has disbanded a team that was designed to communicate the company’s mission to the public and to its own employees. At the same time, the team’s former leader has been given a new role as the company’s “chief futurist.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI confirmed to TechCrunch that the team’s members have now been assigned to other roles. The news was first reported by Platformer.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The disbanded team in question appears to have been formed in September of 2024. Platformer reports that the team was dedicated to promoting “the company’s stated mission to ensure that artificial general intelligence benefits all of humanity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An official OpenAI spokesperson described the team thusly: “The Mission Alignment project was a support function to help employees and the public understand our mission and the impact of AI. That work continues throughout the organization.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post published Wednesday, Josh Achiam, the former head of OpenAI’s mission alignment team, explained his new role as the company’s chief futurist. “My goal is to support OpenAI’s mission — to ensure that artificial general intelligence benefits all of humanity — by studying how the world will change in response to AI, AGI, and beyond,” Achiam wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achiam noted that, in his new role, he would be collaborating with Jason Pruet, an OpenAI physicist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for OpenAI said the rest of the mission alignment team — a group of six or seven people — had subsequently been reassigned to different parts of the company. The spokesperson couldn’t say where exactly the team members had been assigned, but said that they were engaged in similar work in those roles. It was also unclear whether Achiam would have a new team as part of his “futurist” role.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The spokesperson attributed the disbanding of the team to the kinds of routine reorganizations that occur within a fast-moving company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI previously had what it called a “superalignment team” — which was formed in 2023 and focused on studying long-term existential threats posed by AI — but that team was disbanded in 2024. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achiam’s personal website still lists him as head of mission alignment at OpenAI, and describes him as being interested in ensuring that the “long-term future of humanity is good.” His LinkedIn profile shows he had served as head of mission alignment since September 2024.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: This story originally confused mission alignment with another, similarly named team called alignment. It has been corrected and we apologize for the error.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/openai-disbands-mission-alignment-team-which-focused-on-safe-and-trustworthy-ai-development/</guid><pubDate>Wed, 11 Feb 2026 21:57:18 +0000</pubDate></item><item><title>[NEW] AI inference startup Modal Labs in talks to raise at $2.5B valuation, sources say (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/ai-inference-startup-modal-labs-in-talks-to-raise-at-2-5b-valuation-sources-say/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2024-02-14-at-9.26.16AM.png?resize=1200,607" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Modal Labs, a startup specializing in AI inference infrastructure, is talking to VCs about a new round at a valuation of about $2.5 billion, according to four people with knowledge of the deal. Should the deal close at these terms, the funding round would more than double the company’s valuation of $1.1 billion announced less than five months ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;General Catalyst is in talks to lead the round, the people told TechCrunch.&amp;nbsp;Modal’s annualized revenue run rate (ARR) is approximately $50 million, our sources said. The discussions are early, and terms could still change. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Modal Labs co-founder and CEO Erik Bernhardsson denied that his company was actively fundraising and characterized his recent interactions with VCs as general conversations. General Catalyst did not respond to our requests for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal is focused on optimizing inference, the process of running trained AI models to generate answers from user requests. Improving inference efficiency reduces compute costs and cuts down the lag time between a user’s prompt and the AI’s response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal is one of the handful of inference-focused companies attracting intense investor attention now. Last week, its competitor Baseten announced a $300 million raise at a $5 billion valuation, more than doubling the $2.1 billion valuation it reached just months prior in September. Similarly, Fireworks AI, an inference cloud provider, secured $250 million at a $4 billion valuation in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In January, the creators of the open source inference project vLLM announced they had transitioned the tool into a VC-backed startup, Inferact, raising $150 million in seed funding led by Andreessen Horowitz at an $800 million valuation. Meanwhile, TechCrunch reported that the team behind SGLang has commercialized as RadixArk, which sources told us secured seed funding at a $400 million valuation led by Accel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal was co-founded by CEO Erik Bernhardsson in 2021 after he spent more than 15 years building and leading data teams at companies including Spotify and Better.com, where he was CTO.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The startup counts Lux Capital and Redpoint Ventures among its earlier backers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Editor’s Note: This story was updated to include a comment from Modal.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2024-02-14-at-9.26.16AM.png?resize=1200,607" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Modal Labs, a startup specializing in AI inference infrastructure, is talking to VCs about a new round at a valuation of about $2.5 billion, according to four people with knowledge of the deal. Should the deal close at these terms, the funding round would more than double the company’s valuation of $1.1 billion announced less than five months ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;General Catalyst is in talks to lead the round, the people told TechCrunch.&amp;nbsp;Modal’s annualized revenue run rate (ARR) is approximately $50 million, our sources said. The discussions are early, and terms could still change. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Modal Labs co-founder and CEO Erik Bernhardsson denied that his company was actively fundraising and characterized his recent interactions with VCs as general conversations. General Catalyst did not respond to our requests for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal is focused on optimizing inference, the process of running trained AI models to generate answers from user requests. Improving inference efficiency reduces compute costs and cuts down the lag time between a user’s prompt and the AI’s response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal is one of the handful of inference-focused companies attracting intense investor attention now. Last week, its competitor Baseten announced a $300 million raise at a $5 billion valuation, more than doubling the $2.1 billion valuation it reached just months prior in September. Similarly, Fireworks AI, an inference cloud provider, secured $250 million at a $4 billion valuation in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In January, the creators of the open source inference project vLLM announced they had transitioned the tool into a VC-backed startup, Inferact, raising $150 million in seed funding led by Andreessen Horowitz at an $800 million valuation. Meanwhile, TechCrunch reported that the team behind SGLang has commercialized as RadixArk, which sources told us secured seed funding at a $400 million valuation led by Accel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal was co-founded by CEO Erik Bernhardsson in 2021 after he spent more than 15 years building and leading data teams at companies including Spotify and Better.com, where he was CTO.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The startup counts Lux Capital and Redpoint Ventures among its earlier backers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Editor’s Note: This story was updated to include a comment from Modal.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/ai-inference-startup-modal-labs-in-talks-to-raise-at-2-5b-valuation-sources-say/</guid><pubDate>Wed, 11 Feb 2026 22:48:35 +0000</pubDate></item><item><title>[NEW] xAI lays out interplanetary ambitions in public all-hands (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/xai-lays-out-interplanetary-ambitions-in-public-all-hands/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, xAI took the rare step of publishing a full 45-minute all-hands meeting video on X, making it publicly accessible. Details of the Tuesday night meeting were previously reported by The New York Times, which may have influenced xAI’s decision to post the video online.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The full video reveals significant new details about Musk’s plans for the AI lab, including its product roadmap and its ongoing ties to the X platform.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The most immediate revelation concerned a string of departing employees, which Musk described as layoffs resulting from a changing organizational structure at the company. While reorganizations are common, the breadth of the departures has caused significant confusion, particularly as it has meant the loss of a significant portion of the founding team.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As a company grows, especially as quickly as xAI, the structure must evolve,” Musk said on X. “This unfortunately required parting ways with some people. We wish them well in future endeavors.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new organizational system splits xAI into four primary teams: one focused on the Grok chatbot (including voice), another for the app’s coding system, another for the Imagine video generator, and finally a team focused on the Macrohard project, which spans from simple computer use simulation to modeling entire corporations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[Macrohard] is able to do anything on a computer that a computer is able to do,” Toby Pohlen, who will lead the project under the new organizational structure, told his colleagues. “There should be rocket engines fully designed by AI.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3092311" height="444" src="https://techcrunch.com/wp-content/uploads/2026/02/Screen-Shot-2026-02-11-at-3.21.27-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;xAI (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The all-hands also featured claims about new usage and revenue figures for xAI and X. Nikita Bier, X’s head of product, said X had “just crossed” $1 billion in annual recurring revenue from subscriptions, which he attributed to a marketing push during the holidays.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, executives said the xAI’s Imagine tool is generating 50 million videos a day, and more than 6 billion images over the past 30 days, according to their internal metrics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s difficult to separate those figures from the flood of deepfake pornography that overtook X during that same period. The X platform saw engagement skyrocket as AI-generated explicit images became more prevalent, and with an estimated 1.8 million sexualized images generated over just nine days, the image-generation figures likely include substantial amounts of this controversial content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most eye-catching part of the presentation came at the end, when Musk reemphasized the importance of space-based data centers despite the technical challenges involved. Musk went still further, envisioning a moon-based factory for AI satellites, including a lunar mass driver — essentially an electromagnetic catapult — to launch them. With such infrastructure, Musk said, one could launch an AI cluster capable of capturing significant portions of the sun’s total energy output or even expanding to other galaxies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s difficult to imagine what an intelligence of that scale would think about,” Musk said, “but it’s going to be incredibly exciting to see it happen.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, xAI took the rare step of publishing a full 45-minute all-hands meeting video on X, making it publicly accessible. Details of the Tuesday night meeting were previously reported by The New York Times, which may have influenced xAI’s decision to post the video online.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The full video reveals significant new details about Musk’s plans for the AI lab, including its product roadmap and its ongoing ties to the X platform.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The most immediate revelation concerned a string of departing employees, which Musk described as layoffs resulting from a changing organizational structure at the company. While reorganizations are common, the breadth of the departures has caused significant confusion, particularly as it has meant the loss of a significant portion of the founding team.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As a company grows, especially as quickly as xAI, the structure must evolve,” Musk said on X. “This unfortunately required parting ways with some people. We wish them well in future endeavors.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new organizational system splits xAI into four primary teams: one focused on the Grok chatbot (including voice), another for the app’s coding system, another for the Imagine video generator, and finally a team focused on the Macrohard project, which spans from simple computer use simulation to modeling entire corporations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[Macrohard] is able to do anything on a computer that a computer is able to do,” Toby Pohlen, who will lead the project under the new organizational structure, told his colleagues. “There should be rocket engines fully designed by AI.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3092311" height="444" src="https://techcrunch.com/wp-content/uploads/2026/02/Screen-Shot-2026-02-11-at-3.21.27-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;xAI (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The all-hands also featured claims about new usage and revenue figures for xAI and X. Nikita Bier, X’s head of product, said X had “just crossed” $1 billion in annual recurring revenue from subscriptions, which he attributed to a marketing push during the holidays.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, executives said the xAI’s Imagine tool is generating 50 million videos a day, and more than 6 billion images over the past 30 days, according to their internal metrics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s difficult to separate those figures from the flood of deepfake pornography that overtook X during that same period. The X platform saw engagement skyrocket as AI-generated explicit images became more prevalent, and with an estimated 1.8 million sexualized images generated over just nine days, the image-generation figures likely include substantial amounts of this controversial content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most eye-catching part of the presentation came at the end, when Musk reemphasized the importance of space-based data centers despite the technical challenges involved. Musk went still further, envisioning a moon-based factory for AI satellites, including a lunar mass driver — essentially an electromagnetic catapult — to launch them. With such infrastructure, Musk said, one could launch an AI cluster capable of capturing significant portions of the sun’s total energy output or even expanding to other galaxies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s difficult to imagine what an intelligence of that scale would think about,” Musk said, “but it’s going to be incredibly exciting to see it happen.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/xai-lays-out-interplanetary-ambitions-in-public-all-hands/</guid><pubDate>Wed, 11 Feb 2026 23:29:08 +0000</pubDate></item></channel></rss>