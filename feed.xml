<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 10 Nov 2025 18:32:10 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>10% of Nvidia’s cost: Why Tesla-Intel chip partnership demands attention (AI News)</title><link>https://www.artificialintelligence-news.com/news/tesla-intel-chip-partnership-nvidia-cost/</link><description>&lt;p&gt;The potential Tesla-Intel chip partnership could deliver AI chips at just 10% of Nvidia’s cost – a claim that represents a significant development in AI infrastructure that enterprise technology leaders cannot afford to ignore.&lt;/p&gt;&lt;p&gt;On November 6, 2025, Tesla CEO Elon Musk stated publicly at the company’s annual shareholder meeting that the electric vehicle manufacturer is considering working with Intel to produce its fifth-generation AI chips, signalling a major strategic shift in how AI computing hardware might be manufactured and distributed.&lt;/p&gt;&lt;p&gt;“You know, maybe we’ll, we’ll do something with Intel,” Musk told shareholders, according to a &lt;em&gt;Reuters&lt;/em&gt; report. “We haven’t signed any deal, but it’s probably worth having discussions with Intel.” The statement sent Intel shares up 4% in after-hours trading, underscoring how seriously the market views the potential collaboration.&lt;/p&gt;&lt;h3&gt;The strategic context behind the partnership&lt;/h3&gt;&lt;p&gt;Tesla’s consideration of Intel as a manufacturing partner comes at a important juncture for both companies. Tesla is designing its AI5 chip to power its autonomous driving systems.&lt;/p&gt;&lt;p&gt;Currently on its fourth-generation chip, Tesla has identified a significant supply constraint that traditional partnerships with Taiwan’s TSMC and South Korea’s Samsung cannot address fully.&lt;/p&gt;&lt;p&gt;“Even when we extrapolate the best-case scenario for chip production from our suppliers, it’s still not enough,” Musk said during the shareholder meeting. The supply gap has led Tesla to consider building what Musk calls a “terafab” – a massive chip fabrication facility capable of producing at least 100,000 wafer starts per month.&lt;/p&gt;&lt;p&gt;For Intel, the potential partnership offers an important opportunity. The US chipmaker has lagged significantly behind Nvidia in the AI chip race and desperately needs external customers for its newest manufacturing technology.&lt;/p&gt;&lt;p&gt;The US government recently took a 10% stake in Intel, underscoring the strategic importance of maintaining domestic chip manufacturing capabilities.&lt;/p&gt;&lt;h3&gt;Cost and performance implications&lt;/h3&gt;&lt;p&gt;At 10% of Nvidia’s manufacturing cost, the technical specifications Musk outlined during the shareholder meeting could reshape enterprise AI economics. According to Musk, Tesla’s AI5 chip would consume approximately one-third of the power used by Nvidia’s flagship Blackwell chip, and cost just 10% as much to manufacture.&lt;/p&gt;&lt;p&gt;“I’m super hardcore on chips right now, as you may be able to tell,” Musk said. “I have chips on the brain.”&lt;/p&gt;&lt;p&gt;The cost and efficiency projections, if realised, could alter the economics of AI deployment. Enterprise leaders investing heavily in AI infrastructure should monitor whether these performance targets materialise, as they could influence future technology purchasing decisions in the industry.&lt;/p&gt;&lt;p&gt;The chip would be inexpensive, power-efficient, and optimised for Tesla’s own software, Musk said.&lt;/p&gt;&lt;h3&gt;Production timeline and scale&lt;/h3&gt;&lt;p&gt;Tesla’s chip production roadmap provides a timeline for enterprise planning. A small number of AI5 units would be produced in 2026, with high-volume production possible in 2027. Musk indicated in a post on social media that AI6 will use the same fabrication facilities but achieve roughly twice the performance, with volume production targeted for mid-2028.&lt;/p&gt;&lt;p&gt;The scale of Tesla’s ambitions is substantial. The proposed “terafab” would represent an expansion of domestic chip manufacturing capacity, potentially reducing supply chain vulnerabilities that have plagued the technology industry in recent years.&lt;/p&gt;&lt;p&gt;“So I think we may have to do a Tesla terafab. It’s like a giga but way bigger. I can’t see any other way to get to the volume of chips that we’re looking for. So I think we’re probably going to have to build a gigantic chip fab. It’s got to be done,” Musk said.&lt;/p&gt;&lt;h3&gt;What this means for enterprise decision-makers&lt;/h3&gt;&lt;p&gt;Several strategic considerations emerge from any potential Tesla-Intel chip partnership:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Supply chain resilience: &lt;/strong&gt;The move toward domestic chip manufacturing addresses concerns about supply chain concentration in Asia. Enterprise leaders managing technology risk should consider how shifts in chip manufacturing geography might affect their supply chains and vendor relationships.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Cost structure changes: &lt;/strong&gt;If Tesla achieves its stated cost targets, the competitive landscape for AI chips could shift. Organisations should prepare contingency plans for potential price pressure on current suppliers and evaluate whether alternative chip architectures are viable.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Technology sovereignty: &lt;/strong&gt;The US government’s stake in Intel and support for domestic chip manufacturing reflect broader geopolitical considerations. Enterprise leaders in regulated industries or those handling sensitive data should assess how the trends might affect their technology sources.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Innovation pace:&lt;/strong&gt; Tesla’s aggressive timeline for multiple chip generations suggests an accelerating pace of AI hardware innovation. Technology leaders should factor this into refresh cycles and architecture decisions, avoiding premature commitment to current-generation technology.&lt;/p&gt;&lt;h3&gt;The broader industry context&lt;/h3&gt;&lt;p&gt;Musk’s statements occur against the backdrop of US-China technology competition. Export restrictions have impacted Nvidia’s business in China, where its market share has reportedly dropped from 95% to near zero.&lt;/p&gt;&lt;p&gt;Intel declined to comment on Musk’s remarks, and no formal agreement has been announced. However, the public nature of the statements, and the market’s reaction, suggest substantive discussions may soon be underway.&lt;/p&gt;&lt;p&gt;The AI chip landscape is entering a period of flux. Organisations should maintain flexibility in their infrastructure strategy and monitor how partnerships like Tesla-Intel might reshape the competitive dynamics of AI hardware manufacturing.&lt;/p&gt;&lt;p&gt;The decisions made today about chip manufacturing partnerships could determine which organisations have access to cost-effective, high-performance AI infrastructure in the coming years.&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. This comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;The potential Tesla-Intel chip partnership could deliver AI chips at just 10% of Nvidia’s cost – a claim that represents a significant development in AI infrastructure that enterprise technology leaders cannot afford to ignore.&lt;/p&gt;&lt;p&gt;On November 6, 2025, Tesla CEO Elon Musk stated publicly at the company’s annual shareholder meeting that the electric vehicle manufacturer is considering working with Intel to produce its fifth-generation AI chips, signalling a major strategic shift in how AI computing hardware might be manufactured and distributed.&lt;/p&gt;&lt;p&gt;“You know, maybe we’ll, we’ll do something with Intel,” Musk told shareholders, according to a &lt;em&gt;Reuters&lt;/em&gt; report. “We haven’t signed any deal, but it’s probably worth having discussions with Intel.” The statement sent Intel shares up 4% in after-hours trading, underscoring how seriously the market views the potential collaboration.&lt;/p&gt;&lt;h3&gt;The strategic context behind the partnership&lt;/h3&gt;&lt;p&gt;Tesla’s consideration of Intel as a manufacturing partner comes at a important juncture for both companies. Tesla is designing its AI5 chip to power its autonomous driving systems.&lt;/p&gt;&lt;p&gt;Currently on its fourth-generation chip, Tesla has identified a significant supply constraint that traditional partnerships with Taiwan’s TSMC and South Korea’s Samsung cannot address fully.&lt;/p&gt;&lt;p&gt;“Even when we extrapolate the best-case scenario for chip production from our suppliers, it’s still not enough,” Musk said during the shareholder meeting. The supply gap has led Tesla to consider building what Musk calls a “terafab” – a massive chip fabrication facility capable of producing at least 100,000 wafer starts per month.&lt;/p&gt;&lt;p&gt;For Intel, the potential partnership offers an important opportunity. The US chipmaker has lagged significantly behind Nvidia in the AI chip race and desperately needs external customers for its newest manufacturing technology.&lt;/p&gt;&lt;p&gt;The US government recently took a 10% stake in Intel, underscoring the strategic importance of maintaining domestic chip manufacturing capabilities.&lt;/p&gt;&lt;h3&gt;Cost and performance implications&lt;/h3&gt;&lt;p&gt;At 10% of Nvidia’s manufacturing cost, the technical specifications Musk outlined during the shareholder meeting could reshape enterprise AI economics. According to Musk, Tesla’s AI5 chip would consume approximately one-third of the power used by Nvidia’s flagship Blackwell chip, and cost just 10% as much to manufacture.&lt;/p&gt;&lt;p&gt;“I’m super hardcore on chips right now, as you may be able to tell,” Musk said. “I have chips on the brain.”&lt;/p&gt;&lt;p&gt;The cost and efficiency projections, if realised, could alter the economics of AI deployment. Enterprise leaders investing heavily in AI infrastructure should monitor whether these performance targets materialise, as they could influence future technology purchasing decisions in the industry.&lt;/p&gt;&lt;p&gt;The chip would be inexpensive, power-efficient, and optimised for Tesla’s own software, Musk said.&lt;/p&gt;&lt;h3&gt;Production timeline and scale&lt;/h3&gt;&lt;p&gt;Tesla’s chip production roadmap provides a timeline for enterprise planning. A small number of AI5 units would be produced in 2026, with high-volume production possible in 2027. Musk indicated in a post on social media that AI6 will use the same fabrication facilities but achieve roughly twice the performance, with volume production targeted for mid-2028.&lt;/p&gt;&lt;p&gt;The scale of Tesla’s ambitions is substantial. The proposed “terafab” would represent an expansion of domestic chip manufacturing capacity, potentially reducing supply chain vulnerabilities that have plagued the technology industry in recent years.&lt;/p&gt;&lt;p&gt;“So I think we may have to do a Tesla terafab. It’s like a giga but way bigger. I can’t see any other way to get to the volume of chips that we’re looking for. So I think we’re probably going to have to build a gigantic chip fab. It’s got to be done,” Musk said.&lt;/p&gt;&lt;h3&gt;What this means for enterprise decision-makers&lt;/h3&gt;&lt;p&gt;Several strategic considerations emerge from any potential Tesla-Intel chip partnership:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Supply chain resilience: &lt;/strong&gt;The move toward domestic chip manufacturing addresses concerns about supply chain concentration in Asia. Enterprise leaders managing technology risk should consider how shifts in chip manufacturing geography might affect their supply chains and vendor relationships.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Cost structure changes: &lt;/strong&gt;If Tesla achieves its stated cost targets, the competitive landscape for AI chips could shift. Organisations should prepare contingency plans for potential price pressure on current suppliers and evaluate whether alternative chip architectures are viable.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Technology sovereignty: &lt;/strong&gt;The US government’s stake in Intel and support for domestic chip manufacturing reflect broader geopolitical considerations. Enterprise leaders in regulated industries or those handling sensitive data should assess how the trends might affect their technology sources.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Innovation pace:&lt;/strong&gt; Tesla’s aggressive timeline for multiple chip generations suggests an accelerating pace of AI hardware innovation. Technology leaders should factor this into refresh cycles and architecture decisions, avoiding premature commitment to current-generation technology.&lt;/p&gt;&lt;h3&gt;The broader industry context&lt;/h3&gt;&lt;p&gt;Musk’s statements occur against the backdrop of US-China technology competition. Export restrictions have impacted Nvidia’s business in China, where its market share has reportedly dropped from 95% to near zero.&lt;/p&gt;&lt;p&gt;Intel declined to comment on Musk’s remarks, and no formal agreement has been announced. However, the public nature of the statements, and the market’s reaction, suggest substantive discussions may soon be underway.&lt;/p&gt;&lt;p&gt;The AI chip landscape is entering a period of flux. Organisations should maintain flexibility in their infrastructure strategy and monitor how partnerships like Tesla-Intel might reshape the competitive dynamics of AI hardware manufacturing.&lt;/p&gt;&lt;p&gt;The decisions made today about chip manufacturing partnerships could determine which organisations have access to cost-effective, high-performance AI infrastructure in the coming years.&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. This comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/tesla-intel-chip-partnership-nvidia-cost/</guid><pubDate>Mon, 10 Nov 2025 09:13:18 +0000</pubDate></item><item><title>[NEW] 6sense founder Amanda Kahlow raises $30 million for new human-replacement AI sales startup 1mind (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/6sense-founder-amanda-kahlow-raises-30-million-for-new-human-replacement-ai-sales-startup-1mind/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Although LLM-powered AI agents are a&amp;nbsp;fairly new&amp;nbsp;phenomenon,&amp;nbsp;one&amp;nbsp;of the areas where they’ve been most popular&amp;nbsp;so far&amp;nbsp;is&amp;nbsp;in sales.&amp;nbsp;1mind, a&amp;nbsp;startup co-founded by&amp;nbsp;Amanda&amp;nbsp;Kahlow,&amp;nbsp;has been quietly&amp;nbsp;shipping its sales agent, named Mindy, for about a year.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, the startup&amp;nbsp;announced a $30 million&amp;nbsp;Series A round led by Battery Ventures.&amp;nbsp;This brings 1mind’s total raise to $40 million, the company says.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Kahlow is well known in the sales and marketing tech world as the founder&amp;nbsp;and former CEO&amp;nbsp;of 6sense. It was&amp;nbsp;launched in 2013&amp;nbsp;as a lead-generation&amp;nbsp;tool that tracked signals&amp;nbsp;across social media and other sites to identify potential customers. She left&amp;nbsp;in&amp;nbsp;2020.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the sales agent market is already crowded, 1mind, and its agent, named&amp;nbsp;Mindy,&amp;nbsp;aren’t&amp;nbsp;doing&amp;nbsp;what most of them do: sending emails&amp;nbsp;and making cold&amp;nbsp;calls. That’s a crowded market where&amp;nbsp;even her&amp;nbsp;previous&amp;nbsp;firm, 6sense,&amp;nbsp;offers agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not&amp;nbsp;playing in&amp;nbsp;outbound,”&amp;nbsp;Kahlow tells TechCrunch.&amp;nbsp;Mindy is intended to handle inbound sales, going all the way to “closing the deal,”&amp;nbsp;Kahlow&amp;nbsp;says.&amp;nbsp;This agent is used to&amp;nbsp;augment&amp;nbsp;self-service&amp;nbsp;websites and, Kahlow says, to&amp;nbsp;replace the&amp;nbsp;sales&amp;nbsp;engineer on calls&amp;nbsp;for larger enterprise deals. It can also be&amp;nbsp;the&amp;nbsp;onboarding&amp;nbsp;specialist, setting up new customers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our goal is to truly replicate the human experience across go-to-market when you have intent, when buyers are leaning in: they come to your website, or&amp;nbsp;they’re&amp;nbsp;in a zoom call. She can ride along and be the sales engineer,” Kahlow adds, referring to Mindy with the anthropomorphic pronoun.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kahlow&amp;nbsp;even goes&amp;nbsp;so far as&amp;nbsp;to call her&amp;nbsp;AI agents “superhumans,” though they are neither human nor do they&amp;nbsp;possess&amp;nbsp;comic-book-style&amp;nbsp;superhuman powers.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Each agent&amp;nbsp;can, however,&amp;nbsp;be trained to&amp;nbsp;understand an&amp;nbsp;expansive&amp;nbsp;knowledge base that encompasses&amp;nbsp;all of&amp;nbsp;a company’s products, technical details,&amp;nbsp;competitive&amp;nbsp;positions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the startup uses a mix of underlying large-language models including OpenAI andG oogle Gemini, the agent limits hallucinations by using deterministic AI, both Kahlow and Mindy said (I invited the agent to our call). Deterministic AI provides guardrails so once the agent ingests corporate sales materials, it should recite that info without deviation. Beyond that, Mindy is trained to say she doesn’t know an answer&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With a year of operations under its belt,&amp;nbsp;1mind is&amp;nbsp;being used by more than 30 companies, including HubSpot, LinkedIn, and New Relic, to&amp;nbsp;pitch and&amp;nbsp;close deals.&amp;nbsp;Kahlow says that&amp;nbsp;her company’s&amp;nbsp;entire roster of&amp;nbsp;named&amp;nbsp;customers all have annual contracts, not “experimental” budgets, and the “average contract is six figures.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="1Mind AI agent Mindy" class="wp-image-3066197" height="382" src="https://techcrunch.com/wp-content/uploads/2025/11/Mindy-transparent.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;1Mind AI agent Mindy&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;1mind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company&amp;nbsp;also uses the bot internally&amp;nbsp;on its&amp;nbsp;own&amp;nbsp;sales&amp;nbsp;calls.&amp;nbsp;But&amp;nbsp;Kahlow&amp;nbsp;has gone&amp;nbsp;even&amp;nbsp;father. She created&amp;nbsp;an avatar of herself, Amanda, and took it with her to her VC pitches.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;During Battery Ventures’ due diligence, “we used it to go through the data room, asking lots of questions on things like case studies,” Battery partner Neeraj Agrawal told TechCrunch about the avatar Amanda. The data room refers to the stacks of data a startup will share about itself with a VC. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The conversation design is very nuanced, like what case studies it shares and when,” he said. The VC also determined that customers were having long back-and-forth conversations, indicating that they would forget they were talking to an AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That avatar&amp;nbsp;is still accessible via&amp;nbsp;Kahlow’s LinkedIn page,&amp;nbsp;where&amp;nbsp;the public can interact with it. It&amp;nbsp;can answer questions about 1mind’s products, but also other areas, like&amp;nbsp;Kahlow’s&amp;nbsp;views of being a woman in tech.&amp;nbsp;However, having chatted with it, it&amp;nbsp;will — much like the human Kahlow — always&amp;nbsp;attempt&amp;nbsp;to bend the conversation toward&amp;nbsp;1mind.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Eventually,&amp;nbsp;Kahlow&amp;nbsp;says,&amp;nbsp;1mind and other agentic sales startups will even replace&amp;nbsp;higher-end&amp;nbsp;account executive&amp;nbsp;sales roles.&amp;nbsp;Or at least drastically&amp;nbsp;reimagine&amp;nbsp;them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re not there yet where&amp;nbsp;we’re&amp;nbsp;fully replacing the AE.&amp;nbsp;We’re&amp;nbsp;replacing the website.&amp;nbsp;We’re&amp;nbsp;replacing the sales engineer, customer success, but that&amp;nbsp;[AE/customer]&amp;nbsp;relationship there is still happening,” she says.&amp;nbsp;&amp;nbsp;“I think over time, a lot of what the AE&amp;nbsp;is&amp;nbsp;doing right now will go away.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She&amp;nbsp;believes&amp;nbsp;this is&amp;nbsp;currently mostly a&amp;nbsp;trust issue.&amp;nbsp;Agentic tech is so new that a buyer looking to sign an enormous enterprise deal&amp;nbsp;isn’t&amp;nbsp;ready to do it&amp;nbsp;without a human.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Interestingly, once&amp;nbsp;trust develops, she thinks (and is already building for) agentic buyers.&amp;nbsp;Agent-to-agent transactions&amp;nbsp;won’t&amp;nbsp;involve human&amp;nbsp;avatars, but&amp;nbsp;will&amp;nbsp;instead&amp;nbsp;be&amp;nbsp;transfers of&amp;nbsp;information and requirements.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the meantime, 1mind still employs humans:&amp;nbsp;44 of them, including&amp;nbsp;in sales, with 71 job openings, including for&amp;nbsp;account execs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The round included participation from Primary Ventures, Wing Venture Capital, Operator Collective, Harmonic Growth Partners and Success Venture Partners and&amp;nbsp;angels&amp;nbsp;from Monday.com, ZoomInfo, Databricks, Box, Gong, Braze,&amp;nbsp;and&amp;nbsp;Verkada, 1mind said.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Although LLM-powered AI agents are a&amp;nbsp;fairly new&amp;nbsp;phenomenon,&amp;nbsp;one&amp;nbsp;of the areas where they’ve been most popular&amp;nbsp;so far&amp;nbsp;is&amp;nbsp;in sales.&amp;nbsp;1mind, a&amp;nbsp;startup co-founded by&amp;nbsp;Amanda&amp;nbsp;Kahlow,&amp;nbsp;has been quietly&amp;nbsp;shipping its sales agent, named Mindy, for about a year.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, the startup&amp;nbsp;announced a $30 million&amp;nbsp;Series A round led by Battery Ventures.&amp;nbsp;This brings 1mind’s total raise to $40 million, the company says.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Kahlow is well known in the sales and marketing tech world as the founder&amp;nbsp;and former CEO&amp;nbsp;of 6sense. It was&amp;nbsp;launched in 2013&amp;nbsp;as a lead-generation&amp;nbsp;tool that tracked signals&amp;nbsp;across social media and other sites to identify potential customers. She left&amp;nbsp;in&amp;nbsp;2020.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the sales agent market is already crowded, 1mind, and its agent, named&amp;nbsp;Mindy,&amp;nbsp;aren’t&amp;nbsp;doing&amp;nbsp;what most of them do: sending emails&amp;nbsp;and making cold&amp;nbsp;calls. That’s a crowded market where&amp;nbsp;even her&amp;nbsp;previous&amp;nbsp;firm, 6sense,&amp;nbsp;offers agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m not&amp;nbsp;playing in&amp;nbsp;outbound,”&amp;nbsp;Kahlow tells TechCrunch.&amp;nbsp;Mindy is intended to handle inbound sales, going all the way to “closing the deal,”&amp;nbsp;Kahlow&amp;nbsp;says.&amp;nbsp;This agent is used to&amp;nbsp;augment&amp;nbsp;self-service&amp;nbsp;websites and, Kahlow says, to&amp;nbsp;replace the&amp;nbsp;sales&amp;nbsp;engineer on calls&amp;nbsp;for larger enterprise deals. It can also be&amp;nbsp;the&amp;nbsp;onboarding&amp;nbsp;specialist, setting up new customers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our goal is to truly replicate the human experience across go-to-market when you have intent, when buyers are leaning in: they come to your website, or&amp;nbsp;they’re&amp;nbsp;in a zoom call. She can ride along and be the sales engineer,” Kahlow adds, referring to Mindy with the anthropomorphic pronoun.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kahlow&amp;nbsp;even goes&amp;nbsp;so far as&amp;nbsp;to call her&amp;nbsp;AI agents “superhumans,” though they are neither human nor do they&amp;nbsp;possess&amp;nbsp;comic-book-style&amp;nbsp;superhuman powers.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Each agent&amp;nbsp;can, however,&amp;nbsp;be trained to&amp;nbsp;understand an&amp;nbsp;expansive&amp;nbsp;knowledge base that encompasses&amp;nbsp;all of&amp;nbsp;a company’s products, technical details,&amp;nbsp;competitive&amp;nbsp;positions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the startup uses a mix of underlying large-language models including OpenAI andG oogle Gemini, the agent limits hallucinations by using deterministic AI, both Kahlow and Mindy said (I invited the agent to our call). Deterministic AI provides guardrails so once the agent ingests corporate sales materials, it should recite that info without deviation. Beyond that, Mindy is trained to say she doesn’t know an answer&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With a year of operations under its belt,&amp;nbsp;1mind is&amp;nbsp;being used by more than 30 companies, including HubSpot, LinkedIn, and New Relic, to&amp;nbsp;pitch and&amp;nbsp;close deals.&amp;nbsp;Kahlow says that&amp;nbsp;her company’s&amp;nbsp;entire roster of&amp;nbsp;named&amp;nbsp;customers all have annual contracts, not “experimental” budgets, and the “average contract is six figures.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="1Mind AI agent Mindy" class="wp-image-3066197" height="382" src="https://techcrunch.com/wp-content/uploads/2025/11/Mindy-transparent.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;1Mind AI agent Mindy&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;1mind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company&amp;nbsp;also uses the bot internally&amp;nbsp;on its&amp;nbsp;own&amp;nbsp;sales&amp;nbsp;calls.&amp;nbsp;But&amp;nbsp;Kahlow&amp;nbsp;has gone&amp;nbsp;even&amp;nbsp;father. She created&amp;nbsp;an avatar of herself, Amanda, and took it with her to her VC pitches.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;During Battery Ventures’ due diligence, “we used it to go through the data room, asking lots of questions on things like case studies,” Battery partner Neeraj Agrawal told TechCrunch about the avatar Amanda. The data room refers to the stacks of data a startup will share about itself with a VC. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The conversation design is very nuanced, like what case studies it shares and when,” he said. The VC also determined that customers were having long back-and-forth conversations, indicating that they would forget they were talking to an AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That avatar&amp;nbsp;is still accessible via&amp;nbsp;Kahlow’s LinkedIn page,&amp;nbsp;where&amp;nbsp;the public can interact with it. It&amp;nbsp;can answer questions about 1mind’s products, but also other areas, like&amp;nbsp;Kahlow’s&amp;nbsp;views of being a woman in tech.&amp;nbsp;However, having chatted with it, it&amp;nbsp;will — much like the human Kahlow — always&amp;nbsp;attempt&amp;nbsp;to bend the conversation toward&amp;nbsp;1mind.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Eventually,&amp;nbsp;Kahlow&amp;nbsp;says,&amp;nbsp;1mind and other agentic sales startups will even replace&amp;nbsp;higher-end&amp;nbsp;account executive&amp;nbsp;sales roles.&amp;nbsp;Or at least drastically&amp;nbsp;reimagine&amp;nbsp;them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re not there yet where&amp;nbsp;we’re&amp;nbsp;fully replacing the AE.&amp;nbsp;We’re&amp;nbsp;replacing the website.&amp;nbsp;We’re&amp;nbsp;replacing the sales engineer, customer success, but that&amp;nbsp;[AE/customer]&amp;nbsp;relationship there is still happening,” she says.&amp;nbsp;&amp;nbsp;“I think over time, a lot of what the AE&amp;nbsp;is&amp;nbsp;doing right now will go away.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;She&amp;nbsp;believes&amp;nbsp;this is&amp;nbsp;currently mostly a&amp;nbsp;trust issue.&amp;nbsp;Agentic tech is so new that a buyer looking to sign an enormous enterprise deal&amp;nbsp;isn’t&amp;nbsp;ready to do it&amp;nbsp;without a human.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Interestingly, once&amp;nbsp;trust develops, she thinks (and is already building for) agentic buyers.&amp;nbsp;Agent-to-agent transactions&amp;nbsp;won’t&amp;nbsp;involve human&amp;nbsp;avatars, but&amp;nbsp;will&amp;nbsp;instead&amp;nbsp;be&amp;nbsp;transfers of&amp;nbsp;information and requirements.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the meantime, 1mind still employs humans:&amp;nbsp;44 of them, including&amp;nbsp;in sales, with 71 job openings, including for&amp;nbsp;account execs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The round included participation from Primary Ventures, Wing Venture Capital, Operator Collective, Harmonic Growth Partners and Success Venture Partners and&amp;nbsp;angels&amp;nbsp;from Monday.com, ZoomInfo, Databricks, Box, Gong, Braze,&amp;nbsp;and&amp;nbsp;Verkada, 1mind said.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/6sense-founder-amanda-kahlow-raises-30-million-for-new-human-replacement-ai-sales-startup-1mind/</guid><pubDate>Mon, 10 Nov 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Scribe hits $1.3B valuation as it moves to show where AI will actually pay off (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/scribe-hits-1-3b-valuation-as-it-moves-to-show-where-ai-will-actually-pay-off/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/scribe-founders.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After helping several enterprises document how work actually happens, Scribe has raised $75 million at a $1.3 billion post-money valuation to roll out Scribe Optimize, a platform that maps workflows across the enterprise to reveal where automation and AI will actually yield returns — instead of becoming another sunk cost.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The all-equity Series C round was led by StepStone, with participation from existing investors Amplify Partners, Redpoint Ventures, Tiger Global, Morado Ventures, and New York Life Ventures. The new funding comes over a year after Scribe raised its $25 million Series B, capital the five-year-old startup has largely not needed to draw down, co-founder and CEO Jennifer Smith (pictured above, left) said in an exclusive interview. With this round, Scribe plans to accelerate the rollout of Scribe Optimize and related products, as enterprises struggle to determine where AI and automation will have the greatest impact.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Many companies are racing to adopt AI, but Smith told TechCrunch that most still cannot answer a fundamental question: What should we automate first? Enterprises often try to find the answer through interviews, workshops, or by bringing in consultants, she said, approaches that take months and still miss much of what people actually do on a day-to-day basis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Without really knowing how work is done, it is really hard to know where to improve it, where to automate it, where agents can help,” she said. “Scribe Optimize is all about answering that question. Very simply, it mines across workflows for what people are doing when they’re at work, and then it abstracts those up into being able to show you in a single pane of glass, here are the actual workflows that are being done. Here’s how often, how long it takes, etc.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2019 by Smith and Aaron Podolny (CTO) (pictured above, right), Scribe started before the GenAI boom, and its current flagship product, Scribe Capture, helps automatically document how work is done. When someone completes a process or workflow, Capture generates a step-by-step guide using its browser extension and desktop app, along with text and screenshots. Those guides can be shared with colleagues or embedded in internal tools to reduce repeated questions, minimize errors, and expedite onboarding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers using Scribe Capture report saving 35 to 42 hours per person per month and making new hires 40% faster, the startup said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The market of process documentation includes players including Tango, Iorad, UserGuiding, and Spekit. Nonetheless, Smith told TechCrunch that Scribe competes against the status quo of people manually recording workflows.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“People are still using stopwatches to sit behind somebody and understand what this process is,” she said. “Even now, when it comes to deploying AI agents, the irony is that the process of deploying agents is incredibly manual.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To date, Scribe has documented more than 10 million workflows across 40,000 software applications. The startup said that it has more than 5 million users and is used by teams inside 94% of Fortune 500 companies. Further, 78,000 organizations are its paid customers. It counts teams at New York Life, T-Mobile, LinkedIn, Hubspot, and Northern Trust among its users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Users come to Scribe not because their boss tells them to, but because they want to,” Smith told TechCrunch. “It starts with the end user, and then goes up to their team lead, department lead, and then some kind of central function who are all interested in it for the question of, how do we scale, what we know, how to do, and how do we get better?”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The San Francisco-based startup sees the U.K., Canada, Australia, and Europe among its biggest markets after the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Scribe said it has more than doubled its revenue over the past year, though it did not disclose figures, and also said its valuation has increased fivefold since its last round. The startup currently has a headcount of 120 employees, and it plans to double that number in the next 12 months, Smith said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/scribe-founders.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After helping several enterprises document how work actually happens, Scribe has raised $75 million at a $1.3 billion post-money valuation to roll out Scribe Optimize, a platform that maps workflows across the enterprise to reveal where automation and AI will actually yield returns — instead of becoming another sunk cost.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The all-equity Series C round was led by StepStone, with participation from existing investors Amplify Partners, Redpoint Ventures, Tiger Global, Morado Ventures, and New York Life Ventures. The new funding comes over a year after Scribe raised its $25 million Series B, capital the five-year-old startup has largely not needed to draw down, co-founder and CEO Jennifer Smith (pictured above, left) said in an exclusive interview. With this round, Scribe plans to accelerate the rollout of Scribe Optimize and related products, as enterprises struggle to determine where AI and automation will have the greatest impact.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Many companies are racing to adopt AI, but Smith told TechCrunch that most still cannot answer a fundamental question: What should we automate first? Enterprises often try to find the answer through interviews, workshops, or by bringing in consultants, she said, approaches that take months and still miss much of what people actually do on a day-to-day basis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Without really knowing how work is done, it is really hard to know where to improve it, where to automate it, where agents can help,” she said. “Scribe Optimize is all about answering that question. Very simply, it mines across workflows for what people are doing when they’re at work, and then it abstracts those up into being able to show you in a single pane of glass, here are the actual workflows that are being done. Here’s how often, how long it takes, etc.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2019 by Smith and Aaron Podolny (CTO) (pictured above, right), Scribe started before the GenAI boom, and its current flagship product, Scribe Capture, helps automatically document how work is done. When someone completes a process or workflow, Capture generates a step-by-step guide using its browser extension and desktop app, along with text and screenshots. Those guides can be shared with colleagues or embedded in internal tools to reduce repeated questions, minimize errors, and expedite onboarding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers using Scribe Capture report saving 35 to 42 hours per person per month and making new hires 40% faster, the startup said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The market of process documentation includes players including Tango, Iorad, UserGuiding, and Spekit. Nonetheless, Smith told TechCrunch that Scribe competes against the status quo of people manually recording workflows.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“People are still using stopwatches to sit behind somebody and understand what this process is,” she said. “Even now, when it comes to deploying AI agents, the irony is that the process of deploying agents is incredibly manual.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To date, Scribe has documented more than 10 million workflows across 40,000 software applications. The startup said that it has more than 5 million users and is used by teams inside 94% of Fortune 500 companies. Further, 78,000 organizations are its paid customers. It counts teams at New York Life, T-Mobile, LinkedIn, Hubspot, and Northern Trust among its users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Users come to Scribe not because their boss tells them to, but because they want to,” Smith told TechCrunch. “It starts with the end user, and then goes up to their team lead, department lead, and then some kind of central function who are all interested in it for the question of, how do we scale, what we know, how to do, and how do we get better?”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The San Francisco-based startup sees the U.K., Canada, Australia, and Europe among its biggest markets after the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Scribe said it has more than doubled its revenue over the past year, though it did not disclose figures, and also said its valuation has increased fivefold since its last round. The startup currently has a headcount of 120 employees, and it plans to double that number in the next 12 months, Smith said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/scribe-hits-1-3b-valuation-as-it-moves-to-show-where-ai-will-actually-pay-off/</guid><pubDate>Mon, 10 Nov 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] The Download: busting weather myths, and AI heart attack prediction (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/10/1127798/the-download-busting-weather-myths-and-ai-heart-attack-prediction/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why it’s so hard to bust the weather control conspiracy theory&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It was October 2024, and Hurricane Helene had just devastated the US Southeast. Representative Marjorie Taylor Greene of Georgia found an abstract target on which to pin the blame: “Yes they can control the weather,” she posted on X. “It’s ridiculous for anyone to lie and say it can’t be done.”&lt;/p&gt;&lt;p&gt;She was repeating what’s by now a pretty familiar and popular conspiracy theory: that shadowy forces are out there, wielding technology to control the weather and wreak havoc on their enemies. This preposterous claim has grown louder and more common in recent years, especially after extreme weather strikes.&lt;/p&gt;&lt;p&gt;But here’s the thing: While Greene and other believers are not correct, this conspiracy theory—like so many others—holds a kernel of much more modest truth. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Dave Levitan&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is part of &lt;em&gt;MIT Technology Review&lt;/em&gt;’s series “The New Conspiracy Age,” on how the present boom in conspiracy theories is reshaping science and technology. Check out &lt;/strong&gt;&lt;strong&gt;the rest of the series here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI could predict who will have a heart attack&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;For all the modern marvels of cardiology, we struggle to predict who will have a heart attack. Many people never get screened at all. Now, startups are applying AI algorithms to screen millions of CT scans for early signs of heart disease.&lt;/p&gt;&lt;p&gt;This technology could be a breakthrough for public health, applying an old tool to uncover patients whose high risk for a heart attack is hiding in plain sight. But it remains unproven at scale, while raising thorny questions about implementation and even how we define disease. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Vishal Khetpal&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the latest print issue of MIT Technology Review magazine, which is full of fascinating stories about the body. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Spending on AI may be to blame for all those tech layoffs&lt;/strong&gt;&lt;br /&gt;AI isn’t necessarily replacing jobs, but spending on it is gobbling up budgets. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;Junior roles are likely to be the first on the chopping block. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;Are the crazy sums that businesses are sinking into AI sustainable? &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;People are worried that AI will take everyone’s jobs. We’ve been here before. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;2 Anti-vaccine activists gathered in Austin over the weekend&lt;/strong&gt;&lt;br /&gt;They celebrated RFK Jr’s rise and outlined their goals—including eliminating school vaccine mandates. (WP $)&lt;br /&gt;+ &lt;em&gt;We’re on the verge of stopping the next pandemic. But will we? &lt;/em&gt;(Vox)&lt;br /&gt;+ &lt;em&gt;How conspiracy theories infiltrated the doctor’s office.&lt;/em&gt; (MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 People who’ve experienced AI-induced delusions are forming a movement&lt;/strong&gt;&lt;br /&gt;They’re pushing for legal action against chatbot makers. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The looming crackdown on AI companionship. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 AI-generated clips of women being strangled are flooding social media&lt;/strong&gt;&lt;br /&gt;Many of them appear to have been created using OpenAI’s Sora 2. (404 Media)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Tech leaders are obsessed with bioengineering babies&lt;br /&gt;They’re not allowed to, but they’re not letting a little thing like ethics get in the way. (WSJ $)&lt;br /&gt;+ &lt;em&gt;The race to make the perfect baby is creating an ethical mess. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;6 Apple has removed two popular gay dating apps in China&amp;nbsp;&lt;br /&gt;The country ordered it to take down Blued and Finka from its app. (Wired $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 The UK government is worried China could turn off its buses remotely&lt;br /&gt;&lt;/strong&gt;It fears hundreds of Chinese-made electric buses on British roads could be at risk. (FT $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 How AI is changing the world’s newsrooms &lt;/strong&gt;&lt;strong&gt;📰&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;It’s brilliant at analyzing large data sets—but shouldn’t be used to write stories. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 How to contain an invasive species&lt;/strong&gt;&lt;br /&gt;Experts argue that too much red tape is getting in the way. (Undark)&lt;br /&gt;+ &lt;em&gt;The weeds are winning. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 The world’s largest electric ship is charging up 🚢&lt;/strong&gt;&lt;br /&gt;Once it’s ready to go, it’ll serve as a ferry in 90 minute bursts. (IEEE Spectrum)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“We would move heaven and Earth, pun intended, to try to get to the Moon sooner.”&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Dave Limp, CEO of Blue Origin, says the company is raring to work with NASA to get humans back on the Moon, Ars Technica reports.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127800" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/image_5d3eca.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Design thinking was supposed to fix the world. Where did it go wrong?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In the 1990s, a six-step methodology for innovation called design thinking started to grow in popularity. Key to its spread was its replicable aesthetic, represented by the Post-it note: a humble square that anyone can use in infinite ways.&lt;/p&gt;&lt;p&gt;But in recent years, for a number of reasons, the shine of design thinking has been wearing off. Critics have argued that its short-term focus on novel and naive ideas results in unrealistic and ungrounded recommendations.&lt;/p&gt;&lt;p&gt;Today, some groups are working to reform both design thinking’s principles and its methodologies. These new efforts seek a set of design tools capable of equitably serving diverse communities and solving diverse problems well into the future. It’s a much more daunting—and crucial—task than design thinking’s original remit. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Rebecca Ackermann&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ These tree-dwelling toads give birth to live young—who knew?!&lt;br /&gt;+ Now’s the time to practice your baking skills ahead of Thanksgiving.&lt;br /&gt;+ Younguk Yi’s glitching paintings are a lot of fun.&lt;br /&gt;+ Place your bets! This fun game follows three balls in a race to the bottom, but who will win?&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why it’s so hard to bust the weather control conspiracy theory&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It was October 2024, and Hurricane Helene had just devastated the US Southeast. Representative Marjorie Taylor Greene of Georgia found an abstract target on which to pin the blame: “Yes they can control the weather,” she posted on X. “It’s ridiculous for anyone to lie and say it can’t be done.”&lt;/p&gt;&lt;p&gt;She was repeating what’s by now a pretty familiar and popular conspiracy theory: that shadowy forces are out there, wielding technology to control the weather and wreak havoc on their enemies. This preposterous claim has grown louder and more common in recent years, especially after extreme weather strikes.&lt;/p&gt;&lt;p&gt;But here’s the thing: While Greene and other believers are not correct, this conspiracy theory—like so many others—holds a kernel of much more modest truth. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Dave Levitan&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is part of &lt;em&gt;MIT Technology Review&lt;/em&gt;’s series “The New Conspiracy Age,” on how the present boom in conspiracy theories is reshaping science and technology. Check out &lt;/strong&gt;&lt;strong&gt;the rest of the series here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI could predict who will have a heart attack&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;For all the modern marvels of cardiology, we struggle to predict who will have a heart attack. Many people never get screened at all. Now, startups are applying AI algorithms to screen millions of CT scans for early signs of heart disease.&lt;/p&gt;&lt;p&gt;This technology could be a breakthrough for public health, applying an old tool to uncover patients whose high risk for a heart attack is hiding in plain sight. But it remains unproven at scale, while raising thorny questions about implementation and even how we define disease. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Vishal Khetpal&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the latest print issue of MIT Technology Review magazine, which is full of fascinating stories about the body. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Spending on AI may be to blame for all those tech layoffs&lt;/strong&gt;&lt;br /&gt;AI isn’t necessarily replacing jobs, but spending on it is gobbling up budgets. (Fast Company $)&lt;br /&gt;+ &lt;em&gt;Junior roles are likely to be the first on the chopping block. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;Are the crazy sums that businesses are sinking into AI sustainable? &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;People are worried that AI will take everyone’s jobs. We’ve been here before. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;2 Anti-vaccine activists gathered in Austin over the weekend&lt;/strong&gt;&lt;br /&gt;They celebrated RFK Jr’s rise and outlined their goals—including eliminating school vaccine mandates. (WP $)&lt;br /&gt;+ &lt;em&gt;We’re on the verge of stopping the next pandemic. But will we? &lt;/em&gt;(Vox)&lt;br /&gt;+ &lt;em&gt;How conspiracy theories infiltrated the doctor’s office.&lt;/em&gt; (MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 People who’ve experienced AI-induced delusions are forming a movement&lt;/strong&gt;&lt;br /&gt;They’re pushing for legal action against chatbot makers. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The looming crackdown on AI companionship. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 AI-generated clips of women being strangled are flooding social media&lt;/strong&gt;&lt;br /&gt;Many of them appear to have been created using OpenAI’s Sora 2. (404 Media)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Tech leaders are obsessed with bioengineering babies&lt;br /&gt;They’re not allowed to, but they’re not letting a little thing like ethics get in the way. (WSJ $)&lt;br /&gt;+ &lt;em&gt;The race to make the perfect baby is creating an ethical mess. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;6 Apple has removed two popular gay dating apps in China&amp;nbsp;&lt;br /&gt;The country ordered it to take down Blued and Finka from its app. (Wired $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 The UK government is worried China could turn off its buses remotely&lt;br /&gt;&lt;/strong&gt;It fears hundreds of Chinese-made electric buses on British roads could be at risk. (FT $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;8 How AI is changing the world’s newsrooms &lt;/strong&gt;&lt;strong&gt;📰&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;It’s brilliant at analyzing large data sets—but shouldn’t be used to write stories. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 How to contain an invasive species&lt;/strong&gt;&lt;br /&gt;Experts argue that too much red tape is getting in the way. (Undark)&lt;br /&gt;+ &lt;em&gt;The weeds are winning. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 The world’s largest electric ship is charging up 🚢&lt;/strong&gt;&lt;br /&gt;Once it’s ready to go, it’ll serve as a ferry in 90 minute bursts. (IEEE Spectrum)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“We would move heaven and Earth, pun intended, to try to get to the Moon sooner.”&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Dave Limp, CEO of Blue Origin, says the company is raring to work with NASA to get humans back on the Moon, Ars Technica reports.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127800" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/image_5d3eca.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Design thinking was supposed to fix the world. Where did it go wrong?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In the 1990s, a six-step methodology for innovation called design thinking started to grow in popularity. Key to its spread was its replicable aesthetic, represented by the Post-it note: a humble square that anyone can use in infinite ways.&lt;/p&gt;&lt;p&gt;But in recent years, for a number of reasons, the shine of design thinking has been wearing off. Critics have argued that its short-term focus on novel and naive ideas results in unrealistic and ungrounded recommendations.&lt;/p&gt;&lt;p&gt;Today, some groups are working to reform both design thinking’s principles and its methodologies. These new efforts seek a set of design tools capable of equitably serving diverse communities and solving diverse problems well into the future. It’s a much more daunting—and crucial—task than design thinking’s original remit. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Rebecca Ackermann&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ These tree-dwelling toads give birth to live young—who knew?!&lt;br /&gt;+ Now’s the time to practice your baking skills ahead of Thanksgiving.&lt;br /&gt;+ Younguk Yi’s glitching paintings are a lot of fun.&lt;br /&gt;+ Place your bets! This fun game follows three balls in a race to the bottom, but who will win?&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/10/1127798/the-download-busting-weather-myths-and-ai-heart-attack-prediction/</guid><pubDate>Mon, 10 Nov 2025 13:10:00 +0000</pubDate></item><item><title>[NEW] Baseten takes on hyperscalers with new AI training platform that lets you own your model weights (AI | VentureBeat)</title><link>https://venturebeat.com/ai/baseten-takes-on-hyperscalers-with-new-ai-training-platform-that-lets-you</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.baseten.co/"&gt;&lt;u&gt;Baseten&lt;/u&gt;&lt;/a&gt;, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.&lt;/p&gt;&lt;p&gt;The San Francisco-based company announced Thursday the general availability of &lt;a href="https://www.baseten.co/products/training/"&gt;&lt;u&gt;Baseten Training&lt;/u&gt;&lt;/a&gt;, an infrastructure platform designed to help companies fine-tune open-source AI models without the operational headaches of managing GPU clusters, multi-node orchestration, or cloud capacity planning. The move is a calculated expansion beyond Baseten&amp;#x27;s core inference business, driven by what CEO Amir Haghighat describes as relentless customer demand and a strategic imperative to capture the full lifecycle of AI deployment.&lt;/p&gt;&lt;p&gt;&amp;quot;We had a captive audience of customers who kept coming to us saying, &amp;#x27;Hey, I hate this problem,&amp;#x27;&amp;quot; Haghighat said in an interview. &amp;quot;One of them told me, &amp;#x27;Look, I bought a bunch of H100s from a cloud provider. I have to SSH in on Friday, run my fine-tuning job, then check on Monday to see if it worked. Sometimes I realize it just hasn&amp;#x27;t been working all along.&amp;#x27;&amp;quot;&lt;/p&gt;&lt;p&gt;The launch comes at a critical inflection point in enterprise AI adoption. As open-source models from &lt;a href="https://huggingface.co/meta-llama"&gt;&lt;u&gt;Meta&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://huggingface.co/Alibaba-NLP"&gt;&lt;u&gt;Alibaba&lt;/u&gt;&lt;/a&gt;, and others increasingly rival proprietary systems in performance, companies face mounting pressure to reduce their reliance on expensive API calls to services like OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; or Anthropic&amp;#x27;s &lt;a href="https://claude.ai/"&gt;&lt;u&gt;Claude&lt;/u&gt;&lt;/a&gt;. But the path from off-the-shelf open-source model to production-ready custom AI remains treacherous, requiring specialized expertise in machine learning operations, infrastructure management, and performance optimization.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s answer: provide the infrastructure rails while letting companies retain full control over their training code, data, and model weights. It&amp;#x27;s a deliberately low-level approach born from hard-won lessons.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How a failed product taught Baseten what AI training infrastructure really needs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;This isn&amp;#x27;t Baseten&amp;#x27;s first foray into training. The company&amp;#x27;s previous attempt, a product called Blueprints launched roughly two and a half years ago, failed spectacularly — a failure Haghighat now embraces as instructive.&lt;/p&gt;&lt;p&gt;&amp;quot;We had created the abstraction layer a little too high,&amp;quot; he explained. &amp;quot;We were trying to create a magical experience, where as a user, you come in and programmatically choose a base model, choose your data and some hyperparameters, and magically out comes a model.&amp;quot;&lt;/p&gt;&lt;p&gt;The problem? Users didn&amp;#x27;t have the intuition to make the right choices about base models, data quality, or hyperparameters. When their models underperformed, they blamed the product. Baseten found itself in the consulting business rather than the infrastructure business, helping customers debug everything from dataset deduplication to model selection.&lt;/p&gt;&lt;p&gt;&amp;quot;We became consultants,&amp;quot; Haghighat said. &amp;quot;And that&amp;#x27;s not what we had set out to do.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.baseten.co/"&gt;&lt;u&gt;Baseten&lt;/u&gt;&lt;/a&gt; killed Blueprints and refocused entirely on inference, vowing to &amp;quot;earn the right&amp;quot; to expand again. That moment arrived earlier this year, driven by two market realities: the vast majority of Baseten&amp;#x27;s inference revenue comes from custom models that customers train elsewhere, and competing training platforms were using restrictive terms of service to lock customers into their inference products.&lt;/p&gt;&lt;p&gt;&amp;quot;Multiple companies who were building fine-tuning products had in their terms of service that you as a customer cannot take the weights of the fine-tuned model with you somewhere else,&amp;quot; Haghighat said. &amp;quot;I understand why from their perspective — I still don&amp;#x27;t think there is a big company to be made purely on just training or fine-tuning. The sticky part is in inference, the valuable part where value is unlocked is in inference, and ultimately the revenue is in inference.&amp;quot;&lt;/p&gt;&lt;p&gt;Baseten took the opposite approach: customers own their weights and can download them at will. The bet is that superior inference performance will keep them on the platform anyway.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Multi-cloud GPU orchestration and sub-minute scheduling set Baseten apart from hyperscalers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The new &lt;a href="https://www.baseten.co/products/training/"&gt;&lt;u&gt;Baseten Training&lt;/u&gt;&lt;/a&gt; product operates at what Haghighat calls &amp;quot;the infrastructure layer&amp;quot; — lower-level than the failed Blueprints experiment, but with opinionated tooling around reliability, observability, and integration with Baseten&amp;#x27;s inference stack.&lt;/p&gt;&lt;p&gt;Key technical capabilities include multi-node training support across clusters of &lt;a href="https://www.nvidia.com/en-us/data-center/h100/"&gt;&lt;u&gt;NVIDIA H100&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://www.baseten.co/blog/accelerating-inference-nvidia-b200-gpus/?utm_term=b200%20gpu&amp;amp;utm_campaign=Search+-Hosting+Inference&amp;amp;utm_source=adwords&amp;amp;utm_medium=ppc&amp;amp;hsa_acc=9990356727&amp;amp;hsa_cam=21607833837&amp;amp;hsa_grp=179204207220&amp;amp;hsa_ad=747940377782&amp;amp;hsa_src=g&amp;amp;hsa_tgt=kwd-2402895176571&amp;amp;hsa_kw=b200%20gpu&amp;amp;hsa_mt=e&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;gad_source=1&amp;amp;gad_campaignid=21607833837&amp;amp;gbraid=0AAAAAqCKh1tBBfE-A9FM_fms6m5z6IuPk&amp;amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6S_fxqaV-EfbGshbTKib8rt2sQl81yPs6M0y40aoYf1Zy5mV_ECflBoCjPsQAvD_BwE"&gt;&lt;u&gt;B200 GPUs&lt;/u&gt;&lt;/a&gt;, automated checkpointing to protect against node failures, sub-minute job scheduling, and integration with Baseten&amp;#x27;s proprietary &lt;a href="https://www.baseten.co/blog/how-we-built-multi-cloud-capacity-management/"&gt;&lt;u&gt;Multi-Cloud Management (MCM) &lt;/u&gt;&lt;/a&gt;system. That last piece is critical: MCM allows Baseten to dynamically provision GPU capacity across multiple cloud providers and regions, passing cost savings to customers while avoiding the capacity constraints and multi-year contracts typical of hyperscaler deals.&lt;/p&gt;&lt;p&gt;&amp;quot;With hyperscalers, you don&amp;#x27;t get to say, &amp;#x27;Hey, give me three or four B200 nodes while my job is running, and then take it back from me and don&amp;#x27;t charge me for it,&amp;#x27;&amp;quot; Haghighat said. &amp;quot;They say, &amp;#x27;No, you need to sign a three-year contract.&amp;#x27; We don&amp;#x27;t do that.&amp;quot;&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s approach mirrors broader trends in cloud infrastructure, where abstraction layers increasingly allow workloads to move fluidly across providers. When AWS experienced a major outage several weeks ago, Baseten&amp;#x27;s inference services remained operational by automatically routing traffic to other cloud providers — a capability now extended to training workloads.&lt;/p&gt;&lt;p&gt;The technical differentiation extends to Baseten&amp;#x27;s observability tooling, which provides per-GPU metrics for multi-node jobs, granular checkpoint tracking, and a refreshed UI that surfaces infrastructure-level events. The company also introduced an &amp;quot;&lt;a href="https://github.com/basetenlabs/ml-cookbook"&gt;&lt;u&gt;ML Cookbook&lt;/u&gt;&lt;/a&gt;&amp;quot; of open-source training recipes for popular models like Gemma, GPT OSS, and Qwen, designed to help users reach &amp;quot;training success&amp;quot; faster.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Early adopters report 84% cost savings and 50% latency improvements with custom models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Two early customers illustrate the market Baseten is targeting: AI-native companies building specialized vertical solutions that require custom models.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.oxen.ai/"&gt;&lt;u&gt;Oxen AI&lt;/u&gt;&lt;/a&gt;, a platform focused on dataset management and model fine-tuning, exemplifies the partnership model Baseten envisions. CEO Greg Schoeninger articulated a common strategic calculus, telling VentureBeat: &amp;quot;Whenever I&amp;#x27;ve seen a platform try to do both hardware and software, they usually fail at one of them. That&amp;#x27;s why partnering with Baseten to handle infrastructure was the obvious choice.&amp;quot;&lt;/p&gt;&lt;p&gt;Oxen built its customer experience entirely on top of Baseten&amp;#x27;s infrastructure, using the &lt;a href="https://www.baseten.co/resources/changelog/authenticate-from-cli-with-truss-login/"&gt;&lt;u&gt;Baseten CLI&lt;/u&gt;&lt;/a&gt; to programmatically orchestrate training jobs. The system automatically provisions and deprovisions GPUs, fully concealing Baseten&amp;#x27;s interface behind Oxen&amp;#x27;s own. For one Oxen customer, &lt;a href="https://alliumai.com/"&gt;&lt;u&gt;AlliumAI&lt;/u&gt;&lt;/a&gt; — a startup bringing structure to messy retail data — the integration delivered 84% cost savings compared to previous approaches, reducing total inference costs from $46,800 to $7,530.&lt;/p&gt;&lt;p&gt;&amp;quot;Training custom LoRAs has always been one of the most effective ways to leverage open-source models, but it often came with infrastructure headaches,&amp;quot; said Daniel Demillard, CEO of AlliumAI. &amp;quot;With Oxen and Baseten, that complexity disappears. We can train and deploy models at massive scale without ever worrying about CUDA, which GPU to choose, or shutting down servers after training.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://parsed.com/"&gt;&lt;u&gt;Parsed&lt;/u&gt;&lt;/a&gt;, another early customer, tackles a different pain point: helping enterprises reduce dependence on OpenAI by creating specialized models that outperform generalist LLMs on domain-specific tasks. The company works in mission-critical sectors like healthcare, finance, and legal services, where model performance and reliability aren&amp;#x27;t negotiable.&lt;/p&gt;&lt;p&gt;&amp;quot;Prior to switching to Baseten, we were seeing repetitive and degraded performance on our fine-tuned models due to bugs with our previous training provider,&amp;quot; said Charles O&amp;#x27;Neill, Parsed&amp;#x27;s co-founder and chief science officer. &amp;quot;On top of that, we were struggling to easily download and checkpoint weights after training runs.&amp;quot;&lt;/p&gt;&lt;p&gt;With Baseten, Parsed achieved 50% lower end-to-end latency for transcription use cases, spun up HIPAA-compliant EU deployments for testing within 48 hours, and kicked off more than 500 training jobs. The company also leveraged Baseten&amp;#x27;s modified &lt;a href="https://docs.baseten.co/examples/vllm"&gt;&lt;u&gt;vLLM inference framework&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/"&gt;&lt;u&gt;speculative decoding&lt;/u&gt;&lt;/a&gt; — a technique that generates draft tokens to accelerate language model output — to cut latency in half for custom models.&lt;/p&gt;&lt;p&gt;&amp;quot;Fast models matter,&amp;quot; O&amp;#x27;Neill said. &amp;quot;But fast models that get better over time matter more. A model that&amp;#x27;s 2x faster but static loses to one that&amp;#x27;s slightly slower but improving 10% monthly. Baseten gives us both — the performance edge today and the infrastructure for continuous improvement.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why training and inference are more interconnected than the industry realizes&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Parsed example illuminates a deeper strategic rationale for Baseten&amp;#x27;s training expansion: the boundary between training and inference is blurrier than conventional wisdom suggests.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s model performance team uses the training platform extensively to create &amp;quot;draft models&amp;quot; for speculative decoding, a cutting-edge technique that can dramatically accelerate inference. The company recently announced it achieved 650+ tokens per second on OpenAI&amp;#x27;s &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;&lt;u&gt;GPT OSS 120B model&lt;/u&gt;&lt;/a&gt; — a 60% improvement over its launch performance — using &lt;a href="https://arxiv.org/abs/2503.01840"&gt;&lt;u&gt;EAGLE-3&lt;/u&gt;&lt;/a&gt; speculative decoding, which requires training specialized small models to work alongside larger target models.&lt;/p&gt;&lt;p&gt;&amp;quot;Ultimately, inference and training plug in more ways than one might think,&amp;quot; Haghighat said. &amp;quot;When you do speculative decoding in inference, you need to train the draft model. Our model performance team is a big customer of the training product to train these EAGLE heads on a continuous basis.&amp;quot;&lt;/p&gt;&lt;p&gt;This technical interdependence reinforces Baseten&amp;#x27;s thesis that owning both training and inference creates defensible value. The company can optimize the entire lifecycle: a model trained on Baseten can be deployed with a single click to inference endpoints pre-optimized for that architecture, with deployment-from-checkpoint support for chat completion and audio transcription workloads.&lt;/p&gt;&lt;p&gt;The approach contrasts sharply with vertically integrated competitors like &lt;a href="https://replicate.com/"&gt;&lt;u&gt;Replicate&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://modal.com/"&gt;&lt;u&gt;Modal&lt;/u&gt;&lt;/a&gt;, which also offer training and inference but with different architectural tradeoffs. Baseten&amp;#x27;s bet is on lower-level infrastructure flexibility and performance optimization, particularly for companies running custom models at scale.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;As open-source AI models improve, enterprises see fine-tuning as the path away from OpenAI dependency&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Underpinning Baseten&amp;#x27;s entire strategy is a conviction about the trajectory of open-source AI models — namely, that they&amp;#x27;re getting good enough, fast enough, to unlock massive enterprise adoption through fine-tuning.&lt;/p&gt;&lt;p&gt;&amp;quot;Both closed and open-source models are getting better and better in terms of quality,&amp;quot; Haghighat said. &amp;quot;We don&amp;#x27;t even need open source to surpass closed models, because as both of them are getting better, they unlock all these invisible lines of usefulness for different use cases.&amp;quot;&lt;/p&gt;&lt;p&gt;He pointed to the proliferation of reinforcement learning and supervised fine-tuning techniques that allow companies to take an open-source model and make it &amp;quot;as good as the closed model, not at everything, but at this narrow band of capability that they want.&amp;quot;&lt;/p&gt;&lt;p&gt;That trend is already visible in Baseten&amp;#x27;s &lt;a href="https://www.baseten.co/products/model-apis/"&gt;&lt;u&gt;Model APIs business&lt;/u&gt;&lt;/a&gt;, launched alongside Training earlier this year to provide production-grade access to open-source models. The company was the first provider to offer access to &lt;a href="https://api-docs.deepseek.com/news/news1226"&gt;&lt;u&gt;DeepSeek V3&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://api-docs.deepseek.com/news/news250120"&gt;&lt;u&gt;R1&lt;/u&gt;&lt;/a&gt;, and has since added models like &lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;&lt;u&gt;Llama 4&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://qwen.ai/research"&gt;&lt;u&gt;Qwen 3&lt;/u&gt;&lt;/a&gt;, optimized for performance and reliability. Model APIs serves as a top-of-funnel product: companies start with off-the-shelf open-source models, realize they need customization, move to Training for fine-tuning, and ultimately deploy on Baseten&amp;#x27;s &lt;a href="https://www.baseten.co/products/dedicated-deployments/"&gt;&lt;u&gt;Dedicated Deployments&lt;/u&gt;&lt;/a&gt; infrastructure.&lt;/p&gt;&lt;p&gt;Yet Haghighat acknowledged the market remains &amp;quot;fuzzy&amp;quot; around which training techniques will dominate. Baseten is hedging by staying close to the bleeding edge through its &lt;a href="https://www.baseten.co/blog/forward-deployed-engineering/"&gt;&lt;u&gt;Forward Deployed Engineering team&lt;/u&gt;&lt;/a&gt;, which works hands-on with select customers on reinforcement learning, supervised fine-tuning, and other advanced techniques.&lt;/p&gt;&lt;p&gt;&amp;quot;As we do that, we will see patterns emerge about what a productized training product can look like that really addresses the user&amp;#x27;s needs without them having to learn too much about how RL works,&amp;quot; he said. &amp;quot;Are we there as an industry? I would say not quite. I see some attempts at that, but they all seem like almost falling to the same trap that Blueprints fell into—a bit of a walled garden that ties the hands of AI folks behind their back.&amp;quot;&lt;/p&gt;&lt;p&gt;The roadmap ahead includes potential abstractions for common training patterns, expansion into image, audio, and video fine-tuning, and deeper integration of advanced techniques like prefill-decode disaggregation, which separates the initial processing of prompts from token generation to improve efficiency.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Baseten faces crowded field but bets developer experience and performance will win enterprise customers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Baseten enters an increasingly crowded market for AI infrastructure. Hyperscalers like &lt;a href="https://aws.amazon.com/"&gt;&lt;u&gt;AWS&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://cloud.google.com/?hl=en"&gt;&lt;u&gt;Google Cloud&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://azure.microsoft.com/en-us/"&gt;&lt;u&gt;Microsoft Azure&lt;/u&gt;&lt;/a&gt; offer GPU compute for training, while specialized providers like Lambda Labs, CoreWeave, and Together AI compete on price, performance, or ease of use. Then there are vertically integrated platforms like &lt;a href="https://huggingface.co/"&gt;&lt;u&gt;Hugging Face&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://replicate.com/"&gt;&lt;u&gt;Replicate&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://modal.com/"&gt;&lt;u&gt;Modal&lt;/u&gt;&lt;/a&gt; that bundle training, inference, and model hosting.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s differentiation rests on three pillars: its MCM system for multi-cloud capacity management, deep performance optimization expertise built from its inference business, and a developer experience tailored for production deployments rather than experimentation.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s recent &lt;a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"&gt;&lt;u&gt;$150 million Series D&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"&gt;&lt;u&gt;$2.15 billion valuation&lt;/u&gt;&lt;/a&gt; provide runway to invest in both products simultaneously. Major customers include &lt;a href="https://www.descript.com/"&gt;&lt;u&gt;Descript&lt;/u&gt;&lt;/a&gt;, which uses Baseten for transcription workloads; &lt;a href="https://decagon.ai/"&gt;&lt;u&gt;Decagon&lt;/u&gt;&lt;/a&gt;, which runs customer service AI; and &lt;a href="https://sourcegraph.com/"&gt;&lt;u&gt;Sourcegraph&lt;/u&gt;&lt;/a&gt;, which powers coding assistants. All three operate in domains where model customization and performance are competitive advantages.&lt;/p&gt;&lt;p&gt;Timing may be Baseten&amp;#x27;s biggest asset. The confluence of improving open-source models, enterprise discomfort with dependence on proprietary AI providers, and growing sophistication around fine-tuning techniques creates what Haghighat sees as a sustainable market shift.&lt;/p&gt;&lt;p&gt;&amp;quot;There is a lot of use cases for which closed models have gotten there and open ones have not,&amp;quot; he said. &amp;quot;Where I&amp;#x27;m seeing in the market is people using different training techniques — more recently, a lot of reinforcement learning and SFT — to be able to get this open model to be as good as the closed model, not at everything, but at this narrow band of capability that they want. That&amp;#x27;s very palpable in the market.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprises navigating the complex transition from closed to open AI models, Baseten&amp;#x27;s positioning offers a clear value proposition: infrastructure that handles the messy middle of fine-tuning while optimizing for the ultimate goal of performant, reliable, cost-effective inference at scale. The company&amp;#x27;s insistence that customers own their model weights — a stark contrast to competitors using training as a lock-in mechanism — reflects confidence that technical excellence, not contractual restrictions, will drive retention.&lt;/p&gt;&lt;p&gt;Whether Baseten can execute on this vision depends on navigating tensions inherent in its strategy: staying at the infrastructure layer without becoming consultants, providing power and flexibility without overwhelming users with complexity, and building abstractions at exactly the right level as the market matures. The company&amp;#x27;s willingness to kill Blueprints when it failed suggests a pragmatism that could prove decisive in a market where many infrastructure providers over-promise and under-deliver.&lt;/p&gt;&lt;p&gt;&amp;quot;Through and through, we&amp;#x27;re an inference company,&amp;quot; Haghighat emphasized. &amp;quot;The reason that we did training is at the service of inference.&amp;quot;&lt;/p&gt;&lt;p&gt;That clarity of purpose — treating training as a means to an end rather than an end in itself—may be Baseten&amp;#x27;s most important strategic asset. As AI deployment matures from experimentation to production, the companies that solve the full stack stand to capture outsized value. But only if they avoid the trap of technology in search of a problem.&lt;/p&gt;&lt;p&gt;At least Baseten&amp;#x27;s customers no longer have to SSH into boxes on Friday and pray their training jobs complete by Monday. In the infrastructure business, sometimes the best innovation is simply making the painful parts disappear.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.baseten.co/"&gt;&lt;u&gt;Baseten&lt;/u&gt;&lt;/a&gt;, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.&lt;/p&gt;&lt;p&gt;The San Francisco-based company announced Thursday the general availability of &lt;a href="https://www.baseten.co/products/training/"&gt;&lt;u&gt;Baseten Training&lt;/u&gt;&lt;/a&gt;, an infrastructure platform designed to help companies fine-tune open-source AI models without the operational headaches of managing GPU clusters, multi-node orchestration, or cloud capacity planning. The move is a calculated expansion beyond Baseten&amp;#x27;s core inference business, driven by what CEO Amir Haghighat describes as relentless customer demand and a strategic imperative to capture the full lifecycle of AI deployment.&lt;/p&gt;&lt;p&gt;&amp;quot;We had a captive audience of customers who kept coming to us saying, &amp;#x27;Hey, I hate this problem,&amp;#x27;&amp;quot; Haghighat said in an interview. &amp;quot;One of them told me, &amp;#x27;Look, I bought a bunch of H100s from a cloud provider. I have to SSH in on Friday, run my fine-tuning job, then check on Monday to see if it worked. Sometimes I realize it just hasn&amp;#x27;t been working all along.&amp;#x27;&amp;quot;&lt;/p&gt;&lt;p&gt;The launch comes at a critical inflection point in enterprise AI adoption. As open-source models from &lt;a href="https://huggingface.co/meta-llama"&gt;&lt;u&gt;Meta&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://huggingface.co/Alibaba-NLP"&gt;&lt;u&gt;Alibaba&lt;/u&gt;&lt;/a&gt;, and others increasingly rival proprietary systems in performance, companies face mounting pressure to reduce their reliance on expensive API calls to services like OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; or Anthropic&amp;#x27;s &lt;a href="https://claude.ai/"&gt;&lt;u&gt;Claude&lt;/u&gt;&lt;/a&gt;. But the path from off-the-shelf open-source model to production-ready custom AI remains treacherous, requiring specialized expertise in machine learning operations, infrastructure management, and performance optimization.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s answer: provide the infrastructure rails while letting companies retain full control over their training code, data, and model weights. It&amp;#x27;s a deliberately low-level approach born from hard-won lessons.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How a failed product taught Baseten what AI training infrastructure really needs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;This isn&amp;#x27;t Baseten&amp;#x27;s first foray into training. The company&amp;#x27;s previous attempt, a product called Blueprints launched roughly two and a half years ago, failed spectacularly — a failure Haghighat now embraces as instructive.&lt;/p&gt;&lt;p&gt;&amp;quot;We had created the abstraction layer a little too high,&amp;quot; he explained. &amp;quot;We were trying to create a magical experience, where as a user, you come in and programmatically choose a base model, choose your data and some hyperparameters, and magically out comes a model.&amp;quot;&lt;/p&gt;&lt;p&gt;The problem? Users didn&amp;#x27;t have the intuition to make the right choices about base models, data quality, or hyperparameters. When their models underperformed, they blamed the product. Baseten found itself in the consulting business rather than the infrastructure business, helping customers debug everything from dataset deduplication to model selection.&lt;/p&gt;&lt;p&gt;&amp;quot;We became consultants,&amp;quot; Haghighat said. &amp;quot;And that&amp;#x27;s not what we had set out to do.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.baseten.co/"&gt;&lt;u&gt;Baseten&lt;/u&gt;&lt;/a&gt; killed Blueprints and refocused entirely on inference, vowing to &amp;quot;earn the right&amp;quot; to expand again. That moment arrived earlier this year, driven by two market realities: the vast majority of Baseten&amp;#x27;s inference revenue comes from custom models that customers train elsewhere, and competing training platforms were using restrictive terms of service to lock customers into their inference products.&lt;/p&gt;&lt;p&gt;&amp;quot;Multiple companies who were building fine-tuning products had in their terms of service that you as a customer cannot take the weights of the fine-tuned model with you somewhere else,&amp;quot; Haghighat said. &amp;quot;I understand why from their perspective — I still don&amp;#x27;t think there is a big company to be made purely on just training or fine-tuning. The sticky part is in inference, the valuable part where value is unlocked is in inference, and ultimately the revenue is in inference.&amp;quot;&lt;/p&gt;&lt;p&gt;Baseten took the opposite approach: customers own their weights and can download them at will. The bet is that superior inference performance will keep them on the platform anyway.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Multi-cloud GPU orchestration and sub-minute scheduling set Baseten apart from hyperscalers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The new &lt;a href="https://www.baseten.co/products/training/"&gt;&lt;u&gt;Baseten Training&lt;/u&gt;&lt;/a&gt; product operates at what Haghighat calls &amp;quot;the infrastructure layer&amp;quot; — lower-level than the failed Blueprints experiment, but with opinionated tooling around reliability, observability, and integration with Baseten&amp;#x27;s inference stack.&lt;/p&gt;&lt;p&gt;Key technical capabilities include multi-node training support across clusters of &lt;a href="https://www.nvidia.com/en-us/data-center/h100/"&gt;&lt;u&gt;NVIDIA H100&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://www.baseten.co/blog/accelerating-inference-nvidia-b200-gpus/?utm_term=b200%20gpu&amp;amp;utm_campaign=Search+-Hosting+Inference&amp;amp;utm_source=adwords&amp;amp;utm_medium=ppc&amp;amp;hsa_acc=9990356727&amp;amp;hsa_cam=21607833837&amp;amp;hsa_grp=179204207220&amp;amp;hsa_ad=747940377782&amp;amp;hsa_src=g&amp;amp;hsa_tgt=kwd-2402895176571&amp;amp;hsa_kw=b200%20gpu&amp;amp;hsa_mt=e&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;gad_source=1&amp;amp;gad_campaignid=21607833837&amp;amp;gbraid=0AAAAAqCKh1tBBfE-A9FM_fms6m5z6IuPk&amp;amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6S_fxqaV-EfbGshbTKib8rt2sQl81yPs6M0y40aoYf1Zy5mV_ECflBoCjPsQAvD_BwE"&gt;&lt;u&gt;B200 GPUs&lt;/u&gt;&lt;/a&gt;, automated checkpointing to protect against node failures, sub-minute job scheduling, and integration with Baseten&amp;#x27;s proprietary &lt;a href="https://www.baseten.co/blog/how-we-built-multi-cloud-capacity-management/"&gt;&lt;u&gt;Multi-Cloud Management (MCM) &lt;/u&gt;&lt;/a&gt;system. That last piece is critical: MCM allows Baseten to dynamically provision GPU capacity across multiple cloud providers and regions, passing cost savings to customers while avoiding the capacity constraints and multi-year contracts typical of hyperscaler deals.&lt;/p&gt;&lt;p&gt;&amp;quot;With hyperscalers, you don&amp;#x27;t get to say, &amp;#x27;Hey, give me three or four B200 nodes while my job is running, and then take it back from me and don&amp;#x27;t charge me for it,&amp;#x27;&amp;quot; Haghighat said. &amp;quot;They say, &amp;#x27;No, you need to sign a three-year contract.&amp;#x27; We don&amp;#x27;t do that.&amp;quot;&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s approach mirrors broader trends in cloud infrastructure, where abstraction layers increasingly allow workloads to move fluidly across providers. When AWS experienced a major outage several weeks ago, Baseten&amp;#x27;s inference services remained operational by automatically routing traffic to other cloud providers — a capability now extended to training workloads.&lt;/p&gt;&lt;p&gt;The technical differentiation extends to Baseten&amp;#x27;s observability tooling, which provides per-GPU metrics for multi-node jobs, granular checkpoint tracking, and a refreshed UI that surfaces infrastructure-level events. The company also introduced an &amp;quot;&lt;a href="https://github.com/basetenlabs/ml-cookbook"&gt;&lt;u&gt;ML Cookbook&lt;/u&gt;&lt;/a&gt;&amp;quot; of open-source training recipes for popular models like Gemma, GPT OSS, and Qwen, designed to help users reach &amp;quot;training success&amp;quot; faster.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Early adopters report 84% cost savings and 50% latency improvements with custom models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Two early customers illustrate the market Baseten is targeting: AI-native companies building specialized vertical solutions that require custom models.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.oxen.ai/"&gt;&lt;u&gt;Oxen AI&lt;/u&gt;&lt;/a&gt;, a platform focused on dataset management and model fine-tuning, exemplifies the partnership model Baseten envisions. CEO Greg Schoeninger articulated a common strategic calculus, telling VentureBeat: &amp;quot;Whenever I&amp;#x27;ve seen a platform try to do both hardware and software, they usually fail at one of them. That&amp;#x27;s why partnering with Baseten to handle infrastructure was the obvious choice.&amp;quot;&lt;/p&gt;&lt;p&gt;Oxen built its customer experience entirely on top of Baseten&amp;#x27;s infrastructure, using the &lt;a href="https://www.baseten.co/resources/changelog/authenticate-from-cli-with-truss-login/"&gt;&lt;u&gt;Baseten CLI&lt;/u&gt;&lt;/a&gt; to programmatically orchestrate training jobs. The system automatically provisions and deprovisions GPUs, fully concealing Baseten&amp;#x27;s interface behind Oxen&amp;#x27;s own. For one Oxen customer, &lt;a href="https://alliumai.com/"&gt;&lt;u&gt;AlliumAI&lt;/u&gt;&lt;/a&gt; — a startup bringing structure to messy retail data — the integration delivered 84% cost savings compared to previous approaches, reducing total inference costs from $46,800 to $7,530.&lt;/p&gt;&lt;p&gt;&amp;quot;Training custom LoRAs has always been one of the most effective ways to leverage open-source models, but it often came with infrastructure headaches,&amp;quot; said Daniel Demillard, CEO of AlliumAI. &amp;quot;With Oxen and Baseten, that complexity disappears. We can train and deploy models at massive scale without ever worrying about CUDA, which GPU to choose, or shutting down servers after training.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://parsed.com/"&gt;&lt;u&gt;Parsed&lt;/u&gt;&lt;/a&gt;, another early customer, tackles a different pain point: helping enterprises reduce dependence on OpenAI by creating specialized models that outperform generalist LLMs on domain-specific tasks. The company works in mission-critical sectors like healthcare, finance, and legal services, where model performance and reliability aren&amp;#x27;t negotiable.&lt;/p&gt;&lt;p&gt;&amp;quot;Prior to switching to Baseten, we were seeing repetitive and degraded performance on our fine-tuned models due to bugs with our previous training provider,&amp;quot; said Charles O&amp;#x27;Neill, Parsed&amp;#x27;s co-founder and chief science officer. &amp;quot;On top of that, we were struggling to easily download and checkpoint weights after training runs.&amp;quot;&lt;/p&gt;&lt;p&gt;With Baseten, Parsed achieved 50% lower end-to-end latency for transcription use cases, spun up HIPAA-compliant EU deployments for testing within 48 hours, and kicked off more than 500 training jobs. The company also leveraged Baseten&amp;#x27;s modified &lt;a href="https://docs.baseten.co/examples/vllm"&gt;&lt;u&gt;vLLM inference framework&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.baseten.co/blog/a-quick-introduction-to-speculative-decoding/"&gt;&lt;u&gt;speculative decoding&lt;/u&gt;&lt;/a&gt; — a technique that generates draft tokens to accelerate language model output — to cut latency in half for custom models.&lt;/p&gt;&lt;p&gt;&amp;quot;Fast models matter,&amp;quot; O&amp;#x27;Neill said. &amp;quot;But fast models that get better over time matter more. A model that&amp;#x27;s 2x faster but static loses to one that&amp;#x27;s slightly slower but improving 10% monthly. Baseten gives us both — the performance edge today and the infrastructure for continuous improvement.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why training and inference are more interconnected than the industry realizes&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Parsed example illuminates a deeper strategic rationale for Baseten&amp;#x27;s training expansion: the boundary between training and inference is blurrier than conventional wisdom suggests.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s model performance team uses the training platform extensively to create &amp;quot;draft models&amp;quot; for speculative decoding, a cutting-edge technique that can dramatically accelerate inference. The company recently announced it achieved 650+ tokens per second on OpenAI&amp;#x27;s &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;&lt;u&gt;GPT OSS 120B model&lt;/u&gt;&lt;/a&gt; — a 60% improvement over its launch performance — using &lt;a href="https://arxiv.org/abs/2503.01840"&gt;&lt;u&gt;EAGLE-3&lt;/u&gt;&lt;/a&gt; speculative decoding, which requires training specialized small models to work alongside larger target models.&lt;/p&gt;&lt;p&gt;&amp;quot;Ultimately, inference and training plug in more ways than one might think,&amp;quot; Haghighat said. &amp;quot;When you do speculative decoding in inference, you need to train the draft model. Our model performance team is a big customer of the training product to train these EAGLE heads on a continuous basis.&amp;quot;&lt;/p&gt;&lt;p&gt;This technical interdependence reinforces Baseten&amp;#x27;s thesis that owning both training and inference creates defensible value. The company can optimize the entire lifecycle: a model trained on Baseten can be deployed with a single click to inference endpoints pre-optimized for that architecture, with deployment-from-checkpoint support for chat completion and audio transcription workloads.&lt;/p&gt;&lt;p&gt;The approach contrasts sharply with vertically integrated competitors like &lt;a href="https://replicate.com/"&gt;&lt;u&gt;Replicate&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://modal.com/"&gt;&lt;u&gt;Modal&lt;/u&gt;&lt;/a&gt;, which also offer training and inference but with different architectural tradeoffs. Baseten&amp;#x27;s bet is on lower-level infrastructure flexibility and performance optimization, particularly for companies running custom models at scale.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;As open-source AI models improve, enterprises see fine-tuning as the path away from OpenAI dependency&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Underpinning Baseten&amp;#x27;s entire strategy is a conviction about the trajectory of open-source AI models — namely, that they&amp;#x27;re getting good enough, fast enough, to unlock massive enterprise adoption through fine-tuning.&lt;/p&gt;&lt;p&gt;&amp;quot;Both closed and open-source models are getting better and better in terms of quality,&amp;quot; Haghighat said. &amp;quot;We don&amp;#x27;t even need open source to surpass closed models, because as both of them are getting better, they unlock all these invisible lines of usefulness for different use cases.&amp;quot;&lt;/p&gt;&lt;p&gt;He pointed to the proliferation of reinforcement learning and supervised fine-tuning techniques that allow companies to take an open-source model and make it &amp;quot;as good as the closed model, not at everything, but at this narrow band of capability that they want.&amp;quot;&lt;/p&gt;&lt;p&gt;That trend is already visible in Baseten&amp;#x27;s &lt;a href="https://www.baseten.co/products/model-apis/"&gt;&lt;u&gt;Model APIs business&lt;/u&gt;&lt;/a&gt;, launched alongside Training earlier this year to provide production-grade access to open-source models. The company was the first provider to offer access to &lt;a href="https://api-docs.deepseek.com/news/news1226"&gt;&lt;u&gt;DeepSeek V3&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://api-docs.deepseek.com/news/news250120"&gt;&lt;u&gt;R1&lt;/u&gt;&lt;/a&gt;, and has since added models like &lt;a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/"&gt;&lt;u&gt;Llama 4&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://qwen.ai/research"&gt;&lt;u&gt;Qwen 3&lt;/u&gt;&lt;/a&gt;, optimized for performance and reliability. Model APIs serves as a top-of-funnel product: companies start with off-the-shelf open-source models, realize they need customization, move to Training for fine-tuning, and ultimately deploy on Baseten&amp;#x27;s &lt;a href="https://www.baseten.co/products/dedicated-deployments/"&gt;&lt;u&gt;Dedicated Deployments&lt;/u&gt;&lt;/a&gt; infrastructure.&lt;/p&gt;&lt;p&gt;Yet Haghighat acknowledged the market remains &amp;quot;fuzzy&amp;quot; around which training techniques will dominate. Baseten is hedging by staying close to the bleeding edge through its &lt;a href="https://www.baseten.co/blog/forward-deployed-engineering/"&gt;&lt;u&gt;Forward Deployed Engineering team&lt;/u&gt;&lt;/a&gt;, which works hands-on with select customers on reinforcement learning, supervised fine-tuning, and other advanced techniques.&lt;/p&gt;&lt;p&gt;&amp;quot;As we do that, we will see patterns emerge about what a productized training product can look like that really addresses the user&amp;#x27;s needs without them having to learn too much about how RL works,&amp;quot; he said. &amp;quot;Are we there as an industry? I would say not quite. I see some attempts at that, but they all seem like almost falling to the same trap that Blueprints fell into—a bit of a walled garden that ties the hands of AI folks behind their back.&amp;quot;&lt;/p&gt;&lt;p&gt;The roadmap ahead includes potential abstractions for common training patterns, expansion into image, audio, and video fine-tuning, and deeper integration of advanced techniques like prefill-decode disaggregation, which separates the initial processing of prompts from token generation to improve efficiency.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Baseten faces crowded field but bets developer experience and performance will win enterprise customers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Baseten enters an increasingly crowded market for AI infrastructure. Hyperscalers like &lt;a href="https://aws.amazon.com/"&gt;&lt;u&gt;AWS&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://cloud.google.com/?hl=en"&gt;&lt;u&gt;Google Cloud&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://azure.microsoft.com/en-us/"&gt;&lt;u&gt;Microsoft Azure&lt;/u&gt;&lt;/a&gt; offer GPU compute for training, while specialized providers like Lambda Labs, CoreWeave, and Together AI compete on price, performance, or ease of use. Then there are vertically integrated platforms like &lt;a href="https://huggingface.co/"&gt;&lt;u&gt;Hugging Face&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://replicate.com/"&gt;&lt;u&gt;Replicate&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://modal.com/"&gt;&lt;u&gt;Modal&lt;/u&gt;&lt;/a&gt; that bundle training, inference, and model hosting.&lt;/p&gt;&lt;p&gt;Baseten&amp;#x27;s differentiation rests on three pillars: its MCM system for multi-cloud capacity management, deep performance optimization expertise built from its inference business, and a developer experience tailored for production deployments rather than experimentation.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s recent &lt;a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"&gt;&lt;u&gt;$150 million Series D&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.baseten.co/blog/announcing-baseten-150m-series-d/"&gt;&lt;u&gt;$2.15 billion valuation&lt;/u&gt;&lt;/a&gt; provide runway to invest in both products simultaneously. Major customers include &lt;a href="https://www.descript.com/"&gt;&lt;u&gt;Descript&lt;/u&gt;&lt;/a&gt;, which uses Baseten for transcription workloads; &lt;a href="https://decagon.ai/"&gt;&lt;u&gt;Decagon&lt;/u&gt;&lt;/a&gt;, which runs customer service AI; and &lt;a href="https://sourcegraph.com/"&gt;&lt;u&gt;Sourcegraph&lt;/u&gt;&lt;/a&gt;, which powers coding assistants. All three operate in domains where model customization and performance are competitive advantages.&lt;/p&gt;&lt;p&gt;Timing may be Baseten&amp;#x27;s biggest asset. The confluence of improving open-source models, enterprise discomfort with dependence on proprietary AI providers, and growing sophistication around fine-tuning techniques creates what Haghighat sees as a sustainable market shift.&lt;/p&gt;&lt;p&gt;&amp;quot;There is a lot of use cases for which closed models have gotten there and open ones have not,&amp;quot; he said. &amp;quot;Where I&amp;#x27;m seeing in the market is people using different training techniques — more recently, a lot of reinforcement learning and SFT — to be able to get this open model to be as good as the closed model, not at everything, but at this narrow band of capability that they want. That&amp;#x27;s very palpable in the market.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprises navigating the complex transition from closed to open AI models, Baseten&amp;#x27;s positioning offers a clear value proposition: infrastructure that handles the messy middle of fine-tuning while optimizing for the ultimate goal of performant, reliable, cost-effective inference at scale. The company&amp;#x27;s insistence that customers own their model weights — a stark contrast to competitors using training as a lock-in mechanism — reflects confidence that technical excellence, not contractual restrictions, will drive retention.&lt;/p&gt;&lt;p&gt;Whether Baseten can execute on this vision depends on navigating tensions inherent in its strategy: staying at the infrastructure layer without becoming consultants, providing power and flexibility without overwhelming users with complexity, and building abstractions at exactly the right level as the market matures. The company&amp;#x27;s willingness to kill Blueprints when it failed suggests a pragmatism that could prove decisive in a market where many infrastructure providers over-promise and under-deliver.&lt;/p&gt;&lt;p&gt;&amp;quot;Through and through, we&amp;#x27;re an inference company,&amp;quot; Haghighat emphasized. &amp;quot;The reason that we did training is at the service of inference.&amp;quot;&lt;/p&gt;&lt;p&gt;That clarity of purpose — treating training as a means to an end rather than an end in itself—may be Baseten&amp;#x27;s most important strategic asset. As AI deployment matures from experimentation to production, the companies that solve the full stack stand to capture outsized value. But only if they avoid the trap of technology in search of a problem.&lt;/p&gt;&lt;p&gt;At least Baseten&amp;#x27;s customers no longer have to SSH into boxes on Friday and pray their training jobs complete by Monday. In the infrastructure business, sometimes the best innovation is simply making the painful parts disappear.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/baseten-takes-on-hyperscalers-with-new-ai-training-platform-that-lets-you</guid><pubDate>Mon, 10 Nov 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Reimagining cybersecurity in the era of AI and quantum (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/10/1127774/reimagining-cybersecurity-in-the-era-of-ai-and-quantum/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;CISCO&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;AI and quantum technologies are dramatically reconfiguring how cybersecurity functions, redefining the speed and scale with which digital defenders and their adversaries can operate.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127802" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/CISCO-iStock-2156608663.jpg" /&gt;&lt;/figure&gt;  &lt;p&gt;The weaponization of AI tools for cyberattacks is already proving a worthy opponent to current defenses. From reconnaissance to ransomware, cybercriminals can automate attacks faster than ever before with AI. This includes using generative AI to create social engineering attacks at scale, churning out tens of thousands of tailored phishing emails in seconds, or accessing widely available voice cloning software capable of bypassing security defenses for as little as a few dollars. And now, agentic AI raises the stakes by introducing autonomous systems that can reason, act, and adapt like human adversaries.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;But AI isn’t the only force shaping the threat landscape. Quantum computing has the potential to seriously undermine current encryption standards if developed unchecked. Quantum algorithms can solve the mathematical problems underlying most modern cryptography, particularly public-key systems like RSA and Elliptic Curve, widely used for secure online communication, digital signatures, and cryptocurrency.&lt;/p&gt;  &lt;p&gt;“We know quantum is coming. Once it does, it will force a change in how we secure data across everything, including governments, telecoms, and financial systems,” says Peter Bailey, senior vice president and general manager of Cisco’s security business.&lt;/p&gt; 
 &lt;p&gt;“Most organizations are understandably focused on the immediacy of AI threats," says Bailey. “Quantum might sound like science fiction, but those scenarios are coming faster than many realize. It’s critical to start investing now in defenses that can withstand both AI and quantum attacks.”&lt;/p&gt;  &lt;p&gt;Critical to this defense is a zero trust approach to cybersecurity, which assumes no user or device can be inherently trusted. By enforcing continuous verification, zero trust enables constant monitoring and ensures that any attempts to exploit vulnerabilities are quickly detected and addressed in real time. This approach is technology-agnostic and creates a resilient framework even in the face of an ever-changing threat landscape.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Putting up AI defenses&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;AI is lowering the barrier to entry for cyberattacks, enabling hackers even with limited skills or resources to infiltrate, manipulate, and exploit the slightest digital vulnerability.&lt;/p&gt;  &lt;p&gt;Nearly three-quarters (74%) of cybersecurity professionals say AI-enabled threats are already having a significant impact on their organization, and 90% anticipate such threats in the next one to two years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“AI-powered adversaries have advanced techniques and operate at machine speed,” says Bailey. “The only way to keep pace is to use AI to automate response and defend at machine speed.”&lt;/p&gt;  &lt;p&gt;To do this, Bailey says, organizations must modernize systems, platforms, and security operations to automate threat detection and response—processes that have previously relied on human rule-writing and reaction times. These systems must adapt dynamically as environments evolve and criminal tactics change.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;At the same time, companies must strengthen the security of their AI models and data to reduce exposure to manipulation from AI-enabled malware. Such risks could include, for instance, prompt injections, where a malicious user crafts a prompt to manipulate an AI model into performing unintended actions, bypassing its original instructions and safeguards.&lt;/p&gt;  &lt;p&gt;Agentic AI further ups the ante, with hackers able to use AI agents to automate attacks and make tactical decisions without constant human oversight. “Agentic AI has the potential to collapse the cost of the kill chain,” says Bailey. “That means everyday cybercriminals could start executing campaigns that today only well-funded espionage operations can afford.”&lt;/p&gt;  &lt;p&gt;Organizations, in turn, are exploring how AI agents can help them stay ahead. Nearly 40% of companies expect agentic AI to augment or assist teams over the next 12 months, especially in cybersecurity, according to Cisco’s 2025 AI Readiness Index. Use cases include AI agents trained on telemetry, which can identify anomalies or signals from machine data too disparate and unstructured to be deciphered by humans.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Calculating the quantum threat&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;As many cybersecurity teams focus on the very real AI-driven threat, quantum is waiting on the sidelines. Almost three-quarters (73%) of US organizations surveyed by KPMG say they believe it is only a matter of time before cybercriminals are using quantum to decrypt and disrupt today’s cybersecurity protocols. And yet, the majority (81%) also admit they could do more to ensure that their data remains secure.&lt;/p&gt; 

 &lt;p&gt;Companies are right to be concerned. Threat actors are already carrying out harvest now, decrypt later attacks, stockpiling sensitive encrypted data to crack once quantum technology matures. Examples include state-sponsored actors intercepting government communications and cybercriminal networks storing encrypted internet traffic or financial records.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Large technology companies are among the first to roll out quantum defenses. For example, Apple is using cryptography protocol PQ3 to defend against harvest now, decrypt later attacks on its iMessage platform. Google is testing post-quantum cryptography (PQC)—which is resistant to attacks from both quantum and classical computers—in its Chrome browser. And Cisco “has made significant investments in quantum-proofing our software and infrastructure,” says Bailey. “You’ll see more enterprises and governments taking similar steps over the next 18 to 24 months,” he adds.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As regulations like the US Quantum Computing Cybersecurity Preparedness Act lay out requirements for mitigating against quantum threats, including standardized PQC algorithms by the National Institute of Standards and Technology, a wider range of organizations will start preparing their own quantum defenses.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For organizations beginning that journey, Bailey outlines two key actions. First, establish visibility. “Understand what data you have and where it lives,” he says. “Take inventory, assess sensitivity, and review your encryption keys, rotating out any that are weak or outdated.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Second, plan for migration. “Next, assess what it will take to support post-quantum algorithms across your infrastructure. That means addressing not just the technology, but also the process and people implications,” Bailey says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Adopting proactive defense&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Ultimately, the foundation for building resilience against both AI and quantum is a zero trust approach, says Bailey. By embedding zero trust access controls across users, devices, business applications, networks, and clouds, this approach grants only the minimum access required to complete a task and enables continuous monitoring. It can also minimize the attack surface by confining a potential threat to an isolated zone, preventing it from accessing other critical systems.&lt;/p&gt;  &lt;p&gt;Into this zero trust architecture, organizations can integrate specific measures to defend against AI and quantum risks. For instance, quantum-immune cryptography and AI-powered analytics and security tools can be used to identify complex attack patterns and automate real-time responses.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Zero trust slows down attacks and builds resilience,” Bailey says. “It ensures that even if a breach occurs, the crown jewels stay protected and operations can recover quickly.”&lt;/p&gt; 
 &lt;p&gt;Ultimately, companies should not wait for threats to emerge and evolve. They must get ahead now. “This isn’t a what-if scenario; it’s a when,” says Bailey. “Organizations that invest early will be the ones setting the pace, not scrambling to catch up.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;CISCO&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;AI and quantum technologies are dramatically reconfiguring how cybersecurity functions, redefining the speed and scale with which digital defenders and their adversaries can operate.&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127802" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/CISCO-iStock-2156608663.jpg" /&gt;&lt;/figure&gt;  &lt;p&gt;The weaponization of AI tools for cyberattacks is already proving a worthy opponent to current defenses. From reconnaissance to ransomware, cybercriminals can automate attacks faster than ever before with AI. This includes using generative AI to create social engineering attacks at scale, churning out tens of thousands of tailored phishing emails in seconds, or accessing widely available voice cloning software capable of bypassing security defenses for as little as a few dollars. And now, agentic AI raises the stakes by introducing autonomous systems that can reason, act, and adapt like human adversaries.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;But AI isn’t the only force shaping the threat landscape. Quantum computing has the potential to seriously undermine current encryption standards if developed unchecked. Quantum algorithms can solve the mathematical problems underlying most modern cryptography, particularly public-key systems like RSA and Elliptic Curve, widely used for secure online communication, digital signatures, and cryptocurrency.&lt;/p&gt;  &lt;p&gt;“We know quantum is coming. Once it does, it will force a change in how we secure data across everything, including governments, telecoms, and financial systems,” says Peter Bailey, senior vice president and general manager of Cisco’s security business.&lt;/p&gt; 
 &lt;p&gt;“Most organizations are understandably focused on the immediacy of AI threats," says Bailey. “Quantum might sound like science fiction, but those scenarios are coming faster than many realize. It’s critical to start investing now in defenses that can withstand both AI and quantum attacks.”&lt;/p&gt;  &lt;p&gt;Critical to this defense is a zero trust approach to cybersecurity, which assumes no user or device can be inherently trusted. By enforcing continuous verification, zero trust enables constant monitoring and ensures that any attempts to exploit vulnerabilities are quickly detected and addressed in real time. This approach is technology-agnostic and creates a resilient framework even in the face of an ever-changing threat landscape.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Putting up AI defenses&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;AI is lowering the barrier to entry for cyberattacks, enabling hackers even with limited skills or resources to infiltrate, manipulate, and exploit the slightest digital vulnerability.&lt;/p&gt;  &lt;p&gt;Nearly three-quarters (74%) of cybersecurity professionals say AI-enabled threats are already having a significant impact on their organization, and 90% anticipate such threats in the next one to two years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“AI-powered adversaries have advanced techniques and operate at machine speed,” says Bailey. “The only way to keep pace is to use AI to automate response and defend at machine speed.”&lt;/p&gt;  &lt;p&gt;To do this, Bailey says, organizations must modernize systems, platforms, and security operations to automate threat detection and response—processes that have previously relied on human rule-writing and reaction times. These systems must adapt dynamically as environments evolve and criminal tactics change.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;At the same time, companies must strengthen the security of their AI models and data to reduce exposure to manipulation from AI-enabled malware. Such risks could include, for instance, prompt injections, where a malicious user crafts a prompt to manipulate an AI model into performing unintended actions, bypassing its original instructions and safeguards.&lt;/p&gt;  &lt;p&gt;Agentic AI further ups the ante, with hackers able to use AI agents to automate attacks and make tactical decisions without constant human oversight. “Agentic AI has the potential to collapse the cost of the kill chain,” says Bailey. “That means everyday cybercriminals could start executing campaigns that today only well-funded espionage operations can afford.”&lt;/p&gt;  &lt;p&gt;Organizations, in turn, are exploring how AI agents can help them stay ahead. Nearly 40% of companies expect agentic AI to augment or assist teams over the next 12 months, especially in cybersecurity, according to Cisco’s 2025 AI Readiness Index. Use cases include AI agents trained on telemetry, which can identify anomalies or signals from machine data too disparate and unstructured to be deciphered by humans.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Calculating the quantum threat&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;As many cybersecurity teams focus on the very real AI-driven threat, quantum is waiting on the sidelines. Almost three-quarters (73%) of US organizations surveyed by KPMG say they believe it is only a matter of time before cybercriminals are using quantum to decrypt and disrupt today’s cybersecurity protocols. And yet, the majority (81%) also admit they could do more to ensure that their data remains secure.&lt;/p&gt; 

 &lt;p&gt;Companies are right to be concerned. Threat actors are already carrying out harvest now, decrypt later attacks, stockpiling sensitive encrypted data to crack once quantum technology matures. Examples include state-sponsored actors intercepting government communications and cybercriminal networks storing encrypted internet traffic or financial records.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Large technology companies are among the first to roll out quantum defenses. For example, Apple is using cryptography protocol PQ3 to defend against harvest now, decrypt later attacks on its iMessage platform. Google is testing post-quantum cryptography (PQC)—which is resistant to attacks from both quantum and classical computers—in its Chrome browser. And Cisco “has made significant investments in quantum-proofing our software and infrastructure,” says Bailey. “You’ll see more enterprises and governments taking similar steps over the next 18 to 24 months,” he adds.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As regulations like the US Quantum Computing Cybersecurity Preparedness Act lay out requirements for mitigating against quantum threats, including standardized PQC algorithms by the National Institute of Standards and Technology, a wider range of organizations will start preparing their own quantum defenses.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For organizations beginning that journey, Bailey outlines two key actions. First, establish visibility. “Understand what data you have and where it lives,” he says. “Take inventory, assess sensitivity, and review your encryption keys, rotating out any that are weak or outdated.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Second, plan for migration. “Next, assess what it will take to support post-quantum algorithms across your infrastructure. That means addressing not just the technology, but also the process and people implications,” Bailey says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Adopting proactive defense&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Ultimately, the foundation for building resilience against both AI and quantum is a zero trust approach, says Bailey. By embedding zero trust access controls across users, devices, business applications, networks, and clouds, this approach grants only the minimum access required to complete a task and enables continuous monitoring. It can also minimize the attack surface by confining a potential threat to an isolated zone, preventing it from accessing other critical systems.&lt;/p&gt;  &lt;p&gt;Into this zero trust architecture, organizations can integrate specific measures to defend against AI and quantum risks. For instance, quantum-immune cryptography and AI-powered analytics and security tools can be used to identify complex attack patterns and automate real-time responses.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Zero trust slows down attacks and builds resilience,” Bailey says. “It ensures that even if a breach occurs, the crown jewels stay protected and operations can recover quickly.”&lt;/p&gt; 
 &lt;p&gt;Ultimately, companies should not wait for threats to emerge and evolve. They must get ahead now. “This isn’t a what-if scenario; it’s a when,” says Bailey. “Organizations that invest early will be the ones setting the pace, not scrambling to catch up.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/10/1127774/reimagining-cybersecurity-in-the-era-of-ai-and-quantum/</guid><pubDate>Mon, 10 Nov 2025 14:19:28 +0000</pubDate></item><item><title>[NEW] How context engineering can save your company from AI vibe code overload: lessons from Qodo and Monday.com (AI | VentureBeat)</title><link>https://venturebeat.com/ai/how-context-engineering-can-save-your-company-from-ai-vibe-code-overload</link><description>[unable to retrieve full-text content]&lt;p&gt;As cloud project tracking software &lt;a href="https://monday.com/"&gt;monday.com&lt;/a&gt;’s engineering organization scaled past 500 developers, the team began to feel the strain of its own success. Product lines were multiplying, microservices proliferating, and code was flowing faster than human reviewers could keep up. The company needed a way to review thousands of pull requests each month without drowning developers in tedium — or letting quality slip.&lt;/p&gt;&lt;p&gt;That’s when Guy Regev, VP of R&amp;amp;D and head of the Growth and monday Dev teams, started experimenting with a new AI tool from &lt;a href="https://www.qodo.ai/"&gt;Qodo&lt;/a&gt;, an Israeli startup focused on developer agents. What began as a lightweight test soon became a critical part of monday.com’s software delivery infrastructure, as a &lt;a href="https://www.qodo.ai/blog/monday-com-accelerates-review-cycles-and-improves-code-quality-with-qodo/"&gt;new case study&lt;/a&gt; released by both Qodo and monday.com today reveals. &lt;/p&gt;&lt;p&gt;“Qodo doesn’t feel like just another tool—it’s like adding a new developer to the team who actually learns how we work,&amp;quot; Regev told VentureBeat in a recent video call interview, adding that it has &amp;quot;prevented over 800 issues per month from reaching production—some of them could have caused serious security vulnerabilities.&amp;quot;&lt;/p&gt;&lt;p&gt;Unlike code generation tools like GitHub Copilot or Cursor, Qodo isn’t trying to write new code. Instead, it specializes in reviewing it — using what it calls &lt;b&gt;context engineering&lt;/b&gt; to understand not just what changed in a pull request, but why, how it aligns with business logic, and whether it follows internal best practices. &lt;/p&gt;&lt;p&gt;&amp;quot;You can call Claude Code or Cursor and in five minutes get 1,000 lines of code,&amp;quot; said Itamar Friedman, co-founder and CEO of Qodo, in the same video call interview as with Regev. &amp;quot;You have 40 minutes, and you can&amp;#x27;t review that. So you need Qodo to actually review it.”&lt;/p&gt;&lt;p&gt;For monday.com, this capability wasn’t just helpful — it was transformative.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Code Review, at Scale&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At any given time, monday.com’s developers are shipping updates across hundreds of repositories and services. The engineering org works in tightly coordinated teams, each aligned with specific parts of the product: marketing, CRM, dev tools, internal platforms, and more.&lt;/p&gt;&lt;p&gt;That’s where Qodo came in. The company’s platform uses AI not just to check for obvious bugs or style violations, but to evaluate whether a pull request follows team-specific conventions, architectural guidelines, and historical patterns. &lt;/p&gt;&lt;p&gt;It does this by learning from your own codebase — training on previous PRs, comments, merges, and even Slack threads to understand how your team works.&lt;/p&gt;&lt;p&gt;&amp;quot;The comments Qodo gives aren’t generic—they reflect our values, our libraries, even our standards for things like feature flags and privacy,&amp;quot; Regev said. &amp;quot;It’s context-aware in a way traditional tools aren’t.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What “Context Engineering” Actually Means&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Qodo calls its secret sauce &lt;b&gt;context engineering&lt;/b&gt; — a system-level approach to managing everything the model sees when making a decision.&lt;/p&gt;&lt;p&gt; This includes the PR code diff, of course, but also prior discussions, documentation, relevant files from the repo, even test results and configuration data.&lt;/p&gt;&lt;p&gt;The idea is that language models don’t really “think” — they predict the next token based on the inputs they’re given. So the quality of their output depends almost entirely on the quality and structure of their inputs.&lt;/p&gt;&lt;p&gt;As Dana Fine, Qodo’s community manager, put it in a&lt;a href="https://www.qodo.ai/blog/context-engineering/"&gt; blog post&lt;/a&gt;: “You’re not just writing prompts; you’re designing structured input under a fixed token limit. Every token is a design decision.”&lt;/p&gt;&lt;p&gt;This isn’t just theory. In monday.com’s case, it meant Qodo could catch not only the obvious bugs, but the subtle ones that typically slip past human reviewers — hardcoded variables, missing fallbacks, or violations of cross-team architecture conventions.&lt;/p&gt;&lt;p&gt;One example stood out. In a recent PR, Qodo flagged a line that inadvertently exposed a staging environment variable — something no human reviewer caught. Had it been merged, it might have caused problems in production. &lt;/p&gt;&lt;p&gt;&amp;quot;The hours we would spend on fixing this security leak and the legal issue that it would bring would be much more than the hours that we reduce from a pull-request,&amp;quot; said Regev.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Integration into the Pipeline&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Today, Qodo is deeply integrated into monday.com’s development workflow, analyzing pull requests and surfacing context-aware recommendations based on prior team code reviews. &lt;/p&gt;&lt;p&gt;“It doesn’t feel like just another tool... It feels like another teammate that joined the system — one who learns how we work,&amp;quot; Regev noted. &lt;/p&gt;&lt;p&gt;Developers receive suggestions during the review process and remain in control of final decisions — a human-in-the-loop model that was critical for adoption.&lt;/p&gt;&lt;p&gt;Because Qodo integrated directly into GitHub via pull request actions and comments, Monday.com’s infrastructure team didn’t face a steep learning curve.&lt;/p&gt;&lt;p&gt;“It’s just a GitHub action,” said Regev. “It creates a PR with the tests. It’s not like a separate tool we had to learn.”&lt;/p&gt;&lt;p&gt;“The purpose is to actually help the developer learn the code, take ownership, give feedback to each other, and learn from that and establish the standards,&amp;quot; added Friedman.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The Results: Time Saved, Bugs Prevented&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Since rolling out Qodo more broadly, monday.com has seen measurable improvements across multiple teams.&lt;/p&gt;&lt;p&gt;Internal analysis shows that developers save roughly an hour per pull request on average. Multiply that across thousands of PRs per month, and the savings quickly reach thousands of developer hours annually.&lt;/p&gt;&lt;p&gt;These aren’t just cosmetic issues — many relate to business logic, security, or runtime stability. And because Qodo’s suggestions reflect monday.com’s actual conventions, developers are more likely to act on them.&lt;/p&gt;&lt;p&gt;The system’s accuracy is rooted in its data-first design. Qodo trains on each company’s private codebase and historical data, adapting to different team styles and practices. It doesn’t rely on one-size-fits-all rules or external datasets. Everything is tailored.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From Internal Tool to Product Vision&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Regev’s team was so impressed with Qodo’s impact that they’ve started planning deeper integrations between Qodo and Monday Dev, the developer-focused product line monday.com is building.&lt;/p&gt;&lt;p&gt;The vision is to create a workflow where business context — tasks, tickets, customer feedback — flows directly into the code review layer. That way, reviewers can assess not just whether the code “works,” but whether it solves the right problem.&lt;/p&gt;&lt;p&gt;“Before, we had linters, danger rules, static analysis... rule-based... you need to configure all the rules,&amp;quot; Regev said. &amp;quot;But it doesn’t know what you don’t know... Qodo... feels like it’s learning from our engineers.”&lt;/p&gt;&lt;p&gt;This aligns closely with Qodo’s own roadmap. The company doesn’t just review code. It’s building a full platform of developer agents — including Qodo Gen for context-aware code generation, Qodo Merge for automated PR analysis, and Qodo Cover, a regression-testing agent that uses runtime validation to ensure test coverage.&lt;/p&gt;&lt;p&gt;All of this is powered by Qodo’s own infrastructure, including its new open-source embedding model, Qodo-Embed-1-1.5B, which outperformed offerings from OpenAI and Salesforce on code retrieval benchmarks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What’s Next?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Qodo is now offering its platform under a freemium model — free for individuals, discounted for startups through Google Cloud’s Perks program, and enterprise-grade for companies that need SSO, air-gapped deployment, or advanced controls.&lt;/p&gt;&lt;p&gt;The company is already working with teams at NVIDIA, Intuit, and other Fortune 500 companies. And thanks to a recent partnership with Google Cloud, Qodo’s models are available directly inside Vertex AI’s Model Garden, making it easier to integrate into enterprise pipelines.&lt;/p&gt;&lt;p&gt;&amp;quot;Context engines will be the big story of 2026,&amp;quot; Friedman said. &amp;quot;Every enterprise will need to build their own second brain if they want AI that actually understands and helps them.&amp;quot;&lt;/p&gt;&lt;p&gt;As AI systems become more embedded in software development, tools like Qodo are showing how the right context — delivered at the right moment — can transform how teams build, ship, and scale code across the enterprise.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;As cloud project tracking software &lt;a href="https://monday.com/"&gt;monday.com&lt;/a&gt;’s engineering organization scaled past 500 developers, the team began to feel the strain of its own success. Product lines were multiplying, microservices proliferating, and code was flowing faster than human reviewers could keep up. The company needed a way to review thousands of pull requests each month without drowning developers in tedium — or letting quality slip.&lt;/p&gt;&lt;p&gt;That’s when Guy Regev, VP of R&amp;amp;D and head of the Growth and monday Dev teams, started experimenting with a new AI tool from &lt;a href="https://www.qodo.ai/"&gt;Qodo&lt;/a&gt;, an Israeli startup focused on developer agents. What began as a lightweight test soon became a critical part of monday.com’s software delivery infrastructure, as a &lt;a href="https://www.qodo.ai/blog/monday-com-accelerates-review-cycles-and-improves-code-quality-with-qodo/"&gt;new case study&lt;/a&gt; released by both Qodo and monday.com today reveals. &lt;/p&gt;&lt;p&gt;“Qodo doesn’t feel like just another tool—it’s like adding a new developer to the team who actually learns how we work,&amp;quot; Regev told VentureBeat in a recent video call interview, adding that it has &amp;quot;prevented over 800 issues per month from reaching production—some of them could have caused serious security vulnerabilities.&amp;quot;&lt;/p&gt;&lt;p&gt;Unlike code generation tools like GitHub Copilot or Cursor, Qodo isn’t trying to write new code. Instead, it specializes in reviewing it — using what it calls &lt;b&gt;context engineering&lt;/b&gt; to understand not just what changed in a pull request, but why, how it aligns with business logic, and whether it follows internal best practices. &lt;/p&gt;&lt;p&gt;&amp;quot;You can call Claude Code or Cursor and in five minutes get 1,000 lines of code,&amp;quot; said Itamar Friedman, co-founder and CEO of Qodo, in the same video call interview as with Regev. &amp;quot;You have 40 minutes, and you can&amp;#x27;t review that. So you need Qodo to actually review it.”&lt;/p&gt;&lt;p&gt;For monday.com, this capability wasn’t just helpful — it was transformative.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Code Review, at Scale&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At any given time, monday.com’s developers are shipping updates across hundreds of repositories and services. The engineering org works in tightly coordinated teams, each aligned with specific parts of the product: marketing, CRM, dev tools, internal platforms, and more.&lt;/p&gt;&lt;p&gt;That’s where Qodo came in. The company’s platform uses AI not just to check for obvious bugs or style violations, but to evaluate whether a pull request follows team-specific conventions, architectural guidelines, and historical patterns. &lt;/p&gt;&lt;p&gt;It does this by learning from your own codebase — training on previous PRs, comments, merges, and even Slack threads to understand how your team works.&lt;/p&gt;&lt;p&gt;&amp;quot;The comments Qodo gives aren’t generic—they reflect our values, our libraries, even our standards for things like feature flags and privacy,&amp;quot; Regev said. &amp;quot;It’s context-aware in a way traditional tools aren’t.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What “Context Engineering” Actually Means&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Qodo calls its secret sauce &lt;b&gt;context engineering&lt;/b&gt; — a system-level approach to managing everything the model sees when making a decision.&lt;/p&gt;&lt;p&gt; This includes the PR code diff, of course, but also prior discussions, documentation, relevant files from the repo, even test results and configuration data.&lt;/p&gt;&lt;p&gt;The idea is that language models don’t really “think” — they predict the next token based on the inputs they’re given. So the quality of their output depends almost entirely on the quality and structure of their inputs.&lt;/p&gt;&lt;p&gt;As Dana Fine, Qodo’s community manager, put it in a&lt;a href="https://www.qodo.ai/blog/context-engineering/"&gt; blog post&lt;/a&gt;: “You’re not just writing prompts; you’re designing structured input under a fixed token limit. Every token is a design decision.”&lt;/p&gt;&lt;p&gt;This isn’t just theory. In monday.com’s case, it meant Qodo could catch not only the obvious bugs, but the subtle ones that typically slip past human reviewers — hardcoded variables, missing fallbacks, or violations of cross-team architecture conventions.&lt;/p&gt;&lt;p&gt;One example stood out. In a recent PR, Qodo flagged a line that inadvertently exposed a staging environment variable — something no human reviewer caught. Had it been merged, it might have caused problems in production. &lt;/p&gt;&lt;p&gt;&amp;quot;The hours we would spend on fixing this security leak and the legal issue that it would bring would be much more than the hours that we reduce from a pull-request,&amp;quot; said Regev.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Integration into the Pipeline&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Today, Qodo is deeply integrated into monday.com’s development workflow, analyzing pull requests and surfacing context-aware recommendations based on prior team code reviews. &lt;/p&gt;&lt;p&gt;“It doesn’t feel like just another tool... It feels like another teammate that joined the system — one who learns how we work,&amp;quot; Regev noted. &lt;/p&gt;&lt;p&gt;Developers receive suggestions during the review process and remain in control of final decisions — a human-in-the-loop model that was critical for adoption.&lt;/p&gt;&lt;p&gt;Because Qodo integrated directly into GitHub via pull request actions and comments, Monday.com’s infrastructure team didn’t face a steep learning curve.&lt;/p&gt;&lt;p&gt;“It’s just a GitHub action,” said Regev. “It creates a PR with the tests. It’s not like a separate tool we had to learn.”&lt;/p&gt;&lt;p&gt;“The purpose is to actually help the developer learn the code, take ownership, give feedback to each other, and learn from that and establish the standards,&amp;quot; added Friedman.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The Results: Time Saved, Bugs Prevented&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Since rolling out Qodo more broadly, monday.com has seen measurable improvements across multiple teams.&lt;/p&gt;&lt;p&gt;Internal analysis shows that developers save roughly an hour per pull request on average. Multiply that across thousands of PRs per month, and the savings quickly reach thousands of developer hours annually.&lt;/p&gt;&lt;p&gt;These aren’t just cosmetic issues — many relate to business logic, security, or runtime stability. And because Qodo’s suggestions reflect monday.com’s actual conventions, developers are more likely to act on them.&lt;/p&gt;&lt;p&gt;The system’s accuracy is rooted in its data-first design. Qodo trains on each company’s private codebase and historical data, adapting to different team styles and practices. It doesn’t rely on one-size-fits-all rules or external datasets. Everything is tailored.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From Internal Tool to Product Vision&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Regev’s team was so impressed with Qodo’s impact that they’ve started planning deeper integrations between Qodo and Monday Dev, the developer-focused product line monday.com is building.&lt;/p&gt;&lt;p&gt;The vision is to create a workflow where business context — tasks, tickets, customer feedback — flows directly into the code review layer. That way, reviewers can assess not just whether the code “works,” but whether it solves the right problem.&lt;/p&gt;&lt;p&gt;“Before, we had linters, danger rules, static analysis... rule-based... you need to configure all the rules,&amp;quot; Regev said. &amp;quot;But it doesn’t know what you don’t know... Qodo... feels like it’s learning from our engineers.”&lt;/p&gt;&lt;p&gt;This aligns closely with Qodo’s own roadmap. The company doesn’t just review code. It’s building a full platform of developer agents — including Qodo Gen for context-aware code generation, Qodo Merge for automated PR analysis, and Qodo Cover, a regression-testing agent that uses runtime validation to ensure test coverage.&lt;/p&gt;&lt;p&gt;All of this is powered by Qodo’s own infrastructure, including its new open-source embedding model, Qodo-Embed-1-1.5B, which outperformed offerings from OpenAI and Salesforce on code retrieval benchmarks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What’s Next?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Qodo is now offering its platform under a freemium model — free for individuals, discounted for startups through Google Cloud’s Perks program, and enterprise-grade for companies that need SSO, air-gapped deployment, or advanced controls.&lt;/p&gt;&lt;p&gt;The company is already working with teams at NVIDIA, Intuit, and other Fortune 500 companies. And thanks to a recent partnership with Google Cloud, Qodo’s models are available directly inside Vertex AI’s Model Garden, making it easier to integrate into enterprise pipelines.&lt;/p&gt;&lt;p&gt;&amp;quot;Context engines will be the big story of 2026,&amp;quot; Friedman said. &amp;quot;Every enterprise will need to build their own second brain if they want AI that actually understands and helps them.&amp;quot;&lt;/p&gt;&lt;p&gt;As AI systems become more embedded in software development, tools like Qodo are showing how the right context — delivered at the right moment — can transform how teams build, ship, and scale code across the enterprise.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/how-context-engineering-can-save-your-company-from-ai-vibe-code-overload</guid><pubDate>Mon, 10 Nov 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] Google Maps releases new AI tools that let you create interactive projects (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/google-maps-releases-new-ai-tools-to-let-you-create-interactive-projects/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google Maps is adding new AI features, including a builder agent and an MCP server — a tool that connects AI assistants to Google Maps’ technical documentation — to help developers and users create interactive projects using Maps data and code. The company said it is using Gemini models across the board to power these features.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among these new tools, the builder agent is a tool that, just like many other coding tools, lets you describe the kind of interactive map-based prototype you want to build in text and creates one for you. For instance, you can type “create a Street View tour of a city,” “create a map visualizing real-time weather in my region,” or “list pet-friendly hotels in the city.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Once the code is generated, you can export it, test the preview project using your own API keys as needed, or modify the project in Firebase Studio.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066365" height="482" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-10-at-10.17.41AM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The same tool also has a styling agent that lets users create a customized map to match a particular style format or theme. This could help brands create maps with specific color coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google already provides map data grounding via the Gemini API. The company is now introducing a similar feature, called Grounding Lite, which allows developers to ground their own AI models using Model Context Protocol (MCP), a standard that lets AI assistants connect to external data sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With this feature, AI assistants can answer questions like, “How far is the nearest grocery store?” The company is also shipping Contextual View, a low-code Google Maps component that can provide visual understanding to users for such questions. The feature can show a list, a map view, or a 3D display as an answer.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066366" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Contextual-view.jpeg?w=541" width="541" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding a code assistant toolkit, the MCP server, which connects with Google Maps’ documentation. Developers can use this connection to get answers about how to use the Google Maps API and data. Last month, the company launched extensions for Gemini’s command line tool to let developers access Maps data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066369" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Code-Assist.jpeg?w=549" width="549" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also trying to add more Gemini-powered features for Maps on the consumer side. Last week, it enabled users to use Gemini hands-free with Maps for navigation. For users in India, Google added incident alerts and speed limit data to the Maps app in select areas.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google Maps is adding new AI features, including a builder agent and an MCP server — a tool that connects AI assistants to Google Maps’ technical documentation — to help developers and users create interactive projects using Maps data and code. The company said it is using Gemini models across the board to power these features.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among these new tools, the builder agent is a tool that, just like many other coding tools, lets you describe the kind of interactive map-based prototype you want to build in text and creates one for you. For instance, you can type “create a Street View tour of a city,” “create a map visualizing real-time weather in my region,” or “list pet-friendly hotels in the city.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Once the code is generated, you can export it, test the preview project using your own API keys as needed, or modify the project in Firebase Studio.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066365" height="482" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-10-at-10.17.41AM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The same tool also has a styling agent that lets users create a customized map to match a particular style format or theme. This could help brands create maps with specific color coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google already provides map data grounding via the Gemini API. The company is now introducing a similar feature, called Grounding Lite, which allows developers to ground their own AI models using Model Context Protocol (MCP), a standard that lets AI assistants connect to external data sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With this feature, AI assistants can answer questions like, “How far is the nearest grocery store?” The company is also shipping Contextual View, a low-code Google Maps component that can provide visual understanding to users for such questions. The feature can show a list, a map view, or a 3D display as an answer.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066366" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Contextual-view.jpeg?w=541" width="541" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding a code assistant toolkit, the MCP server, which connects with Google Maps’ documentation. Developers can use this connection to get answers about how to use the Google Maps API and data. Last month, the company launched extensions for Gemini’s command line tool to let developers access Maps data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3066369" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Code-Assist.jpeg?w=549" width="549" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also trying to add more Gemini-powered features for Maps on the consumer side. Last week, it enabled users to use Gemini hands-free with Maps for navigation. For users in India, Google added incident alerts and speed limit data to the Maps app in select areas.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/google-maps-releases-new-ai-tools-to-let-you-create-interactive-projects/</guid><pubDate>Mon, 10 Nov 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] The State of AI: Energy is king, and the US is falling behind (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/10/1126805/the-state-of-ai-energy-is-king-and-the-us-is-falling-behind/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;Welcome back to&amp;nbsp;The State of AI, a new collaboration between the Financial Times and MIT Technology Review. Every Monday, writers from both publications debate one aspect of the generative AI revolution and how it is reshaping global power.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;This week, Casey Crownhart, senior reporter for energy at MIT Technology Review and Pilita Clark, FT's columnist, consider how China's rapid renewables buildout could help it leapfrog on AI progress.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127502" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Casey Crownhart writes&lt;/strong&gt;:&lt;/p&gt;  &lt;p&gt;In the age of AI, the biggest barrier to progress isn’t money but energy. That should be particularly worrying here in the US, where massive data centers are waiting to come online, and it doesn’t look as if the country will build the steady power supply or infrastructure needed to serve them all.&lt;/p&gt; 
 &lt;p&gt;It wasn’t always like this. For about a decade before 2020, data centers were able to offset increased demand with efficiency improvements. Now, though, electricity demand is ticking up in the US, with billions of queries to popular AI models each day—and efficiency gains aren’t keeping pace. With too little new power capacity coming online, the strain is starting to show: Electricity bills are ballooning for people who live in places where data centers place a growing load on the grid.&lt;/p&gt;  &lt;p&gt;If we want AI to have the chance to deliver on big promises without driving electricity prices sky-high for the rest of us, the US needs to learn some lessons from the rest of the world on energy abundance. Just look at China.&lt;/p&gt; 
 &lt;p&gt;China installed 429 GW of new power generation capacity in 2024, more than six times the net capacity added in the US during that time.&lt;/p&gt;  &lt;p&gt;China still generates much of its electricity with coal, but that makes up a declining share of the mix. Rather, the country is focused on installing solar, wind, nuclear, and gas at record rates.&lt;/p&gt;  &lt;p&gt;The US, meanwhile, is focused on reviving its ailing coal industry. Coal-fired power plants are polluting and, crucially, expensive to run. Aging plants in the US are also less reliable than they used to be, generating electricity just 42% of the time, compared with a 61% capacity factor in 2014.&lt;/p&gt;  &lt;p&gt;It’s not a great situation. And unless the US changes something, we risk becoming consumers as opposed to innovators in both energy and AI tech. Already, China earns more from exporting renewables than the US does from oil and gas exports.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Building and permitting new renewable power plants would certainly help, since they’re currently the cheapest and fastest to bring online. But wind and solar are politically unpopular with the current administration. Natural gas is an obvious candidate, though there are concerns about delays with key equipment.&lt;/p&gt;  &lt;p&gt;One quick fix would be for data centers to be more flexible. If they agreed not to suck electricity from the grid during times of stress, new AI infrastructure might be able to come online without any new energy infrastructure.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;One study from Duke University found that if data centers agree to curtail their consumption just 0.25% of the time (roughly 22 hours over the course of the year), the grid could provide power for about 76 GW of new demand. That’s like adding about 5% of the entire grid’s capacity without needing to build anything new.&lt;/p&gt;  &lt;p&gt;But flexibility wouldn’t be enough to truly meet the swell in AI electricity demand. What do you think, Pilita? What would get the US out of these energy constraints? Is there anything else we should be thinking about when it comes to AI and its energy use?&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;Pilita Clark responds:&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I agree. Data centers that can cut their power use at times of grid stress should be the norm, not the exception. Likewise, we need more deals like those giving cheaper electricity to data centers that let power utilities access their backup generators. Both reduce the need to build more power plants, which makes sense regardless of how much electricity AI ends up using.&lt;/p&gt;  &lt;p&gt;This is a critical point for countries across the world, because we still don’t know exactly how much power AI is going to consume.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Forecasts for what data centers will need in as little as five years’ time vary wildly, from less than twice today’s rates to four times as much.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;This is partly because there’s a lack of public data about AI systems’ energy needs. It’s also because we don’t know how much more efficient these systems will become. The US chip designer Nvidia said last year that its specialized chips had become 45,000 times more energy efficient over the previous eight years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Moreover, we have been very wrong about tech energy needs before. At the height of the dot-com boom in 1999, it was erroneously claimed that the internet would need half the US’s electricity within a decade—necessitating a lot more coal power.&lt;/p&gt;  &lt;p&gt;Still, some countries are clearly feeling the pressure already. In Ireland, data centers chew up so much power that new connections have been restricted around Dublin to avoid straining the grid.&lt;/p&gt;  &lt;p&gt;Some regulators are eyeing new rules forcing tech companies to provide enough power generation to match their demand. I hope such efforts grow. I also hope AI itself helps boost power abundance and, crucially, accelerates the global energy transition needed to combat climate change. OpenAI’s Sam Altman said in 2023 that “once we have a really powerful super intelligence, addressing climate change will not be particularly difficult.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The evidence so far is not promising, especially in the US, where renewable projects are being axed. Still, the US may end up being an outlier in a world where ever cheaper renewables made up more than 90% of new power capacity added globally last year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Europe is aiming to power one of its biggest data centers predominantly with renewables and batteries. But the country leading the green energy expansion is clearly China.&lt;/p&gt; 
 &lt;p&gt;The 20th century was dominated by countries rich in the fossil fuels whose reign the US now wants to prolong. China, in contrast, may become the world’s first green electrostate. If it does this in a way that helps it win an AI race the US has so far controlled, it will mark a striking chapter in economic, technological, and geopolitical history.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Casey Crownhart replies:&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;I share your skepticism of tech executives’ claims that AI will be a groundbreaking help in the race to address climate change. To be fair, AI is progressing rapidly. But we don’t have time to wait for technologies standing on big claims with nothing to back them up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When it comes to the grid, for example, experts say there’s potential for AI to help with planning and even operating, but these efforts are still experimental.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, much of the world is making measurable progress on transitioning to newer, greener forms of energy. How that will affect the AI boom remains to be seen. What is clear is that AI is changing our grid and our world, and we need to be clear-eyed about the consequences.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Further reading&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt; reporters did the math on the energy needs of an AI query.&lt;/p&gt;  &lt;p&gt;There are still a few reasons to be optimistic about AI’s energy demands.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The &lt;em&gt;FT&lt;/em&gt;’s visual data team take a look&lt;strong&gt; &lt;/strong&gt;inside the relentless race for AI capacity.&lt;/p&gt;  &lt;p&gt;And global &lt;em&gt;FT&lt;/em&gt; reporters ask whether data centers can ever truly be green.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;Welcome back to&amp;nbsp;The State of AI, a new collaboration between the Financial Times and MIT Technology Review. Every Monday, writers from both publications debate one aspect of the generative AI revolution and how it is reshaping global power.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;This week, Casey Crownhart, senior reporter for energy at MIT Technology Review and Pilita Clark, FT's columnist, consider how China's rapid renewables buildout could help it leapfrog on AI progress.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127502" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Casey Crownhart writes&lt;/strong&gt;:&lt;/p&gt;  &lt;p&gt;In the age of AI, the biggest barrier to progress isn’t money but energy. That should be particularly worrying here in the US, where massive data centers are waiting to come online, and it doesn’t look as if the country will build the steady power supply or infrastructure needed to serve them all.&lt;/p&gt; 
 &lt;p&gt;It wasn’t always like this. For about a decade before 2020, data centers were able to offset increased demand with efficiency improvements. Now, though, electricity demand is ticking up in the US, with billions of queries to popular AI models each day—and efficiency gains aren’t keeping pace. With too little new power capacity coming online, the strain is starting to show: Electricity bills are ballooning for people who live in places where data centers place a growing load on the grid.&lt;/p&gt;  &lt;p&gt;If we want AI to have the chance to deliver on big promises without driving electricity prices sky-high for the rest of us, the US needs to learn some lessons from the rest of the world on energy abundance. Just look at China.&lt;/p&gt; 
 &lt;p&gt;China installed 429 GW of new power generation capacity in 2024, more than six times the net capacity added in the US during that time.&lt;/p&gt;  &lt;p&gt;China still generates much of its electricity with coal, but that makes up a declining share of the mix. Rather, the country is focused on installing solar, wind, nuclear, and gas at record rates.&lt;/p&gt;  &lt;p&gt;The US, meanwhile, is focused on reviving its ailing coal industry. Coal-fired power plants are polluting and, crucially, expensive to run. Aging plants in the US are also less reliable than they used to be, generating electricity just 42% of the time, compared with a 61% capacity factor in 2014.&lt;/p&gt;  &lt;p&gt;It’s not a great situation. And unless the US changes something, we risk becoming consumers as opposed to innovators in both energy and AI tech. Already, China earns more from exporting renewables than the US does from oil and gas exports.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Building and permitting new renewable power plants would certainly help, since they’re currently the cheapest and fastest to bring online. But wind and solar are politically unpopular with the current administration. Natural gas is an obvious candidate, though there are concerns about delays with key equipment.&lt;/p&gt;  &lt;p&gt;One quick fix would be for data centers to be more flexible. If they agreed not to suck electricity from the grid during times of stress, new AI infrastructure might be able to come online without any new energy infrastructure.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;One study from Duke University found that if data centers agree to curtail their consumption just 0.25% of the time (roughly 22 hours over the course of the year), the grid could provide power for about 76 GW of new demand. That’s like adding about 5% of the entire grid’s capacity without needing to build anything new.&lt;/p&gt;  &lt;p&gt;But flexibility wouldn’t be enough to truly meet the swell in AI electricity demand. What do you think, Pilita? What would get the US out of these energy constraints? Is there anything else we should be thinking about when it comes to AI and its energy use?&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;Pilita Clark responds:&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I agree. Data centers that can cut their power use at times of grid stress should be the norm, not the exception. Likewise, we need more deals like those giving cheaper electricity to data centers that let power utilities access their backup generators. Both reduce the need to build more power plants, which makes sense regardless of how much electricity AI ends up using.&lt;/p&gt;  &lt;p&gt;This is a critical point for countries across the world, because we still don’t know exactly how much power AI is going to consume.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Forecasts for what data centers will need in as little as five years’ time vary wildly, from less than twice today’s rates to four times as much.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;This is partly because there’s a lack of public data about AI systems’ energy needs. It’s also because we don’t know how much more efficient these systems will become. The US chip designer Nvidia said last year that its specialized chips had become 45,000 times more energy efficient over the previous eight years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Moreover, we have been very wrong about tech energy needs before. At the height of the dot-com boom in 1999, it was erroneously claimed that the internet would need half the US’s electricity within a decade—necessitating a lot more coal power.&lt;/p&gt;  &lt;p&gt;Still, some countries are clearly feeling the pressure already. In Ireland, data centers chew up so much power that new connections have been restricted around Dublin to avoid straining the grid.&lt;/p&gt;  &lt;p&gt;Some regulators are eyeing new rules forcing tech companies to provide enough power generation to match their demand. I hope such efforts grow. I also hope AI itself helps boost power abundance and, crucially, accelerates the global energy transition needed to combat climate change. OpenAI’s Sam Altman said in 2023 that “once we have a really powerful super intelligence, addressing climate change will not be particularly difficult.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The evidence so far is not promising, especially in the US, where renewable projects are being axed. Still, the US may end up being an outlier in a world where ever cheaper renewables made up more than 90% of new power capacity added globally last year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Europe is aiming to power one of its biggest data centers predominantly with renewables and batteries. But the country leading the green energy expansion is clearly China.&lt;/p&gt; 
 &lt;p&gt;The 20th century was dominated by countries rich in the fossil fuels whose reign the US now wants to prolong. China, in contrast, may become the world’s first green electrostate. If it does this in a way that helps it win an AI race the US has so far controlled, it will mark a striking chapter in economic, technological, and geopolitical history.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Casey Crownhart replies:&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;I share your skepticism of tech executives’ claims that AI will be a groundbreaking help in the race to address climate change. To be fair, AI is progressing rapidly. But we don’t have time to wait for technologies standing on big claims with nothing to back them up.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When it comes to the grid, for example, experts say there’s potential for AI to help with planning and even operating, but these efforts are still experimental.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, much of the world is making measurable progress on transitioning to newer, greener forms of energy. How that will affect the AI boom remains to be seen. What is clear is that AI is changing our grid and our world, and we need to be clear-eyed about the consequences.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Further reading&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt; reporters did the math on the energy needs of an AI query.&lt;/p&gt;  &lt;p&gt;There are still a few reasons to be optimistic about AI’s energy demands.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The &lt;em&gt;FT&lt;/em&gt;’s visual data team take a look&lt;strong&gt; &lt;/strong&gt;inside the relentless race for AI capacity.&lt;/p&gt;  &lt;p&gt;And global &lt;em&gt;FT&lt;/em&gt; reporters ask whether data centers can ever truly be green.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/10/1126805/the-state-of-ai-energy-is-king-and-the-us-is-falling-behind/</guid><pubDate>Mon, 10 Nov 2025 16:45:00 +0000</pubDate></item><item><title>[NEW] Roundtables: Surviving the New Age of Conspiracies (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/10/1127749/roundtables-surviving-the-new-age-of-conspiracies/</link><description>[unable to retrieve full-text content]Everything is a conspiracy theory now. MIT Technology Review’s new series, “The New Conspiracy Age,” explores how this moment is changing science and technology. Join features editor Amanda Silverman, executive editor Niall Firth, and Mike Rothschild, journalist and conspiracy theory expert, for a conversation about how we can make sense of them all. Going live&amp;#8230;</description><content:encoded>[unable to retrieve full-text content]Everything is a conspiracy theory now. MIT Technology Review’s new series, “The New Conspiracy Age,” explores how this moment is changing science and technology. Join features editor Amanda Silverman, executive editor Niall Firth, and Mike Rothschild, journalist and conspiracy theory expert, for a conversation about how we can make sense of them all. Going live&amp;#8230;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/10/1127749/roundtables-surviving-the-new-age-of-conspiracies/</guid><pubDate>Mon, 10 Nov 2025 18:03:17 +0000</pubDate></item><item><title>[NEW] Wikipedia urges AI companies to use its paid API, and stop scraping (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/wikipedia-urges-ai-companies-to-use-its-paid-api-and-stop-scraping/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1873370000.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Wikipedia on Monday laid out a simple plan to ensure its website continues to be supported in the AI era, despite its declining traffic. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post, the Wikimedia Foundation, the organization that runs the popular online encyclopedia, called on AI developers to use its content “responsibly” by ensuring its contributions are properly attributed and that content is accessed through its paid product, the Wikimedia Enterprise platform.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The opt-in, paid product allows companies to use Wikipedia’s content at scale without “severely taxing Wikipedia’s servers,” the Wikimedia Foundation blog post explains. In addition, the product’s paid nature allows AI companies to support the organization’s nonprofit mission. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the post doesn’t go so far as to threaten penalties or any sort of legal action for use of its material through scraping, Wikipedia recently noted that AI bots had been scraping its website while trying to appear human. After updating its bot detection systems, the organization found that its unusually high traffic in May and June had come from AI bots that were trying to “evade detection.” Meanwhile, it said that “human page views” had declined 8% year-over-year. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, Wikipedia is laying out its guidelines for AI developers and providers, saying that generative AI developers should provide attribution to give credit to the human contributors whose content it uses to create its outputs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For people to trust information shared on the internet, platforms should make it clear where the information is sourced from and elevate opportunities to visit and participate in those sources,” the post reads. “With fewer visits to Wikipedia, fewer volunteers may grow and enrich the content, and fewer individual donors may support this work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the organization released its AI strategy for editors, which said it would use AI to help editors with workflows around tedious tasks, automating translation, and other tools that help its editors, not replace them. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1873370000.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Wikipedia on Monday laid out a simple plan to ensure its website continues to be supported in the AI era, despite its declining traffic. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post, the Wikimedia Foundation, the organization that runs the popular online encyclopedia, called on AI developers to use its content “responsibly” by ensuring its contributions are properly attributed and that content is accessed through its paid product, the Wikimedia Enterprise platform.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The opt-in, paid product allows companies to use Wikipedia’s content at scale without “severely taxing Wikipedia’s servers,” the Wikimedia Foundation blog post explains. In addition, the product’s paid nature allows AI companies to support the organization’s nonprofit mission. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the post doesn’t go so far as to threaten penalties or any sort of legal action for use of its material through scraping, Wikipedia recently noted that AI bots had been scraping its website while trying to appear human. After updating its bot detection systems, the organization found that its unusually high traffic in May and June had come from AI bots that were trying to “evade detection.” Meanwhile, it said that “human page views” had declined 8% year-over-year. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, Wikipedia is laying out its guidelines for AI developers and providers, saying that generative AI developers should provide attribution to give credit to the human contributors whose content it uses to create its outputs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For people to trust information shared on the internet, platforms should make it clear where the information is sourced from and elevate opportunities to visit and participate in those sources,” the post reads. “With fewer visits to Wikipedia, fewer volunteers may grow and enrich the content, and fewer individual donors may support this work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the organization released its AI strategy for editors, which said it would use AI to help editors with workflows around tedious tasks, automating translation, and other tools that help its editors, not replace them. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/wikipedia-urges-ai-companies-to-use-its-paid-api-and-stop-scraping/</guid><pubDate>Mon, 10 Nov 2025 18:30:59 +0000</pubDate></item></channel></rss>