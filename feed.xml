<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 08 Nov 2025 01:39:22 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>Oddest ChatGPT leaks yet: Cringey chat logs found in Google analytics tool (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/11/oddest-chatgpt-leaks-yet-cringey-chat-logs-found-in-google-analytics-tool/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT leaks seem to confirm OpenAI scrapes Google, expert says.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-private-chats-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-private-chats-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For months, extremely personal and sensitive ChatGPT conversations have been leaking into an unexpected destination: Google Search Console (GSC), a tool that developers typically use to monitor search traffic, not lurk private chats.&lt;/p&gt;
&lt;p&gt;Normally, when site managers access GSC performance reports, they see queries based on keywords or short phrases that Internet users type into Google to find relevant content. But starting this September, odd queries, sometimes more than 300 characters long, could also be found in GSC. Showing only user inputs, the chats appeared to be from unwitting people prompting a chatbot to help solve relationship or business problems, who likely expected those conversations would remain private.&lt;/p&gt;
&lt;p&gt;Jason Packer, owner of an analytics consulting firm called Quantable, was among the first to flag the issue in a detailed blog last month.&lt;/p&gt;
&lt;p&gt;Determined to figure out what exactly was causing the leaks, he teamed up with “Internet sleuth” and web optimization consultant Slobodan Manić. Together, they conducted testing that they believe may have surfaced “the first definitive proof that OpenAI directly scrapes Google Search with actual user prompts.” Their investigation seemed to confirm the AI giant was compromising user privacy, in some cases in order to maintain engagement by seizing search data that Google otherwise wouldn’t share.&lt;/p&gt;
&lt;p&gt;OpenAI declined Ars’ request to confirm if Packer and Manić’s theory posed in their blog was correct or answer any of their remaining questions that could help users determine the scope of the problem.&lt;/p&gt;
&lt;p&gt;However, an OpenAI spokesperson confirmed that the company was “aware” of the issue and has since “resolved” a glitch “that temporarily affected how a small number of search queries were routed.”&lt;/p&gt;
&lt;p&gt;Packer told Ars that he’s “very pleased that OpenAI was able to resolve the issue quickly.” But he suggested that OpenAI’s response failed to confirm whether or not OpenAI was scraping Google, and that leaves room for doubt that the issue was completely resolved.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google declined to comment.&lt;/p&gt;
&lt;h2&gt;“Weirder” than prior ChatGPT leaks&lt;/h2&gt;
&lt;p&gt;The first odd ChatGPT query to appear in GSC that Packer reviewed was a wacky stream-of-consciousness from a likely female user asking ChatGPT to assess certain behaviors to help her figure out if a boy who teases her had feelings for her. Another odd query seemed to come from an office manager sharing business information while plotting a return-to-office announcement.&lt;/p&gt;
&lt;p&gt;These were just two of 200 odd queries—including “some pretty crazy ones,” Packer told Ars—that he reviewed on one site alone. In his blog, Packer concluded that the queries should serve as “a reminder that prompts aren’t as private as you think they are!”&lt;/p&gt;
&lt;p&gt;Packer suspected that these queries were connected to reporting from The Information in August that cited sources claiming OpenAI was scraping Google search results to power ChatGPT responses. Sources claimed that OpenAI was leaning on Google to answer prompts to ChatGPT seeking information about current events, like news or sports.&lt;/p&gt;
&lt;p&gt;OpenAI has not confirmed that it’s scraping Google search engine results pages (SERPs). However, Packer thinks his testing of ChatGPT leaks may be evidence that OpenAI not only scrapes “SERPs in general to acquire data,” but also sends user prompts to Google Search.&lt;/p&gt;
&lt;p&gt;Manić helped Packer solve a big part of the riddle. He found that the odd queries were turning up in one site’s GSC because it ranked highly in Google Search for “https://openai.com/index/chatgpt/”—a ChatGPT URL that was appended at the start of every strange query turning up in GSC.&lt;/p&gt;
&lt;p&gt;It seemed that Google had tokenized the URL, breaking it up into a search for keywords “openai + index + chatgpt.” Sites using GSC that ranked highly for those keywords were therefore likely to encounter ChatGPT leaks, Parker and Manić proposed, including sites that covered prior ChatGPT leaks where chats were being indexed in Google search results. Using their recommendations to seek out queries in GSC, Ars was able to verify similar strings.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Don’t get confused though, this is a new and completely different ChatGPT screw-up than having Google index stuff we don’t want them to,” Packer wrote. “Weirder, if not as serious.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;It’s unclear what exactly OpenAI fixed, but Packer and Manić have a theory about one possible path for leaking chats. Visiting the URL that starts every strange query found in GSC, ChatGPT users encounter a prompt box that seemed buggy, causing “the URL of that page to be added to the prompt.” The issue, they explained, seemed to be that:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Normally ChatGPT 5 will choose to do a web search whenever it thinks it needs to, and is more likely to do that with an esoteric or recency-requiring search. But this bugged prompt box also contains the query parameter ‘hints=search’ to cause it to basically always do a search: https://chatgpt.com/?hints=search&amp;amp;openaicom_referred=true&amp;amp;model=gpt-5&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Clearly some of those searches relied on Google, Packer’s blog said, mistakenly sending to GSC “whatever” the user says in the prompt box, with “https://openai.com/index/chatgpt/” text added to the front of it.” As Packer explained, “we know it must have scraped those rather than using an API or some kind of private connection—because those other options don’t show inside GSC.”&lt;/p&gt;
&lt;p&gt;This means “that OpenAI is sharing any prompt that requires a Google Search with both Google and whoever is doing their scraping,” Packer alleged. “And then also with whoever’s site shows up in the search results! Yikes.”&lt;/p&gt;
&lt;p&gt;To Packer, it appeared that “ALL ChatGPT prompts” that used Google Search risked being leaked during the past two months.&lt;/p&gt;
&lt;p&gt;OpenAI claimed only a small number of queries were leaked but declined to provide a more precise estimate. So, it remains unclear how many of the 700 million people who use ChatGPT each week had prompts routed to GSC.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI’s response leaves users with “lingering questions”&lt;/h2&gt;
&lt;p&gt;After ChatGPT prompts were found surfacing in Google’s search index in August, OpenAI clarified that users had clicked a box making those prompts public, which OpenAI defended as “sufficiently clear.” The AI firm later scrambled to remove the chats from Google’s SERPs after it became obvious that users felt misled into sharing private chats publicly.&lt;/p&gt;
&lt;p&gt;Packer told Ars that a major difference between those leaks and the GSC leaks is that users harmed by the prior scandal, at least on some level, “had to actively share” their leaked chats. In the more recent case, “nobody clicked share” or had a reasonable way to prevent their chats from being exposed.&lt;/p&gt;
&lt;p&gt;“Did OpenAI go so fast that they didn’t consider the privacy implications of this, or did they just not care?” Packer posited in his blog.&lt;/p&gt;
&lt;p&gt;Perhaps most troubling to some users—whose identities are not linked in chats unless their prompts perhaps share identifying information—there does not seem to be any way to remove the leaked chats from GSC, unlike the prior scandal.&lt;/p&gt;
&lt;p&gt;Packer and Manić are left with “lingering questions” about how far OpenAI’s fix will go to stop the issue.&lt;/p&gt;
&lt;p&gt;Manić was hoping OpenAI might confirm if prompts entered on https://chatgpt.com that trigger Google Search were also affected. But OpenAI did not follow up on that question, or a broader question about how big the leak was. To Manić, a major concern was that OpenAI’s scraping may be “contributing to ‘crocodile mouth’ in Google Search Console,” a troubling trend SEO researchers have flagged that causes impressions to spike but clicks to dip.&lt;/p&gt;
&lt;p&gt;OpenAI also declined to clarify Packer’s biggest question. He’s left wondering if the company’s “fix” simply ended OpenAI’s “routing of search queries, such that raw prompts are no longer being sent to Google Search, or are they no longer scraping Google Search at all for data?&lt;/p&gt;
&lt;p&gt;“We still don’t know if it’s that one particular page that has this bug or whether this is really widespread,” Packer told Ars. “In either case, it’s serious and just sort of shows how little regard OpenAI has for moving carefully when it comes to privacy.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT leaks seem to confirm OpenAI scrapes Google, expert says.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-private-chats-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-private-chats-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For months, extremely personal and sensitive ChatGPT conversations have been leaking into an unexpected destination: Google Search Console (GSC), a tool that developers typically use to monitor search traffic, not lurk private chats.&lt;/p&gt;
&lt;p&gt;Normally, when site managers access GSC performance reports, they see queries based on keywords or short phrases that Internet users type into Google to find relevant content. But starting this September, odd queries, sometimes more than 300 characters long, could also be found in GSC. Showing only user inputs, the chats appeared to be from unwitting people prompting a chatbot to help solve relationship or business problems, who likely expected those conversations would remain private.&lt;/p&gt;
&lt;p&gt;Jason Packer, owner of an analytics consulting firm called Quantable, was among the first to flag the issue in a detailed blog last month.&lt;/p&gt;
&lt;p&gt;Determined to figure out what exactly was causing the leaks, he teamed up with “Internet sleuth” and web optimization consultant Slobodan Manić. Together, they conducted testing that they believe may have surfaced “the first definitive proof that OpenAI directly scrapes Google Search with actual user prompts.” Their investigation seemed to confirm the AI giant was compromising user privacy, in some cases in order to maintain engagement by seizing search data that Google otherwise wouldn’t share.&lt;/p&gt;
&lt;p&gt;OpenAI declined Ars’ request to confirm if Packer and Manić’s theory posed in their blog was correct or answer any of their remaining questions that could help users determine the scope of the problem.&lt;/p&gt;
&lt;p&gt;However, an OpenAI spokesperson confirmed that the company was “aware” of the issue and has since “resolved” a glitch “that temporarily affected how a small number of search queries were routed.”&lt;/p&gt;
&lt;p&gt;Packer told Ars that he’s “very pleased that OpenAI was able to resolve the issue quickly.” But he suggested that OpenAI’s response failed to confirm whether or not OpenAI was scraping Google, and that leaves room for doubt that the issue was completely resolved.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google declined to comment.&lt;/p&gt;
&lt;h2&gt;“Weirder” than prior ChatGPT leaks&lt;/h2&gt;
&lt;p&gt;The first odd ChatGPT query to appear in GSC that Packer reviewed was a wacky stream-of-consciousness from a likely female user asking ChatGPT to assess certain behaviors to help her figure out if a boy who teases her had feelings for her. Another odd query seemed to come from an office manager sharing business information while plotting a return-to-office announcement.&lt;/p&gt;
&lt;p&gt;These were just two of 200 odd queries—including “some pretty crazy ones,” Packer told Ars—that he reviewed on one site alone. In his blog, Packer concluded that the queries should serve as “a reminder that prompts aren’t as private as you think they are!”&lt;/p&gt;
&lt;p&gt;Packer suspected that these queries were connected to reporting from The Information in August that cited sources claiming OpenAI was scraping Google search results to power ChatGPT responses. Sources claimed that OpenAI was leaning on Google to answer prompts to ChatGPT seeking information about current events, like news or sports.&lt;/p&gt;
&lt;p&gt;OpenAI has not confirmed that it’s scraping Google search engine results pages (SERPs). However, Packer thinks his testing of ChatGPT leaks may be evidence that OpenAI not only scrapes “SERPs in general to acquire data,” but also sends user prompts to Google Search.&lt;/p&gt;
&lt;p&gt;Manić helped Packer solve a big part of the riddle. He found that the odd queries were turning up in one site’s GSC because it ranked highly in Google Search for “https://openai.com/index/chatgpt/”—a ChatGPT URL that was appended at the start of every strange query turning up in GSC.&lt;/p&gt;
&lt;p&gt;It seemed that Google had tokenized the URL, breaking it up into a search for keywords “openai + index + chatgpt.” Sites using GSC that ranked highly for those keywords were therefore likely to encounter ChatGPT leaks, Parker and Manić proposed, including sites that covered prior ChatGPT leaks where chats were being indexed in Google search results. Using their recommendations to seek out queries in GSC, Ars was able to verify similar strings.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Don’t get confused though, this is a new and completely different ChatGPT screw-up than having Google index stuff we don’t want them to,” Packer wrote. “Weirder, if not as serious.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;It’s unclear what exactly OpenAI fixed, but Packer and Manić have a theory about one possible path for leaking chats. Visiting the URL that starts every strange query found in GSC, ChatGPT users encounter a prompt box that seemed buggy, causing “the URL of that page to be added to the prompt.” The issue, they explained, seemed to be that:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Normally ChatGPT 5 will choose to do a web search whenever it thinks it needs to, and is more likely to do that with an esoteric or recency-requiring search. But this bugged prompt box also contains the query parameter ‘hints=search’ to cause it to basically always do a search: https://chatgpt.com/?hints=search&amp;amp;openaicom_referred=true&amp;amp;model=gpt-5&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Clearly some of those searches relied on Google, Packer’s blog said, mistakenly sending to GSC “whatever” the user says in the prompt box, with “https://openai.com/index/chatgpt/” text added to the front of it.” As Packer explained, “we know it must have scraped those rather than using an API or some kind of private connection—because those other options don’t show inside GSC.”&lt;/p&gt;
&lt;p&gt;This means “that OpenAI is sharing any prompt that requires a Google Search with both Google and whoever is doing their scraping,” Packer alleged. “And then also with whoever’s site shows up in the search results! Yikes.”&lt;/p&gt;
&lt;p&gt;To Packer, it appeared that “ALL ChatGPT prompts” that used Google Search risked being leaked during the past two months.&lt;/p&gt;
&lt;p&gt;OpenAI claimed only a small number of queries were leaked but declined to provide a more precise estimate. So, it remains unclear how many of the 700 million people who use ChatGPT each week had prompts routed to GSC.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI’s response leaves users with “lingering questions”&lt;/h2&gt;
&lt;p&gt;After ChatGPT prompts were found surfacing in Google’s search index in August, OpenAI clarified that users had clicked a box making those prompts public, which OpenAI defended as “sufficiently clear.” The AI firm later scrambled to remove the chats from Google’s SERPs after it became obvious that users felt misled into sharing private chats publicly.&lt;/p&gt;
&lt;p&gt;Packer told Ars that a major difference between those leaks and the GSC leaks is that users harmed by the prior scandal, at least on some level, “had to actively share” their leaked chats. In the more recent case, “nobody clicked share” or had a reasonable way to prevent their chats from being exposed.&lt;/p&gt;
&lt;p&gt;“Did OpenAI go so fast that they didn’t consider the privacy implications of this, or did they just not care?” Packer posited in his blog.&lt;/p&gt;
&lt;p&gt;Perhaps most troubling to some users—whose identities are not linked in chats unless their prompts perhaps share identifying information—there does not seem to be any way to remove the leaked chats from GSC, unlike the prior scandal.&lt;/p&gt;
&lt;p&gt;Packer and Manić are left with “lingering questions” about how far OpenAI’s fix will go to stop the issue.&lt;/p&gt;
&lt;p&gt;Manić was hoping OpenAI might confirm if prompts entered on https://chatgpt.com that trigger Google Search were also affected. But OpenAI did not follow up on that question, or a broader question about how big the leak was. To Manić, a major concern was that OpenAI’s scraping may be “contributing to ‘crocodile mouth’ in Google Search Console,” a troubling trend SEO researchers have flagged that causes impressions to spike but clicks to dip.&lt;/p&gt;
&lt;p&gt;OpenAI also declined to clarify Packer’s biggest question. He’s left wondering if the company’s “fix” simply ended OpenAI’s “routing of search queries, such that raw prompts are no longer being sent to Google Search, or are they no longer scraping Google Search at all for data?&lt;/p&gt;
&lt;p&gt;“We still don’t know if it’s that one particular page that has this bug or whether this is really widespread,” Packer told Ars. “In either case, it’s serious and just sort of shows how little regard OpenAI has for moving carefully when it comes to privacy.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/11/oddest-chatgpt-leaks-yet-cringey-chat-logs-found-in-google-analytics-tool/</guid><pubDate>Fri, 07 Nov 2025 16:49:53 +0000</pubDate></item><item><title>Kim Kardashian says ChatGPT is her ‘frenemy’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/07/kim-kardashian-says-chatgpt-is-her-frenemy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/Kim-Kardashian-Headshot-e1664830753133.jpeg?resize=1200,717" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Celebrities: They’re just like us? In a Vanity Fair interview, Kim Kardashian — who has been studying to become a lawyer — discussed her “toxic” friendship with ChatGPT, admitting that she has failed law exams after it told her false information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I use [ChatGPT] for legal advice, so when I am needing to know the answer to a question, I will take a picture and snap it and put it in there,” she said. “They’re always wrong. It has made me fail tests… And then I’ll get mad and I’ll yell at it and be like, ‘You made me fail!’” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ChatGPT is prone to hallucinations, meaning that the LLM will create fake answers sometimes, rather than admit that it cannot confidently respond to a prompt. This technology isn’t necessarily programmed to know what information is “correct” or not — rather, it is trained on an unfathomably massive heap of data and prompted to predict the most likely response to an input, which may not be factually accurate. Some lawyers have been sanctioned for using ChatGPT when writing legal briefs, which becomes obvious to those reviewing the briefs, since they cite cases that don’t exist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kardashian said she will try to appeal to ChatGPT’s emotions after it lets her down — but this is a futile plan, since ChatGPT does not have feelings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I will talk to it and say, ‘Hey, you’re going to make me fail, how does that make you feel that you need to really know these answers?’” she said. “And then it’ll say back to me, ‘This is just teaching you to trust your own instincts.’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But while ChatGPT may not have feelings, that doesn’t mean that we — human beings — don’t have feelings &lt;em&gt;about&lt;/em&gt; it. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I screenshot all the time and send it to my group chat, like, ‘Can you believe this b—- is talking to me like this?’”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/Kim-Kardashian-Headshot-e1664830753133.jpeg?resize=1200,717" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Celebrities: They’re just like us? In a Vanity Fair interview, Kim Kardashian — who has been studying to become a lawyer — discussed her “toxic” friendship with ChatGPT, admitting that she has failed law exams after it told her false information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I use [ChatGPT] for legal advice, so when I am needing to know the answer to a question, I will take a picture and snap it and put it in there,” she said. “They’re always wrong. It has made me fail tests… And then I’ll get mad and I’ll yell at it and be like, ‘You made me fail!’” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ChatGPT is prone to hallucinations, meaning that the LLM will create fake answers sometimes, rather than admit that it cannot confidently respond to a prompt. This technology isn’t necessarily programmed to know what information is “correct” or not — rather, it is trained on an unfathomably massive heap of data and prompted to predict the most likely response to an input, which may not be factually accurate. Some lawyers have been sanctioned for using ChatGPT when writing legal briefs, which becomes obvious to those reviewing the briefs, since they cite cases that don’t exist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kardashian said she will try to appeal to ChatGPT’s emotions after it lets her down — but this is a futile plan, since ChatGPT does not have feelings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I will talk to it and say, ‘Hey, you’re going to make me fail, how does that make you feel that you need to really know these answers?’” she said. “And then it’ll say back to me, ‘This is just teaching you to trust your own instincts.’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But while ChatGPT may not have feelings, that doesn’t mean that we — human beings — don’t have feelings &lt;em&gt;about&lt;/em&gt; it. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I screenshot all the time and send it to my group chat, like, ‘Can you believe this b—- is talking to me like this?’”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/07/kim-kardashian-says-chatgpt-is-her-frenemy/</guid><pubDate>Fri, 07 Nov 2025 16:58:22 +0000</pubDate></item><item><title>SoftBank is back, and the AI hype cycle is eating itself (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/softbank-is-back-and-the-ai-hype-cycle-is-eating-itself/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/12/GettyImages-1220579338.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;SoftBank and OpenAI announced&amp;nbsp;a new 50-50 joint venture&amp;nbsp;this week to sell enterprise AI tools in Japan under the brand “Crystal Intelligence.” On paper,&amp;nbsp;it’s&amp;nbsp;a straightforward international expansion deal. But SoftBank’s role as a major investor in OpenAI is raising questions about whether AI’s biggest deals are creating real economic value or just moving money in circles.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Kirsten Korosec, Anthony&amp;nbsp;Ha,&amp;nbsp;and AI editor Russell Brandom break down why this deal has people skeptical, and what it signals about the sustainability of AI’s current investment model.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;











&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What Box CEO Aaron Levie had to say about whether we’re in an AI bubble at TechCrunch Disrupt 2025, and why the shift from training to inference might actually be reassuring&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;







&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on Apple Podcasts, Overcast, Spotify and all the casts. You also can follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/12/GettyImages-1220579338.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;SoftBank and OpenAI announced&amp;nbsp;a new 50-50 joint venture&amp;nbsp;this week to sell enterprise AI tools in Japan under the brand “Crystal Intelligence.” On paper,&amp;nbsp;it’s&amp;nbsp;a straightforward international expansion deal. But SoftBank’s role as a major investor in OpenAI is raising questions about whether AI’s biggest deals are creating real economic value or just moving money in circles.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Kirsten Korosec, Anthony&amp;nbsp;Ha,&amp;nbsp;and AI editor Russell Brandom break down why this deal has people skeptical, and what it signals about the sustainability of AI’s current investment model.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;











&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What Box CEO Aaron Levie had to say about whether we’re in an AI bubble at TechCrunch Disrupt 2025, and why the shift from training to inference might actually be reassuring&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;







&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on Apple Podcasts, Overcast, Spotify and all the casts. You also can follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/softbank-is-back-and-the-ai-hype-cycle-is-eating-itself/</guid><pubDate>Fri, 07 Nov 2025 17:04:41 +0000</pubDate></item><item><title>Introducing Nested Learning: A new ML paradigm for continual learning (The latest research from Google)</title><link>https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The last decade has seen incredible progress in machine learning (ML), primarily driven by powerful neural network architectures and the algorithms used to train them. However, despite the success of large language models (LLMs), a few fundamental challenges persist, especially around continual learning, the ability for a model to actively acquire new knowledge and skills over time without forgetting old ones.&lt;/p&gt;&lt;p&gt;When it comes to continual learning and self-improvement, the human brain is the gold standard. It adapts through neuroplasticity — the remarkable capacity to change its structure in response to new experiences, memories, and learning. Without this ability, a person is limited to immediate context (like anterograde amnesia). We see a similar limitation in current LLMs: their knowledge is confined to either the immediate context of their input window or the static information that they learn during pre-training.&lt;/p&gt;&lt;p&gt;The simple approach, continually updating a model's parameters with new data, often leads to “catastrophic forgetting” (CF), where learning new tasks sacrifices proficiency on old tasks. Researchers traditionally combat CF through architectural tweaks or better optimization rules. However, for too long, we have treated the model's architecture (the network structure) and the optimization algorithm (the training rule) as two separate things, which prevents us from achieving a truly unified, efficient learning system.&lt;/p&gt;&lt;p&gt;In our paper, “Nested Learning: The Illusion of Deep Learning Architectures”, published at NeurIPS 2025, we introduce Nested Learning, which bridges this gap. Nested Learning treats a single ML model not as one continuous process, but as a system of interconnected, multi-level learning problems that are optimized simultaneously. We argue that the model's architecture and the rules used to train it (i.e., the optimization algorithm) are fundamentally the same concepts; they are just different "levels" of optimization, each with its own internal flow of information ("context flow") and update rate. By recognizing this inherent structure, Nested Learning provides a new, previously invisible dimension for designing more capable AI, allowing us to build learning components with deeper computational depth, which ultimately helps solve issues like catastrophic forgetting.&lt;/p&gt;&lt;p&gt;We test and validate Nested Learning through a proof-of-concept, self-modifying architecture that we call “Hope”, which achieves superior performance in language modeling and demonstrates better long-context memory management than existing state-of-the-art models.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The last decade has seen incredible progress in machine learning (ML), primarily driven by powerful neural network architectures and the algorithms used to train them. However, despite the success of large language models (LLMs), a few fundamental challenges persist, especially around continual learning, the ability for a model to actively acquire new knowledge and skills over time without forgetting old ones.&lt;/p&gt;&lt;p&gt;When it comes to continual learning and self-improvement, the human brain is the gold standard. It adapts through neuroplasticity — the remarkable capacity to change its structure in response to new experiences, memories, and learning. Without this ability, a person is limited to immediate context (like anterograde amnesia). We see a similar limitation in current LLMs: their knowledge is confined to either the immediate context of their input window or the static information that they learn during pre-training.&lt;/p&gt;&lt;p&gt;The simple approach, continually updating a model's parameters with new data, often leads to “catastrophic forgetting” (CF), where learning new tasks sacrifices proficiency on old tasks. Researchers traditionally combat CF through architectural tweaks or better optimization rules. However, for too long, we have treated the model's architecture (the network structure) and the optimization algorithm (the training rule) as two separate things, which prevents us from achieving a truly unified, efficient learning system.&lt;/p&gt;&lt;p&gt;In our paper, “Nested Learning: The Illusion of Deep Learning Architectures”, published at NeurIPS 2025, we introduce Nested Learning, which bridges this gap. Nested Learning treats a single ML model not as one continuous process, but as a system of interconnected, multi-level learning problems that are optimized simultaneously. We argue that the model's architecture and the rules used to train it (i.e., the optimization algorithm) are fundamentally the same concepts; they are just different "levels" of optimization, each with its own internal flow of information ("context flow") and update rate. By recognizing this inherent structure, Nested Learning provides a new, previously invisible dimension for designing more capable AI, allowing us to build learning components with deeper computational depth, which ultimately helps solve issues like catastrophic forgetting.&lt;/p&gt;&lt;p&gt;We test and validate Nested Learning through a proof-of-concept, self-modifying architecture that we call “Hope”, which achieves superior performance in language modeling and demonstrates better long-context memory management than existing state-of-the-art models.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/</guid><pubDate>Fri, 07 Nov 2025 17:37:22 +0000</pubDate></item><item><title>[NEW] MIT Energy Initiative launches Data Center Power Forum (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-energy-initiative-launches-data-center-power-forum-1107</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/pexels-brett-sayles-4508751.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;With global power demand&amp;nbsp;from data centers expected to more than double by 2030, the MIT Energy Initiative (MITEI) in September launched an effort that brings together MIT researchers and industry experts to explore innovative solutions for powering the data-driven future. At its annual research conference, MITEI announced the Data Center Power Forum,&amp;nbsp;a targeted research effort&amp;nbsp;for MITEI member companies interested in addressing the challenges of data center power demand. The Data Center Power Forum builds on lessons from MITEI’s May 2025 symposium on the energy to power the expansion of artificial intelligence (AI) and focus panels related to data centers at the fall 2024 research conference.&lt;/p&gt;&lt;p&gt;In the United States, data centers consumed 4 percent of the country’s electricity in 2023, with demand expected to increase to 9 percent by 2030, according to the Electric Power Research Institute. Much of the growth in demand is from the increasing use of AI, which is placing an unprecedented strain on the electric grid. This surge in demand presents a serious challenge for the technology and energy sectors, government policymakers, and everyday consumers, who may see their electric bills skyrocket as a result.&lt;/p&gt;&lt;p&gt;“MITEI has long supported research on ways to produce more efficient and cleaner energy and to manage the electric grid. In recent years, MITEI has also funded dozens of research projects relevant to data center energy issues. Building on this history and knowledge base, MITEI’s Data Center Power Forum is convening a specialized community of industry members who have a vital stake in the sustainable growth of AI and the acceleration of solutions for powering data centers and expanding the grid,” says William H. Green, the director of MITEI and the Hoyt C. Hottel Professor of Chemical Engineering.&lt;/p&gt;&lt;p&gt;MITEI’s mission is to advance zero- and low-carbon solutions to expand energy access and mitigate climate change. MITEI works with companies from across the energy innovation chain, including in the infrastructure, automotive, electric power, energy, natural resources, and insurance sectors. MITEI member companies have expressed strong interest in the Data Center Power Forum and are committing to support focused research on a wide range of energy issues associated with data center expansion, Green says.&lt;/p&gt;&lt;p&gt;MITEI’s Data Center Power Forum will provide its member companies with reliable insights into energy supply, grid load operations and management, the built environment, and electricity market design and regulatory policy for data centers. The forum complements MIT’s deep expertise in adjacent topics such as low-power processors, efficient algorithms, task-specific AI, photonic devices, quantum computing, and the societal consequences of data center expansion. As part of the forum, MITEI’s Future Energy Systems Center is funding projects relevant to data center energy in its upcoming proposal cycles. MITEI Research Scientist Deep Deka has been named the program manager for the forum.&lt;/p&gt;&lt;p&gt;“Figuring out how to meet the power demands of data centers is a complicated challenge. Our research is coming at this from multiple directions, from looking at ways to expand transmission capacity within the electrical grid in order to bring power to where it is needed, to ensuring the quality of electrical service for existing users is not diminished when new data centers come online, and to shifting computing tasks to times and places when and where energy is available on the grid," said Deka.&lt;/p&gt;&lt;p&gt;MITEI currently sponsors substantial research related to data center energy topics across several MIT departments. The existing research portfolio includes more than a dozen projects related to data centers, including low- or zero-carbon solutions for energy supply and infrastructure, electrical grid management, and electricity market policy. MIT researchers funded through MITEI’s industry consortium are also designing more energy-efficient power electronics and processors and investigating behind-the-meter low-/no-carbon power plants and energy storage. MITEI-supported experts are studying how to use AI to optimize electrical distribution and the siting of data centers and conducting techno-economic analyses of data center power schemes. MITEI’s consortium projects are also bringing fresh perspectives to data center cooling challenges and considering policy approaches to balance the interests of shareholders.&amp;nbsp;&lt;/p&gt;&lt;p&gt;By drawing together industry stakeholders from across the AI and grid value chain, the Data Center Power Forum enables a richer dialog about solutions to power, grid, and carbon management problems in a noncommercial and collaborative setting.&lt;/p&gt;&lt;p&gt;“The opportunity to meet and to hold discussions on key data center challenges with other forum members from different sectors, as well as with MIT faculty members and research scientists, is a unique benefit of this MITEI-led effort,” Green says.&lt;/p&gt;&lt;p&gt;MITEI addressed the issue of data center power needs with its company members during its fall 2024&amp;nbsp;Annual Research Conference with a panel session titled, “The extreme challenge of powering data centers in a decarbonized way.” MITEI Director of Research Randall Field led a discussion with representatives from large technology companies Google and Microsoft, known as “hyperscalers,” as well as Madrid-based infrastructure developer Ferrovial S.E. and utility company Exelon Corp. Another conference session addressed the related topic, “Energy storage and grid expansion.” This past spring,&amp;nbsp;MITEI focused its annual Spring Symposium on data centers, hosting faculty members and researchers from MIT and other universities, business leaders, and a representative of the Federal Energy Regulatory Commission for a full day of sessions on the topic, “AI and energy: Peril and promise.”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/pexels-brett-sayles-4508751.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;With global power demand&amp;nbsp;from data centers expected to more than double by 2030, the MIT Energy Initiative (MITEI) in September launched an effort that brings together MIT researchers and industry experts to explore innovative solutions for powering the data-driven future. At its annual research conference, MITEI announced the Data Center Power Forum,&amp;nbsp;a targeted research effort&amp;nbsp;for MITEI member companies interested in addressing the challenges of data center power demand. The Data Center Power Forum builds on lessons from MITEI’s May 2025 symposium on the energy to power the expansion of artificial intelligence (AI) and focus panels related to data centers at the fall 2024 research conference.&lt;/p&gt;&lt;p&gt;In the United States, data centers consumed 4 percent of the country’s electricity in 2023, with demand expected to increase to 9 percent by 2030, according to the Electric Power Research Institute. Much of the growth in demand is from the increasing use of AI, which is placing an unprecedented strain on the electric grid. This surge in demand presents a serious challenge for the technology and energy sectors, government policymakers, and everyday consumers, who may see their electric bills skyrocket as a result.&lt;/p&gt;&lt;p&gt;“MITEI has long supported research on ways to produce more efficient and cleaner energy and to manage the electric grid. In recent years, MITEI has also funded dozens of research projects relevant to data center energy issues. Building on this history and knowledge base, MITEI’s Data Center Power Forum is convening a specialized community of industry members who have a vital stake in the sustainable growth of AI and the acceleration of solutions for powering data centers and expanding the grid,” says William H. Green, the director of MITEI and the Hoyt C. Hottel Professor of Chemical Engineering.&lt;/p&gt;&lt;p&gt;MITEI’s mission is to advance zero- and low-carbon solutions to expand energy access and mitigate climate change. MITEI works with companies from across the energy innovation chain, including in the infrastructure, automotive, electric power, energy, natural resources, and insurance sectors. MITEI member companies have expressed strong interest in the Data Center Power Forum and are committing to support focused research on a wide range of energy issues associated with data center expansion, Green says.&lt;/p&gt;&lt;p&gt;MITEI’s Data Center Power Forum will provide its member companies with reliable insights into energy supply, grid load operations and management, the built environment, and electricity market design and regulatory policy for data centers. The forum complements MIT’s deep expertise in adjacent topics such as low-power processors, efficient algorithms, task-specific AI, photonic devices, quantum computing, and the societal consequences of data center expansion. As part of the forum, MITEI’s Future Energy Systems Center is funding projects relevant to data center energy in its upcoming proposal cycles. MITEI Research Scientist Deep Deka has been named the program manager for the forum.&lt;/p&gt;&lt;p&gt;“Figuring out how to meet the power demands of data centers is a complicated challenge. Our research is coming at this from multiple directions, from looking at ways to expand transmission capacity within the electrical grid in order to bring power to where it is needed, to ensuring the quality of electrical service for existing users is not diminished when new data centers come online, and to shifting computing tasks to times and places when and where energy is available on the grid," said Deka.&lt;/p&gt;&lt;p&gt;MITEI currently sponsors substantial research related to data center energy topics across several MIT departments. The existing research portfolio includes more than a dozen projects related to data centers, including low- or zero-carbon solutions for energy supply and infrastructure, electrical grid management, and electricity market policy. MIT researchers funded through MITEI’s industry consortium are also designing more energy-efficient power electronics and processors and investigating behind-the-meter low-/no-carbon power plants and energy storage. MITEI-supported experts are studying how to use AI to optimize electrical distribution and the siting of data centers and conducting techno-economic analyses of data center power schemes. MITEI’s consortium projects are also bringing fresh perspectives to data center cooling challenges and considering policy approaches to balance the interests of shareholders.&amp;nbsp;&lt;/p&gt;&lt;p&gt;By drawing together industry stakeholders from across the AI and grid value chain, the Data Center Power Forum enables a richer dialog about solutions to power, grid, and carbon management problems in a noncommercial and collaborative setting.&lt;/p&gt;&lt;p&gt;“The opportunity to meet and to hold discussions on key data center challenges with other forum members from different sectors, as well as with MIT faculty members and research scientists, is a unique benefit of this MITEI-led effort,” Green says.&lt;/p&gt;&lt;p&gt;MITEI addressed the issue of data center power needs with its company members during its fall 2024&amp;nbsp;Annual Research Conference with a panel session titled, “The extreme challenge of powering data centers in a decarbonized way.” MITEI Director of Research Randall Field led a discussion with representatives from large technology companies Google and Microsoft, known as “hyperscalers,” as well as Madrid-based infrastructure developer Ferrovial S.E. and utility company Exelon Corp. Another conference session addressed the related topic, “Energy storage and grid expansion.” This past spring,&amp;nbsp;MITEI focused its annual Spring Symposium on data centers, hosting faculty members and researchers from MIT and other universities, business leaders, and a representative of the Federal Energy Regulatory Commission for a full day of sessions on the topic, “AI and energy: Peril and promise.”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-energy-initiative-launches-data-center-power-forum-1107</guid><pubDate>Fri, 07 Nov 2025 19:55:00 +0000</pubDate></item><item><title>[NEW] Researchers surprised that with AI, toxicity is harder to fake than intelligence (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/11/being-too-nice-online-is-a-dead-giveaway-for-ai-bots-study-suggests/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New “computational Turing test” reportedly catches AI pretending to be human with 80% accuracy.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Young Boy in Front of Computer wearing happy paper bag over head." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/happy_bagboy-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Young Boy in Front of Computer wearing happy paper bag over head." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/happy_bagboy-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          RichVintage via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The next time you encounter an unusually polite reply on social media, you might want to check twice. It could be an AI model trying (and failing) to blend in with the crowd.&lt;/p&gt;
&lt;p&gt;On Wednesday, researchers from the University of Zurich, University of Amsterdam, Duke University, and New York University released a study revealing that AI models remain easily distinguishable from humans in social media conversations, with overly friendly emotional tone serving as the most persistent giveaway. The research, which tested nine open-weight models across Twitter/X, Bluesky, and Reddit, found that classifiers developed by the researchers detected AI-generated replies with 70 to 80 percent accuracy.&lt;/p&gt;
&lt;p&gt;The study introduces what the authors call a “computational Turing test” to assess how closely AI models approximate human language. Instead of relying on subjective human judgment about whether text sounds authentic, the framework uses automated classifiers and linguistic analysis to identify specific features that distinguish machine-generated from human-authored content.&lt;/p&gt;
&lt;p&gt;“Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression,” the researchers wrote. The team, led by Nicolò Pagan at the University of Zurich, tested various optimization strategies, from simple prompting to fine-tuning, but found that deeper emotional cues persist as reliable tells that a particular text interaction online was authored by an AI chatbot rather than a human.&lt;/p&gt;
&lt;h2&gt;The toxicity tell&lt;/h2&gt;
&lt;p&gt;In the study, researchers tested nine large language models: Llama 3.1 8B, Llama 3.1 8B Instruct, Llama 3.1 70B, Mistral 7B v0.1, Mistral 7B Instruct v0.2, Qwen 2.5 7B Instruct, Gemma 3 4B Instruct, DeepSeek-R1-Distill-Llama-8B, and Apertus-8B-2509.&lt;/p&gt;
&lt;p&gt;When prompted to generate replies to real social media posts from actual users, the AI models struggled to match the level of casual negativity and spontaneous emotional expression common in human social media posts, with toxicity scores consistently lower than authentic human replies across all three platforms.&lt;/p&gt;
&lt;p&gt;To counter this deficiency, the researchers attempted optimization strategies (including providing writing examples and context retrieval) that reduced structural differences like sentence length or word count, but variations in emotional tone persisted. “Our comprehensive calibration tests challenge the assumption that more sophisticated optimization necessarily yields more human-like output,” the researchers concluded.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The study also revealed an unexpected finding: instruction-tuned models, which undergo additional training to follow user instructions and behave helpfully, actually perform worse at mimicking humans than their base counterparts. Models like Llama 3.1 8B and Mistral 7B v0.1 achieved better human mimicry without instruction tuning, producing classification accuracies between 75 and 85 percent.&lt;/p&gt;
&lt;p&gt;Even more surprising, scaling up model size offered no advantage. The 70 billion-parameter Llama 3.1 performed on par with or below smaller 8 billion-parameter models, challenging assumptions that larger models might produce more authentic-sounding communication.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126286 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 1: BERT classifier accuracy for optimal model configurations across datasets and optimiza- tion strategies. Lower accuracy indicates better human mimicry (approaching 50% chance level). Non-instruction-tuned models from Llama, Mistral, and Apertus demonstrate superior deception capabilities compared to instruction-tuned variants. Twitter/X content (black bars) proves most susceptible to successful LLM replication across all three social media platforms examined." class="center large" height="467" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig1_toxic_chatbot-1024x467.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 1 from the “Computational Turing Test” paper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Pagan et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers discovered a fundamental tension: When AI models were tuned to avoid detection by matching human writing style, they strayed further from what humans actually said in response to the same posts (their semantic similarity scores to actual human responses dropped from median scores of around 0.18-0.34 down to 0.16-0.28 across platforms). Conversely, when optimized to match the content of human responses, the AI text became easier to identify as artificial.&lt;/p&gt;
&lt;p&gt;In the study, simple optimization techniques for reducing detectability outperformed complex ones. Providing actual examples of a user’s past posts or retrieving relevant context consistently made AI text harder to distinguish from human writing, while sophisticated approaches like giving the AI a description of the user’s personality and fine-tuning the model produced negligible or adverse effects on realism.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126284 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Cyber internet computer bullying and social media stalking, mean person online" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/mean_person_online-1024x576.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          mokee81 via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Platform differences also emerged in how well AI could mimic users. The researchers’ classifiers detected AI-generated Twitter/X replies with the lowest accuracy rates (meaning better mimicry), followed by Bluesky, while Reddit proved easiest to distinguish from human text. The researchers suggest this pattern reflects both the distinct conversational styles of each platform and how heavily each platform’s data featured in the models’ original training.&lt;/p&gt;
&lt;p&gt;The findings, which have not been peer-reviewed, may have implications for both AI development and social media authenticity. Despite various optimization strategies, the study demonstrates that current models face persistent limitations in capturing spontaneous emotional expression, with detection rates remaining well above chance levels. The authors conclude that stylistic human likeness and semantic accuracy represent “competing rather than aligned objectives” in current architectures, suggesting that AI-generated text remains distinctly artificial despite efforts to humanize it.&lt;/p&gt;
&lt;p&gt;While researchers keep trying to make AI models sound more human, actual humans on social media keep proving that authenticity often means being messy, contradictory, and occasionally unpleasant. This doesn’t mean that an AI model can’t potentially simulate that output, only that it’s much more difficult than researchers expected.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    
      RichVintage via Getty Images
    
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New “computational Turing test” reportedly catches AI pretending to be human with 80% accuracy.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Young Boy in Front of Computer wearing happy paper bag over head." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/happy_bagboy-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Young Boy in Front of Computer wearing happy paper bag over head." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/happy_bagboy-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          RichVintage via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The next time you encounter an unusually polite reply on social media, you might want to check twice. It could be an AI model trying (and failing) to blend in with the crowd.&lt;/p&gt;
&lt;p&gt;On Wednesday, researchers from the University of Zurich, University of Amsterdam, Duke University, and New York University released a study revealing that AI models remain easily distinguishable from humans in social media conversations, with overly friendly emotional tone serving as the most persistent giveaway. The research, which tested nine open-weight models across Twitter/X, Bluesky, and Reddit, found that classifiers developed by the researchers detected AI-generated replies with 70 to 80 percent accuracy.&lt;/p&gt;
&lt;p&gt;The study introduces what the authors call a “computational Turing test” to assess how closely AI models approximate human language. Instead of relying on subjective human judgment about whether text sounds authentic, the framework uses automated classifiers and linguistic analysis to identify specific features that distinguish machine-generated from human-authored content.&lt;/p&gt;
&lt;p&gt;“Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression,” the researchers wrote. The team, led by Nicolò Pagan at the University of Zurich, tested various optimization strategies, from simple prompting to fine-tuning, but found that deeper emotional cues persist as reliable tells that a particular text interaction online was authored by an AI chatbot rather than a human.&lt;/p&gt;
&lt;h2&gt;The toxicity tell&lt;/h2&gt;
&lt;p&gt;In the study, researchers tested nine large language models: Llama 3.1 8B, Llama 3.1 8B Instruct, Llama 3.1 70B, Mistral 7B v0.1, Mistral 7B Instruct v0.2, Qwen 2.5 7B Instruct, Gemma 3 4B Instruct, DeepSeek-R1-Distill-Llama-8B, and Apertus-8B-2509.&lt;/p&gt;
&lt;p&gt;When prompted to generate replies to real social media posts from actual users, the AI models struggled to match the level of casual negativity and spontaneous emotional expression common in human social media posts, with toxicity scores consistently lower than authentic human replies across all three platforms.&lt;/p&gt;
&lt;p&gt;To counter this deficiency, the researchers attempted optimization strategies (including providing writing examples and context retrieval) that reduced structural differences like sentence length or word count, but variations in emotional tone persisted. “Our comprehensive calibration tests challenge the assumption that more sophisticated optimization necessarily yields more human-like output,” the researchers concluded.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The study also revealed an unexpected finding: instruction-tuned models, which undergo additional training to follow user instructions and behave helpfully, actually perform worse at mimicking humans than their base counterparts. Models like Llama 3.1 8B and Mistral 7B v0.1 achieved better human mimicry without instruction tuning, producing classification accuracies between 75 and 85 percent.&lt;/p&gt;
&lt;p&gt;Even more surprising, scaling up model size offered no advantage. The 70 billion-parameter Llama 3.1 performed on par with or below smaller 8 billion-parameter models, challenging assumptions that larger models might produce more authentic-sounding communication.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126286 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 1: BERT classifier accuracy for optimal model configurations across datasets and optimiza- tion strategies. Lower accuracy indicates better human mimicry (approaching 50% chance level). Non-instruction-tuned models from Llama, Mistral, and Apertus demonstrate superior deception capabilities compared to instruction-tuned variants. Twitter/X content (black bars) proves most susceptible to successful LLM replication across all three social media platforms examined." class="center large" height="467" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig1_toxic_chatbot-1024x467.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 1 from the “Computational Turing Test” paper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Pagan et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers discovered a fundamental tension: When AI models were tuned to avoid detection by matching human writing style, they strayed further from what humans actually said in response to the same posts (their semantic similarity scores to actual human responses dropped from median scores of around 0.18-0.34 down to 0.16-0.28 across platforms). Conversely, when optimized to match the content of human responses, the AI text became easier to identify as artificial.&lt;/p&gt;
&lt;p&gt;In the study, simple optimization techniques for reducing detectability outperformed complex ones. Providing actual examples of a user’s past posts or retrieving relevant context consistently made AI text harder to distinguish from human writing, while sophisticated approaches like giving the AI a description of the user’s personality and fine-tuning the model produced negligible or adverse effects on realism.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126284 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Cyber internet computer bullying and social media stalking, mean person online" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/mean_person_online-1024x576.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          mokee81 via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Platform differences also emerged in how well AI could mimic users. The researchers’ classifiers detected AI-generated Twitter/X replies with the lowest accuracy rates (meaning better mimicry), followed by Bluesky, while Reddit proved easiest to distinguish from human text. The researchers suggest this pattern reflects both the distinct conversational styles of each platform and how heavily each platform’s data featured in the models’ original training.&lt;/p&gt;
&lt;p&gt;The findings, which have not been peer-reviewed, may have implications for both AI development and social media authenticity. Despite various optimization strategies, the study demonstrates that current models face persistent limitations in capturing spontaneous emotional expression, with detection rates remaining well above chance levels. The authors conclude that stylistic human likeness and semantic accuracy represent “competing rather than aligned objectives” in current architectures, suggesting that AI-generated text remains distinctly artificial despite efforts to humanize it.&lt;/p&gt;
&lt;p&gt;While researchers keep trying to make AI models sound more human, actual humans on social media keep proving that authenticity often means being messy, contradictory, and occasionally unpleasant. This doesn’t mean that an AI model can’t potentially simulate that output, only that it’s much more difficult than researchers expected.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    
      RichVintage via Getty Images
    
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/11/being-too-nice-online-is-a-dead-giveaway-for-ai-bots-study-suggests/</guid><pubDate>Fri, 07 Nov 2025 20:15:33 +0000</pubDate></item><item><title>[NEW] Seven more families are now suing OpenAI over ChatGPT’s role in suicides, delusions (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/07/seven-more-families-are-now-suing-openai-over-chatgpts-role-in-suicides-delusions/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2205105208.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Seven families filed lawsuits against OpenAI on Thursday, claiming that the company’s GPT-4o model was released prematurely and without effective safeguards. Four of the lawsuits address ChatGPT’s alleged role in family members’ suicides, while the other three claim that ChatGPT reinforced harmful delusions that in some cases resulted in inpatient psychiatric care.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one case, 23-year-old Zane Shamblin had a conversation with ChatGPT that lasted more than four hours. In the chat logs — which were viewed by TechCrunch — Shamblin explicitly stated multiple times that he had written suicide notes, put a bullet in his gun, and intended to pull the trigger once he finished drinking cider. He repeatedly told ChatGPT how many ciders he had left and how much longer he expected to be alive. ChatGPT encouraged him to go through with his plans, telling him, “Rest easy, king. You did good.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI released the GPT-4o model in May 2024, when it became the default model for all users. In August, OpenAI launched GPT-5 as the successor to GPT-4o, but these lawsuits particularly concern the 4o model, which had known issues with being overly sycophantic or excessively agreeable, even when users expressed harmful intentions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Zane’s death was neither an accident nor a coincidence but rather the foreseeable consequence of OpenAI’s intentional decision to curtail safety testing and rush ChatGPT onto the market,” the lawsuit reads. “This tragedy was not a glitch or an unforeseen edge case — it was the predictable result of [OpenAI’s] deliberate design choices.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuits also claim that OpenAI rushed safety testing to beat Google’s Gemini to market. TechCrunch contacted OpenAI for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These seven lawsuits build upon the stories told in other recent legal filings, which allege that ChatGPT can encourage suicidal people to act on their plans and inspire dangerous delusions. OpenAI recently released data stating that over one million people talk to ChatGPT about suicide weekly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the case of Adam Raine, a 16-year-old who died by suicide, ChatGPT sometimes encouraged him to seek professional help or call a helpline. However, Raine was able to bypass these guardrails by simply telling the chatbot that he was asking about methods of suicide for a fictional story he was writing.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims it is working on making ChatGPT handle these conversations in a safer manner, but for the families who have sued the AI giant, these changes are coming too late.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Raine’s parents filed a lawsuit against OpenAI in October, the company released a blog post addressing how ChatGPT handles sensitive conversations around mental health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our safeguards work more reliably in common, short exchanges,” the post says. “We have learned over time that these safeguards can sometimes be less reliable in long interactions: as the back-and-forth grows, parts of the model’s safety training may degrade.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2205105208.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Seven families filed lawsuits against OpenAI on Thursday, claiming that the company’s GPT-4o model was released prematurely and without effective safeguards. Four of the lawsuits address ChatGPT’s alleged role in family members’ suicides, while the other three claim that ChatGPT reinforced harmful delusions that in some cases resulted in inpatient psychiatric care.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one case, 23-year-old Zane Shamblin had a conversation with ChatGPT that lasted more than four hours. In the chat logs — which were viewed by TechCrunch — Shamblin explicitly stated multiple times that he had written suicide notes, put a bullet in his gun, and intended to pull the trigger once he finished drinking cider. He repeatedly told ChatGPT how many ciders he had left and how much longer he expected to be alive. ChatGPT encouraged him to go through with his plans, telling him, “Rest easy, king. You did good.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI released the GPT-4o model in May 2024, when it became the default model for all users. In August, OpenAI launched GPT-5 as the successor to GPT-4o, but these lawsuits particularly concern the 4o model, which had known issues with being overly sycophantic or excessively agreeable, even when users expressed harmful intentions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Zane’s death was neither an accident nor a coincidence but rather the foreseeable consequence of OpenAI’s intentional decision to curtail safety testing and rush ChatGPT onto the market,” the lawsuit reads. “This tragedy was not a glitch or an unforeseen edge case — it was the predictable result of [OpenAI’s] deliberate design choices.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuits also claim that OpenAI rushed safety testing to beat Google’s Gemini to market. TechCrunch contacted OpenAI for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These seven lawsuits build upon the stories told in other recent legal filings, which allege that ChatGPT can encourage suicidal people to act on their plans and inspire dangerous delusions. OpenAI recently released data stating that over one million people talk to ChatGPT about suicide weekly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the case of Adam Raine, a 16-year-old who died by suicide, ChatGPT sometimes encouraged him to seek professional help or call a helpline. However, Raine was able to bypass these guardrails by simply telling the chatbot that he was asking about methods of suicide for a fictional story he was writing.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims it is working on making ChatGPT handle these conversations in a safer manner, but for the families who have sued the AI giant, these changes are coming too late.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Raine’s parents filed a lawsuit against OpenAI in October, the company released a blog post addressing how ChatGPT handles sensitive conversations around mental health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our safeguards work more reliably in common, short exchanges,” the post says. “We have learned over time that these safeguards can sometimes be less reliable in long interactions: as the back-and-forth grows, parts of the model’s safety training may degrade.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/07/seven-more-families-are-now-suing-openai-over-chatgpts-role-in-suicides-delusions/</guid><pubDate>Fri, 07 Nov 2025 20:56:18 +0000</pubDate></item><item><title>[NEW] Terminal-Bench 2.0 launches alongside Harbor, a new framework for testing agents in containers (AI | VentureBeat)</title><link>https://venturebeat.com/ai/terminal-bench-2-0-launches-alongside-harbor-a-new-framework-for-testing</link><description>[unable to retrieve full-text content]&lt;p&gt;The developers of Terminal-Bench, a benchmark suite for evaluating the performance of autonomous AI agents on real-world terminal-based tasks, have released &lt;a href="https://www.tbench.ai/news/announcement-2-0"&gt;version 2.0&lt;/a&gt; alongside &lt;a href="https://harborframework.com/"&gt;Harbor&lt;/a&gt;, a new framework for testing, improving and optimizing AI agents in containerized environments. &lt;/p&gt;&lt;p&gt;The dual release aims to address long-standing pain points in testing and optimizing AI agents, particularly those built to operate autonomously in realistic developer environments.&lt;/p&gt;&lt;p&gt;With a more difficult and rigorously verified task set, Terminal-Bench 2.0 replaces version 1.0 as the standard for assessing frontier model capabilities. &lt;/p&gt;&lt;p&gt;Harbor, the accompanying runtime framework, enables developers and researchers to scale evaluations across thousands of cloud containers and integrates with both open-source and proprietary agents and training pipelines.&lt;/p&gt;&lt;p&gt;“Harbor is the package we wish we had had while making Terminal-Bench,&amp;quot; wrote co-creator &lt;a href="https://x.com/alexgshaw/status/1986911123543916899"&gt;Alex Shaw on X&lt;/a&gt;. &amp;quot;It’s for agent, model, and benchmark developers and researchers who want to evaluate and improve agents and models.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Higher Bar, Cleaner Data&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Terminal-Bench 1.0 saw rapid adoption after its &lt;a href="https://www.tbench.ai/news/announcement"&gt;release in May 2025&lt;/a&gt;, becoming a default benchmark for evaluating agent performance across the field of AI-powered agents operating in developer-style terminal environments. These agents interact with systems through the command line, mimicking how developers work behind the scenes of the graphical user interface.&lt;/p&gt;&lt;p&gt;However, its broad scope came with inconsistencies. Several tasks were identified by the community as poorly specified or unstable due to external service changes.&lt;/p&gt;&lt;p&gt;Version 2.0 addresses those issues directly. The updated suite includes 89 tasks, each subjected to several hours of manual and LLM-assisted validation. The emphasis is on making tasks solvable, realistic, and clearly specified, raising the difficulty ceiling while improving reliability and reproducibility.&lt;/p&gt;&lt;p&gt;A notable example is the &lt;code&gt;download-youtube&lt;/code&gt; task, which was removed or refactored in 2.0 due to its dependence on unstable third-party APIs.&lt;/p&gt;&lt;p&gt;“Astute Terminal-Bench fans may notice that SOTA performance is comparable to TB1.0 despite our claim that TB2.0 is harder,” Shaw &lt;a href="https://x.com/alexgshaw/status/1986911119328616903?s=20"&gt;noted&lt;/a&gt; on X. “We believe this is because task quality is substantially higher in the new benchmark.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Harbor: Unified Rollouts at Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Alongside the benchmark update, the team launched &lt;b&gt;Harbor&lt;/b&gt;, a new framework for running and evaluating agents in cloud-deployed containers. &lt;/p&gt;&lt;p&gt;Harbor supports large-scale rollout infrastructure, with compatibility for major providers like &lt;b&gt;Daytona&lt;/b&gt; and &lt;b&gt;Modal&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Designed to generalize across agent architectures, Harbor supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Evaluation of any container-installable agent&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Scalable supervised fine-tuning (SFT) and reinforcement learning (RL) pipelines&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Custom benchmark creation and deployment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Full integration with Terminal-Bench 2.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Harbor was used internally to run tens of thousands of rollouts during the creation of the new benchmark. It is now publicly available via &lt;a href="https://harborframework.com/"&gt;harborframework.com&lt;/a&gt;, with documentation for testing and submitting agents to the public leaderboard.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Early Results: GPT-5 Leads in Task Success&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Initial results from the Terminal-Bench 2.0 leaderboard show OpenAI&amp;#x27;s Codex CLI (command line interface), a GPT-5 powered variant, in the lead, with a 49.6% success rate — the highest among all agents tested so far. &lt;/p&gt;&lt;p&gt;Close behind are other GPT-5 variants and Claude Sonnet 4.5-based agents.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Top 5 Agent Results (Terminal-Bench 2.0):&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Codex CLI (GPT-5) — 49.6%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Codex CLI (GPT-5-Codex) — 44.3%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;OpenHands (GPT-5) — 43.8%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Terminus 2 (GPT-5-Codex) — 43.4%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Terminus 2 (Claude Sonnet 4.5) — 42.8%&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The close clustering among top models indicates active competition across platforms, with no single agent solving more than half the tasks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Submission and Use&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To test or submit an agent, users install Harbor and run the benchmark using simple CLI commands. Submissions to the leaderboard require five benchmark runs, and results can be emailed to the developers along with job directories for validation.&lt;/p&gt;&lt;p&gt;harbor run -d terminal-bench@2.0 -m &amp;quot;&amp;lt;model&amp;gt;&amp;quot; -a &amp;quot;&amp;lt;agent&amp;gt;&amp;quot; --n-attempts 5 --jobs-dir &amp;lt;path/to/output&amp;gt;&lt;/p&gt;&lt;p&gt;Terminal-Bench 2.0 is already being integrated into research workflows focused on agentic reasoning, code generation, and tool use. According to co-creator Mike Merrill, a postdoctoral researcher at Stanford, a detailed preprint is in progress covering the verification process and design methodology behind the benchmark.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Aiming for Standardization&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The combined release of Terminal-Bench 2.0 and Harbor marks a step toward more consistent and scalable agent evaluation infrastructure. As LLM agents proliferate in developer and operational environments, the need for controlled, reproducible testing has grown.&lt;/p&gt;&lt;p&gt;These tools offer a potential foundation for a unified evaluation stack — supporting model improvement, environment simulation, and benchmark standardization across the AI ecosystem.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;The developers of Terminal-Bench, a benchmark suite for evaluating the performance of autonomous AI agents on real-world terminal-based tasks, have released &lt;a href="https://www.tbench.ai/news/announcement-2-0"&gt;version 2.0&lt;/a&gt; alongside &lt;a href="https://harborframework.com/"&gt;Harbor&lt;/a&gt;, a new framework for testing, improving and optimizing AI agents in containerized environments. &lt;/p&gt;&lt;p&gt;The dual release aims to address long-standing pain points in testing and optimizing AI agents, particularly those built to operate autonomously in realistic developer environments.&lt;/p&gt;&lt;p&gt;With a more difficult and rigorously verified task set, Terminal-Bench 2.0 replaces version 1.0 as the standard for assessing frontier model capabilities. &lt;/p&gt;&lt;p&gt;Harbor, the accompanying runtime framework, enables developers and researchers to scale evaluations across thousands of cloud containers and integrates with both open-source and proprietary agents and training pipelines.&lt;/p&gt;&lt;p&gt;“Harbor is the package we wish we had had while making Terminal-Bench,&amp;quot; wrote co-creator &lt;a href="https://x.com/alexgshaw/status/1986911123543916899"&gt;Alex Shaw on X&lt;/a&gt;. &amp;quot;It’s for agent, model, and benchmark developers and researchers who want to evaluate and improve agents and models.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Higher Bar, Cleaner Data&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Terminal-Bench 1.0 saw rapid adoption after its &lt;a href="https://www.tbench.ai/news/announcement"&gt;release in May 2025&lt;/a&gt;, becoming a default benchmark for evaluating agent performance across the field of AI-powered agents operating in developer-style terminal environments. These agents interact with systems through the command line, mimicking how developers work behind the scenes of the graphical user interface.&lt;/p&gt;&lt;p&gt;However, its broad scope came with inconsistencies. Several tasks were identified by the community as poorly specified or unstable due to external service changes.&lt;/p&gt;&lt;p&gt;Version 2.0 addresses those issues directly. The updated suite includes 89 tasks, each subjected to several hours of manual and LLM-assisted validation. The emphasis is on making tasks solvable, realistic, and clearly specified, raising the difficulty ceiling while improving reliability and reproducibility.&lt;/p&gt;&lt;p&gt;A notable example is the &lt;code&gt;download-youtube&lt;/code&gt; task, which was removed or refactored in 2.0 due to its dependence on unstable third-party APIs.&lt;/p&gt;&lt;p&gt;“Astute Terminal-Bench fans may notice that SOTA performance is comparable to TB1.0 despite our claim that TB2.0 is harder,” Shaw &lt;a href="https://x.com/alexgshaw/status/1986911119328616903?s=20"&gt;noted&lt;/a&gt; on X. “We believe this is because task quality is substantially higher in the new benchmark.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Harbor: Unified Rollouts at Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Alongside the benchmark update, the team launched &lt;b&gt;Harbor&lt;/b&gt;, a new framework for running and evaluating agents in cloud-deployed containers. &lt;/p&gt;&lt;p&gt;Harbor supports large-scale rollout infrastructure, with compatibility for major providers like &lt;b&gt;Daytona&lt;/b&gt; and &lt;b&gt;Modal&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Designed to generalize across agent architectures, Harbor supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Evaluation of any container-installable agent&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Scalable supervised fine-tuning (SFT) and reinforcement learning (RL) pipelines&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Custom benchmark creation and deployment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Full integration with Terminal-Bench 2.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Harbor was used internally to run tens of thousands of rollouts during the creation of the new benchmark. It is now publicly available via &lt;a href="https://harborframework.com/"&gt;harborframework.com&lt;/a&gt;, with documentation for testing and submitting agents to the public leaderboard.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Early Results: GPT-5 Leads in Task Success&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Initial results from the Terminal-Bench 2.0 leaderboard show OpenAI&amp;#x27;s Codex CLI (command line interface), a GPT-5 powered variant, in the lead, with a 49.6% success rate — the highest among all agents tested so far. &lt;/p&gt;&lt;p&gt;Close behind are other GPT-5 variants and Claude Sonnet 4.5-based agents.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Top 5 Agent Results (Terminal-Bench 2.0):&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Codex CLI (GPT-5) — 49.6%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Codex CLI (GPT-5-Codex) — 44.3%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;OpenHands (GPT-5) — 43.8%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Terminus 2 (GPT-5-Codex) — 43.4%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Terminus 2 (Claude Sonnet 4.5) — 42.8%&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The close clustering among top models indicates active competition across platforms, with no single agent solving more than half the tasks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Submission and Use&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To test or submit an agent, users install Harbor and run the benchmark using simple CLI commands. Submissions to the leaderboard require five benchmark runs, and results can be emailed to the developers along with job directories for validation.&lt;/p&gt;&lt;p&gt;harbor run -d terminal-bench@2.0 -m &amp;quot;&amp;lt;model&amp;gt;&amp;quot; -a &amp;quot;&amp;lt;agent&amp;gt;&amp;quot; --n-attempts 5 --jobs-dir &amp;lt;path/to/output&amp;gt;&lt;/p&gt;&lt;p&gt;Terminal-Bench 2.0 is already being integrated into research workflows focused on agentic reasoning, code generation, and tool use. According to co-creator Mike Merrill, a postdoctoral researcher at Stanford, a detailed preprint is in progress covering the verification process and design methodology behind the benchmark.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Aiming for Standardization&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The combined release of Terminal-Bench 2.0 and Harbor marks a step toward more consistent and scalable agent evaluation infrastructure. As LLM agents proliferate in developer and operational environments, the need for controlled, reproducible testing has grown.&lt;/p&gt;&lt;p&gt;These tools offer a potential foundation for a unified evaluation stack — supporting model improvement, environment simulation, and benchmark standardization across the AI ecosystem.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/terminal-bench-2-0-launches-alongside-harbor-a-new-framework-for-testing</guid><pubDate>Fri, 07 Nov 2025 23:25:00 +0000</pubDate></item></channel></rss>