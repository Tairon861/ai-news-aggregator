<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 24 Oct 2025 01:37:45 +0000</lastBuildDate><item><title>Cluely’s Roy Lee joins TechCrunch Disrupt 2025 to show how rage-baiting cuts through the AI noise (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/cluelys-roy-lee-joins-techcrunch-disrupt-2025-to-show-how-rage-baiting-cuts-through-the-ai-noise/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cluely’s&amp;nbsp;Roy Lee&amp;nbsp;isn’t&amp;nbsp;afraid to stir the pot — and&amp;nbsp;he’s&amp;nbsp;taking that bold energy to the&amp;nbsp;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&amp;nbsp;at&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27-29 at San Francisco’s Moscone West.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’ll&amp;nbsp;dive into how&amp;nbsp;he’s&amp;nbsp;building one of the most talked-about startups in AI, why courting controversy can be a growth strategy, and what it takes to stand out in a noisy consumer tech market.&amp;nbsp;Don’t&amp;nbsp;miss this chance to see Roy Lee shake things up at Disrupt.&amp;nbsp;&lt;strong&gt;Register here to save up to $444 on your&amp;nbsp;pass, and&amp;nbsp;get 60% off a second.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Roy Lee" class="wp-image-3061224" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Roy-Lee-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-how-roy-lee-s-rage-baiting-built-a-15-million-ai-startup"&gt;How Roy Lee’s&amp;nbsp;rage-baiting built a $15 million AI startup&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Roy Lee&amp;nbsp;is the co-founder and CEO of&amp;nbsp;Cluely, an AI meeting assistant that offers real-time insights&amp;nbsp;and turns conversations&amp;nbsp;into searchable and shareable reports. Hundreds of companies are pitching&amp;nbsp;similar products, so why do we care about&amp;nbsp;Cluely? What really sets Lee apart is his mastery of attention. In an era when most startups fight to be noticed, Lee has made rage-baiting social media content&amp;nbsp;his business model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Lee originally went viral as a Columbia&amp;nbsp;University student&amp;nbsp;who&amp;nbsp;got in&amp;nbsp;trouble for&amp;nbsp;cheating on Big Tech interviews with an AI assistant he built, all of which he documented on social media. Lee then dropped out&amp;nbsp;and launched&amp;nbsp;Cluely, which was initially pitched as the “cheat on everything startup.” It was a tagline designed to ruffle feathers and spur engagement — a strategy that&amp;nbsp;Cluely&amp;nbsp;has become famous for, and others have tried to copy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Say&amp;nbsp;what you will about Lee’s marketing efforts, but&amp;nbsp;Cluely&amp;nbsp;routinely breaks through the noise of social media and press releases, earning the startup visibility that companies 10 times its size struggle to buy. Lee operates Cluely on the principle that most companies are playing it too safe and that attention is all you need to win the consumer&amp;nbsp;market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cluely&amp;nbsp;raised $15 million&amp;nbsp;in a Series A from&amp;nbsp;Andreessen Horowitz&amp;nbsp;earlier this year and has spent its funding on flashy launch videos, dozens of content creator interns, and a massive office in San Francisco. Lee is one of the founders at the center of the AI bubble, and&amp;nbsp;he’s&amp;nbsp;used it to his advantage.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Cluely founders" class="wp-image-2996785" height="453" src="https://techcrunch.com/wp-content/uploads/2025/04/IMG-0036-from-100CANON.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Cluely founders.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Cluely&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-learn-how-stirring-the-pot-fuels-growth-grab-your-pass-before-prices-rise"&gt;Learn how stirring the pot fuels growth — grab your pass before prices rise&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt&amp;nbsp;2025&lt;/strong&gt;, Lee will share how he thinks about virality as a marketing strategy, his thoughts on the AI bubble, and how&amp;nbsp;Cluely&amp;nbsp;is building a lasting business on top of its social media fame.&amp;nbsp;&lt;strong&gt;Register now to save up to $444 on your pass, plus get 60% off a second&lt;/strong&gt;.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Prices increase when event doors open on October 27.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 Aravind Srinivas" class="wp-image-3051285" height="453" src="https://techcrunch.com/wp-content/uploads/2025/09/Perplexity-Disrupt-Stage-2024.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cluely’s&amp;nbsp;Roy Lee&amp;nbsp;isn’t&amp;nbsp;afraid to stir the pot — and&amp;nbsp;he’s&amp;nbsp;taking that bold energy to the&amp;nbsp;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&amp;nbsp;at&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27-29 at San Francisco’s Moscone West.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’ll&amp;nbsp;dive into how&amp;nbsp;he’s&amp;nbsp;building one of the most talked-about startups in AI, why courting controversy can be a growth strategy, and what it takes to stand out in a noisy consumer tech market.&amp;nbsp;Don’t&amp;nbsp;miss this chance to see Roy Lee shake things up at Disrupt.&amp;nbsp;&lt;strong&gt;Register here to save up to $444 on your&amp;nbsp;pass, and&amp;nbsp;get 60% off a second.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Roy Lee" class="wp-image-3061224" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Roy-Lee-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-how-roy-lee-s-rage-baiting-built-a-15-million-ai-startup"&gt;How Roy Lee’s&amp;nbsp;rage-baiting built a $15 million AI startup&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Roy Lee&amp;nbsp;is the co-founder and CEO of&amp;nbsp;Cluely, an AI meeting assistant that offers real-time insights&amp;nbsp;and turns conversations&amp;nbsp;into searchable and shareable reports. Hundreds of companies are pitching&amp;nbsp;similar products, so why do we care about&amp;nbsp;Cluely? What really sets Lee apart is his mastery of attention. In an era when most startups fight to be noticed, Lee has made rage-baiting social media content&amp;nbsp;his business model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Lee originally went viral as a Columbia&amp;nbsp;University student&amp;nbsp;who&amp;nbsp;got in&amp;nbsp;trouble for&amp;nbsp;cheating on Big Tech interviews with an AI assistant he built, all of which he documented on social media. Lee then dropped out&amp;nbsp;and launched&amp;nbsp;Cluely, which was initially pitched as the “cheat on everything startup.” It was a tagline designed to ruffle feathers and spur engagement — a strategy that&amp;nbsp;Cluely&amp;nbsp;has become famous for, and others have tried to copy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Say&amp;nbsp;what you will about Lee’s marketing efforts, but&amp;nbsp;Cluely&amp;nbsp;routinely breaks through the noise of social media and press releases, earning the startup visibility that companies 10 times its size struggle to buy. Lee operates Cluely on the principle that most companies are playing it too safe and that attention is all you need to win the consumer&amp;nbsp;market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cluely&amp;nbsp;raised $15 million&amp;nbsp;in a Series A from&amp;nbsp;Andreessen Horowitz&amp;nbsp;earlier this year and has spent its funding on flashy launch videos, dozens of content creator interns, and a massive office in San Francisco. Lee is one of the founders at the center of the AI bubble, and&amp;nbsp;he’s&amp;nbsp;used it to his advantage.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Cluely founders" class="wp-image-2996785" height="453" src="https://techcrunch.com/wp-content/uploads/2025/04/IMG-0036-from-100CANON.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Cluely founders.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Cluely&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-learn-how-stirring-the-pot-fuels-growth-grab-your-pass-before-prices-rise"&gt;Learn how stirring the pot fuels growth — grab your pass before prices rise&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt&amp;nbsp;2025&lt;/strong&gt;, Lee will share how he thinks about virality as a marketing strategy, his thoughts on the AI bubble, and how&amp;nbsp;Cluely&amp;nbsp;is building a lasting business on top of its social media fame.&amp;nbsp;&lt;strong&gt;Register now to save up to $444 on your pass, plus get 60% off a second&lt;/strong&gt;.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Prices increase when event doors open on October 27.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 Aravind Srinivas" class="wp-image-3051285" height="453" src="https://techcrunch.com/wp-content/uploads/2025/09/Perplexity-Disrupt-Stage-2024.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/cluelys-roy-lee-joins-techcrunch-disrupt-2025-to-show-how-rage-baiting-cuts-through-the-ai-noise/</guid><pubDate>Thu, 23 Oct 2025 15:00:00 +0000</pubDate></item><item><title>Open Source AI Week — How Developers and Contributors Are Advancing AI Innovation (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/open-source-ai-week/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="startup-showcase"&gt;&lt;b&gt;NVIDIA Inception Startups Highlight AI Innovation 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86210 size-medium" height="640" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/PTC-Startup-Winners-960x640.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;At the PyTorch Conference’s Startup Showcase, 11 startups — including members from the NVIDIA Inception program — are sharing their work developing practical AI applications and connecting with investors, potential customers and peers.&lt;/p&gt;
&lt;p&gt;Runhouse, an AI infrastructure startup optimizing model deployment and orchestration, was crowned the 2025 PyTorch Startup Showcase Award Winner. The Community Choice Award was presented to CuraVoice, with CEO Sakhi Patel, CTO Shrey Modi, and advisor Rahul Vishwakarma accepting the award on behalf of the team.&lt;/p&gt;
&lt;p&gt;CuraVoice provides an AI-powered voice simulation platform — powered by NVIDIA Riva for speech recognition and text-to-speech, and NVIDIA NeMo for conversational AI models — for healthcare students and professionals, offering interactive exercises and adaptive feedback to improve patient communication skills.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86207"&gt;&lt;img alt="alt" class="wp-image-86207 size-thumbnail" height="400" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/curavoice-award-400x400.jpg" width="400" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86207"&gt;Shrey Modi, CTO of CuraVoice, accepts the PyTorch Startup Showcase Community Choice Award.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In addition to CuraVoice, other Inception members, including Backfield AI, Graphsignal, Okahu AI, Snapshot AI and XOR, were featured participants in the Startup Showcase.&lt;/p&gt;
&lt;p&gt;Snapshot AI delivers actionable, real-time insights to engineering teams using recursive retrieval-augmented generation (RAG), transformers and multimodal AI. The company’s platform taps into the NVIDIA CUDA Toolkit to deliver high-performance analysis and rapid insights at scale.&lt;/p&gt;
&lt;p&gt;XOR is a cybersecurity startup offering AI agents that automatically fix vulnerabilities in the supply chain of other AIs. The company helps enterprises eliminate vulnerabilities while complying with regulatory requirements. XOR’s agentic technology uses NVIDIA cuVS vector search for indexing, real-time retrieval and code analysis. The company also uses GPU-based machine learning to train models to detect hidden backdoor patterns and prioritize of high-value security outcomes.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86204"&gt;&lt;img alt="alt" class="wp-image-86204 size-medium" height="778" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/startup-showcase-960x778.jpg" width="960" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86204"&gt;From left to right: Dmitri Melikyan (Graphsignal, Inc.), Tobias Heldt (XOR), Youssef Harkati (BrightOnLABS), Vidhi Kothari (Seer Systems), Jonah Sargent (Node One) and Scott Suchyta (NVIDIA) at the Startup Showcase.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 class="wp-block-heading" id="highlights"&gt;&lt;b&gt;Highlights From Open Source AI Week 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Attendees of Open Source AI Week are getting a peek at the latest advancements and creative projects that are shaping the future of open technology.&lt;/p&gt;
&lt;p&gt;Here’s a look at what’s happening onsite:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;&lt;b&gt;The world’s smallest AI supercomputer: &lt;/b&gt;NVIDIA DGX Spark represents the cutting edge of AI computing hardware for enterprise and research applications.​&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86192 size-thumbnail" height="400" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/unitree-blog-400x400.jpg" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Humanoids and robot dogs, up close: &lt;/b&gt;Unitree robots are on display, captivating attendees with advanced mobility powered by the latest robotics technology.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Why open source is important:&lt;/b&gt; Learn how it can empower developers to build stronger communities, iterate on features, and seamlessly integrate the best of open source AI.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="research"&gt;&lt;b&gt;Accelerating AI Research Through Open Models 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;A study from the Center for Security and Emerging Technology (CSET) published today shows how access to open model weights unlocks more opportunities for experimentation, customization and collaboration across the global research community.​&lt;/p&gt;
&lt;p&gt;The report outlines seven high-impact research use cases where open models are making a difference — including fine-tuning, continued pretraining, model compression and interpretability.&lt;/p&gt;
&lt;p&gt;With access to weights, developers can adapt models for new domains, explore new architectures and extend functionality to meet their specific needs. This also supports trust and reproducibility. When teams can run experiments on their own hardware, share updates and revisit earlier versions, they gain control and confidence in their results.&lt;/p&gt;
&lt;p&gt;Additionally, the study found that nearly all open model users share their data, weights and code, building a fast-growing culture of collaboration. This open exchange of tools and knowledge strengthens partnerships between academia, startups and enterprises, facilitating innovation.&lt;/p&gt;
&lt;p&gt;NVIDIA is committed to empowering the research community through the NVIDIA Nemotron family of open models — featuring not just open weights, but also pretraining and post-training datasets, detailed training recipes, and research papers that share the latest breakthroughs.&lt;/p&gt;
&lt;p&gt;Read the full CSET study to learn how open models are helping the AI community move forward.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="keynote"&gt;&lt;b&gt;Advancing Embodied Intelligence Through Open-Source Innovation 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86142 size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ptc-keynote-960x540.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;At the PyTorch Conference, Jim Fan, director of robotics and distinguished research scientist at NVIDIA, discussed the Physical Turing Test — a way of measuring the performance of intelligent machines in the physical world.&lt;/p&gt;
&lt;p&gt;With conversational AI now capable of fluent, lifelike communication, Fan noted that the next challenge is enabling machines to act with similar naturalism. The Physical Turing Test asks: can an intelligent machine perform a real-world task so fluidly that a human cannot tell whether a person or a robot completed it?&lt;/p&gt;
&lt;p&gt;Fan highlighted that progress in embodied AI and physical AI depends on generating large amounts of diverse data, access to open robot foundation models and simulation frameworks — and walked through a unified workflow for developing embodied AI.&lt;/p&gt;
&lt;p&gt;With synthetic data workflows like NVIDIA Isaac GR00T-Dreams — built on NVIDIA Cosmos world foundation models— developers can generate virtual worlds from images and prompts, speeding the creation of large sets of diverse and physically accurate data.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;That data can then be used to post-train NVIDIA Isaac GR00T N open foundation models for generalized humanoid robot reasoning and skills. But before the models are deployed in the real world, these new robot skills need to be tested in simulation.&lt;/p&gt;
&lt;p&gt;Open simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab allow robots to “practice” countless times across millions of virtual environments before operating in the real world, dramatically accelerating learning and deployment cycles.&lt;/p&gt;
&lt;p&gt;Plus, with Newton, an open-source, differentiable physics engine built on NVIDIA Warp and OpenUSD, developers can bring high-fidelity simulation to complex robotic dynamics such as motion, balance and contact — reducing the simulation-to-real gap.&lt;/p&gt;
&lt;p&gt;This accelerates the creation of physically capable AI systems that learn faster, perform more safely and operate effectively in real-world environments.&lt;/p&gt;
&lt;p&gt;However, scaling embodied intelligence isn’t just about compute — it’s about access. Fan reaffirmed NVIDIA’s commitment to open source, emphasizing how the company’s frameworks and foundation models are shared to empower developers and researchers globally.&lt;/p&gt;
&lt;p&gt;Developers can get started with NVIDIA’s open embodied and physical AI models on Hugging Face.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="nemotron"&gt;&lt;b&gt;Llama‑Embed‑Nemotron‑8B Ranks Among Top Open Models for Multilingual Retrieval 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA’s Llama‑Embed‑Nemotron‑8B model has been recognized as the top open and portable model on the Multilingual Text Embedding Benchmark leaderboard.&lt;/p&gt;
&lt;p&gt;Built on the meta‑llama/Llama‑3.1‑8B architecture, Llama‑Embed‑Nemotron‑8B is a research text embedding model that converts text into 4,096‑dimensional vector representations. Designed for flexibility, it supports a wide range of use cases, including retrieval, reranking, semantic similarity and classification across more than 1,000 languages.&lt;/p&gt;
&lt;p&gt;Trained on a diverse collection of 16 million query–document pairs — half from public sources and half synthetically generated — the model benefits from refined data generation techniques, hard‑negative mining and model‑merging approaches that contribute to its broad generalization capabilities.&lt;/p&gt;
&lt;p&gt;This result builds on NVIDIA’s ongoing research in open, high‑performing AI models. Following earlier leaderboard recognition for the Llama NeMo Retriever ColEmbed model, the success of Llama‑Embed‑Nemotron‑8B highlights the value of openness, transparency and collaboration in advancing AI for the developer community.&lt;/p&gt;
&lt;p&gt;Check out Llama-Embed-Nemotron-8B on Hugging Face, and learn more about the model, including architectural highlights, training methodology and performance evaluation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-86053" height="470" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ezgif.com-video-to-gif-converter-7.gif" width="800" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Open Source Teaches Us About Making AI Better&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open models are shaping the future of AI, enabling developers, enterprises and governments to innovate with transparency, customization and trust. In the latest episode of the NVIDIA AI Podcast, NVIDIA’s Bryan Catanzaro and Jonathan Cohen discuss how open models, datasets and research are laying the foundation for shared progress across the AI ecosystem.&lt;/p&gt;
&lt;p&gt;The NVIDIA Nemotron family of open models represents a full-stack approach to AI development, connecting model design to the underlying hardware and software that power it. By releasing Nemotron models, data and training methodologies openly, NVIDIA aims to help others refine, adapt and build upon its work, resulting in a faster exchange of ideas and more efficient systems.&lt;/p&gt;
&lt;p&gt;“When we as a community come together — contributing ideas, data and models — we all move faster,” said Catanzaro in the episode. “Open technologies make that possible.”&lt;/p&gt;

&lt;p&gt;There’s more happening this week at Open Source AI Week, including the start of the PyTorch Conference — bringing together developers, researchers and innovators pushing the boundaries of open AI.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Attendees can tune in to the special keynote address by Jim Fan, director of robotics and distinguished research scientist at NVIDIA, to hear the latest advancements in robotics — from simulation and synthetic data to accelerated computing. The keynote, titled “The Physical Turing Test: Solving General Purpose Robotics,” will take place on Wednesday, Oct. 22, from 9:50-10:05 a.m. PT.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;b&gt;Andrej Karpathy’s Nanochat Teaches Developers How to Train LLMs in Four Hours 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Computer scientist Andrej Karpathy recently introduced Nanochat, calling it “the best ChatGPT that $100 can buy.” Nanochat is an open-source, full-stack large language model (LLM) implementation built for transparency and experimentation. In about 8,000 lines of minimal, dependency-light code, Nanochat runs the entire LLM pipeline — from tokenization and pretraining to fine-tuning, inference and chat — all through a simple web user interface.&lt;br /&gt;NVIDIA is supporting Karpathy’s open-source Nanochat project by releasing two NVIDIA Launchables, making it easy to deploy and experiment with Nanochat across various NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;With NVIDIA Launchables, developers can train and interact with their own conversational model in hours with a single click. The Launchables dynamically support different-sized GPUs — including NVIDIA H100 and L40S GPUs — on various clouds without need for modification. They also automatically work on any eight-GPU instance on NVIDIA Brev, so developers can get compute access immediately.&lt;/p&gt;
&lt;p&gt;The &lt;b&gt;first 10 users&lt;/b&gt; to deploy these Launchables will also receive free compute access to NVIDIA H100 or L40S GPUs.&lt;/p&gt;
&lt;p&gt;Start training with Nanochat by deploying a Launchable:&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86022 size-medium" height="527" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/launchable-960x527.png" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Andrej Karpathy’s Next Experiments Begin With NVIDIA DGX Spark&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today, Karpathy received an NVIDIA DGX Spark — the world’s smallest AI supercomputer, designed to bring the power of Blackwell right to a developer’s desktop. With up to a petaflop of AI processing power and 128GB of unified memory in a compact form factor, DGX Spark empowers innovators like Karpathy to experiment, fine-tune and run massive models locally.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="pytorch"&gt;&lt;b&gt;Building the Future of AI With PyTorch and NVIDIA 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;PyTorch, the fastest-growing AI framework, derives its performance from the NVIDIA CUDA platform and uses the Python programming language to unlock developer productivity. This year, NVIDIA added Python as a first-class language to the CUDA platform, giving the PyTorch developer community greater access to CUDA.&lt;/p&gt;
&lt;p&gt;CUDA Python includes key components that make GPU acceleration in Python easier than ever, with built-in support for kernel fusion, extension module integration and simplified packaging for fast deployment.&lt;/p&gt;
&lt;p&gt;Following PyTorch’s open collaboration model, CUDA Python is available on GitHub and PyPI.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86034"&gt;&lt;img alt="alt" class="wp-image-86034 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/pytorch-infographic-1-1680x672.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86034"&gt;According to PyPI Stats, PyTorch averaged over two million daily downloads, peaking at 2,303,217 on October 14, and had 65 million total downloads last month.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Every month, developers worldwide download hundreds of millions of NVIDIA libraries — including CUDA, cuDNN, cuBLAS and CUTLASS — mostly within Python and PyTorch environments. CUDA Python provides nvmath-python, a new library that acts as the bridge between Python code and these highly optimized GPU libraries.&lt;/p&gt;
&lt;p&gt;Plus, kernel enhancements and support for next-generation frameworks make NVIDIA accelerated computing more efficient, adaptable and widely accessible.&lt;/p&gt;
&lt;p&gt;NVIDIA maintains a long-standing collaboration with the PyTorch community through open-source contributions and technical leadership, as well as by sponsoring and participating in community events and activations.&lt;/p&gt;
&lt;p&gt;At PyTorch Conference 2025 in San Francisco, NVIDIA will host a keynote address, five technical sessions and nine poster presentations.&lt;/p&gt;
&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="open-source-ai"&gt;&lt;b&gt;NVIDIA Spotlights Open Source Innovation 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open Source AI Week kicks off on Monday with a series of hackathons, workshops and meetups spotlighting the latest advances in AI, machine learning and open-source innovation.&lt;/p&gt;
&lt;p&gt;The event brings together leading organizations, researchers and open-source communities to share knowledge, collaborate on tools and explore how openness accelerates AI development.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to expand access to advanced AI innovation by providing open-source tools, models and datasets designed to empower developers. With more than 1,000 open-source tools on NVIDIA GitHub repositories and over 500 models and 100 datasets on the NVIDIA Hugging Face collections, NVIDIA is accelerating the pace of open, collaborative AI development.&lt;/p&gt;
&lt;p&gt;Over the past year, NVIDIA has become the top contributor in Hugging Face repositories, reflecting a deep commitment to sharing models, frameworks and research that empower the community.&lt;/p&gt;

&lt;p&gt;Openly available models, tools and datasets are essential to driving innovation and progress. By empowering anyone to use, modify and share technology, it fosters transparency and accelerates discovery, fueling breakthroughs that benefit both industry and communities alike. That’s why NVIDIA is committed to supporting the open source ecosystem.&lt;/p&gt;
&lt;p&gt;We’re on the ground all week — stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward, with the PyTorch Conference serving as the flagship event.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="startup-showcase"&gt;&lt;b&gt;NVIDIA Inception Startups Highlight AI Innovation 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86210 size-medium" height="640" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/PTC-Startup-Winners-960x640.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;At the PyTorch Conference’s Startup Showcase, 11 startups — including members from the NVIDIA Inception program — are sharing their work developing practical AI applications and connecting with investors, potential customers and peers.&lt;/p&gt;
&lt;p&gt;Runhouse, an AI infrastructure startup optimizing model deployment and orchestration, was crowned the 2025 PyTorch Startup Showcase Award Winner. The Community Choice Award was presented to CuraVoice, with CEO Sakhi Patel, CTO Shrey Modi, and advisor Rahul Vishwakarma accepting the award on behalf of the team.&lt;/p&gt;
&lt;p&gt;CuraVoice provides an AI-powered voice simulation platform — powered by NVIDIA Riva for speech recognition and text-to-speech, and NVIDIA NeMo for conversational AI models — for healthcare students and professionals, offering interactive exercises and adaptive feedback to improve patient communication skills.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86207"&gt;&lt;img alt="alt" class="wp-image-86207 size-thumbnail" height="400" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/curavoice-award-400x400.jpg" width="400" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86207"&gt;Shrey Modi, CTO of CuraVoice, accepts the PyTorch Startup Showcase Community Choice Award.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In addition to CuraVoice, other Inception members, including Backfield AI, Graphsignal, Okahu AI, Snapshot AI and XOR, were featured participants in the Startup Showcase.&lt;/p&gt;
&lt;p&gt;Snapshot AI delivers actionable, real-time insights to engineering teams using recursive retrieval-augmented generation (RAG), transformers and multimodal AI. The company’s platform taps into the NVIDIA CUDA Toolkit to deliver high-performance analysis and rapid insights at scale.&lt;/p&gt;
&lt;p&gt;XOR is a cybersecurity startup offering AI agents that automatically fix vulnerabilities in the supply chain of other AIs. The company helps enterprises eliminate vulnerabilities while complying with regulatory requirements. XOR’s agentic technology uses NVIDIA cuVS vector search for indexing, real-time retrieval and code analysis. The company also uses GPU-based machine learning to train models to detect hidden backdoor patterns and prioritize of high-value security outcomes.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86204"&gt;&lt;img alt="alt" class="wp-image-86204 size-medium" height="778" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/startup-showcase-960x778.jpg" width="960" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86204"&gt;From left to right: Dmitri Melikyan (Graphsignal, Inc.), Tobias Heldt (XOR), Youssef Harkati (BrightOnLABS), Vidhi Kothari (Seer Systems), Jonah Sargent (Node One) and Scott Suchyta (NVIDIA) at the Startup Showcase.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 class="wp-block-heading" id="highlights"&gt;&lt;b&gt;Highlights From Open Source AI Week 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Attendees of Open Source AI Week are getting a peek at the latest advancements and creative projects that are shaping the future of open technology.&lt;/p&gt;
&lt;p&gt;Here’s a look at what’s happening onsite:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;&lt;b&gt;The world’s smallest AI supercomputer: &lt;/b&gt;NVIDIA DGX Spark represents the cutting edge of AI computing hardware for enterprise and research applications.​&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86192 size-thumbnail" height="400" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/unitree-blog-400x400.jpg" width="400" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Humanoids and robot dogs, up close: &lt;/b&gt;Unitree robots are on display, captivating attendees with advanced mobility powered by the latest robotics technology.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Why open source is important:&lt;/b&gt; Learn how it can empower developers to build stronger communities, iterate on features, and seamlessly integrate the best of open source AI.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="research"&gt;&lt;b&gt;Accelerating AI Research Through Open Models 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;A study from the Center for Security and Emerging Technology (CSET) published today shows how access to open model weights unlocks more opportunities for experimentation, customization and collaboration across the global research community.​&lt;/p&gt;
&lt;p&gt;The report outlines seven high-impact research use cases where open models are making a difference — including fine-tuning, continued pretraining, model compression and interpretability.&lt;/p&gt;
&lt;p&gt;With access to weights, developers can adapt models for new domains, explore new architectures and extend functionality to meet their specific needs. This also supports trust and reproducibility. When teams can run experiments on their own hardware, share updates and revisit earlier versions, they gain control and confidence in their results.&lt;/p&gt;
&lt;p&gt;Additionally, the study found that nearly all open model users share their data, weights and code, building a fast-growing culture of collaboration. This open exchange of tools and knowledge strengthens partnerships between academia, startups and enterprises, facilitating innovation.&lt;/p&gt;
&lt;p&gt;NVIDIA is committed to empowering the research community through the NVIDIA Nemotron family of open models — featuring not just open weights, but also pretraining and post-training datasets, detailed training recipes, and research papers that share the latest breakthroughs.&lt;/p&gt;
&lt;p&gt;Read the full CSET study to learn how open models are helping the AI community move forward.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="keynote"&gt;&lt;b&gt;Advancing Embodied Intelligence Through Open-Source Innovation 🔗&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86142 size-medium" height="540" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ptc-keynote-960x540.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;At the PyTorch Conference, Jim Fan, director of robotics and distinguished research scientist at NVIDIA, discussed the Physical Turing Test — a way of measuring the performance of intelligent machines in the physical world.&lt;/p&gt;
&lt;p&gt;With conversational AI now capable of fluent, lifelike communication, Fan noted that the next challenge is enabling machines to act with similar naturalism. The Physical Turing Test asks: can an intelligent machine perform a real-world task so fluidly that a human cannot tell whether a person or a robot completed it?&lt;/p&gt;
&lt;p&gt;Fan highlighted that progress in embodied AI and physical AI depends on generating large amounts of diverse data, access to open robot foundation models and simulation frameworks — and walked through a unified workflow for developing embodied AI.&lt;/p&gt;
&lt;p&gt;With synthetic data workflows like NVIDIA Isaac GR00T-Dreams — built on NVIDIA Cosmos world foundation models— developers can generate virtual worlds from images and prompts, speeding the creation of large sets of diverse and physically accurate data.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;That data can then be used to post-train NVIDIA Isaac GR00T N open foundation models for generalized humanoid robot reasoning and skills. But before the models are deployed in the real world, these new robot skills need to be tested in simulation.&lt;/p&gt;
&lt;p&gt;Open simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab allow robots to “practice” countless times across millions of virtual environments before operating in the real world, dramatically accelerating learning and deployment cycles.&lt;/p&gt;
&lt;p&gt;Plus, with Newton, an open-source, differentiable physics engine built on NVIDIA Warp and OpenUSD, developers can bring high-fidelity simulation to complex robotic dynamics such as motion, balance and contact — reducing the simulation-to-real gap.&lt;/p&gt;
&lt;p&gt;This accelerates the creation of physically capable AI systems that learn faster, perform more safely and operate effectively in real-world environments.&lt;/p&gt;
&lt;p&gt;However, scaling embodied intelligence isn’t just about compute — it’s about access. Fan reaffirmed NVIDIA’s commitment to open source, emphasizing how the company’s frameworks and foundation models are shared to empower developers and researchers globally.&lt;/p&gt;
&lt;p&gt;Developers can get started with NVIDIA’s open embodied and physical AI models on Hugging Face.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="nemotron"&gt;&lt;b&gt;Llama‑Embed‑Nemotron‑8B Ranks Among Top Open Models for Multilingual Retrieval 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA’s Llama‑Embed‑Nemotron‑8B model has been recognized as the top open and portable model on the Multilingual Text Embedding Benchmark leaderboard.&lt;/p&gt;
&lt;p&gt;Built on the meta‑llama/Llama‑3.1‑8B architecture, Llama‑Embed‑Nemotron‑8B is a research text embedding model that converts text into 4,096‑dimensional vector representations. Designed for flexibility, it supports a wide range of use cases, including retrieval, reranking, semantic similarity and classification across more than 1,000 languages.&lt;/p&gt;
&lt;p&gt;Trained on a diverse collection of 16 million query–document pairs — half from public sources and half synthetically generated — the model benefits from refined data generation techniques, hard‑negative mining and model‑merging approaches that contribute to its broad generalization capabilities.&lt;/p&gt;
&lt;p&gt;This result builds on NVIDIA’s ongoing research in open, high‑performing AI models. Following earlier leaderboard recognition for the Llama NeMo Retriever ColEmbed model, the success of Llama‑Embed‑Nemotron‑8B highlights the value of openness, transparency and collaboration in advancing AI for the developer community.&lt;/p&gt;
&lt;p&gt;Check out Llama-Embed-Nemotron-8B on Hugging Face, and learn more about the model, including architectural highlights, training methodology and performance evaluation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-86053" height="470" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ezgif.com-video-to-gif-converter-7.gif" width="800" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Open Source Teaches Us About Making AI Better&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open models are shaping the future of AI, enabling developers, enterprises and governments to innovate with transparency, customization and trust. In the latest episode of the NVIDIA AI Podcast, NVIDIA’s Bryan Catanzaro and Jonathan Cohen discuss how open models, datasets and research are laying the foundation for shared progress across the AI ecosystem.&lt;/p&gt;
&lt;p&gt;The NVIDIA Nemotron family of open models represents a full-stack approach to AI development, connecting model design to the underlying hardware and software that power it. By releasing Nemotron models, data and training methodologies openly, NVIDIA aims to help others refine, adapt and build upon its work, resulting in a faster exchange of ideas and more efficient systems.&lt;/p&gt;
&lt;p&gt;“When we as a community come together — contributing ideas, data and models — we all move faster,” said Catanzaro in the episode. “Open technologies make that possible.”&lt;/p&gt;

&lt;p&gt;There’s more happening this week at Open Source AI Week, including the start of the PyTorch Conference — bringing together developers, researchers and innovators pushing the boundaries of open AI.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Attendees can tune in to the special keynote address by Jim Fan, director of robotics and distinguished research scientist at NVIDIA, to hear the latest advancements in robotics — from simulation and synthetic data to accelerated computing. The keynote, titled “The Physical Turing Test: Solving General Purpose Robotics,” will take place on Wednesday, Oct. 22, from 9:50-10:05 a.m. PT.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;b&gt;Andrej Karpathy’s Nanochat Teaches Developers How to Train LLMs in Four Hours 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Computer scientist Andrej Karpathy recently introduced Nanochat, calling it “the best ChatGPT that $100 can buy.” Nanochat is an open-source, full-stack large language model (LLM) implementation built for transparency and experimentation. In about 8,000 lines of minimal, dependency-light code, Nanochat runs the entire LLM pipeline — from tokenization and pretraining to fine-tuning, inference and chat — all through a simple web user interface.&lt;br /&gt;NVIDIA is supporting Karpathy’s open-source Nanochat project by releasing two NVIDIA Launchables, making it easy to deploy and experiment with Nanochat across various NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;With NVIDIA Launchables, developers can train and interact with their own conversational model in hours with a single click. The Launchables dynamically support different-sized GPUs — including NVIDIA H100 and L40S GPUs — on various clouds without need for modification. They also automatically work on any eight-GPU instance on NVIDIA Brev, so developers can get compute access immediately.&lt;/p&gt;
&lt;p&gt;The &lt;b&gt;first 10 users&lt;/b&gt; to deploy these Launchables will also receive free compute access to NVIDIA H100 or L40S GPUs.&lt;/p&gt;
&lt;p&gt;Start training with Nanochat by deploying a Launchable:&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86022 size-medium" height="527" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/launchable-960x527.png" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Andrej Karpathy’s Next Experiments Begin With NVIDIA DGX Spark&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today, Karpathy received an NVIDIA DGX Spark — the world’s smallest AI supercomputer, designed to bring the power of Blackwell right to a developer’s desktop. With up to a petaflop of AI processing power and 128GB of unified memory in a compact form factor, DGX Spark empowers innovators like Karpathy to experiment, fine-tune and run massive models locally.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="pytorch"&gt;&lt;b&gt;Building the Future of AI With PyTorch and NVIDIA 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;PyTorch, the fastest-growing AI framework, derives its performance from the NVIDIA CUDA platform and uses the Python programming language to unlock developer productivity. This year, NVIDIA added Python as a first-class language to the CUDA platform, giving the PyTorch developer community greater access to CUDA.&lt;/p&gt;
&lt;p&gt;CUDA Python includes key components that make GPU acceleration in Python easier than ever, with built-in support for kernel fusion, extension module integration and simplified packaging for fast deployment.&lt;/p&gt;
&lt;p&gt;Following PyTorch’s open collaboration model, CUDA Python is available on GitHub and PyPI.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86034"&gt;&lt;img alt="alt" class="wp-image-86034 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/pytorch-infographic-1-1680x672.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86034"&gt;According to PyPI Stats, PyTorch averaged over two million daily downloads, peaking at 2,303,217 on October 14, and had 65 million total downloads last month.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Every month, developers worldwide download hundreds of millions of NVIDIA libraries — including CUDA, cuDNN, cuBLAS and CUTLASS — mostly within Python and PyTorch environments. CUDA Python provides nvmath-python, a new library that acts as the bridge between Python code and these highly optimized GPU libraries.&lt;/p&gt;
&lt;p&gt;Plus, kernel enhancements and support for next-generation frameworks make NVIDIA accelerated computing more efficient, adaptable and widely accessible.&lt;/p&gt;
&lt;p&gt;NVIDIA maintains a long-standing collaboration with the PyTorch community through open-source contributions and technical leadership, as well as by sponsoring and participating in community events and activations.&lt;/p&gt;
&lt;p&gt;At PyTorch Conference 2025 in San Francisco, NVIDIA will host a keynote address, five technical sessions and nine poster presentations.&lt;/p&gt;
&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="open-source-ai"&gt;&lt;b&gt;NVIDIA Spotlights Open Source Innovation 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open Source AI Week kicks off on Monday with a series of hackathons, workshops and meetups spotlighting the latest advances in AI, machine learning and open-source innovation.&lt;/p&gt;
&lt;p&gt;The event brings together leading organizations, researchers and open-source communities to share knowledge, collaborate on tools and explore how openness accelerates AI development.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to expand access to advanced AI innovation by providing open-source tools, models and datasets designed to empower developers. With more than 1,000 open-source tools on NVIDIA GitHub repositories and over 500 models and 100 datasets on the NVIDIA Hugging Face collections, NVIDIA is accelerating the pace of open, collaborative AI development.&lt;/p&gt;
&lt;p&gt;Over the past year, NVIDIA has become the top contributor in Hugging Face repositories, reflecting a deep commitment to sharing models, frameworks and research that empower the community.&lt;/p&gt;

&lt;p&gt;Openly available models, tools and datasets are essential to driving innovation and progress. By empowering anyone to use, modify and share technology, it fosters transparency and accelerates discovery, fueling breakthroughs that benefit both industry and communities alike. That’s why NVIDIA is committed to supporting the open source ecosystem.&lt;/p&gt;
&lt;p&gt;We’re on the ground all week — stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward, with the PyTorch Conference serving as the flagship event.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/open-source-ai-week/</guid><pubDate>Thu, 23 Oct 2025 15:00:33 +0000</pubDate></item><item><title>Sora update to bring AI videos of your pets, new social features, and soon, an Android version (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/sora-update-to-bring-ai-videos-of-your-pets-new-social-features-and-soon-an-android-version/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/sora-app-GettyImages-2240278671.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is teasing a series of updates coming to its viral app for AI-generated videos, Sora, which recently shot to the top of the App Store after a late September launch. The app, which remains at No. 1 in the U.S. and Canada, will introduce video editing tools, offer the ability for users to create character “cameos” of pets and other objects, improve social features, and more. Plus, the company says the Android version of the app is “actually coming soon.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement was made on X by Sora head Bill Peebles, who says the new creation tools will arrive in the next few days. These include the ability to turn pets and other items, like a “favorite stuffed toy and pretty much anything else,” into cameos.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;sora roadmap update: in the spirit of building this app openly, here's what we're landing soon.&lt;/p&gt;&lt;p&gt;first, more creation tools. character cameos are coming in the next few days: you'll be able to cameo your dog, guinea pig, favorite stuffed toy, and pretty much anything else you… pic.twitter.com/GX7CJXWRcZ&lt;/p&gt;— Bill Peebles (@billpeeb) October 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The term “cameo” refers to the Sora feature where users can make AI personas of themselves after providing the app with a recorded video as a reference file. These cameos can be shared with friends and others, allowing them to make videos with your AI character.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re expecting people to register lots of crazy new cameos with this feature. To make them easier to find, we’re updating the generation UI to show the latest trending cameos in real time,” Peebles wrote on X.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, he says the app will introduce basic video editing features, starting with stitching together multiple clips. More tools will be added over time. An updated social experience is also underway, which would add new ways of using Sora with friends. These may include dedicated channels specific to a university, company, sports club, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it’s working to reduce the excessive moderation of generations, which some users have complained is too strict, and improve the overall app performance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though the app is currently listed on the Google Play Store for pre-registration, OpenAI hasn’t yet shared a launch date for its arrival. Peebles says it should be live “soon,” but he didn’t offer more details.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Third-party app store data from Appfigures estimates Sora has seen around 2 million downloads since its launch less than a month ago. This figure is notable, given that the app remains invite-only and launched in just the U.S. and Canada. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/sora-app-GettyImages-2240278671.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is teasing a series of updates coming to its viral app for AI-generated videos, Sora, which recently shot to the top of the App Store after a late September launch. The app, which remains at No. 1 in the U.S. and Canada, will introduce video editing tools, offer the ability for users to create character “cameos” of pets and other objects, improve social features, and more. Plus, the company says the Android version of the app is “actually coming soon.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement was made on X by Sora head Bill Peebles, who says the new creation tools will arrive in the next few days. These include the ability to turn pets and other items, like a “favorite stuffed toy and pretty much anything else,” into cameos.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;sora roadmap update: in the spirit of building this app openly, here's what we're landing soon.&lt;/p&gt;&lt;p&gt;first, more creation tools. character cameos are coming in the next few days: you'll be able to cameo your dog, guinea pig, favorite stuffed toy, and pretty much anything else you… pic.twitter.com/GX7CJXWRcZ&lt;/p&gt;— Bill Peebles (@billpeeb) October 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The term “cameo” refers to the Sora feature where users can make AI personas of themselves after providing the app with a recorded video as a reference file. These cameos can be shared with friends and others, allowing them to make videos with your AI character.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re expecting people to register lots of crazy new cameos with this feature. To make them easier to find, we’re updating the generation UI to show the latest trending cameos in real time,” Peebles wrote on X.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, he says the app will introduce basic video editing features, starting with stitching together multiple clips. More tools will be added over time. An updated social experience is also underway, which would add new ways of using Sora with friends. These may include dedicated channels specific to a university, company, sports club, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it’s working to reduce the excessive moderation of generations, which some users have complained is too strict, and improve the overall app performance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though the app is currently listed on the Google Play Store for pre-registration, OpenAI hasn’t yet shared a launch date for its arrival. Peebles says it should be live “soon,” but he didn’t offer more details.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Third-party app store data from Appfigures estimates Sora has seen around 2 million downloads since its launch less than a month ago. This figure is notable, given that the app remains invite-only and launched in just the U.S. and Canada. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/sora-update-to-bring-ai-videos-of-your-pets-new-social-features-and-soon-an-android-version/</guid><pubDate>Thu, 23 Oct 2025 15:19:58 +0000</pubDate></item><item><title>Palantir enters $200M partnership with telco Lumen to offer enterprise AI services (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/palantir-enters-200m-partnership-with-telco-lumen-for-enterprise-ai-services/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/09/GettyImages-1228810115.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Palantir said on Thursday it had struck a partnership with Lumen Technologies that will see the telecommunications company using the data management company’s AI software to build capabilities to support enterprise AI services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companies are calling the deal “a multi-year, multi-million-dollar strategic partnership,” but Bloomberg reports, citing anonymous sources, that Lumen is going to spend over $200 million on Palantir’s tech over the course of several years.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The agreement will see Lumen pairing Palantir’s Foundry and Artificial Intelligence Platform (AIP) with its own edge computing, broadband infrastructure, and other digital services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership comes as Lumen tries to transform itself from a traditional telecom provider into a more modern tech infrastructure play. The company in September said it was “collaborating” with Palantir to integrate the latter’s Foundry and AI software and services into its operations, finance, and technology functions, “to transform its network and services with speed.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Lumen would not confirm the deal value, its spokesperson Joe Goode told TechCrunch in an email that the company’s use of Palantir’s tech was a “material contributor” to achieving $350 million in cost reductions in 2025. He added that what the companies learned during that collaboration informed their decision to take their partnership to the enterprise market. Lumen has committed to reducing expenses by $1 billion by 2027, and it says it’s already ahead of plan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Palantir demonstrated that its Foundry and AIP platforms could unlock Lumen’s data faster and cheaper than traditional data-lake migrations, and together we’re pursuing solutions to help large enterprises do the same,” Goode said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Palantir, this is the latest in a long string of deals for which it has partnered up with companies and government organizations of all stripes to sell its AI products and services. Including the Lumen deal, Palantir has struck 19 partnerships this year alone, across aviation, healthcare, telecom, contract management, data management, defense, and more sectors.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“By bringing AI into real-world operations through a connected ecosystem, we’re empowering businesses to reinvent how they operate, compete, and grow,” Kate Johnson, CEO of Lumen Technologies, said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Palantir did not immediately respond to a request for comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/09/GettyImages-1228810115.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Palantir said on Thursday it had struck a partnership with Lumen Technologies that will see the telecommunications company using the data management company’s AI software to build capabilities to support enterprise AI services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companies are calling the deal “a multi-year, multi-million-dollar strategic partnership,” but Bloomberg reports, citing anonymous sources, that Lumen is going to spend over $200 million on Palantir’s tech over the course of several years.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The agreement will see Lumen pairing Palantir’s Foundry and Artificial Intelligence Platform (AIP) with its own edge computing, broadband infrastructure, and other digital services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership comes as Lumen tries to transform itself from a traditional telecom provider into a more modern tech infrastructure play. The company in September said it was “collaborating” with Palantir to integrate the latter’s Foundry and AI software and services into its operations, finance, and technology functions, “to transform its network and services with speed.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Lumen would not confirm the deal value, its spokesperson Joe Goode told TechCrunch in an email that the company’s use of Palantir’s tech was a “material contributor” to achieving $350 million in cost reductions in 2025. He added that what the companies learned during that collaboration informed their decision to take their partnership to the enterprise market. Lumen has committed to reducing expenses by $1 billion by 2027, and it says it’s already ahead of plan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Palantir demonstrated that its Foundry and AIP platforms could unlock Lumen’s data faster and cheaper than traditional data-lake migrations, and together we’re pursuing solutions to help large enterprises do the same,” Goode said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Palantir, this is the latest in a long string of deals for which it has partnered up with companies and government organizations of all stripes to sell its AI products and services. Including the Lumen deal, Palantir has struck 19 partnerships this year alone, across aviation, healthcare, telecom, contract management, data management, defense, and more sectors.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“By bringing AI into real-world operations through a connected ecosystem, we’re empowering businesses to reinvent how they operate, compete, and grow,” Kate Johnson, CEO of Lumen Technologies, said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Palantir did not immediately respond to a request for comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/palantir-enters-200m-partnership-with-telco-lumen-for-enterprise-ai-services/</guid><pubDate>Thu, 23 Oct 2025 15:23:49 +0000</pubDate></item><item><title>Autonomy in the real world? Druid AI unveils AI agent ‘factory’ (AI News)</title><link>https://www.artificialintelligence-news.com/news/druid-ai-agentic-factory-automation-in-the-real-world/</link><description>&lt;p&gt; At its London Symbiosis 4 event on 22 October, Druid AI introduced what it terms Virtual Authoring Teams – a new generation of AI agents that can design, test, and deploy other AI agents. The announcement marks a move towards what the company calls a ‘factory model’ for AI automation.&lt;/p&gt;&lt;p&gt; According to Druid, the system enables organisations to build enterprise-grade AI agents up to ten times faster, and the platform offers orchestration facilities, plus compliance safeguards and measurable ROI tracking. The orchestration engine, Druid Conductor, serves as a control layer that integrates data, tooling, and human oversight into a single framework.&lt;/p&gt;&lt;p&gt; In addition to the Druid Conductor is the Druid Agentic Marketplace, a repository of pre-built, industry-specific agents for banking, healthcare, education, and insurance. With its solutions, Druid wants to make agentic AI accessible to non-technical users, but provide scalability capability suitable for enterprise use.&lt;/p&gt;&lt;p&gt; Chief Executive Joe Kim described it as “AI [that] actually works” – a bold claim in a market flooded with experimentation and unproven automation frameworks.&lt;/p&gt;&lt;h3&gt;The new agentic battleground&lt;/h3&gt;&lt;p&gt; Druid is not alone in its pursuit. Similar platforms, the likes of Cognigy, Kore.ai, and Amelia, each represent heavy investment in multi-agent orchestration environments. OpenAI’s GPTs and Anthropic’s Claude Projects also allow users to design semi-autonomous digital workers without coding expertise.&lt;/p&gt;&lt;p&gt; Google’s Vertex AI Agents and Microsoft’s Copilot Studio are moving in the same direction, placing agentic AI as an extension to enterprise ecosystems rather than stand-alone products.&lt;/p&gt;&lt;p&gt; The difference between the competing platforms lies in execution – some focus on workflow automation, others on conversational depth or ease of integration with other parts of the IT stack.&lt;/p&gt;&lt;p&gt; For technology buyers, such diversity is an opportunity and a risk. Vendors are racing to define what agentic AI means in practice, and there’s an undoubted element of agentic AI being 2025’s buzzword, implying differentiation between pure LLM models and practical tools useful in business contexts. Some vendors view agentic as an architecture – modular, distributed, and explainable, while others frame agentic AI as a layer of automation that builds itself – or rather, can discover what powers it’s been granted, and use them according to natural language instructions. The truth of agentic AI’s abilities sits somewhere between engineering promises and operational reality.&lt;/p&gt;&lt;h3&gt;The business case – and the caveats&lt;/h3&gt;&lt;p&gt; Agentic AI systems promise extraordinary benefits. They can accelerate routine development, coordinate multiple business functions, and use data repositories that were once siloed. For enterprises under pressure to deliver digital transformation with limited headcount, the idea of self-building AI teams is compelling.&lt;/p&gt;&lt;p&gt; But the use of the conditional tense in many vendors’ marketing materials and descriptions is telling: agentic AI &lt;i&gt;can&lt;/i&gt; achieve savings, &lt;i&gt;could&lt;/i&gt; drive faster operations, and so on.&lt;/p&gt;&lt;p&gt; Business leaders should approach such systems with a clear head. There are few proven case studies beyond pilot programmes inside large corporations (those with mature data governance and deep budgets), and even in those organisations, the returns have been uneven. Failures are rarely shouted from the rooftops, after all.&lt;/p&gt;&lt;p&gt; The biggest risks are not technical – they’re organisational. Delegating complex decision-making to automated agents without sufficient oversight introduces potential bias, compliance breaches, and reputational exposure. Systems can also generate automation debt: a growing tangle of interconnected bots that become difficult to monitor or update as business processes evolve.&lt;/p&gt;&lt;p&gt; The issue of necessary organisational change is troubling on two counts, furthermore. Most business processes have evolved a particular way for good reasons, so why change them to implement a new, largely unproven technology? Secondly, what’s often proposed is change that’s instigated by technology implementation. Shouldn’t processes change for strategic reasons, and technology support that change? Is this a case of the IT tail wagging the business dog?&lt;/p&gt;&lt;p&gt; Security remains a further concern. Each agent increases the surface area for potential breaches or data misuse, particularly when they are designed to communicate and collaborate autonomously. As more workflows become self-directed, ensuring traceability and accountability becomes essential, and more difficult to unpick as complexity increases. The necessary headcount to monitor results and ensure rigorous oversight could negate any ROI agentic AI offers.&lt;/p&gt;&lt;h3&gt;Why agentic AI attracts enterprises&lt;/h3&gt;&lt;p&gt; Despite the challenges, the attraction is easy to understand. A successful agentic system can transform the speed at which an enterprise experiments and scales. By delegating repeatable cognitive tasks – from compliance checks to customer service triage – organisations can redirect human activity elsewhere.&lt;/p&gt;&lt;p&gt; Druid’s Virtual Authoring Teams encapsulate the logic: automate the automation. Its marketplace of domain-specific agents offers enterprises a head start, promising faster deployments and measurable ROI. For sectors struggling with talent shortages and regulatory pressure, that is an appealing prospect.&lt;/p&gt;&lt;p&gt; Moreover, Druid’s emphasis on explainable AI and its orchestration layer suggests an awareness of corporate caution. Its stated pillars – control, accuracy, and results – are designed to reassure boards that transparency can coexist with speed. If the system truly delivers what the company claims, it could narrow the gap between AI experimentation and scalable transformation.&lt;/p&gt;&lt;h3&gt;Balancing autonomy with accountability&lt;/h3&gt;&lt;p&gt; Still, for every organisation embracing agentic AI, another remains unconvinced. Many enterprises are wary of over-promising vendors and pilot fatigue. A technology capable of designing and deploying its own successors raises operational questions. What happens when an agent acts beyond its creator’s intent? How do governance frameworks keep pace?&lt;/p&gt;&lt;p&gt; Business leaders must treat autonomy as a spectrum, not a goal. The near future of enterprise AI will likely blend human-supervised automation with limited agentic autonomy. Systems like Druid’s may act as orchestration hubs rather than fully independent actors.&lt;/p&gt;&lt;h3&gt;From hype to utility&lt;/h3&gt;&lt;p&gt; Agentic AI represents a natural evolution of automation in a wild frontier. Its potential is obvious, yet the market still lacks broad, evidence-based validation of sustained business outcomes. It may just be early days, or may be hyperbole drowning out the voices of reason.&lt;/p&gt;&lt;p&gt; For now, agentic systems &lt;i&gt;do work&lt;/i&gt; in controlled contexts – contact-centre operations, document processing, and IT service management. Scaling agentic AI across organisations will require maturity not just in technology, but in culture, process design, and methods of oversight.&lt;/p&gt;&lt;p&gt; As Druid and its peers expand their offerings, enterprises will need to weigh the cost of control against the promised wins from better automation. The next two years will determine whether AI factories become a part of business operations, or another layer of abstraction with its own overheads.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Black and grey wolf (female from Druid pack,’Half Black’) walking in road near Lamar River bridge” by YellowstoneNPS is marked with Public Domain Mark 1.0. )&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt; At its London Symbiosis 4 event on 22 October, Druid AI introduced what it terms Virtual Authoring Teams – a new generation of AI agents that can design, test, and deploy other AI agents. The announcement marks a move towards what the company calls a ‘factory model’ for AI automation.&lt;/p&gt;&lt;p&gt; According to Druid, the system enables organisations to build enterprise-grade AI agents up to ten times faster, and the platform offers orchestration facilities, plus compliance safeguards and measurable ROI tracking. The orchestration engine, Druid Conductor, serves as a control layer that integrates data, tooling, and human oversight into a single framework.&lt;/p&gt;&lt;p&gt; In addition to the Druid Conductor is the Druid Agentic Marketplace, a repository of pre-built, industry-specific agents for banking, healthcare, education, and insurance. With its solutions, Druid wants to make agentic AI accessible to non-technical users, but provide scalability capability suitable for enterprise use.&lt;/p&gt;&lt;p&gt; Chief Executive Joe Kim described it as “AI [that] actually works” – a bold claim in a market flooded with experimentation and unproven automation frameworks.&lt;/p&gt;&lt;h3&gt;The new agentic battleground&lt;/h3&gt;&lt;p&gt; Druid is not alone in its pursuit. Similar platforms, the likes of Cognigy, Kore.ai, and Amelia, each represent heavy investment in multi-agent orchestration environments. OpenAI’s GPTs and Anthropic’s Claude Projects also allow users to design semi-autonomous digital workers without coding expertise.&lt;/p&gt;&lt;p&gt; Google’s Vertex AI Agents and Microsoft’s Copilot Studio are moving in the same direction, placing agentic AI as an extension to enterprise ecosystems rather than stand-alone products.&lt;/p&gt;&lt;p&gt; The difference between the competing platforms lies in execution – some focus on workflow automation, others on conversational depth or ease of integration with other parts of the IT stack.&lt;/p&gt;&lt;p&gt; For technology buyers, such diversity is an opportunity and a risk. Vendors are racing to define what agentic AI means in practice, and there’s an undoubted element of agentic AI being 2025’s buzzword, implying differentiation between pure LLM models and practical tools useful in business contexts. Some vendors view agentic as an architecture – modular, distributed, and explainable, while others frame agentic AI as a layer of automation that builds itself – or rather, can discover what powers it’s been granted, and use them according to natural language instructions. The truth of agentic AI’s abilities sits somewhere between engineering promises and operational reality.&lt;/p&gt;&lt;h3&gt;The business case – and the caveats&lt;/h3&gt;&lt;p&gt; Agentic AI systems promise extraordinary benefits. They can accelerate routine development, coordinate multiple business functions, and use data repositories that were once siloed. For enterprises under pressure to deliver digital transformation with limited headcount, the idea of self-building AI teams is compelling.&lt;/p&gt;&lt;p&gt; But the use of the conditional tense in many vendors’ marketing materials and descriptions is telling: agentic AI &lt;i&gt;can&lt;/i&gt; achieve savings, &lt;i&gt;could&lt;/i&gt; drive faster operations, and so on.&lt;/p&gt;&lt;p&gt; Business leaders should approach such systems with a clear head. There are few proven case studies beyond pilot programmes inside large corporations (those with mature data governance and deep budgets), and even in those organisations, the returns have been uneven. Failures are rarely shouted from the rooftops, after all.&lt;/p&gt;&lt;p&gt; The biggest risks are not technical – they’re organisational. Delegating complex decision-making to automated agents without sufficient oversight introduces potential bias, compliance breaches, and reputational exposure. Systems can also generate automation debt: a growing tangle of interconnected bots that become difficult to monitor or update as business processes evolve.&lt;/p&gt;&lt;p&gt; The issue of necessary organisational change is troubling on two counts, furthermore. Most business processes have evolved a particular way for good reasons, so why change them to implement a new, largely unproven technology? Secondly, what’s often proposed is change that’s instigated by technology implementation. Shouldn’t processes change for strategic reasons, and technology support that change? Is this a case of the IT tail wagging the business dog?&lt;/p&gt;&lt;p&gt; Security remains a further concern. Each agent increases the surface area for potential breaches or data misuse, particularly when they are designed to communicate and collaborate autonomously. As more workflows become self-directed, ensuring traceability and accountability becomes essential, and more difficult to unpick as complexity increases. The necessary headcount to monitor results and ensure rigorous oversight could negate any ROI agentic AI offers.&lt;/p&gt;&lt;h3&gt;Why agentic AI attracts enterprises&lt;/h3&gt;&lt;p&gt; Despite the challenges, the attraction is easy to understand. A successful agentic system can transform the speed at which an enterprise experiments and scales. By delegating repeatable cognitive tasks – from compliance checks to customer service triage – organisations can redirect human activity elsewhere.&lt;/p&gt;&lt;p&gt; Druid’s Virtual Authoring Teams encapsulate the logic: automate the automation. Its marketplace of domain-specific agents offers enterprises a head start, promising faster deployments and measurable ROI. For sectors struggling with talent shortages and regulatory pressure, that is an appealing prospect.&lt;/p&gt;&lt;p&gt; Moreover, Druid’s emphasis on explainable AI and its orchestration layer suggests an awareness of corporate caution. Its stated pillars – control, accuracy, and results – are designed to reassure boards that transparency can coexist with speed. If the system truly delivers what the company claims, it could narrow the gap between AI experimentation and scalable transformation.&lt;/p&gt;&lt;h3&gt;Balancing autonomy with accountability&lt;/h3&gt;&lt;p&gt; Still, for every organisation embracing agentic AI, another remains unconvinced. Many enterprises are wary of over-promising vendors and pilot fatigue. A technology capable of designing and deploying its own successors raises operational questions. What happens when an agent acts beyond its creator’s intent? How do governance frameworks keep pace?&lt;/p&gt;&lt;p&gt; Business leaders must treat autonomy as a spectrum, not a goal. The near future of enterprise AI will likely blend human-supervised automation with limited agentic autonomy. Systems like Druid’s may act as orchestration hubs rather than fully independent actors.&lt;/p&gt;&lt;h3&gt;From hype to utility&lt;/h3&gt;&lt;p&gt; Agentic AI represents a natural evolution of automation in a wild frontier. Its potential is obvious, yet the market still lacks broad, evidence-based validation of sustained business outcomes. It may just be early days, or may be hyperbole drowning out the voices of reason.&lt;/p&gt;&lt;p&gt; For now, agentic systems &lt;i&gt;do work&lt;/i&gt; in controlled contexts – contact-centre operations, document processing, and IT service management. Scaling agentic AI across organisations will require maturity not just in technology, but in culture, process design, and methods of oversight.&lt;/p&gt;&lt;p&gt; As Druid and its peers expand their offerings, enterprises will need to weigh the cost of control against the promised wins from better automation. The next two years will determine whether AI factories become a part of business operations, or another layer of abstraction with its own overheads.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Black and grey wolf (female from Druid pack,’Half Black’) walking in road near Lamar River bridge” by YellowstoneNPS is marked with Public Domain Mark 1.0. )&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/druid-ai-agentic-factory-automation-in-the-real-world/</guid><pubDate>Thu, 23 Oct 2025 15:46:57 +0000</pubDate></item><item><title>Tensormesh raises $4.5M to squeeze more inference out of AI server loads (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/tensormesh-raises-4-5m-to-squeeze-more-inference-out-of-ai-server-loads/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Humanoids.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With the AI infrastructure push reaching staggering proportions, there’s more pressure than ever to squeeze as much inference as possible out of the GPUs they have. And for researchers with expertise in a particular technique, it’s a great time to raise funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s part of the driving force behind Tensormesh, launching out of stealth this week with $4.5 million in seed funding. The investment was led by Laude Ventures, with additional angel funding from database pioneer Michael Franklin.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tensormesh is using the money to build a commercial version of the open source LMCache utility, launched and maintained by Tensormesh co-founder Yihua Cheng. Used well, LMCache can reduce inference costs by as much as 10x — a power that’s made it a staple in open source deployments and drawn in integrations from heavy hitters like Google and Nvidia. Now Tensormesh is planning to parlay that academic reputation into a viable business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The core of the product is the key-value cache (or KV cache), a memory system used to process complex inputs more efficiently by condensing them down to their key values. In traditional architectures, the KV cache is discarded at the end of each query — but Tensormesh co-founder and CEO Junchen Jiang argues that this is an enormous source of inefficiency.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s like having a very smart analyst reading all the data, but they forget what they have learned after each question,” says Jiang.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead of discarding that cache, Tensormesh’s systems hold on to it, allowing it to be redeployed when the model executes a similar process in a separate query. Because GPU memory is so precious, this can mean spreading data across several different storage layers, but the reward is significantly more inference power for the same server load.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change is particularly powerful for chat interfaces, since models need to continually refer back to the growing chat log as the conversation progresses. Agentic systems have a similar issue, with a growing log of actions and goals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In theory, these are changes AI companies can execute on their own — but the technical complexity makes it a daunting task. Given the Tensormesh team’s work researching the process and the intricacy of the detail itself, the company is betting there will be lots of demand for an out-of-the-box product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Keeping the KV cache in a secondary storage system and reused efficiently without slowing the whole system down is a very challenging problem,” says Jiang. “We’ve seen people hire 20 engineers and spend three or four months to build such a system. Or they can use our product and do it very efficiently.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Humanoids.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With the AI infrastructure push reaching staggering proportions, there’s more pressure than ever to squeeze as much inference as possible out of the GPUs they have. And for researchers with expertise in a particular technique, it’s a great time to raise funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s part of the driving force behind Tensormesh, launching out of stealth this week with $4.5 million in seed funding. The investment was led by Laude Ventures, with additional angel funding from database pioneer Michael Franklin.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tensormesh is using the money to build a commercial version of the open source LMCache utility, launched and maintained by Tensormesh co-founder Yihua Cheng. Used well, LMCache can reduce inference costs by as much as 10x — a power that’s made it a staple in open source deployments and drawn in integrations from heavy hitters like Google and Nvidia. Now Tensormesh is planning to parlay that academic reputation into a viable business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The core of the product is the key-value cache (or KV cache), a memory system used to process complex inputs more efficiently by condensing them down to their key values. In traditional architectures, the KV cache is discarded at the end of each query — but Tensormesh co-founder and CEO Junchen Jiang argues that this is an enormous source of inefficiency.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s like having a very smart analyst reading all the data, but they forget what they have learned after each question,” says Jiang.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead of discarding that cache, Tensormesh’s systems hold on to it, allowing it to be redeployed when the model executes a similar process in a separate query. Because GPU memory is so precious, this can mean spreading data across several different storage layers, but the reward is significantly more inference power for the same server load.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change is particularly powerful for chat interfaces, since models need to continually refer back to the growing chat log as the conversation progresses. Agentic systems have a similar issue, with a growing log of actions and goals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In theory, these are changes AI companies can execute on their own — but the technical complexity makes it a daunting task. Given the Tensormesh team’s work researching the process and the intricacy of the detail itself, the company is betting there will be lots of demand for an out-of-the-box product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Keeping the KV cache in a secondary storage system and reused efficiently without slowing the whole system down is a very challenging problem,” says Jiang. “We’ve seen people hire 20 engineers and spend three or four months to build such a system. Or they can use our product and do it very efficiently.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/tensormesh-raises-4-5m-to-squeeze-more-inference-out-of-ai-server-loads/</guid><pubDate>Thu, 23 Oct 2025 16:00:00 +0000</pubDate></item><item><title>Microsoft’s Mico is a ‘Clippy’ for the AI era (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/microsofts-mico-is-a-clippy-for-the-ai-era/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft has a new Clippy, and it’s an AI friend called Mico. At the company’s Copilot fall release press event on Thursday, the company introduced a range of new features and updates for its AI chatbot, but one that telegraphed how the tech giant intends to bring AI to consumers was the official introduction of its AI chatbot’s “face” — an expressive avatar blob named Mico.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company explains that Mico (its name a nod to “Microsoft Copilot”) is meant to offer consumers a “warm” and “customizable” visual presence that “listens, reacts, and even changes colors to reflect your interactions.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If the talking AI helper immediately brings to mind Microsoft’s infamous productivity assistant, Clippy, you wouldn’t be wrong in thinking that. It seems that Microsoft has decided to embrace the reference to its age-old companion, as there’s even an Easter egg where, if you tap Mico a number of times, it will transform into Clippy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is enabled by default when you’re using Copilot’s voice mode, but users can turn it off if they choose. It’s initially available in the U.S., Canada, and the U.K. and will be able to save memories of your conversations and learn from your feedback, Microsoft says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A “Learn Live” mode for U.S. users can make Copilot a tutor that guides you through concepts instead of just providing an answer. The company notes it’s made other improvements in areas like health-related questions and deep research, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we build this,&amp;nbsp;we’re&amp;nbsp;not chasing engagement or&amp;nbsp;optimizing for&amp;nbsp;screen time.&amp;nbsp;We’re&amp;nbsp;building&amp;nbsp;AI&amp;nbsp;that gets you back to your life. That deepens human connection. That earns your trust,” wrote CEO of Microsoft AI, Mustafa Suleyman, in an announcement.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft is not the only chatbot maker to anthropomorphize its AI. Market leader ChatGPT, for instance, offers a visual experience as well, with a number of different voice options. Meanwhile, xAI’s Grok has turned its AI into risqué AI companions. Across the app stores, AI companion apps are already pulling in millions, indicating there is consumer demand for AI characters to some extent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, whether consumers will respond to Mico’s floating blob remains to be seen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it’s also working to evolve Copilot’s personality and tone, with the introduction of a new mode called “Real Talk.” This will allow the AI to mirror the user’s conversational style, but won’t be as sycophantic as other AI assistants have been. Instead, Microsoft says that it will feel like something that’s “grounded in its own perspective,” and will push back and challenge your ideas, which could encourage you to see things from a different point of view.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061438" height="363" src="https://techcrunch.com/wp-content/uploads/2025/10/mico-anime.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Mico.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Microsoft&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Finding a balance between a helpful, conversational AI and one that leads users down rabbit holes has proven tricky. Several incidents of AI chatbot psychosis have been reported, where AI users come to have their delusional beliefs reinforced by their conversation with the chatbot.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The fall Copilot update introduced a number of other new features to Microsoft’s AI, including the ability to bring friends into your Copilot AI chats, support for long-term memory, connectors to link productivity apps like email and cloud storage, and expanded AI integrations for its browser, Microsoft Edge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said it’s working to evolve Edge into an AI browser that would be able to see your tabs, summarize and compare information, and take action for you on things like booking a hotel or filling out forms. This would allow Edge to compete with other AI browsers, including OpenAI’s ChatGPT Atlas, Perplexity’s Comet, Dia, and others, as well as market leader Chrome, which has integrated its Gemini AI. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;All of today's @Copilot announcements boil down to one core idea: we're betting on humanist AI. An AI that always puts humans first.&lt;br /&gt;– Copilot Groups&lt;br /&gt;– AI browser&lt;br /&gt;– our new character Mico&lt;br /&gt;– memory updates&lt;br /&gt;– Copilot for health&lt;br /&gt;+ more in this morning's event https://t.co/GNtBAC8Nh6 pic.twitter.com/dZaQ9SmbFG&lt;/p&gt;— Mustafa Suleyman (@mustafasuleyman) October 23, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft has a new Clippy, and it’s an AI friend called Mico. At the company’s Copilot fall release press event on Thursday, the company introduced a range of new features and updates for its AI chatbot, but one that telegraphed how the tech giant intends to bring AI to consumers was the official introduction of its AI chatbot’s “face” — an expressive avatar blob named Mico.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company explains that Mico (its name a nod to “Microsoft Copilot”) is meant to offer consumers a “warm” and “customizable” visual presence that “listens, reacts, and even changes colors to reflect your interactions.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If the talking AI helper immediately brings to mind Microsoft’s infamous productivity assistant, Clippy, you wouldn’t be wrong in thinking that. It seems that Microsoft has decided to embrace the reference to its age-old companion, as there’s even an Easter egg where, if you tap Mico a number of times, it will transform into Clippy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is enabled by default when you’re using Copilot’s voice mode, but users can turn it off if they choose. It’s initially available in the U.S., Canada, and the U.K. and will be able to save memories of your conversations and learn from your feedback, Microsoft says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A “Learn Live” mode for U.S. users can make Copilot a tutor that guides you through concepts instead of just providing an answer. The company notes it’s made other improvements in areas like health-related questions and deep research, too. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we build this,&amp;nbsp;we’re&amp;nbsp;not chasing engagement or&amp;nbsp;optimizing for&amp;nbsp;screen time.&amp;nbsp;We’re&amp;nbsp;building&amp;nbsp;AI&amp;nbsp;that gets you back to your life. That deepens human connection. That earns your trust,” wrote CEO of Microsoft AI, Mustafa Suleyman, in an announcement.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft is not the only chatbot maker to anthropomorphize its AI. Market leader ChatGPT, for instance, offers a visual experience as well, with a number of different voice options. Meanwhile, xAI’s Grok has turned its AI into risqué AI companions. Across the app stores, AI companion apps are already pulling in millions, indicating there is consumer demand for AI characters to some extent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, whether consumers will respond to Mico’s floating blob remains to be seen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it’s also working to evolve Copilot’s personality and tone, with the introduction of a new mode called “Real Talk.” This will allow the AI to mirror the user’s conversational style, but won’t be as sycophantic as other AI assistants have been. Instead, Microsoft says that it will feel like something that’s “grounded in its own perspective,” and will push back and challenge your ideas, which could encourage you to see things from a different point of view.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061438" height="363" src="https://techcrunch.com/wp-content/uploads/2025/10/mico-anime.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Mico.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Microsoft&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Finding a balance between a helpful, conversational AI and one that leads users down rabbit holes has proven tricky. Several incidents of AI chatbot psychosis have been reported, where AI users come to have their delusional beliefs reinforced by their conversation with the chatbot.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The fall Copilot update introduced a number of other new features to Microsoft’s AI, including the ability to bring friends into your Copilot AI chats, support for long-term memory, connectors to link productivity apps like email and cloud storage, and expanded AI integrations for its browser, Microsoft Edge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said it’s working to evolve Edge into an AI browser that would be able to see your tabs, summarize and compare information, and take action for you on things like booking a hotel or filling out forms. This would allow Edge to compete with other AI browsers, including OpenAI’s ChatGPT Atlas, Perplexity’s Comet, Dia, and others, as well as market leader Chrome, which has integrated its Gemini AI. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;All of today's @Copilot announcements boil down to one core idea: we're betting on humanist AI. An AI that always puts humans first.&lt;br /&gt;– Copilot Groups&lt;br /&gt;– AI browser&lt;br /&gt;– our new character Mico&lt;br /&gt;– memory updates&lt;br /&gt;– Copilot for health&lt;br /&gt;+ more in this morning's event https://t.co/GNtBAC8Nh6 pic.twitter.com/dZaQ9SmbFG&lt;/p&gt;— Mustafa Suleyman (@mustafasuleyman) October 23, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/microsofts-mico-is-a-clippy-for-the-ai-era/</guid><pubDate>Thu, 23 Oct 2025 17:08:23 +0000</pubDate></item><item><title>Two days after OpenAI’s Atlas, Microsoft relaunches a nearly identical AI browser (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/two-days-after-openais-atlas-microsoft-launches-a-nearly-identical-ai-browser/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft released a new batch of features for its AI assistant Thursday, including an ambitious project that builds artificial intelligence directly into one of its most central products. More than a simple extension, the Copilot Mode in Microsoft’s Edge browser is Microsoft’s take on the long-hyped AI browser category — an intelligent and flexible AI assistant that follows you as you browse the web.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mustafa Suleyman, the CEO of Microsoft AI, even described the new product in those terms in the announcement. “Copilot Mode in Edge is evolving into an AI browser that is your dynamic, intelligent companion,” Suleyman wrote in the announcement post. “With your permission, Copilot can see and reason over your open tabs, summarize and compare information, and even take actions like booking a hotel or filling out forms.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The official launch for Edge’s Copilot Mode was in July, when it rolled out with basic features like a Search bar on new tabs and natural voice navigation. But the mode was opt-in and didn’t get quite the attention you’d expect. At Thursday’s event, Microsoft got more ambitious, introducing “Actions” that allow Copilot to fill out forms or book hotels and “Journeys” that let Copilot trace connections between your open tabs. It’s not a huge shift in the product, but it was enough to put the idea of the AI browser at center stage in the event.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes just two days after a similar launch from OpenAI, which showed off its new Atlas browser. Of course, Copilot’s release has been scheduled for weeks, and new Copilot Mode has probably been in development for months. Neither company invented the idea of an AI-assisted web browser. But the visual similarity between the two products is hard to ignore.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061444" height="440" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-23-at-1.01.41-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A demo frame from Microsoft’s CopIlot for Edge announcement.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Microsoft (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061446" height="451" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-23-at-1.03.36-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A Demo frame from OpenAI’s Atlas announcement.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Microsoft (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;These are two very similar pictures. The Copilot for Edge background is a little darker, there’s text instead of a logo, and the close/minimize buttons follow Windows conventions instead of MacOS conventions. Beyond that, Copilot puts its “ride-along” function in a new tab instead of a split-screen&amp;nbsp;… but that’s about it. It’s pretty much the same product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of the similarity is functional: People like clean browsers, and there are only so many ways to integrate a chatbot window into the “new tab” screen. For users, the main difference will come from the underlying models, so maybe a little facial similarity doesn’t make too big a difference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Browsers mostly look the same anyway. But given the high stakes of the AI race and the tense state of play between the two companies, it seems significant that we got both of these browsers in the same week.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: A previous version of this post erroneously referred to the October 23 event as the initial launch of Copilot for Edge. In fact, the feature was launched in July. TechCrunch regrets the error.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft released a new batch of features for its AI assistant Thursday, including an ambitious project that builds artificial intelligence directly into one of its most central products. More than a simple extension, the Copilot Mode in Microsoft’s Edge browser is Microsoft’s take on the long-hyped AI browser category — an intelligent and flexible AI assistant that follows you as you browse the web.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mustafa Suleyman, the CEO of Microsoft AI, even described the new product in those terms in the announcement. “Copilot Mode in Edge is evolving into an AI browser that is your dynamic, intelligent companion,” Suleyman wrote in the announcement post. “With your permission, Copilot can see and reason over your open tabs, summarize and compare information, and even take actions like booking a hotel or filling out forms.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The official launch for Edge’s Copilot Mode was in July, when it rolled out with basic features like a Search bar on new tabs and natural voice navigation. But the mode was opt-in and didn’t get quite the attention you’d expect. At Thursday’s event, Microsoft got more ambitious, introducing “Actions” that allow Copilot to fill out forms or book hotels and “Journeys” that let Copilot trace connections between your open tabs. It’s not a huge shift in the product, but it was enough to put the idea of the AI browser at center stage in the event.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes just two days after a similar launch from OpenAI, which showed off its new Atlas browser. Of course, Copilot’s release has been scheduled for weeks, and new Copilot Mode has probably been in development for months. Neither company invented the idea of an AI-assisted web browser. But the visual similarity between the two products is hard to ignore.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061444" height="440" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-23-at-1.01.41-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A demo frame from Microsoft’s CopIlot for Edge announcement.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Microsoft (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061446" height="451" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-23-at-1.03.36-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A Demo frame from OpenAI’s Atlas announcement.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Microsoft (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;These are two very similar pictures. The Copilot for Edge background is a little darker, there’s text instead of a logo, and the close/minimize buttons follow Windows conventions instead of MacOS conventions. Beyond that, Copilot puts its “ride-along” function in a new tab instead of a split-screen&amp;nbsp;… but that’s about it. It’s pretty much the same product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of the similarity is functional: People like clean browsers, and there are only so many ways to integrate a chatbot window into the “new tab” screen. For users, the main difference will come from the underlying models, so maybe a little facial similarity doesn’t make too big a difference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Browsers mostly look the same anyway. But given the high stakes of the AI race and the tense state of play between the two companies, it seems significant that we got both of these browsers in the same week.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: A previous version of this post erroneously referred to the October 23 event as the initial launch of Copilot for Edge. In fact, the feature was launched in July. TechCrunch regrets the error.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/two-days-after-openais-atlas-microsoft-launches-a-nearly-identical-ai-browser/</guid><pubDate>Thu, 23 Oct 2025 18:03:01 +0000</pubDate></item><item><title>[NEW] Microsoft Copilot gets 12 big updates for fall, including new AI assistant character Mico (AI | VentureBeat)</title><link>https://venturebeat.com/ai/microsoft-copilot-gets-12-big-updates-for-fall-including-new-ai-assistant</link><description>[unable to retrieve full-text content]&lt;p&gt;Microsoft today held a &lt;a href="https://youtu.be/j4jXM8yTdnQ?si=6nQuwRzBk6luy2fd"&gt;live announcement event online &lt;/a&gt;for its Copilot AI digital assistant, with &lt;b&gt;Mustafa Suleyman&lt;/b&gt;, CEO of Microsoft&amp;#x27;s AI division, and other presenters unveiling a new generation of features that deepen integration across Windows, Edge, and Microsoft 365, positioning the platform as a practical assistant for people during work and off-time, while allowing them  to preserve control and safety of their data.&lt;/p&gt;&lt;p&gt;The new &lt;b&gt;Copilot 2025 Fall Update features&lt;/b&gt; also up the ante in terms of capabilities and the accessibility of generative AI assistance from Microsoft to users, so businesses relying on Microsoft products, and those who seek to offer complimentary or competing products, would do well to review them.&lt;/p&gt;&lt;p&gt;Suleyman emphasized that the updates reflect a shift from hype to usefulness. “Technology should work in service of people, not the other way around,” he said. “Copilot is not just a product—it’s a promise that AI can be helpful, supportive, and deeply personal.”&lt;/p&gt;&lt;p&gt;Intriguingly, the announcement also sought to shine a greater spotlight on Microsoft&amp;#x27;s own homegrown AI models, as opposed to those of its partner and investment OpenAI, which previously powered the entire Copilot experience. Instead, Suleyman wrote today in a &lt;a href="https://www.microsoft.com/en-us/microsoft-copilot/blog/2025/10/23/human-centered-ai/"&gt;blog post:&lt;/a&gt; &lt;/p&gt;&lt;p&gt;&lt;i&gt;“At the foundation of it all is our strategy to put the best models to work for you – both those we build and those we don’t. Over the past few months, we have released in-house models like MAI-Voice-1, MAI-1-Preview and MAI-Vision-1, and are rapidly iterating.”&lt;/i&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;12 Features That Redefine Copilot&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Fall Release consolidates Copilot’s identity around twelve key capabilities—each with potential to streamline organizational knowledge work, development, or support operations.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Groups&lt;/b&gt; – Shared Copilot sessions where up to 32 participants can brainstorm, co-author, or plan simultaneously. For distributed teams, it effectively merges a meeting chat, task board, and generative workspace. Copilot maintains context, summarizes decisions, and tracks open actions. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Imagine&lt;/b&gt; – A collaborative hub for creating and remixing AI-generated content. In an enterprise setting, Imagine enables rapid prototyping of visuals, marketing drafts, or training materials.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mico&lt;/b&gt; – A new character identity for Copilot that introduces expressive feedback and emotional expression in the form of a cute, amorphous blob. Echoing Microsoft’s historic character interfaces like &lt;b&gt;Clippy&lt;/b&gt; (Office 97) or &lt;b&gt;Cortana&lt;/b&gt; (2014), Mico serves as a unifying UX layer across modalities.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Real Talk&lt;/b&gt; – A conversational mode that adapts to a user’s communication style and offers calibrated pushback — ending the sycophancy that some users have complained about with other AI models such as &lt;a href="https://venturebeat.com/ai/openai-rolls-back-chatgpts-sycophancy-and-explains-what-went-wrong"&gt;prior versions of OpenAI&amp;#x27;s ChatGPT&lt;/a&gt;. For professionals, it allows Socratic problem-solving rather than passive answer generation, making Copilot more credible in technical collaboration.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Memory &amp;amp; Personalization&lt;/b&gt; – Long-term contextual memory that lets Copilot recall key details—training plans, dates, goals—at the user’s direction.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Connectors&lt;/b&gt; – Integration with OneDrive, Outlook, Gmail, Google Drive, and Google Calendar for natural-language search across accounts.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Proactive Actions (Preview)&lt;/b&gt; – Context-based prompts and next-step suggestions derived from recent activity.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Copilot for Health&lt;/b&gt; – Health information grounded in credible medical sources such as Harvard Health, with tools allowing users to locate and compare doctors.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Learn Live&lt;/b&gt; – A Socratic, voice-driven tutoring experience using questions, visuals, and whiteboards.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Copilot Mode in Edge&lt;/b&gt; – Converts Microsoft Edge into an “AI browser” that summarizes, compares, and executes web actions by voice.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Copilot on Windows&lt;/b&gt; – Deep integration across Windows 11 PCs with “Hey Copilot” activation, Copilot Vision guidance, and quick access to files and apps.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Copilot Pages and Copilot Search&lt;/b&gt; – A collaborative file canvas plus a unified search experience combining AI-generated, cited answers with standard web results.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The Fall Release is immediately available in the United States, with rollout to the UK, Canada, and other markets in progress. &lt;/p&gt;&lt;p&gt;Some functions—such as Groups, Journeys, and Copilot for Health—remain U.S.-only for now. Proactive Actions requires a Microsoft 365 Personal, Family, or Premium subscription.&lt;/p&gt;&lt;p&gt;Together these updates illustrate Microsoft’s pivot from static productivity suites to contextual AI infrastructure, with the Copilot brand acting as the connective tissue across user roles.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Clippy to Mico: The Return of a Guided Interface&lt;/b&gt;&lt;/h3&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;One of the most notable introductions is Mico, a small animated companion that is available within Copilot’s voice-enabled experiences, including the Copilot app on Windows, iOS, and Android, as well as in Study Mode and other conversational contexts. It serves as an optional visual companion that appears during interactive or voice-based sessions, rather than across all Copilot interfaces.&lt;/p&gt;&lt;p&gt;Mico listens, reacts with expressions, and changes color to reflect tone and emotion — bringing a visual warmth to an AI assistant experience that has traditionally been text-heavy.&lt;/p&gt;&lt;p&gt;Mico’s design recalls earlier eras of Microsoft’s history with character-based assistants. In the mid-1990s, Microsoft experimented with Microsoft Bob (1995), a software interface that used cartoon characters like a dog named Rover to guide users through everyday computing tasks. While innovative for its time, Bob was discontinued after a year due to performance and usability issues.&lt;/p&gt;&lt;p&gt;A few years later came Clippy, the Office Assistant introduced in Microsoft Office 97. Officially known as “Clippit,” the animated paperclip would pop up to offer help and tips within Word and other Office applications. Clippy became widely recognized—sometimes humorously so—for interrupting users with unsolicited advice. Microsoft retired Clippy from Office in 2001, though the character remains a nostalgic symbol of early AI-driven assistance.&lt;/p&gt;&lt;p&gt;More recently, Cortana, launched in 2014 as Microsoft’s digital voice assistant for Windows and mobile devices, aimed to provide natural-language interaction similar to Apple’s Siri or Amazon’s Alexa. Despite positive early reception, Cortana’s role diminished as Microsoft refocused on enterprise productivity and AI integration. The service was officially discontinued on Windows in 2023.&lt;/p&gt;&lt;p&gt;Mico, by contrast, represents a modern reimagining of that tradition—combining the personality of early assistants with the intelligence and adaptability of contemporary AI models. Where Clippy offered canned responses, Mico listens, learns, and reflects a user’s mood in real time. The goal, as Suleyman framed it, is to create an AI that feels “helpful, supportive, and deeply personal.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Groups Are Microsoft&amp;#x27;s Version of Claude and ChatGPT Projects&lt;/b&gt;&lt;/h3&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;During Microsoft’s launch video, product researcher Wendy described Groups as a transformative shift: “You can finally bring in other people directly to the conversation that you’re having with Copilot,” she said. “It’s the only place you can do this.”&lt;/p&gt;&lt;p&gt;Up to 32 users can join a shared Copilot session, brainstorming, editing, or planning together while the AI manages logistics such as summarizing discussion threads, tallying votes, and splitting tasks. Participants can enter or exit sessions using a link, maintaining full visibility into ongoing work.&lt;/p&gt;&lt;p&gt;Instead of a single user prompting an AI and later sharing results, Groups lets teams prompt and iterate together in one unified conversation. &lt;/p&gt;&lt;p&gt;In some ways, it&amp;#x27;s an answer to Anthropic’s Claude Projects and OpenAI’s ChatGPT Projects, both launched within the last year as tools to centralize team workspaces and shared AI context. &lt;/p&gt;&lt;p&gt;Where Claude and ChatGPT Projects allow users to aggregate files, prompts, and conversations into a single container, Groups extends that model into real-time, multi-participant collaboration. &lt;/p&gt;&lt;p&gt;Unlike Anthropic’s and OpenAI’s implementations, Groups is deeply embedded within Microsoft’s productivity environment.  &lt;/p&gt;&lt;p&gt;Like other Copilot experiences connected to Outlook and OneDrive, Groups operates within Microsoft’s enterprise identity framework, governed by Microsoft 365 and Entra ID (formerly Azure Active Directory) authentication and consent models&lt;/p&gt;&lt;p&gt;This means conversations, shared artifacts, and generated summaries are governed under the same compliance policies that already protect Outlook, Teams, and SharePoint data.&lt;/p&gt;&lt;p&gt;Hours after the unveiling, OpenAI hit back against its own investor in the escalating AI competition between the &amp;quot;frenemies&amp;quot; by expanding its Shared Projects feature beyond its current Enterprise, Team, and Edu subscriber availability to &lt;a href="https://x.com/OpenAI/status/1981432799212249119"&gt;users of its free, Plus, and Pro subscription tiers. &lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Operational Impact for AI and Data Teams&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Memory &amp;amp; Personalization and Connectors effectively extend a lightweight orchestration layer across Microsoft’s ecosystem. &lt;/p&gt;&lt;p&gt;Instead of building separate context-stores or retrieval APIs, teams can leverage Copilot’s secure integration with OneDrive or SharePoint as a governed data backbone. &lt;/p&gt;&lt;p&gt;A presenter explained that Copilot’s memory “naturally picks up on important details and remembers them long after you’ve had the conversation,” yet remains editable. &lt;/p&gt;&lt;p&gt;For data engineers, Copilot Search and Connectors reduce friction in data discovery across multiple systems. Natural-language retrieval from internal and cloud repositories may lower the cost of knowledge management initiatives by consolidating search endpoints.&lt;/p&gt;&lt;p&gt;For security directors, Copilot’s explicit consent requirements and on/off toggles in Edge and Windows help maintain data residency standards. The company reiterated during the livestream that Copilot “acts only with user permission and within organizational privacy controls.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Copilot Mode in Edge: The AI Browser for Research and Automation&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Copilot Mode in Edge stands out for offering AI-assisted information workflows. &lt;/p&gt;&lt;p&gt;The browser can now parse open tabs, summarize differences, and perform transactional steps.&lt;/p&gt;&lt;p&gt;“Historically, browsers have been static—just endless clicking and tab-hopping,” said a presenter during Microsoft’s livestream. “We asked not how browsers should work, but how people work.”&lt;/p&gt;&lt;p&gt;In practice, an analyst could prompt Edge to compare supplier documentation, extract structured data, and auto-fill procurement forms—all with consistent citation. &lt;/p&gt;&lt;p&gt;Voice-only navigation enables accessibility and multitasking, while Journeys, a companion feature, organizes browsing sessions into storylines for later review.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Copilot on Windows: The Operating System as an AI Surface&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In Windows 11, Copilot now functions as an embedded assistant. With the wake-word “Hey Copilot,” users can initiate context-aware commands without leaving the desktop—drafting documentation, troubleshooting configuration issues, or summarizing system logs.&lt;/p&gt;&lt;p&gt;A presenter described it as a “super assistant plugged into all your files and applications.” For enterprises standardizing on Windows 11, this positions Copilot as a native productivity layer rather than an add-on, reducing training friction and promoting secure, on-device reasoning.&lt;/p&gt;&lt;p&gt;Copilot Vision, now in early deployment, adds visual comprehension. IT staff can capture a screen region and ask Copilot to interpret error messages, explain configuration options, or generate support tickets automatically.&lt;/p&gt;&lt;p&gt;Combined with Copilot Pages, which supports up to twenty concurrent file uploads, this enables more efficient cross-document analysis for audits, RFPs, or code reviews.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Leveraging MAI Models for Multimodal Workflows&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At the foundation of these capabilities are Microsoft’s proprietary &lt;b&gt;MAI-Voice-1&lt;/b&gt;, &lt;b&gt;MAI-1 Preview&lt;/b&gt;, and &lt;b&gt;MAI-Vision-1&lt;/b&gt; models—trained in-house to handle text, voice, and visual inputs cohesively.&lt;/p&gt;&lt;p&gt;For engineering teams managing LLM orchestration, this architecture introduces several potential efficiencies:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Unified multimodal reasoning&lt;/b&gt; – Reduces the need for separate ASR (speech-to-text) and image-parsing services.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Fine-tuning continuity&lt;/b&gt; – Because Microsoft owns the model stack, updates propagate across Copilot experiences without re-integration.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Predictable latency and governance&lt;/b&gt; – In-house hosting under Azure compliance frameworks simplifies security certification for regulated industries.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A presenter described the new stack as “the foundation for immersive, creative, and dynamic experiences that still respect enterprise boundaries.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Strategic Pivot Toward Contextual AI&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For years, Microsoft positioned Copilot primarily as a productivity companion. With the Fall 2025 release, it crosses into operational AI infrastructure—a set of extensible services for reasoning over data and processes.&lt;/p&gt;&lt;p&gt;Suleyman described this evolution succinctly: “Judge an AI by how much it elevates human potential, not just by its own smarts.” For CIOs and technical leads, the elevation comes from efficiency and interoperability.&lt;/p&gt;&lt;p&gt;Copilot now acts as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A connective interface linking files, communications, and cloud data.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A reasoning agent capable of understanding context across sessions and modalities.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A secure orchestration layer compatible with Microsoft’s compliance and identity framework.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Suleyman’s insistence that “technology should work in service of people” now extends to organizations as well: technology that serves teams, not workloads; systems that adapt to enterprise context rather than demand it.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Microsoft today held a &lt;a href="https://youtu.be/j4jXM8yTdnQ?si=6nQuwRzBk6luy2fd"&gt;live announcement event online &lt;/a&gt;for its Copilot AI digital assistant, with &lt;b&gt;Mustafa Suleyman&lt;/b&gt;, CEO of Microsoft&amp;#x27;s AI division, and other presenters unveiling a new generation of features that deepen integration across Windows, Edge, and Microsoft 365, positioning the platform as a practical assistant for people during work and off-time, while allowing them  to preserve control and safety of their data.&lt;/p&gt;&lt;p&gt;The new &lt;b&gt;Copilot 2025 Fall Update features&lt;/b&gt; also up the ante in terms of capabilities and the accessibility of generative AI assistance from Microsoft to users, so businesses relying on Microsoft products, and those who seek to offer complimentary or competing products, would do well to review them.&lt;/p&gt;&lt;p&gt;Suleyman emphasized that the updates reflect a shift from hype to usefulness. “Technology should work in service of people, not the other way around,” he said. “Copilot is not just a product—it’s a promise that AI can be helpful, supportive, and deeply personal.”&lt;/p&gt;&lt;p&gt;Intriguingly, the announcement also sought to shine a greater spotlight on Microsoft&amp;#x27;s own homegrown AI models, as opposed to those of its partner and investment OpenAI, which previously powered the entire Copilot experience. Instead, Suleyman wrote today in a &lt;a href="https://www.microsoft.com/en-us/microsoft-copilot/blog/2025/10/23/human-centered-ai/"&gt;blog post:&lt;/a&gt; &lt;/p&gt;&lt;p&gt;&lt;i&gt;“At the foundation of it all is our strategy to put the best models to work for you – both those we build and those we don’t. Over the past few months, we have released in-house models like MAI-Voice-1, MAI-1-Preview and MAI-Vision-1, and are rapidly iterating.”&lt;/i&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;12 Features That Redefine Copilot&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Fall Release consolidates Copilot’s identity around twelve key capabilities—each with potential to streamline organizational knowledge work, development, or support operations.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Groups&lt;/b&gt; – Shared Copilot sessions where up to 32 participants can brainstorm, co-author, or plan simultaneously. For distributed teams, it effectively merges a meeting chat, task board, and generative workspace. Copilot maintains context, summarizes decisions, and tracks open actions. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Imagine&lt;/b&gt; – A collaborative hub for creating and remixing AI-generated content. In an enterprise setting, Imagine enables rapid prototyping of visuals, marketing drafts, or training materials.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mico&lt;/b&gt; – A new character identity for Copilot that introduces expressive feedback and emotional expression in the form of a cute, amorphous blob. Echoing Microsoft’s historic character interfaces like &lt;b&gt;Clippy&lt;/b&gt; (Office 97) or &lt;b&gt;Cortana&lt;/b&gt; (2014), Mico serves as a unifying UX layer across modalities.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Real Talk&lt;/b&gt; – A conversational mode that adapts to a user’s communication style and offers calibrated pushback — ending the sycophancy that some users have complained about with other AI models such as &lt;a href="https://venturebeat.com/ai/openai-rolls-back-chatgpts-sycophancy-and-explains-what-went-wrong"&gt;prior versions of OpenAI&amp;#x27;s ChatGPT&lt;/a&gt;. For professionals, it allows Socratic problem-solving rather than passive answer generation, making Copilot more credible in technical collaboration.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Memory &amp;amp; Personalization&lt;/b&gt; – Long-term contextual memory that lets Copilot recall key details—training plans, dates, goals—at the user’s direction.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Connectors&lt;/b&gt; – Integration with OneDrive, Outlook, Gmail, Google Drive, and Google Calendar for natural-language search across accounts.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Proactive Actions (Preview)&lt;/b&gt; – Context-based prompts and next-step suggestions derived from recent activity.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Copilot for Health&lt;/b&gt; – Health information grounded in credible medical sources such as Harvard Health, with tools allowing users to locate and compare doctors.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Learn Live&lt;/b&gt; – A Socratic, voice-driven tutoring experience using questions, visuals, and whiteboards.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Copilot Mode in Edge&lt;/b&gt; – Converts Microsoft Edge into an “AI browser” that summarizes, compares, and executes web actions by voice.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Copilot on Windows&lt;/b&gt; – Deep integration across Windows 11 PCs with “Hey Copilot” activation, Copilot Vision guidance, and quick access to files and apps.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Copilot Pages and Copilot Search&lt;/b&gt; – A collaborative file canvas plus a unified search experience combining AI-generated, cited answers with standard web results.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The Fall Release is immediately available in the United States, with rollout to the UK, Canada, and other markets in progress. &lt;/p&gt;&lt;p&gt;Some functions—such as Groups, Journeys, and Copilot for Health—remain U.S.-only for now. Proactive Actions requires a Microsoft 365 Personal, Family, or Premium subscription.&lt;/p&gt;&lt;p&gt;Together these updates illustrate Microsoft’s pivot from static productivity suites to contextual AI infrastructure, with the Copilot brand acting as the connective tissue across user roles.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Clippy to Mico: The Return of a Guided Interface&lt;/b&gt;&lt;/h3&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;One of the most notable introductions is Mico, a small animated companion that is available within Copilot’s voice-enabled experiences, including the Copilot app on Windows, iOS, and Android, as well as in Study Mode and other conversational contexts. It serves as an optional visual companion that appears during interactive or voice-based sessions, rather than across all Copilot interfaces.&lt;/p&gt;&lt;p&gt;Mico listens, reacts with expressions, and changes color to reflect tone and emotion — bringing a visual warmth to an AI assistant experience that has traditionally been text-heavy.&lt;/p&gt;&lt;p&gt;Mico’s design recalls earlier eras of Microsoft’s history with character-based assistants. In the mid-1990s, Microsoft experimented with Microsoft Bob (1995), a software interface that used cartoon characters like a dog named Rover to guide users through everyday computing tasks. While innovative for its time, Bob was discontinued after a year due to performance and usability issues.&lt;/p&gt;&lt;p&gt;A few years later came Clippy, the Office Assistant introduced in Microsoft Office 97. Officially known as “Clippit,” the animated paperclip would pop up to offer help and tips within Word and other Office applications. Clippy became widely recognized—sometimes humorously so—for interrupting users with unsolicited advice. Microsoft retired Clippy from Office in 2001, though the character remains a nostalgic symbol of early AI-driven assistance.&lt;/p&gt;&lt;p&gt;More recently, Cortana, launched in 2014 as Microsoft’s digital voice assistant for Windows and mobile devices, aimed to provide natural-language interaction similar to Apple’s Siri or Amazon’s Alexa. Despite positive early reception, Cortana’s role diminished as Microsoft refocused on enterprise productivity and AI integration. The service was officially discontinued on Windows in 2023.&lt;/p&gt;&lt;p&gt;Mico, by contrast, represents a modern reimagining of that tradition—combining the personality of early assistants with the intelligence and adaptability of contemporary AI models. Where Clippy offered canned responses, Mico listens, learns, and reflects a user’s mood in real time. The goal, as Suleyman framed it, is to create an AI that feels “helpful, supportive, and deeply personal.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Groups Are Microsoft&amp;#x27;s Version of Claude and ChatGPT Projects&lt;/b&gt;&lt;/h3&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;During Microsoft’s launch video, product researcher Wendy described Groups as a transformative shift: “You can finally bring in other people directly to the conversation that you’re having with Copilot,” she said. “It’s the only place you can do this.”&lt;/p&gt;&lt;p&gt;Up to 32 users can join a shared Copilot session, brainstorming, editing, or planning together while the AI manages logistics such as summarizing discussion threads, tallying votes, and splitting tasks. Participants can enter or exit sessions using a link, maintaining full visibility into ongoing work.&lt;/p&gt;&lt;p&gt;Instead of a single user prompting an AI and later sharing results, Groups lets teams prompt and iterate together in one unified conversation. &lt;/p&gt;&lt;p&gt;In some ways, it&amp;#x27;s an answer to Anthropic’s Claude Projects and OpenAI’s ChatGPT Projects, both launched within the last year as tools to centralize team workspaces and shared AI context. &lt;/p&gt;&lt;p&gt;Where Claude and ChatGPT Projects allow users to aggregate files, prompts, and conversations into a single container, Groups extends that model into real-time, multi-participant collaboration. &lt;/p&gt;&lt;p&gt;Unlike Anthropic’s and OpenAI’s implementations, Groups is deeply embedded within Microsoft’s productivity environment.  &lt;/p&gt;&lt;p&gt;Like other Copilot experiences connected to Outlook and OneDrive, Groups operates within Microsoft’s enterprise identity framework, governed by Microsoft 365 and Entra ID (formerly Azure Active Directory) authentication and consent models&lt;/p&gt;&lt;p&gt;This means conversations, shared artifacts, and generated summaries are governed under the same compliance policies that already protect Outlook, Teams, and SharePoint data.&lt;/p&gt;&lt;p&gt;Hours after the unveiling, OpenAI hit back against its own investor in the escalating AI competition between the &amp;quot;frenemies&amp;quot; by expanding its Shared Projects feature beyond its current Enterprise, Team, and Edu subscriber availability to &lt;a href="https://x.com/OpenAI/status/1981432799212249119"&gt;users of its free, Plus, and Pro subscription tiers. &lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Operational Impact for AI and Data Teams&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Memory &amp;amp; Personalization and Connectors effectively extend a lightweight orchestration layer across Microsoft’s ecosystem. &lt;/p&gt;&lt;p&gt;Instead of building separate context-stores or retrieval APIs, teams can leverage Copilot’s secure integration with OneDrive or SharePoint as a governed data backbone. &lt;/p&gt;&lt;p&gt;A presenter explained that Copilot’s memory “naturally picks up on important details and remembers them long after you’ve had the conversation,” yet remains editable. &lt;/p&gt;&lt;p&gt;For data engineers, Copilot Search and Connectors reduce friction in data discovery across multiple systems. Natural-language retrieval from internal and cloud repositories may lower the cost of knowledge management initiatives by consolidating search endpoints.&lt;/p&gt;&lt;p&gt;For security directors, Copilot’s explicit consent requirements and on/off toggles in Edge and Windows help maintain data residency standards. The company reiterated during the livestream that Copilot “acts only with user permission and within organizational privacy controls.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Copilot Mode in Edge: The AI Browser for Research and Automation&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Copilot Mode in Edge stands out for offering AI-assisted information workflows. &lt;/p&gt;&lt;p&gt;The browser can now parse open tabs, summarize differences, and perform transactional steps.&lt;/p&gt;&lt;p&gt;“Historically, browsers have been static—just endless clicking and tab-hopping,” said a presenter during Microsoft’s livestream. “We asked not how browsers should work, but how people work.”&lt;/p&gt;&lt;p&gt;In practice, an analyst could prompt Edge to compare supplier documentation, extract structured data, and auto-fill procurement forms—all with consistent citation. &lt;/p&gt;&lt;p&gt;Voice-only navigation enables accessibility and multitasking, while Journeys, a companion feature, organizes browsing sessions into storylines for later review.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Copilot on Windows: The Operating System as an AI Surface&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In Windows 11, Copilot now functions as an embedded assistant. With the wake-word “Hey Copilot,” users can initiate context-aware commands without leaving the desktop—drafting documentation, troubleshooting configuration issues, or summarizing system logs.&lt;/p&gt;&lt;p&gt;A presenter described it as a “super assistant plugged into all your files and applications.” For enterprises standardizing on Windows 11, this positions Copilot as a native productivity layer rather than an add-on, reducing training friction and promoting secure, on-device reasoning.&lt;/p&gt;&lt;p&gt;Copilot Vision, now in early deployment, adds visual comprehension. IT staff can capture a screen region and ask Copilot to interpret error messages, explain configuration options, or generate support tickets automatically.&lt;/p&gt;&lt;p&gt;Combined with Copilot Pages, which supports up to twenty concurrent file uploads, this enables more efficient cross-document analysis for audits, RFPs, or code reviews.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Leveraging MAI Models for Multimodal Workflows&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At the foundation of these capabilities are Microsoft’s proprietary &lt;b&gt;MAI-Voice-1&lt;/b&gt;, &lt;b&gt;MAI-1 Preview&lt;/b&gt;, and &lt;b&gt;MAI-Vision-1&lt;/b&gt; models—trained in-house to handle text, voice, and visual inputs cohesively.&lt;/p&gt;&lt;p&gt;For engineering teams managing LLM orchestration, this architecture introduces several potential efficiencies:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Unified multimodal reasoning&lt;/b&gt; – Reduces the need for separate ASR (speech-to-text) and image-parsing services.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Fine-tuning continuity&lt;/b&gt; – Because Microsoft owns the model stack, updates propagate across Copilot experiences without re-integration.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Predictable latency and governance&lt;/b&gt; – In-house hosting under Azure compliance frameworks simplifies security certification for regulated industries.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A presenter described the new stack as “the foundation for immersive, creative, and dynamic experiences that still respect enterprise boundaries.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Strategic Pivot Toward Contextual AI&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For years, Microsoft positioned Copilot primarily as a productivity companion. With the Fall 2025 release, it crosses into operational AI infrastructure—a set of extensible services for reasoning over data and processes.&lt;/p&gt;&lt;p&gt;Suleyman described this evolution succinctly: “Judge an AI by how much it elevates human potential, not just by its own smarts.” For CIOs and technical leads, the elevation comes from efficiency and interoperability.&lt;/p&gt;&lt;p&gt;Copilot now acts as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A connective interface linking files, communications, and cloud data.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A reasoning agent capable of understanding context across sessions and modalities.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A secure orchestration layer compatible with Microsoft’s compliance and identity framework.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Suleyman’s insistence that “technology should work in service of people” now extends to organizations as well: technology that serves teams, not workloads; systems that adapt to enterprise context rather than demand it.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/microsoft-copilot-gets-12-big-updates-for-fall-including-new-ai-assistant</guid><pubDate>Thu, 23 Oct 2025 19:01:00 +0000</pubDate></item><item><title>[NEW] Instagram users can now use Meta AI editing tools directly in IG Stories (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/instagram-users-can-now-use-meta-ai-editing-tools-directly-in-ig-stories/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is bringing its AI-powered photo and video editing tools directly to Instagram Stories, allowing users to enter text prompts to add or remove things in photos and videos or even change it completely.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Meta has already offered image editing capabilities on Instagram, those tools were limited to interactions with the Meta AI chatbot. The addition of text-based prompts in Stories now makes these editing features much more accessible.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new features can be found in the “Restyle” menu located at the top of Instagram Stories when tapping the paintbrush icon. To edit an image, choose “add,” “remove,” or “change” to describe what you want to change in the prompt bar. For instance, Meta explains you can ask it to change your hair color, add a crown to your head, or insert a sunset background.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061623" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/download-1.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;There are also preset effects you can choose from that can alter your outfit or change the image’s style. For example, you can add items like sunglasses or a biker jacket, and you can also apply a watercolor effect to the image. For videos, you can make it look like it’s snowing or add flames.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s important to note that when you’re using Meta AI on Instagram, you’re accepting Meta’s AI Terms of Service, which allow your media and facial features to be analyzed by AI. According to these terms, when you upload your photos, Meta can “summarize image contents, modify images, and generate new content based on the image.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061624" height="373" src="https://techcrunch.com/wp-content/uploads/2025/10/download.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Meta continues to introduce AI updates to remain competitive in the market. It was most recently spotted testing a “Write with Meta AI” prompt that helps Instagram users come up with clever comments for posts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, the company launched a new AI-generated video feed called “Vibes” in the Meta AI app, likely boosting downloads. New data from Similarweb shows that the app’s daily active users on iOS and Android increased to 2.7 million as of October 17, up from about 775,000 four weeks ago.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, to address any concerns parents may have, Meta announced earlier this month that it is introducing new parental control features that enable parents to disable chats with AI characters and monitor the topics their teenagers are discussing with the Meta AI chatbot.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is bringing its AI-powered photo and video editing tools directly to Instagram Stories, allowing users to enter text prompts to add or remove things in photos and videos or even change it completely.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Meta has already offered image editing capabilities on Instagram, those tools were limited to interactions with the Meta AI chatbot. The addition of text-based prompts in Stories now makes these editing features much more accessible.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new features can be found in the “Restyle” menu located at the top of Instagram Stories when tapping the paintbrush icon. To edit an image, choose “add,” “remove,” or “change” to describe what you want to change in the prompt bar. For instance, Meta explains you can ask it to change your hair color, add a crown to your head, or insert a sunset background.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061623" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/download-1.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;There are also preset effects you can choose from that can alter your outfit or change the image’s style. For example, you can add items like sunglasses or a biker jacket, and you can also apply a watercolor effect to the image. For videos, you can make it look like it’s snowing or add flames.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s important to note that when you’re using Meta AI on Instagram, you’re accepting Meta’s AI Terms of Service, which allow your media and facial features to be analyzed by AI. According to these terms, when you upload your photos, Meta can “summarize image contents, modify images, and generate new content based on the image.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3061624" height="373" src="https://techcrunch.com/wp-content/uploads/2025/10/download.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Meta continues to introduce AI updates to remain competitive in the market. It was most recently spotted testing a “Write with Meta AI” prompt that helps Instagram users come up with clever comments for posts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, the company launched a new AI-generated video feed called “Vibes” in the Meta AI app, likely boosting downloads. New data from Similarweb shows that the app’s daily active users on iOS and Android increased to 2.7 million as of October 17, up from about 775,000 four weeks ago.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, to address any concerns parents may have, Meta announced earlier this month that it is introducing new parental control features that enable parents to disable chats with AI characters and monitor the topics their teenagers are discussing with the Meta AI chatbot.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/instagram-users-can-now-use-meta-ai-editing-tools-directly-in-ig-stories/</guid><pubDate>Thu, 23 Oct 2025 19:22:40 +0000</pubDate></item><item><title>[NEW] 20-year-old dropouts built AI notetaker Turbo AI to 5 million users (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/20-year-old-dropouts-built-ai-notetaker-turbo-ai-to-5-million-users/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/IMG_2422.jpeg?resize=1200,742" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Five million users. Eight-figure annual recurring revenue. Twenty thousand new users joining daily. These are some solid numbers for a startup called Turbo AI launched in early 2024 by Rudy Arora and Sarthak Dhawan, two 20-year-old college dropouts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most of this growth has come in the past six months, the founders tell TechCrunch, during which their AI-powered note-taking and study tool grew from one million to five million users, while remaining profitable.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They say the idea for Turbo stemmed from a classroom problem many college students face, which is trying to take notes while paying attention to a lecture at the same time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I would always struggle with taking notes because I just couldn’t both listen to the teacher and write at the same time. I just couldn’t do it,” said CEO Dhawan. “Every time I tried to take notes, I’d stop paying attention. And when I listened, I couldn’t take notes. I was like, what if I could use AI?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So the pair built Turbolearn as a side project to allow them to record lectures and automatically generate notes, flashcards, and quizzes. They began sharing it with friends, then it spread to classmates across Duke and Northwestern, where they were enrolled until dropping out this year. Within months, the app had reached other universities, including Harvard and MIT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The product takes the usual note-taker formula—record, transcribe, summarize—and makes it interactive with study notes, quizzes, and flashcards, along with a built-in chat assistant that explains key terms or concepts. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, recordings in large halls often pick up background noise, so the founders built features that allow students to upload PDFs, lectures, YouTube videos, or readings instead. That’s now a more common use case than live lecture recordings.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Students will upload a 30-page lecture and spend two hours going through 75 quiz questions in a row. You don’t do that unless it’s really working,” said Dhawan, noting that students love how the product saves time and helps them retain information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not just students using Turbo AI, though—as reflected by the name change from Turbolearn (a study app) to Turbo AI (an AI notetaker and learning assistant). Professionals have adopted it too, including consultants, lawyers, doctors, and even analysts at Goldman Sachs and McKinsey, the founders say. Some, for instance, upload reports and use Turbo to generate summaries or convert them into podcasts they can listen to on their commute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Arora and Dhawan have been friends since middle school and have collaborated on multiple projects over the years. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Dhawan previously built UMax, an advice app that promised to make people more attractive and reached #1 on the App Store with 20 million users and $6 million in annual revenue. Arora, meanwhile, specializes in using social media strategies to drive explosive growth and attract millions of users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Building viral apps is a rare skill. But despite the scale of their previous projects, the founders only felt the need to drop out for Turbo because they saw an opportunity to build a lasting business. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, unlike many fast-growing AI companies, they’re cautious about raising too much money too early, having taken in only $750,000 last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We raised that before we had a lot of traction. Since then, we’ve had a lot of inbound interest, but we’re taking our time because we’re cash-flow positive and have been profitable our entire time as a company,” said Arora, who added that their 15-person team is based in Los Angeles and focused on staying close to student and creator communities at colleges like UCLA.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Students pay about $20 a month for the product, but the founders say they’re exploring other pricing options to reflect the price sensitivity of students, even as the app scales beyond the target group. “Right now, we’re experimenting with other pricing and running a lot of A/B tests to see what works,” Arora added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Turbo AI sits between fully manual tools like Google Docs and fully automated note-takers like Otter or Fireflies. Users can let the AI take notes or write alongside it, the founders say. That approach has helped Turbo stand out even as competitors like Y Combinator–backed YouLearn target similar student audiences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What’s cool now is that when students think of an AI notetaker or AI study tool, we’re the first ones that come to mind,” Dhawan said.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-wp-embed is-provider-techcrunch wp-block-embed-techcrunch"&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/IMG_2422.jpeg?resize=1200,742" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Five million users. Eight-figure annual recurring revenue. Twenty thousand new users joining daily. These are some solid numbers for a startup called Turbo AI launched in early 2024 by Rudy Arora and Sarthak Dhawan, two 20-year-old college dropouts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most of this growth has come in the past six months, the founders tell TechCrunch, during which their AI-powered note-taking and study tool grew from one million to five million users, while remaining profitable.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They say the idea for Turbo stemmed from a classroom problem many college students face, which is trying to take notes while paying attention to a lecture at the same time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I would always struggle with taking notes because I just couldn’t both listen to the teacher and write at the same time. I just couldn’t do it,” said CEO Dhawan. “Every time I tried to take notes, I’d stop paying attention. And when I listened, I couldn’t take notes. I was like, what if I could use AI?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So the pair built Turbolearn as a side project to allow them to record lectures and automatically generate notes, flashcards, and quizzes. They began sharing it with friends, then it spread to classmates across Duke and Northwestern, where they were enrolled until dropping out this year. Within months, the app had reached other universities, including Harvard and MIT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The product takes the usual note-taker formula—record, transcribe, summarize—and makes it interactive with study notes, quizzes, and flashcards, along with a built-in chat assistant that explains key terms or concepts. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, recordings in large halls often pick up background noise, so the founders built features that allow students to upload PDFs, lectures, YouTube videos, or readings instead. That’s now a more common use case than live lecture recordings.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Students will upload a 30-page lecture and spend two hours going through 75 quiz questions in a row. You don’t do that unless it’s really working,” said Dhawan, noting that students love how the product saves time and helps them retain information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not just students using Turbo AI, though—as reflected by the name change from Turbolearn (a study app) to Turbo AI (an AI notetaker and learning assistant). Professionals have adopted it too, including consultants, lawyers, doctors, and even analysts at Goldman Sachs and McKinsey, the founders say. Some, for instance, upload reports and use Turbo to generate summaries or convert them into podcasts they can listen to on their commute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Arora and Dhawan have been friends since middle school and have collaborated on multiple projects over the years. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Dhawan previously built UMax, an advice app that promised to make people more attractive and reached #1 on the App Store with 20 million users and $6 million in annual revenue. Arora, meanwhile, specializes in using social media strategies to drive explosive growth and attract millions of users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Building viral apps is a rare skill. But despite the scale of their previous projects, the founders only felt the need to drop out for Turbo because they saw an opportunity to build a lasting business. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, unlike many fast-growing AI companies, they’re cautious about raising too much money too early, having taken in only $750,000 last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We raised that before we had a lot of traction. Since then, we’ve had a lot of inbound interest, but we’re taking our time because we’re cash-flow positive and have been profitable our entire time as a company,” said Arora, who added that their 15-person team is based in Los Angeles and focused on staying close to student and creator communities at colleges like UCLA.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Students pay about $20 a month for the product, but the founders say they’re exploring other pricing options to reflect the price sensitivity of students, even as the app scales beyond the target group. “Right now, we’re experimenting with other pricing and running a lot of A/B tests to see what works,” Arora added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Turbo AI sits between fully manual tools like Google Docs and fully automated note-takers like Otter or Fireflies. Users can let the AI take notes or write alongside it, the founders say. That approach has helped Turbo stand out even as competitors like Y Combinator–backed YouLearn target similar student audiences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What’s cool now is that when students think of an AI notetaker or AI study tool, we’re the first ones that come to mind,” Dhawan said.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-wp-embed is-provider-techcrunch wp-block-embed-techcrunch"&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/20-year-old-dropouts-built-ai-notetaker-turbo-ai-to-5-million-users/</guid><pubDate>Thu, 23 Oct 2025 19:25:00 +0000</pubDate></item><item><title>[NEW] OpenAI buys Sky, an AI interface for Mac (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/openai-buys-sky-an-ai-interface-for-mac/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/ArtCard_SKY_16_9.webp?resize=1200,677" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced on Thursday it has acquired Software Applications, Inc., the makers of an AI-powered natural language interface for Mac computers called Sky. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The software product, which had not been released to the public, is designed to work alongside you throughout your day, as you use apps on the computer, writing, planning, coding, and more. Similar to AI browsers, Sky can see what’s on the your screen and take action in your apps for you.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move is a significant step toward embedding OpenAI’s technology into consumers’ everyday lives and within businesses that run on Mac. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve always wanted computers to be more empowering, customizable, and intuitive. With LLMs, we can finally put the pieces together. That’s why we built Sky, an AI experience that floats over your desktop to help you think and create. We’re thrilled to join OpenAI to bring that vision to hundreds of millions of people,” Software Applications co-founder and CEO Ari Weinstein said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, the team behind Sky had another prominent exit before this. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Weinstein and Conrad Kramer previously co-founded Workflow, which they sold to Apple, where it became the technology now known as Shortcuts. Both continued to work at Apple for several years before leaving to found Software Applications in August 2023. Sky’s third co-founder and COO, Kim Beverett, was a senior program and product manager at Apple, where she spent nearly 10 years working on technology like Safari, WebKit, Privacy, Messages, Mail, Phone, FaceTime, and SharePlay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple, which has so far been behind on AI, is expected to launch an overhauled Siri with AI smarts next year. Apple has already shipped other features that use its AI tech known as Apple Intelligence, including writing helpers, live translation, image creation, visual search, and more. It’s also working with OpenAI to shuffle off queries Siri can’t answer to ChatGPT. Apple Intelligence works across platforms, including Mac.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Apple offers a Foundation Models framework that provides access to local AI models, allowing developers to build AI into their apps directly. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, Apple values privacy as a core part of its AI offering, and an agentic system that views your screen and takes action on your behalf could raise concerns for some of its more security-minded customers. Agentic AI is still in its early days, and recent reviews indicate that AI browsers have a lot of safety risks. It could take Apple time to launch a Mac AI system comparable to Sky, as a result.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deal terms for OpenAI’s acquistion were not revealed, but Sky’s maker had raised $6.5 million from investors, including OpenAI CEO Sam Altman, Figma CEO Dylan Field, Context Ventures, and Stellation Capital, according to data from Pitchbook. OpenAI disclosed that Altman held a passive interest in the startup through an investment fund. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal was led by Head of ChatGPT Nick Turley and OpenAI’s CEO of Applications, Fidji Simo, and approved by OpenAI’s board. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/ArtCard_SKY_16_9.webp?resize=1200,677" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced on Thursday it has acquired Software Applications, Inc., the makers of an AI-powered natural language interface for Mac computers called Sky. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The software product, which had not been released to the public, is designed to work alongside you throughout your day, as you use apps on the computer, writing, planning, coding, and more. Similar to AI browsers, Sky can see what’s on the your screen and take action in your apps for you.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move is a significant step toward embedding OpenAI’s technology into consumers’ everyday lives and within businesses that run on Mac. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve always wanted computers to be more empowering, customizable, and intuitive. With LLMs, we can finally put the pieces together. That’s why we built Sky, an AI experience that floats over your desktop to help you think and create. We’re thrilled to join OpenAI to bring that vision to hundreds of millions of people,” Software Applications co-founder and CEO Ari Weinstein said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, the team behind Sky had another prominent exit before this. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Weinstein and Conrad Kramer previously co-founded Workflow, which they sold to Apple, where it became the technology now known as Shortcuts. Both continued to work at Apple for several years before leaving to found Software Applications in August 2023. Sky’s third co-founder and COO, Kim Beverett, was a senior program and product manager at Apple, where she spent nearly 10 years working on technology like Safari, WebKit, Privacy, Messages, Mail, Phone, FaceTime, and SharePlay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple, which has so far been behind on AI, is expected to launch an overhauled Siri with AI smarts next year. Apple has already shipped other features that use its AI tech known as Apple Intelligence, including writing helpers, live translation, image creation, visual search, and more. It’s also working with OpenAI to shuffle off queries Siri can’t answer to ChatGPT. Apple Intelligence works across platforms, including Mac.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Apple offers a Foundation Models framework that provides access to local AI models, allowing developers to build AI into their apps directly. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, Apple values privacy as a core part of its AI offering, and an agentic system that views your screen and takes action on your behalf could raise concerns for some of its more security-minded customers. Agentic AI is still in its early days, and recent reviews indicate that AI browsers have a lot of safety risks. It could take Apple time to launch a Mac AI system comparable to Sky, as a result.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deal terms for OpenAI’s acquistion were not revealed, but Sky’s maker had raised $6.5 million from investors, including OpenAI CEO Sam Altman, Figma CEO Dylan Field, Context Ventures, and Stellation Capital, according to data from Pitchbook. OpenAI disclosed that Altman held a passive interest in the startup through an investment fund. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal was led by Head of ChatGPT Nick Turley and OpenAI’s CEO of Applications, Fidji Simo, and approved by OpenAI’s board. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/openai-buys-sky-an-ai-interface-for-mac/</guid><pubDate>Thu, 23 Oct 2025 20:53:02 +0000</pubDate></item><item><title>[NEW] Researchers show that training on “junk data” can lead to LLM “brain rot” (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/researchers-show-that-training-on-junk-data-can-lead-to-llm-brain-rot/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Models trained on short, popular, and/or “superficial” tweets perform worse on benchmarks.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1908316227-640x640.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1908316227-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      *robot voice* Six-Seven

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On the surface, it seems obvious that training an LLM with “high quality” data will lead to better performance than feeding it any old “low quality” junk you can find. Now, a group of researchers is attempting to quantify just how much this kind of low quality data can cause an LLM to experience effects akin to human “brain rot.”&lt;/p&gt;
&lt;p&gt;For a pre-print paper published this month, the researchers from Texas A&amp;amp;M, the University of Texas, and Purdue University drew inspiration from existing research showing how humans who consume “large volumes of trivial and unchallenging online content” can develop problems with attention, memory, and social cognition. That led them to what they’re calling the “LLM brain rot hypothesis,” summed up as the idea that “continual pre-training on junk web text induces lasting cognitive decline in LLMs.”&lt;/p&gt;
&lt;p&gt;Figuring out what counts as “junk web text” and what counts as “quality content” is far from a simple or fully objective process, of course. But the researchers used a few different metrics to tease a “junk dataset” and “control dataset” from HuggingFace’s corpus of 100 million tweets.&lt;/p&gt;
&lt;p&gt;Since brain rot in humans is “a consequence of Internet addiction,” they write, junk tweets should be ones “that can maximize users’ engagement in a trivial manner.” As such, the researchers created one “junk” dataset by collecting tweets with high engagement numbers (likes, retweets, replies, and quotes) and shorter lengths, figuring that “more popular but shorter tweets will be considered to be junk data.”&lt;/p&gt;
&lt;p&gt;For a second “junk” metric, the researchers drew from marketing research to define the “semantic quality” of the tweets themselves. Using a complex GPT-4o prompt, they sought to pull out tweets that focused on “superficial topics (like conspiracy theories, exaggerated claims, unsupported assertions or superficial lifestyle content)” or that had an “attention-drawing style (such as sensationalized headlines using clickbait language or excessive trigger words).” A random sample of these LLM-based classifications was spot-checked against evaluations from three graduate students with a 76 percent matching rate.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;One man’s junk data is another man’s treasure data?&lt;/h2&gt;
&lt;figure class="ars-wp-img-shortcode id-2123924 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="1040" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/aibrainrot.png" width="2772" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An outline of the methodology used for this research.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Xing et al

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;With these two separate (but partially overlapping) “junk” data sets defined, the researchers pre-trained four LLMs using different ratios of “junk” and “control” data. They then ran those variably trained models through benchmarks to measure reasoning capability (ARC AI2 Reasoning Challenge), long-context memory (RULER), adherence to ethical norms (HH-RLHF and AdvBench), and demonstrated “personality style” (TRAIT).&lt;/p&gt;
&lt;p&gt;The results showed that adding more “junk data” to the training sets had a statistically significant effect on the reasoning and long-context benchmarks across models. The effects were more mixed on the other benchmarks, though. For example, a 50/50 mix of “junk” and control data used for the Llama 8B model generated better scores on some benchmarks (ethical norms, high Openness, low Neuroticism, and Machiavellianism) than either “fully junk” or “fully control” training data sets.&lt;/p&gt;
&lt;p&gt;Based on these results, the researchers warn that “heavily relying on Internet data leads LLM pre-training to the trap of content contamination.” They go on to “call for a re-examination of current data collection from the Internet and continual pre-training practices” and warn that “careful curation and quality control will be essential to prevent cumulative harms” in future models.&lt;/p&gt;
&lt;p&gt;That could be especially true as more and more of the Internet is taken up by AI-generated content that may contribute to “model collapse” if used to train future models. But we can always just destroy a bunch of print books to get quality training data, right?&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Models trained on short, popular, and/or “superficial” tweets perform worse on benchmarks.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1908316227-640x640.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1908316227-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      *robot voice* Six-Seven

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On the surface, it seems obvious that training an LLM with “high quality” data will lead to better performance than feeding it any old “low quality” junk you can find. Now, a group of researchers is attempting to quantify just how much this kind of low quality data can cause an LLM to experience effects akin to human “brain rot.”&lt;/p&gt;
&lt;p&gt;For a pre-print paper published this month, the researchers from Texas A&amp;amp;M, the University of Texas, and Purdue University drew inspiration from existing research showing how humans who consume “large volumes of trivial and unchallenging online content” can develop problems with attention, memory, and social cognition. That led them to what they’re calling the “LLM brain rot hypothesis,” summed up as the idea that “continual pre-training on junk web text induces lasting cognitive decline in LLMs.”&lt;/p&gt;
&lt;p&gt;Figuring out what counts as “junk web text” and what counts as “quality content” is far from a simple or fully objective process, of course. But the researchers used a few different metrics to tease a “junk dataset” and “control dataset” from HuggingFace’s corpus of 100 million tweets.&lt;/p&gt;
&lt;p&gt;Since brain rot in humans is “a consequence of Internet addiction,” they write, junk tweets should be ones “that can maximize users’ engagement in a trivial manner.” As such, the researchers created one “junk” dataset by collecting tweets with high engagement numbers (likes, retweets, replies, and quotes) and shorter lengths, figuring that “more popular but shorter tweets will be considered to be junk data.”&lt;/p&gt;
&lt;p&gt;For a second “junk” metric, the researchers drew from marketing research to define the “semantic quality” of the tweets themselves. Using a complex GPT-4o prompt, they sought to pull out tweets that focused on “superficial topics (like conspiracy theories, exaggerated claims, unsupported assertions or superficial lifestyle content)” or that had an “attention-drawing style (such as sensationalized headlines using clickbait language or excessive trigger words).” A random sample of these LLM-based classifications was spot-checked against evaluations from three graduate students with a 76 percent matching rate.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;One man’s junk data is another man’s treasure data?&lt;/h2&gt;
&lt;figure class="ars-wp-img-shortcode id-2123924 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="1040" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/aibrainrot.png" width="2772" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An outline of the methodology used for this research.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Xing et al

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;With these two separate (but partially overlapping) “junk” data sets defined, the researchers pre-trained four LLMs using different ratios of “junk” and “control” data. They then ran those variably trained models through benchmarks to measure reasoning capability (ARC AI2 Reasoning Challenge), long-context memory (RULER), adherence to ethical norms (HH-RLHF and AdvBench), and demonstrated “personality style” (TRAIT).&lt;/p&gt;
&lt;p&gt;The results showed that adding more “junk data” to the training sets had a statistically significant effect on the reasoning and long-context benchmarks across models. The effects were more mixed on the other benchmarks, though. For example, a 50/50 mix of “junk” and control data used for the Llama 8B model generated better scores on some benchmarks (ethical norms, high Openness, low Neuroticism, and Machiavellianism) than either “fully junk” or “fully control” training data sets.&lt;/p&gt;
&lt;p&gt;Based on these results, the researchers warn that “heavily relying on Internet data leads LLM pre-training to the trap of content contamination.” They go on to “call for a re-examination of current data collection from the Internet and continual pre-training practices” and warn that “careful curation and quality control will be essential to prevent cumulative harms” in future models.&lt;/p&gt;
&lt;p&gt;That could be especially true as more and more of the Internet is taken up by AI-generated content that may contribute to “model collapse” if used to train future models. But we can always just destroy a bunch of print books to get quality training data, right?&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/researchers-show-that-training-on-junk-data-can-lead-to-llm-brain-rot/</guid><pubDate>Thu, 23 Oct 2025 21:20:48 +0000</pubDate></item><item><title>[NEW] Lawsuit: Reddit caught Perplexity “red-handed” stealing data from Google results (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/10/reddit-sues-to-block-perplexity-from-scraping-google-search-results/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Scraper accused of stealing Reddit content “shocked” by lawsuit.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2236792840-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2236792840-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          SOPA Images / Contributor | LightRocket

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;In a lawsuit filed on Wednesday, Reddit accused an AI search engine, Perplexity, of conspiring with several companies to illegally scrape Reddit content from Google search results, allegedly dodging anti-scraping methods that require substantial investments from both Google and Reddit.&lt;/p&gt;
&lt;p&gt;Reddit alleged that Perplexity feeds off Reddit and Google, claiming to be “the world’s first answer engine” but really doing “nothing groundbreaking.”&lt;/p&gt;
&lt;p&gt;“Its answer engine simply uses a different company’s” large language model “to parse through a massive number of Google search results to see if it can answer a user’s question based on those results,” the lawsuit said. “But Perplexity can only run its ‘answer engine’ by wrongfully accessing and scraping Reddit content appearing in Google’s own search results from Google’s own search engine.”&lt;/p&gt;
&lt;p&gt;Likening companies involved in the alleged conspiracy to “bank robbers,” Reddit claimed it caught Perplexity “red-handed” stealing content that its “answer engine” should not have had access to.&lt;/p&gt;
&lt;p&gt;Baiting Perplexity with “the digital equivalent of marked bills,” Reddit tested out posting content that could only be found in Google search engine results pages (SERPs) and “within hours, queries to Perplexity’s ‘answer engine’ produced the contents of that test post.”&lt;/p&gt;
&lt;p&gt;“The only way that Perplexity could have obtained that Reddit content and then used it in its ‘answer engine’ is if it and/or its Co-Defendants scraped Google SERPs for that Reddit content and Perplexity then quickly incorporated that data into its answer engine,” Reddit’s lawsuit said.&lt;/p&gt;
&lt;p&gt;In a Reddit post, Perplexity denied any wrongdoing, describing its answer engine as summarizing Reddit discussions and citing Reddit threads in answers, just like anyone who shares links or posts on Reddit might do. Perplexity suggested that Reddit was attacking the open Internet by trying to extort licensing fees for Reddit content, despite knowing that Perplexity doesn’t train foundational models. Reddit’s endgame, Perplexity alleged, was to use the Perplexity lawsuit as a “show of force in Reddit’s training data negotiations with Google and OpenAI.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“We won’t be extorted, and we won’t help Reddit extort Google, even if they’re our (huge) competitor,” Perplexity wrote. “Perplexity will play fair, but we won’t cave. And we won’t let bigger companies use us in shell games. ”&lt;/p&gt;
&lt;p&gt;Reddit likely anticipated Perplexity’s defense of the “open Internet,” noting in its complaint that “Reddit’s current Robots Exclusion Protocol file (‘robots.txt’) says, ‘Reddit believes in an open Internet, but not the misuse of public content.'”&lt;/p&gt;
&lt;h2&gt;Google reveals how scrapers steal from search results&lt;/h2&gt;
&lt;p&gt;To block scraping, Reddit uses various measures, such as “registered user-identification limits, IP-rate limits, captcha bot protection, and anomaly-detection tools,” the complaint said.&lt;/p&gt;
&lt;p&gt;Similarly, Google relies on “anti-scraping systems and teams dedicated to preventing unauthorized access to its products and services,” Reddit said, noting Google prohibits “unauthorized automated access” to its SERPs.&lt;/p&gt;
&lt;p&gt;To back its claims, Reddit subpoenaed Google to find out more about how the search giant blocks AI scrapers from accessing content on SERPs. Google confirmed it relies on “a technological access control system called ‘SearchGuard,’ which is designed to prevent automated systems from accessing and obtaining wholesale search results and indexed data while allowing individual users—i.e., humans—access to Google’s search results, including results that feature Reddit data.”&lt;/p&gt;
&lt;p&gt;“SearchGuard prevents unauthorized access to Google’s search data by imposing a barrier challenge that cannot be solved in the ordinary course by automated systems unless they take affirmative actions to circumvent the SearchGuard system,” Reddit’s complaint explained.&lt;/p&gt;
&lt;p&gt;Bypassing these anti-scraping systems violates the Digital Millennium Copyright Act, Reddit alleged, as well as laws against unfair trade and unjust enrichment. Seemingly, Google’s SearchGuard may currently be the easiest to bypass for alleged conspirators who supposedly pivoted to looting Google SERPs after realizing they couldn’t access Reddit content directly on the platform.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Scrapers shocked by Reddit lawsuit&lt;/h2&gt;
&lt;p&gt;Reddit accused three companies of conspiring with Perplexity—”a Lithuanian data scraper” called Oxylabs UAB, “a former Russian botnet” known as AWMProxy, and SerpApi, a Texas company that sells services for scraping search engines.&lt;/p&gt;
&lt;p&gt;Oxylabs “is explicit that its scraping service is meant to circumvent Google’s technological measures,” Reddit alleged, pointing to an Oxylabs’ website called “How to Scrape Google Search Results.”&lt;/p&gt;
&lt;p&gt;SerpApi touts&amp;nbsp;the same service, including some options to scrape SERPs at “ludicrous speeds.” To trick browsers, SerpApi’s fastest option uses “a server-swarm to hide from, avoid, or simply overwhelm by brute force effective measures Google has put in place to ward off automated access to search engine results,” Reddit alleged. SerpApi also allegedly provides users “with tips to reduce the chance of being blocked while web scraping, such as by sending ‘fake user-agent string[s],’ shifting IP addresses to avoid multiple requests from the same address, and using proxies ‘to make traffic look like regular user traffic’ and thereby ‘impersonate’ user traffic.”&lt;/p&gt;
&lt;p&gt;According to Reddit, the three companies disguise “their web scrapers as regular people (among other techniques) to circumvent or bypass the security restrictions meant to stop them.” During a two-week span in July, they scraped “almost three billion” SERPs containing Reddit text, URLs, images, and videos, a subpoena requesting information from Google revealed.&lt;/p&gt;
&lt;p&gt;Ars could not immediately reach AWMProxy for comment. However, the other companies were surprised by Reddit’s lawsuit, while vowing to defend their business models.&lt;/p&gt;
&lt;p&gt;SerpApi’s spokesperson told Ars that Reddit did not notify the company before filing the lawsuit.&lt;/p&gt;
&lt;p&gt;“We strongly disagree with Reddit’s allegations and intend to vigorously defend ourselves in court,” SerpApi’s spokesperson said. “In the eight years we’ve been in business, SerpApi has always operated on the right side of the law. As stated on our website, ‘The crawling and parsing of public data is protected by the First Amendment of the United States Constitution. We value freedom of speech tremendously.’”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Additionally, SerpAPI works “closely with our attorneys to ensure that our services comply with all applicable laws and fair use principles. SerpApi stands firmly behind its business model and conduct, and we will continue to defend our rights to the fullest extent,” the spokesperson said.&lt;/p&gt;
&lt;p&gt;Oxylabs’ chief governance strategy officer, Denas Grybauskas, told Ars that Reddit’s complaint seemed baffling since the other companies involved in the litigation are “unrelated and unaffiliated.”&lt;/p&gt;
&lt;p&gt;“We are shocked and disappointed by this news, as Reddit has made no attempt to speak with us directly or communicate any potential concerns,” Grybauskas said. “Oxylabs has always been and will continue to be a pioneer and an industry leader in public data collection, and it will not hesitate to defend itself against these allegations. Oxylabs’ position is that no company should claim ownership of public data that does not belong to them. It is possible that it is just an attempt to sell the same public data at an inflated price.”&lt;/p&gt;
&lt;p&gt;Grybauskas defended Oxylabs’ business as creating “real-world value for thousands of businesses and researchers, such as those driving open-source investigations, disinformation tackling, or environmental monitoring.”&lt;/p&gt;
&lt;p&gt;“We strongly believe that our core business principles make the Internet a better place and serve the public good,” Grybauskas said. “Oxylabs provides infrastructure for compliant access to publicly available information, and we demand every customer to use our services lawfully. ”&lt;/p&gt;
&lt;h2&gt;Reddit cited threats to licensing deals&lt;/h2&gt;
&lt;p&gt;Apparently, Reddit caught on to the alleged scheme after sending cease-and-desist letters to Perplexity to stop scraping Reddit content that its answer engine was citing. Rather than ending the scraping, Reddit claimed Perplexity’s citations increased “forty-fold.” Since Perplexity is a customer listed on SerpApi’s website, Reddit hypothesized the two were conspiring to skirt Google’s anti-circumvention tools, the complaint said, along with the other companies.&lt;/p&gt;
&lt;p&gt;In a statement provided to Ars, Ben Lee, chief legal officer at Reddit, said that Oxylabs, AWMProxy, and SerpApi were “textbook examples” of scrapers that “bypass technological protections to steal data, then sell it to clients hungry for training material.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Unable to scrape Reddit directly, they mask their identities, hide their locations, and disguise their web scrapers to steal Reddit content from Google Search,” Lee said. “Perplexity is a willing customer of at least one of these scrapers, choosing to buy stolen data rather than enter into a lawful agreement with Reddit itself.”&lt;/p&gt;
&lt;p&gt;On Reddit, Perplexity pushed back on Reddit’s claims that Perplexity ignored requests to license Reddit content.&lt;/p&gt;
&lt;p&gt;“Untrue. Whenever anyone asks us about content licensing, we explain that Perplexity, as an application-layer company, does not train AI models on content,” Perplexity said. “Never has. So, it is impossible for us to sign a license agreement to do so.”&lt;/p&gt;
&lt;p&gt;Reddit supposedly “insisted we pay anyway, despite lawfully accessing Reddit data,” Perplexity said. “Bowing to strong arm&amp;nbsp;tactics just isn’t how we do business.”&lt;/p&gt;
&lt;p&gt;Perplexity’s spokesperson, Jesse Dwyer, told Ars the company chose to post its statement on Reddit “to illustrate a simple point.”&lt;/p&gt;
&lt;p&gt;“It is a public Reddit link accessible to anyone, yet by the logic of Reddit’s lawsuit, if you mention it or cite it in any way (which is your job as a reporter), they might just sue you,” Dwyer said.&lt;/p&gt;
&lt;p&gt;But Reddit claimed that its business and reputation have been “damaged” by “misappropriation of Reddit data and circumvention of technological control measures.” Without a licensing deal ensuring that Perplexity and others are respecting Reddit policies, Reddit cannot control who has access to data, how they’re using data, and if data use conflicts with Reddit’s privacy policy and user agreement, the complaint said.&lt;/p&gt;
&lt;p&gt;Further, Reddit’s worried that Perplexity’s workaround could catch on, potentially messing up Reddit’s other licensing deals. All the while, Reddit noted, it has to invest “significant resources” in anti-scraping technology, with Reddit ultimately suffering damages, including “lost profits and business opportunities, reputational harm, and loss of user trust.”&lt;/p&gt;
&lt;p&gt;Reddit’s hoping the court will grant an injunction barring companies from scraping Reddit content from Google SERPs. It also wants companies blocked from both selling Reddit data and “developing or distributing any technology or product that is used for the unauthorized circumvention of technological control measures and scraping of Reddit data.”&lt;/p&gt;
&lt;p&gt;If Reddit wins, companies could be required to pay substantial damages or to disgorge profits from the sale of Reddit content.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Advance Publications, which owns Ars Technica parent Condé Nast, is the largest shareholder in Reddit.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Scraper accused of stealing Reddit content “shocked” by lawsuit.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2236792840-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2236792840-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          SOPA Images / Contributor | LightRocket

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;In a lawsuit filed on Wednesday, Reddit accused an AI search engine, Perplexity, of conspiring with several companies to illegally scrape Reddit content from Google search results, allegedly dodging anti-scraping methods that require substantial investments from both Google and Reddit.&lt;/p&gt;
&lt;p&gt;Reddit alleged that Perplexity feeds off Reddit and Google, claiming to be “the world’s first answer engine” but really doing “nothing groundbreaking.”&lt;/p&gt;
&lt;p&gt;“Its answer engine simply uses a different company’s” large language model “to parse through a massive number of Google search results to see if it can answer a user’s question based on those results,” the lawsuit said. “But Perplexity can only run its ‘answer engine’ by wrongfully accessing and scraping Reddit content appearing in Google’s own search results from Google’s own search engine.”&lt;/p&gt;
&lt;p&gt;Likening companies involved in the alleged conspiracy to “bank robbers,” Reddit claimed it caught Perplexity “red-handed” stealing content that its “answer engine” should not have had access to.&lt;/p&gt;
&lt;p&gt;Baiting Perplexity with “the digital equivalent of marked bills,” Reddit tested out posting content that could only be found in Google search engine results pages (SERPs) and “within hours, queries to Perplexity’s ‘answer engine’ produced the contents of that test post.”&lt;/p&gt;
&lt;p&gt;“The only way that Perplexity could have obtained that Reddit content and then used it in its ‘answer engine’ is if it and/or its Co-Defendants scraped Google SERPs for that Reddit content and Perplexity then quickly incorporated that data into its answer engine,” Reddit’s lawsuit said.&lt;/p&gt;
&lt;p&gt;In a Reddit post, Perplexity denied any wrongdoing, describing its answer engine as summarizing Reddit discussions and citing Reddit threads in answers, just like anyone who shares links or posts on Reddit might do. Perplexity suggested that Reddit was attacking the open Internet by trying to extort licensing fees for Reddit content, despite knowing that Perplexity doesn’t train foundational models. Reddit’s endgame, Perplexity alleged, was to use the Perplexity lawsuit as a “show of force in Reddit’s training data negotiations with Google and OpenAI.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“We won’t be extorted, and we won’t help Reddit extort Google, even if they’re our (huge) competitor,” Perplexity wrote. “Perplexity will play fair, but we won’t cave. And we won’t let bigger companies use us in shell games. ”&lt;/p&gt;
&lt;p&gt;Reddit likely anticipated Perplexity’s defense of the “open Internet,” noting in its complaint that “Reddit’s current Robots Exclusion Protocol file (‘robots.txt’) says, ‘Reddit believes in an open Internet, but not the misuse of public content.'”&lt;/p&gt;
&lt;h2&gt;Google reveals how scrapers steal from search results&lt;/h2&gt;
&lt;p&gt;To block scraping, Reddit uses various measures, such as “registered user-identification limits, IP-rate limits, captcha bot protection, and anomaly-detection tools,” the complaint said.&lt;/p&gt;
&lt;p&gt;Similarly, Google relies on “anti-scraping systems and teams dedicated to preventing unauthorized access to its products and services,” Reddit said, noting Google prohibits “unauthorized automated access” to its SERPs.&lt;/p&gt;
&lt;p&gt;To back its claims, Reddit subpoenaed Google to find out more about how the search giant blocks AI scrapers from accessing content on SERPs. Google confirmed it relies on “a technological access control system called ‘SearchGuard,’ which is designed to prevent automated systems from accessing and obtaining wholesale search results and indexed data while allowing individual users—i.e., humans—access to Google’s search results, including results that feature Reddit data.”&lt;/p&gt;
&lt;p&gt;“SearchGuard prevents unauthorized access to Google’s search data by imposing a barrier challenge that cannot be solved in the ordinary course by automated systems unless they take affirmative actions to circumvent the SearchGuard system,” Reddit’s complaint explained.&lt;/p&gt;
&lt;p&gt;Bypassing these anti-scraping systems violates the Digital Millennium Copyright Act, Reddit alleged, as well as laws against unfair trade and unjust enrichment. Seemingly, Google’s SearchGuard may currently be the easiest to bypass for alleged conspirators who supposedly pivoted to looting Google SERPs after realizing they couldn’t access Reddit content directly on the platform.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Scrapers shocked by Reddit lawsuit&lt;/h2&gt;
&lt;p&gt;Reddit accused three companies of conspiring with Perplexity—”a Lithuanian data scraper” called Oxylabs UAB, “a former Russian botnet” known as AWMProxy, and SerpApi, a Texas company that sells services for scraping search engines.&lt;/p&gt;
&lt;p&gt;Oxylabs “is explicit that its scraping service is meant to circumvent Google’s technological measures,” Reddit alleged, pointing to an Oxylabs’ website called “How to Scrape Google Search Results.”&lt;/p&gt;
&lt;p&gt;SerpApi touts&amp;nbsp;the same service, including some options to scrape SERPs at “ludicrous speeds.” To trick browsers, SerpApi’s fastest option uses “a server-swarm to hide from, avoid, or simply overwhelm by brute force effective measures Google has put in place to ward off automated access to search engine results,” Reddit alleged. SerpApi also allegedly provides users “with tips to reduce the chance of being blocked while web scraping, such as by sending ‘fake user-agent string[s],’ shifting IP addresses to avoid multiple requests from the same address, and using proxies ‘to make traffic look like regular user traffic’ and thereby ‘impersonate’ user traffic.”&lt;/p&gt;
&lt;p&gt;According to Reddit, the three companies disguise “their web scrapers as regular people (among other techniques) to circumvent or bypass the security restrictions meant to stop them.” During a two-week span in July, they scraped “almost three billion” SERPs containing Reddit text, URLs, images, and videos, a subpoena requesting information from Google revealed.&lt;/p&gt;
&lt;p&gt;Ars could not immediately reach AWMProxy for comment. However, the other companies were surprised by Reddit’s lawsuit, while vowing to defend their business models.&lt;/p&gt;
&lt;p&gt;SerpApi’s spokesperson told Ars that Reddit did not notify the company before filing the lawsuit.&lt;/p&gt;
&lt;p&gt;“We strongly disagree with Reddit’s allegations and intend to vigorously defend ourselves in court,” SerpApi’s spokesperson said. “In the eight years we’ve been in business, SerpApi has always operated on the right side of the law. As stated on our website, ‘The crawling and parsing of public data is protected by the First Amendment of the United States Constitution. We value freedom of speech tremendously.’”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Additionally, SerpAPI works “closely with our attorneys to ensure that our services comply with all applicable laws and fair use principles. SerpApi stands firmly behind its business model and conduct, and we will continue to defend our rights to the fullest extent,” the spokesperson said.&lt;/p&gt;
&lt;p&gt;Oxylabs’ chief governance strategy officer, Denas Grybauskas, told Ars that Reddit’s complaint seemed baffling since the other companies involved in the litigation are “unrelated and unaffiliated.”&lt;/p&gt;
&lt;p&gt;“We are shocked and disappointed by this news, as Reddit has made no attempt to speak with us directly or communicate any potential concerns,” Grybauskas said. “Oxylabs has always been and will continue to be a pioneer and an industry leader in public data collection, and it will not hesitate to defend itself against these allegations. Oxylabs’ position is that no company should claim ownership of public data that does not belong to them. It is possible that it is just an attempt to sell the same public data at an inflated price.”&lt;/p&gt;
&lt;p&gt;Grybauskas defended Oxylabs’ business as creating “real-world value for thousands of businesses and researchers, such as those driving open-source investigations, disinformation tackling, or environmental monitoring.”&lt;/p&gt;
&lt;p&gt;“We strongly believe that our core business principles make the Internet a better place and serve the public good,” Grybauskas said. “Oxylabs provides infrastructure for compliant access to publicly available information, and we demand every customer to use our services lawfully. ”&lt;/p&gt;
&lt;h2&gt;Reddit cited threats to licensing deals&lt;/h2&gt;
&lt;p&gt;Apparently, Reddit caught on to the alleged scheme after sending cease-and-desist letters to Perplexity to stop scraping Reddit content that its answer engine was citing. Rather than ending the scraping, Reddit claimed Perplexity’s citations increased “forty-fold.” Since Perplexity is a customer listed on SerpApi’s website, Reddit hypothesized the two were conspiring to skirt Google’s anti-circumvention tools, the complaint said, along with the other companies.&lt;/p&gt;
&lt;p&gt;In a statement provided to Ars, Ben Lee, chief legal officer at Reddit, said that Oxylabs, AWMProxy, and SerpApi were “textbook examples” of scrapers that “bypass technological protections to steal data, then sell it to clients hungry for training material.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Unable to scrape Reddit directly, they mask their identities, hide their locations, and disguise their web scrapers to steal Reddit content from Google Search,” Lee said. “Perplexity is a willing customer of at least one of these scrapers, choosing to buy stolen data rather than enter into a lawful agreement with Reddit itself.”&lt;/p&gt;
&lt;p&gt;On Reddit, Perplexity pushed back on Reddit’s claims that Perplexity ignored requests to license Reddit content.&lt;/p&gt;
&lt;p&gt;“Untrue. Whenever anyone asks us about content licensing, we explain that Perplexity, as an application-layer company, does not train AI models on content,” Perplexity said. “Never has. So, it is impossible for us to sign a license agreement to do so.”&lt;/p&gt;
&lt;p&gt;Reddit supposedly “insisted we pay anyway, despite lawfully accessing Reddit data,” Perplexity said. “Bowing to strong arm&amp;nbsp;tactics just isn’t how we do business.”&lt;/p&gt;
&lt;p&gt;Perplexity’s spokesperson, Jesse Dwyer, told Ars the company chose to post its statement on Reddit “to illustrate a simple point.”&lt;/p&gt;
&lt;p&gt;“It is a public Reddit link accessible to anyone, yet by the logic of Reddit’s lawsuit, if you mention it or cite it in any way (which is your job as a reporter), they might just sue you,” Dwyer said.&lt;/p&gt;
&lt;p&gt;But Reddit claimed that its business and reputation have been “damaged” by “misappropriation of Reddit data and circumvention of technological control measures.” Without a licensing deal ensuring that Perplexity and others are respecting Reddit policies, Reddit cannot control who has access to data, how they’re using data, and if data use conflicts with Reddit’s privacy policy and user agreement, the complaint said.&lt;/p&gt;
&lt;p&gt;Further, Reddit’s worried that Perplexity’s workaround could catch on, potentially messing up Reddit’s other licensing deals. All the while, Reddit noted, it has to invest “significant resources” in anti-scraping technology, with Reddit ultimately suffering damages, including “lost profits and business opportunities, reputational harm, and loss of user trust.”&lt;/p&gt;
&lt;p&gt;Reddit’s hoping the court will grant an injunction barring companies from scraping Reddit content from Google SERPs. It also wants companies blocked from both selling Reddit data and “developing or distributing any technology or product that is used for the unauthorized circumvention of technological control measures and scraping of Reddit data.”&lt;/p&gt;
&lt;p&gt;If Reddit wins, companies could be required to pay substantial damages or to disgorge profits from the sale of Reddit content.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Advance Publications, which owns Ars Technica parent Condé Nast, is the largest shareholder in Reddit.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/10/reddit-sues-to-block-perplexity-from-scraping-google-search-results/</guid><pubDate>Thu, 23 Oct 2025 21:54:39 +0000</pubDate></item><item><title>[NEW] With new acquisition, OpenAI signals plans to integrate deeper into the OS (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/openai-acquires-the-team-that-made-apples-shortcuts/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The acquired firm was working on a tool to control macOS directly with AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI and Sky logos, side by side." class="absolute inset-0 w-full h-full object-cover hidden" height="320" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/openai-sky-640x320.webp" width="640" /&gt;
                  &lt;img alt="The OpenAI and Sky logos, side by side." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/openai-sky-1152x648.webp" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The simple graphic used to announce the acquisition.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI has acquired Software Applications Incorporated (SAI), perhaps best known for the core team that produced what became Shortcuts on Apple platforms. More recently, the team has been working on Sky, a context-aware AI interface layer on top of macOS. The financial terms of the acquisition have not been publicly disclosed.&lt;/p&gt;
&lt;p&gt;“AI progress isn’t only about advancing intelligence—it’s about unlocking it through interfaces that understand context, adapt to your intent, and work seamlessly,” an OpenAI rep wrote in the company’s blog post about the acquisition. The post goes on to specify that OpenAI plans to “bring Sky’s deep macOS integration and product craft into ChatGPT, and all members of the team will join OpenAI.”&lt;/p&gt;
&lt;p&gt;That includes SAI co-founders Ari Weinstein (CEO), Conrad Kramer (CTO), and Kim Beverett (Product Lead)—all of whom worked together for several years at Apple after Apple acquired Weinstein and Kramer’s previous company, which produced an automation tool called Workflows, to integrate Shortcuts across Apple’s software platforms.&lt;/p&gt;
&lt;p&gt;The three SAI founders left Apple to work on Sky, which leverages Apple APIs and accessibility features to provide context about what’s on screen to a large language model; the LLM takes plain language user commands and executes them across multiple applications. At its best, the tool aimed to be a bit like Shortcuts, but with no setup, generating workflows on the fly based on user prompts.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It bears some resemblance to features of Atlas, the ChatGPT-driven web browser that OpenAI launched earlier this week, and this acquisition piles on even more evidence that OpenAI has ambitions beyond a question-and-answer chatbot.&lt;/p&gt;
&lt;p&gt;OpenAI can use the SAI team’s knowledge of the macOS platform to develop new ways for ChatGPT not just to make suggestions about, but to agentically work directly on users’ macOS environments.&lt;/p&gt;
&lt;p&gt;So far, most of OpenAI’s native desktop efforts have been on macOS; this may be because its core audience of early adopters includes a large cohort of front-end web and mobile application developers, many of whom use macOS as their primary platform.&lt;/p&gt;
&lt;p&gt;It’s unclear at this point whether development on Sky will continue as originally planned or be adapted into something meaningfully different under the OpenAI umbrella—but the public statements by both companies about the acquisition suggest this is about more than just a macOS application.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The acquired firm was working on a tool to control macOS directly with AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI and Sky logos, side by side." class="absolute inset-0 w-full h-full object-cover hidden" height="320" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/openai-sky-640x320.webp" width="640" /&gt;
                  &lt;img alt="The OpenAI and Sky logos, side by side." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/openai-sky-1152x648.webp" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The simple graphic used to announce the acquisition.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI has acquired Software Applications Incorporated (SAI), perhaps best known for the core team that produced what became Shortcuts on Apple platforms. More recently, the team has been working on Sky, a context-aware AI interface layer on top of macOS. The financial terms of the acquisition have not been publicly disclosed.&lt;/p&gt;
&lt;p&gt;“AI progress isn’t only about advancing intelligence—it’s about unlocking it through interfaces that understand context, adapt to your intent, and work seamlessly,” an OpenAI rep wrote in the company’s blog post about the acquisition. The post goes on to specify that OpenAI plans to “bring Sky’s deep macOS integration and product craft into ChatGPT, and all members of the team will join OpenAI.”&lt;/p&gt;
&lt;p&gt;That includes SAI co-founders Ari Weinstein (CEO), Conrad Kramer (CTO), and Kim Beverett (Product Lead)—all of whom worked together for several years at Apple after Apple acquired Weinstein and Kramer’s previous company, which produced an automation tool called Workflows, to integrate Shortcuts across Apple’s software platforms.&lt;/p&gt;
&lt;p&gt;The three SAI founders left Apple to work on Sky, which leverages Apple APIs and accessibility features to provide context about what’s on screen to a large language model; the LLM takes plain language user commands and executes them across multiple applications. At its best, the tool aimed to be a bit like Shortcuts, but with no setup, generating workflows on the fly based on user prompts.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It bears some resemblance to features of Atlas, the ChatGPT-driven web browser that OpenAI launched earlier this week, and this acquisition piles on even more evidence that OpenAI has ambitions beyond a question-and-answer chatbot.&lt;/p&gt;
&lt;p&gt;OpenAI can use the SAI team’s knowledge of the macOS platform to develop new ways for ChatGPT not just to make suggestions about, but to agentically work directly on users’ macOS environments.&lt;/p&gt;
&lt;p&gt;So far, most of OpenAI’s native desktop efforts have been on macOS; this may be because its core audience of early adopters includes a large cohort of front-end web and mobile application developers, many of whom use macOS as their primary platform.&lt;/p&gt;
&lt;p&gt;It’s unclear at this point whether development on Sky will continue as originally planned or be adapted into something meaningfully different under the OpenAI umbrella—but the public statements by both companies about the acquisition suggest this is about more than just a macOS application.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/openai-acquires-the-team-that-made-apples-shortcuts/</guid><pubDate>Thu, 23 Oct 2025 22:08:40 +0000</pubDate></item><item><title>[NEW] OpenAI launches company knowledge in ChatGPT, letting you access your firm's data from Google Drive, Slack, GitHub (AI | VentureBeat)</title><link>https://venturebeat.com/ai/openai-launches-company-knowledge-in-chatgpt-letting-you-access-your-firms</link><description>[unable to retrieve full-text content]&lt;p&gt;Is the Google Search for internal enterprise knowledge finally here...but from &lt;i&gt;OpenAI&lt;/i&gt;? It certainly seems that way. &lt;/p&gt;&lt;p&gt;Today, OpenAI has &lt;a href="https://openai.com/index/introducing-company-knowledge/"&gt;launched company knowledge in ChatGPT&lt;/a&gt;, a major new capability for subscribers to ChatGPT&amp;#x27;s paid Business, Enterprise, and Edu plans that lets them call up their company&amp;#x27;s data directly from third-party workplace apps including Slack, SharePoint, Google Drive, Gmail, GitHub, HubSpot and combine it in ChatGPT outputs to them. &lt;/p&gt;&lt;p&gt;As OpenAI&amp;#x27;s CEO of Applications Fidji Simo put it in a post on the social network X: &amp;quot;it brings all the context from your apps (Slack, Google Drive, GitHub, etc) together in ChatGPT so you can get answers that are specific to your business.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Intriguingly, OpenAI&amp;#x27;s blog post on the feature states that is &amp;quot;powered by a version of GPT‑5 that’s trained to look across multiple sources to give more comprehensive and accurate answers,&amp;quot; which sounds to me like a new fine-tuned version of the &lt;a href="https://venturebeat.com/ai/openai-launches-gpt-5-not-agi-but-capable-of-generating-software-on-demand"&gt;model family the company released back in August&lt;/a&gt;, though there are no additional details on how it was trained. &lt;/p&gt;&lt;p&gt;Nonetheless, company knowledge in ChatGPT is rolling out globally and is designed to make ChatGPT a central point of access for verified organizational information, supported by secure integrations and enterprise-grade compliance controls, and give employees way faster access to their company&amp;#x27;s information while working.&lt;/p&gt;&lt;p&gt;Now, instead of toggling over to Slack to find the assignment you were given and instructions, or tabbing over to Google Drive and opening up specific files to find the names and numbers you need to call, ChatGPT can deliver all that type of information directly into your chat session — if your company enables the proper connections.&lt;/p&gt;&lt;p&gt;As OpenAI Chief Operating Officer Brad Lightcap &lt;a href="https://x.com/bradlightcap/status/1981454865454027007"&gt;wrote in a post on the social network X&lt;/a&gt;: &amp;quot;company knowledge has changed how i use chatgpt at work more than anything we have built so far - let us know what you think!&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;It builds upon the third-party app connectors &lt;a href="https://venturebeat.com/ai/openai-adds-new-chatgpt-third-party-tool-connectors-to-dropbox-ms-teams-as-altman-clarifies-gpt-5-prioritization"&gt;unveiled back in August 2025&lt;/a&gt;, though those were only for individual users on the ChatGPT Plus plans.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Connecting ChatGPT to Workplace Systems&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Enterprise teams often face the challenge of fragmented data across various internal tools—email, chat, file storage, project management, and customer platforms. &lt;/p&gt;&lt;p&gt;Company knowledge bridges those silos by enabling ChatGPT to connect to approved systems like, and other supported apps through enterprise-managed connectors.&lt;/p&gt;&lt;p&gt;Each response generated with company knowledge includes citations and direct links to the original sources, allowing teams to verify where specific details originated. This transparency helps organizations maintain data trustworthiness while increasing productivity.&lt;/p&gt;&lt;p&gt;OpenAI confirms that company knowledge uses a version of GPT-5 optimized for multi-source reasoning and cross-system synthesis, providing detailed, contextually accurate results even across disparate sources.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Built for Enterprise Control and Security&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Company knowledge was designed from the ground up for enterprise governance and compliance. It respects existing permissions within connected apps — ChatGPT can only access what a user is already authorized to view— and never trains on company data by default.&lt;/p&gt;&lt;p&gt;Security features include industry-standard encryption, support for SSO and SCIM for account provisioning, and IP allowlisting to restrict access to approved corporate networks. &lt;/p&gt;&lt;p&gt;Enterprise administrators can also define role-based access control (RBAC) policies and manage permissions at a group or department level.&lt;/p&gt;&lt;p&gt;OpenAI’s Enterprise Compliance API provides a full audit trail, allowing administrators to review conversation logs for reporting and regulatory purposes. &lt;/p&gt;&lt;p&gt;This capability helps enterprises meet internal governance standards and industry-specific requirements such as SOC 2 and ISO 27001 compliance.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Admin Configuration and Connector Management&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For enterprise deployment, administrators must enable company knowledge and its connectors within the ChatGPT workspace. Once connectors are active, users can authenticate their own accounts for each work app they need to access.&lt;/p&gt;&lt;p&gt;In Enterprise and Edu plans, connectors are off by default and require explicit admin approval before employees can use them. Admins can selectively enable connectors, manage access by role, and require SSO-based authentication for enhanced control.&lt;/p&gt;&lt;p&gt;Business plan users, by contrast, have connectors enabled automatically if available in their workspace. Admins can still oversee which connectors are approved, ensuring alignment with internal IT and data policies.&lt;/p&gt;&lt;p&gt;Company knowledge becomes available to any user with at least one active connector, and admins can configure group-level permissions for different teams — such as restricting GitHub access to engineering while enabling Google Drive or HubSpot for marketing and sales.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;How Company Knowledge Works in Practice&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Activating company knowledge is straightforward. Users can start a new or existing conversation in ChatGPT and select “Company knowledge” under the message composer or from the tools menu. &lt;/p&gt;&lt;p&gt;After authenticating their connected apps, they can ask questions as usual—such as “Summarize this account’s latest feedback and risks” or “Compile a Q4 performance summary from project trackers.”&lt;/p&gt;&lt;p&gt;ChatGPT searches across the connected tools, retrieves relevant context, and produces an answer with full citations and source links. &lt;/p&gt;&lt;p&gt;The system can combine data across apps — for instance, blending Slack updates, Google Docs notes, and HubSpot CRM records — to create an integrated view of a project, client, or initiative.&lt;/p&gt;&lt;p&gt;When company knowledge is not selected, ChatGPT may still use connectors in a limited capacity as part of the default experience, but responses will not include detailed citations or multi-source synthesis.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Advanced Use Cases for Enterprise Teams&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For development and operations leaders, company knowledge can act as a centralized intelligence layer that surfaces real-time updates and dependencies across complex workflows. ChatGPT can, for example, summarize open GitHub pull requests, highlight unresolved Linear tickets, and cross-reference Slack engineering discussions—all in a single output.&lt;/p&gt;&lt;p&gt;Technical teams can also use it for incident retrospectives or release planning by pulling relevant information from issue trackers, logs, and meeting notes. Procurement or finance leaders can use it to consolidate purchase requests or budget updates across shared drives and internal communications.&lt;/p&gt;&lt;p&gt;Because the model can reference structured and unstructured data simultaneously, it supports wide-ranging scenarios—from compliance documentation reviews to cross-departmental performance summaries.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Privacy, Data Residency, and Compliance&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Enterprise data protection is a central design element of company knowledge. ChatGPT processes data in line with OpenAI’s enterprise-grade security model, ensuring that no connected app data leaves the secure boundary of the organization’s authorized environment.&lt;/p&gt;&lt;p&gt;Data residency policies vary by connector. Certain integrations, such as Slack, support region-specific data storage, while others—like Google Drive and SharePoint—are available for U.S.-based customers with or without at-rest data residency. Organizations with regional compliance obligations can review connector-specific security documentation for details.&lt;/p&gt;&lt;p&gt;No geo restrictions apply to company knowledge, making it suitable for multinational organizations operating across multiple jurisdictions.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Limitations and Future Enhancements&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At present, users must manually enable company knowledge in each new ChatGPT conversation. &lt;/p&gt;&lt;p&gt;OpenAI is developing a unified interface that will automatically integrate company knowledge with other ChatGPT tools—such as browsing and chart generation—so that users won’t need to toggle between modes.&lt;/p&gt;&lt;p&gt;When enabled, company knowledge temporarily disables web browsing and visual output generation, though users can switch modes within the same conversation to re-enable those features.&lt;/p&gt;&lt;p&gt;OpenAI also continues to expand the network of supported tools. Recent updates have added connectors for Asana, GitLab Issues, and ClickUp, and OpenAI plans to support future MCP (Model Context Protocol) connectors to enable custom, developer-built integrations.&lt;/p&gt;&lt;p&gt;Several important details about &lt;i&gt;company knowledge&lt;/i&gt; remain unclear based on OpenAI’s published materials. It’s not yet known whether the system can detect and exclude information labeled as confidential, whether organizations can opt in or out of data training separately for this feature, or if users will eventually be able to select which model powers it. &lt;/p&gt;&lt;p&gt;OpenAI has also not said whether this version of GPT-5 is new or specific to the feature, or what service-level guarantees exist to ensure accuracy and prevent hallucinations in company-specific responses. VentureBeat has emailed OpenAI spokespeople with these and related questions and is awaiting a response, which we will publish if and when we receive it.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Availability and Getting Started&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Company knowledge is now available to all ChatGPT Business, Enterprise, and Edu users. Organizations can begin by enabling the feature under the ChatGPT message composer and connecting approved work apps.&lt;/p&gt;&lt;p&gt;For enterprise rollouts, OpenAI recommends a phased deployment: first enabling core connectors (such as Google Drive and Slack), configuring RBAC and SSO, then expanding to specialized systems once data access policies are verified.&lt;/p&gt;&lt;p&gt;Procurement and security leaders evaluating the feature should note that company knowledge is covered under existing ChatGPT Enterprise terms and uses the same encryption, compliance, and service-level guarantees.&lt;/p&gt;&lt;p&gt;With company knowledge, OpenAI aims to make ChatGPT not just a conversational assistant but an intelligent interface to enterprise data—delivering secure, context-aware insights that help technical and business leaders act with confidence.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Is the Google Search for internal enterprise knowledge finally here...but from &lt;i&gt;OpenAI&lt;/i&gt;? It certainly seems that way. &lt;/p&gt;&lt;p&gt;Today, OpenAI has &lt;a href="https://openai.com/index/introducing-company-knowledge/"&gt;launched company knowledge in ChatGPT&lt;/a&gt;, a major new capability for subscribers to ChatGPT&amp;#x27;s paid Business, Enterprise, and Edu plans that lets them call up their company&amp;#x27;s data directly from third-party workplace apps including Slack, SharePoint, Google Drive, Gmail, GitHub, HubSpot and combine it in ChatGPT outputs to them. &lt;/p&gt;&lt;p&gt;As OpenAI&amp;#x27;s CEO of Applications Fidji Simo put it in a post on the social network X: &amp;quot;it brings all the context from your apps (Slack, Google Drive, GitHub, etc) together in ChatGPT so you can get answers that are specific to your business.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Intriguingly, OpenAI&amp;#x27;s blog post on the feature states that is &amp;quot;powered by a version of GPT‑5 that’s trained to look across multiple sources to give more comprehensive and accurate answers,&amp;quot; which sounds to me like a new fine-tuned version of the &lt;a href="https://venturebeat.com/ai/openai-launches-gpt-5-not-agi-but-capable-of-generating-software-on-demand"&gt;model family the company released back in August&lt;/a&gt;, though there are no additional details on how it was trained. &lt;/p&gt;&lt;p&gt;Nonetheless, company knowledge in ChatGPT is rolling out globally and is designed to make ChatGPT a central point of access for verified organizational information, supported by secure integrations and enterprise-grade compliance controls, and give employees way faster access to their company&amp;#x27;s information while working.&lt;/p&gt;&lt;p&gt;Now, instead of toggling over to Slack to find the assignment you were given and instructions, or tabbing over to Google Drive and opening up specific files to find the names and numbers you need to call, ChatGPT can deliver all that type of information directly into your chat session — if your company enables the proper connections.&lt;/p&gt;&lt;p&gt;As OpenAI Chief Operating Officer Brad Lightcap &lt;a href="https://x.com/bradlightcap/status/1981454865454027007"&gt;wrote in a post on the social network X&lt;/a&gt;: &amp;quot;company knowledge has changed how i use chatgpt at work more than anything we have built so far - let us know what you think!&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;It builds upon the third-party app connectors &lt;a href="https://venturebeat.com/ai/openai-adds-new-chatgpt-third-party-tool-connectors-to-dropbox-ms-teams-as-altman-clarifies-gpt-5-prioritization"&gt;unveiled back in August 2025&lt;/a&gt;, though those were only for individual users on the ChatGPT Plus plans.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Connecting ChatGPT to Workplace Systems&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Enterprise teams often face the challenge of fragmented data across various internal tools—email, chat, file storage, project management, and customer platforms. &lt;/p&gt;&lt;p&gt;Company knowledge bridges those silos by enabling ChatGPT to connect to approved systems like, and other supported apps through enterprise-managed connectors.&lt;/p&gt;&lt;p&gt;Each response generated with company knowledge includes citations and direct links to the original sources, allowing teams to verify where specific details originated. This transparency helps organizations maintain data trustworthiness while increasing productivity.&lt;/p&gt;&lt;p&gt;OpenAI confirms that company knowledge uses a version of GPT-5 optimized for multi-source reasoning and cross-system synthesis, providing detailed, contextually accurate results even across disparate sources.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Built for Enterprise Control and Security&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Company knowledge was designed from the ground up for enterprise governance and compliance. It respects existing permissions within connected apps — ChatGPT can only access what a user is already authorized to view— and never trains on company data by default.&lt;/p&gt;&lt;p&gt;Security features include industry-standard encryption, support for SSO and SCIM for account provisioning, and IP allowlisting to restrict access to approved corporate networks. &lt;/p&gt;&lt;p&gt;Enterprise administrators can also define role-based access control (RBAC) policies and manage permissions at a group or department level.&lt;/p&gt;&lt;p&gt;OpenAI’s Enterprise Compliance API provides a full audit trail, allowing administrators to review conversation logs for reporting and regulatory purposes. &lt;/p&gt;&lt;p&gt;This capability helps enterprises meet internal governance standards and industry-specific requirements such as SOC 2 and ISO 27001 compliance.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Admin Configuration and Connector Management&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For enterprise deployment, administrators must enable company knowledge and its connectors within the ChatGPT workspace. Once connectors are active, users can authenticate their own accounts for each work app they need to access.&lt;/p&gt;&lt;p&gt;In Enterprise and Edu plans, connectors are off by default and require explicit admin approval before employees can use them. Admins can selectively enable connectors, manage access by role, and require SSO-based authentication for enhanced control.&lt;/p&gt;&lt;p&gt;Business plan users, by contrast, have connectors enabled automatically if available in their workspace. Admins can still oversee which connectors are approved, ensuring alignment with internal IT and data policies.&lt;/p&gt;&lt;p&gt;Company knowledge becomes available to any user with at least one active connector, and admins can configure group-level permissions for different teams — such as restricting GitHub access to engineering while enabling Google Drive or HubSpot for marketing and sales.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;How Company Knowledge Works in Practice&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Activating company knowledge is straightforward. Users can start a new or existing conversation in ChatGPT and select “Company knowledge” under the message composer or from the tools menu. &lt;/p&gt;&lt;p&gt;After authenticating their connected apps, they can ask questions as usual—such as “Summarize this account’s latest feedback and risks” or “Compile a Q4 performance summary from project trackers.”&lt;/p&gt;&lt;p&gt;ChatGPT searches across the connected tools, retrieves relevant context, and produces an answer with full citations and source links. &lt;/p&gt;&lt;p&gt;The system can combine data across apps — for instance, blending Slack updates, Google Docs notes, and HubSpot CRM records — to create an integrated view of a project, client, or initiative.&lt;/p&gt;&lt;p&gt;When company knowledge is not selected, ChatGPT may still use connectors in a limited capacity as part of the default experience, but responses will not include detailed citations or multi-source synthesis.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Advanced Use Cases for Enterprise Teams&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For development and operations leaders, company knowledge can act as a centralized intelligence layer that surfaces real-time updates and dependencies across complex workflows. ChatGPT can, for example, summarize open GitHub pull requests, highlight unresolved Linear tickets, and cross-reference Slack engineering discussions—all in a single output.&lt;/p&gt;&lt;p&gt;Technical teams can also use it for incident retrospectives or release planning by pulling relevant information from issue trackers, logs, and meeting notes. Procurement or finance leaders can use it to consolidate purchase requests or budget updates across shared drives and internal communications.&lt;/p&gt;&lt;p&gt;Because the model can reference structured and unstructured data simultaneously, it supports wide-ranging scenarios—from compliance documentation reviews to cross-departmental performance summaries.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Privacy, Data Residency, and Compliance&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Enterprise data protection is a central design element of company knowledge. ChatGPT processes data in line with OpenAI’s enterprise-grade security model, ensuring that no connected app data leaves the secure boundary of the organization’s authorized environment.&lt;/p&gt;&lt;p&gt;Data residency policies vary by connector. Certain integrations, such as Slack, support region-specific data storage, while others—like Google Drive and SharePoint—are available for U.S.-based customers with or without at-rest data residency. Organizations with regional compliance obligations can review connector-specific security documentation for details.&lt;/p&gt;&lt;p&gt;No geo restrictions apply to company knowledge, making it suitable for multinational organizations operating across multiple jurisdictions.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Limitations and Future Enhancements&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At present, users must manually enable company knowledge in each new ChatGPT conversation. &lt;/p&gt;&lt;p&gt;OpenAI is developing a unified interface that will automatically integrate company knowledge with other ChatGPT tools—such as browsing and chart generation—so that users won’t need to toggle between modes.&lt;/p&gt;&lt;p&gt;When enabled, company knowledge temporarily disables web browsing and visual output generation, though users can switch modes within the same conversation to re-enable those features.&lt;/p&gt;&lt;p&gt;OpenAI also continues to expand the network of supported tools. Recent updates have added connectors for Asana, GitLab Issues, and ClickUp, and OpenAI plans to support future MCP (Model Context Protocol) connectors to enable custom, developer-built integrations.&lt;/p&gt;&lt;p&gt;Several important details about &lt;i&gt;company knowledge&lt;/i&gt; remain unclear based on OpenAI’s published materials. It’s not yet known whether the system can detect and exclude information labeled as confidential, whether organizations can opt in or out of data training separately for this feature, or if users will eventually be able to select which model powers it. &lt;/p&gt;&lt;p&gt;OpenAI has also not said whether this version of GPT-5 is new or specific to the feature, or what service-level guarantees exist to ensure accuracy and prevent hallucinations in company-specific responses. VentureBeat has emailed OpenAI spokespeople with these and related questions and is awaiting a response, which we will publish if and when we receive it.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Availability and Getting Started&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Company knowledge is now available to all ChatGPT Business, Enterprise, and Edu users. Organizations can begin by enabling the feature under the ChatGPT message composer and connecting approved work apps.&lt;/p&gt;&lt;p&gt;For enterprise rollouts, OpenAI recommends a phased deployment: first enabling core connectors (such as Google Drive and Slack), configuring RBAC and SSO, then expanding to specialized systems once data access policies are verified.&lt;/p&gt;&lt;p&gt;Procurement and security leaders evaluating the feature should note that company knowledge is covered under existing ChatGPT Enterprise terms and uses the same encryption, compliance, and service-level guarantees.&lt;/p&gt;&lt;p&gt;With company knowledge, OpenAI aims to make ChatGPT not just a conversational assistant but an intelligent interface to enterprise data—delivering secure, context-aware insights that help technical and business leaders act with confidence.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-launches-company-knowledge-in-chatgpt-letting-you-access-your-firms</guid><pubDate>Thu, 23 Oct 2025 22:19:00 +0000</pubDate></item><item><title>[NEW] With an Intel recovery underway, all eyes turn to its foundry business (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/23/with-an-intel-recovery-underway-all-eyes-turn-to-its-foundry-business/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2228936694.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Intel’s third-quarter earnings beat Wall Street expectations Thursday, results buoyed by a bump in revenue combined with larger cuts, and multiple, sizable investments over the last two months as CEO Lip-Bu Tan looks to turn around the struggling semiconductor giant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intel’s revenue results and its $4.1 billion in net income provides a far rosier view than its string of quarterly losses. But the company’s recovery story deserves several chapters dedicated to cost cutting via layoffs and other reductions as well as a series of high-profile investments from Softbank, Nvidia, and the U.S. government.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Intel added $20 billion to its balance sheet during the third quarter, the company announced on its third-quarter earnings presentation on Thursday, sending its stock soaring. This growth was largely due to three sizable investments in the company over the last three months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In August, SoftBank invested $2 billion. A few days later, the U.S. Government took an unprecedented 10% equity stake in Intel. The company has received $5.7 billion of the planned $8.9 billion from the U.S. Government thus far. Nvidia also bought a $5 billion stake in Intel in September as part of a broader deal to develop chips together over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The actions we took to strengthen the balance sheet give us greater operational flexibility and position us well to continue to execute our strategy with confidence,” Tan said on the company’s earnings call. “In particular, I’m honored by the trust and confidence President Trump and Secretary [Howard] Lutnick have placed in me. Their support highlights Intel’s strategic role as the only U.S.-based semiconductor company with leading edge logic, [research and development] and manufacturing.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also received $5.2 billion from closing the sale of its ownership stake of Altera, a hardware company it had owned since 2015, on September 12. It also sold its stake in Mobileye, an autonomous driving tech company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intel grew its quarterly revenue by $800 million in the third quarter to $13.7 billion, compared to $12.9 billion. Intel had net income of $4.1 billion in the third quarter, a steep reversal from the $16.6 billion loss it reported in the same year-ago period. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-the-foundry-biz"&gt;The foundry biz &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the strong quarter, there weren’t many details on what will happen next with Intel’s foundry business, which makes custom chips for customers. The business has floundered from the start and has been a focus of Tan, who initiated significant layoffs in its foundry business this summer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The business appears to be a priority of the Trump administration; a key condition of the government’s investment in Intel includes language that it will penalize Intel if it divested from its foundry business over the next five years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wall Street is keeping a close eye on foundry for signs of the company’s long-term growth. Intel analysts told TechCrunch in August that the company did not need cash to turn itself around but rather a strategy to get its foundry business on track.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tan said that Intel thinks its foundry business is “uniquely positioned” to capitalize on the swelling demand for chips but was light on the details — beyond saying that the company is actively engaging with potential foundry customers — and added that the growth of the foundry business would remain disciplined.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Building a world-class foundry is a long-term effort founded on trust,” Tan said. “As a foundry, we need to ensure that our process can be easily used by a variety of customers, each with their unique way of building their own products. We must learn to delight our customers as they count on us to build wafers, to meet all their needs for powerful performance, yield, cost, and schedule.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2228936694.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Intel’s third-quarter earnings beat Wall Street expectations Thursday, results buoyed by a bump in revenue combined with larger cuts, and multiple, sizable investments over the last two months as CEO Lip-Bu Tan looks to turn around the struggling semiconductor giant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intel’s revenue results and its $4.1 billion in net income provides a far rosier view than its string of quarterly losses. But the company’s recovery story deserves several chapters dedicated to cost cutting via layoffs and other reductions as well as a series of high-profile investments from Softbank, Nvidia, and the U.S. government.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Intel added $20 billion to its balance sheet during the third quarter, the company announced on its third-quarter earnings presentation on Thursday, sending its stock soaring. This growth was largely due to three sizable investments in the company over the last three months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In August, SoftBank invested $2 billion. A few days later, the U.S. Government took an unprecedented 10% equity stake in Intel. The company has received $5.7 billion of the planned $8.9 billion from the U.S. Government thus far. Nvidia also bought a $5 billion stake in Intel in September as part of a broader deal to develop chips together over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The actions we took to strengthen the balance sheet give us greater operational flexibility and position us well to continue to execute our strategy with confidence,” Tan said on the company’s earnings call. “In particular, I’m honored by the trust and confidence President Trump and Secretary [Howard] Lutnick have placed in me. Their support highlights Intel’s strategic role as the only U.S.-based semiconductor company with leading edge logic, [research and development] and manufacturing.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also received $5.2 billion from closing the sale of its ownership stake of Altera, a hardware company it had owned since 2015, on September 12. It also sold its stake in Mobileye, an autonomous driving tech company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intel grew its quarterly revenue by $800 million in the third quarter to $13.7 billion, compared to $12.9 billion. Intel had net income of $4.1 billion in the third quarter, a steep reversal from the $16.6 billion loss it reported in the same year-ago period. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-the-foundry-biz"&gt;The foundry biz &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the strong quarter, there weren’t many details on what will happen next with Intel’s foundry business, which makes custom chips for customers. The business has floundered from the start and has been a focus of Tan, who initiated significant layoffs in its foundry business this summer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The business appears to be a priority of the Trump administration; a key condition of the government’s investment in Intel includes language that it will penalize Intel if it divested from its foundry business over the next five years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wall Street is keeping a close eye on foundry for signs of the company’s long-term growth. Intel analysts told TechCrunch in August that the company did not need cash to turn itself around but rather a strategy to get its foundry business on track.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tan said that Intel thinks its foundry business is “uniquely positioned” to capitalize on the swelling demand for chips but was light on the details — beyond saying that the company is actively engaging with potential foundry customers — and added that the growth of the foundry business would remain disciplined.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Building a world-class foundry is a long-term effort founded on trust,” Tan said. “As a foundry, we need to ensure that our process can be easily used by a variety of customers, each with their unique way of building their own products. We must learn to delight our customers as they count on us to build wafers, to meet all their needs for powerful performance, yield, cost, and schedule.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/23/with-an-intel-recovery-underway-all-eyes-turn-to-its-foundry-business/</guid><pubDate>Fri, 24 Oct 2025 00:09:29 +0000</pubDate></item></channel></rss>