<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 11 Nov 2025 06:33:39 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>Chronosphere takes on Datadog with AI that explains itself, not just outages (AI | VentureBeat)</title><link>https://venturebeat.com/ai/chronosphere-takes-on-datadog-with-ai-that-explains-itself-not-just-outages</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://chronosphere.io/"&gt;&lt;u&gt;Chronosphere&lt;/u&gt;&lt;/a&gt;, a New York-based observability startup &lt;a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"&gt;&lt;u&gt;valued at $1.6 billion&lt;/u&gt;&lt;/a&gt;, announced Monday it will launch &lt;a href="https://chronosphere.io/ai-guided-troubleshooting/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt; capabilities designed to help engineers diagnose and fix production software failures — a problem that has intensified as artificial intelligence tools accelerate code creation while making systems harder to debug.&lt;/p&gt;&lt;p&gt;The new features combine AI-driven analysis with what Chronosphere calls a &lt;a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"&gt;&lt;u&gt;Temporal Knowledge Graph&lt;/u&gt;&lt;/a&gt;, a continuously updated map of an organization&amp;#x27;s services, infrastructure dependencies, and system changes over time. The technology aims to address a mounting challenge in enterprise software: developers are writing code faster than ever with AI assistance, but troubleshooting remains largely manual, creating bottlenecks when applications fail.&lt;/p&gt;&lt;p&gt;&amp;quot;For AI to be effective in observability, it needs more than pattern recognition and summarization,&amp;quot; said Martin Mao, Chronosphere&amp;#x27;s CEO and co-founder, in an exclusive interview with VentureBeat. &amp;quot;Chronosphere has spent years building the data foundation and analytical depth needed for AI to actually help engineers. With our Temporal Knowledge Graph and advanced analytics capabilities, we&amp;#x27;re giving AI the understanding it needs to make observability truly intelligent — and giving engineers the confidence to trust its guidance.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement comes as the observability market — software that monitors complex cloud applications— faces mounting pressure to justify escalating costs. Enterprise log data volumes have grown &lt;a href="https://chronosphere.io/learn/observability-log-data-trends/"&gt;&lt;u&gt;250% year-over-year&lt;/u&gt;&lt;/a&gt;, according to Chronosphere&amp;#x27;s own research, while a study from MIT and the University of Pennsylvania found that generative &lt;a href="https://economics.mit.edu/sites/default/files/inline-files/draft_copilot_experiments.pdf"&gt;&lt;u&gt;AI has spurred a 13.5% increase in weekly code commits&lt;/u&gt;&lt;/a&gt;, signifying faster development velocity but also greater system complexity.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI writes code 13% faster, but debugging stays stubbornly manual&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite advances in automated code generation, debugging production failures remains stubbornly manual. When a major e-commerce site slows during checkout or a banking app fails to process transactions, engineers must sift through millions of data points — server logs, application traces, infrastructure metrics, recent code deployments — to identify root causes.&lt;/p&gt;&lt;p&gt;Chronosphere&amp;#x27;s answer is what it calls &lt;a href="https://chronosphere.io/ai-guided-troubleshooting/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt;, built on four core capabilities: automated &amp;quot;Suggestions&amp;quot; that propose investigation paths backed by data; the Temporal Knowledge Graph that maps system relationships and changes; Investigation Notebooks that document each troubleshooting step for future reference; and natural language query building.&lt;/p&gt;&lt;p&gt;Mao explained the &lt;a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"&gt;&lt;u&gt;Temporal Knowledge Graph&lt;/u&gt;&lt;/a&gt; in practical terms: &amp;quot;It&amp;#x27;s a living, time-aware model of your system. It stitches together telemetry—metrics, traces, logs—infrastructure context, change events like deploys and feature flags, and even human input like notes and runbooks into a single, queryable map that updates as your system evolves.&amp;quot;&lt;/p&gt;&lt;p&gt;This differs fundamentally from the service dependency maps offered by competitors like &lt;a href="https://www.datadoghq.com/dg/monitor/free-trial/?utm_source=google&amp;amp;utm_medium=paid-search&amp;amp;utm_campaign=dg-brand-ww&amp;amp;utm_keyword=datadog&amp;amp;utm_matchtype=b&amp;amp;igaag=95325237782&amp;amp;igaat=&amp;amp;igacm=9551169254&amp;amp;igacr=673270769690&amp;amp;igakw=datadog&amp;amp;igamt=b&amp;amp;igant=g&amp;amp;utm_campaignid=9551169254&amp;amp;utm_adgroupid=95325237782&amp;amp;gad_source=1&amp;amp;gad_campaignid=9551169254&amp;amp;gbraid=0AAAAADFY9NlOpf6xdtUzDLzD2BUR67UTl&amp;amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6T3Vk9lkeno-VLHlyRmDR0PFF8gTUGxe72EBr8QGbSpTNY5qtp63eRoC25kQAvD_BwE"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.dynatrace.com/"&gt;&lt;u&gt;Dynatrace&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.splunk.com/"&gt;&lt;u&gt;Splunk&lt;/u&gt;&lt;/a&gt;, Mao argued. &amp;quot;It adds time, not just topology,&amp;quot; he said. &amp;quot;It tracks how services and dependencies change over time and connects those changes to incidents—what changed and why. Many tools rely on standardized integrations; our graph goes a step further to normalize custom, non-standard telemetry so application-specific signals aren&amp;#x27;t a blind spot.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Chronosphere shows its work instead of making automatic decisions&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Unlike purely automated systems, &lt;a href="https://chronosphere.io/"&gt;&lt;u&gt;Chronosphere&lt;/u&gt;&lt;/a&gt; designed its AI features to keep engineers in the driver&amp;#x27;s seat—a deliberate choice meant to address what Mao calls the &amp;quot;confident-but-wrong guidance&amp;quot; problem plaguing early AI observability tools.&lt;/p&gt;&lt;p&gt;&amp;quot;&amp;#x27;Keeping engineers in control&amp;#x27; means the AI shows its work, proposes next steps, and lets engineers verify or override — never auto-deciding behind the scenes,&amp;quot; Mao explained. &amp;quot;Every Suggestion includes the evidence—timing, dependencies, error patterns — and a &amp;#x27;Why was this suggested?&amp;#x27; view, so they can inspect what was checked and ruled out before acting.&amp;quot;&lt;/p&gt;&lt;p&gt;He walked through a concrete example: &amp;quot;An SLO [service level objective] alert fires on Checkout. Chronosphere immediately surfaces a ranked Suggestion: errors appear to have started in the dependent Payment service. An engineer can click Investigate to see the charts and reasoning and, if it holds up, choose to dig deeper. As they steer into Payment, the system adapts with new Suggestions scoped to that service—all from one view, no tab-hopping.&amp;quot;&lt;/p&gt;&lt;p&gt;In this scenario, the engineer asks &amp;quot;what changed?&amp;quot; and the system pulls in change events. &amp;quot;Our Notebook capability makes the causal chain plain: a feature-flag update preceded pod memory exhaustion in Payment; Checkout&amp;#x27;s spike is a downstream symptom,&amp;quot; Mao said. &amp;quot;They can decide to roll back the flag. That whole path — suggestions followed, evidence viewed, conclusions—is captured automatically in an Investigation Notebook, and the outcome feeds the Temporal Knowledge Graph so similar future incidents are faster to resolve.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How a $1.6 billion startup takes on Datadog, Dynatrace, and Splunk&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere enters an increasingly crowded field. &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt;, the publicly traded observability leader valued at over $40 billion, has introduced its own AI-powered troubleshooting features. So have Dynatrace and Splunk. All three offer comprehensive &amp;quot;all-in-one&amp;quot; platforms that promise single-pane-of-glass visibility.&lt;/p&gt;&lt;p&gt;Mao distinguished Chronosphere&amp;#x27;s approach on technical grounds. &amp;quot;Early &amp;#x27;AI for observability&amp;#x27; leaned heavily on pattern-spotting and summarization, which tends to break down during real incidents,&amp;quot; he said. &amp;quot;These approaches often stop at correlating anomalies or producing fluent explanations without the deeper analysis and causal reasoning observability leaders need. They can feel impressive in demos but disappoint in production—they summarize signals rather than explain cause and effect.&amp;quot;&lt;/p&gt;&lt;p&gt;A specific technical gap, he argued, involves custom application telemetry. &amp;quot;Most platforms reason over standardized integrations—Kubernetes, common cloud services, popular databases—ignoring the most telling clues that live in custom app telemetry,&amp;quot; Mao said. &amp;quot;With an incomplete picture, large language models will &amp;#x27;fill in the gaps,&amp;#x27; producing confident-but-wrong guidance that sends teams down dead ends.&amp;quot;&lt;/p&gt;&lt;p&gt;Chronosphere&amp;#x27;s competitive positioning received validation in July when Gartner named it a Leader in the &lt;a href="https://www.gartner.com/en/documents/6688834"&gt;&lt;u&gt;2025 Magic Quadrant for Observability Platforms&lt;/u&gt;&lt;/a&gt; for the second consecutive year. The firm was recognized based on both &amp;quot;Completeness of Vision&amp;quot; and &amp;quot;Ability to Execute.&amp;quot; In December 2024, Chronosphere also tied for the highest overall rating among recognized vendors in Gartner Peer Insights&amp;#x27; &amp;quot;Voice of the Customer&amp;quot; report, scoring 4.7 out of 5 based on 70 reviews.&lt;/p&gt;&lt;p&gt;Yet the company faces intensifying competition for high-profile customers. UBS analysts noted in July that OpenAI now runs both Datadog and Chronosphere side-by-side to monitor GPU workloads, suggesting the AI leader is evaluating alternatives. While UBS maintained its buy rating on Datadog, the analysts warned that growing Chronosphere usage could pressure Datadog&amp;#x27;s pricing power.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the 84% cost reduction claims—and what CIOs should actually measure&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond technical capabilities, Chronosphere has built its market position on cost control — a critical factor as observability spending spirals. The company claims its platform reduces data volumes and associated costs by 84% on average while cutting critical incidents by up to 75%.&lt;/p&gt;&lt;p&gt;When pressed for specific customer examples with real numbers, Mao pointed to several case studies. &amp;quot;Robinhood has seen a 5x improvement in reliability and a 4x improvement in Mean Time to Detection,&amp;quot; he said. &amp;quot;DoorDash used Chronosphere to improve governance and standardize monitoring practices. Astronomer achieved over 85% cost reduction by shaping data on ingest, and Affirm scaled their load 10x during a Black Friday event with no issues, highlighting the platform&amp;#x27;s reliability under extreme conditions.&amp;quot;&lt;/p&gt;&lt;p&gt;The cost argument matters because, as &lt;a href="https://chronosphere.io/news/chronosphere-logs-raises-bar-in-observability/"&gt;&lt;u&gt;Paul Nashawaty&lt;/u&gt;&lt;/a&gt;, principal analyst at CUBE Research, noted when Chronosphere launched its Logs 2.0 product in June: &amp;quot;Organizations are drowning in telemetry data, with over 70% of observability spend going toward storing logs that are never queried.&amp;quot;&lt;/p&gt;&lt;p&gt;For CIOs fatigued by &amp;quot;AI-powered&amp;quot; announcements, Mao acknowledged skepticism is warranted. &amp;quot;The way to cut through it is to test whether the AI shortens incidents, reduces toil, and builds reusable knowledge in your own environment, not in a demo,&amp;quot; he advised. He recommended CIOs evaluate three factors: transparency and control (does the system show its reasoning?), coverage of custom telemetry (can it handle non-standardized data?), and manual toil avoided (how many ad-hoc queries and tool-switches are eliminated?).&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Chronosphere partners with five vendors instead of building everything itself&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Alongside the AI troubleshooting announcement, Chronosphere revealed a new &lt;a href="https://chronosphere.io/partners/"&gt;&lt;u&gt;Partner Program&lt;/u&gt;&lt;/a&gt; integrating five specialized vendors to fill gaps in its platform: Arize for large language model monitoring, Embrace for real user monitoring, Polar Signals for continuous profiling, Checkly for synthetic monitoring, and Rootly for incident management.&lt;/p&gt;&lt;p&gt;The strategy represents a deliberate bet against the all-in-one platforms dominating the market. &amp;quot;While an all-in-one platform may be sufficient for smaller organizations, global enterprises demand best-in-class depth across each domain,&amp;quot; Mao said. &amp;quot;This is what drove us to build our Partner Program and invest in seamless integrations with leading providers—so our customers can operate with confidence and clarity at every layer of observability.&amp;quot;&lt;/p&gt;&lt;p&gt;Noah Smolen, head of partnerships at Arize, said the collaboration addresses a specific enterprise need. &amp;quot;With a wide array of Fortune 500 customers, we understand the high bar needed to ensure AI agent systems are ready to deploy and stay incident-free, especially given the pace of AI adoption in the enterprise,&amp;quot; Smolen said. &amp;quot;Our partnership with Chronosphere comes at a time when an integrated purpose-built cloud-native and AI-observability suite solves a huge pain point for forward-thinking C-suite leaders who demand the very best across their entire observability stack.&amp;quot;&lt;/p&gt;&lt;p&gt;Similarly, JJ Tang, CEO and founder of Rootly, emphasized the incident resolution benefits. &amp;quot;Incidents hinder innovation and revenue, and the challenge lies in sifting through vast amounts of observability data, mobilizing teams, and resolving issues quickly,&amp;quot; Tang said. &amp;quot;Integrating Chronosphere with Rootly allows engineers to collaborate with context and resolve issues faster within their existing communication channels, drastically reducing time to resolution and ultimately improving reliability—78% plus decreases in repeat Sev0 and Sev1 incidents.&amp;quot;&lt;/p&gt;&lt;p&gt;When asked how total costs compare when customers use multiple partner contracts versus a single platform, Mao acknowledged the current complexity. &amp;quot;At present, mutual customers typically maintain separate contracts unless they engage through a services partner or system integrator,&amp;quot; he said. However, he argued the economics still favor the composable approach: &amp;quot;Our combined technologies deliver exceptional value—in most circumstances at just a fraction of the price of a single-platform solution. Beyond the savings, customers gain a richer, more unified observability experience that unlocks deeper insights and greater efficiency, especially for large-scale environments.&amp;quot;&lt;/p&gt;&lt;p&gt;The company plans to streamline this over time. &amp;quot;As the ISV program matures, we&amp;#x27;re focused on delivering a more streamlined experience by transitioning to a single, unified contract that simplifies procurement and accelerates time to value,&amp;quot; Mao said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How two Uber engineers turned Halloween outages into a billion-dollar startup&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere&amp;#x27;s origins trace to 2019, when Mao and co-founder Rob Skillington left Uber after building the ride-hailing giant&amp;#x27;s internal observability platform. At Uber, Mao&amp;#x27;s team had faced a crisis: the company&amp;#x27;s in-house tools would fail on its two busiest nights — Halloween and New Year&amp;#x27;s Eve — cutting off visibility into whether customers could request rides or drivers could locate passengers.&lt;/p&gt;&lt;p&gt;The solution they built at Uber used open-source software and ultimately allowed the company to operate without outages, even during high-volume events. But the broader market insight came at an industry conference in December 2018, when major cloud providers threw their weight behind Kubernetes, Google&amp;#x27;s container orchestration technology.&lt;/p&gt;&lt;p&gt;&amp;quot;This meant that most technology architectures were eventually going to look like Uber&amp;#x27;s,&amp;quot; Mao recalled in an &lt;a href="https://greylock.com/greymatter/chronosphere-is-making-the-observability-platform-built-for-control/"&gt;&lt;u&gt;August 2024 profile by Greylock Partners&lt;/u&gt;&lt;/a&gt;, Chronosphere&amp;#x27;s lead investor. &amp;quot;And that meant every company, not just a few big tech companies and the Walmarts of the world, would have the exact same problem we had solved at Uber.&amp;quot;&lt;/p&gt;&lt;p&gt;Chronosphere has since raised more than &lt;a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"&gt;&lt;u&gt;$343 million in funding&lt;/u&gt;&lt;/a&gt; across multiple rounds led by Greylock, Lux Capital, General Atlantic, Addition, and Founders Fund. The company operates as a remote-first organization with offices in New York, Austin, Boston, San Francisco, and Seattle, employing approximately 299 people according to LinkedIn data.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s customer base includes &lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.zillow.com"&gt;&lt;u&gt;Zillow&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.snap.com/"&gt;&lt;u&gt;Snap&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://robinhood.com/us/en/"&gt;&lt;u&gt;Robinhood&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.affirm.com/"&gt;&lt;u&gt;Affirm&lt;/u&gt;&lt;/a&gt; — predominantly high-growth technology companies operating cloud-native, Kubernetes-based infrastructures at massive scale.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What&amp;#x27;s available now—and what enterprises can expect in 2026&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere&amp;#x27;s &lt;a href="https://chronosphere.io/news/ai-guided-troubleshooting-redefines-observability/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt; capabilities, including Suggestions and Investigation Notebooks, entered limited availability Monday with select customers. The company plans full general availability in 2026. The &lt;a href="https://docs.chronosphere.io/integrate/mcp-server"&gt;&lt;u&gt;Model Context Protocol (MCP) Server&lt;/u&gt;&lt;/a&gt;, which enables engineers to integrate Chronosphere directly into internal AI workflows and query observability data through AI-enabled development environments, is available immediately for all Chronosphere customers.&lt;/p&gt;&lt;p&gt;The phased rollout reflects the company&amp;#x27;s cautious approach to deploying AI in production environments where mistakes carry real costs. By gathering feedback from early adopters before broad release, Chronosphere aims to refine its guidance algorithms and validate that its suggestions genuinely accelerate troubleshooting rather than simply generating impressive demonstrations.&lt;/p&gt;&lt;p&gt;The longer game, however, extends beyond individual product features. Chronosphere&amp;#x27;s dual bet — on transparent AI that shows its reasoning and on a partner ecosystem rather than all-in-one integration — amounts to a fundamental thesis about how enterprise observability will evolve as systems grow more complex.&lt;/p&gt;&lt;p&gt;If that thesis proves correct, the company that solves observability for the AI age won&amp;#x27;t be the one with the most automated black box. It will be the one that earns engineers&amp;#x27; trust by explaining what it knows, admitting what it doesn&amp;#x27;t, and letting humans make the final call. In an industry drowning in data and promised silver bullets, Chronosphere is wagering that showing your work still matters — even when AI is doing the math.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://chronosphere.io/"&gt;&lt;u&gt;Chronosphere&lt;/u&gt;&lt;/a&gt;, a New York-based observability startup &lt;a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"&gt;&lt;u&gt;valued at $1.6 billion&lt;/u&gt;&lt;/a&gt;, announced Monday it will launch &lt;a href="https://chronosphere.io/ai-guided-troubleshooting/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt; capabilities designed to help engineers diagnose and fix production software failures — a problem that has intensified as artificial intelligence tools accelerate code creation while making systems harder to debug.&lt;/p&gt;&lt;p&gt;The new features combine AI-driven analysis with what Chronosphere calls a &lt;a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"&gt;&lt;u&gt;Temporal Knowledge Graph&lt;/u&gt;&lt;/a&gt;, a continuously updated map of an organization&amp;#x27;s services, infrastructure dependencies, and system changes over time. The technology aims to address a mounting challenge in enterprise software: developers are writing code faster than ever with AI assistance, but troubleshooting remains largely manual, creating bottlenecks when applications fail.&lt;/p&gt;&lt;p&gt;&amp;quot;For AI to be effective in observability, it needs more than pattern recognition and summarization,&amp;quot; said Martin Mao, Chronosphere&amp;#x27;s CEO and co-founder, in an exclusive interview with VentureBeat. &amp;quot;Chronosphere has spent years building the data foundation and analytical depth needed for AI to actually help engineers. With our Temporal Knowledge Graph and advanced analytics capabilities, we&amp;#x27;re giving AI the understanding it needs to make observability truly intelligent — and giving engineers the confidence to trust its guidance.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement comes as the observability market — software that monitors complex cloud applications— faces mounting pressure to justify escalating costs. Enterprise log data volumes have grown &lt;a href="https://chronosphere.io/learn/observability-log-data-trends/"&gt;&lt;u&gt;250% year-over-year&lt;/u&gt;&lt;/a&gt;, according to Chronosphere&amp;#x27;s own research, while a study from MIT and the University of Pennsylvania found that generative &lt;a href="https://economics.mit.edu/sites/default/files/inline-files/draft_copilot_experiments.pdf"&gt;&lt;u&gt;AI has spurred a 13.5% increase in weekly code commits&lt;/u&gt;&lt;/a&gt;, signifying faster development velocity but also greater system complexity.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI writes code 13% faster, but debugging stays stubbornly manual&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite advances in automated code generation, debugging production failures remains stubbornly manual. When a major e-commerce site slows during checkout or a banking app fails to process transactions, engineers must sift through millions of data points — server logs, application traces, infrastructure metrics, recent code deployments — to identify root causes.&lt;/p&gt;&lt;p&gt;Chronosphere&amp;#x27;s answer is what it calls &lt;a href="https://chronosphere.io/ai-guided-troubleshooting/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt;, built on four core capabilities: automated &amp;quot;Suggestions&amp;quot; that propose investigation paths backed by data; the Temporal Knowledge Graph that maps system relationships and changes; Investigation Notebooks that document each troubleshooting step for future reference; and natural language query building.&lt;/p&gt;&lt;p&gt;Mao explained the &lt;a href="https://chronosphere.io/learn/how-chronosphere-built-a-deployment-system-with-temporal/"&gt;&lt;u&gt;Temporal Knowledge Graph&lt;/u&gt;&lt;/a&gt; in practical terms: &amp;quot;It&amp;#x27;s a living, time-aware model of your system. It stitches together telemetry—metrics, traces, logs—infrastructure context, change events like deploys and feature flags, and even human input like notes and runbooks into a single, queryable map that updates as your system evolves.&amp;quot;&lt;/p&gt;&lt;p&gt;This differs fundamentally from the service dependency maps offered by competitors like &lt;a href="https://www.datadoghq.com/dg/monitor/free-trial/?utm_source=google&amp;amp;utm_medium=paid-search&amp;amp;utm_campaign=dg-brand-ww&amp;amp;utm_keyword=datadog&amp;amp;utm_matchtype=b&amp;amp;igaag=95325237782&amp;amp;igaat=&amp;amp;igacm=9551169254&amp;amp;igacr=673270769690&amp;amp;igakw=datadog&amp;amp;igamt=b&amp;amp;igant=g&amp;amp;utm_campaignid=9551169254&amp;amp;utm_adgroupid=95325237782&amp;amp;gad_source=1&amp;amp;gad_campaignid=9551169254&amp;amp;gbraid=0AAAAADFY9NlOpf6xdtUzDLzD2BUR67UTl&amp;amp;gclid=CjwKCAiAt8bIBhBpEiwAzH1w6T3Vk9lkeno-VLHlyRmDR0PFF8gTUGxe72EBr8QGbSpTNY5qtp63eRoC25kQAvD_BwE"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.dynatrace.com/"&gt;&lt;u&gt;Dynatrace&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.splunk.com/"&gt;&lt;u&gt;Splunk&lt;/u&gt;&lt;/a&gt;, Mao argued. &amp;quot;It adds time, not just topology,&amp;quot; he said. &amp;quot;It tracks how services and dependencies change over time and connects those changes to incidents—what changed and why. Many tools rely on standardized integrations; our graph goes a step further to normalize custom, non-standard telemetry so application-specific signals aren&amp;#x27;t a blind spot.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Chronosphere shows its work instead of making automatic decisions&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Unlike purely automated systems, &lt;a href="https://chronosphere.io/"&gt;&lt;u&gt;Chronosphere&lt;/u&gt;&lt;/a&gt; designed its AI features to keep engineers in the driver&amp;#x27;s seat—a deliberate choice meant to address what Mao calls the &amp;quot;confident-but-wrong guidance&amp;quot; problem plaguing early AI observability tools.&lt;/p&gt;&lt;p&gt;&amp;quot;&amp;#x27;Keeping engineers in control&amp;#x27; means the AI shows its work, proposes next steps, and lets engineers verify or override — never auto-deciding behind the scenes,&amp;quot; Mao explained. &amp;quot;Every Suggestion includes the evidence—timing, dependencies, error patterns — and a &amp;#x27;Why was this suggested?&amp;#x27; view, so they can inspect what was checked and ruled out before acting.&amp;quot;&lt;/p&gt;&lt;p&gt;He walked through a concrete example: &amp;quot;An SLO [service level objective] alert fires on Checkout. Chronosphere immediately surfaces a ranked Suggestion: errors appear to have started in the dependent Payment service. An engineer can click Investigate to see the charts and reasoning and, if it holds up, choose to dig deeper. As they steer into Payment, the system adapts with new Suggestions scoped to that service—all from one view, no tab-hopping.&amp;quot;&lt;/p&gt;&lt;p&gt;In this scenario, the engineer asks &amp;quot;what changed?&amp;quot; and the system pulls in change events. &amp;quot;Our Notebook capability makes the causal chain plain: a feature-flag update preceded pod memory exhaustion in Payment; Checkout&amp;#x27;s spike is a downstream symptom,&amp;quot; Mao said. &amp;quot;They can decide to roll back the flag. That whole path — suggestions followed, evidence viewed, conclusions—is captured automatically in an Investigation Notebook, and the outcome feeds the Temporal Knowledge Graph so similar future incidents are faster to resolve.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How a $1.6 billion startup takes on Datadog, Dynatrace, and Splunk&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere enters an increasingly crowded field. &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt;, the publicly traded observability leader valued at over $40 billion, has introduced its own AI-powered troubleshooting features. So have Dynatrace and Splunk. All three offer comprehensive &amp;quot;all-in-one&amp;quot; platforms that promise single-pane-of-glass visibility.&lt;/p&gt;&lt;p&gt;Mao distinguished Chronosphere&amp;#x27;s approach on technical grounds. &amp;quot;Early &amp;#x27;AI for observability&amp;#x27; leaned heavily on pattern-spotting and summarization, which tends to break down during real incidents,&amp;quot; he said. &amp;quot;These approaches often stop at correlating anomalies or producing fluent explanations without the deeper analysis and causal reasoning observability leaders need. They can feel impressive in demos but disappoint in production—they summarize signals rather than explain cause and effect.&amp;quot;&lt;/p&gt;&lt;p&gt;A specific technical gap, he argued, involves custom application telemetry. &amp;quot;Most platforms reason over standardized integrations—Kubernetes, common cloud services, popular databases—ignoring the most telling clues that live in custom app telemetry,&amp;quot; Mao said. &amp;quot;With an incomplete picture, large language models will &amp;#x27;fill in the gaps,&amp;#x27; producing confident-but-wrong guidance that sends teams down dead ends.&amp;quot;&lt;/p&gt;&lt;p&gt;Chronosphere&amp;#x27;s competitive positioning received validation in July when Gartner named it a Leader in the &lt;a href="https://www.gartner.com/en/documents/6688834"&gt;&lt;u&gt;2025 Magic Quadrant for Observability Platforms&lt;/u&gt;&lt;/a&gt; for the second consecutive year. The firm was recognized based on both &amp;quot;Completeness of Vision&amp;quot; and &amp;quot;Ability to Execute.&amp;quot; In December 2024, Chronosphere also tied for the highest overall rating among recognized vendors in Gartner Peer Insights&amp;#x27; &amp;quot;Voice of the Customer&amp;quot; report, scoring 4.7 out of 5 based on 70 reviews.&lt;/p&gt;&lt;p&gt;Yet the company faces intensifying competition for high-profile customers. UBS analysts noted in July that OpenAI now runs both Datadog and Chronosphere side-by-side to monitor GPU workloads, suggesting the AI leader is evaluating alternatives. While UBS maintained its buy rating on Datadog, the analysts warned that growing Chronosphere usage could pressure Datadog&amp;#x27;s pricing power.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the 84% cost reduction claims—and what CIOs should actually measure&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond technical capabilities, Chronosphere has built its market position on cost control — a critical factor as observability spending spirals. The company claims its platform reduces data volumes and associated costs by 84% on average while cutting critical incidents by up to 75%.&lt;/p&gt;&lt;p&gt;When pressed for specific customer examples with real numbers, Mao pointed to several case studies. &amp;quot;Robinhood has seen a 5x improvement in reliability and a 4x improvement in Mean Time to Detection,&amp;quot; he said. &amp;quot;DoorDash used Chronosphere to improve governance and standardize monitoring practices. Astronomer achieved over 85% cost reduction by shaping data on ingest, and Affirm scaled their load 10x during a Black Friday event with no issues, highlighting the platform&amp;#x27;s reliability under extreme conditions.&amp;quot;&lt;/p&gt;&lt;p&gt;The cost argument matters because, as &lt;a href="https://chronosphere.io/news/chronosphere-logs-raises-bar-in-observability/"&gt;&lt;u&gt;Paul Nashawaty&lt;/u&gt;&lt;/a&gt;, principal analyst at CUBE Research, noted when Chronosphere launched its Logs 2.0 product in June: &amp;quot;Organizations are drowning in telemetry data, with over 70% of observability spend going toward storing logs that are never queried.&amp;quot;&lt;/p&gt;&lt;p&gt;For CIOs fatigued by &amp;quot;AI-powered&amp;quot; announcements, Mao acknowledged skepticism is warranted. &amp;quot;The way to cut through it is to test whether the AI shortens incidents, reduces toil, and builds reusable knowledge in your own environment, not in a demo,&amp;quot; he advised. He recommended CIOs evaluate three factors: transparency and control (does the system show its reasoning?), coverage of custom telemetry (can it handle non-standardized data?), and manual toil avoided (how many ad-hoc queries and tool-switches are eliminated?).&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Chronosphere partners with five vendors instead of building everything itself&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Alongside the AI troubleshooting announcement, Chronosphere revealed a new &lt;a href="https://chronosphere.io/partners/"&gt;&lt;u&gt;Partner Program&lt;/u&gt;&lt;/a&gt; integrating five specialized vendors to fill gaps in its platform: Arize for large language model monitoring, Embrace for real user monitoring, Polar Signals for continuous profiling, Checkly for synthetic monitoring, and Rootly for incident management.&lt;/p&gt;&lt;p&gt;The strategy represents a deliberate bet against the all-in-one platforms dominating the market. &amp;quot;While an all-in-one platform may be sufficient for smaller organizations, global enterprises demand best-in-class depth across each domain,&amp;quot; Mao said. &amp;quot;This is what drove us to build our Partner Program and invest in seamless integrations with leading providers—so our customers can operate with confidence and clarity at every layer of observability.&amp;quot;&lt;/p&gt;&lt;p&gt;Noah Smolen, head of partnerships at Arize, said the collaboration addresses a specific enterprise need. &amp;quot;With a wide array of Fortune 500 customers, we understand the high bar needed to ensure AI agent systems are ready to deploy and stay incident-free, especially given the pace of AI adoption in the enterprise,&amp;quot; Smolen said. &amp;quot;Our partnership with Chronosphere comes at a time when an integrated purpose-built cloud-native and AI-observability suite solves a huge pain point for forward-thinking C-suite leaders who demand the very best across their entire observability stack.&amp;quot;&lt;/p&gt;&lt;p&gt;Similarly, JJ Tang, CEO and founder of Rootly, emphasized the incident resolution benefits. &amp;quot;Incidents hinder innovation and revenue, and the challenge lies in sifting through vast amounts of observability data, mobilizing teams, and resolving issues quickly,&amp;quot; Tang said. &amp;quot;Integrating Chronosphere with Rootly allows engineers to collaborate with context and resolve issues faster within their existing communication channels, drastically reducing time to resolution and ultimately improving reliability—78% plus decreases in repeat Sev0 and Sev1 incidents.&amp;quot;&lt;/p&gt;&lt;p&gt;When asked how total costs compare when customers use multiple partner contracts versus a single platform, Mao acknowledged the current complexity. &amp;quot;At present, mutual customers typically maintain separate contracts unless they engage through a services partner or system integrator,&amp;quot; he said. However, he argued the economics still favor the composable approach: &amp;quot;Our combined technologies deliver exceptional value—in most circumstances at just a fraction of the price of a single-platform solution. Beyond the savings, customers gain a richer, more unified observability experience that unlocks deeper insights and greater efficiency, especially for large-scale environments.&amp;quot;&lt;/p&gt;&lt;p&gt;The company plans to streamline this over time. &amp;quot;As the ISV program matures, we&amp;#x27;re focused on delivering a more streamlined experience by transitioning to a single, unified contract that simplifies procurement and accelerates time to value,&amp;quot; Mao said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How two Uber engineers turned Halloween outages into a billion-dollar startup&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere&amp;#x27;s origins trace to 2019, when Mao and co-founder Rob Skillington left Uber after building the ride-hailing giant&amp;#x27;s internal observability platform. At Uber, Mao&amp;#x27;s team had faced a crisis: the company&amp;#x27;s in-house tools would fail on its two busiest nights — Halloween and New Year&amp;#x27;s Eve — cutting off visibility into whether customers could request rides or drivers could locate passengers.&lt;/p&gt;&lt;p&gt;The solution they built at Uber used open-source software and ultimately allowed the company to operate without outages, even during high-volume events. But the broader market insight came at an industry conference in December 2018, when major cloud providers threw their weight behind Kubernetes, Google&amp;#x27;s container orchestration technology.&lt;/p&gt;&lt;p&gt;&amp;quot;This meant that most technology architectures were eventually going to look like Uber&amp;#x27;s,&amp;quot; Mao recalled in an &lt;a href="https://greylock.com/greymatter/chronosphere-is-making-the-observability-platform-built-for-control/"&gt;&lt;u&gt;August 2024 profile by Greylock Partners&lt;/u&gt;&lt;/a&gt;, Chronosphere&amp;#x27;s lead investor. &amp;quot;And that meant every company, not just a few big tech companies and the Walmarts of the world, would have the exact same problem we had solved at Uber.&amp;quot;&lt;/p&gt;&lt;p&gt;Chronosphere has since raised more than &lt;a href="https://chronosphere.io/news/chronosphere-marks-a-year-of-new-capabilities/"&gt;&lt;u&gt;$343 million in funding&lt;/u&gt;&lt;/a&gt; across multiple rounds led by Greylock, Lux Capital, General Atlantic, Addition, and Founders Fund. The company operates as a remote-first organization with offices in New York, Austin, Boston, San Francisco, and Seattle, employing approximately 299 people according to LinkedIn data.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s customer base includes &lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.zillow.com"&gt;&lt;u&gt;Zillow&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.snap.com/"&gt;&lt;u&gt;Snap&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://robinhood.com/us/en/"&gt;&lt;u&gt;Robinhood&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.affirm.com/"&gt;&lt;u&gt;Affirm&lt;/u&gt;&lt;/a&gt; — predominantly high-growth technology companies operating cloud-native, Kubernetes-based infrastructures at massive scale.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What&amp;#x27;s available now—and what enterprises can expect in 2026&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Chronosphere&amp;#x27;s &lt;a href="https://chronosphere.io/news/ai-guided-troubleshooting-redefines-observability/"&gt;&lt;u&gt;AI-Guided Troubleshooting&lt;/u&gt;&lt;/a&gt; capabilities, including Suggestions and Investigation Notebooks, entered limited availability Monday with select customers. The company plans full general availability in 2026. The &lt;a href="https://docs.chronosphere.io/integrate/mcp-server"&gt;&lt;u&gt;Model Context Protocol (MCP) Server&lt;/u&gt;&lt;/a&gt;, which enables engineers to integrate Chronosphere directly into internal AI workflows and query observability data through AI-enabled development environments, is available immediately for all Chronosphere customers.&lt;/p&gt;&lt;p&gt;The phased rollout reflects the company&amp;#x27;s cautious approach to deploying AI in production environments where mistakes carry real costs. By gathering feedback from early adopters before broad release, Chronosphere aims to refine its guidance algorithms and validate that its suggestions genuinely accelerate troubleshooting rather than simply generating impressive demonstrations.&lt;/p&gt;&lt;p&gt;The longer game, however, extends beyond individual product features. Chronosphere&amp;#x27;s dual bet — on transparent AI that shows its reasoning and on a partner ecosystem rather than all-in-one integration — amounts to a fundamental thesis about how enterprise observability will evolve as systems grow more complex.&lt;/p&gt;&lt;p&gt;If that thesis proves correct, the company that solves observability for the AI age won&amp;#x27;t be the one with the most automated black box. It will be the one that earns engineers&amp;#x27; trust by explaining what it knows, admitting what it doesn&amp;#x27;t, and letting humans make the final call. In an industry drowning in data and promised silver bullets, Chronosphere is wagering that showing your work still matters — even when AI is doing the math.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/chronosphere-takes-on-datadog-with-ai-that-explains-itself-not-just-outages</guid><pubDate>Mon, 10 Nov 2025 19:00:00 +0000</pubDate></item><item><title>Google brings Gemini to the Google TV Streamer (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/google-brings-gemini-to-the-google-tv-streamer/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/Google-TV-Streamer-set-up.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Monday that it’s starting to roll out Gemini to the Google TV Streamer, replacing Google Assistant. The tech giant says the change will enable users to use their voice more naturally to access content and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, users can now ask something like, “I like dramas but my wife likes comedies. What’s a movie we can watch together?” when looking for movie recommendations. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, users could quickly catch up on a show they’re returning to by asking something like, “What happened at the end of Outlander last season?” In another example, Google says users can even ask something like “What’s the new hospital drama everyone’s talking about?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says Gemini for TV goes beyond entertainment, as users can ask the AI assistant any other type of question as well, just like they can with Gemini on their phone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, users could bring learning to their TV by asking Gemini to “Explain why volcanoes erupt to my third grader.” Gemini can also guide users through DIY projects or recipes with YouTube videos, Google says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access Gemini on the Google TV Streamer, you need to press your remote’s microphone button. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the update is rolling out “over the next few weeks” to users aged 18 and older. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as Google announced back in September that it was introducing Gemini for Google TV to select TCL devices. The company said at the time that later in the year, Gemini would be arriving on the 2025 Hisense U7, U8, and UX models, and 2025 TCL QM7K, QM8K, and X11K models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the Google TV Streamer, Gemini is also available on the Walmart Onn 4K Pro streaming device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Monday’s announcement doesn’t come as a surprise, as the change is part of Google’s plans to replace Google Assistant with Gemini across all of its devices and platforms. Plus, the company had announced at CES in January that Gemini would be coming to Google TV this year.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/Google-TV-Streamer-set-up.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Monday that it’s starting to roll out Gemini to the Google TV Streamer, replacing Google Assistant. The tech giant says the change will enable users to use their voice more naturally to access content and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, users can now ask something like, “I like dramas but my wife likes comedies. What’s a movie we can watch together?” when looking for movie recommendations. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, users could quickly catch up on a show they’re returning to by asking something like, “What happened at the end of Outlander last season?” In another example, Google says users can even ask something like “What’s the new hospital drama everyone’s talking about?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says Gemini for TV goes beyond entertainment, as users can ask the AI assistant any other type of question as well, just like they can with Gemini on their phone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, users could bring learning to their TV by asking Gemini to “Explain why volcanoes erupt to my third grader.” Gemini can also guide users through DIY projects or recipes with YouTube videos, Google says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To access Gemini on the Google TV Streamer, you need to press your remote’s microphone button. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the update is rolling out “over the next few weeks” to users aged 18 and older. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as Google announced back in September that it was introducing Gemini for Google TV to select TCL devices. The company said at the time that later in the year, Gemini would be arriving on the 2025 Hisense U7, U8, and UX models, and 2025 TCL QM7K, QM8K, and X11K models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to the Google TV Streamer, Gemini is also available on the Walmart Onn 4K Pro streaming device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Monday’s announcement doesn’t come as a surprise, as the change is part of Google’s plans to replace Google Assistant with Gemini across all of its devices and platforms. Plus, the company had announced at CES in January that Gemini would be coming to Google TV this year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/google-brings-gemini-to-the-google-tv-streamer/</guid><pubDate>Mon, 10 Nov 2025 19:28:57 +0000</pubDate></item><item><title>Meta returns to open source AI with Omnilingual ASR models that can transcribe 1,600+ languages natively (AI | VentureBeat)</title><link>https://venturebeat.com/ai/meta-returns-to-open-source-ai-with-omnilingual-asr-models-that-can</link><description>[unable to retrieve full-text content]&lt;p&gt;Meta has just released a new &lt;a href="https://ai.meta.com/blog/omnilingual-asr"&gt;multilingual automatic speech recognition (ASR) system&lt;/a&gt; supporting 1,600+ languages — dwarfing OpenAI’s open source Whisper model, which supports just 99. &lt;/p&gt;&lt;p&gt;Is architecture also allows developers to extend that support to thousands more. Through a feature called zero-shot in-context learning, users can provide a few paired examples of audio and text in a new language at inference time, enabling the model to transcribe additional utterances in that language without any retraining.&lt;/p&gt;&lt;p&gt;In practice, this expands potential coverage to more than 5,400 languages — roughly every spoken language with a known script.&lt;/p&gt;&lt;p&gt;It’s a shift from static model capabilities to a flexible framework that communities can adapt themselves. So while the 1,600 languages reflect official training coverage, the broader figure represents Omnilingual ASR’s capacity to generalize on demand, making it the most extensible speech recognition system released to date.&lt;/p&gt;&lt;p&gt;Best of all: it&amp;#x27;s been open sourced under&lt;a href="https://github.com/facebookresearch/omnilingual-asr?tab=License-1-ov-file#readme"&gt; a plain Apache 2.0 license&lt;/a&gt; — not a restrictive, quasi open-source Llama license like the company&amp;#x27;s prior releases, which limited use by larger enterprises unless they paid licensing fees — meaning researchers and developers are free to take and implement it right away, for free, without restrictions, even in commercial and enterprise-grade projects!&lt;/p&gt;&lt;p&gt;Released on November 10 on &lt;a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/"&gt;Meta&amp;#x27;s website&lt;/a&gt;, &lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;Github&lt;/a&gt;, along with a &lt;a href="https://huggingface.co/spaces/facebook/omniasr-transcriptions"&gt;demo space on Hugging Face&lt;/a&gt; and &lt;a href="https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/"&gt;technical paper&lt;/a&gt;, Meta’s Omnilingual ASR suite includes a family of speech recognition models, a 7-billion parameter multilingual audio representation model, and a massive speech corpus spanning over 350 previously underserved languages. &lt;/p&gt;&lt;p&gt;All resources are freely available under open licenses, and the models support speech-to-text transcription out of the box.&lt;/p&gt;&lt;p&gt;“By open sourcing these models and dataset, we aim to break down language barriers, expand digital access, and empower communities worldwide,” Meta posted on its &lt;a href="https://x.com/AIatMeta/status/1987957744138416389"&gt;@AIatMeta account on X&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Designed for Speech-to-Text Transcription&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At its core, Omnilingual ASR is a speech-to-text system. &lt;/p&gt;&lt;p&gt;The models are trained to convert spoken language into written text, supporting applications like voice assistants, transcription tools, subtitles, oral archive digitization, and accessibility features for low-resource languages.&lt;/p&gt;&lt;p&gt;Unlike earlier ASR models that required extensive labeled training data, Omnilingual ASR includes a zero-shot variant. &lt;/p&gt;&lt;p&gt;This version can transcribe languages it has never seen before—using just a few paired examples of audio and corresponding text. &lt;/p&gt;&lt;p&gt;This lowers the barrier for adding new or endangered languages dramatically, removing the need for large corpora or retraining.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Model Family and Technical Design&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Omnilingual ASR suite includes multiple model families trained on more than 4.3 million hours of audio from 1,600+ languages:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;wav2vec 2.0 models for self-supervised speech representation learning (300M–7B parameters)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CTC-based ASR models for efficient supervised transcription&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LLM-ASR models combining a speech encoder with a Transformer-based text decoder for state-of-the-art transcription&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LLM-ZeroShot ASR model, enabling inference-time adaptation to unseen languages&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All models follow an encoder–decoder design: raw audio is converted into a language-agnostic representation, then decoded into written text.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why the Scale Matters&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While Whisper and similar models have advanced ASR capabilities for global languages, they fall short on the long tail of human linguistic diversity. Whisper supports 99 languages. Meta’s system:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Directly supports 1,600+ languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Can generalize to 5,400+ languages using in-context learning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Achieves character error rates (CER) under 10% in 78% of supported languages&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Among those supported are more than 500 languages never previously covered by any ASR model, according to Meta’s research paper.&lt;/p&gt;&lt;p&gt;This expansion opens new possibilities for communities whose languages are often excluded from digital tools&lt;/p&gt;&lt;p&gt;Here’s the revised and expanded background section, integrating the broader context of Meta’s 2025 AI strategy, leadership changes, and Llama 4’s reception, complete with in-text citations and links:&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Background: Meta’s AI Overhaul and a Rebound from Llama 4&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of Omnilingual ASR arrives at a pivotal moment in Meta’s AI strategy, following a year marked by organizational turbulence, leadership changes, and uneven product execution. &lt;/p&gt;&lt;p&gt;Omnilingual ASR is the first major open-source model release since the rollout of Llama 4, Meta’s latest large language model, which&lt;a href="https://ftr.bazqux.com/epseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way"&gt; debuted in April 2025&lt;/a&gt; to &lt;a href="https://venturebeat.com/ai/meta-defends-llama-4-release-against-reports-of-mixed-quality-blames-bugs"&gt;mixed and ultimately poor reviews&lt;/a&gt;, with scant enterprise adoption compared to Chinese open source model competitors.&lt;/p&gt;&lt;p&gt;The failure led Meta founder and CEO Mark Zuckerberg to appoint Alexandr Wang, co-founder and prior CEO of AI data supplier Scale AI, &lt;a href="https://www.cnbc.com/2025/06/10/zuckerberg-makes-metas-biggest-bet-on-ai-14-billion-scale-ai-deal.html"&gt;as Chief AI Officer&lt;/a&gt;, and embark on an &lt;a href="https://www.wired.com/story/meta-poaches-openai-researcher-yang-song/"&gt;extensive and costly hiring spree&lt;/a&gt; that shocked the AI and business communities with &lt;a href="https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/"&gt;eye-watering pay packages for top AI researchers&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In contrast, Omnilingual ASR represents a strategic and reputational reset. It returns Meta to a domain where the company has historically led — multilingual AI — and offers a truly extensible, community-oriented stack with minimal barriers to entry. &lt;/p&gt;&lt;p&gt;The system’s support for 1,600+ languages and its extensibility to over 5,000 more via zero-shot in-context learning reassert Meta’s engineering credibility in language technology. &lt;/p&gt;&lt;p&gt;Importantly, it does so through a free and permissively licensed release, under Apache 2.0, with transparent dataset sourcing and reproducible training protocols.&lt;/p&gt;&lt;p&gt;This shift aligns with broader themes in Meta’s 2025 strategy. The company has refocused its narrative around a “personal superintelligence” vision, investing heavily in infrastructure (including a September release of custom AI accelerators and Arm-based inference stacks) &lt;a href="https://engineering.fb.com/2025/09/22/data-infrastructure/meta-custom-accelerators/"&gt;source&lt;/a&gt; while downplaying the metaverse in favor of foundational AI capabilities. The return to public training data in Europe after a regulatory pause also underscores its intention to compete globally, despite privacy scrutiny &lt;a href="https://apnews.com/article/meta-eu-data-training-2025"&gt;source&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Omnilingual ASR, then, is more than a model release — it’s a calculated move to reassert control of the narrative: from the fragmented rollout of Llama 4 to a high-utility, research-grounded contribution that aligns with Meta’s long-term AI platform strategy.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community-Centered Dataset Collection&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To achieve this scale, Meta partnered with researchers and community organizations in Africa, Asia, and elsewhere to create the Omnilingual ASR Corpus, a 3,350-hour dataset across 348 low-resource languages. Contributors were compensated local speakers, and recordings were gathered in collaboration with groups like:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;African Next Voices&lt;/b&gt;: A Gates Foundation–supported consortium including Maseno University (Kenya), University of Pretoria, and Data Science Nigeria&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mozilla Foundation’s Common Voice&lt;/b&gt;, supported through the Open Multilingual Speech Fund&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Lanfrica / NaijaVoices&lt;/b&gt;, which created data for 11 African languages including Igala, Serer, and Urhobo&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The data collection focused on natural, unscripted speech. Prompts were designed to be culturally relevant and open-ended, such as “Is it better to have a few close friends or many casual acquaintances? Why?” Transcriptions used established writing systems, with quality assurance built into every step.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance and Hardware Considerations&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The largest model in the suite, the omniASR_LLM_7B, requires ~17GB of GPU memory for inference, making it suitable for deployment on high-end hardware. Smaller models (300M–1B) can run on lower-power devices and deliver real-time transcription speeds.&lt;/p&gt;&lt;p&gt;Performance benchmarks show strong results even in low-resource scenarios:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;CER &amp;lt;10% in 95% of high-resource and mid-resource languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CER &amp;lt;10% in 36% of low-resource languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Robustness in noisy conditions and unseen domains, especially with fine-tuning&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The zero-shot system, omniASR_LLM_7B_ZS, can transcribe new languages with minimal setup. Users provide a few sample audio–text pairs, and the model generates transcriptions for new utterances in the same language.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Open Access and Developer Tooling&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;All models and the dataset are licensed under permissive terms:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Apache 2.0&lt;/b&gt; for models and code&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;CC-BY 4.0&lt;/b&gt; for the &lt;a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus"&gt;Omnilingual ASR Corpus on HuggingFace&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Installation is supported via PyPI and uv:&lt;/p&gt;&lt;p&gt;&lt;code&gt;pip install omnilingual-asr&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Meta also provides:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A HuggingFace dataset integration&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Pre-built inference pipelines&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Language-code conditioning for improved accuracy&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Developers can view the full list of supported languages using the API:&lt;/p&gt;&lt;p&gt;&lt;code&gt;from omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;print(len(supported_langs))
print(supported_langs)&lt;/code&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Broader Implications&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Omnilingual ASR reframes language coverage in ASR from a fixed list to an &lt;b&gt;extensible framework&lt;/b&gt;. It enables:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Community-driven inclusion of underrepresented languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Digital access for oral and endangered languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Research on speech tech in linguistically diverse contexts&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Crucially, Meta emphasizes ethical considerations throughout—advocating for open-source participation and collaboration with native-speaking communities.&lt;/p&gt;&lt;p&gt;“No model can ever anticipate and include all of the world’s languages in advance,” the Omnilingual ASR paper states, “but Omnilingual ASR makes it possible for communities to extend recognition with their own data.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Access the Tools&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;All resources are now available at:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Code + Models&lt;/b&gt;: &lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;github.com/facebookresearch/omnilingual-asr&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dataset&lt;/b&gt;: &lt;a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus"&gt;huggingface.co/datasets/facebook/omnilingual-asr-corpus&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Blogpost&lt;/b&gt;: &lt;a href="https://ai.meta.com/blog/omnilingual-asr"&gt;ai.meta.com/blog/omnilingual-asr&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;What This Means for Enterprises&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For enterprise developers, especially those operating in multilingual or international markets, Omnilingual ASR significantly lowers the barrier to deploying speech-to-text systems across a broader range of customers and geographies. &lt;/p&gt;&lt;p&gt;Instead of relying on commercial ASR APIs that support only a narrow set of high-resource languages, teams can now integrate an open-source pipeline that covers over 1,600 languages out of the box—with the option to extend it to thousands more via zero-shot learning.&lt;/p&gt;&lt;p&gt;This flexibility is especially valuable for enterprises working in sectors like voice-based customer support, transcription services, accessibility, education, or civic technology, where local language coverage can be a competitive or regulatory necessity. Because the models are released under the permissive Apache 2.0 license, businesses can fine-tune, deploy, or integrate them into proprietary systems without restrictive terms.&lt;/p&gt;&lt;p&gt;It also represents a shift in the ASR landscape—from centralized, cloud-gated offerings to community-extendable infrastructure. By making multilingual speech recognition more accessible, customizable, and cost-effective, Omnilingual ASR opens the door to a new generation of enterprise speech applications built around linguistic inclusion rather than linguistic limitation.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Meta has just released a new &lt;a href="https://ai.meta.com/blog/omnilingual-asr"&gt;multilingual automatic speech recognition (ASR) system&lt;/a&gt; supporting 1,600+ languages — dwarfing OpenAI’s open source Whisper model, which supports just 99. &lt;/p&gt;&lt;p&gt;Is architecture also allows developers to extend that support to thousands more. Through a feature called zero-shot in-context learning, users can provide a few paired examples of audio and text in a new language at inference time, enabling the model to transcribe additional utterances in that language without any retraining.&lt;/p&gt;&lt;p&gt;In practice, this expands potential coverage to more than 5,400 languages — roughly every spoken language with a known script.&lt;/p&gt;&lt;p&gt;It’s a shift from static model capabilities to a flexible framework that communities can adapt themselves. So while the 1,600 languages reflect official training coverage, the broader figure represents Omnilingual ASR’s capacity to generalize on demand, making it the most extensible speech recognition system released to date.&lt;/p&gt;&lt;p&gt;Best of all: it&amp;#x27;s been open sourced under&lt;a href="https://github.com/facebookresearch/omnilingual-asr?tab=License-1-ov-file#readme"&gt; a plain Apache 2.0 license&lt;/a&gt; — not a restrictive, quasi open-source Llama license like the company&amp;#x27;s prior releases, which limited use by larger enterprises unless they paid licensing fees — meaning researchers and developers are free to take and implement it right away, for free, without restrictions, even in commercial and enterprise-grade projects!&lt;/p&gt;&lt;p&gt;Released on November 10 on &lt;a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/"&gt;Meta&amp;#x27;s website&lt;/a&gt;, &lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;Github&lt;/a&gt;, along with a &lt;a href="https://huggingface.co/spaces/facebook/omniasr-transcriptions"&gt;demo space on Hugging Face&lt;/a&gt; and &lt;a href="https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/"&gt;technical paper&lt;/a&gt;, Meta’s Omnilingual ASR suite includes a family of speech recognition models, a 7-billion parameter multilingual audio representation model, and a massive speech corpus spanning over 350 previously underserved languages. &lt;/p&gt;&lt;p&gt;All resources are freely available under open licenses, and the models support speech-to-text transcription out of the box.&lt;/p&gt;&lt;p&gt;“By open sourcing these models and dataset, we aim to break down language barriers, expand digital access, and empower communities worldwide,” Meta posted on its &lt;a href="https://x.com/AIatMeta/status/1987957744138416389"&gt;@AIatMeta account on X&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Designed for Speech-to-Text Transcription&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At its core, Omnilingual ASR is a speech-to-text system. &lt;/p&gt;&lt;p&gt;The models are trained to convert spoken language into written text, supporting applications like voice assistants, transcription tools, subtitles, oral archive digitization, and accessibility features for low-resource languages.&lt;/p&gt;&lt;p&gt;Unlike earlier ASR models that required extensive labeled training data, Omnilingual ASR includes a zero-shot variant. &lt;/p&gt;&lt;p&gt;This version can transcribe languages it has never seen before—using just a few paired examples of audio and corresponding text. &lt;/p&gt;&lt;p&gt;This lowers the barrier for adding new or endangered languages dramatically, removing the need for large corpora or retraining.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Model Family and Technical Design&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Omnilingual ASR suite includes multiple model families trained on more than 4.3 million hours of audio from 1,600+ languages:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;wav2vec 2.0 models for self-supervised speech representation learning (300M–7B parameters)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CTC-based ASR models for efficient supervised transcription&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LLM-ASR models combining a speech encoder with a Transformer-based text decoder for state-of-the-art transcription&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LLM-ZeroShot ASR model, enabling inference-time adaptation to unseen languages&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All models follow an encoder–decoder design: raw audio is converted into a language-agnostic representation, then decoded into written text.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why the Scale Matters&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While Whisper and similar models have advanced ASR capabilities for global languages, they fall short on the long tail of human linguistic diversity. Whisper supports 99 languages. Meta’s system:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Directly supports 1,600+ languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Can generalize to 5,400+ languages using in-context learning&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Achieves character error rates (CER) under 10% in 78% of supported languages&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Among those supported are more than 500 languages never previously covered by any ASR model, according to Meta’s research paper.&lt;/p&gt;&lt;p&gt;This expansion opens new possibilities for communities whose languages are often excluded from digital tools&lt;/p&gt;&lt;p&gt;Here’s the revised and expanded background section, integrating the broader context of Meta’s 2025 AI strategy, leadership changes, and Llama 4’s reception, complete with in-text citations and links:&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Background: Meta’s AI Overhaul and a Rebound from Llama 4&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of Omnilingual ASR arrives at a pivotal moment in Meta’s AI strategy, following a year marked by organizational turbulence, leadership changes, and uneven product execution. &lt;/p&gt;&lt;p&gt;Omnilingual ASR is the first major open-source model release since the rollout of Llama 4, Meta’s latest large language model, which&lt;a href="https://ftr.bazqux.com/epseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way"&gt; debuted in April 2025&lt;/a&gt; to &lt;a href="https://venturebeat.com/ai/meta-defends-llama-4-release-against-reports-of-mixed-quality-blames-bugs"&gt;mixed and ultimately poor reviews&lt;/a&gt;, with scant enterprise adoption compared to Chinese open source model competitors.&lt;/p&gt;&lt;p&gt;The failure led Meta founder and CEO Mark Zuckerberg to appoint Alexandr Wang, co-founder and prior CEO of AI data supplier Scale AI, &lt;a href="https://www.cnbc.com/2025/06/10/zuckerberg-makes-metas-biggest-bet-on-ai-14-billion-scale-ai-deal.html"&gt;as Chief AI Officer&lt;/a&gt;, and embark on an &lt;a href="https://www.wired.com/story/meta-poaches-openai-researcher-yang-song/"&gt;extensive and costly hiring spree&lt;/a&gt; that shocked the AI and business communities with &lt;a href="https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/"&gt;eye-watering pay packages for top AI researchers&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In contrast, Omnilingual ASR represents a strategic and reputational reset. It returns Meta to a domain where the company has historically led — multilingual AI — and offers a truly extensible, community-oriented stack with minimal barriers to entry. &lt;/p&gt;&lt;p&gt;The system’s support for 1,600+ languages and its extensibility to over 5,000 more via zero-shot in-context learning reassert Meta’s engineering credibility in language technology. &lt;/p&gt;&lt;p&gt;Importantly, it does so through a free and permissively licensed release, under Apache 2.0, with transparent dataset sourcing and reproducible training protocols.&lt;/p&gt;&lt;p&gt;This shift aligns with broader themes in Meta’s 2025 strategy. The company has refocused its narrative around a “personal superintelligence” vision, investing heavily in infrastructure (including a September release of custom AI accelerators and Arm-based inference stacks) &lt;a href="https://engineering.fb.com/2025/09/22/data-infrastructure/meta-custom-accelerators/"&gt;source&lt;/a&gt; while downplaying the metaverse in favor of foundational AI capabilities. The return to public training data in Europe after a regulatory pause also underscores its intention to compete globally, despite privacy scrutiny &lt;a href="https://apnews.com/article/meta-eu-data-training-2025"&gt;source&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Omnilingual ASR, then, is more than a model release — it’s a calculated move to reassert control of the narrative: from the fragmented rollout of Llama 4 to a high-utility, research-grounded contribution that aligns with Meta’s long-term AI platform strategy.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community-Centered Dataset Collection&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To achieve this scale, Meta partnered with researchers and community organizations in Africa, Asia, and elsewhere to create the Omnilingual ASR Corpus, a 3,350-hour dataset across 348 low-resource languages. Contributors were compensated local speakers, and recordings were gathered in collaboration with groups like:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;African Next Voices&lt;/b&gt;: A Gates Foundation–supported consortium including Maseno University (Kenya), University of Pretoria, and Data Science Nigeria&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mozilla Foundation’s Common Voice&lt;/b&gt;, supported through the Open Multilingual Speech Fund&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Lanfrica / NaijaVoices&lt;/b&gt;, which created data for 11 African languages including Igala, Serer, and Urhobo&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The data collection focused on natural, unscripted speech. Prompts were designed to be culturally relevant and open-ended, such as “Is it better to have a few close friends or many casual acquaintances? Why?” Transcriptions used established writing systems, with quality assurance built into every step.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance and Hardware Considerations&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The largest model in the suite, the omniASR_LLM_7B, requires ~17GB of GPU memory for inference, making it suitable for deployment on high-end hardware. Smaller models (300M–1B) can run on lower-power devices and deliver real-time transcription speeds.&lt;/p&gt;&lt;p&gt;Performance benchmarks show strong results even in low-resource scenarios:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;CER &amp;lt;10% in 95% of high-resource and mid-resource languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CER &amp;lt;10% in 36% of low-resource languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Robustness in noisy conditions and unseen domains, especially with fine-tuning&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The zero-shot system, omniASR_LLM_7B_ZS, can transcribe new languages with minimal setup. Users provide a few sample audio–text pairs, and the model generates transcriptions for new utterances in the same language.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Open Access and Developer Tooling&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;All models and the dataset are licensed under permissive terms:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Apache 2.0&lt;/b&gt; for models and code&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;CC-BY 4.0&lt;/b&gt; for the &lt;a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus"&gt;Omnilingual ASR Corpus on HuggingFace&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Installation is supported via PyPI and uv:&lt;/p&gt;&lt;p&gt;&lt;code&gt;pip install omnilingual-asr&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Meta also provides:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A HuggingFace dataset integration&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Pre-built inference pipelines&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Language-code conditioning for improved accuracy&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Developers can view the full list of supported languages using the API:&lt;/p&gt;&lt;p&gt;&lt;code&gt;from omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langs&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;print(len(supported_langs))
print(supported_langs)&lt;/code&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Broader Implications&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Omnilingual ASR reframes language coverage in ASR from a fixed list to an &lt;b&gt;extensible framework&lt;/b&gt;. It enables:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Community-driven inclusion of underrepresented languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Digital access for oral and endangered languages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Research on speech tech in linguistically diverse contexts&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Crucially, Meta emphasizes ethical considerations throughout—advocating for open-source participation and collaboration with native-speaking communities.&lt;/p&gt;&lt;p&gt;“No model can ever anticipate and include all of the world’s languages in advance,” the Omnilingual ASR paper states, “but Omnilingual ASR makes it possible for communities to extend recognition with their own data.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Access the Tools&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;All resources are now available at:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Code + Models&lt;/b&gt;: &lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;github.com/facebookresearch/omnilingual-asr&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dataset&lt;/b&gt;: &lt;a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus"&gt;huggingface.co/datasets/facebook/omnilingual-asr-corpus&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Blogpost&lt;/b&gt;: &lt;a href="https://ai.meta.com/blog/omnilingual-asr"&gt;ai.meta.com/blog/omnilingual-asr&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;What This Means for Enterprises&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For enterprise developers, especially those operating in multilingual or international markets, Omnilingual ASR significantly lowers the barrier to deploying speech-to-text systems across a broader range of customers and geographies. &lt;/p&gt;&lt;p&gt;Instead of relying on commercial ASR APIs that support only a narrow set of high-resource languages, teams can now integrate an open-source pipeline that covers over 1,600 languages out of the box—with the option to extend it to thousands more via zero-shot learning.&lt;/p&gt;&lt;p&gt;This flexibility is especially valuable for enterprises working in sectors like voice-based customer support, transcription services, accessibility, education, or civic technology, where local language coverage can be a competitive or regulatory necessity. Because the models are released under the permissive Apache 2.0 license, businesses can fine-tune, deploy, or integrate them into proprietary systems without restrictive terms.&lt;/p&gt;&lt;p&gt;It also represents a shift in the ASR landscape—from centralized, cloud-gated offerings to community-extendable infrastructure. By making multilingual speech recognition more accessible, customizable, and cost-effective, Omnilingual ASR opens the door to a new generation of enterprise speech applications built around linguistic inclusion rather than linguistic limitation.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/meta-returns-to-open-source-ai-with-omnilingual-asr-models-that-can</guid><pubDate>Mon, 10 Nov 2025 20:27:00 +0000</pubDate></item><item><title>Kaltura acquires eSelf, founded by creator of Snap’s AI, in $27M deal (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/kaltura-acquires-eself-founded-by-creator-of-snaps-ai-in-27m-deal/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Kaltura-EX-Avatar-Town-Hall-Assistant.png?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kaltura, a New York-headquartered AI video platform company, is acquiring eSelf.ai, an Israel-based startup behind conversational avatars — AI-generated digital humans that can talk with users — for about $27 million. Kaltura announced today that it has signed a definitive agreement to acquire eSelf, a platform supporting more than 30 languages and featuring a user-friendly studio for creating, customizing, and deploying photorealistic digital avatars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Co-founded in 2023 by CEO Alan Bekker, who previously sold his first startup, Voca, to Snap in 2020 — and CTO Eylon Shoshan, eSelf brings deep technical expertise in speech-to-video generation, low-latency speech recognition, and screen understanding, which allows avatars to see and respond to what’s on a user’s screen. The eSelf co-founders will join Kaltura to oversee the integration of eSelf’s technology into the company, with all current eSelf employees coming on board as well.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The two-year-old startup has a small but strong team of around 15 AI experts, Ron Yekutiel, co-founder and CEO of Kaltura, told TechCrunch. He noted that Bekker’s former company specialized in natural language processing, which helps computers understand human speech, and computer vision, saying it was a “very leading company in the area of conversational speech bots. And so he’s an expert [in this field], and that’s what we bought,” Yekutiel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura offers a suite of cloud-based software solutions designed for advanced video applications, including a corporate video portal akin to a private YouTube, tools for webinars and virtual events, and integrations that embed video learning into university learning management systems, or platforms that organize online coursework.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Nasdaq-listed company also delivers virtual classroom products and end-to-end TV streaming solutions. Kaltura’s video platform serves over 800 enterprise customers, helping them engage users across sales, marketing, customer care, education, and entertainment. Its clients include tech giants like Amazon, Oracle, Salesforce, SAP, Adobe, and IBM, as well as leading banks, insurance companies, consulting firms, pharmaceutical companies, and universities in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura plans to integrate eSelf.ai’s virtual agent technology across its video offerings; the integration aims to enable agents that can listen, speak, and interpret user screens in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This acquisition was so strategic. We were actively evaluating multiple companies to find the right fit. We determined that they [eSelf] were best-in-class for real-time, synchronous conversation — not just video-on-demand lip-syncing — and that they had an impressive speech-to-text and text-to-speech technology stack,” Yekutiel said in an interview with TechCrunch. “Beyond the technology, there was also a strong cultural and geographic alignment, which was critical for us.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-why-a-video-company-is-betting-on-conversational-avatars"&gt;&lt;strong&gt;Why a video company is betting on conversational avatars&lt;/strong&gt;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;For the past two decades, businesses have mostly used video for streaming, uploading, and managing content. But that’s changing fast. Thanks to AI, videos can now be generated instantly — hyper-personalized and contextual — giving every viewer their own custom experience, tailored exactly to what they need in that moment, Yekutiel explained.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We started with video, then moved to personalized video, and now, with eSelf’s technology, we’re adding human-like capabilities — faces, eyes, mouths, ears — to make our AI agents conversational and expressive,” Yekutiel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura is evolving from a video platform into a video-based customer and employee experience provider, where video serves as the interface. Unlike most avatar companies that offer only a “face,” it delivers the full workflow — avatar, intelligence, and enterprise-connected knowledge. The focus isn’t just streaming video; it’s driving measurable business results and ROI, the CEO added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company plans to launch standalone, embeddable agents for uses including sales, marketing, customer support, and training. Target sectors include education, media and telecom, e-commerce, financial services, healthcare, and pharmaceuticals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked about media reports saying Kaltura was exploring a sale or merger at a $400 million to $500 million valuation, Yekutiel told TechCrunch that Kaltura has explored opportunities with a range of companies, including potential “acquisitions, mergers with similarly sized firms, and connections with some larger players.” But it never got close to a transaction like the ones being reported, he said. He also pointed to Kaltura’s recent acquisitions, including its fourth company, as evidence of the company’s continued commitment to its current strategy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This marks Kaltura’s fourth acquisition to date. The company acquired cloud TV solution Tvinci in 2014, followed by Rapt Media in 2018, and video conferencing platform Newrow in 2020. eSelf’s most recent funding round was its&amp;nbsp;$4.5 million announced in December 2024.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura, which went public in 2021, reports around $180 million in revenue, is profitable on an adjusted EBITDA and cash flow basis, and has about 600 employees.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Kaltura-EX-Avatar-Town-Hall-Assistant.png?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kaltura, a New York-headquartered AI video platform company, is acquiring eSelf.ai, an Israel-based startup behind conversational avatars — AI-generated digital humans that can talk with users — for about $27 million. Kaltura announced today that it has signed a definitive agreement to acquire eSelf, a platform supporting more than 30 languages and featuring a user-friendly studio for creating, customizing, and deploying photorealistic digital avatars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Co-founded in 2023 by CEO Alan Bekker, who previously sold his first startup, Voca, to Snap in 2020 — and CTO Eylon Shoshan, eSelf brings deep technical expertise in speech-to-video generation, low-latency speech recognition, and screen understanding, which allows avatars to see and respond to what’s on a user’s screen. The eSelf co-founders will join Kaltura to oversee the integration of eSelf’s technology into the company, with all current eSelf employees coming on board as well.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The two-year-old startup has a small but strong team of around 15 AI experts, Ron Yekutiel, co-founder and CEO of Kaltura, told TechCrunch. He noted that Bekker’s former company specialized in natural language processing, which helps computers understand human speech, and computer vision, saying it was a “very leading company in the area of conversational speech bots. And so he’s an expert [in this field], and that’s what we bought,” Yekutiel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura offers a suite of cloud-based software solutions designed for advanced video applications, including a corporate video portal akin to a private YouTube, tools for webinars and virtual events, and integrations that embed video learning into university learning management systems, or platforms that organize online coursework.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Nasdaq-listed company also delivers virtual classroom products and end-to-end TV streaming solutions. Kaltura’s video platform serves over 800 enterprise customers, helping them engage users across sales, marketing, customer care, education, and entertainment. Its clients include tech giants like Amazon, Oracle, Salesforce, SAP, Adobe, and IBM, as well as leading banks, insurance companies, consulting firms, pharmaceutical companies, and universities in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura plans to integrate eSelf.ai’s virtual agent technology across its video offerings; the integration aims to enable agents that can listen, speak, and interpret user screens in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This acquisition was so strategic. We were actively evaluating multiple companies to find the right fit. We determined that they [eSelf] were best-in-class for real-time, synchronous conversation — not just video-on-demand lip-syncing — and that they had an impressive speech-to-text and text-to-speech technology stack,” Yekutiel said in an interview with TechCrunch. “Beyond the technology, there was also a strong cultural and geographic alignment, which was critical for us.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-why-a-video-company-is-betting-on-conversational-avatars"&gt;&lt;strong&gt;Why a video company is betting on conversational avatars&lt;/strong&gt;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;For the past two decades, businesses have mostly used video for streaming, uploading, and managing content. But that’s changing fast. Thanks to AI, videos can now be generated instantly — hyper-personalized and contextual — giving every viewer their own custom experience, tailored exactly to what they need in that moment, Yekutiel explained.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We started with video, then moved to personalized video, and now, with eSelf’s technology, we’re adding human-like capabilities — faces, eyes, mouths, ears — to make our AI agents conversational and expressive,” Yekutiel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura is evolving from a video platform into a video-based customer and employee experience provider, where video serves as the interface. Unlike most avatar companies that offer only a “face,” it delivers the full workflow — avatar, intelligence, and enterprise-connected knowledge. The focus isn’t just streaming video; it’s driving measurable business results and ROI, the CEO added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company plans to launch standalone, embeddable agents for uses including sales, marketing, customer support, and training. Target sectors include education, media and telecom, e-commerce, financial services, healthcare, and pharmaceuticals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked about media reports saying Kaltura was exploring a sale or merger at a $400 million to $500 million valuation, Yekutiel told TechCrunch that Kaltura has explored opportunities with a range of companies, including potential “acquisitions, mergers with similarly sized firms, and connections with some larger players.” But it never got close to a transaction like the ones being reported, he said. He also pointed to Kaltura’s recent acquisitions, including its fourth company, as evidence of the company’s continued commitment to its current strategy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This marks Kaltura’s fourth acquisition to date. The company acquired cloud TV solution Tvinci in 2014, followed by Rapt Media in 2018, and video conferencing platform Newrow in 2020. eSelf’s most recent funding round was its&amp;nbsp;$4.5 million announced in December 2024.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaltura, which went public in 2021, reports around $180 million in revenue, is profitable on an adjusted EBITDA and cash flow basis, and has about 600 employees.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/kaltura-acquires-eself-founded-by-creator-of-snaps-ai-in-27m-deal/</guid><pubDate>Mon, 10 Nov 2025 21:05:00 +0000</pubDate></item><item><title>A better way of thinking about the AI bubble (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/a-better-way-of-thinking-about-the-ai-bubble/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2015/05/bubbles.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People often think about tech bubbles in apocalyptic terms, but it doesn’t have to be as serious as all that. In economic terms, a bubble is a bet that turned out to be too big, leaving you with more supply than demand.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot: It’s not all or nothing, and even good bets can turn sour if you aren’t careful about how you make them.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What makes the question of the AI bubble so tricky to answer is&amp;nbsp;mismatched&amp;nbsp;timelines&amp;nbsp;between the breakneck pace of AI software development and the slow crawl of constructing and powering a data center.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because&amp;nbsp;these data centers take years to build, a lot will inevitably change between now and when they come online.&amp;nbsp;The supply chain that powers AI services is so complex and fluid that&amp;nbsp;it’s&amp;nbsp;hard to have any clarity on how much supply&amp;nbsp;we’ll&amp;nbsp;need a few years from now.&amp;nbsp;It&amp;nbsp;isn’t&amp;nbsp;simply a matter of how much people will be using AI in 2028, but how&amp;nbsp;they’ll&amp;nbsp;be using it, and whether&amp;nbsp;we’ll&amp;nbsp;have any breakthroughs in energy, semiconductor design,&amp;nbsp;or power transmission in the meantime.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When a bet is this big, there are lots of ways&amp;nbsp;it can go wrong — and AI bets are getting&amp;nbsp;very big&amp;nbsp;indeed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, Reuters reported that an Oracle-linked data center campus in New Mexico has drawn as much as&amp;nbsp;$18 billion&amp;nbsp;in credit&amp;nbsp;from a consortium of 20 banks. Oracle has already contracted&amp;nbsp;$300 billion&amp;nbsp;in cloud services to OpenAI, and the companies have&amp;nbsp;joined with&amp;nbsp;SoftBank to build&amp;nbsp;$500 billion&amp;nbsp;in total AI infrastructure as part of the “Stargate” project. Meta, not to be outdone, has&amp;nbsp;pledged to spend&amp;nbsp;$600 billion&amp;nbsp;on infrastructure over the next three years.&amp;nbsp;We’ve&amp;nbsp;been tracking all the major commitments&amp;nbsp;here&amp;nbsp;— and the sheer volume has made it hard to keep up.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, there is real uncertainty about how fast demand for AI services will grow.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A McKinsey survey released last week&amp;nbsp;looked&amp;nbsp;at how top firms are employing AI tools. The results were mixed.&amp;nbsp;Almost all&amp;nbsp;the businesses contacted are using AI in some way,&amp;nbsp;yet&amp;nbsp;few are using it&amp;nbsp;on&amp;nbsp;any real&amp;nbsp;scale. AI has&amp;nbsp;allowed&amp;nbsp;companies to&amp;nbsp;cost-cut in specific use cases, but&amp;nbsp;it’s&amp;nbsp;not making a dent on the overall business. In short, most companies are still in “wait and see” mode. If&amp;nbsp;you’re&amp;nbsp;counting on those companies to buy space in your data center, you may be waiting a long time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But even if AI demand is endless, these projects could run into more straightforward infrastructure problems. Last week, Satya Nadella surprised podcast listeners&amp;nbsp;by saying he was more concerned with&amp;nbsp;running out of data center space&amp;nbsp;than running out of chips. (As he put it, “It’s not a supply issue of chips; it’s the fact that I don’t have warm shells to plug into.”) At the same time, whole data centers are sitting idle because they can’t handle the power demands of the latest generation of chips.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Nvidia and OpenAI have been moving forward as fast as they possibly can, the electrical grid and built environment are still moving at the same pace they always have. That leaves lots of opportunity for expensive bottlenecks, even if everything else goes right.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;We get deeper into the idea in this week’s Equity podcast, which you can listen to below.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2015/05/bubbles.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People often think about tech bubbles in apocalyptic terms, but it doesn’t have to be as serious as all that. In economic terms, a bubble is a bet that turned out to be too big, leaving you with more supply than demand.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot: It’s not all or nothing, and even good bets can turn sour if you aren’t careful about how you make them.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What makes the question of the AI bubble so tricky to answer is&amp;nbsp;mismatched&amp;nbsp;timelines&amp;nbsp;between the breakneck pace of AI software development and the slow crawl of constructing and powering a data center.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because&amp;nbsp;these data centers take years to build, a lot will inevitably change between now and when they come online.&amp;nbsp;The supply chain that powers AI services is so complex and fluid that&amp;nbsp;it’s&amp;nbsp;hard to have any clarity on how much supply&amp;nbsp;we’ll&amp;nbsp;need a few years from now.&amp;nbsp;It&amp;nbsp;isn’t&amp;nbsp;simply a matter of how much people will be using AI in 2028, but how&amp;nbsp;they’ll&amp;nbsp;be using it, and whether&amp;nbsp;we’ll&amp;nbsp;have any breakthroughs in energy, semiconductor design,&amp;nbsp;or power transmission in the meantime.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When a bet is this big, there are lots of ways&amp;nbsp;it can go wrong — and AI bets are getting&amp;nbsp;very big&amp;nbsp;indeed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, Reuters reported that an Oracle-linked data center campus in New Mexico has drawn as much as&amp;nbsp;$18 billion&amp;nbsp;in credit&amp;nbsp;from a consortium of 20 banks. Oracle has already contracted&amp;nbsp;$300 billion&amp;nbsp;in cloud services to OpenAI, and the companies have&amp;nbsp;joined with&amp;nbsp;SoftBank to build&amp;nbsp;$500 billion&amp;nbsp;in total AI infrastructure as part of the “Stargate” project. Meta, not to be outdone, has&amp;nbsp;pledged to spend&amp;nbsp;$600 billion&amp;nbsp;on infrastructure over the next three years.&amp;nbsp;We’ve&amp;nbsp;been tracking all the major commitments&amp;nbsp;here&amp;nbsp;— and the sheer volume has made it hard to keep up.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, there is real uncertainty about how fast demand for AI services will grow.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A McKinsey survey released last week&amp;nbsp;looked&amp;nbsp;at how top firms are employing AI tools. The results were mixed.&amp;nbsp;Almost all&amp;nbsp;the businesses contacted are using AI in some way,&amp;nbsp;yet&amp;nbsp;few are using it&amp;nbsp;on&amp;nbsp;any real&amp;nbsp;scale. AI has&amp;nbsp;allowed&amp;nbsp;companies to&amp;nbsp;cost-cut in specific use cases, but&amp;nbsp;it’s&amp;nbsp;not making a dent on the overall business. In short, most companies are still in “wait and see” mode. If&amp;nbsp;you’re&amp;nbsp;counting on those companies to buy space in your data center, you may be waiting a long time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But even if AI demand is endless, these projects could run into more straightforward infrastructure problems. Last week, Satya Nadella surprised podcast listeners&amp;nbsp;by saying he was more concerned with&amp;nbsp;running out of data center space&amp;nbsp;than running out of chips. (As he put it, “It’s not a supply issue of chips; it’s the fact that I don’t have warm shells to plug into.”) At the same time, whole data centers are sitting idle because they can’t handle the power demands of the latest generation of chips.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Nvidia and OpenAI have been moving forward as fast as they possibly can, the electrical grid and built environment are still moving at the same pace they always have. That leaves lots of opportunity for expensive bottlenecks, even if everything else goes right.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;We get deeper into the idea in this week’s Equity podcast, which you can listen to below.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/a-better-way-of-thinking-about-the-ai-bubble/</guid><pubDate>Mon, 10 Nov 2025 21:16:41 +0000</pubDate></item><item><title>Researchers isolate memorization from reasoning in AI neural networks (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/study-finds-ai-models-store-memories-and-logic-in-different-neural-regions/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Basic arithmetic ability lives in the memorization pathways, not logic circuits.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Benj Edwards / Kirillm via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;When engineers build AI language models like GPT-5 from training data, at least two major processing features emerge: memorization (reciting exact text they’ve seen before, like famous quotes or passages from books) and reasoning (solving new problems using general principles). New research from AI startup Goodfire.ai provides the first potentially clear evidence that these different functions actually work through completely separate neural pathways in the model’s architecture.&lt;/p&gt;
&lt;p&gt;The researchers discovered that this separation proves remarkably clean. In a preprint paper released in late October, they described that when they removed the memorization pathways, models lost 97 percent of their ability to recite training data verbatim but kept nearly all their “logical reasoning” ability intact.&lt;/p&gt;
&lt;p&gt;For example, at layer 22 in Allen Institute for AI’s OLMo-7B language model, the bottom 50 percent of weight components showed 23 percent higher activation on memorized data, while the top 10 percent showed 26 percent higher activation on general, non-memorized text. This mechanistic split enabled the researchers to surgically remove memorization while preserving other capabilities.&lt;/p&gt;
&lt;p&gt;Perhaps most surprisingly, the researchers found that arithmetic operations seem to share the same neural pathways as memorization rather than logical reasoning. When they removed memorization circuits, mathematical performance plummeted to 66 percent while logical tasks remained nearly untouched. This discovery may explain why AI language models notoriously struggle with math without the use of external tools. They’re attempting to recall arithmetic from a limited memorization table rather than computing it, like a student who memorized times tables but never learned how multiplication works. The finding suggests that at current scales, language models treat “2+2=4” more like a memorized fact than a logical operation.&lt;/p&gt;
&lt;p&gt;It’s worth noting that “reasoning” in AI research covers a spectrum of abilities that don’t necessarily match what we might call reasoning in humans. The logical reasoning that survived memory removal in this latest research includes tasks like evaluating true/false statements and following if-then rules, which are essentially applying learned patterns to new inputs. This also differs from the deeper “mathematical reasoning” required for proofs or novel problem-solving, which current AI models struggle with even when their pattern-matching abilities remain intact.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Looking ahead, if the information removal techniques receive further development in the future, AI companies could potentially one day remove, say, copyrighted content, private information, or harmful memorized text from a neural network without destroying the model’s ability to perform transformative tasks. However, since neural networks store information in distributed ways that are still not completely understood, for the time being, the researchers say their method “cannot guarantee complete elimination of sensitive information.” These are early steps in a new research direction for AI.&lt;/p&gt;
&lt;h2&gt;Traveling the neural landscape&lt;/h2&gt;
&lt;p&gt;To understand how researchers from Goodfire distinguished memorization from reasoning in these neural networks, it helps to know about a concept in AI called the “loss landscape.” The “loss landscape” is a way of visualizing how wrong or right an AI model’s predictions are as you adjust its internal settings (which are called “weights”).&lt;/p&gt;
&lt;p&gt;Imagine you’re tuning a complex machine with millions of dials. The “loss” measures the number of mistakes the machine makes. High loss means many errors, low loss means few errors. The “landscape” is what you’d see if you could map out the error rate for every possible combination of dial settings.&lt;/p&gt;
&lt;p&gt;During training, AI models essentially “roll downhill” in this landscape (gradient descent), adjusting their weights to find the valleys where they make the fewest mistakes. This process provides AI model outputs, like answers to questions.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126528 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 1: Overview of our approach. We collect activations and gradients from a sample of training data (a), which allows us to approximate loss curvature w.r.t. a weight matrix using K-FAC (b). We decompose these weight matrices into components (each the same size as the matrix), ordered from high to low curvature. In language models, we show that data from different tasks interacts with parts of the spectrum of components differently (c)." class="center large" height="705" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig1_curve-1024x705.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 1 from the paper “From Memorization to Reasoning in the Spectrum of Loss Curvature.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Merullo et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers analyzed the “curvature” of the loss landscapes of particular AI language models, measuring how sensitive the model’s performance is to small changes in different neural network weights. Sharp peaks and valleys represent high curvature (where tiny changes cause big effects), while flat plains represent low curvature (where changes have minimal impact).&lt;/p&gt;
&lt;p&gt;Using a technique called K-FAC (Kronecker-Factored Approximate Curvature), they found that individual memorized facts create sharp spikes in this landscape, but because each memorized item spikes in a different direction, when averaged together they create a flat profile. Meanwhile, reasoning abilities that many different inputs rely on maintain consistent moderate curves across the landscape, like rolling hills that remain roughly the same shape regardless of the direction from which you approach them.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Directions that implement shared mechanisms used by many inputs add coherently and remain high-curvature on average,” the researchers write, describing reasoning pathways. In contrast, memorization uses “idiosyncratic sharp directions associated with specific examples” that appear flat when averaged across data.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Different tasks reveal a spectrum of mechanisms&lt;/h2&gt;
&lt;p&gt;The researchers tested their technique on multiple AI systems to verify the findings held across different architectures. They primarily used Allen Institute’s OLMo-2 family of open language models, specifically the 7-billion and 1-billion parameter versions, chosen because their training data is openly accessible. For vision models, they trained custom 86-million parameter Vision Transformers (ViT-Base models) on ImageNet with intentionally mislabeled data to create controlled memorization. They also validated their findings against existing memorization removal methods like BalancedSubnet to establish performance benchmarks.&lt;/p&gt;
&lt;p&gt;The team tested their discovery by selectively removing low-curvature weight components from these trained models. Memorized content dropped to 3.4 percent recall from nearly 100 percent. Meanwhile, logical reasoning tasks maintained 95 to 106 percent of baseline performance.&lt;/p&gt;
&lt;p&gt;These logical tasks included Boolean expression evaluation, logical deduction puzzles where solvers must track relationships like “if A is taller than B,” object tracking through multiple swaps, and benchmarks like BoolQ for yes/no reasoning, Winogrande for common sense inference, and OpenBookQA for science questions requiring reasoning from provided facts. Some tasks fell between these extremes, revealing a spectrum of mechanisms.&lt;/p&gt;
&lt;p&gt;Mathematical operations and closed-book fact retrieval shared pathways with memorization, dropping to 66 to 86 percent performance after editing. The researchers found arithmetic particularly brittle. Even when models generated identical reasoning chains, they failed at the calculation step after low-curvature components were removed.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126527 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 3: Sensitivity of different kinds of tasks to ablation of flatter eigenvectors. Parametric knowledge retrieval, arithmetic, and memorization are brittle, but openbook fact retrieval and logical reasoning is robust and maintain around 100% of original performance." class="center large" height="574" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig3_curve-1024x574.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 3 from the paper “From Memorization to Reasoning in the Spectrum of Loss Curvature.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Merullo et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;“Arithmetic problems themselves are memorized at the 7B scale, or because they require narrowly used directions to do precise calculations,” the team explains. Open-book question answering, which relies on provided context rather than internal knowledge, proved most robust to the editing procedure, maintaining nearly full performance.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Curiously, the mechanism separation varied by information type. Common facts like country capitals barely changed after editing, while rare facts like company CEOs dropped 78 percent. This suggests models allocate distinct neural resources based on how frequently information appears in training.&lt;/p&gt;
&lt;p&gt;The K-FAC technique outperformed existing memorization removal methods without needing training examples of memorized content. On unseen historical quotes, K-FAC achieved 16.1 percent memorization versus 60 percent for the previous best method, BalancedSubnet.&lt;/p&gt;
&lt;p&gt;Vision transformers showed similar patterns. When trained with intentionally mislabeled images, the models developed distinct pathways for memorizing wrong labels versus learning correct patterns. Removing memorization pathways restored 66.5 percent accuracy on previously mislabeled images.&lt;/p&gt;
&lt;h2&gt;Limits of memory removal&lt;/h2&gt;
&lt;p&gt;However, the researchers acknowledged that their technique isn’t perfect. Once-removed memories might return if the model receives more training, as other research has shown that current unlearning methods only suppress information rather than completely erasing it from the neural network’s weights. That means the “forgotten” content can be reactivated with just a few training steps targeting those suppressed areas.&lt;/p&gt;
&lt;p&gt;The researchers also can’t fully explain why some abilities, like math, break so easily when memorization is removed. It’s unclear whether the model actually memorized all its arithmetic or whether math just happens to use similar neural circuits as memorization. Additionally, some sophisticated capabilities might look like memorization to their detection method, even when they’re actually complex reasoning patterns. Finally, the mathematical tools they use to measure the model’s “landscape” can become unreliable at the extremes, though this doesn’t affect the actual editing process.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Basic arithmetic ability lives in the memorization pathways, not logic circuits.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Robot sitting on a bunch of books, reading a book, looking surprised." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Benj Edwards / Kirillm via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;When engineers build AI language models like GPT-5 from training data, at least two major processing features emerge: memorization (reciting exact text they’ve seen before, like famous quotes or passages from books) and reasoning (solving new problems using general principles). New research from AI startup Goodfire.ai provides the first potentially clear evidence that these different functions actually work through completely separate neural pathways in the model’s architecture.&lt;/p&gt;
&lt;p&gt;The researchers discovered that this separation proves remarkably clean. In a preprint paper released in late October, they described that when they removed the memorization pathways, models lost 97 percent of their ability to recite training data verbatim but kept nearly all their “logical reasoning” ability intact.&lt;/p&gt;
&lt;p&gt;For example, at layer 22 in Allen Institute for AI’s OLMo-7B language model, the bottom 50 percent of weight components showed 23 percent higher activation on memorized data, while the top 10 percent showed 26 percent higher activation on general, non-memorized text. This mechanistic split enabled the researchers to surgically remove memorization while preserving other capabilities.&lt;/p&gt;
&lt;p&gt;Perhaps most surprisingly, the researchers found that arithmetic operations seem to share the same neural pathways as memorization rather than logical reasoning. When they removed memorization circuits, mathematical performance plummeted to 66 percent while logical tasks remained nearly untouched. This discovery may explain why AI language models notoriously struggle with math without the use of external tools. They’re attempting to recall arithmetic from a limited memorization table rather than computing it, like a student who memorized times tables but never learned how multiplication works. The finding suggests that at current scales, language models treat “2+2=4” more like a memorized fact than a logical operation.&lt;/p&gt;
&lt;p&gt;It’s worth noting that “reasoning” in AI research covers a spectrum of abilities that don’t necessarily match what we might call reasoning in humans. The logical reasoning that survived memory removal in this latest research includes tasks like evaluating true/false statements and following if-then rules, which are essentially applying learned patterns to new inputs. This also differs from the deeper “mathematical reasoning” required for proofs or novel problem-solving, which current AI models struggle with even when their pattern-matching abilities remain intact.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Looking ahead, if the information removal techniques receive further development in the future, AI companies could potentially one day remove, say, copyrighted content, private information, or harmful memorized text from a neural network without destroying the model’s ability to perform transformative tasks. However, since neural networks store information in distributed ways that are still not completely understood, for the time being, the researchers say their method “cannot guarantee complete elimination of sensitive information.” These are early steps in a new research direction for AI.&lt;/p&gt;
&lt;h2&gt;Traveling the neural landscape&lt;/h2&gt;
&lt;p&gt;To understand how researchers from Goodfire distinguished memorization from reasoning in these neural networks, it helps to know about a concept in AI called the “loss landscape.” The “loss landscape” is a way of visualizing how wrong or right an AI model’s predictions are as you adjust its internal settings (which are called “weights”).&lt;/p&gt;
&lt;p&gt;Imagine you’re tuning a complex machine with millions of dials. The “loss” measures the number of mistakes the machine makes. High loss means many errors, low loss means few errors. The “landscape” is what you’d see if you could map out the error rate for every possible combination of dial settings.&lt;/p&gt;
&lt;p&gt;During training, AI models essentially “roll downhill” in this landscape (gradient descent), adjusting their weights to find the valleys where they make the fewest mistakes. This process provides AI model outputs, like answers to questions.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126528 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 1: Overview of our approach. We collect activations and gradients from a sample of training data (a), which allows us to approximate loss curvature w.r.t. a weight matrix using K-FAC (b). We decompose these weight matrices into components (each the same size as the matrix), ordered from high to low curvature. In language models, we show that data from different tasks interacts with parts of the spectrum of components differently (c)." class="center large" height="705" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig1_curve-1024x705.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 1 from the paper “From Memorization to Reasoning in the Spectrum of Loss Curvature.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Merullo et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers analyzed the “curvature” of the loss landscapes of particular AI language models, measuring how sensitive the model’s performance is to small changes in different neural network weights. Sharp peaks and valleys represent high curvature (where tiny changes cause big effects), while flat plains represent low curvature (where changes have minimal impact).&lt;/p&gt;
&lt;p&gt;Using a technique called K-FAC (Kronecker-Factored Approximate Curvature), they found that individual memorized facts create sharp spikes in this landscape, but because each memorized item spikes in a different direction, when averaged together they create a flat profile. Meanwhile, reasoning abilities that many different inputs rely on maintain consistent moderate curves across the landscape, like rolling hills that remain roughly the same shape regardless of the direction from which you approach them.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Directions that implement shared mechanisms used by many inputs add coherently and remain high-curvature on average,” the researchers write, describing reasoning pathways. In contrast, memorization uses “idiosyncratic sharp directions associated with specific examples” that appear flat when averaged across data.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Different tasks reveal a spectrum of mechanisms&lt;/h2&gt;
&lt;p&gt;The researchers tested their technique on multiple AI systems to verify the findings held across different architectures. They primarily used Allen Institute’s OLMo-2 family of open language models, specifically the 7-billion and 1-billion parameter versions, chosen because their training data is openly accessible. For vision models, they trained custom 86-million parameter Vision Transformers (ViT-Base models) on ImageNet with intentionally mislabeled data to create controlled memorization. They also validated their findings against existing memorization removal methods like BalancedSubnet to establish performance benchmarks.&lt;/p&gt;
&lt;p&gt;The team tested their discovery by selectively removing low-curvature weight components from these trained models. Memorized content dropped to 3.4 percent recall from nearly 100 percent. Meanwhile, logical reasoning tasks maintained 95 to 106 percent of baseline performance.&lt;/p&gt;
&lt;p&gt;These logical tasks included Boolean expression evaluation, logical deduction puzzles where solvers must track relationships like “if A is taller than B,” object tracking through multiple swaps, and benchmarks like BoolQ for yes/no reasoning, Winogrande for common sense inference, and OpenBookQA for science questions requiring reasoning from provided facts. Some tasks fell between these extremes, revealing a spectrum of mechanisms.&lt;/p&gt;
&lt;p&gt;Mathematical operations and closed-book fact retrieval shared pathways with memorization, dropping to 66 to 86 percent performance after editing. The researchers found arithmetic particularly brittle. Even when models generated identical reasoning chains, they failed at the calculation step after low-curvature components were removed.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126527 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 3: Sensitivity of different kinds of tasks to ablation of flatter eigenvectors. Parametric knowledge retrieval, arithmetic, and memorization are brittle, but openbook fact retrieval and logical reasoning is robust and maintain around 100% of original performance." class="center large" height="574" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig3_curve-1024x574.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 3 from the paper “From Memorization to Reasoning in the Spectrum of Loss Curvature.”

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Merullo et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;“Arithmetic problems themselves are memorized at the 7B scale, or because they require narrowly used directions to do precise calculations,” the team explains. Open-book question answering, which relies on provided context rather than internal knowledge, proved most robust to the editing procedure, maintaining nearly full performance.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Curiously, the mechanism separation varied by information type. Common facts like country capitals barely changed after editing, while rare facts like company CEOs dropped 78 percent. This suggests models allocate distinct neural resources based on how frequently information appears in training.&lt;/p&gt;
&lt;p&gt;The K-FAC technique outperformed existing memorization removal methods without needing training examples of memorized content. On unseen historical quotes, K-FAC achieved 16.1 percent memorization versus 60 percent for the previous best method, BalancedSubnet.&lt;/p&gt;
&lt;p&gt;Vision transformers showed similar patterns. When trained with intentionally mislabeled images, the models developed distinct pathways for memorizing wrong labels versus learning correct patterns. Removing memorization pathways restored 66.5 percent accuracy on previously mislabeled images.&lt;/p&gt;
&lt;h2&gt;Limits of memory removal&lt;/h2&gt;
&lt;p&gt;However, the researchers acknowledged that their technique isn’t perfect. Once-removed memories might return if the model receives more training, as other research has shown that current unlearning methods only suppress information rather than completely erasing it from the neural network’s weights. That means the “forgotten” content can be reactivated with just a few training steps targeting those suppressed areas.&lt;/p&gt;
&lt;p&gt;The researchers also can’t fully explain why some abilities, like math, break so easily when memorization is removed. It’s unclear whether the model actually memorized all its arithmetic or whether math just happens to use similar neural circuits as memorization. Additionally, some sophisticated capabilities might look like memorization to their detection method, even when they’re actually complex reasoning patterns. Finally, the mathematical tools they use to measure the model’s “landscape” can become unreliable at the extremes, though this doesn’t affect the actual editing process.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/study-finds-ai-models-store-memories-and-logic-in-different-neural-regions/</guid><pubDate>Mon, 10 Nov 2025 23:06:42 +0000</pubDate></item><item><title>The circular money problem at the heart of AI’s biggest deals (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/the-circular-money-problem-at-the-heart-of-ais-biggest-deals/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30660881"&gt;





&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;SoftBank and OpenAI announced&amp;nbsp;a new 50-50 joint venture&amp;nbsp;this week to sell enterprise AI tools in Japan under the brand “Crystal Intelligence.” On paper,&amp;nbsp;it’s&amp;nbsp;a straightforward international expansion deal. But SoftBank’s role as a major investor in OpenAI is raising questions about whether AI’s biggest deals are creating real economic value or just moving money in circles.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Kirsten Korosec, Anthony&amp;nbsp;Ha,&amp;nbsp;and AI editor Russell Brandom break down why this deal has people skeptical, and what it signals about the sustainability of AI’s current investment model, and more.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X and Threads, at @EquityPod.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30660881"&gt;





&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;SoftBank and OpenAI announced&amp;nbsp;a new 50-50 joint venture&amp;nbsp;this week to sell enterprise AI tools in Japan under the brand “Crystal Intelligence.” On paper,&amp;nbsp;it’s&amp;nbsp;a straightforward international expansion deal. But SoftBank’s role as a major investor in OpenAI is raising questions about whether AI’s biggest deals are creating real economic value or just moving money in circles.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Kirsten Korosec, Anthony&amp;nbsp;Ha,&amp;nbsp;and AI editor Russell Brandom break down why this deal has people skeptical, and what it signals about the sustainability of AI’s current investment model, and more.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X and Threads, at @EquityPod.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/the-circular-money-problem-at-the-heart-of-ais-biggest-deals/</guid><pubDate>Mon, 10 Nov 2025 23:32:36 +0000</pubDate></item><item><title>Lovable says it’s nearing 8 million users as the year-old AI coding startup eyes more corporate employees (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/10/lovable-says-its-nearing-8-million-users-as-the-year-old-ai-coding-startup-eyes-more-corporate-employees/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-10-at-11.20.57-PM.png?resize=1200,654" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Lovable, the Stockholm-based AI coding platform, is closing in on 8 million users, CEO Anton Osika told this editor during a sit-down on Monday, a major jump from the 2.3 million active users number the company shared in July. Osika said the company — which was founded almost exactly one year ago — is also seeing “100,000 new products built on Lovable every single day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The metrics suggest rapid growth of the startup, which has raised $228 million in total funding to date, including a $200 million round this summer that valued the company at $1.8 billion. Rumors have swirled in recent weeks — potentially sparked by its own investors — that new backers want to invest at a $5 billion valuation, though Osika said the company isn’t capital constrained and declined to discuss fundraising plans.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speaking to me onstage at the Web Summit event in Lisbon, Osika notably didn’t share another number: Lovable’s current annual recurring revenue. The company, which uses a mix of free and paid tiers, hit $100 million in ARR this June, a milestone it trumpeted publicly. But questions have emerged since about whether the vibe coding boom is sustainable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Research from Barclays this summer, along with Google Trends data, showed that traffic to some of the buzziest services, including Lovable and Vercel’s v0, had declined after peaking earlier this year. (Traffic to Lovable was down 40% as of September, according to the Barclays analysts.) “This waning traffic begs the question on whether app/site vibecoding has peaked out already or has just had a bit of a lull before interest ramps up,” they reportedly wrote in a note to investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Osika insisted retention remains strong, citing more than 100% net dollar retention — meaning users spend more over time. He also said the company has “just passed” the 100-employee mark and is now importing leadership talent from San Francisco to bolster its Stockholm headquarters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable emerged from GPT Engineer, an open source tool Osika built that went viral among developers. But he says he quickly realized the bigger opportunity lay with the 99% of people who don’t know how to code. “I woke up a few days after building GPT Engineer and I realized, look, we’re going to reimagine how you build software,” Osika said. “I biked to my co-founder’s place, and I said, I have this great idea. I woke him up.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The platform has attracted an eclectic user base. More than half of Fortune 500 companies are using Lovable to “supercharge creativity,” according to Osika. At the same time, he said, an 11-year-old in Lisbon built a Facebook clone for his school, while a Swedish duo is making $700,000 annually from a startup they launched seven months ago on the platform. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“What I hear from people trying Lovable is, ‘It just works,’” Osika said, crediting what he described as Swedish design sensibility.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Security remains a thornier issue for the vibe coding sector. When I raised a recent incident in which an app built with vibe coding tools leaked 72,000 images into the wild, including GPS data and user IDs, Osika acknowledged the problem. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The part of the engineering organization where we’re moving the quickest on hiring is security engineers,” he said, adding that his goal is to make building with Lovable “more secure than building with just human-written code.” In fact, he said, before users can deploy, Lovable now runs multiple security checks, though the platform still requires users building sensitive applications — banking apps, for instance — to hire security experts, just as they would with traditional development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Osika was similarly matter-of-fact when I asked about competition from OpenAI and Anthropic, the AI giants whose models power Lovable but that have also released their own coding agents. He sees the market as big enough for multiple winners. “If we can unlock more human creativity and human agency  . . . and just driving the change so that anyone can create if they have good ideas, [and] build businesses on top of that, that should be celebrated, regardless of whoever does that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a decidedly collegial stance in an industry not known for it. (Even Osika has engaged in some light social media sparring with Amjad Masad of competitor Replit.) But he said his focus right now is on building “the most intuitive experience for humans” rather than obsessing over rivals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Osika described Lovable’s mission as building “the last piece of software” — a platform where everything a product organization needs, from understanding users to deploying mission-critical features, can be done through a simple interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Demo, don’t memo,” a popular phrase among product leaders, captures how companies now use Lovable, he said. Employees can now quickly prototype ideas rather than writing long presentations, then test them with early users before committing resources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For all the hypergrowth and investor attention, Osika — dressed simply in a beige T-shirt and matching button-down, floppy hair framing his face — appeared very much at ease. The 30-something former particle physicist, who was the first employee at Sauna Labs before founding Lovable, has gone from open source developer to venture-backed founder to must-have conference guest in rapid succession. Yet he seemed more interested in discussing European work culture than dwelling on his company’s trajectory or the attention suddenly being showered on him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What I care about is that everyone who’s at the company, they’re mission driven, they really care about what they’re doing and how we as a team succeed,” he said, pushing back against Silicon Valley’s intensifying hustle culture. “The best people in my team today, most of them, they have kids, and they really, really care about what we’re doing. They’re not working 12 hours, six days a week.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though he added: “Although it’s a startup, so they’re probably working more than most jobs.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-10-at-11.20.57-PM.png?resize=1200,654" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Lovable, the Stockholm-based AI coding platform, is closing in on 8 million users, CEO Anton Osika told this editor during a sit-down on Monday, a major jump from the 2.3 million active users number the company shared in July. Osika said the company — which was founded almost exactly one year ago — is also seeing “100,000 new products built on Lovable every single day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The metrics suggest rapid growth of the startup, which has raised $228 million in total funding to date, including a $200 million round this summer that valued the company at $1.8 billion. Rumors have swirled in recent weeks — potentially sparked by its own investors — that new backers want to invest at a $5 billion valuation, though Osika said the company isn’t capital constrained and declined to discuss fundraising plans.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speaking to me onstage at the Web Summit event in Lisbon, Osika notably didn’t share another number: Lovable’s current annual recurring revenue. The company, which uses a mix of free and paid tiers, hit $100 million in ARR this June, a milestone it trumpeted publicly. But questions have emerged since about whether the vibe coding boom is sustainable.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Research from Barclays this summer, along with Google Trends data, showed that traffic to some of the buzziest services, including Lovable and Vercel’s v0, had declined after peaking earlier this year. (Traffic to Lovable was down 40% as of September, according to the Barclays analysts.) “This waning traffic begs the question on whether app/site vibecoding has peaked out already or has just had a bit of a lull before interest ramps up,” they reportedly wrote in a note to investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Osika insisted retention remains strong, citing more than 100% net dollar retention — meaning users spend more over time. He also said the company has “just passed” the 100-employee mark and is now importing leadership talent from San Francisco to bolster its Stockholm headquarters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lovable emerged from GPT Engineer, an open source tool Osika built that went viral among developers. But he says he quickly realized the bigger opportunity lay with the 99% of people who don’t know how to code. “I woke up a few days after building GPT Engineer and I realized, look, we’re going to reimagine how you build software,” Osika said. “I biked to my co-founder’s place, and I said, I have this great idea. I woke him up.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The platform has attracted an eclectic user base. More than half of Fortune 500 companies are using Lovable to “supercharge creativity,” according to Osika. At the same time, he said, an 11-year-old in Lisbon built a Facebook clone for his school, while a Swedish duo is making $700,000 annually from a startup they launched seven months ago on the platform. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“What I hear from people trying Lovable is, ‘It just works,’” Osika said, crediting what he described as Swedish design sensibility.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Security remains a thornier issue for the vibe coding sector. When I raised a recent incident in which an app built with vibe coding tools leaked 72,000 images into the wild, including GPS data and user IDs, Osika acknowledged the problem. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The part of the engineering organization where we’re moving the quickest on hiring is security engineers,” he said, adding that his goal is to make building with Lovable “more secure than building with just human-written code.” In fact, he said, before users can deploy, Lovable now runs multiple security checks, though the platform still requires users building sensitive applications — banking apps, for instance — to hire security experts, just as they would with traditional development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Osika was similarly matter-of-fact when I asked about competition from OpenAI and Anthropic, the AI giants whose models power Lovable but that have also released their own coding agents. He sees the market as big enough for multiple winners. “If we can unlock more human creativity and human agency  . . . and just driving the change so that anyone can create if they have good ideas, [and] build businesses on top of that, that should be celebrated, regardless of whoever does that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a decidedly collegial stance in an industry not known for it. (Even Osika has engaged in some light social media sparring with Amjad Masad of competitor Replit.) But he said his focus right now is on building “the most intuitive experience for humans” rather than obsessing over rivals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Osika described Lovable’s mission as building “the last piece of software” — a platform where everything a product organization needs, from understanding users to deploying mission-critical features, can be done through a simple interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Demo, don’t memo,” a popular phrase among product leaders, captures how companies now use Lovable, he said. Employees can now quickly prototype ideas rather than writing long presentations, then test them with early users before committing resources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For all the hypergrowth and investor attention, Osika — dressed simply in a beige T-shirt and matching button-down, floppy hair framing his face — appeared very much at ease. The 30-something former particle physicist, who was the first employee at Sauna Labs before founding Lovable, has gone from open source developer to venture-backed founder to must-have conference guest in rapid succession. Yet he seemed more interested in discussing European work culture than dwelling on his company’s trajectory or the attention suddenly being showered on him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What I care about is that everyone who’s at the company, they’re mission driven, they really care about what they’re doing and how we as a team succeed,” he said, pushing back against Silicon Valley’s intensifying hustle culture. “The best people in my team today, most of them, they have kids, and they really, really care about what we’re doing. They’re not working 12 hours, six days a week.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though he added: “Although it’s a startup, so they’re probably working more than most jobs.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/10/lovable-says-its-nearing-8-million-users-as-the-year-old-ai-coding-startup-eyes-more-corporate-employees/</guid><pubDate>Mon, 10 Nov 2025 23:53:01 +0000</pubDate></item><item><title>[NEW] Understanding the nuances of human-like intelligence (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/understanding-nuances-human-intelligence-phillip-isola-1111</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Phillip-Isola-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;What can we learn about human intelligence by studying how machines “think?” Can we better understand ourselves if we better understand the artificial intelligence systems that are becoming a more significant part of our everyday lives?&lt;/p&gt;&lt;p&gt;These questions may be deeply philosophical, but for Phillip Isola, finding the answers is as much about computation as it is about cogitation.&lt;/p&gt;&lt;p&gt;Isola, the newly tenured associate professor in the Department of Electrical Engineering and Computer Science (EECS), studies the fundamental mechanisms involved in human-like intelligence from a computational perspective.&lt;/p&gt;&lt;p&gt;While understanding intelligence is the overarching goal, his work focuses mainly on computer vision and machine learning. Isola is particularly interested in exploring how intelligence emerges in AI models, how these models learn to represent the world around them, and what their “brains” share with the brains of their human creators.&lt;/p&gt;&lt;p&gt;“I see all the different kinds of intelligence as having a lot of commonalities, and I’d like to understand those commonalities. What is it that all animals, humans, and AIs have in common?” says Isola, who is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).&lt;/p&gt;&lt;p&gt;To Isola, a better scientific understanding of the intelligence that AI agents possess will help the world integrate them safely and effectively into society, maximizing their potential to benefit humanity.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Asking questions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Isola began pondering scientific questions at a young age.&lt;/p&gt;&lt;p&gt;While growing up in San Francisco, he and his father frequently went hiking along the northern California coastline or camping around Point Reyes and in the hills of Marin County.&lt;/p&gt;&lt;p&gt;He was fascinated by geological processes and often wondered what made the natural world work. In school, Isola was driven by an insatiable curiosity, and while he gravitated toward technical subjects like math and science, there was no limit to what he wanted to learn.&lt;/p&gt;&lt;p&gt;Not entirely sure what to study as an undergraduate at Yale University, Isola dabbled until he came upon cognitive sciences.&lt;/p&gt;&lt;p&gt;“My earlier interest had been with nature — how the world works. But then I realized that the brain was even more interesting, and more complex than even the formation of the planets. Now, I wanted to know what makes us tick,” he says.&lt;/p&gt;&lt;p&gt;As a first-year student, he started working in the lab of his cognitive sciences professor and soon-to-be mentor, Brian Scholl, a member of the Yale Department of Psychology. He remained in that lab throughout his time as an undergraduate.&lt;/p&gt;&lt;p&gt;After spending a gap year working with some childhood friends at an indie video game company, Isola was ready to dive back into the complex world of the human brain. He enrolled in the graduate program in brain and cognitive sciences at MIT.&lt;/p&gt;&lt;p&gt;“Grad school was where I felt like I finally found my place. I had a lot of great experiences at Yale and in other phases of my life, but when I got to MIT, I realized this was the work I really loved and these are the people who think similarly to me,” he says.&lt;/p&gt;&lt;p&gt;Isola credits his PhD advisor, Ted Adelson, the John and Dorothy Wilson Professor of Vision Science, as a major influence on his future path. He was inspired by Adelson’s focus on understanding fundamental principles, rather than only chasing new engineering benchmarks, which are formalized tests used to measure the performance of a system.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A computational perspective&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;At MIT, Isola’s research drifted toward computer science and artificial intelligence.&lt;/p&gt;&lt;p&gt;“I still loved all those questions from cognitive sciences, but I felt I could make more progress on some of those questions if I came at it from a purely computational perspective,” he says.&lt;/p&gt;&lt;p&gt;His thesis was focused on perceptual grouping, which involves the mechanisms people and machines use to organize discrete parts of an image as a single, coherent object.&lt;/p&gt;&lt;p&gt;If machines can learn perceptual groupings on their own, that could enable AI systems to recognize objects without human intervention. This type of self-supervised learning has applications in areas such autonomous vehicles, medical imaging, robotics, and automatic language translation.&lt;/p&gt;&lt;p&gt;After graduating from MIT, Isola completed a postdoc at the University of California at Berkeley so he could broaden his perspectives by working in a lab solely focused on computer science.&lt;/p&gt;&lt;p&gt;“That experience helped my work become a lot more impactful because I learned to balance understanding fundamental, abstract principles of intelligence with the pursuit of some more concrete benchmarks,” Isola recalls.&lt;/p&gt;&lt;p&gt;At Berkeley, he developed image-to-image translation frameworks, an early form of generative AI model that could turn a sketch into a photographic image, for instance, or turn a black-and-white photo into a color one.&lt;/p&gt;&lt;p&gt;He entered the academic job market and accepted a faculty position at MIT, but Isola deferred for a year to work at a then-small startup called OpenAI.&lt;/p&gt;&lt;p&gt;“It was a nonprofit, and I liked the idealistic mission at that time. They were really good at reinforcement learning, and I thought that seemed like an important topic to learn more about,” he says.&lt;/p&gt;&lt;p&gt;He enjoyed working in a lab with so much scientific freedom, but after a year Isola was ready to return to MIT and start his own research group.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Studying human-like intelligence&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Running a research lab instantly appealed to him.&lt;/p&gt;&lt;p&gt;“I really love the early stage of an idea. I feel like I am a sort of startup incubator where I am constantly able to do new things and learn new things,” he says.&lt;/p&gt;&lt;p&gt;Building on his interest in cognitive sciences and desire to understand the human brain, his group studies the fundamental computations involved in the human-like intelligence that emerges in machines.&lt;/p&gt;&lt;p&gt;One primary focus is representation learning, or the ability of humans and machines to represent and perceive the sensory world around them.&lt;/p&gt;&lt;p&gt;In recent work, he and his collaborators observed that the many varied types of machine-learning models, from LLMs to computer vision models to audio models, seem to represent the world in similar ways.&lt;/p&gt;&lt;p&gt;These models are designed to do vastly different tasks, but there are many similarities in their architectures. And as they get bigger and are trained on more data, their internal structures become more alike.&lt;/p&gt;&lt;p&gt;This led Isola and his team to introduce the Platonic Representation Hypothesis (drawing its name from the Greek philosopher Plato) which says that the representations all these models learn are converging toward a shared, underlying representation of reality.&lt;/p&gt;&lt;p&gt;“Language, images, sound — all of these are different shadows on the wall from which you can infer that there is some kind of underlying physical process — some kind of causal reality — out there. If you train models on all these different types of data, they should converge on that world model in the end,” Isola says.&lt;/p&gt;&lt;p&gt;A related area his team studies is self-supervised learning. This involves the ways in which AI models learn to group related pixels in an image or words in a sentence without having labeled examples to learn from.&lt;/p&gt;&lt;p&gt;Because data are expensive and labels are limited, using only labeled data to train models could hold back the capabilities of AI systems. With self-supervised learning, the goal is to develop models that can come up with an accurate internal representation of the world on their own.&lt;/p&gt;&lt;p&gt;“If you can come up with a good representation of the world, that should make subsequent problem solving easier,” he explains.&lt;/p&gt;&lt;p&gt;The focus of Isola’s research is more about finding something new and surprising than about building complex systems that can outdo the latest machine-learning benchmarks.&lt;/p&gt;&lt;p&gt;While this approach has yielded much success in uncovering innovative techniques and architectures, it means the work sometimes lacks a concrete end goal, which can lead to challenges.&lt;/p&gt;&lt;p&gt;For instance, keeping a team aligned and the funding flowing can be difficult when the lab is focused on searching for unexpected results, he says.&lt;/p&gt;&lt;p&gt;“In a sense, we are always working in the dark. It is high-risk and high-reward work. Every once in while, we find some kernel of truth that is new and surprising,” he says.&lt;/p&gt;&lt;p&gt;In addition to pursuing knowledge, Isola is passionate about imparting knowledge to the next generation of scientists and engineers. Among his favorite courses to teach is 6.7960 (Deep Learning), which he and several other MIT faculty members launched four years ago.&lt;/p&gt;&lt;p&gt;The class has seen exponential growth, from 30 students in its initial offering to more than 700 this fall.&lt;/p&gt;&lt;p&gt;And while the popularity of AI means there is no shortage of interested students, the speed at which the field moves can make it difficult to separate the hype from truly significant advances.&lt;/p&gt;&lt;p&gt;“I tell the students they have to take everything we say in the class with a grain of salt. Maybe in a few years we’ll tell them something different. We are really on the edge of knowledge with this course,” he says.&lt;/p&gt;&lt;p&gt;But Isola also emphasizes to students that, for all the hype surrounding the latest AI models, intelligent machines are far simpler than most people suspect.&lt;/p&gt;&lt;p&gt;“Human ingenuity, creativity, and emotions — many people believe these can never be modeled. That might turn out to be true, but I think intelligence is fairly simple once we understand it,” he says.&lt;/p&gt;&lt;p&gt;Even though his current work focuses on deep-learning models, Isola is still fascinated by the complexity of the human brain and continues to collaborate with researchers who study cognitive sciences.&lt;/p&gt;&lt;p&gt;All the while, he has remained captivated by the beauty of the natural world that inspired his first interest in science.&lt;/p&gt;&lt;p&gt;Although he has less time for hobbies these days, Isola enjoys hiking and backpacking in the mountains or on Cape Cod, skiing and kayaking, or finding scenic places to spend time when he travels for scientific conferences.&lt;/p&gt;&lt;p&gt;And while he looks forward to exploring new questions in his lab at MIT, Isola can’t help but contemplate how the role of intelligent machines might change the course of his work.&lt;/p&gt;&lt;p&gt;He believes that artificial general intelligence (AGI), or the point where machines can learn and apply their knowledge as well as humans can, is not that far off.&lt;/p&gt;&lt;p&gt;“I don’t think AIs will just do everything for us and we’ll go and enjoy life at the beach. I think there is going to be this coexistence between smart machines and humans who still have a lot of agency and control. Now, I’m thinking about the interesting questions and applications once that happens. How can I help the world in this post-AGI future? I don’t have any answers yet, but it’s on my mind,” he says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Phillip-Isola-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;What can we learn about human intelligence by studying how machines “think?” Can we better understand ourselves if we better understand the artificial intelligence systems that are becoming a more significant part of our everyday lives?&lt;/p&gt;&lt;p&gt;These questions may be deeply philosophical, but for Phillip Isola, finding the answers is as much about computation as it is about cogitation.&lt;/p&gt;&lt;p&gt;Isola, the newly tenured associate professor in the Department of Electrical Engineering and Computer Science (EECS), studies the fundamental mechanisms involved in human-like intelligence from a computational perspective.&lt;/p&gt;&lt;p&gt;While understanding intelligence is the overarching goal, his work focuses mainly on computer vision and machine learning. Isola is particularly interested in exploring how intelligence emerges in AI models, how these models learn to represent the world around them, and what their “brains” share with the brains of their human creators.&lt;/p&gt;&lt;p&gt;“I see all the different kinds of intelligence as having a lot of commonalities, and I’d like to understand those commonalities. What is it that all animals, humans, and AIs have in common?” says Isola, who is also a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).&lt;/p&gt;&lt;p&gt;To Isola, a better scientific understanding of the intelligence that AI agents possess will help the world integrate them safely and effectively into society, maximizing their potential to benefit humanity.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Asking questions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Isola began pondering scientific questions at a young age.&lt;/p&gt;&lt;p&gt;While growing up in San Francisco, he and his father frequently went hiking along the northern California coastline or camping around Point Reyes and in the hills of Marin County.&lt;/p&gt;&lt;p&gt;He was fascinated by geological processes and often wondered what made the natural world work. In school, Isola was driven by an insatiable curiosity, and while he gravitated toward technical subjects like math and science, there was no limit to what he wanted to learn.&lt;/p&gt;&lt;p&gt;Not entirely sure what to study as an undergraduate at Yale University, Isola dabbled until he came upon cognitive sciences.&lt;/p&gt;&lt;p&gt;“My earlier interest had been with nature — how the world works. But then I realized that the brain was even more interesting, and more complex than even the formation of the planets. Now, I wanted to know what makes us tick,” he says.&lt;/p&gt;&lt;p&gt;As a first-year student, he started working in the lab of his cognitive sciences professor and soon-to-be mentor, Brian Scholl, a member of the Yale Department of Psychology. He remained in that lab throughout his time as an undergraduate.&lt;/p&gt;&lt;p&gt;After spending a gap year working with some childhood friends at an indie video game company, Isola was ready to dive back into the complex world of the human brain. He enrolled in the graduate program in brain and cognitive sciences at MIT.&lt;/p&gt;&lt;p&gt;“Grad school was where I felt like I finally found my place. I had a lot of great experiences at Yale and in other phases of my life, but when I got to MIT, I realized this was the work I really loved and these are the people who think similarly to me,” he says.&lt;/p&gt;&lt;p&gt;Isola credits his PhD advisor, Ted Adelson, the John and Dorothy Wilson Professor of Vision Science, as a major influence on his future path. He was inspired by Adelson’s focus on understanding fundamental principles, rather than only chasing new engineering benchmarks, which are formalized tests used to measure the performance of a system.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A computational perspective&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;At MIT, Isola’s research drifted toward computer science and artificial intelligence.&lt;/p&gt;&lt;p&gt;“I still loved all those questions from cognitive sciences, but I felt I could make more progress on some of those questions if I came at it from a purely computational perspective,” he says.&lt;/p&gt;&lt;p&gt;His thesis was focused on perceptual grouping, which involves the mechanisms people and machines use to organize discrete parts of an image as a single, coherent object.&lt;/p&gt;&lt;p&gt;If machines can learn perceptual groupings on their own, that could enable AI systems to recognize objects without human intervention. This type of self-supervised learning has applications in areas such autonomous vehicles, medical imaging, robotics, and automatic language translation.&lt;/p&gt;&lt;p&gt;After graduating from MIT, Isola completed a postdoc at the University of California at Berkeley so he could broaden his perspectives by working in a lab solely focused on computer science.&lt;/p&gt;&lt;p&gt;“That experience helped my work become a lot more impactful because I learned to balance understanding fundamental, abstract principles of intelligence with the pursuit of some more concrete benchmarks,” Isola recalls.&lt;/p&gt;&lt;p&gt;At Berkeley, he developed image-to-image translation frameworks, an early form of generative AI model that could turn a sketch into a photographic image, for instance, or turn a black-and-white photo into a color one.&lt;/p&gt;&lt;p&gt;He entered the academic job market and accepted a faculty position at MIT, but Isola deferred for a year to work at a then-small startup called OpenAI.&lt;/p&gt;&lt;p&gt;“It was a nonprofit, and I liked the idealistic mission at that time. They were really good at reinforcement learning, and I thought that seemed like an important topic to learn more about,” he says.&lt;/p&gt;&lt;p&gt;He enjoyed working in a lab with so much scientific freedom, but after a year Isola was ready to return to MIT and start his own research group.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Studying human-like intelligence&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Running a research lab instantly appealed to him.&lt;/p&gt;&lt;p&gt;“I really love the early stage of an idea. I feel like I am a sort of startup incubator where I am constantly able to do new things and learn new things,” he says.&lt;/p&gt;&lt;p&gt;Building on his interest in cognitive sciences and desire to understand the human brain, his group studies the fundamental computations involved in the human-like intelligence that emerges in machines.&lt;/p&gt;&lt;p&gt;One primary focus is representation learning, or the ability of humans and machines to represent and perceive the sensory world around them.&lt;/p&gt;&lt;p&gt;In recent work, he and his collaborators observed that the many varied types of machine-learning models, from LLMs to computer vision models to audio models, seem to represent the world in similar ways.&lt;/p&gt;&lt;p&gt;These models are designed to do vastly different tasks, but there are many similarities in their architectures. And as they get bigger and are trained on more data, their internal structures become more alike.&lt;/p&gt;&lt;p&gt;This led Isola and his team to introduce the Platonic Representation Hypothesis (drawing its name from the Greek philosopher Plato) which says that the representations all these models learn are converging toward a shared, underlying representation of reality.&lt;/p&gt;&lt;p&gt;“Language, images, sound — all of these are different shadows on the wall from which you can infer that there is some kind of underlying physical process — some kind of causal reality — out there. If you train models on all these different types of data, they should converge on that world model in the end,” Isola says.&lt;/p&gt;&lt;p&gt;A related area his team studies is self-supervised learning. This involves the ways in which AI models learn to group related pixels in an image or words in a sentence without having labeled examples to learn from.&lt;/p&gt;&lt;p&gt;Because data are expensive and labels are limited, using only labeled data to train models could hold back the capabilities of AI systems. With self-supervised learning, the goal is to develop models that can come up with an accurate internal representation of the world on their own.&lt;/p&gt;&lt;p&gt;“If you can come up with a good representation of the world, that should make subsequent problem solving easier,” he explains.&lt;/p&gt;&lt;p&gt;The focus of Isola’s research is more about finding something new and surprising than about building complex systems that can outdo the latest machine-learning benchmarks.&lt;/p&gt;&lt;p&gt;While this approach has yielded much success in uncovering innovative techniques and architectures, it means the work sometimes lacks a concrete end goal, which can lead to challenges.&lt;/p&gt;&lt;p&gt;For instance, keeping a team aligned and the funding flowing can be difficult when the lab is focused on searching for unexpected results, he says.&lt;/p&gt;&lt;p&gt;“In a sense, we are always working in the dark. It is high-risk and high-reward work. Every once in while, we find some kernel of truth that is new and surprising,” he says.&lt;/p&gt;&lt;p&gt;In addition to pursuing knowledge, Isola is passionate about imparting knowledge to the next generation of scientists and engineers. Among his favorite courses to teach is 6.7960 (Deep Learning), which he and several other MIT faculty members launched four years ago.&lt;/p&gt;&lt;p&gt;The class has seen exponential growth, from 30 students in its initial offering to more than 700 this fall.&lt;/p&gt;&lt;p&gt;And while the popularity of AI means there is no shortage of interested students, the speed at which the field moves can make it difficult to separate the hype from truly significant advances.&lt;/p&gt;&lt;p&gt;“I tell the students they have to take everything we say in the class with a grain of salt. Maybe in a few years we’ll tell them something different. We are really on the edge of knowledge with this course,” he says.&lt;/p&gt;&lt;p&gt;But Isola also emphasizes to students that, for all the hype surrounding the latest AI models, intelligent machines are far simpler than most people suspect.&lt;/p&gt;&lt;p&gt;“Human ingenuity, creativity, and emotions — many people believe these can never be modeled. That might turn out to be true, but I think intelligence is fairly simple once we understand it,” he says.&lt;/p&gt;&lt;p&gt;Even though his current work focuses on deep-learning models, Isola is still fascinated by the complexity of the human brain and continues to collaborate with researchers who study cognitive sciences.&lt;/p&gt;&lt;p&gt;All the while, he has remained captivated by the beauty of the natural world that inspired his first interest in science.&lt;/p&gt;&lt;p&gt;Although he has less time for hobbies these days, Isola enjoys hiking and backpacking in the mountains or on Cape Cod, skiing and kayaking, or finding scenic places to spend time when he travels for scientific conferences.&lt;/p&gt;&lt;p&gt;And while he looks forward to exploring new questions in his lab at MIT, Isola can’t help but contemplate how the role of intelligent machines might change the course of his work.&lt;/p&gt;&lt;p&gt;He believes that artificial general intelligence (AGI), or the point where machines can learn and apply their knowledge as well as humans can, is not that far off.&lt;/p&gt;&lt;p&gt;“I don’t think AIs will just do everything for us and we’ll go and enjoy life at the beach. I think there is going to be this coexistence between smart machines and humans who still have a lot of agency and control. Now, I’m thinking about the interesting questions and applications once that happens. How can I help the world in this post-AGI future? I don’t have any answers yet, but it’s on my mind,” he says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/understanding-nuances-human-intelligence-phillip-isola-1111</guid><pubDate>Tue, 11 Nov 2025 05:00:00 +0000</pubDate></item></channel></rss>