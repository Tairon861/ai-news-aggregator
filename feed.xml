<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 09 Feb 2026 07:08:20 +0000</lastBuildDate><item><title>Okay, I’m slightly less mad about that ‘Magnificent Ambersons’ AI project (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/08/okay-im-slightly-less-mad-about-that-magnificent-ambersons-ai-project/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-3348882.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When a startup announced plans last fall to recreate lost footage from Orson Welles’ classic film “The Magnificent Ambersons” using generative AI, I was skeptical. More than that, I was baffled why anyone would spend time and money on something that seemed guaranteed to outrage cinephiles while offering negligible commercial value.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This week, an in-depth profile by the New Yorker’s Michael Schulman provides more details about the project. If nothing else, it helps explain why the startup Fable and its founder Edward Saatchi are pursuing it: It seems to come from a genuine love of Welles and his work.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Saatchi (whose father was a founder of advertising firm Saatchi &amp;amp; Saatchi) recalled a childhood of watching films in a private screening room with his “movie mad” parents. He said he first saw “Ambersons” when he was twelve.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The profile also explains why “Ambersons,” while much less famous than Welles’ first film “Citizen Kane,” remains so tantalizing — Welles himself claimed it was a “much better picture” than “Kane,” but after a disastrous preview screening, the studio cut 43 minutes from the film, added an abrupt and unconvincing happy ending, and eventually destroyed the excised footage to make space in its vaults.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“To me, this is the holy grail of lost cinema,” Saatchi said. “It just seemed intuitively that there would be some way to undo what had happened.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Saatchi is only the latest Welles devotee to dream of recreating the lost footage. In fact, Fable is working with filmmaker Brian Rose, who already spent years trying to achieve the same thing with animated scenes based on the movie’s script and photographs, and on Welles’ notes. (Rose said that after he screened the results for friends and family, “a lot of them were scratching their heads.”)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So while Fable is using more advanced technology — filming scenes in live action, then eventually overlaying them with digital recreations of the original actors and their voices — this project is best understood as a slicker, better-funded version of Rose’s work. It’s a fan’s attempt to glimpse Welles’ vision.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, while the New Yorker article includes a few clips of Rose’s animations, as well as images of Fable’s AI actors, there’s no footage showing the results of Fable’s live action-AI hybrid.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By the company’s own admission, there are significant challenges, whether that’s fixing obvious blunders like a two-headed version of the actor Joseph Cotten, or the more subjective task of recreating the complex beauty of the film’s cinematography. (Saatchi even described a “happiness” problem, with the AI tending to make the film’s women look inappropriately happy.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for whether this footage will ever be released to the public, Saatchi admitted it was “a total mistake” not to speak to Welles’ estate before his announcement. Since then, he has reportedly been working to win over both the estate and Warner Bros., which owns the rights to the film. Welles’ daughter Beatrice told Schulman that while she remains “skeptical,” she now believes “they are going into this project with enormous respect toward my father and this beautiful movie.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The actor and biographer Simon Callow — who’s currently writing the fourth book in his multi-volume Welles biography — has also agreed to advise the project, which he described as a “great idea.” (Callow is a family friend of the Saatchis.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But not everyone has been convinced. Melissa Galt said her mother, the actress Anne Baxter, would “not have agreed with that at all.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s not the truth,” Galt said. “It’s a creation of someone else’s truth. But it’s not the original, and she was a purist.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And while I’ve become more sympathetic to Saatchi’s aims, I still agree with Galt: At its best, this project will only result in a novelty, a dream of what the movie might have been.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In fact, Galt’s description of her mother’s position that “once the movie was done, it was done,” reminded me of a recent essay in which the writer Aaron Bady compared AI to the vampires in “Sinners.” Bady argued that when it comes to art, both vampires and AI will always come up short, because “what makes art possible” is a knowledge of mortality and limitations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There is no work of art without an ending, without the point at which the work ends (even if the world continues),” he wrote, adding, “Without death, without loss, and without the space between my body and yours, separating my memories from yours, we cannot make art or desire or feeling.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that light, Saatchi’s insistence that there &lt;em&gt;must&lt;/em&gt; be “some way to undo what had happened” feels, if not outright vampiric, then at least a little childish in its unwillingness to accept that some losses are permanent. It may not, perhaps, be all that different from a startup founder claiming they can make grief obsolete — or a studio executive insisting that “The Magnificent Ambersons” needed a happy ending.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-3348882.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When a startup announced plans last fall to recreate lost footage from Orson Welles’ classic film “The Magnificent Ambersons” using generative AI, I was skeptical. More than that, I was baffled why anyone would spend time and money on something that seemed guaranteed to outrage cinephiles while offering negligible commercial value.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This week, an in-depth profile by the New Yorker’s Michael Schulman provides more details about the project. If nothing else, it helps explain why the startup Fable and its founder Edward Saatchi are pursuing it: It seems to come from a genuine love of Welles and his work.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Saatchi (whose father was a founder of advertising firm Saatchi &amp;amp; Saatchi) recalled a childhood of watching films in a private screening room with his “movie mad” parents. He said he first saw “Ambersons” when he was twelve.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The profile also explains why “Ambersons,” while much less famous than Welles’ first film “Citizen Kane,” remains so tantalizing — Welles himself claimed it was a “much better picture” than “Kane,” but after a disastrous preview screening, the studio cut 43 minutes from the film, added an abrupt and unconvincing happy ending, and eventually destroyed the excised footage to make space in its vaults.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“To me, this is the holy grail of lost cinema,” Saatchi said. “It just seemed intuitively that there would be some way to undo what had happened.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Saatchi is only the latest Welles devotee to dream of recreating the lost footage. In fact, Fable is working with filmmaker Brian Rose, who already spent years trying to achieve the same thing with animated scenes based on the movie’s script and photographs, and on Welles’ notes. (Rose said that after he screened the results for friends and family, “a lot of them were scratching their heads.”)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So while Fable is using more advanced technology — filming scenes in live action, then eventually overlaying them with digital recreations of the original actors and their voices — this project is best understood as a slicker, better-funded version of Rose’s work. It’s a fan’s attempt to glimpse Welles’ vision.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, while the New Yorker article includes a few clips of Rose’s animations, as well as images of Fable’s AI actors, there’s no footage showing the results of Fable’s live action-AI hybrid.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By the company’s own admission, there are significant challenges, whether that’s fixing obvious blunders like a two-headed version of the actor Joseph Cotten, or the more subjective task of recreating the complex beauty of the film’s cinematography. (Saatchi even described a “happiness” problem, with the AI tending to make the film’s women look inappropriately happy.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for whether this footage will ever be released to the public, Saatchi admitted it was “a total mistake” not to speak to Welles’ estate before his announcement. Since then, he has reportedly been working to win over both the estate and Warner Bros., which owns the rights to the film. Welles’ daughter Beatrice told Schulman that while she remains “skeptical,” she now believes “they are going into this project with enormous respect toward my father and this beautiful movie.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The actor and biographer Simon Callow — who’s currently writing the fourth book in his multi-volume Welles biography — has also agreed to advise the project, which he described as a “great idea.” (Callow is a family friend of the Saatchis.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But not everyone has been convinced. Melissa Galt said her mother, the actress Anne Baxter, would “not have agreed with that at all.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s not the truth,” Galt said. “It’s a creation of someone else’s truth. But it’s not the original, and she was a purist.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And while I’ve become more sympathetic to Saatchi’s aims, I still agree with Galt: At its best, this project will only result in a novelty, a dream of what the movie might have been.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In fact, Galt’s description of her mother’s position that “once the movie was done, it was done,” reminded me of a recent essay in which the writer Aaron Bady compared AI to the vampires in “Sinners.” Bady argued that when it comes to art, both vampires and AI will always come up short, because “what makes art possible” is a knowledge of mortality and limitations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There is no work of art without an ending, without the point at which the work ends (even if the world continues),” he wrote, adding, “Without death, without loss, and without the space between my body and yours, separating my memories from yours, we cannot make art or desire or feeling.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that light, Saatchi’s insistence that there &lt;em&gt;must&lt;/em&gt; be “some way to undo what had happened” feels, if not outright vampiric, then at least a little childish in its unwillingness to accept that some losses are permanent. It may not, perhaps, be all that different from a startup founder claiming they can make grief obsolete — or a studio executive insisting that “The Magnificent Ambersons” needed a happy ending.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/08/okay-im-slightly-less-mad-about-that-magnificent-ambersons-ai-project/</guid><pubDate>Sun, 08 Feb 2026 19:36:28 +0000</pubDate></item><item><title>Crypto.com places $70M bet on AI.com domain ahead of Super Bowl (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/08/crypto-com-places-70m-bet-on-ai-com-domain-ahead-of-super-bowl/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Screenshot-2026-02-08-at-12.27.26-PM.png?resize=1200,671" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Just in time to create a new Super Bowl ad, Crypto.com founder Kris Marszalek has made the priciest domain purchase in history, buying AI.com for $70 million, according to the Financial Times. The deal, paid entirely in cryptocurrency to an unknown seller, shatters previous records. (Broker Larry Fischer, who facilitated the sale, is presumably celebrating his good fortune.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Marszalek plans to debut the site during Sunday’s big game, offering consumers a personal AI agent for messaging, app usage, and stock trading. “If you take a long-term view — 10 to 20 years – [AI] is going to be one of the greatest technological waves of our lifetime,” he told the FT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The purchase rewrites the domain record books — not that crypto industry itself is known for its restraint when it comes to spending. Previously, CarInsurance.com held the crown at $49.7 million (2010), followed by VacationRentals.com ($35 million in 2007) and Voice.com ($30 million in 2019). Other eye-popping sales include PrivateJet.com ($30 million), 360.com ($17 million), and Sex.com, which has sold twice for over $13 million each time, though its second owner went bankrupt trying to monetize it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With assets like AI.com, there are no substitutes,” Fischer told the FT. “When one becomes available, the opportunity may never present itself again.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether these mega-dollar domains actually deliver returns remains an open question. But for Marszalek, who already owns Crypto.com and dropped $700 million on stadium naming rights, owning two category-defining domains is apparently worth the outlay.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Screenshot-2026-02-08-at-12.27.26-PM.png?resize=1200,671" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Just in time to create a new Super Bowl ad, Crypto.com founder Kris Marszalek has made the priciest domain purchase in history, buying AI.com for $70 million, according to the Financial Times. The deal, paid entirely in cryptocurrency to an unknown seller, shatters previous records. (Broker Larry Fischer, who facilitated the sale, is presumably celebrating his good fortune.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Marszalek plans to debut the site during Sunday’s big game, offering consumers a personal AI agent for messaging, app usage, and stock trading. “If you take a long-term view — 10 to 20 years – [AI] is going to be one of the greatest technological waves of our lifetime,” he told the FT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The purchase rewrites the domain record books — not that crypto industry itself is known for its restraint when it comes to spending. Previously, CarInsurance.com held the crown at $49.7 million (2010), followed by VacationRentals.com ($35 million in 2007) and Voice.com ($30 million in 2019). Other eye-popping sales include PrivateJet.com ($30 million), 360.com ($17 million), and Sex.com, which has sold twice for over $13 million each time, though its second owner went bankrupt trying to monetize it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With assets like AI.com, there are no substitutes,” Fischer told the FT. “When one becomes available, the opportunity may never present itself again.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether these mega-dollar domains actually deliver returns remains an open question. But for Marszalek, who already owns Crypto.com and dropped $700 million on stadium naming rights, owning two category-defining domains is apparently worth the outlay.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/08/crypto-com-places-70m-bet-on-ai-com-domain-ahead-of-super-bowl/</guid><pubDate>Sun, 08 Feb 2026 20:19:47 +0000</pubDate></item><item><title>[NEW] Study: Platforms that rank the latest LLMs can be unreliable (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/study-platforms-rank-latest-llms-can-be-unreliable-0209</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-LLM-Rankings-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;A firm that wants to use a large language model (LLM) to summarize sales reports or triage customer inquiries can choose between hundreds of unique LLMs with dozens of model variations, each with slightly different performance.&lt;/p&gt;&lt;p&gt;To narrow down the choice, companies often rely on LLM ranking platforms, which gather user feedback on model interactions to rank the latest LLMs based on how they perform on certain tasks.&lt;/p&gt;&lt;p&gt;But MIT researchers found that a handful of user interactions can skew the results, leading someone to mistakenly believe one LLM is the ideal choice for a particular use case. Their study reveals that removing a tiny fraction of crowdsourced data can change which models are top-ranked.&lt;/p&gt;&lt;p&gt;They developed a fast method to test ranking platforms and determine whether they are susceptible to this problem. The evaluation technique identifies the individual votes most responsible for skewing the results so users can inspect these influential votes.&lt;/p&gt;&lt;p&gt;The researchers say this work underscores the need for more rigorous strategies to evaluate model rankings. While they didn’t focus on mitigation in this study, they provide suggestions that may improve the robustness of these platforms, such as gathering more detailed feedback to create the rankings.&lt;/p&gt;&lt;p&gt;The study also offers a word of warning to users who may rely on rankings when making decisions about LLMs that could have far-reaching and costly impacts on a business or organization.&lt;/p&gt;&lt;p&gt;“We were surprised that these ranking platforms were so sensitive to this problem. If it turns out the top-ranked LLM depends on only two or three pieces of user feedback out of tens of thousands, then one can’t assume the top-ranked LLM is going to be consistently outperforming all the other LLMs when it is deployed,” says Tamara Broderick, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS); a member of the Laboratory for Information and Decision Systems (LIDS) and the Institute for Data, Systems, and Society; an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author of this study.&lt;/p&gt;&lt;p&gt;She is joined on the paper by lead authors and EECS graduate students Jenny Huang and Yunyi Shen as well as Dennis Wei, a senior research scientist at IBM Research. The study will be presented at the International Conference on Learning Representations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Dropping data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While there are many types of LLM ranking platforms, the most popular variations ask users to submit a query to two models and pick which LLM provides the better response.&lt;/p&gt;&lt;p&gt;The platforms aggregate the results of these matchups to produce rankings that show which LLM performed best on certain tasks, such as coding or visual understanding.&lt;/p&gt;&lt;p&gt;By choosing a top-performing LLM, a user likely expects that model’s top ranking to generalize, meaning it should outperform other models on their similar, but not identical, application with a set of new data.&lt;/p&gt;&lt;p&gt;The MIT researchers previously studied generalization in areas like statistics and economics. That work revealed certain cases where dropping a small percentage of data can change a model’s results, indicating that those studies’ conclusions might not hold beyond their narrow setting.&lt;/p&gt;&lt;p&gt;The researchers wanted to see if the same analysis could be applied to LLM ranking platforms.&lt;/p&gt;&lt;p&gt;“At the end of the day, a user wants to know whether they are choosing the best LLM. If only a few prompts are driving this ranking, that suggests the ranking might not be the end-all-be-all,” Broderick says.&lt;/p&gt;&lt;p&gt;But it would be impossible to test the data-dropping phenomenon manually. For instance, one ranking they evaluated had more than 57,000 votes. Testing a data drop of 0.1 percent means removing each subset of 57 votes out of the 57,000, (there are more than 10&lt;sup&gt;194&amp;nbsp;&lt;/sup&gt;subsets), and then recalculating the ranking.&lt;/p&gt;&lt;p&gt;Instead, the researchers developed an efficient approximation method, based on their prior work, and adapted it to fit LLM ranking systems.&lt;/p&gt;&lt;p&gt;“While we have theory to prove the approximation works under certain assumptions, the user doesn’t need to trust that. Our method tells the user the problematic data points at the end, so they can just drop those data points, re-run the analysis, and check to see if they get a change in the rankings,” she says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Surprisingly sensitive&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When the researchers applied their technique to popular ranking platforms, they were surprised to see how few data points they needed to drop to cause significant changes in the top LLMs. In one instance, removing just two votes out of more than 57,000, which is 0.0035 percent, changed which model is top-ranked.&lt;/p&gt;&lt;p&gt;A different ranking platform, which uses expert annotators and higher quality prompts, was more robust. Here, removing 83 out of 2,575 evaluations (about 3 percent) flipped the top models.&lt;/p&gt;&lt;p&gt;Their examination revealed that many influential votes may have been a result of user error. In some cases, it appeared there was a clear answer as to which LLM performed better, but the user chose the other model instead, Broderick says.&lt;/p&gt;&lt;p&gt;“We can never know what was in the user’s mind at that time, but maybe they mis-clicked or weren’t paying attention, or they honestly didn’t know which one was better. The big takeaway here is that you don’t want noise, user error, or some outlier determining which is the top-ranked LLM,” she adds.&lt;/p&gt;&lt;p&gt;The researchers suggest that gathering additional feedback from users, such as confidence levels in each vote, would provide richer information that could help mitigate this problem. Ranking platforms could also use human mediators to assess crowdsourced responses.&lt;/p&gt;&lt;p&gt;For the researchers’ part, they want to continue exploring generalization in other contexts while also developing better approximation methods that can capture more examples of non-robustness.&lt;/p&gt;&lt;p&gt;“Broderick and her students’ work shows how you can get valid estimates of the influence of specific data on downstream processes, despite the intractability of exhaustive calculations given the size of modern machine-learning models and datasets,” says Jessica Hullman, the Ginni Rometty Professor of Computer Science at Northwestern University, who was not involved with this work. &amp;nbsp;“The recent work provides a glimpse into the strong data dependencies in routinely applied — but also very fragile — methods for aggregating human preferences and using them to update a model. Seeing how few preferences could really change the behavior of a fine-tuned model could inspire more thoughtful methods for collecting these data.”&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the Office of Naval Research, the MIT-IBM Watson AI Lab, the National Science Foundation, Amazon, and a CSAIL seed award.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-LLM-Rankings-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;A firm that wants to use a large language model (LLM) to summarize sales reports or triage customer inquiries can choose between hundreds of unique LLMs with dozens of model variations, each with slightly different performance.&lt;/p&gt;&lt;p&gt;To narrow down the choice, companies often rely on LLM ranking platforms, which gather user feedback on model interactions to rank the latest LLMs based on how they perform on certain tasks.&lt;/p&gt;&lt;p&gt;But MIT researchers found that a handful of user interactions can skew the results, leading someone to mistakenly believe one LLM is the ideal choice for a particular use case. Their study reveals that removing a tiny fraction of crowdsourced data can change which models are top-ranked.&lt;/p&gt;&lt;p&gt;They developed a fast method to test ranking platforms and determine whether they are susceptible to this problem. The evaluation technique identifies the individual votes most responsible for skewing the results so users can inspect these influential votes.&lt;/p&gt;&lt;p&gt;The researchers say this work underscores the need for more rigorous strategies to evaluate model rankings. While they didn’t focus on mitigation in this study, they provide suggestions that may improve the robustness of these platforms, such as gathering more detailed feedback to create the rankings.&lt;/p&gt;&lt;p&gt;The study also offers a word of warning to users who may rely on rankings when making decisions about LLMs that could have far-reaching and costly impacts on a business or organization.&lt;/p&gt;&lt;p&gt;“We were surprised that these ranking platforms were so sensitive to this problem. If it turns out the top-ranked LLM depends on only two or three pieces of user feedback out of tens of thousands, then one can’t assume the top-ranked LLM is going to be consistently outperforming all the other LLMs when it is deployed,” says Tamara Broderick, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS); a member of the Laboratory for Information and Decision Systems (LIDS) and the Institute for Data, Systems, and Society; an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author of this study.&lt;/p&gt;&lt;p&gt;She is joined on the paper by lead authors and EECS graduate students Jenny Huang and Yunyi Shen as well as Dennis Wei, a senior research scientist at IBM Research. The study will be presented at the International Conference on Learning Representations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Dropping data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While there are many types of LLM ranking platforms, the most popular variations ask users to submit a query to two models and pick which LLM provides the better response.&lt;/p&gt;&lt;p&gt;The platforms aggregate the results of these matchups to produce rankings that show which LLM performed best on certain tasks, such as coding or visual understanding.&lt;/p&gt;&lt;p&gt;By choosing a top-performing LLM, a user likely expects that model’s top ranking to generalize, meaning it should outperform other models on their similar, but not identical, application with a set of new data.&lt;/p&gt;&lt;p&gt;The MIT researchers previously studied generalization in areas like statistics and economics. That work revealed certain cases where dropping a small percentage of data can change a model’s results, indicating that those studies’ conclusions might not hold beyond their narrow setting.&lt;/p&gt;&lt;p&gt;The researchers wanted to see if the same analysis could be applied to LLM ranking platforms.&lt;/p&gt;&lt;p&gt;“At the end of the day, a user wants to know whether they are choosing the best LLM. If only a few prompts are driving this ranking, that suggests the ranking might not be the end-all-be-all,” Broderick says.&lt;/p&gt;&lt;p&gt;But it would be impossible to test the data-dropping phenomenon manually. For instance, one ranking they evaluated had more than 57,000 votes. Testing a data drop of 0.1 percent means removing each subset of 57 votes out of the 57,000, (there are more than 10&lt;sup&gt;194&amp;nbsp;&lt;/sup&gt;subsets), and then recalculating the ranking.&lt;/p&gt;&lt;p&gt;Instead, the researchers developed an efficient approximation method, based on their prior work, and adapted it to fit LLM ranking systems.&lt;/p&gt;&lt;p&gt;“While we have theory to prove the approximation works under certain assumptions, the user doesn’t need to trust that. Our method tells the user the problematic data points at the end, so they can just drop those data points, re-run the analysis, and check to see if they get a change in the rankings,” she says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Surprisingly sensitive&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When the researchers applied their technique to popular ranking platforms, they were surprised to see how few data points they needed to drop to cause significant changes in the top LLMs. In one instance, removing just two votes out of more than 57,000, which is 0.0035 percent, changed which model is top-ranked.&lt;/p&gt;&lt;p&gt;A different ranking platform, which uses expert annotators and higher quality prompts, was more robust. Here, removing 83 out of 2,575 evaluations (about 3 percent) flipped the top models.&lt;/p&gt;&lt;p&gt;Their examination revealed that many influential votes may have been a result of user error. In some cases, it appeared there was a clear answer as to which LLM performed better, but the user chose the other model instead, Broderick says.&lt;/p&gt;&lt;p&gt;“We can never know what was in the user’s mind at that time, but maybe they mis-clicked or weren’t paying attention, or they honestly didn’t know which one was better. The big takeaway here is that you don’t want noise, user error, or some outlier determining which is the top-ranked LLM,” she adds.&lt;/p&gt;&lt;p&gt;The researchers suggest that gathering additional feedback from users, such as confidence levels in each vote, would provide richer information that could help mitigate this problem. Ranking platforms could also use human mediators to assess crowdsourced responses.&lt;/p&gt;&lt;p&gt;For the researchers’ part, they want to continue exploring generalization in other contexts while also developing better approximation methods that can capture more examples of non-robustness.&lt;/p&gt;&lt;p&gt;“Broderick and her students’ work shows how you can get valid estimates of the influence of specific data on downstream processes, despite the intractability of exhaustive calculations given the size of modern machine-learning models and datasets,” says Jessica Hullman, the Ginni Rometty Professor of Computer Science at Northwestern University, who was not involved with this work. &amp;nbsp;“The recent work provides a glimpse into the strong data dependencies in routinely applied — but also very fragile — methods for aggregating human preferences and using them to update a model. Seeing how few preferences could really change the behavior of a fine-tuned model could inspire more thoughtful methods for collecting these data.”&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the Office of Naval Research, the MIT-IBM Watson AI Lab, the National Science Foundation, Amazon, and a CSAIL seed award.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/study-platforms-rank-latest-llms-can-be-unreliable-0209</guid><pubDate>Mon, 09 Feb 2026 05:00:00 +0000</pubDate></item></channel></rss>