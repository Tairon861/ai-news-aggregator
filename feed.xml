<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 12 Nov 2025 01:45:47 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>A former physician has launched Robyn, an empathetic AI companion (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/11/a-former-physician-has-launched-robyn-an-empathetic-ai-companion/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Jenny Shao was a practicing physician and was in residency at Harvard. During the pandemic, Shao saw that people in isolation had a neurological impact, and they needed support. This drove her to leave her medical career, the Harvard residency, to launch a start offering an AI assistant called Robyn. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robyn is intended to be an empathic, emotionally intelligent AI for people.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Navigating human relationships with AI assistants is a tricky space. On the one hand, there are general-purpose chatbots like ChatGPT; on the other, there are companion/friendship/avatar apps like Character.AI, Replika, and Friend, and even therapy apps like Feeling Great. A study in July indicated that 72% of U.S. teens have used AI companion apps. These apps have been accused of playing a part in the suicides of multiple people through various lawsuits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shao said that it is trying to position Robyn in a way that it is neither a friendship app nor a replacement for a therapist or a clinical practitioner.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As a physician, I have seen things go badly when tech companies try to replace your doctor. Robyn is and won’t ever be a clinical [replacement]. It is equivalent to someone who knows you very well. Usually, their role is to support you. You can think of Robyn as your emotionally intelligent partner,” Shao said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Screenshot showing with the Robyn home screen where you can type and start chat with the AI chatbot. " class="wp-image-3066728" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-11-at-10.55.09-AM.jpeg?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image Credits: Robyn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The founder said that with Robyn, her startup has tried to replicate the way humans remember things. Shao previously worked under Nobel Laureate Eric Kandel’s, who won the 2000 Nobel Prize in Physiology or Medicine, lab to research human memory. Shao said she put those learnings into Robyn to have the AI understand users more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robyn, which is available on iOS, has an onboarding process like many journaling or mental health apps. The app asks you about yourself, your goals, how you react when you are challenged, and what kind of tone you Robyn to respond in.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="An image that describes Robyn's memory feature and shows a chat where the AI chatbot recalls something which user said in the past. " class="wp-image-3066731" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/3-preview.jpeg?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image credits: Robyn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Once you complete the onboarding, you can chat with Robyn about different topics. For instance, when I asked it to build a morning routine for me, it asked me a bunch of questions and also had a detailed conversation about having a minimum screen time at the beginning of the day. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As you chat more with Robyn, the app will give you more insights into your pattern and also describe different traits about you, including your emotional fingerprint, attachment style, love language, growth edge, and inner critic. The startup has also made a demo website to analyze profiles on X and give insights as to what kinds of insights they would get out of Robyn.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="An image where Robyn describes your spirit and memory based on the chats that you've had. " class="wp-image-3066729" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Spirit-Animal.jpeg?w=471" width="471" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image credits: Robyn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Shao said that the company takes safety seriously and has been putting in guardrails even when she was testing the chatbot as a solo user. The app gives users a crisis line number and points them to the nearest ER if they talk about self-harm. The assistant also pushes back on certain topics and answers. If you ask it to show the latest sports score or ask it to count to 1,000, Robyn will say that it can’t perform these actions but help you with any personal stuff. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has raised $5.5 million in seed funding led by M13 with participation from Google Maps co-founder Lars Rasmussen, early Canva investor Bill Tai, ex-Yahoo CFO Ken Goldman, and X.ai co-founder Christian Szegedy. The startup had three team members at the start of the year and has now grown to 10 people. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rasmussen said that the app’s emotional memory system was impressive, and Shao’s mission of helping people attracted him to invest in the app.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="And I was describing Robyn's weekly insight features that gives you some takeaways from the week based on the conversations. " class="wp-image-3066730" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/7-preview.jpeg?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image Credits: Robyn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re living through a massive disconnection problem. People are surrounded by technology but feel less understood than ever. Robyn tackles that head-on. It’s solving emotional disconnection, helping people reflect, recognize their own patterns, and reconnect with who they are.&amp;nbsp;It’s not about therapy or replacing l relationships. It’s about strengthening someone’s capacity to connect — with themselves first, and then with others,” he told TechCrunch over email.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A big challenge for Robyn would be to maintain the safety of its users and also make sure users don’t anthropomorphize the chatbot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Latif Parecha, a partner at M13, said that Robyn’s ultimate goal is to foster human connections, but for AIs operating in this realm, there needs to be guardrails.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There needs to be guardrails in place for escalation for situations where people are in real danger. Especially, as AI will be part of our lives just like are family and friends are,” Parecha told TechCrunch over a call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has been testing Robyn with a select set of users for a few months and launching today in the U.S. The app is paid, and the subscription costs $19.99 a month or $199 a year. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Jenny Shao was a practicing physician and was in residency at Harvard. During the pandemic, Shao saw that people in isolation had a neurological impact, and they needed support. This drove her to leave her medical career, the Harvard residency, to launch a start offering an AI assistant called Robyn. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robyn is intended to be an empathic, emotionally intelligent AI for people.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Navigating human relationships with AI assistants is a tricky space. On the one hand, there are general-purpose chatbots like ChatGPT; on the other, there are companion/friendship/avatar apps like Character.AI, Replika, and Friend, and even therapy apps like Feeling Great. A study in July indicated that 72% of U.S. teens have used AI companion apps. These apps have been accused of playing a part in the suicides of multiple people through various lawsuits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shao said that it is trying to position Robyn in a way that it is neither a friendship app nor a replacement for a therapist or a clinical practitioner.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As a physician, I have seen things go badly when tech companies try to replace your doctor. Robyn is and won’t ever be a clinical [replacement]. It is equivalent to someone who knows you very well. Usually, their role is to support you. You can think of Robyn as your emotionally intelligent partner,” Shao said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Screenshot showing with the Robyn home screen where you can type and start chat with the AI chatbot. " class="wp-image-3066728" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-11-at-10.55.09-AM.jpeg?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image Credits: Robyn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The founder said that with Robyn, her startup has tried to replicate the way humans remember things. Shao previously worked under Nobel Laureate Eric Kandel’s, who won the 2000 Nobel Prize in Physiology or Medicine, lab to research human memory. Shao said she put those learnings into Robyn to have the AI understand users more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Robyn, which is available on iOS, has an onboarding process like many journaling or mental health apps. The app asks you about yourself, your goals, how you react when you are challenged, and what kind of tone you Robyn to respond in.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="An image that describes Robyn's memory feature and shows a chat where the AI chatbot recalls something which user said in the past. " class="wp-image-3066731" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/3-preview.jpeg?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image credits: Robyn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Once you complete the onboarding, you can chat with Robyn about different topics. For instance, when I asked it to build a morning routine for me, it asked me a bunch of questions and also had a detailed conversation about having a minimum screen time at the beginning of the day. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As you chat more with Robyn, the app will give you more insights into your pattern and also describe different traits about you, including your emotional fingerprint, attachment style, love language, growth edge, and inner critic. The startup has also made a demo website to analyze profiles on X and give insights as to what kinds of insights they would get out of Robyn.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="An image where Robyn describes your spirit and memory based on the chats that you've had. " class="wp-image-3066729" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Spirit-Animal.jpeg?w=471" width="471" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image credits: Robyn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Shao said that the company takes safety seriously and has been putting in guardrails even when she was testing the chatbot as a solo user. The app gives users a crisis line number and points them to the nearest ER if they talk about self-harm. The assistant also pushes back on certain topics and answers. If you ask it to show the latest sports score or ask it to count to 1,000, Robyn will say that it can’t perform these actions but help you with any personal stuff. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has raised $5.5 million in seed funding led by M13 with participation from Google Maps co-founder Lars Rasmussen, early Canva investor Bill Tai, ex-Yahoo CFO Ken Goldman, and X.ai co-founder Christian Szegedy. The startup had three team members at the start of the year and has now grown to 10 people. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rasmussen said that the app’s emotional memory system was impressive, and Shao’s mission of helping people attracted him to invest in the app.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="And I was describing Robyn's weekly insight features that gives you some takeaways from the week based on the conversations. " class="wp-image-3066730" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/7-preview.jpeg?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image Credits: Robyn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re living through a massive disconnection problem. People are surrounded by technology but feel less understood than ever. Robyn tackles that head-on. It’s solving emotional disconnection, helping people reflect, recognize their own patterns, and reconnect with who they are.&amp;nbsp;It’s not about therapy or replacing l relationships. It’s about strengthening someone’s capacity to connect — with themselves first, and then with others,” he told TechCrunch over email.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A big challenge for Robyn would be to maintain the safety of its users and also make sure users don’t anthropomorphize the chatbot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Latif Parecha, a partner at M13, said that Robyn’s ultimate goal is to foster human connections, but for AIs operating in this realm, there needs to be guardrails.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There needs to be guardrails in place for escalation for situations where people are in real danger. Especially, as AI will be part of our lives just like are family and friends are,” Parecha told TechCrunch over a call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup has been testing Robyn with a select set of users for a few months and launching today in the U.S. The app is paid, and the subscription costs $19.99 a month or $199 a year. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/11/a-former-physician-has-launched-robyn-an-empathetic-ai-companion/</guid><pubDate>Tue, 11 Nov 2025 14:00:00 +0000</pubDate></item><item><title>Meta’s chief AI scientist Yann LeCun reportedly plans to leave to build his own startup (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/11/metas-chief-ai-scientist-yann-lecun-reportedly-plans-to-leave-to-build-his-own-startup/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2194768816.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta may be about to lose one of its most renowned AI heads: Yann LeCun, a chief AI scientist at the company, is planning to leave the company to build his own startup, the Financial Times reported, citing anonymous sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LeCun, a professor at New York University, senior researcher at Meta, and winner of the prestigious A.M. Turing Award, plans to leave in the coming months, and is already in talks to raise capital for a startup that would focus on continuing his work on world models, the report added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A world model is an AI system that develops an internal understanding of its environment so it can simulate cause-and-effect scenarios to predict outcomes. Top labs and startups like Google DeepMind and World Labs are also developing world models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LeCun’s departure would come at a pivotal time for Meta, which has of late changed how it approaches AI development in response to concerns that it is being outpaced by rivals like OpenAI, Google, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has reportedly started revamping its AI organization after hiring over 50 engineers and researchers from its competitors to build out a new AI unit, dubbed Meta Superintelligence Labs (MSL). Notably, Meta in June invested $14.3 billion in data-labeling vendor Scale AI and brought on board its CEO Alexandr Wang to run the new division.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those decisions, sources told TechCrunch in August, have made things increasingly chaotic at Meta’s AI unit, with new talent expressing frustration with navigating the bureaucracy of a big company, while Meta’s previous generative AI team has seen its scope limited.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LeCun’s long-term research work at the company under its Fundamental AI Research Lab (FAIR) division has been overshadowed by CEO Mark Zuckerberg’s decisions to overhaul things after the company’s previous family of AI models, Llama 4, failed to keep up with rival models. Unlike MSL, FAIR is designed to focus on long-term AI research — techniques that may be used five to 10 years from now.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;LeCun has been openly skeptical about how AI technology — specifically LLMs — is currently being marketed as the cure for all of humankind’s ails. He even tweeted that AI systems have a long way to go. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It seems to me that before ‘urgently figuring out how to control AI systems much smarter than us’ we need to have the beginning of a hint of a design for a system smarter than a house cat,” he wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta did not immediately return a request for comment outside regular business hours. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2194768816.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta may be about to lose one of its most renowned AI heads: Yann LeCun, a chief AI scientist at the company, is planning to leave the company to build his own startup, the Financial Times reported, citing anonymous sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LeCun, a professor at New York University, senior researcher at Meta, and winner of the prestigious A.M. Turing Award, plans to leave in the coming months, and is already in talks to raise capital for a startup that would focus on continuing his work on world models, the report added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A world model is an AI system that develops an internal understanding of its environment so it can simulate cause-and-effect scenarios to predict outcomes. Top labs and startups like Google DeepMind and World Labs are also developing world models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LeCun’s departure would come at a pivotal time for Meta, which has of late changed how it approaches AI development in response to concerns that it is being outpaced by rivals like OpenAI, Google, and Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has reportedly started revamping its AI organization after hiring over 50 engineers and researchers from its competitors to build out a new AI unit, dubbed Meta Superintelligence Labs (MSL). Notably, Meta in June invested $14.3 billion in data-labeling vendor Scale AI and brought on board its CEO Alexandr Wang to run the new division.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those decisions, sources told TechCrunch in August, have made things increasingly chaotic at Meta’s AI unit, with new talent expressing frustration with navigating the bureaucracy of a big company, while Meta’s previous generative AI team has seen its scope limited.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LeCun’s long-term research work at the company under its Fundamental AI Research Lab (FAIR) division has been overshadowed by CEO Mark Zuckerberg’s decisions to overhaul things after the company’s previous family of AI models, Llama 4, failed to keep up with rival models. Unlike MSL, FAIR is designed to focus on long-term AI research — techniques that may be used five to 10 years from now.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;LeCun has been openly skeptical about how AI technology — specifically LLMs — is currently being marketed as the cure for all of humankind’s ails. He even tweeted that AI systems have a long way to go. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It seems to me that before ‘urgently figuring out how to control AI systems much smarter than us’ we need to have the beginning of a hint of a design for a system smarter than a house cat,” he wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta did not immediately return a request for comment outside regular business hours. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/11/metas-chief-ai-scientist-yann-lecun-reportedly-plans-to-leave-to-build-his-own-startup/</guid><pubDate>Tue, 11 Nov 2025 14:58:45 +0000</pubDate></item><item><title>You won’t believe the excuses lawyers have after getting busted for using AI (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/11/lawyers-keep-giving-weak-sauce-excuses-for-fake-ai-citations-in-court-docs/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      I got hacked; I lost my login; it was a rough draft; toggling windows is hard.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/ai-shrugging-lawyer.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Amid what one judge called an “epidemic” of fake AI-generated case citations bogging down courts, some common excuses are emerging from lawyers hoping to dodge the most severe sanctions for filings deemed misleading.&lt;/p&gt;
&lt;p&gt;Using a database compiled by French lawyer and AI researcher Damien Charlotin, Ars reviewed 23 cases where lawyers were sanctioned for AI hallucinations. In many, judges noted that the simplest path to avoid or diminish sanctions was to admit that AI was used as soon as it’s detected, act humble, self-report the error to relevant legal associations, and voluntarily take classes on AI and law. But not every lawyer takes the path of least resistance, Ars’ review found, with many instead offering excuses that no judge found credible. Some even lie about their AI use, judges concluded.&lt;/p&gt;
&lt;p&gt;Since 2023—when fake AI citations started being publicized—the most popular excuse has been that the lawyer didn’t know AI was used to draft a filing.&lt;/p&gt;
&lt;p&gt;Sometimes that means arguing that you didn’t realize you were using AI, as in the case of a California lawyer who got stung by Google’s AI Overviews, which he claimed he took for typical Google search results. Most often, lawyers using this excuse tend to blame an underling, but clients have been blamed, too. A Texas lawyer this month was sanctioned after deflecting so much that the court had to eventually put his client on the stand after he revealed she played a significant role in drafting the aberrant filing.&lt;/p&gt;
&lt;p&gt;“Is your client an attorney?” the court asked.&lt;/p&gt;
&lt;p&gt;“No, not at all your Honor, just was essentially helping me with the theories of the case,” the lawyer said.&lt;/p&gt;
&lt;p&gt;Another popular dodge comes from lawyers who feign ignorance that chatbots are prone to hallucinating facts.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Recent cases suggest this excuse may be mutating into variants. Last month, a sanctioned Oklahoma lawyer admitted that he didn’t expect ChatGPT to add new citations when all he asked the bot to do was “make his writing more persuasive.” And in September, a California lawyer got in a similar bind—and was sanctioned a whopping $10,000, a fine the judge called “conservative.” That lawyer had asked ChatGPT to “enhance” his briefs, “then ran the ‘enhanced’ briefs through other AI platforms to check for errors,” neglecting to ever read the “enhanced” briefs.&lt;/p&gt;
&lt;p&gt;Neither of those tired old excuses hold much weight today, especially in courts that have drawn up guidance to address AI hallucinations. But rather than quickly acknowledge their missteps, as courts are begging lawyers to do, several lawyers appear to have gotten desperate. Ars found a bunch citing common tech issues as the reason for citing fake cases.&lt;/p&gt;
&lt;h2&gt;When in doubt, blame hackers?&lt;/h2&gt;
&lt;p&gt;For an extreme case, look to a New York City civil court, where a lawyer, Innocent Chinweze, first admitted to using Microsoft Copilot to draft an errant filing, then bizarrely pivoted to claim that the AI citations were due to malware found on his computer.&lt;/p&gt;
&lt;p&gt;Chinweze said he had created a draft with correct citations but then got hacked, allowing bad actors “unauthorized remote access” to supposedly add the errors in his filing.&lt;/p&gt;
&lt;p&gt;The judge was skeptical, describing the excuse as an “incredible and unsupported statement,” particularly since there was no evidence of the prior draft existing. Instead, Chinweze asked to bring in an expert to testify that the hack had occurred, requesting to end the proceedings on sanctions until after the court weighed the expert’s analysis.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The judge, Kimon C. Thermos, didn’t have to weigh this argument, however, because after the court broke for lunch, the lawyer once again “dramatically” changed his position.&lt;/p&gt;
&lt;p&gt;“He no longer wished to adjourn for an expert to testify regarding malware or unauthorized access to his computer,” Thermos wrote in an order issuing sanctions. “He retreated” to “his original position that he used Copilot to aid in his research and didn’t realize that it could generate fake cases.”&lt;/p&gt;
&lt;p&gt;Possibly more galling to Thermos than the lawyer’s weird malware argument, though, was a document that Chinweze filed on the day of his sanctions hearing. That document included multiple summaries preceded by this text, the judge noted:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Some case metadata and case summaries were written with the help of AI, which can produce inaccuracies. You should read the full case before relying on it for legal research purposes.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thermos admonished Chinweze for continuing to use AI recklessly. He blasted the filing as “an incoherent document that is eighty-eight pages long, has no structure, contains the full text of most of the cases cited,” and “shows distinct indications that parts of the discussion/analysis of the cited cases were written by artificial intelligence.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Ultimately, Thermos ordered Chinweze to pay $1,000, the most typical fine lawyers received in the cases Ars reviewed. The judge then took an extra non-monetary step to sanction Chinweze, referring the lawyer to a grievance committee, “given that his misconduct was substantial and seriously implicated his honesty, trustworthiness, and fitness to practice law.”&lt;/p&gt;
&lt;p&gt;Ars could not immediately reach Chinweze for comment.&lt;/p&gt;
&lt;h2&gt;Toggling windows on a laptop is hard&lt;/h2&gt;
&lt;p&gt;&lt;span style="margin: 0px; padding: 0px;"&gt;In Alabama, an attorney named James A. Johnson made an “embarrassing mistake,” he said, primarily because toggling windows on a laptop is hard, US District Judge Terry F. Moorer noted in an October&amp;nbsp;order on sanctions.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Johnson explained that he had accidentally used an AI tool that he didn’t realize could hallucinate. It happened while he was “at an out-of-state hospital attending to the care of a family member recovering from surgery.” He rushed to draft the filing, he said, because he got a notice that his client’s conference had suddenly been “moved up on the court’s schedule.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Under time pressure and difficult personal circumstance,” Johnson explained, he decided against using Fastcase, a research tool provided by the Alabama State Bar, to research the filing. Working on his laptop, he opted instead to use “a Microsoft Word plug-in called Ghostwriter Legal” because “it appeared automatically in the sidebar of Word while Fastcase required opening a separate browser to access through the Alabama State Bar website.”&lt;/p&gt;
&lt;p&gt;To Johnson, it felt “tedious to toggle back and forth between programs on [his] laptop with the touchpad,” and that meant he “unfortunately fell victim to the allure of a new program that was open and available.”&lt;/p&gt;
&lt;p&gt;Moorer seemed unimpressed by Johnson’s claim that he understood tools like ChatGPT were unreliable but didn’t expect the same from other AI legal tools—particularly since “information from Ghostwriter Legal made it clear that it used ChatGPT as its default AI program,” Moorer wrote.&lt;/p&gt;
&lt;p&gt;The lawyer’s client was similarly put off, deciding to drop Johnson on the spot, even though that risked “a significant delay of trial.” Moorer noted that Johnson seemed shaken by his client’s abrupt decision, evidenced by “his look of shock, dismay, and display of emotion.”&lt;/p&gt;
&lt;p&gt;And switching to a new lawyer could eat up more of that money. Moorer further noted that Johnson seemingly let AI do his homework while working on behalf of the government. But as the judge noted, “public funds for appointed counsel are not a bottomless well and are limited resource.”&lt;/p&gt;
&lt;p&gt;“It has become clear that basic reprimands and small fines are not sufficient to deter this type of misconduct because if it were, we would not be here,” Moorer concluded.&lt;/p&gt;
&lt;p&gt;Ruling that Johnson’s reliance on AI was “tantamount to bad faith,” Moorer imposed a $5,000 fine. The judge also would have&amp;nbsp;“considered potential disqualification, but that was rendered moot” since Johnson’s client had already dismissed him.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Asked for comment, Johnson told Ars that “the court made plainly erroneous findings of fact and the sanctions are on appeal.”&lt;/p&gt;
&lt;h2&gt;Plagued by login issues&lt;/h2&gt;
&lt;p&gt;As a lawyer in Georgia tells it, sometimes fake AI citations may be filed because a lawyer accidentally filed a rough draft instead of the final version.&lt;/p&gt;
&lt;p&gt;Other lawyers claim they turn to AI as needed when they have trouble accessing legal tools like Westlaw or LexisNexis.&lt;/p&gt;
&lt;p&gt;For example, in Iowa, a lawyer told an appeals court that she regretted relying on “secondary AI-driven research tools” after experiencing “login issues her with her Westlaw subscription.” Although the court was “sympathetic to issues with technology, such as login issues,” the lawyer was sanctioned, primarily because she only admitted to using AI after the court ordered her to explain her mistakes. In her case, however, she got to choose between paying a minimal $150 fine or attending “two hours of legal ethics training particular to AI.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Less sympathetic was a lawyer who got caught lying about the AI tool she blamed for inaccuracies, a Louisiana case suggested. In that case, a judge demanded to see the research history after a lawyer claimed that AI hallucinations came from “using Westlaw Precision, an AI-assisted research tool, rather than Westlaw’s standalone legal database.”&lt;/p&gt;
&lt;p&gt;It turned out that the lawyer had outsourced the research, relying on a “currently suspended” lawyer’s AI citations, and had only “assumed” the lawyer’s mistakes were from Westlaw’s AI tool. It’s unclear what tool was actually used by the suspended lawyer, who likely lost access to a Westlaw login, but the judge ordered a $1,000 penalty after the lawyer who signed the filing “agreed that Westlaw did not generate the fabricated citations.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Judge warned of “serial hallucinators”&lt;/h2&gt;
&lt;p&gt;Another lawyer, William T. Panichi in Illinois, has been sanctioned at least three times, Ars’ review found.&lt;/p&gt;
&lt;p&gt;In response to his initial penalties ordered in July, he admitted to being tempted by AI while he was “between research software.”&lt;/p&gt;
&lt;p&gt;In that case, the court was frustrated to find that the lawyer had contradicted himself, and it ordered more severe sanctions as a result.&lt;/p&gt;
&lt;p&gt;Panichi “simultaneously admitted to using AI to generate the briefs, not doing any of his own independent research, and even that he ‘barely did any personal work [him]self on this appeal,'” the court order said, while also defending charging a higher fee—supposedly because this case “was out of the ordinary in terms of time spent” and his office “did some exceptional work” getting information.&lt;/p&gt;
&lt;p&gt;The court deemed this AI misuse so bad that Panichi was ordered to disgorge a “payment of $6,925.62 that he received” in addition to a $1,000 penalty.&lt;/p&gt;
&lt;p&gt;“If I’m lucky enough to be able to continue practicing before the appellate court, I’m not going to do it again,” Panichi told the court in July, just before getting hit with two more rounds of sanctions in August.&lt;/p&gt;
&lt;p&gt;Panichi did not immediately respond to Ars’ request for comment.&lt;/p&gt;
&lt;p&gt;When AI-generated hallucinations are found, penalties are often paid to the court, the other parties’ lawyers, or both, depending on whose time and resources were wasted fact-checking fake cases.&lt;/p&gt;
&lt;p&gt;Lawyers seem more likely to argue against paying sanctions to the other parties’ attorneys, hoping to keep sanctions as low as possible. One lawyer even argued that “it only takes 7.6 seconds, not hours, to type citations into LexisNexis or Westlaw,” while seemingly neglecting the fact that she did not take those precious seconds to check her own citations.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The judge in the case, Nancy Miller, was clear that “such statements display an astounding lack of awareness of counsel’s obligations,” noting that “the responsibility for correcting erroneous and fake citations never shifts to opposing counsel or the court, even if they are the first to notice the errors.”&lt;/p&gt;
&lt;p&gt;“The duty to mitigate the harms caused by such errors remains with the signor,” Miller said. “The sooner such errors are properly corrected, either by withdrawing or amending and supplementing the offending pleadings, the less time is wasted by everyone involved, and fewer costs are incurred.”&lt;/p&gt;
&lt;p&gt;Texas US District Judge Marina Garcia Marmolejo agreed, explaining that even more time is wasted determining how other judges have responded to fake AI-generated citations.&lt;/p&gt;
&lt;p&gt;“At one of the busiest court dockets in the nation, there are scant resources to spare ferreting out erroneous AI citations in the first place, let alone surveying the burgeoning caselaw on this subject,” she said.&lt;/p&gt;
&lt;p&gt;At least one Florida court was “shocked, shocked” to find that a lawyer was refusing to pay what the other party’s attorneys said they were owed after misusing AI. The lawyer in that case, James Martin Paul, asked to pay less than a quarter of the fees and costs owed, arguing that Charlotin’s database showed he might otherwise owe penalties that “would be the largest sanctions paid out for the use of AI generative case law to date.”&lt;/p&gt;
&lt;p&gt;But caving to Paul’s arguments “would only benefit serial hallucinators,” the Florida court found. Ultimately, Paul was sanctioned more than $85,000 for what the court said was “far more egregious” conduct than other offenders in the database, chastising him for “repeated, abusive, bad-faith conduct that cannot be recognized as legitimate legal practice and must be deterred.”&lt;/p&gt;
&lt;p&gt;Paul did not immediately respond to Ars’ request to comment.&lt;/p&gt;
&lt;p&gt;Michael B. Slade, a US bankruptcy judge in Illinois, seems to be done weighing excuses, calling on all lawyers to stop taking AI shortcuts that are burdening courts.&lt;/p&gt;
&lt;p&gt;“At this point, to be blunt, any lawyer unaware that using generative AI platforms to do legal research is playing with fire is living in a cloud,” Slade wrote.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This story was updated on November 11 to clarify a judge’s comments on misuse of public funds.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      I got hacked; I lost my login; it was a rough draft; toggling windows is hard.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/ai-shrugging-lawyer.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Amid what one judge called an “epidemic” of fake AI-generated case citations bogging down courts, some common excuses are emerging from lawyers hoping to dodge the most severe sanctions for filings deemed misleading.&lt;/p&gt;
&lt;p&gt;Using a database compiled by French lawyer and AI researcher Damien Charlotin, Ars reviewed 23 cases where lawyers were sanctioned for AI hallucinations. In many, judges noted that the simplest path to avoid or diminish sanctions was to admit that AI was used as soon as it’s detected, act humble, self-report the error to relevant legal associations, and voluntarily take classes on AI and law. But not every lawyer takes the path of least resistance, Ars’ review found, with many instead offering excuses that no judge found credible. Some even lie about their AI use, judges concluded.&lt;/p&gt;
&lt;p&gt;Since 2023—when fake AI citations started being publicized—the most popular excuse has been that the lawyer didn’t know AI was used to draft a filing.&lt;/p&gt;
&lt;p&gt;Sometimes that means arguing that you didn’t realize you were using AI, as in the case of a California lawyer who got stung by Google’s AI Overviews, which he claimed he took for typical Google search results. Most often, lawyers using this excuse tend to blame an underling, but clients have been blamed, too. A Texas lawyer this month was sanctioned after deflecting so much that the court had to eventually put his client on the stand after he revealed she played a significant role in drafting the aberrant filing.&lt;/p&gt;
&lt;p&gt;“Is your client an attorney?” the court asked.&lt;/p&gt;
&lt;p&gt;“No, not at all your Honor, just was essentially helping me with the theories of the case,” the lawyer said.&lt;/p&gt;
&lt;p&gt;Another popular dodge comes from lawyers who feign ignorance that chatbots are prone to hallucinating facts.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Recent cases suggest this excuse may be mutating into variants. Last month, a sanctioned Oklahoma lawyer admitted that he didn’t expect ChatGPT to add new citations when all he asked the bot to do was “make his writing more persuasive.” And in September, a California lawyer got in a similar bind—and was sanctioned a whopping $10,000, a fine the judge called “conservative.” That lawyer had asked ChatGPT to “enhance” his briefs, “then ran the ‘enhanced’ briefs through other AI platforms to check for errors,” neglecting to ever read the “enhanced” briefs.&lt;/p&gt;
&lt;p&gt;Neither of those tired old excuses hold much weight today, especially in courts that have drawn up guidance to address AI hallucinations. But rather than quickly acknowledge their missteps, as courts are begging lawyers to do, several lawyers appear to have gotten desperate. Ars found a bunch citing common tech issues as the reason for citing fake cases.&lt;/p&gt;
&lt;h2&gt;When in doubt, blame hackers?&lt;/h2&gt;
&lt;p&gt;For an extreme case, look to a New York City civil court, where a lawyer, Innocent Chinweze, first admitted to using Microsoft Copilot to draft an errant filing, then bizarrely pivoted to claim that the AI citations were due to malware found on his computer.&lt;/p&gt;
&lt;p&gt;Chinweze said he had created a draft with correct citations but then got hacked, allowing bad actors “unauthorized remote access” to supposedly add the errors in his filing.&lt;/p&gt;
&lt;p&gt;The judge was skeptical, describing the excuse as an “incredible and unsupported statement,” particularly since there was no evidence of the prior draft existing. Instead, Chinweze asked to bring in an expert to testify that the hack had occurred, requesting to end the proceedings on sanctions until after the court weighed the expert’s analysis.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The judge, Kimon C. Thermos, didn’t have to weigh this argument, however, because after the court broke for lunch, the lawyer once again “dramatically” changed his position.&lt;/p&gt;
&lt;p&gt;“He no longer wished to adjourn for an expert to testify regarding malware or unauthorized access to his computer,” Thermos wrote in an order issuing sanctions. “He retreated” to “his original position that he used Copilot to aid in his research and didn’t realize that it could generate fake cases.”&lt;/p&gt;
&lt;p&gt;Possibly more galling to Thermos than the lawyer’s weird malware argument, though, was a document that Chinweze filed on the day of his sanctions hearing. That document included multiple summaries preceded by this text, the judge noted:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Some case metadata and case summaries were written with the help of AI, which can produce inaccuracies. You should read the full case before relying on it for legal research purposes.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thermos admonished Chinweze for continuing to use AI recklessly. He blasted the filing as “an incoherent document that is eighty-eight pages long, has no structure, contains the full text of most of the cases cited,” and “shows distinct indications that parts of the discussion/analysis of the cited cases were written by artificial intelligence.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Ultimately, Thermos ordered Chinweze to pay $1,000, the most typical fine lawyers received in the cases Ars reviewed. The judge then took an extra non-monetary step to sanction Chinweze, referring the lawyer to a grievance committee, “given that his misconduct was substantial and seriously implicated his honesty, trustworthiness, and fitness to practice law.”&lt;/p&gt;
&lt;p&gt;Ars could not immediately reach Chinweze for comment.&lt;/p&gt;
&lt;h2&gt;Toggling windows on a laptop is hard&lt;/h2&gt;
&lt;p&gt;&lt;span style="margin: 0px; padding: 0px;"&gt;In Alabama, an attorney named James A. Johnson made an “embarrassing mistake,” he said, primarily because toggling windows on a laptop is hard, US District Judge Terry F. Moorer noted in an October&amp;nbsp;order on sanctions.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Johnson explained that he had accidentally used an AI tool that he didn’t realize could hallucinate. It happened while he was “at an out-of-state hospital attending to the care of a family member recovering from surgery.” He rushed to draft the filing, he said, because he got a notice that his client’s conference had suddenly been “moved up on the court’s schedule.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Under time pressure and difficult personal circumstance,” Johnson explained, he decided against using Fastcase, a research tool provided by the Alabama State Bar, to research the filing. Working on his laptop, he opted instead to use “a Microsoft Word plug-in called Ghostwriter Legal” because “it appeared automatically in the sidebar of Word while Fastcase required opening a separate browser to access through the Alabama State Bar website.”&lt;/p&gt;
&lt;p&gt;To Johnson, it felt “tedious to toggle back and forth between programs on [his] laptop with the touchpad,” and that meant he “unfortunately fell victim to the allure of a new program that was open and available.”&lt;/p&gt;
&lt;p&gt;Moorer seemed unimpressed by Johnson’s claim that he understood tools like ChatGPT were unreliable but didn’t expect the same from other AI legal tools—particularly since “information from Ghostwriter Legal made it clear that it used ChatGPT as its default AI program,” Moorer wrote.&lt;/p&gt;
&lt;p&gt;The lawyer’s client was similarly put off, deciding to drop Johnson on the spot, even though that risked “a significant delay of trial.” Moorer noted that Johnson seemed shaken by his client’s abrupt decision, evidenced by “his look of shock, dismay, and display of emotion.”&lt;/p&gt;
&lt;p&gt;And switching to a new lawyer could eat up more of that money. Moorer further noted that Johnson seemingly let AI do his homework while working on behalf of the government. But as the judge noted, “public funds for appointed counsel are not a bottomless well and are limited resource.”&lt;/p&gt;
&lt;p&gt;“It has become clear that basic reprimands and small fines are not sufficient to deter this type of misconduct because if it were, we would not be here,” Moorer concluded.&lt;/p&gt;
&lt;p&gt;Ruling that Johnson’s reliance on AI was “tantamount to bad faith,” Moorer imposed a $5,000 fine. The judge also would have&amp;nbsp;“considered potential disqualification, but that was rendered moot” since Johnson’s client had already dismissed him.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Asked for comment, Johnson told Ars that “the court made plainly erroneous findings of fact and the sanctions are on appeal.”&lt;/p&gt;
&lt;h2&gt;Plagued by login issues&lt;/h2&gt;
&lt;p&gt;As a lawyer in Georgia tells it, sometimes fake AI citations may be filed because a lawyer accidentally filed a rough draft instead of the final version.&lt;/p&gt;
&lt;p&gt;Other lawyers claim they turn to AI as needed when they have trouble accessing legal tools like Westlaw or LexisNexis.&lt;/p&gt;
&lt;p&gt;For example, in Iowa, a lawyer told an appeals court that she regretted relying on “secondary AI-driven research tools” after experiencing “login issues her with her Westlaw subscription.” Although the court was “sympathetic to issues with technology, such as login issues,” the lawyer was sanctioned, primarily because she only admitted to using AI after the court ordered her to explain her mistakes. In her case, however, she got to choose between paying a minimal $150 fine or attending “two hours of legal ethics training particular to AI.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Less sympathetic was a lawyer who got caught lying about the AI tool she blamed for inaccuracies, a Louisiana case suggested. In that case, a judge demanded to see the research history after a lawyer claimed that AI hallucinations came from “using Westlaw Precision, an AI-assisted research tool, rather than Westlaw’s standalone legal database.”&lt;/p&gt;
&lt;p&gt;It turned out that the lawyer had outsourced the research, relying on a “currently suspended” lawyer’s AI citations, and had only “assumed” the lawyer’s mistakes were from Westlaw’s AI tool. It’s unclear what tool was actually used by the suspended lawyer, who likely lost access to a Westlaw login, but the judge ordered a $1,000 penalty after the lawyer who signed the filing “agreed that Westlaw did not generate the fabricated citations.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Judge warned of “serial hallucinators”&lt;/h2&gt;
&lt;p&gt;Another lawyer, William T. Panichi in Illinois, has been sanctioned at least three times, Ars’ review found.&lt;/p&gt;
&lt;p&gt;In response to his initial penalties ordered in July, he admitted to being tempted by AI while he was “between research software.”&lt;/p&gt;
&lt;p&gt;In that case, the court was frustrated to find that the lawyer had contradicted himself, and it ordered more severe sanctions as a result.&lt;/p&gt;
&lt;p&gt;Panichi “simultaneously admitted to using AI to generate the briefs, not doing any of his own independent research, and even that he ‘barely did any personal work [him]self on this appeal,'” the court order said, while also defending charging a higher fee—supposedly because this case “was out of the ordinary in terms of time spent” and his office “did some exceptional work” getting information.&lt;/p&gt;
&lt;p&gt;The court deemed this AI misuse so bad that Panichi was ordered to disgorge a “payment of $6,925.62 that he received” in addition to a $1,000 penalty.&lt;/p&gt;
&lt;p&gt;“If I’m lucky enough to be able to continue practicing before the appellate court, I’m not going to do it again,” Panichi told the court in July, just before getting hit with two more rounds of sanctions in August.&lt;/p&gt;
&lt;p&gt;Panichi did not immediately respond to Ars’ request for comment.&lt;/p&gt;
&lt;p&gt;When AI-generated hallucinations are found, penalties are often paid to the court, the other parties’ lawyers, or both, depending on whose time and resources were wasted fact-checking fake cases.&lt;/p&gt;
&lt;p&gt;Lawyers seem more likely to argue against paying sanctions to the other parties’ attorneys, hoping to keep sanctions as low as possible. One lawyer even argued that “it only takes 7.6 seconds, not hours, to type citations into LexisNexis or Westlaw,” while seemingly neglecting the fact that she did not take those precious seconds to check her own citations.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The judge in the case, Nancy Miller, was clear that “such statements display an astounding lack of awareness of counsel’s obligations,” noting that “the responsibility for correcting erroneous and fake citations never shifts to opposing counsel or the court, even if they are the first to notice the errors.”&lt;/p&gt;
&lt;p&gt;“The duty to mitigate the harms caused by such errors remains with the signor,” Miller said. “The sooner such errors are properly corrected, either by withdrawing or amending and supplementing the offending pleadings, the less time is wasted by everyone involved, and fewer costs are incurred.”&lt;/p&gt;
&lt;p&gt;Texas US District Judge Marina Garcia Marmolejo agreed, explaining that even more time is wasted determining how other judges have responded to fake AI-generated citations.&lt;/p&gt;
&lt;p&gt;“At one of the busiest court dockets in the nation, there are scant resources to spare ferreting out erroneous AI citations in the first place, let alone surveying the burgeoning caselaw on this subject,” she said.&lt;/p&gt;
&lt;p&gt;At least one Florida court was “shocked, shocked” to find that a lawyer was refusing to pay what the other party’s attorneys said they were owed after misusing AI. The lawyer in that case, James Martin Paul, asked to pay less than a quarter of the fees and costs owed, arguing that Charlotin’s database showed he might otherwise owe penalties that “would be the largest sanctions paid out for the use of AI generative case law to date.”&lt;/p&gt;
&lt;p&gt;But caving to Paul’s arguments “would only benefit serial hallucinators,” the Florida court found. Ultimately, Paul was sanctioned more than $85,000 for what the court said was “far more egregious” conduct than other offenders in the database, chastising him for “repeated, abusive, bad-faith conduct that cannot be recognized as legitimate legal practice and must be deterred.”&lt;/p&gt;
&lt;p&gt;Paul did not immediately respond to Ars’ request to comment.&lt;/p&gt;
&lt;p&gt;Michael B. Slade, a US bankruptcy judge in Illinois, seems to be done weighing excuses, calling on all lawyers to stop taking AI shortcuts that are burdening courts.&lt;/p&gt;
&lt;p&gt;“At this point, to be blunt, any lawyer unaware that using generative AI platforms to do legal research is playing with fire is living in a cloud,” Slade wrote.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This story was updated on November 11 to clarify a judge’s comments on misuse of public funds.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/11/lawyers-keep-giving-weak-sauce-excuses-for-fake-ai-citations-in-court-docs/</guid><pubDate>Tue, 11 Nov 2025 15:54:11 +0000</pubDate></item><item><title>Wonderful raised $100M Series A to put AI agents on the front lines of customer service (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/11/wonderful-raised-100m-series-a-to-put-ai-agents-on-the-front-lines-of-customer-service/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Wonderful-Founders.png?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Israeli&amp;nbsp;AI agent&amp;nbsp;startup&amp;nbsp;Wonderful&amp;nbsp;has raised $100 million in a Series A round led by Index Ventures, with participation from Insight Partners, IVP, Bessemer, and Vine Ventures. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The large round, in a&amp;nbsp;market already crowded&amp;nbsp;with AI agent startups, suggests&amp;nbsp;Wonderful has convinced top tier investors&amp;nbsp;it’s&amp;nbsp;not just another GPT wrapper, but a company building&amp;nbsp;the infrastructure and orchestration that could scale if multi-agent systems take off.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The round brings&amp;nbsp;Wonderful’s&amp;nbsp;total funding to $134 million just four months after the startup came out of stealth with a seed round&amp;nbsp;and a promise to help&amp;nbsp;enterprises deploy customer-facing AI agents across voice, chat, and email in every market and every language.&amp;nbsp;The startup says it tailors the platform to each market it serves, fine-tuning for language, cultural norms, and regulatory environments, and even&amp;nbsp;organizes local teams to manage deployment.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That approach&amp;nbsp;has led to rapid growth&amp;nbsp;for&amp;nbsp;the young startup, which claims its AI agents are already managing tens of thousands of customer requests daily with an 80% resolve rate. Since launching, Wonderful has expanded to Italy, Switzerland, the Netherlands, Greece, Poland, Romania, the Baltics, the&amp;nbsp;Adriatics and the UAE.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With its fresh funding, Wonderful intends to launch in Germany, Austria, the Nordics and Portugal in 2025, and plans to expand in the Asia-Pacific region in early 2026.&amp;nbsp;But the&amp;nbsp;company&amp;nbsp;doesn’t&amp;nbsp;intend to&amp;nbsp;stop with&amp;nbsp;customer support&amp;nbsp;agents. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since&amp;nbsp;its&amp;nbsp;system plugs deeply into an enterprise customer’s existing software and can be tailored for each market, the startup says it will be able to give agents the capabilities to perform new tasks with minimal extra effort. It is currently exploring areas like&amp;nbsp;employee training, sales enablement, regulatory compliance, internal IT support, and onboarding, the company said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The promise of AI agents is clear, but putting that into practice, and critically, into production, is a huge challenge,” Bar Winkler, CEO and co-founder of Wonderful, said in a statement. “It requires marrying best-in-class technology together with flawless delivery, on the ground with customers.&amp;nbsp;That’s&amp;nbsp;been our approach with Wonderful, and&amp;nbsp;it’s&amp;nbsp;what has driven the accelerated adoption&amp;nbsp;we’ve&amp;nbsp;seen across markets in the last few months.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Customer-facing AI agents are&amp;nbsp;emerging&amp;nbsp;as the first real beachhead for the technology, and&amp;nbsp;investors were likely attracted by Wonderful’s&amp;nbsp;focus here. These use cases help enterprises cut costs by augmenting or replacing human support staff, and&amp;nbsp;they integrate&amp;nbsp;readily into existing call&amp;nbsp;centre&amp;nbsp;infrastructure. Crucially, they also carry less risk than having an AI make internal decisions autonomously —&amp;nbsp;a&amp;nbsp;use case that most enterprises&amp;nbsp;aren’t&amp;nbsp;ready to&amp;nbsp;adopt at&amp;nbsp;scale yet.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Index Ventures partner Hannah Seal pointed to&amp;nbsp;Wonderful’s&amp;nbsp;ability to “[move] from concept to global scale in less than a year” as a source of confidence for investors. The company’s true edge, she said, is its ability to deploy agents for global enterprises that function across every market and language.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jeff&amp;nbsp;Horing, co-founder and managing director at Insight Partners,&amp;nbsp;said&amp;nbsp;the adoption Wonderful is seeing across industries shows “just how valuable culturally fluent agents can be.”&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Wonderful-Founders.png?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Israeli&amp;nbsp;AI agent&amp;nbsp;startup&amp;nbsp;Wonderful&amp;nbsp;has raised $100 million in a Series A round led by Index Ventures, with participation from Insight Partners, IVP, Bessemer, and Vine Ventures. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The large round, in a&amp;nbsp;market already crowded&amp;nbsp;with AI agent startups, suggests&amp;nbsp;Wonderful has convinced top tier investors&amp;nbsp;it’s&amp;nbsp;not just another GPT wrapper, but a company building&amp;nbsp;the infrastructure and orchestration that could scale if multi-agent systems take off.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The round brings&amp;nbsp;Wonderful’s&amp;nbsp;total funding to $134 million just four months after the startup came out of stealth with a seed round&amp;nbsp;and a promise to help&amp;nbsp;enterprises deploy customer-facing AI agents across voice, chat, and email in every market and every language.&amp;nbsp;The startup says it tailors the platform to each market it serves, fine-tuning for language, cultural norms, and regulatory environments, and even&amp;nbsp;organizes local teams to manage deployment.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That approach&amp;nbsp;has led to rapid growth&amp;nbsp;for&amp;nbsp;the young startup, which claims its AI agents are already managing tens of thousands of customer requests daily with an 80% resolve rate. Since launching, Wonderful has expanded to Italy, Switzerland, the Netherlands, Greece, Poland, Romania, the Baltics, the&amp;nbsp;Adriatics and the UAE.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With its fresh funding, Wonderful intends to launch in Germany, Austria, the Nordics and Portugal in 2025, and plans to expand in the Asia-Pacific region in early 2026.&amp;nbsp;But the&amp;nbsp;company&amp;nbsp;doesn’t&amp;nbsp;intend to&amp;nbsp;stop with&amp;nbsp;customer support&amp;nbsp;agents. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since&amp;nbsp;its&amp;nbsp;system plugs deeply into an enterprise customer’s existing software and can be tailored for each market, the startup says it will be able to give agents the capabilities to perform new tasks with minimal extra effort. It is currently exploring areas like&amp;nbsp;employee training, sales enablement, regulatory compliance, internal IT support, and onboarding, the company said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The promise of AI agents is clear, but putting that into practice, and critically, into production, is a huge challenge,” Bar Winkler, CEO and co-founder of Wonderful, said in a statement. “It requires marrying best-in-class technology together with flawless delivery, on the ground with customers.&amp;nbsp;That’s&amp;nbsp;been our approach with Wonderful, and&amp;nbsp;it’s&amp;nbsp;what has driven the accelerated adoption&amp;nbsp;we’ve&amp;nbsp;seen across markets in the last few months.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Customer-facing AI agents are&amp;nbsp;emerging&amp;nbsp;as the first real beachhead for the technology, and&amp;nbsp;investors were likely attracted by Wonderful’s&amp;nbsp;focus here. These use cases help enterprises cut costs by augmenting or replacing human support staff, and&amp;nbsp;they integrate&amp;nbsp;readily into existing call&amp;nbsp;centre&amp;nbsp;infrastructure. Crucially, they also carry less risk than having an AI make internal decisions autonomously —&amp;nbsp;a&amp;nbsp;use case that most enterprises&amp;nbsp;aren’t&amp;nbsp;ready to&amp;nbsp;adopt at&amp;nbsp;scale yet.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Index Ventures partner Hannah Seal pointed to&amp;nbsp;Wonderful’s&amp;nbsp;ability to “[move] from concept to global scale in less than a year” as a source of confidence for investors. The company’s true edge, she said, is its ability to deploy agents for global enterprises that function across every market and language.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jeff&amp;nbsp;Horing, co-founder and managing director at Insight Partners,&amp;nbsp;said&amp;nbsp;the adoption Wonderful is seeing across industries shows “just how valuable culturally fluent agents can be.”&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/11/wonderful-raised-100m-series-a-to-put-ai-agents-on-the-front-lines-of-customer-service/</guid><pubDate>Tue, 11 Nov 2025 16:08:47 +0000</pubDate></item><item><title>BlueCodeAgent: A blue teaming agent enabled by automated red teaming for CodeGen AI (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/bluecodeagent-a-blue-teaming-agent-enabled-by-automated-red-teaming-for-codegen-ai/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a blue-to-green gradient background: the first icon shows a circle with connected nodes, the second shows a circuit, and the third shows a flowchart" class="wp-image-1154392" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/BlueCodeAgent-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="introduction"&gt;Introduction&lt;/h2&gt;



&lt;p&gt;Large&amp;nbsp;language&amp;nbsp;models&amp;nbsp;(LLMs)&amp;nbsp;are now widely used for automated code generation across software engineering tasks. However, this powerful capability in code generation also introduces security concerns. Code generation systems could be misused for harmful purposes, such as generating malicious code.&amp;nbsp;It&amp;nbsp;could also&amp;nbsp;produce&amp;nbsp;bias-filled&amp;nbsp;code reflecting&amp;nbsp;underlying logic that is&amp;nbsp;discriminatory&amp;nbsp;or unethical. Additionally, even when completing benign tasks, LLMs may inadvertently produce vulnerable code that&amp;nbsp;contains&amp;nbsp;security flaws (e.g., injection risks, unsafe input handling). These unsafe outcomes undermine the trustworthiness of code generation models and pose threats to the broader software ecosystem, where safety and reliability are critical.&lt;/p&gt;



&lt;p&gt;Many&amp;nbsp;studies have explored red teaming code LLMs, testing whether the models can reject unsafe requests and whether their generated code&amp;nbsp;exhibits&amp;nbsp;insecure patterns. For more details, see our earlier MSR blog post on&amp;nbsp;RedCodeAgent. While red teaming has significantly improved our understanding of model failure modes, progress on blue teaming—i.e., developing effective defensive mechanisms to detect and prevent such failures—remains&amp;nbsp;relatively limited. Current blue teaming approaches face several challenges: (1)&amp;nbsp;Poor alignment with security concepts:&amp;nbsp;additional&amp;nbsp;safety&amp;nbsp;prompts&amp;nbsp;struggle to help models&amp;nbsp;understand high-level notions,&amp;nbsp;such as what constitutes a malicious or bias instruction, and typically lack actionable principles to guide safe decision-making. A case study is shown in Figure 1.&amp;nbsp;(2)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Over-conservatism:&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;especially in the domain of vulnerable code detection, models tend to misclassify safe code as unsafe, leading to more false positives and reduced developer trust&lt;strong&gt;.&lt;/strong&gt;&amp;nbsp;(3)&amp;nbsp;Incomplete risk coverage: without a strong knowledge foundation, models perform poorly when dealing with subtle or previously unseen risks.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To address these challenges, researchers from the University of Chicago, University of California, Santa Barbara, University of Illinois Urbana–Champaign, VirtueAI, and Microsoft Research recently released a paper: BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI. This work makes the following key contributions:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Diverse red-teaming pipeline:&lt;/strong&gt; The authors design a comprehensive red-teaming process that integrates multiple strategies to synthesize diverse red-teaming data for effective knowledge accumulation.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Knowledge-enhanced blue teaming:&lt;/strong&gt; Building on the foundation of red-teaming knowledge, BlueCodeAgent significantly improves blue-teaming performance by leveraging constitutions derived from knowledge and dynamic testing.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Principled-Level Defense and Nuanced-Level analysis:&lt;/strong&gt; The authors propose two complementary strategies—Principled-Level Defense (via constitutions) and Nuanced-Level Analysis (via dynamic testing)—and demonstrate their synergistic effects in vulnerable code detection tasks.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Generalization to seen and unseen risks:&lt;/strong&gt; Empowered by comprehensive red-teaming knowledge, BlueCodeAgent generalizes effectively to unseen risks. Overall, BlueCodeAgent achieves an average 12.7% improvement in F1 score across four datasets and three tasks, attributed to its ability to distill actionable constitutions that enhance context-aware risk detection.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. A case study of BlueCodeAgent on the bias instruction detection task. Even when concepts such as “biased” are explicitly included in additional safety prompts, models often fail to recognize biased requests (left). BlueCodeAgent (right) addresses this gap by summarizing constitutions from knowledge and applying concrete, actionable constraints benefited from red teaming to improve the defense. " class="wp-image-1154397" height="371" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/BlueCodeAgent_figure1.png" width="1202" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. A case study of BlueCodeAgent on the bias instruction detection task. Even when concepts such as “biased” are explicitly included in additional safety prompts, models often fail to recognize biased requests (left). BlueCodeAgent (right) addresses this gap by summarizing constitutions from knowledge and applying concrete, actionable constraints benefited from red teaming to improve the defense. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="a-blue-teaming-agent-enabled-by-red-teaming"&gt;A blue teaming agent enabled by red teaming&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: Overview of BlueCodeAgent, an end-to-end blue teaming framework powered by automated red teaming for code security. By integrating knowledge derived from diverse red teaming and conducting dynamic sandbox-based testing, BlueCodeAgent substantially strengthens the defensive capabilities beyond static LLM analysis. " class="wp-image-1154396" height="581" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/BlueCodeAgent_figure2.png" width="1222" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2: Overview of BlueCodeAgent, an end-to-end blue teaming framework powered by automated red teaming for code security. By integrating knowledge derived from diverse red teaming and conducting dynamic sandbox-based testing, BlueCodeAgent substantially strengthens the defensive capabilities beyond static LLM analysis. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 2 presents an overview of the pipeline. The framework unifies both sides of the process: red teaming generates diverse risky cases and behaviors, which are then distilled into actionable constitutions that encode safety rules on the blue-teaming side. These constitutions guide BlueCodeAgent to more effectively detect unsafe textual inputs and code outputs, mitigating limitations such as poor alignment with abstract security concepts.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This work targets three major risk categories, covering both input/textual-level risks—including biased and malicious instructions—and output/code-level risks, where models may generate vulnerable code. These categories represent risks that have been widely studied in prior research.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="diverse-red-teaming-process-for-knowledge-accumulation"&gt;Diverse red-teaming process for knowledge accumulation&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Since different tasks require distinct attack strategies, the&amp;nbsp;red-teaming&amp;nbsp;employs multiple attack methods to generate realistic and diverse data. Specifically, the red-teaming process is divided into three categories:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Policy-based instance generation&lt;/strong&gt;: To synthesize policy-grounded red-teaming data, diverse security and ethical policies are first collected. These high-level principles are then used to prompt an uncensored model to generate instances that intentionally violate the specified policies.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seed-based adversarial prompt optimization&lt;/strong&gt;: Existing adversarial instructions are often overly simplistic and easily rejected by models. To overcome this limitation, an adaptive red-teaming agent invokes various jailbreak tools to iteratively refine initial seed prompts until the prompts achieve high attack success rates.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Knowledge-driven vulnerability generation&lt;/strong&gt;: To synthesize both vulnerable and safe code samples under realistic programming scenarios, domain knowledge of common software weaknesses (CWE) is leveraged to generate diverse code examples.&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 class="wp-block-heading" id="knowledge-enhanced-blue-teaming-agent"&gt;Knowledge-enhanced blue teaming agent&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;After accumulating red-teaming knowledge data, BlueCodeAgent set up &lt;strong&gt;Principled-Level Defense via Constitution Construction&lt;/strong&gt; and &lt;strong&gt;Nuanced-Level Analysis via Dynamic Testing&lt;/strong&gt;.&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Principled-Level Defense via Constitution Construction&lt;/strong&gt;&amp;nbsp;&lt;br /&gt;Based on the most relevant knowledge data&lt;strong&gt;, &lt;/strong&gt;BlueCodeAgent summarizes red-teamed knowledge into actionable constitutions—explicit rules and principles distilled from prior attack data. These constitutions serve as normative guidelines, enabling the model to stay aligned with ethical and security principles even when confronted with novel or unseen adversarial inputs.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Nuanced-Level Analysis via Dynamic Testing&lt;/strong&gt;&amp;nbsp;&lt;br /&gt;In vulnerable code detection, BlueCodeAgent augments static reasoning with dynamic sandbox-based analysis, executing generated code within isolated Docker environments to verify whether the model-reported vulnerabilities manifest as actual unsafe behaviors. This dynamic validation effectively mitigates the model’s tendency toward over-conservatism, where benign code is mistakenly flagged as vulnerable.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="insights-from-bluecodeagent"&gt;Insights from BlueCodeAgent&amp;nbsp;&lt;/h2&gt;



&lt;h3 class="wp-block-heading" id="bluecodeagent-outperforms-prompting-baselines"&gt;BlueCodeAgent outperforms prompting baselines&amp;nbsp;&lt;/h3&gt;



&lt;p&gt;As shown in Figure 3, BlueCodeAgent significantly outperforms other baselines. Several findings are highlighted.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;(1) Even when test categories differ from knowledge categories to simulate unseen scenarios, BlueCodeAgent effectively leverages previously seen risks to handle unseen ones, benefiting from its knowledge-enhanced safety reasoning.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;(2) BlueCodeAgent is model-agnostic, working consistently across diverse base LLMs, including both open-source and commercial models. Its F1 scores for bias and malicious instruction detection approach 1.0, highlighting strong effectiveness.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;(3) BlueCodeAgent achieves a strong balance between safety and usability. It accurately identifies unsafe inputs while maintaining a reasonable false-positive rate on benign ones, resulting in a consistently high F1 score.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;(4) By contrast, prompting with general or fine-grained safety reminders remains insufficient for effective blue teaming, as models struggle to internalize abstract safety concepts and apply them to unseen risky scenarios. BlueCodeAgent bridges this gap by distilling actionable constitutions from knowledge, using concrete and interpretable safety constraints to enhance model alignment.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3. F1 scores on bias instruction detection task (BlueCodeEval-Bias) in the first row and on malicious instruction detection task (BlueCodeEval-Mal, RedCode-based) in the second row. " class="wp-image-1154395" height="602" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/BlueCodeAgent_figure3.png" width="993" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt;&amp;nbsp;F1 scores on bias instruction detection task (BlueCodeEval-Bias) in the first row and on malicious instruction detection task (BlueCodeEval-Mal) in the second row.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="complementary-effects-of-constitutions-and-dynamic-testing"&gt;Complementary effects of constitutions and dynamic testing&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;In vulnerability detection tasks, models tend to behave conservatively—an effect also noted in prior research. They are often more likely to flag code as &lt;em&gt;unsafe&lt;/em&gt; rather than &lt;em&gt;safe&lt;/em&gt;. This bias is understandable: confirming that code is completely free from vulnerabilities is generally harder than spotting a potential issue.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To mitigate this over-conservatism, BlueCodeAgent integrates dynamic testing into its analysis pipeline. When BlueCodeAgent identifies a potential vulnerability, it triggers a reliable model (Claude-3.7-Sonnet-20250219) to generate test cases and corresponding executable code that embeds the suspicious snippet. These test cases are then run in a controlled environment to verify whether the vulnerability actually manifests. The final judgment combines the LLM’s analysis of the static code, the generated test code, run-time execution results, and constitutions derived from knowledge.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Researchers find the two components—constitutions and dynamic testing—play complementary roles. Constitutions expand the model’s understanding of risk, increasing true positives (TP) and reducing false negatives (FN). Dynamic testing, on the other hand, focuses on reducing false positives (FP) by validating whether predicted vulnerabilities can truly be triggered at run-time. Together, they make BlueCodeAgent both more accurate and more reliable in blue-teaming scenarios.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="summary"&gt;Summary&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;BlueCodeAgent introduces an end-to-end blue-teaming framework designed to address risks in code generation. The key insight behind BlueCodeAgent is that comprehensive red-teaming can greatly strengthen blue-teaming defenses. Based on this idea, the framework first builds a red-teaming process with diverse strategies for generating red-teaming data. It then constructs a blue-teaming agent that retrieves relevant examples from the red-teaming knowledge base and summarizes safety constitutions to guide LLMs in making accurate defensive decisions. A dynamic testing component is further added to reduce false positives in vulnerability detection.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Looking ahead, several directions hold promise.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;First, it is valuable to explore the generalization of BlueCodeAgent to other categories of code-generation risks beyond bias, malicious code, and vulnerable code. This may require designing and integrating novel red-teaming strategies into BlueCodeAgent and creating corresponding benchmarks for new risks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Second, scaling BlueCodeAgent to the file and repository levels could further enhance its real-world utility, which requires equipping agents with more advanced context retrieval tools and memory components.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Finally, beyond code generation, it is also important to extend BlueCodeAgent to mitigate risks in other modalities, including text, image, video, and audio, as well as in multimodal applications.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a blue-to-green gradient background: the first icon shows a circle with connected nodes, the second shows a circuit, and the third shows a flowchart" class="wp-image-1154392" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/BlueCodeAgent-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="introduction"&gt;Introduction&lt;/h2&gt;



&lt;p&gt;Large&amp;nbsp;language&amp;nbsp;models&amp;nbsp;(LLMs)&amp;nbsp;are now widely used for automated code generation across software engineering tasks. However, this powerful capability in code generation also introduces security concerns. Code generation systems could be misused for harmful purposes, such as generating malicious code.&amp;nbsp;It&amp;nbsp;could also&amp;nbsp;produce&amp;nbsp;bias-filled&amp;nbsp;code reflecting&amp;nbsp;underlying logic that is&amp;nbsp;discriminatory&amp;nbsp;or unethical. Additionally, even when completing benign tasks, LLMs may inadvertently produce vulnerable code that&amp;nbsp;contains&amp;nbsp;security flaws (e.g., injection risks, unsafe input handling). These unsafe outcomes undermine the trustworthiness of code generation models and pose threats to the broader software ecosystem, where safety and reliability are critical.&lt;/p&gt;



&lt;p&gt;Many&amp;nbsp;studies have explored red teaming code LLMs, testing whether the models can reject unsafe requests and whether their generated code&amp;nbsp;exhibits&amp;nbsp;insecure patterns. For more details, see our earlier MSR blog post on&amp;nbsp;RedCodeAgent. While red teaming has significantly improved our understanding of model failure modes, progress on blue teaming—i.e., developing effective defensive mechanisms to detect and prevent such failures—remains&amp;nbsp;relatively limited. Current blue teaming approaches face several challenges: (1)&amp;nbsp;Poor alignment with security concepts:&amp;nbsp;additional&amp;nbsp;safety&amp;nbsp;prompts&amp;nbsp;struggle to help models&amp;nbsp;understand high-level notions,&amp;nbsp;such as what constitutes a malicious or bias instruction, and typically lack actionable principles to guide safe decision-making. A case study is shown in Figure 1.&amp;nbsp;(2)&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Over-conservatism:&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;especially in the domain of vulnerable code detection, models tend to misclassify safe code as unsafe, leading to more false positives and reduced developer trust&lt;strong&gt;.&lt;/strong&gt;&amp;nbsp;(3)&amp;nbsp;Incomplete risk coverage: without a strong knowledge foundation, models perform poorly when dealing with subtle or previously unseen risks.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To address these challenges, researchers from the University of Chicago, University of California, Santa Barbara, University of Illinois Urbana–Champaign, VirtueAI, and Microsoft Research recently released a paper: BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI. This work makes the following key contributions:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Diverse red-teaming pipeline:&lt;/strong&gt; The authors design a comprehensive red-teaming process that integrates multiple strategies to synthesize diverse red-teaming data for effective knowledge accumulation.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Knowledge-enhanced blue teaming:&lt;/strong&gt; Building on the foundation of red-teaming knowledge, BlueCodeAgent significantly improves blue-teaming performance by leveraging constitutions derived from knowledge and dynamic testing.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Principled-Level Defense and Nuanced-Level analysis:&lt;/strong&gt; The authors propose two complementary strategies—Principled-Level Defense (via constitutions) and Nuanced-Level Analysis (via dynamic testing)—and demonstrate their synergistic effects in vulnerable code detection tasks.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Generalization to seen and unseen risks:&lt;/strong&gt; Empowered by comprehensive red-teaming knowledge, BlueCodeAgent generalizes effectively to unseen risks. Overall, BlueCodeAgent achieves an average 12.7% improvement in F1 score across four datasets and three tasks, attributed to its ability to distill actionable constitutions that enhance context-aware risk detection.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. A case study of BlueCodeAgent on the bias instruction detection task. Even when concepts such as “biased” are explicitly included in additional safety prompts, models often fail to recognize biased requests (left). BlueCodeAgent (right) addresses this gap by summarizing constitutions from knowledge and applying concrete, actionable constraints benefited from red teaming to improve the defense. " class="wp-image-1154397" height="371" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/BlueCodeAgent_figure1.png" width="1202" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. A case study of BlueCodeAgent on the bias instruction detection task. Even when concepts such as “biased” are explicitly included in additional safety prompts, models often fail to recognize biased requests (left). BlueCodeAgent (right) addresses this gap by summarizing constitutions from knowledge and applying concrete, actionable constraints benefited from red teaming to improve the defense. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="a-blue-teaming-agent-enabled-by-red-teaming"&gt;A blue teaming agent enabled by red teaming&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: Overview of BlueCodeAgent, an end-to-end blue teaming framework powered by automated red teaming for code security. By integrating knowledge derived from diverse red teaming and conducting dynamic sandbox-based testing, BlueCodeAgent substantially strengthens the defensive capabilities beyond static LLM analysis. " class="wp-image-1154396" height="581" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/BlueCodeAgent_figure2.png" width="1222" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2: Overview of BlueCodeAgent, an end-to-end blue teaming framework powered by automated red teaming for code security. By integrating knowledge derived from diverse red teaming and conducting dynamic sandbox-based testing, BlueCodeAgent substantially strengthens the defensive capabilities beyond static LLM analysis. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 2 presents an overview of the pipeline. The framework unifies both sides of the process: red teaming generates diverse risky cases and behaviors, which are then distilled into actionable constitutions that encode safety rules on the blue-teaming side. These constitutions guide BlueCodeAgent to more effectively detect unsafe textual inputs and code outputs, mitigating limitations such as poor alignment with abstract security concepts.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This work targets three major risk categories, covering both input/textual-level risks—including biased and malicious instructions—and output/code-level risks, where models may generate vulnerable code. These categories represent risks that have been widely studied in prior research.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="diverse-red-teaming-process-for-knowledge-accumulation"&gt;Diverse red-teaming process for knowledge accumulation&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Since different tasks require distinct attack strategies, the&amp;nbsp;red-teaming&amp;nbsp;employs multiple attack methods to generate realistic and diverse data. Specifically, the red-teaming process is divided into three categories:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Policy-based instance generation&lt;/strong&gt;: To synthesize policy-grounded red-teaming data, diverse security and ethical policies are first collected. These high-level principles are then used to prompt an uncensored model to generate instances that intentionally violate the specified policies.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seed-based adversarial prompt optimization&lt;/strong&gt;: Existing adversarial instructions are often overly simplistic and easily rejected by models. To overcome this limitation, an adaptive red-teaming agent invokes various jailbreak tools to iteratively refine initial seed prompts until the prompts achieve high attack success rates.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Knowledge-driven vulnerability generation&lt;/strong&gt;: To synthesize both vulnerable and safe code samples under realistic programming scenarios, domain knowledge of common software weaknesses (CWE) is leveraged to generate diverse code examples.&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 class="wp-block-heading" id="knowledge-enhanced-blue-teaming-agent"&gt;Knowledge-enhanced blue teaming agent&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;After accumulating red-teaming knowledge data, BlueCodeAgent set up &lt;strong&gt;Principled-Level Defense via Constitution Construction&lt;/strong&gt; and &lt;strong&gt;Nuanced-Level Analysis via Dynamic Testing&lt;/strong&gt;.&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Principled-Level Defense via Constitution Construction&lt;/strong&gt;&amp;nbsp;&lt;br /&gt;Based on the most relevant knowledge data&lt;strong&gt;, &lt;/strong&gt;BlueCodeAgent summarizes red-teamed knowledge into actionable constitutions—explicit rules and principles distilled from prior attack data. These constitutions serve as normative guidelines, enabling the model to stay aligned with ethical and security principles even when confronted with novel or unseen adversarial inputs.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Nuanced-Level Analysis via Dynamic Testing&lt;/strong&gt;&amp;nbsp;&lt;br /&gt;In vulnerable code detection, BlueCodeAgent augments static reasoning with dynamic sandbox-based analysis, executing generated code within isolated Docker environments to verify whether the model-reported vulnerabilities manifest as actual unsafe behaviors. This dynamic validation effectively mitigates the model’s tendency toward over-conservatism, where benign code is mistakenly flagged as vulnerable.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="insights-from-bluecodeagent"&gt;Insights from BlueCodeAgent&amp;nbsp;&lt;/h2&gt;



&lt;h3 class="wp-block-heading" id="bluecodeagent-outperforms-prompting-baselines"&gt;BlueCodeAgent outperforms prompting baselines&amp;nbsp;&lt;/h3&gt;



&lt;p&gt;As shown in Figure 3, BlueCodeAgent significantly outperforms other baselines. Several findings are highlighted.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;(1) Even when test categories differ from knowledge categories to simulate unseen scenarios, BlueCodeAgent effectively leverages previously seen risks to handle unseen ones, benefiting from its knowledge-enhanced safety reasoning.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;(2) BlueCodeAgent is model-agnostic, working consistently across diverse base LLMs, including both open-source and commercial models. Its F1 scores for bias and malicious instruction detection approach 1.0, highlighting strong effectiveness.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;(3) BlueCodeAgent achieves a strong balance between safety and usability. It accurately identifies unsafe inputs while maintaining a reasonable false-positive rate on benign ones, resulting in a consistently high F1 score.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;(4) By contrast, prompting with general or fine-grained safety reminders remains insufficient for effective blue teaming, as models struggle to internalize abstract safety concepts and apply them to unseen risky scenarios. BlueCodeAgent bridges this gap by distilling actionable constitutions from knowledge, using concrete and interpretable safety constraints to enhance model alignment.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3. F1 scores on bias instruction detection task (BlueCodeEval-Bias) in the first row and on malicious instruction detection task (BlueCodeEval-Mal, RedCode-based) in the second row. " class="wp-image-1154395" height="602" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/BlueCodeAgent_figure3.png" width="993" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt;&amp;nbsp;F1 scores on bias instruction detection task (BlueCodeEval-Bias) in the first row and on malicious instruction detection task (BlueCodeEval-Mal) in the second row.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="complementary-effects-of-constitutions-and-dynamic-testing"&gt;Complementary effects of constitutions and dynamic testing&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;In vulnerability detection tasks, models tend to behave conservatively—an effect also noted in prior research. They are often more likely to flag code as &lt;em&gt;unsafe&lt;/em&gt; rather than &lt;em&gt;safe&lt;/em&gt;. This bias is understandable: confirming that code is completely free from vulnerabilities is generally harder than spotting a potential issue.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To mitigate this over-conservatism, BlueCodeAgent integrates dynamic testing into its analysis pipeline. When BlueCodeAgent identifies a potential vulnerability, it triggers a reliable model (Claude-3.7-Sonnet-20250219) to generate test cases and corresponding executable code that embeds the suspicious snippet. These test cases are then run in a controlled environment to verify whether the vulnerability actually manifests. The final judgment combines the LLM’s analysis of the static code, the generated test code, run-time execution results, and constitutions derived from knowledge.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Researchers find the two components—constitutions and dynamic testing—play complementary roles. Constitutions expand the model’s understanding of risk, increasing true positives (TP) and reducing false negatives (FN). Dynamic testing, on the other hand, focuses on reducing false positives (FP) by validating whether predicted vulnerabilities can truly be triggered at run-time. Together, they make BlueCodeAgent both more accurate and more reliable in blue-teaming scenarios.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="summary"&gt;Summary&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;BlueCodeAgent introduces an end-to-end blue-teaming framework designed to address risks in code generation. The key insight behind BlueCodeAgent is that comprehensive red-teaming can greatly strengthen blue-teaming defenses. Based on this idea, the framework first builds a red-teaming process with diverse strategies for generating red-teaming data. It then constructs a blue-teaming agent that retrieves relevant examples from the red-teaming knowledge base and summarizes safety constitutions to guide LLMs in making accurate defensive decisions. A dynamic testing component is further added to reduce false positives in vulnerability detection.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Looking ahead, several directions hold promise.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;First, it is valuable to explore the generalization of BlueCodeAgent to other categories of code-generation risks beyond bias, malicious code, and vulnerable code. This may require designing and integrating novel red-teaming strategies into BlueCodeAgent and creating corresponding benchmarks for new risks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Second, scaling BlueCodeAgent to the file and repository levels could further enhance its real-world utility, which requires equipping agents with more advanced context retrieval tools and memory components.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Finally, beyond code generation, it is also important to extend BlueCodeAgent to mitigate risks in other modalities, including text, image, video, and audio, as well as in multimodal applications.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/bluecodeagent-a-blue-teaming-agent-enabled-by-automated-red-teaming-for-codegen-ai/</guid><pubDate>Tue, 11 Nov 2025 17:00:00 +0000</pubDate></item><item><title>Google announces even more AI in Photos app, powered by Nano Banana (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/11/googles-nano-banana-ai-image-editing-is-finally-coming-to-google-photos/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google’s Nano Banana is powering a raft of new features in the app.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Photos nano banana" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GP_Nov-AI-Feature_Blog-Post-Hero-640x361.png" width="640" /&gt;
                  &lt;img alt="Photos nano banana" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GP_Nov-AI-Feature_Blog-Post-Hero-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;We’re running out of ways to tell you that Google is releasing more generative AI features, but that’s what’s happening in Google Photos today. The Big G is finally making good on its promise to add its market-leading Nano Banana image-editing model to the app. The model powers a couple of features, and it’s not just for Google’s Android platform. Nano Banana edits are also coming to the iOS version of the app.&lt;/p&gt;
&lt;p&gt;Nano Banana started making waves when it appeared earlier this year as an unbranded demo. You simply feed the model an image and tell it what edits you want to see. Google said Nano Banana was destined for the Photos app back in October, but it’s only now beginning the rollout. The Photos app already had conversational editing in the “Help Me Edit” feature, but it was running an older non-fruit model that produced inferior results. Nano Banana editing will produce AI slop, yes, but it’s &lt;em&gt;better&lt;/em&gt; slop.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2126656-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Nano-Banana-in-Help-me-edit.mp4?_=1" type="video/mp4" /&gt;Nano Banana in Help me edit&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Nano Banana in Help me edit

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google says the updated Help Me Edit feature has access to your private face groups, so you can use names in your instructions. For example, you could type “Remove Riley’s sunglasses,” and Nano Banana will identify Riley in the photo (assuming you have a person of that name saved) and make the edit without further instructions. You can also ask for more fantastical edits in Help Me Edit, changing the style of the image from top to bottom.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google is very invested in getting people to use its AI tools, but less-savvy users might not be familiar enough with AI prompting to get the most out of Nano Banana. So Google Photos is also getting a collection of AI templates in a new “Create with AI” section. This menu will offer pre-formed prompts based on popular in-app edits. Some of the options you’ll see include “put me in a high fashion photoshoot,” “create a professional headshot,” and “put me in a winter holiday card.”&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2126656-2" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Ask-button.mp4?_=2" type="video/mp4" /&gt;The Ask button is yet another route to Gemini chitchat.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Ask button is yet another route to Gemini chitchat.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The app is also getting a new “Ask” button, which is not to be confused with “Ask Photos.” The former is a new contextual button that appears when viewing a photo, and the latter is Google’s controversial natural language search feature. Ask Photos is expanding to more than 100 new countries this week, but the new Ask button will only be available in the US for now. When looking at a photo, you can tap the Ask button to get information about the content of the photo or find related images. You can also describe edits you’d like to see in this interface, and Nano Banana will make them for you.&lt;/p&gt;
&lt;p&gt;According to Google, these new features are rolling out now—in Google-ese, “now” usually means a few days for full visibility.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google’s Nano Banana is powering a raft of new features in the app.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Photos nano banana" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GP_Nov-AI-Feature_Blog-Post-Hero-640x361.png" width="640" /&gt;
                  &lt;img alt="Photos nano banana" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GP_Nov-AI-Feature_Blog-Post-Hero-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;We’re running out of ways to tell you that Google is releasing more generative AI features, but that’s what’s happening in Google Photos today. The Big G is finally making good on its promise to add its market-leading Nano Banana image-editing model to the app. The model powers a couple of features, and it’s not just for Google’s Android platform. Nano Banana edits are also coming to the iOS version of the app.&lt;/p&gt;
&lt;p&gt;Nano Banana started making waves when it appeared earlier this year as an unbranded demo. You simply feed the model an image and tell it what edits you want to see. Google said Nano Banana was destined for the Photos app back in October, but it’s only now beginning the rollout. The Photos app already had conversational editing in the “Help Me Edit” feature, but it was running an older non-fruit model that produced inferior results. Nano Banana editing will produce AI slop, yes, but it’s &lt;em&gt;better&lt;/em&gt; slop.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2126656-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Nano-Banana-in-Help-me-edit.mp4?_=1" type="video/mp4" /&gt;Nano Banana in Help me edit&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Nano Banana in Help me edit

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google says the updated Help Me Edit feature has access to your private face groups, so you can use names in your instructions. For example, you could type “Remove Riley’s sunglasses,” and Nano Banana will identify Riley in the photo (assuming you have a person of that name saved) and make the edit without further instructions. You can also ask for more fantastical edits in Help Me Edit, changing the style of the image from top to bottom.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google is very invested in getting people to use its AI tools, but less-savvy users might not be familiar enough with AI prompting to get the most out of Nano Banana. So Google Photos is also getting a collection of AI templates in a new “Create with AI” section. This menu will offer pre-formed prompts based on popular in-app edits. Some of the options you’ll see include “put me in a high fashion photoshoot,” “create a professional headshot,” and “put me in a winter holiday card.”&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2126656-2" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Ask-button.mp4?_=2" type="video/mp4" /&gt;The Ask button is yet another route to Gemini chitchat.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Ask button is yet another route to Gemini chitchat.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The app is also getting a new “Ask” button, which is not to be confused with “Ask Photos.” The former is a new contextual button that appears when viewing a photo, and the latter is Google’s controversial natural language search feature. Ask Photos is expanding to more than 100 new countries this week, but the new Ask button will only be available in the US for now. When looking at a photo, you can tap the Ask button to get information about the content of the photo or find related images. You can also describe edits you’d like to see in this interface, and Nano Banana will make them for you.&lt;/p&gt;
&lt;p&gt;According to Google, these new features are rolling out now—in Google-ese, “now” usually means a few days for full visibility.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/11/googles-nano-banana-ai-image-editing-is-finally-coming-to-google-photos/</guid><pubDate>Tue, 11 Nov 2025 17:00:11 +0000</pubDate></item><item><title>Wiz: Security lapses emerge amid the global AI race (AI News)</title><link>https://www.artificialintelligence-news.com/news/wiz-security-lapses-emerge-amid-global-ai-race/</link><description>&lt;p&gt;According to Wiz, the race among AI companies is causing many to overlook basic security hygiene practices.&lt;/p&gt;&lt;p&gt;65 percent of the 50 leading AI firms the cybersecurity firm analysed had leaked verified secrets on GitHub. The exposures include API keys, tokens, and sensitive credentials, often buried in code repositories that standard security tools do not check.&lt;/p&gt;&lt;p&gt;Glyn Morgan, Country Manager for UK&amp;amp;I at Salt Security, described this trend as a preventable and basic error. “When AI firms accidentally expose their API keys they lay bare a glaring avoidable security failure,” he said.&lt;/p&gt;&lt;p&gt;“It’s the textbook example of governance paired with a security configuration, two of the risk categories that OWASP flags. By pushing credentials into code repositories they hand attackers a golden ticket to systems, data, and models, effectively sidestepping the usual defensive layers.”&lt;/p&gt;&lt;p&gt;Wiz’s report highlights the increasingly complex supply chain security risk. The problem extends beyond internal development teams; as enterprises increasingly partner with AI startups, they may inherit their security posture. The researchers warn that some of the leaks they found “could have exposed organisational structures, training data, or even private models.”&lt;/p&gt;&lt;p&gt;The financial stakes are considerable. The companies analysed with verified leaks have a combined valuation of over $400 billion.&lt;/p&gt;&lt;p&gt;The report, which focused on companies listed in the Forbes AI 50, provides examples of the risks:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;LangChain was found to have exposed multiple Langsmith API keys, some with permissions to manage the organisation and list its members. This type of information is highly valued by attackers for reconnaissance.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;An enterprise-tier API key for ElevenLabs was discovered sitting in a plaintext file.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;An unnamed AI 50 company had a HuggingFace token exposed in a deleted code fork. This single token “allow[ed] access to about 1K private models”. The same company also leaked WeightsAndBiases keys, exposing the “training data for many private models.”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Wiz report suggests this problem is so prevalent because traditional security scanning methods are no longer sufficient. Relying on basic scans of a company’s main GitHub repositories is a “commoditised approach” that misses the most severe risks .&lt;/p&gt;&lt;p&gt;The researchers describe the situation as an “iceberg” (i.e. the most obvious risks are visible, but the greater danger lies “below the surface”.) To find these hidden risks, the researchers adopted a three-dimensional scanning methodology they call “Depth, Perimeter, and Coverage”:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Depth:&lt;/strong&gt; Their deep scan analysed the “full commit history, commit history on forks, deleted forks, workflow logs and gists”—areas most scanners “never touch”.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Perimeter:&lt;/strong&gt; The scan was expanded beyond the core company organisation to include organisation members and contributors. These individuals might “inadvertently check company-related secrets into their own public repositories”. The team identified these adjacent accounts by tracking code contributors, organisation followers, and even “correlations in related networks like HuggingFace and npm.”&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Coverage:&lt;/strong&gt; The researchers specifically looked for new AI-related secret types that traditional scanners often miss, such as keys for platforms like WeightsAndBiases, Groq, and Perplexity.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This expanded attack surface is particularly worrying given the apparent lack of security maturity at many fast-moving companies. The report notes that when researchers tried to disclose the leaks, almost half of disclosures either failed to reach the target or received no response. Many firms lacked an official disclosure channel or simply failed to resolve the issue when notified.&lt;/p&gt;&lt;p&gt;Wiz’s findings serve as a warning for enterprise technology executives, highlighting three immediate action items for managing both internal and third-party security risk.&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Security leaders must treat their employees as part of their company’s attack surface. The report recommends creating a Version Control System (VCS) member policy to be applied during employee onboarding. This policy should mandate practices such as using multi-factor authentication for personal accounts and maintaining a strict separation between personal and professional activity on platforms like GitHub.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="2"&gt;&lt;li&gt;Internal secret scanning must evolve beyond basic repository checks. The report urges companies to mandate public VCS secret scanning as a “non-negotiable defense”. This scanning must adopt the aforementioned “Depth, Perimeter, and Coverage” mindset to find threats lurking below the surface.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="3"&gt;&lt;li&gt;This level of scrutiny must be extended to the entire AI supply chain. When evaluating or integrating tools from AI vendors, CISOs should probe their secrets management and vulnerability disclosure practices. The report notes that many AI service providers are leaking their own API keys and should “prioritise detection for their own secret types.”&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The central message for enterprises is that the tools and platforms defining the next generation of technology are being built at a pace that often outstrips security governance. As Wiz concludes, “For AI innovators, the message is clear: speed cannot compromise security”. For the enterprises that depend on that innovation, the same warning applies.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Exclusive: Dubai’s Digital Government chief says speed trumps spending in AI efficiency race&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;According to Wiz, the race among AI companies is causing many to overlook basic security hygiene practices.&lt;/p&gt;&lt;p&gt;65 percent of the 50 leading AI firms the cybersecurity firm analysed had leaked verified secrets on GitHub. The exposures include API keys, tokens, and sensitive credentials, often buried in code repositories that standard security tools do not check.&lt;/p&gt;&lt;p&gt;Glyn Morgan, Country Manager for UK&amp;amp;I at Salt Security, described this trend as a preventable and basic error. “When AI firms accidentally expose their API keys they lay bare a glaring avoidable security failure,” he said.&lt;/p&gt;&lt;p&gt;“It’s the textbook example of governance paired with a security configuration, two of the risk categories that OWASP flags. By pushing credentials into code repositories they hand attackers a golden ticket to systems, data, and models, effectively sidestepping the usual defensive layers.”&lt;/p&gt;&lt;p&gt;Wiz’s report highlights the increasingly complex supply chain security risk. The problem extends beyond internal development teams; as enterprises increasingly partner with AI startups, they may inherit their security posture. The researchers warn that some of the leaks they found “could have exposed organisational structures, training data, or even private models.”&lt;/p&gt;&lt;p&gt;The financial stakes are considerable. The companies analysed with verified leaks have a combined valuation of over $400 billion.&lt;/p&gt;&lt;p&gt;The report, which focused on companies listed in the Forbes AI 50, provides examples of the risks:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;LangChain was found to have exposed multiple Langsmith API keys, some with permissions to manage the organisation and list its members. This type of information is highly valued by attackers for reconnaissance.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;An enterprise-tier API key for ElevenLabs was discovered sitting in a plaintext file.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;An unnamed AI 50 company had a HuggingFace token exposed in a deleted code fork. This single token “allow[ed] access to about 1K private models”. The same company also leaked WeightsAndBiases keys, exposing the “training data for many private models.”&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Wiz report suggests this problem is so prevalent because traditional security scanning methods are no longer sufficient. Relying on basic scans of a company’s main GitHub repositories is a “commoditised approach” that misses the most severe risks .&lt;/p&gt;&lt;p&gt;The researchers describe the situation as an “iceberg” (i.e. the most obvious risks are visible, but the greater danger lies “below the surface”.) To find these hidden risks, the researchers adopted a three-dimensional scanning methodology they call “Depth, Perimeter, and Coverage”:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Depth:&lt;/strong&gt; Their deep scan analysed the “full commit history, commit history on forks, deleted forks, workflow logs and gists”—areas most scanners “never touch”.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Perimeter:&lt;/strong&gt; The scan was expanded beyond the core company organisation to include organisation members and contributors. These individuals might “inadvertently check company-related secrets into their own public repositories”. The team identified these adjacent accounts by tracking code contributors, organisation followers, and even “correlations in related networks like HuggingFace and npm.”&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Coverage:&lt;/strong&gt; The researchers specifically looked for new AI-related secret types that traditional scanners often miss, such as keys for platforms like WeightsAndBiases, Groq, and Perplexity.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This expanded attack surface is particularly worrying given the apparent lack of security maturity at many fast-moving companies. The report notes that when researchers tried to disclose the leaks, almost half of disclosures either failed to reach the target or received no response. Many firms lacked an official disclosure channel or simply failed to resolve the issue when notified.&lt;/p&gt;&lt;p&gt;Wiz’s findings serve as a warning for enterprise technology executives, highlighting three immediate action items for managing both internal and third-party security risk.&lt;/p&gt;&lt;ol class="wp-block-list"&gt;&lt;li&gt;Security leaders must treat their employees as part of their company’s attack surface. The report recommends creating a Version Control System (VCS) member policy to be applied during employee onboarding. This policy should mandate practices such as using multi-factor authentication for personal accounts and maintaining a strict separation between personal and professional activity on platforms like GitHub.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="2"&gt;&lt;li&gt;Internal secret scanning must evolve beyond basic repository checks. The report urges companies to mandate public VCS secret scanning as a “non-negotiable defense”. This scanning must adopt the aforementioned “Depth, Perimeter, and Coverage” mindset to find threats lurking below the surface.&lt;/li&gt;&lt;/ol&gt;&lt;ol class="wp-block-list" start="3"&gt;&lt;li&gt;This level of scrutiny must be extended to the entire AI supply chain. When evaluating or integrating tools from AI vendors, CISOs should probe their secrets management and vulnerability disclosure practices. The report notes that many AI service providers are leaking their own API keys and should “prioritise detection for their own secret types.”&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The central message for enterprises is that the tools and platforms defining the next generation of technology are being built at a pace that often outstrips security governance. As Wiz concludes, “For AI innovators, the message is clear: speed cannot compromise security”. For the enterprises that depend on that innovation, the same warning applies.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Exclusive: Dubai’s Digital Government chief says speed trumps spending in AI efficiency race&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/wiz-security-lapses-emerge-amid-global-ai-race/</guid><pubDate>Tue, 11 Nov 2025 17:05:25 +0000</pubDate></item><item><title>How AI startups should be thinking about product-market fit (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/11/how-ai-startups-should-be-thinking-about-product-market-fit/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/disrupt-2025-product-market-fit-panel.jpeg?resize=1200,859" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For all their pitches promising something new, AI startups share many of the same questions as startups in years past: How do they know when they’ve achieved the holy grail of product-market fit?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Product-market fit has been studied extensively over the years; entire books have been written about how to master the art. But as with so many things, AI is upending established practices.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Honestly, it just could not be more different from all the playbooks that we’ve all been taught in tech in the past,” Ann Bordetsky, a partner at New Enterprise Associates, told a standing room-only crowd at TechCrunch Disrupt in San Francisco. “It’s a completely different ball game.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Top of the list is the pace of change in the AI world. “The technology itself isn’t static,” she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even still, there are ways that founders and operators can evaluate whether they have product-market fit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the best things to watch, Murali Joshi, a partner at Iconiq, told the audience, is “durability of spend.” AI is still early in the adoption curve at many companies, and so much of their spend is focused on experimentation rather than integration.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Increasingly, we’re seeing people really shift away from just experimental AI budgets to core office of the CXO budgets,” Joshi said. “Digging into that is super critical to ensure that this is a tool, a solution, a platform that’s here to stay, versus something that they’re just testing and trying out.” &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Joshi also suggested startups consider classic metrics: daily, weekly, and monthly active users. “How frequently are your customers engaging with the tool and the product that they’re paying for?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bordetsky agreed, adding that qualitative data can help provide nuance to some of the quantitative metrics which might suggest, but not confirm, whether customers are likely to stick with a product. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you talk to customers or users, even in qualitative interviews, which we do tend to do a lot early on, that comes through very clearly,” she said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Interviewing people in the executive suite can be helpful, too, Joshi said. “Where does this sit in the tech stack?” he suggests asking them. He said that startups should think about how they can make themselves “more sticky as a product in terms of the core workflows.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lastly, it’s important for AI startups to think about product-market fit as a continuum, Bordetsky said. Product-market fit is not sort of one point in time,” she said. “It’s learning to think about how you maybe start with a little bit of product market fit in your space, but then really strengthen that over time.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/disrupt-2025-product-market-fit-panel.jpeg?resize=1200,859" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For all their pitches promising something new, AI startups share many of the same questions as startups in years past: How do they know when they’ve achieved the holy grail of product-market fit?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Product-market fit has been studied extensively over the years; entire books have been written about how to master the art. But as with so many things, AI is upending established practices.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Honestly, it just could not be more different from all the playbooks that we’ve all been taught in tech in the past,” Ann Bordetsky, a partner at New Enterprise Associates, told a standing room-only crowd at TechCrunch Disrupt in San Francisco. “It’s a completely different ball game.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Top of the list is the pace of change in the AI world. “The technology itself isn’t static,” she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even still, there are ways that founders and operators can evaluate whether they have product-market fit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the best things to watch, Murali Joshi, a partner at Iconiq, told the audience, is “durability of spend.” AI is still early in the adoption curve at many companies, and so much of their spend is focused on experimentation rather than integration.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Increasingly, we’re seeing people really shift away from just experimental AI budgets to core office of the CXO budgets,” Joshi said. “Digging into that is super critical to ensure that this is a tool, a solution, a platform that’s here to stay, versus something that they’re just testing and trying out.” &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Joshi also suggested startups consider classic metrics: daily, weekly, and monthly active users. “How frequently are your customers engaging with the tool and the product that they’re paying for?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bordetsky agreed, adding that qualitative data can help provide nuance to some of the quantitative metrics which might suggest, but not confirm, whether customers are likely to stick with a product. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you talk to customers or users, even in qualitative interviews, which we do tend to do a lot early on, that comes through very clearly,” she said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Interviewing people in the executive suite can be helpful, too, Joshi said. “Where does this sit in the tech stack?” he suggests asking them. He said that startups should think about how they can make themselves “more sticky as a product in terms of the core workflows.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lastly, it’s important for AI startups to think about product-market fit as a continuum, Bordetsky said. Product-market fit is not sort of one point in time,” she said. “It’s learning to think about how you maybe start with a little bit of product market fit in your space, but then really strengthen that over time.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/11/how-ai-startups-should-be-thinking-about-product-market-fit/</guid><pubDate>Tue, 11 Nov 2025 17:15:00 +0000</pubDate></item><item><title>Immortality startup Eternos nabs $10.3M, pivots to personal AI that sounds like you (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/11/immortality-startup-eternos-pivots-to-a-personal-ai-that-sounds-like-you/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Group-Photo-4-.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In 2023, after nearly three decades as CEO of the company he founded, Robert LoCascio stepped down as CEO of LivePerson, the public firm credited with pioneering web chat in 1997.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Generative AI advances inspired his next project, which he calls “the highest bar” for the technology: replicating human beings with their life stories and personality. In 2024, he founded and self-funded, Eternos, a legacy service that allows people to preserve their voice and stories for loved ones after they pass away. Now, it’s got a new name and modified mission. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup gained significant media attention after its first client, the terminally ill Michael Bommer, revealed how he worked with Eternos to create a digital replica of himself after spending 25 hours talking to Eternos about his life, interests, and worldview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LoCascio was set on building a legacy business, but what surprised him was that most of the people considering using Eternos weren’t preparing for death.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Eternos developed the Human Life Model (HLM) — a framework that uses only an individual’s data, rather than general LLM data, to capture their unique values, life story, and decision-making traits. LoCascio saw an opportunity to use this technology to help individuals create personal AIs for professional and personal use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced Tuesday it has rebranded as Uare.ai and raised $10.3 million in seed funding led by Mayfield and Boldstart Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I started to realize that the big models, they’re taking our datasets, and they’re getting smarter because of us,” LoCascio told TechCrunch. “We don’t have to take that path. You own the model, and you can share it and monetize it.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The vision for Uare.ai is to be a scaling tool for creators and professionals. Since the personal AI models hold the individual’s full expertise, a digital replica can be put to work to generate content, handle customer interactions, and even execute projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once Uare.ai’s platform launches later this year, individuals will be able to start training their HLMs by responding to Uare.ai questions about their lives using text, voice, and even video.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The first part is getting a human life story. Where’d you come from? Tell me a story about your childhood. What’s a crossroad in your life when you were younger?” LoCascio said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Uare.ai then asks the person to submit additional facts about their life, including information about their profession. “We blend the facts with this human life story, and that gives us your model,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike Character.ai and other chatbots, Uare.ai’s model won’t turn to general LLMs to fill the gaps about anything that’s not in HLM. “Our AIs will say, I don’t know if they can’t answer the question,” LoCascio said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Uare.ai intends to generate revenue through subscription fees or take a share of the revenue generated by customers who earn income from their digital twins.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another startup developing personal AIs is Sequoia-backed Delphi, which has attracted people with large followings, including Arnold Schwarzenegger, and enables others to interact with his replicated knowledge via voice or text.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Navin Chaddha, managing partner at Mayfield, believes Uare.ai stands out from competitors because it targets individual professionals like CPAs. Plus, it has LoCascio, a very successful entrepreneur at the helm, he told TechCrunch.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Group-Photo-4-.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In 2023, after nearly three decades as CEO of the company he founded, Robert LoCascio stepped down as CEO of LivePerson, the public firm credited with pioneering web chat in 1997.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Generative AI advances inspired his next project, which he calls “the highest bar” for the technology: replicating human beings with their life stories and personality. In 2024, he founded and self-funded, Eternos, a legacy service that allows people to preserve their voice and stories for loved ones after they pass away. Now, it’s got a new name and modified mission. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup gained significant media attention after its first client, the terminally ill Michael Bommer, revealed how he worked with Eternos to create a digital replica of himself after spending 25 hours talking to Eternos about his life, interests, and worldview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LoCascio was set on building a legacy business, but what surprised him was that most of the people considering using Eternos weren’t preparing for death.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Eternos developed the Human Life Model (HLM) — a framework that uses only an individual’s data, rather than general LLM data, to capture their unique values, life story, and decision-making traits. LoCascio saw an opportunity to use this technology to help individuals create personal AIs for professional and personal use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced Tuesday it has rebranded as Uare.ai and raised $10.3 million in seed funding led by Mayfield and Boldstart Ventures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I started to realize that the big models, they’re taking our datasets, and they’re getting smarter because of us,” LoCascio told TechCrunch. “We don’t have to take that path. You own the model, and you can share it and monetize it.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The vision for Uare.ai is to be a scaling tool for creators and professionals. Since the personal AI models hold the individual’s full expertise, a digital replica can be put to work to generate content, handle customer interactions, and even execute projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once Uare.ai’s platform launches later this year, individuals will be able to start training their HLMs by responding to Uare.ai questions about their lives using text, voice, and even video.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The first part is getting a human life story. Where’d you come from? Tell me a story about your childhood. What’s a crossroad in your life when you were younger?” LoCascio said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Uare.ai then asks the person to submit additional facts about their life, including information about their profession. “We blend the facts with this human life story, and that gives us your model,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike Character.ai and other chatbots, Uare.ai’s model won’t turn to general LLMs to fill the gaps about anything that’s not in HLM. “Our AIs will say, I don’t know if they can’t answer the question,” LoCascio said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Uare.ai intends to generate revenue through subscription fees or take a share of the revenue generated by customers who earn income from their digital twins.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another startup developing personal AIs is Sequoia-backed Delphi, which has attracted people with large followings, including Arnold Schwarzenegger, and enables others to interact with his replicated knowledge via voice or text.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Navin Chaddha, managing partner at Mayfield, believes Uare.ai stands out from competitors because it targets individual professionals like CPAs. Plus, it has LoCascio, a very successful entrepreneur at the helm, he told TechCrunch.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/11/immortality-startup-eternos-pivots-to-a-personal-ai-that-sounds-like-you/</guid><pubDate>Tue, 11 Nov 2025 17:29:58 +0000</pubDate></item><item><title>[NEW] Only 9% of developers think AI code can be used without human oversight, BairesDev survey reveals (AI | VentureBeat)</title><link>https://venturebeat.com/ai/only-9-of-developers-think-ai-code-can-be-used-without-human-oversight</link><description>[unable to retrieve full-text content]&lt;p&gt;Senior software developers are preparing for a major shift in how they work as artificial intelligence becomes central to their workflows, according to &lt;a href="https://www.bairesdev.com/"&gt;BairesDev’s&lt;/a&gt; latest &lt;i&gt;Dev Barometer&lt;/i&gt; report &lt;a href="https://www.bairesdev.com/blog/dev-barometer-q4-the-ai-native-workforce/"&gt;published today&lt;/a&gt;. VentureBeat was given an exclusive early look and the findings below come directly from that report. &lt;/p&gt;&lt;p&gt;The quarterly global survey, which polled 501 developers and 19 project managers across 92 software initiatives, finds that nearly two-thirds (65%) of senior developers expect their roles to be redefined by AI in 2026. &lt;/p&gt;&lt;p&gt;The data highlights a transformation underway in software development: fewer routine coding tasks, more emphasis on design and strategy, and a rising need for AI fluency.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Coders to Strategists&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Among those anticipating change, 74% say they expect to shift from hands-on coding to designing solutions. &lt;/p&gt;&lt;p&gt;Another 61% plan to integrate AI-generated code into their workflows, and half foresee spending more time on system strategy and architecture.&lt;/p&gt;&lt;p&gt;“It’s not about lines of code anymore,” said Justice Erolin, Chief Technology Officer at BairesDev, in a recent interview with &lt;i&gt;VentureBeat&lt;/i&gt; conducted over video call. “It’s about the quality and type of code, and the kind of work developers are doing.”&lt;/p&gt;&lt;p&gt;Erolin said the company is watching developers evolve from individual contributors into system thinkers.&lt;/p&gt;&lt;p&gt;“AI is great at code scaffolding and generating unit tests, saving developers around eight hours a week,” he explained. “That time can now be used for solution architecture and strategy work—areas where AI still falls short.”&lt;/p&gt;&lt;p&gt;The survey’s data reflects this shift. Developers are moving toward higher-value tasks while automation takes over much of the repetitive coding that once occupied junior engineers.&lt;/p&gt;&lt;p&gt;Erolin noted that BairesDev’s internal data mirrors these findings. “We’re seeing a shift where senior engineers with AI tools are outperforming, and even replacing, the traditional senior-plus-junior team setup,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Realism About AI’s Limits&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite widespread enthusiasm, developers remain cautious about AI’s reliability.&lt;/p&gt;&lt;p&gt;Over half (56%) describe AI-generated code as “somewhat reliable,” saying it still requires validation for accuracy and security.&lt;b&gt; Only 9%&lt;/b&gt; trust it enough to use without human oversight.&lt;/p&gt;&lt;p&gt;Erolin agreed with that sentiment. “AI doesn’t replace human oversight,” he said. “Even as tools improve, developers still need to understand how individual components fit into the bigger system.” &lt;/p&gt;&lt;p&gt;He added that the biggest constraint in large language models today is “their context window”—the limited ability to retain and reason across entire systems. “Engineers need to think holistically about architecture, not just individual lines of code,” he said.&lt;/p&gt;&lt;p&gt;The CTO described 2025 as a turning point for how engineers use AI tools like GitHub Copilot, Cursor, Claude, and OpenAI’s models. “We’re tracking what tools and models our engineers use,” he said. “But the bigger story is how those tools impact learning, productivity, and oversight.”&lt;/p&gt;&lt;p&gt;That tempered optimism aligns with BairesDev’s previous &lt;i&gt;Dev Barometer&lt;/i&gt; findings, which reported that 92% of developers were already using AI-assisted coding by Q3 2025, saving an average of 7.3 hours per week.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Year of Upskilling&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In 2025, AI integration already brought tangible professional benefits. 74% of developers said the technology strengthened their technical skills, 50% reported better work-life balance, and 37% said AI tools expanded their career opportunities.&lt;/p&gt;&lt;p&gt;Erolin said the company is seeing AI emerge as “a top use case for upskilling.” Developers use it to “learn new technologies faster and fill knowledge gaps,” he noted. “When developers understand how AI works and its limitations, they can use it to enhance—not replace—their critical thinking. They prompt better and learn more efficiently.”&lt;/p&gt;&lt;p&gt;Still, he warned of a potential long-term risk in the industry’s current trajectory. “If junior engineers are being replaced or not hired, we’ll face a shortage of qualified senior engineers in ten years as current ones retire,” Erolin said.&lt;/p&gt;&lt;p&gt;The &lt;i&gt;Dev Barometer&lt;/i&gt; findings echo that concern. Developers expect leaner teams, but many also worry that fewer entry-level opportunities could lead to long-term talent pipeline issues.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Leaner Teams, New Priorities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Developers expect 2026 to bring smaller, more specialized teams. 58% say automation will reduce entry-level tasks, while 63% expect new career paths to emerge as AI redefines team structures. 59% anticipate that AI will create entirely new specialized roles.&lt;/p&gt;&lt;p&gt;According to BairesDev’s data, developers currently divide their time between writing code (48%), debugging (42%), and documentation (35%). Only 19% report focusing primarily on creative problem-solving and innovation—a share that’s expected to grow as AI removes lower-level coding tasks.&lt;/p&gt;&lt;p&gt;The report also highlights where developers see the fastest-growing areas for 2026: AI/ML (67%), data analytics (46%), and cybersecurity (45%). In parallel, 63% of project managers said developers will need more training in AI, cloud, and security.&lt;/p&gt;&lt;p&gt;Erolin described the next generation of developers as “T-shaped engineers”—people with broad system knowledge and deep expertise in one or more areas. “The most important developer moving forward will be the T-shaped engineer,” he said. “Broad in understanding, deep in skill.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;AI as an Industry Standard&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Q4 &lt;i&gt;Dev Barometer&lt;/i&gt; frames AI not as an experiment but as a foundation for how teams will operate in 2026. Developers are moving beyond using AI as a coding shortcut and instead incorporating it into architecture, validation, and design decisions.&lt;/p&gt;&lt;p&gt;Erolin emphasized that BairesDev is already adapting its internal teams to this new reality. “Our engineers are full-time with us, and we staff them out where they’re needed,” he said. “Some clients need help for six months to a year; others outsource their entire dev team to us.”&lt;/p&gt;&lt;p&gt;He said BairesDev provides “about 5,000 software engineers from Latin America, offering clients timezone-aligned, culturally aligned, and highly fluent English-speaking talent.”&lt;/p&gt;&lt;p&gt;As developers integrate AI deeper into their daily work, Erolin believes the competitive advantage will belong to those who understand both the technology’s capabilities and its constraints. “When developers learn to collaborate with AI instead of compete against it, that’s when the real productivity and creativity gains happen,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Background: Who BairesDev Is&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Founded in Buenos Aires in 2009 by Nacho De Marco and Paul Azorin, BairesDev began with a mission to connect what it describes as the “top 1%” of Latin American developers with global companies seeking high-quality software solutions. The company grew from those early roots into a major nearshore software development and staffing provider, offering everything from individual developer placements to full end-to-end project outsourcing.&lt;/p&gt;&lt;p&gt;Today, BairesDev claims to have delivered more than 1,200 projects across 130+ industries, serving hundreds of clients ranging from startups to Fortune 500 firms such as Google, Adobe, and Rolls-Royce. It operates with a remote-first model and a workforce of over 4,000 professionals across more than 40 countries, aligning its teams to North American time zones.&lt;/p&gt;&lt;p&gt;The company emphasizes three core advantages: access to elite technical talent across 100+ technologies, rapid scalability for project needs, and nearshore proximity for real-time collaboration. It reports client relationships averaging over three years and a satisfaction rate around 91%.&lt;/p&gt;&lt;p&gt;BairesDev’s unique position—bridging Latin American talent with global enterprise clients—gives it an unusually data-rich perspective on how AI is transforming software development at scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Takeaway&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The &lt;i&gt;Dev Barometer&lt;/i&gt;’s Q4 2025 results suggest 2026 will mark a turning point for software engineering. Developers are becoming system architects rather than pure coders, AI literacy is becoming a baseline requirement, and traditional entry-level roles may give way to new, specialized positions.&lt;/p&gt;&lt;p&gt;As AI becomes embedded in every stage of development—from design to testing—developers who can combine technical fluency with strategic thinking are set to lead the next era of software creation.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Senior software developers are preparing for a major shift in how they work as artificial intelligence becomes central to their workflows, according to &lt;a href="https://www.bairesdev.com/"&gt;BairesDev’s&lt;/a&gt; latest &lt;i&gt;Dev Barometer&lt;/i&gt; report &lt;a href="https://www.bairesdev.com/blog/dev-barometer-q4-the-ai-native-workforce/"&gt;published today&lt;/a&gt;. VentureBeat was given an exclusive early look and the findings below come directly from that report. &lt;/p&gt;&lt;p&gt;The quarterly global survey, which polled 501 developers and 19 project managers across 92 software initiatives, finds that nearly two-thirds (65%) of senior developers expect their roles to be redefined by AI in 2026. &lt;/p&gt;&lt;p&gt;The data highlights a transformation underway in software development: fewer routine coding tasks, more emphasis on design and strategy, and a rising need for AI fluency.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Coders to Strategists&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Among those anticipating change, 74% say they expect to shift from hands-on coding to designing solutions. &lt;/p&gt;&lt;p&gt;Another 61% plan to integrate AI-generated code into their workflows, and half foresee spending more time on system strategy and architecture.&lt;/p&gt;&lt;p&gt;“It’s not about lines of code anymore,” said Justice Erolin, Chief Technology Officer at BairesDev, in a recent interview with &lt;i&gt;VentureBeat&lt;/i&gt; conducted over video call. “It’s about the quality and type of code, and the kind of work developers are doing.”&lt;/p&gt;&lt;p&gt;Erolin said the company is watching developers evolve from individual contributors into system thinkers.&lt;/p&gt;&lt;p&gt;“AI is great at code scaffolding and generating unit tests, saving developers around eight hours a week,” he explained. “That time can now be used for solution architecture and strategy work—areas where AI still falls short.”&lt;/p&gt;&lt;p&gt;The survey’s data reflects this shift. Developers are moving toward higher-value tasks while automation takes over much of the repetitive coding that once occupied junior engineers.&lt;/p&gt;&lt;p&gt;Erolin noted that BairesDev’s internal data mirrors these findings. “We’re seeing a shift where senior engineers with AI tools are outperforming, and even replacing, the traditional senior-plus-junior team setup,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Realism About AI’s Limits&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite widespread enthusiasm, developers remain cautious about AI’s reliability.&lt;/p&gt;&lt;p&gt;Over half (56%) describe AI-generated code as “somewhat reliable,” saying it still requires validation for accuracy and security.&lt;b&gt; Only 9%&lt;/b&gt; trust it enough to use without human oversight.&lt;/p&gt;&lt;p&gt;Erolin agreed with that sentiment. “AI doesn’t replace human oversight,” he said. “Even as tools improve, developers still need to understand how individual components fit into the bigger system.” &lt;/p&gt;&lt;p&gt;He added that the biggest constraint in large language models today is “their context window”—the limited ability to retain and reason across entire systems. “Engineers need to think holistically about architecture, not just individual lines of code,” he said.&lt;/p&gt;&lt;p&gt;The CTO described 2025 as a turning point for how engineers use AI tools like GitHub Copilot, Cursor, Claude, and OpenAI’s models. “We’re tracking what tools and models our engineers use,” he said. “But the bigger story is how those tools impact learning, productivity, and oversight.”&lt;/p&gt;&lt;p&gt;That tempered optimism aligns with BairesDev’s previous &lt;i&gt;Dev Barometer&lt;/i&gt; findings, which reported that 92% of developers were already using AI-assisted coding by Q3 2025, saving an average of 7.3 hours per week.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Year of Upskilling&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In 2025, AI integration already brought tangible professional benefits. 74% of developers said the technology strengthened their technical skills, 50% reported better work-life balance, and 37% said AI tools expanded their career opportunities.&lt;/p&gt;&lt;p&gt;Erolin said the company is seeing AI emerge as “a top use case for upskilling.” Developers use it to “learn new technologies faster and fill knowledge gaps,” he noted. “When developers understand how AI works and its limitations, they can use it to enhance—not replace—their critical thinking. They prompt better and learn more efficiently.”&lt;/p&gt;&lt;p&gt;Still, he warned of a potential long-term risk in the industry’s current trajectory. “If junior engineers are being replaced or not hired, we’ll face a shortage of qualified senior engineers in ten years as current ones retire,” Erolin said.&lt;/p&gt;&lt;p&gt;The &lt;i&gt;Dev Barometer&lt;/i&gt; findings echo that concern. Developers expect leaner teams, but many also worry that fewer entry-level opportunities could lead to long-term talent pipeline issues.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Leaner Teams, New Priorities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Developers expect 2026 to bring smaller, more specialized teams. 58% say automation will reduce entry-level tasks, while 63% expect new career paths to emerge as AI redefines team structures. 59% anticipate that AI will create entirely new specialized roles.&lt;/p&gt;&lt;p&gt;According to BairesDev’s data, developers currently divide their time between writing code (48%), debugging (42%), and documentation (35%). Only 19% report focusing primarily on creative problem-solving and innovation—a share that’s expected to grow as AI removes lower-level coding tasks.&lt;/p&gt;&lt;p&gt;The report also highlights where developers see the fastest-growing areas for 2026: AI/ML (67%), data analytics (46%), and cybersecurity (45%). In parallel, 63% of project managers said developers will need more training in AI, cloud, and security.&lt;/p&gt;&lt;p&gt;Erolin described the next generation of developers as “T-shaped engineers”—people with broad system knowledge and deep expertise in one or more areas. “The most important developer moving forward will be the T-shaped engineer,” he said. “Broad in understanding, deep in skill.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;AI as an Industry Standard&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Q4 &lt;i&gt;Dev Barometer&lt;/i&gt; frames AI not as an experiment but as a foundation for how teams will operate in 2026. Developers are moving beyond using AI as a coding shortcut and instead incorporating it into architecture, validation, and design decisions.&lt;/p&gt;&lt;p&gt;Erolin emphasized that BairesDev is already adapting its internal teams to this new reality. “Our engineers are full-time with us, and we staff them out where they’re needed,” he said. “Some clients need help for six months to a year; others outsource their entire dev team to us.”&lt;/p&gt;&lt;p&gt;He said BairesDev provides “about 5,000 software engineers from Latin America, offering clients timezone-aligned, culturally aligned, and highly fluent English-speaking talent.”&lt;/p&gt;&lt;p&gt;As developers integrate AI deeper into their daily work, Erolin believes the competitive advantage will belong to those who understand both the technology’s capabilities and its constraints. “When developers learn to collaborate with AI instead of compete against it, that’s when the real productivity and creativity gains happen,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Background: Who BairesDev Is&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Founded in Buenos Aires in 2009 by Nacho De Marco and Paul Azorin, BairesDev began with a mission to connect what it describes as the “top 1%” of Latin American developers with global companies seeking high-quality software solutions. The company grew from those early roots into a major nearshore software development and staffing provider, offering everything from individual developer placements to full end-to-end project outsourcing.&lt;/p&gt;&lt;p&gt;Today, BairesDev claims to have delivered more than 1,200 projects across 130+ industries, serving hundreds of clients ranging from startups to Fortune 500 firms such as Google, Adobe, and Rolls-Royce. It operates with a remote-first model and a workforce of over 4,000 professionals across more than 40 countries, aligning its teams to North American time zones.&lt;/p&gt;&lt;p&gt;The company emphasizes three core advantages: access to elite technical talent across 100+ technologies, rapid scalability for project needs, and nearshore proximity for real-time collaboration. It reports client relationships averaging over three years and a satisfaction rate around 91%.&lt;/p&gt;&lt;p&gt;BairesDev’s unique position—bridging Latin American talent with global enterprise clients—gives it an unusually data-rich perspective on how AI is transforming software development at scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The Takeaway&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The &lt;i&gt;Dev Barometer&lt;/i&gt;’s Q4 2025 results suggest 2026 will mark a turning point for software engineering. Developers are becoming system architects rather than pure coders, AI literacy is becoming a baseline requirement, and traditional entry-level roles may give way to new, specialized positions.&lt;/p&gt;&lt;p&gt;As AI becomes embedded in every stage of development—from design to testing—developers who can combine technical fluency with strategic thinking are set to lead the next era of software creation.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/only-9-of-developers-think-ai-code-can-be-used-without-human-oversight</guid><pubDate>Tue, 11 Nov 2025 19:43:00 +0000</pubDate></item><item><title>[NEW] SoftBank’s Nvidia sale rattles market, raises questions (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/11/softbanks-nvidia-sale-rattles-market-raises-questions/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/05/GettyImages-1142417807.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Masayoshi Son isn’t known for half measures. The SoftBank founder’s career has been studded with eyebrow-raising bets, each one seemingly more outrageous than the last. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His latest move is to cash out his entire $5.8 billion Nvidia stake to go all-in on AI. And while it surprised the business world on Tuesday, it maybe should not. At this point, it’s almost more surprising when the 68-year-old Son doesn’t push his chips to the center of the table.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Consider that during the late 1990s dot-com bubble, Son’s net worth soared to about $78 billion by February 2000, briefly making him the richest person in the world. Then came the ugly dot-com implosion months later. He lost $70 billion personally – which, at the time, was the largest financial loss by any individual in history — as SoftBank’s market cap plummeted 98% from $180 billion to just $2.5 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amid that terribleness, Son made what would become his most legendary bet: a $20 million investment in Alibaba in 2000, one decided (the story goes) after just a six-minute meeting with Jack Ma. That stake would eventually grow to be worth $150 billion by 2020, transforming him into one of the venture industry’s most celebrated figures and funding his comeback.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That Alibaba success has often made it harder to see when Son has stayed too long at the table. When Son needed capital to launch his first Vision Fund in 2017, he didn’t hesitate to seek $45 billion from Saudi Arabia’s Public Investment Fund – long before taking Saudi money became acceptable in Silicon Valley. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After journalist Jamal Khashoggi was murdered in October 2018, Son condemned the killing as “horrific and deeply regrettable” but insisted SoftBank couldn’t “turn our backs on the Saudi people,” maintaining the firm’s commitment to managing the kingdom’s capital. In fact, the Vision Fund actually ramped up dealmaking soon after.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That didn’t turn out so well. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A big bet on Uber generated paper losses for years. Then came WeWork. Son overrode his lieutenants’ objections, fell “in love” with founder Adam Neumann, and assigned the co-working company a dizzying valuation of $47 billion in early 2019 after making several previous investments in the company. But WeWork’s IPO plans collapsed after it published a famously troubling S-1 filing. The company never quite recovered – even after pushing out Neumann and instituting a series of belt-tightening measures – ultimately costing SoftBank $11.5 billion in equity losses and another $2.2 billion in debt. (Son reportedly later called it “a stain on my life.”)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Son has been mounting another comeback for years, and Tuesday will undoubtedly be remembered as an important moment in his turnaround tale. Indeed, it will likely be recalled as the day SoftBank revealed it had sold all 32.1 million of its Nvidia shares – not to diversify its bets but instead to double down elsewhere, including on a planned $30 billion commitment to OpenAI and to participate (it reportedly hopes) in a $1 trillion AI manufacturing hub in Arizona.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If selling that position still gives Son some heartburn, that’s understandable. At about $181.58 per share, SoftBank exited just 14% below Nvidia’s all-time high of $212.19, which is a strong look. That’s remarkably close to peak valuation for such a huge position. Still, the move marks SoftBank’s second complete exit from Nvidia, and the first one was exceedingly costly. (In 2019, SoftBank sold a $4 billion stake in the company for $3.6 billion, shares that would now be worth more than $150 billion.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move also rattled the market. As of this writing, Nvidia shares are down nearly 3% following the disclosure, even as analysts emphasize that the sale “should not be seen as a cautious or negative stance on Nvidia,” but rather reflects SoftBank needing capital for its AI ambitions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wall Street can’t help but wonder: does Son see something right now that others do not? Judging by his track record, maybe — and that ambiguity is all investors have to go on.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/05/GettyImages-1142417807.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Masayoshi Son isn’t known for half measures. The SoftBank founder’s career has been studded with eyebrow-raising bets, each one seemingly more outrageous than the last. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His latest move is to cash out his entire $5.8 billion Nvidia stake to go all-in on AI. And while it surprised the business world on Tuesday, it maybe should not. At this point, it’s almost more surprising when the 68-year-old Son doesn’t push his chips to the center of the table.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Consider that during the late 1990s dot-com bubble, Son’s net worth soared to about $78 billion by February 2000, briefly making him the richest person in the world. Then came the ugly dot-com implosion months later. He lost $70 billion personally – which, at the time, was the largest financial loss by any individual in history — as SoftBank’s market cap plummeted 98% from $180 billion to just $2.5 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amid that terribleness, Son made what would become his most legendary bet: a $20 million investment in Alibaba in 2000, one decided (the story goes) after just a six-minute meeting with Jack Ma. That stake would eventually grow to be worth $150 billion by 2020, transforming him into one of the venture industry’s most celebrated figures and funding his comeback.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That Alibaba success has often made it harder to see when Son has stayed too long at the table. When Son needed capital to launch his first Vision Fund in 2017, he didn’t hesitate to seek $45 billion from Saudi Arabia’s Public Investment Fund – long before taking Saudi money became acceptable in Silicon Valley. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After journalist Jamal Khashoggi was murdered in October 2018, Son condemned the killing as “horrific and deeply regrettable” but insisted SoftBank couldn’t “turn our backs on the Saudi people,” maintaining the firm’s commitment to managing the kingdom’s capital. In fact, the Vision Fund actually ramped up dealmaking soon after.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That didn’t turn out so well. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A big bet on Uber generated paper losses for years. Then came WeWork. Son overrode his lieutenants’ objections, fell “in love” with founder Adam Neumann, and assigned the co-working company a dizzying valuation of $47 billion in early 2019 after making several previous investments in the company. But WeWork’s IPO plans collapsed after it published a famously troubling S-1 filing. The company never quite recovered – even after pushing out Neumann and instituting a series of belt-tightening measures – ultimately costing SoftBank $11.5 billion in equity losses and another $2.2 billion in debt. (Son reportedly later called it “a stain on my life.”)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Son has been mounting another comeback for years, and Tuesday will undoubtedly be remembered as an important moment in his turnaround tale. Indeed, it will likely be recalled as the day SoftBank revealed it had sold all 32.1 million of its Nvidia shares – not to diversify its bets but instead to double down elsewhere, including on a planned $30 billion commitment to OpenAI and to participate (it reportedly hopes) in a $1 trillion AI manufacturing hub in Arizona.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If selling that position still gives Son some heartburn, that’s understandable. At about $181.58 per share, SoftBank exited just 14% below Nvidia’s all-time high of $212.19, which is a strong look. That’s remarkably close to peak valuation for such a huge position. Still, the move marks SoftBank’s second complete exit from Nvidia, and the first one was exceedingly costly. (In 2019, SoftBank sold a $4 billion stake in the company for $3.6 billion, shares that would now be worth more than $150 billion.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move also rattled the market. As of this writing, Nvidia shares are down nearly 3% following the disclosure, even as analysts emphasize that the sale “should not be seen as a cautious or negative stance on Nvidia,” but rather reflects SoftBank needing capital for its AI ambitions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wall Street can’t help but wonder: does Son see something right now that others do not? Judging by his track record, maybe — and that ambiguity is all investors have to go on.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/11/softbanks-nvidia-sale-rattles-market-raises-questions/</guid><pubDate>Tue, 11 Nov 2025 19:52:31 +0000</pubDate></item><item><title>[NEW] Google says new cloud-based “Private AI Compute” is just as secure as local processing (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/11/google-says-new-cloud-based-private-ai-compute-is-just-as-secure-as-local-processing/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New system allows devices to connect directly to secure space in Google’s AI servers.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-640x361.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google’s current mission is to weave generative AI into as many products as it can, getting everyone accustomed to, and maybe even dependent on, working with confabulatory robots. That means it needs to feed the bots a lot of your data, and that’s getting easier with the company’s new Private AI Compute. Google claims its new secure cloud environment will power better AI experiences without sacrificing your privacy.&lt;/p&gt;
&lt;p&gt;The pitch sounds a lot like Apple’s Private Cloud Compute. Google’s Private AI Compute runs on “one seamless Google stack” powered by the company’s custom Tensor Processing Units (TPUs). These chips have integrated secure elements, and the new system allows devices to connect directly to the protected space via an encrypted link.&lt;/p&gt;
&lt;p&gt;Google’s TPUs rely on an AMD-based Trusted Execution Environment (TEE) that encrypts and isolates memory from the host. Theoretically, that means no one else—not even Google itself—can access your data. Google says independent analysis by NCC Group shows that Private AI Compute meets its strict privacy guidelines.&lt;/p&gt;
&lt;p&gt;According to Google, the Private AI Compute service is just as secure as using local processing on your device. However, Google’s cloud has a lot more processing power than your laptop or phone, enabling the use of Google’s largest and most capable Gemini models.&lt;/p&gt;
&lt;h2&gt;Edge vs. Cloud&lt;/h2&gt;
&lt;p&gt;As Google has added more AI features to devices like Pixel phones, it has talked up the power of its on-device neural processing units (NPUs). Pixels and a few other phones run Gemini Nano models, allowing the phone to process AI workloads securely on “the edge” without sending any of your data to the Internet. With the release of the Pixel 10, Google upgraded Gemini Nano to handle even more data with the help of researchers from DeepMind.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;NPUs can’t do it all, though. While Gemini Nano is getting more capable, it can’t compete with models that run on massive, high-wattage servers. That might be why some AI features, like the temporarily unavailable Daily Brief, don’t do much on the Pixels. Magic Cue, which surfaces personal data based on screen context, is probably in a similar place. Google now says that Magic Cue will get “even more helpful” thanks to the Private AI Compute system.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2114347 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Pixel 10 flat" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Google-Pixel-10-5.jpg" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Magic Cue debuted on the Pixel 10, but it doesn’t do much yet.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google has also released a Pixel feature drop today, but there aren’t many new features of note (unless you’ve been hankering for Wicked themes). As part of the update, Magic Cue will begin using the Private AI Compute system to generate suggestions. The more powerful model &lt;em&gt;might&lt;/em&gt; be able to tease out more actionable details from your data. Google also notes the Recorder app will be able to summarize in more languages thanks to the secure cloud.&lt;/p&gt;
&lt;p&gt;So what Google is saying here is that more of your data is being offloaded to the cloud so that Magic Cue can generate useful suggestions, which would be a change. Since launch, we’ve only seen Magic Cue appear a handful of times, and it’s not offering anything interesting when it does.&lt;/p&gt;
&lt;p&gt;There are still reasons to use local AI, even if the cloud system has “the same security and privacy assurances,” as Google claims. An NPU offers superior latency because your data doesn’t have to go anywhere, and it’s more reliable, as AI features will still work without an Internet connection. Google believes this hybrid approach is the way forward for generative AI, which requires significant processing even for seemingly simple tasks. We can expect to see more AI features reaching out to Google’s secure cloud soon.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New system allows devices to connect directly to secure space in Google’s AI servers.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-640x361.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google’s current mission is to weave generative AI into as many products as it can, getting everyone accustomed to, and maybe even dependent on, working with confabulatory robots. That means it needs to feed the bots a lot of your data, and that’s getting easier with the company’s new Private AI Compute. Google claims its new secure cloud environment will power better AI experiences without sacrificing your privacy.&lt;/p&gt;
&lt;p&gt;The pitch sounds a lot like Apple’s Private Cloud Compute. Google’s Private AI Compute runs on “one seamless Google stack” powered by the company’s custom Tensor Processing Units (TPUs). These chips have integrated secure elements, and the new system allows devices to connect directly to the protected space via an encrypted link.&lt;/p&gt;
&lt;p&gt;Google’s TPUs rely on an AMD-based Trusted Execution Environment (TEE) that encrypts and isolates memory from the host. Theoretically, that means no one else—not even Google itself—can access your data. Google says independent analysis by NCC Group shows that Private AI Compute meets its strict privacy guidelines.&lt;/p&gt;
&lt;p&gt;According to Google, the Private AI Compute service is just as secure as using local processing on your device. However, Google’s cloud has a lot more processing power than your laptop or phone, enabling the use of Google’s largest and most capable Gemini models.&lt;/p&gt;
&lt;h2&gt;Edge vs. Cloud&lt;/h2&gt;
&lt;p&gt;As Google has added more AI features to devices like Pixel phones, it has talked up the power of its on-device neural processing units (NPUs). Pixels and a few other phones run Gemini Nano models, allowing the phone to process AI workloads securely on “the edge” without sending any of your data to the Internet. With the release of the Pixel 10, Google upgraded Gemini Nano to handle even more data with the help of researchers from DeepMind.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;NPUs can’t do it all, though. While Gemini Nano is getting more capable, it can’t compete with models that run on massive, high-wattage servers. That might be why some AI features, like the temporarily unavailable Daily Brief, don’t do much on the Pixels. Magic Cue, which surfaces personal data based on screen context, is probably in a similar place. Google now says that Magic Cue will get “even more helpful” thanks to the Private AI Compute system.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2114347 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Pixel 10 flat" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/Google-Pixel-10-5.jpg" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Magic Cue debuted on the Pixel 10, but it doesn’t do much yet.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google has also released a Pixel feature drop today, but there aren’t many new features of note (unless you’ve been hankering for Wicked themes). As part of the update, Magic Cue will begin using the Private AI Compute system to generate suggestions. The more powerful model &lt;em&gt;might&lt;/em&gt; be able to tease out more actionable details from your data. Google also notes the Recorder app will be able to summarize in more languages thanks to the secure cloud.&lt;/p&gt;
&lt;p&gt;So what Google is saying here is that more of your data is being offloaded to the cloud so that Magic Cue can generate useful suggestions, which would be a change. Since launch, we’ve only seen Magic Cue appear a handful of times, and it’s not offering anything interesting when it does.&lt;/p&gt;
&lt;p&gt;There are still reasons to use local AI, even if the cloud system has “the same security and privacy assurances,” as Google claims. An NPU offers superior latency because your data doesn’t have to go anywhere, and it’s more reliable, as AI features will still work without an Internet connection. Google believes this hybrid approach is the way forward for generative AI, which requires significant processing even for seemingly simple tasks. We can expect to see more AI features reaching out to Google’s secure cloud soon.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/11/google-says-new-cloud-based-private-ai-compute-is-just-as-secure-as-local-processing/</guid><pubDate>Tue, 11 Nov 2025 21:34:10 +0000</pubDate></item><item><title>[NEW] Meta’s SPICE framework lets AI systems teach themselves to reason (AI | VentureBeat)</title><link>https://venturebeat.com/ai/metas-spice-framework-lets-ai-systems-teach-themselves-to-reason</link><description>[unable to retrieve full-text content]&lt;p&gt;Researchers at &lt;a href="https://ai.meta.com/research/"&gt;Meta FAIR&lt;/a&gt; and the &lt;a href="https://nus.edu.sg/"&gt;National University of Singapore&lt;/a&gt; have developed a new reinforcement learning framework for self-improving AI systems. &lt;/p&gt;&lt;p&gt;Called &lt;a href="https://arxiv.org/abs/2510.24684"&gt;Self-Play In Corpus Environments (SPICE)&lt;/a&gt;, the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.&lt;/p&gt;&lt;p&gt;While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.&lt;/p&gt;&lt;h2&gt;The challenge of self-improving AI&lt;/h2&gt;&lt;p&gt;The goal of self-improving AI is to create systems that can &lt;a href="https://venturebeat.com/ai/the-era-of-experience-will-unleash-self-learning-ai-agents-across-the-web-heres-how-to-prepare"&gt;enhance their capabilities by interacting with their environment&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.&lt;/p&gt;&lt;p&gt;Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;F&lt;!-- --&gt;actual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”&lt;/p&gt;&lt;h2&gt;How SPICE works&lt;/h2&gt;&lt;p&gt;SPICE is a self-play framework where a single model acts in two distinct roles. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A &amp;quot;Challenger&amp;quot; constructs a curriculum of challenging problems from a large corpus of documents. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &amp;quot;Reasoner&amp;quot; then attempts to solve these problems without access to the source documents. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.&lt;/p&gt;&lt;p&gt;Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.&lt;/p&gt;&lt;p&gt;The adversarial dynamic between the two roles creates an automatic curriculum. &lt;/p&gt;&lt;p&gt;The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&amp;#x27;s capability (not too easy and also not impossible). &lt;/p&gt;&lt;p&gt;The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. &lt;/p&gt;&lt;p&gt;Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. &lt;/p&gt;&lt;p&gt;This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.&lt;/p&gt;&lt;h2&gt;SPICE in action&lt;/h2&gt;&lt;p&gt;The researchers evaluated SPICE on several base models, including &lt;a href="https://venturebeat.com/ai/alibaba-launches-open-source-qwen3-model-that-surpasses-openai-o1-and-deepseek-r1"&gt;Qwen3-4B-Base&lt;/a&gt; and &lt;a href="https://github.com/GAIR-NLP/OctoThinker"&gt;OctoThinker-3B-Hybrid-Base&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed &amp;quot;Strong Challenger&amp;quot; (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.&lt;/p&gt;&lt;p&gt;Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. &lt;/p&gt;&lt;p&gt;The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.&lt;/p&gt;&lt;p&gt;A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. &lt;/p&gt;&lt;p&gt;In one experiment, the Reasoner&amp;#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. &lt;/p&gt;&lt;p&gt;Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.&lt;/p&gt;&lt;p&gt;The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”&lt;/p&gt;&lt;p&gt;Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Researchers at &lt;a href="https://ai.meta.com/research/"&gt;Meta FAIR&lt;/a&gt; and the &lt;a href="https://nus.edu.sg/"&gt;National University of Singapore&lt;/a&gt; have developed a new reinforcement learning framework for self-improving AI systems. &lt;/p&gt;&lt;p&gt;Called &lt;a href="https://arxiv.org/abs/2510.24684"&gt;Self-Play In Corpus Environments (SPICE)&lt;/a&gt;, the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.&lt;/p&gt;&lt;p&gt;While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.&lt;/p&gt;&lt;h2&gt;The challenge of self-improving AI&lt;/h2&gt;&lt;p&gt;The goal of self-improving AI is to create systems that can &lt;a href="https://venturebeat.com/ai/the-era-of-experience-will-unleash-self-learning-ai-agents-across-the-web-heres-how-to-prepare"&gt;enhance their capabilities by interacting with their environment&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.&lt;/p&gt;&lt;p&gt;Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;F&lt;!-- --&gt;actual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”&lt;/p&gt;&lt;h2&gt;How SPICE works&lt;/h2&gt;&lt;p&gt;SPICE is a self-play framework where a single model acts in two distinct roles. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A &amp;quot;Challenger&amp;quot; constructs a curriculum of challenging problems from a large corpus of documents. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &amp;quot;Reasoner&amp;quot; then attempts to solve these problems without access to the source documents. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.&lt;/p&gt;&lt;p&gt;Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.&lt;/p&gt;&lt;p&gt;The adversarial dynamic between the two roles creates an automatic curriculum. &lt;/p&gt;&lt;p&gt;The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&amp;#x27;s capability (not too easy and also not impossible). &lt;/p&gt;&lt;p&gt;The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. &lt;/p&gt;&lt;p&gt;Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. &lt;/p&gt;&lt;p&gt;This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.&lt;/p&gt;&lt;h2&gt;SPICE in action&lt;/h2&gt;&lt;p&gt;The researchers evaluated SPICE on several base models, including &lt;a href="https://venturebeat.com/ai/alibaba-launches-open-source-qwen3-model-that-surpasses-openai-o1-and-deepseek-r1"&gt;Qwen3-4B-Base&lt;/a&gt; and &lt;a href="https://github.com/GAIR-NLP/OctoThinker"&gt;OctoThinker-3B-Hybrid-Base&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed &amp;quot;Strong Challenger&amp;quot; (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.&lt;/p&gt;&lt;p&gt;Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. &lt;/p&gt;&lt;p&gt;The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.&lt;/p&gt;&lt;p&gt;A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. &lt;/p&gt;&lt;p&gt;In one experiment, the Reasoner&amp;#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. &lt;/p&gt;&lt;p&gt;Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.&lt;/p&gt;&lt;p&gt;The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”&lt;/p&gt;&lt;p&gt;Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/metas-spice-framework-lets-ai-systems-teach-themselves-to-reason</guid><pubDate>Tue, 11 Nov 2025 22:21:00 +0000</pubDate></item><item><title>[NEW] Baidu just dropped an open-source multimodal AI that it claims beats GPT-5 and Gemini (AI | VentureBeat)</title><link>https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.baidu.com/"&gt;&lt;u&gt;Baidu Inc.&lt;/u&gt;&lt;/a&gt;, China&amp;#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.&lt;/p&gt;&lt;p&gt;The model, dubbed &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt;, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.&lt;/p&gt;&lt;p&gt;What sets Baidu&amp;#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.&lt;/p&gt;&lt;p&gt;&amp;quot;Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,&amp;quot; Baidu wrote in the model&amp;#x27;s &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;technical documentation&lt;/u&gt;&lt;/a&gt; on Hugging Face, the AI model repository where the system was released.&lt;/p&gt;&lt;p&gt;The company said the model underwent &amp;quot;an extensive mid-training phase&amp;quot; that incorporated &amp;quot;a vast and highly diverse corpus of premium visual-language reasoning data,&amp;quot; dramatically boosting its ability to align visual and textual information semantically.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;How the model mimics human visual problem-solving through dynamic image analysis&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the model&amp;#x27;s most distinctive feature is what Baidu calls &amp;quot;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;Thinking with Images&lt;/u&gt;&lt;/a&gt;&amp;quot; — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.&lt;/p&gt;&lt;p&gt;&amp;quot;The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,&amp;quot; according to the model card. When paired with tools like image search, Baidu claims this feature &amp;quot;dramatically elevates the model&amp;#x27;s ability to process fine-grained details and handle long-tail visual knowledge.&amp;quot;&lt;/p&gt;&lt;p&gt;This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.&lt;/p&gt;&lt;p&gt;The model also supports what Baidu describes as enhanced &amp;quot;visual grounding&amp;quot; capabilities with &amp;quot;more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,&amp;quot; suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Baidu&amp;#x27;s performance claims draw scrutiny as independent testing remains pending&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu&amp;#x27;s assertion that the model outperforms Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; and OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5-High&lt;/u&gt;&lt;/a&gt; on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.&lt;/p&gt;&lt;p&gt;The company released the model under the permissive &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking#license"&gt;&lt;u&gt;Apache 2.0 license&lt;/u&gt;&lt;/a&gt;, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.&lt;/p&gt;&lt;p&gt;&amp;quot;&lt;a href="https://x.com/orionintx/status/1988214142030037115"&gt;&lt;u&gt;Apache 2.0 is smart&lt;/u&gt;&lt;/a&gt;,&amp;quot; wrote one X user responding to Baidu&amp;#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.&lt;/p&gt;&lt;p&gt;According to Baidu&amp;#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as &amp;quot;multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,&amp;quot; aided by what the company characterizes as &amp;quot;large-scale reinforcement learning.&amp;quot; &lt;/p&gt;&lt;p&gt;For STEM problem solving, Baidu claims that &amp;quot;leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.&amp;quot; The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.&lt;/p&gt;&lt;p&gt;For video understanding, Baidu claims the model possesses &amp;quot;outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.&amp;quot; Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Inside the mixture-of-experts architecture that powers efficient multimodal processing&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Under the hood, &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; employs a &lt;a href="https://huggingface.co/blog/moe"&gt;&lt;u&gt;Mixture-of-Experts (MoE) architecture&lt;/u&gt;&lt;/a&gt; — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.&lt;/p&gt;&lt;p&gt;This approach offers substantial practical advantages for enterprise deployments. According to Baidu&amp;#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.&lt;/p&gt;&lt;p&gt;The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&amp;#x27;s capabilities. The company used &amp;quot;cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.&amp;quot;&lt;/p&gt;&lt;p&gt;Baidu also notes that in response to &amp;quot;strong community demand,&amp;quot; the company &amp;quot;significantly strengthened the model&amp;#x27;s grounding performance with improved instruction-following capabilities.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The new model fits into Baidu&amp;#x27;s ambitious multimodal AI ecosystem&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The new release is one component of Baidu&amp;#x27;s broader &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;ERNIE 4.5 model family&lt;/u&gt;&lt;/a&gt;, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-424B-A47B-Base-PT"&gt;&lt;u&gt;ERNIE-4.5-VL-424B-A47B&lt;/u&gt;&lt;/a&gt; with 424 billion total parameters down to a compact 0.3 billion parameter dense model.&lt;/p&gt;&lt;p&gt;According to Baidu&amp;#x27;s &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; on the ERNIE 4.5 family, the models incorporate &amp;quot;a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.&amp;quot;&lt;/p&gt;&lt;p&gt;This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design &amp;quot;has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.&amp;quot;&lt;/p&gt;&lt;p&gt;The company reported achieving &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;47% Model FLOPs Utilization (MFU)&lt;/u&gt;&lt;/a&gt; — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the &lt;a href="https://github.com/PaddlePaddle/Paddle"&gt;&lt;u&gt;PaddlePaddle&lt;/u&gt;&lt;/a&gt; deep learning framework developed in-house.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Comprehensive developer tools aim to simplify enterprise deployment and integration&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through &lt;a href="https://github.com/PaddlePaddle/ERNIE"&gt;&lt;u&gt;ERNIEKit&lt;/u&gt;&lt;/a&gt;, what the company describes as an &amp;quot;industrial-grade training and compression development toolkit.&amp;quot;&lt;/p&gt;&lt;p&gt;The model offers full compatibility with popular open-source frameworks including &lt;a href="https://huggingface.co/docs/transformers/en/index"&gt;&lt;u&gt;Hugging Face Transformers&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm"&gt;&lt;u&gt;vLLM&lt;/u&gt;&lt;/a&gt; (a high-performance inference engine), and Baidu&amp;#x27;s own &lt;a href="https://github.com/PaddlePaddle/FastDeploy"&gt;&lt;u&gt;FastDeploy toolkit&lt;/u&gt;&lt;/a&gt;. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.&lt;/p&gt;&lt;p&gt;Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.&lt;/p&gt;&lt;p&gt;For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&amp;#x27;s &amp;quot;reasoning-parser&amp;quot; and &amp;quot;tool-call-parser&amp;quot; capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.&lt;/p&gt;&lt;p&gt;The company also offers &lt;a href="https://yiyan.baidu.com/blog/posts/fastdeploy2.0/"&gt;&lt;u&gt;FastDeploy&lt;/u&gt;&lt;/a&gt;, a proprietary inference toolkit that Baidu claims delivers &amp;quot;production-ready, easy-to-use multi-hardware deployment solutions&amp;quot; with support for various quantization schemes that can reduce memory requirements and increase inference speed.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why this release matters for the enterprise AI market at a critical inflection point&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release comes at a pivotal moment in the enterprise AI market. As organizations move &lt;a href="https://www.glean.com/perspectives/enterprise-insights-from-ai"&gt;&lt;u&gt;beyond experimental chatbot deployments&lt;/u&gt;&lt;/a&gt; toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.&lt;/p&gt;&lt;p&gt;Several enterprise use cases appear particularly well-suited to the model&amp;#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&amp;#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.&lt;/p&gt;&lt;p&gt;&amp;quot;With all these new models, where&amp;#x27;s the best place to actually build and scale? Access to compute is everything,&amp;quot; &lt;a href="https://x.com/orionintx/status/1988214142030037115"&gt;&lt;u&gt;wrote one X user&lt;/u&gt;&lt;/a&gt; in response to Baidu&amp;#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.&lt;/p&gt;&lt;p&gt;The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; in production applications without ongoing licensing fees or usage restrictions.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Competition intensifies as Chinese tech giant takes aim at Google and OpenAI&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu&amp;#x27;s release intensifies competition in the vision-language model space, where &lt;a href="https://www.google.com/?zx=1762903123628&amp;amp;no_sw_cr=1"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt;, and Chinese companies including &lt;a href="https://www.alibaba.com/"&gt;&lt;u&gt;Alibaba&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.bytedance.com/en/"&gt;&lt;u&gt;ByteDance&lt;/u&gt;&lt;/a&gt; have all released capable systems in recent months.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; and OpenAI&amp;#x27;s &lt;a href="https://artificialanalysis.ai/models/gpt-5"&gt;&lt;u&gt;GPT-5-High&lt;/u&gt;&lt;/a&gt; are substantially larger models backed by the deep resources of two of the world&amp;#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.&lt;/p&gt;&lt;p&gt;&amp;quot;Impressive that ERNIE is outperforming Gemini 2.5 Pro,&amp;quot; wrote one social media commenter, expressing surprise at the claimed results.&lt;/p&gt;&lt;p&gt;However, some observers counseled caution about benchmark comparisons. &amp;quot;It&amp;#x27;s fascinating to see how multimodal models are evolving, especially with features like &amp;#x27;Thinking with Images,&amp;#x27;&amp;quot; &lt;a href="https://x.com/_junaidkhalid1/status/1988259730871963652"&gt;&lt;u&gt;wrote one X user&lt;/u&gt;&lt;/a&gt;. &amp;quot;That said, I&amp;#x27;m curious if ERNIE-4.5&amp;#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart&amp;quot; understanding rather than general-purpose vision tasks.&lt;/p&gt;&lt;p&gt;Industry analysts note that &lt;a href="https://www.nytimes.com/2024/04/15/technology/ai-models-measurement.html"&gt;&lt;u&gt;benchmark performance often fails to capture real-world behavior&lt;/u&gt;&lt;/a&gt; across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical limitations and infrastructure requirements that enterprises must consider&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&amp;#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.&lt;/p&gt;&lt;p&gt;Questions also remain about the model&amp;#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&amp;#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;What technical decision-makers need to evaluate beyond the benchmark numbers&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s &lt;a href="https://huggingface.co/blog/moe"&gt;&lt;u&gt;MoE architecture&lt;/u&gt;&lt;/a&gt;, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.&lt;/p&gt;&lt;p&gt;The &amp;quot;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;Thinking with Images&lt;/u&gt;&lt;/a&gt;&amp;quot; feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&amp;#x27;s documentation suggests this capability works best &amp;quot;when paired with tools like image zooming and image search,&amp;quot; implying that organizations may need to build additional infrastructure to fully leverage this functionality.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.&lt;/p&gt;&lt;p&gt;Organizations considering deployment should also evaluate Baidu&amp;#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking#license"&gt;&lt;u&gt;Apache 2.0 license&lt;/u&gt;&lt;/a&gt; ensures the model remains available, future improvements and support depend on Baidu&amp;#x27;s strategic priorities.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Developer community responds with enthusiasm tempered by practical requests&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Early response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.&lt;/p&gt;&lt;p&gt;&amp;quot;Release MNN and GGUF so I can run it on my phone,&amp;quot; &lt;a href="https://x.com/Elaina43114880/status/1988327740496638345"&gt;&lt;u&gt;wrote one developer&lt;/u&gt;&lt;/a&gt;, highlighting demand for mobile deployment options.&lt;/p&gt;&lt;p&gt;Other developers praised Baidu&amp;#x27;s technical choices while requesting additional resources. &amp;quot;Fantastic model! Did you use discoveries from PaddleOCR?&amp;quot; &lt;a href="https://x.com/J3rryH0well/status/1988281431421055439"&gt;&lt;u&gt;asked one user&lt;/u&gt;&lt;/a&gt;, referencing Baidu&amp;#x27;s open-source optical character recognition toolkit.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. &amp;quot;ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,&amp;quot; &lt;a href="https://x.com/Mr_Pratap_Singh/status/1988304605877596177"&gt;&lt;u&gt;joked one observer&lt;/u&gt;&lt;/a&gt;. &amp;quot;But hey, if you&amp;#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&amp;#x27;ve earned the right to a dramatic name!&amp;quot;&lt;/p&gt;&lt;p&gt;Baidu plans to showcase the ERNIE lineup during its &lt;a href="https://ir.baidu.com/news-releases/news-release-details/baidu-host-baidu-world-annual-flagship-technology-conference-nov"&gt;&lt;u&gt;Baidu World 2025 conference&lt;/u&gt;&lt;/a&gt; on November 13, where the company is expected to provide additional details about the model&amp;#x27;s development, performance validation, and future roadmap.&lt;/p&gt;&lt;p&gt;The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.&lt;/p&gt;&lt;p&gt;For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; is reshaping the economics of AI deployment and accelerating adoption across industries.&lt;/p&gt;&lt;p&gt;Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: &amp;quot;Open source plus commercial use equals chef&amp;#x27;s kiss. Baidu not playing around.&amp;quot;&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.baidu.com/"&gt;&lt;u&gt;Baidu Inc.&lt;/u&gt;&lt;/a&gt;, China&amp;#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.&lt;/p&gt;&lt;p&gt;The model, dubbed &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt;, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.&lt;/p&gt;&lt;p&gt;What sets Baidu&amp;#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.&lt;/p&gt;&lt;p&gt;&amp;quot;Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,&amp;quot; Baidu wrote in the model&amp;#x27;s &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;technical documentation&lt;/u&gt;&lt;/a&gt; on Hugging Face, the AI model repository where the system was released.&lt;/p&gt;&lt;p&gt;The company said the model underwent &amp;quot;an extensive mid-training phase&amp;quot; that incorporated &amp;quot;a vast and highly diverse corpus of premium visual-language reasoning data,&amp;quot; dramatically boosting its ability to align visual and textual information semantically.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;How the model mimics human visual problem-solving through dynamic image analysis&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Perhaps the model&amp;#x27;s most distinctive feature is what Baidu calls &amp;quot;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;Thinking with Images&lt;/u&gt;&lt;/a&gt;&amp;quot; — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.&lt;/p&gt;&lt;p&gt;&amp;quot;The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,&amp;quot; according to the model card. When paired with tools like image search, Baidu claims this feature &amp;quot;dramatically elevates the model&amp;#x27;s ability to process fine-grained details and handle long-tail visual knowledge.&amp;quot;&lt;/p&gt;&lt;p&gt;This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.&lt;/p&gt;&lt;p&gt;The model also supports what Baidu describes as enhanced &amp;quot;visual grounding&amp;quot; capabilities with &amp;quot;more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,&amp;quot; suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Baidu&amp;#x27;s performance claims draw scrutiny as independent testing remains pending&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu&amp;#x27;s assertion that the model outperforms Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; and OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5-High&lt;/u&gt;&lt;/a&gt; on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.&lt;/p&gt;&lt;p&gt;The company released the model under the permissive &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking#license"&gt;&lt;u&gt;Apache 2.0 license&lt;/u&gt;&lt;/a&gt;, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.&lt;/p&gt;&lt;p&gt;&amp;quot;&lt;a href="https://x.com/orionintx/status/1988214142030037115"&gt;&lt;u&gt;Apache 2.0 is smart&lt;/u&gt;&lt;/a&gt;,&amp;quot; wrote one X user responding to Baidu&amp;#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.&lt;/p&gt;&lt;p&gt;According to Baidu&amp;#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as &amp;quot;multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,&amp;quot; aided by what the company characterizes as &amp;quot;large-scale reinforcement learning.&amp;quot; &lt;/p&gt;&lt;p&gt;For STEM problem solving, Baidu claims that &amp;quot;leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.&amp;quot; The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.&lt;/p&gt;&lt;p&gt;For video understanding, Baidu claims the model possesses &amp;quot;outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.&amp;quot; Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Inside the mixture-of-experts architecture that powers efficient multimodal processing&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Under the hood, &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; employs a &lt;a href="https://huggingface.co/blog/moe"&gt;&lt;u&gt;Mixture-of-Experts (MoE) architecture&lt;/u&gt;&lt;/a&gt; — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.&lt;/p&gt;&lt;p&gt;This approach offers substantial practical advantages for enterprise deployments. According to Baidu&amp;#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.&lt;/p&gt;&lt;p&gt;The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&amp;#x27;s capabilities. The company used &amp;quot;cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.&amp;quot;&lt;/p&gt;&lt;p&gt;Baidu also notes that in response to &amp;quot;strong community demand,&amp;quot; the company &amp;quot;significantly strengthened the model&amp;#x27;s grounding performance with improved instruction-following capabilities.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The new model fits into Baidu&amp;#x27;s ambitious multimodal AI ecosystem&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The new release is one component of Baidu&amp;#x27;s broader &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;ERNIE 4.5 model family&lt;/u&gt;&lt;/a&gt;, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-424B-A47B-Base-PT"&gt;&lt;u&gt;ERNIE-4.5-VL-424B-A47B&lt;/u&gt;&lt;/a&gt; with 424 billion total parameters down to a compact 0.3 billion parameter dense model.&lt;/p&gt;&lt;p&gt;According to Baidu&amp;#x27;s &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; on the ERNIE 4.5 family, the models incorporate &amp;quot;a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.&amp;quot;&lt;/p&gt;&lt;p&gt;This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design &amp;quot;has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.&amp;quot;&lt;/p&gt;&lt;p&gt;The company reported achieving &lt;a href="https://ernie.baidu.com/blog/posts/ernie4.5/"&gt;&lt;u&gt;47% Model FLOPs Utilization (MFU)&lt;/u&gt;&lt;/a&gt; — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the &lt;a href="https://github.com/PaddlePaddle/Paddle"&gt;&lt;u&gt;PaddlePaddle&lt;/u&gt;&lt;/a&gt; deep learning framework developed in-house.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Comprehensive developer tools aim to simplify enterprise deployment and integration&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through &lt;a href="https://github.com/PaddlePaddle/ERNIE"&gt;&lt;u&gt;ERNIEKit&lt;/u&gt;&lt;/a&gt;, what the company describes as an &amp;quot;industrial-grade training and compression development toolkit.&amp;quot;&lt;/p&gt;&lt;p&gt;The model offers full compatibility with popular open-source frameworks including &lt;a href="https://huggingface.co/docs/transformers/en/index"&gt;&lt;u&gt;Hugging Face Transformers&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm"&gt;&lt;u&gt;vLLM&lt;/u&gt;&lt;/a&gt; (a high-performance inference engine), and Baidu&amp;#x27;s own &lt;a href="https://github.com/PaddlePaddle/FastDeploy"&gt;&lt;u&gt;FastDeploy toolkit&lt;/u&gt;&lt;/a&gt;. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.&lt;/p&gt;&lt;p&gt;Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.&lt;/p&gt;&lt;p&gt;For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&amp;#x27;s &amp;quot;reasoning-parser&amp;quot; and &amp;quot;tool-call-parser&amp;quot; capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.&lt;/p&gt;&lt;p&gt;The company also offers &lt;a href="https://yiyan.baidu.com/blog/posts/fastdeploy2.0/"&gt;&lt;u&gt;FastDeploy&lt;/u&gt;&lt;/a&gt;, a proprietary inference toolkit that Baidu claims delivers &amp;quot;production-ready, easy-to-use multi-hardware deployment solutions&amp;quot; with support for various quantization schemes that can reduce memory requirements and increase inference speed.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why this release matters for the enterprise AI market at a critical inflection point&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release comes at a pivotal moment in the enterprise AI market. As organizations move &lt;a href="https://www.glean.com/perspectives/enterprise-insights-from-ai"&gt;&lt;u&gt;beyond experimental chatbot deployments&lt;/u&gt;&lt;/a&gt; toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.&lt;/p&gt;&lt;p&gt;Several enterprise use cases appear particularly well-suited to the model&amp;#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&amp;#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.&lt;/p&gt;&lt;p&gt;&amp;quot;With all these new models, where&amp;#x27;s the best place to actually build and scale? Access to compute is everything,&amp;quot; &lt;a href="https://x.com/orionintx/status/1988214142030037115"&gt;&lt;u&gt;wrote one X user&lt;/u&gt;&lt;/a&gt; in response to Baidu&amp;#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.&lt;/p&gt;&lt;p&gt;The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; in production applications without ongoing licensing fees or usage restrictions.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Competition intensifies as Chinese tech giant takes aim at Google and OpenAI&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu&amp;#x27;s release intensifies competition in the vision-language model space, where &lt;a href="https://www.google.com/?zx=1762903123628&amp;amp;no_sw_cr=1"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt;, and Chinese companies including &lt;a href="https://www.alibaba.com/"&gt;&lt;u&gt;Alibaba&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.bytedance.com/en/"&gt;&lt;u&gt;ByteDance&lt;/u&gt;&lt;/a&gt; have all released capable systems in recent months.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; and OpenAI&amp;#x27;s &lt;a href="https://artificialanalysis.ai/models/gpt-5"&gt;&lt;u&gt;GPT-5-High&lt;/u&gt;&lt;/a&gt; are substantially larger models backed by the deep resources of two of the world&amp;#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.&lt;/p&gt;&lt;p&gt;&amp;quot;Impressive that ERNIE is outperforming Gemini 2.5 Pro,&amp;quot; wrote one social media commenter, expressing surprise at the claimed results.&lt;/p&gt;&lt;p&gt;However, some observers counseled caution about benchmark comparisons. &amp;quot;It&amp;#x27;s fascinating to see how multimodal models are evolving, especially with features like &amp;#x27;Thinking with Images,&amp;#x27;&amp;quot; &lt;a href="https://x.com/_junaidkhalid1/status/1988259730871963652"&gt;&lt;u&gt;wrote one X user&lt;/u&gt;&lt;/a&gt;. &amp;quot;That said, I&amp;#x27;m curious if ERNIE-4.5&amp;#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart&amp;quot; understanding rather than general-purpose vision tasks.&lt;/p&gt;&lt;p&gt;Industry analysts note that &lt;a href="https://www.nytimes.com/2024/04/15/technology/ai-models-measurement.html"&gt;&lt;u&gt;benchmark performance often fails to capture real-world behavior&lt;/u&gt;&lt;/a&gt; across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical limitations and infrastructure requirements that enterprises must consider&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&amp;#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.&lt;/p&gt;&lt;p&gt;Questions also remain about the model&amp;#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&amp;#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;What technical decision-makers need to evaluate beyond the benchmark numbers&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s &lt;a href="https://huggingface.co/blog/moe"&gt;&lt;u&gt;MoE architecture&lt;/u&gt;&lt;/a&gt;, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.&lt;/p&gt;&lt;p&gt;The &amp;quot;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;Thinking with Images&lt;/u&gt;&lt;/a&gt;&amp;quot; feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&amp;#x27;s documentation suggests this capability works best &amp;quot;when paired with tools like image zooming and image search,&amp;quot; implying that organizations may need to build additional infrastructure to fully leverage this functionality.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.&lt;/p&gt;&lt;p&gt;Organizations considering deployment should also evaluate Baidu&amp;#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking#license"&gt;&lt;u&gt;Apache 2.0 license&lt;/u&gt;&lt;/a&gt; ensures the model remains available, future improvements and support depend on Baidu&amp;#x27;s strategic priorities.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Developer community responds with enthusiasm tempered by practical requests&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Early response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.&lt;/p&gt;&lt;p&gt;&amp;quot;Release MNN and GGUF so I can run it on my phone,&amp;quot; &lt;a href="https://x.com/Elaina43114880/status/1988327740496638345"&gt;&lt;u&gt;wrote one developer&lt;/u&gt;&lt;/a&gt;, highlighting demand for mobile deployment options.&lt;/p&gt;&lt;p&gt;Other developers praised Baidu&amp;#x27;s technical choices while requesting additional resources. &amp;quot;Fantastic model! Did you use discoveries from PaddleOCR?&amp;quot; &lt;a href="https://x.com/J3rryH0well/status/1988281431421055439"&gt;&lt;u&gt;asked one user&lt;/u&gt;&lt;/a&gt;, referencing Baidu&amp;#x27;s open-source optical character recognition toolkit.&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. &amp;quot;ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,&amp;quot; &lt;a href="https://x.com/Mr_Pratap_Singh/status/1988304605877596177"&gt;&lt;u&gt;joked one observer&lt;/u&gt;&lt;/a&gt;. &amp;quot;But hey, if you&amp;#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&amp;#x27;ve earned the right to a dramatic name!&amp;quot;&lt;/p&gt;&lt;p&gt;Baidu plans to showcase the ERNIE lineup during its &lt;a href="https://ir.baidu.com/news-releases/news-release-details/baidu-host-baidu-world-annual-flagship-technology-conference-nov"&gt;&lt;u&gt;Baidu World 2025 conference&lt;/u&gt;&lt;/a&gt; on November 13, where the company is expected to provide additional details about the model&amp;#x27;s development, performance validation, and future roadmap.&lt;/p&gt;&lt;p&gt;The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.&lt;/p&gt;&lt;p&gt;For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like &lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;&lt;u&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/u&gt;&lt;/a&gt; is reshaping the economics of AI deployment and accelerating adoption across industries.&lt;/p&gt;&lt;p&gt;Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: &amp;quot;Open source plus commercial use equals chef&amp;#x27;s kiss. Baidu not playing around.&amp;quot;&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5</guid><pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate></item></channel></rss>