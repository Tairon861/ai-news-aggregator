<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 12 Sep 2025 01:27:54 +0000</lastBuildDate><item><title> ()</title><link>https://www.wired.com/feed/category/artificial-intelligence/rss</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://www.wired.com/feed/category/artificial-intelligence/rss</guid></item><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Yext Scout Guides Brands Through AI Search Challenges (AI News)</title><link>https://www.artificialintelligence-news.com/news/yext-scout-guides-brands-through-ai-search-challenges/</link><description>&lt;p&gt;Customers are discovering brands and learning about products and services in new ways from traditional search to AI search, to AI agents and more, the discovery journey has completely changed, and brands need to adapt to the new paradigm.&lt;/p&gt;&lt;p&gt;Launched earlier this year, Yext Scout is an AI search and competitive intelligence agent that’s designed to boost brand visibility and uncover critical insights related to the new AI-driven search platforms, alongside ‘traditional’ searches as we know them.&lt;/p&gt;&lt;p&gt;Scout is part of Yext, the leading brand visibility platform. Scout provides performance benchmarks laid out against a brand’s local competitors, and creates smart recommendations that you can act on, in real-time.&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-109380" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/Winning-Search-in-EMEA-Banner-ads-728x90-1-1.jpg" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;Yext will be exploring the massive impact AI has had on search and user behaviours, and how Scout can inform marketing professionals, at a webinar, ‘Winning Search in EMEA: How Yext Scout Drives Visibility Across Local and AI Platforms.’ It’s scheduled for Wednesday, 24 September at 1pm BST / 2pm CEST.&lt;/p&gt;&lt;p&gt;With AI now ever-present in digital interactions, platforms like ChatGPT, Gemini, Perplexity, and latterly, Grok, have huge influence over consumers’ discovery and interaction with brands.&lt;/p&gt;&lt;p&gt;Increasingly presented top-of-page, AI discoveries are replacing the traditional search engine results page with conversational answers that remove users’ need to sift through potentially unreliable results. Brands need guidance to understand how to optimise their content so it ranks optimally in these emerging channels.&lt;/p&gt;&lt;p&gt;Measurement of effectiveness is also new territory, putting many companies behind their better-informed, if not better-providing competitors. The Yext webinar on 24 September will help brand and marketing professionals improve visibility, track and tailor sentiment, and measure performance across social content, reviews, and local listings.&lt;/p&gt;&lt;p&gt;Join Yext for the exclusive session, ‘Winning Search in EMEA: How Yext Scout Drives Visibility Across Local and AI Platforms,’ on Wednesday 24 September at 1pm BST / 2pm CEST.&lt;/p&gt;&lt;p&gt;Register your place here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Customers are discovering brands and learning about products and services in new ways from traditional search to AI search, to AI agents and more, the discovery journey has completely changed, and brands need to adapt to the new paradigm.&lt;/p&gt;&lt;p&gt;Launched earlier this year, Yext Scout is an AI search and competitive intelligence agent that’s designed to boost brand visibility and uncover critical insights related to the new AI-driven search platforms, alongside ‘traditional’ searches as we know them.&lt;/p&gt;&lt;p&gt;Scout is part of Yext, the leading brand visibility platform. Scout provides performance benchmarks laid out against a brand’s local competitors, and creates smart recommendations that you can act on, in real-time.&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-109380" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/Winning-Search-in-EMEA-Banner-ads-728x90-1-1.jpg" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;Yext will be exploring the massive impact AI has had on search and user behaviours, and how Scout can inform marketing professionals, at a webinar, ‘Winning Search in EMEA: How Yext Scout Drives Visibility Across Local and AI Platforms.’ It’s scheduled for Wednesday, 24 September at 1pm BST / 2pm CEST.&lt;/p&gt;&lt;p&gt;With AI now ever-present in digital interactions, platforms like ChatGPT, Gemini, Perplexity, and latterly, Grok, have huge influence over consumers’ discovery and interaction with brands.&lt;/p&gt;&lt;p&gt;Increasingly presented top-of-page, AI discoveries are replacing the traditional search engine results page with conversational answers that remove users’ need to sift through potentially unreliable results. Brands need guidance to understand how to optimise their content so it ranks optimally in these emerging channels.&lt;/p&gt;&lt;p&gt;Measurement of effectiveness is also new territory, putting many companies behind their better-informed, if not better-providing competitors. The Yext webinar on 24 September will help brand and marketing professionals improve visibility, track and tailor sentiment, and measure performance across social content, reviews, and local listings.&lt;/p&gt;&lt;p&gt;Join Yext for the exclusive session, ‘Winning Search in EMEA: How Yext Scout Drives Visibility Across Local and AI Platforms,’ on Wednesday 24 September at 1pm BST / 2pm CEST.&lt;/p&gt;&lt;p&gt;Register your place here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/yext-scout-guides-brands-through-ai-search-challenges/</guid><pubDate>Thu, 11 Sep 2025 14:19:15 +0000</pubDate></item><item><title>Box CEO Aaron Levie on AI’s ‘era of context’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/11/box-ceo-aaron-levie-on-ais-era-of-context/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Thursday, Box launched its developer conference Boxworks by announcing a new set of AI features, building agentic AI models into the backbone of the company’s products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s more product announcements than usual for the conference, reflecting the increasingly fast pace of AI development at the company: Box launched its AI studio last year, followed by a new set of data-extraction agents in February, and others for search and deep research in May. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, the company is rolling out a new system called Box Automate that works as a kind of operating system for AI agents, breaking workflows into different segments that can be augmented with AI as necessary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I spoke with CEO Aaron Levie about the company’s approach to AI, and the perilous work of competing with foundation model companies. Unsurprisingly, he was very bullish about the possibilities for AI agents in the modern workplace, but he was also clear-eyed about the limitations of current models and how to manage those limitations with existing technology.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This interview has been edited for length and clarity.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch: You’re announcing a bunch of AI products today, so I want to start by asking about the big-picture vision. Why build AI agents into a cloud content-management service?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aaron Levie: &lt;/strong&gt;So the thing that we think about all day long – and what our focus is at Box – is how much work is changing due to AI. And the vast majority of the impact right now is on workflows involving unstructured data. We’ve already been able to automate anything that deals with structured data that goes into a database. If you think about CRM systems, ERP systems, HR systems, we’ve already had years of automation in that space. But where we’ve never had automation is anything that touches unstructured data.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Think about any kind of legal review process, any kind of marketing asset management process, any kind of M&amp;amp;A deal review — all of those workflows deal with lots of unstructured data. People have to review that data, make updates to it, make decisions and so on. We’ve never been able to bring much automation to those workflows. We’ve been able to sort of describe them in software, but computers just haven’t been good enough at reading a document or looking at a marketing asset. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So for us, AI agents mean that, for the first time ever, we can actually tap into all of this unstructured data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: What about the risks of deploying agents in a business context? Some of your customers must be nervous about deploying something like this on sensitive data.&lt;/strong&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;What we’ve been seeing from customers is they want to know that every single time they run that workflow, the agent is going to execute more or less the same way, at the same point in the workflow, and not have things kind of go off the rails. You don’t want to have an agent make some compounding mistake where, after they do the first couple 100 submissions, they start to kind of run wild. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It becomes really important to have the right demarcation points, where the agent starts and the other parts of the system end. For every workflow, there’s this question of what needs to have deterministic guardrails, and what can be fully agentic and non-deterministic.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What you can do with Box Automate is decide how much work you want each individual agent to do before it hands off to a different agent. So you might have a submission agent that’s separate from the review agent, and so on. It’s allowing you to basically deploy AI agents at scale in any kind of workflow or business process in the organization.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A visualization of the Box Automate workflow" class="wp-image-3045173" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Box-Automate-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A Box Automate workflow, with AI agents deployed for specific tasks. &lt;strong&gt;Image Credits:&lt;/strong&gt; Box&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: What kind of problems do you guard against by splitting up the workflow?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;We’ve already seen some of the limitations even in the most advanced fully agentic systems like Claude Code. At some point in the task, the model runs out of context-window room to continue making good decisions. There’s no free lunch right now in AI. You can’t just have a long-running agent with unlimited context window go after any task in your business. So you have to break up the workflow and use sub-agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think we’re in the era of context within AI. What AI models and agents need is context, and the context that they need to work off is sitting inside your unstructured data. So our whole system is really designed to figure out what context you can give the AI agent to ensure that they perform as effectively as possible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: There is a bigger debate in the industry about the benefits of big, powerful frontier models compared to models that are smaller and more reliable. Does this put you on the side of the smaller models?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;I should probably clarify: Nothing about our system prevents the task from being arbitrarily long or complex. What we’re trying to do is create the right guardrails so that you get to decide how agentic you want that task to be. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;We don’t have a particular philosophy as to where people should be on that continuum. We’re just trying to design a future-proof architecture. We’ve designed this in such a way where, as the models improve and as agentic capabilities improve, you will just get all of those benefits directly in our platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: The other concern is data control. Because models are trained on so much data, there’s a real fear that sensitive data will get regurgitated or misused. How does that factor in?&lt;/strong&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;It’s where a lot of AI deployments go wrong. People think, “Hey, this is easy. I’ll give an AI model access to all of my unstructured data, and it’ll answer questions for people.” And then it starts to give you answers on data that you don’t have access to or you shouldn’t have access to. You need a very powerful layer that handles access controls, data security, permissions, data governance, compliance, everything.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So we’re benefiting from the couple decades that we’ve spent building up a system that basically handles that exact problem: How do you ensure only the right person has access to each piece of data in the enterprise? So when an agent answers a question, you know deterministically that it can’t draw on any data that that person shouldn’t have access to. That is just something fundamentally built into our system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: Earlier this week, Anthropic released a new feature for directly uploading files to Claude.ai. It’s a long way from the sort of file management that Box does, but you must be thinking about possible competition from the foundation model companies. How do you approach that strategically?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;So if you think about what enterprises need when they deploy AI at scale, they need security, permissions and control. They need the user interface, they need powerful APIs, they want their choice of AI models, because one day, one AI model powers some use case for them that is better than another, but then that might change, and they don’t want to be locked into one particular platform. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So what we’ve built is a system that lets you have effectively all of those capabilities. We’re doing the storage, the security, the permissions, the vector embedding, and we connect to every leading AI model that’s out there.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Thursday, Box launched its developer conference Boxworks by announcing a new set of AI features, building agentic AI models into the backbone of the company’s products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s more product announcements than usual for the conference, reflecting the increasingly fast pace of AI development at the company: Box launched its AI studio last year, followed by a new set of data-extraction agents in February, and others for search and deep research in May. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, the company is rolling out a new system called Box Automate that works as a kind of operating system for AI agents, breaking workflows into different segments that can be augmented with AI as necessary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I spoke with CEO Aaron Levie about the company’s approach to AI, and the perilous work of competing with foundation model companies. Unsurprisingly, he was very bullish about the possibilities for AI agents in the modern workplace, but he was also clear-eyed about the limitations of current models and how to manage those limitations with existing technology.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This interview has been edited for length and clarity.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch: You’re announcing a bunch of AI products today, so I want to start by asking about the big-picture vision. Why build AI agents into a cloud content-management service?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Aaron Levie: &lt;/strong&gt;So the thing that we think about all day long – and what our focus is at Box – is how much work is changing due to AI. And the vast majority of the impact right now is on workflows involving unstructured data. We’ve already been able to automate anything that deals with structured data that goes into a database. If you think about CRM systems, ERP systems, HR systems, we’ve already had years of automation in that space. But where we’ve never had automation is anything that touches unstructured data.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Think about any kind of legal review process, any kind of marketing asset management process, any kind of M&amp;amp;A deal review — all of those workflows deal with lots of unstructured data. People have to review that data, make updates to it, make decisions and so on. We’ve never been able to bring much automation to those workflows. We’ve been able to sort of describe them in software, but computers just haven’t been good enough at reading a document or looking at a marketing asset. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So for us, AI agents mean that, for the first time ever, we can actually tap into all of this unstructured data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: What about the risks of deploying agents in a business context? Some of your customers must be nervous about deploying something like this on sensitive data.&lt;/strong&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;What we’ve been seeing from customers is they want to know that every single time they run that workflow, the agent is going to execute more or less the same way, at the same point in the workflow, and not have things kind of go off the rails. You don’t want to have an agent make some compounding mistake where, after they do the first couple 100 submissions, they start to kind of run wild. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It becomes really important to have the right demarcation points, where the agent starts and the other parts of the system end. For every workflow, there’s this question of what needs to have deterministic guardrails, and what can be fully agentic and non-deterministic.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What you can do with Box Automate is decide how much work you want each individual agent to do before it hands off to a different agent. So you might have a submission agent that’s separate from the review agent, and so on. It’s allowing you to basically deploy AI agents at scale in any kind of workflow or business process in the organization.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A visualization of the Box Automate workflow" class="wp-image-3045173" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Box-Automate-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A Box Automate workflow, with AI agents deployed for specific tasks. &lt;strong&gt;Image Credits:&lt;/strong&gt; Box&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: What kind of problems do you guard against by splitting up the workflow?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;We’ve already seen some of the limitations even in the most advanced fully agentic systems like Claude Code. At some point in the task, the model runs out of context-window room to continue making good decisions. There’s no free lunch right now in AI. You can’t just have a long-running agent with unlimited context window go after any task in your business. So you have to break up the workflow and use sub-agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think we’re in the era of context within AI. What AI models and agents need is context, and the context that they need to work off is sitting inside your unstructured data. So our whole system is really designed to figure out what context you can give the AI agent to ensure that they perform as effectively as possible.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: There is a bigger debate in the industry about the benefits of big, powerful frontier models compared to models that are smaller and more reliable. Does this put you on the side of the smaller models?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;I should probably clarify: Nothing about our system prevents the task from being arbitrarily long or complex. What we’re trying to do is create the right guardrails so that you get to decide how agentic you want that task to be. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;We don’t have a particular philosophy as to where people should be on that continuum. We’re just trying to design a future-proof architecture. We’ve designed this in such a way where, as the models improve and as agentic capabilities improve, you will just get all of those benefits directly in our platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: The other concern is data control. Because models are trained on so much data, there’s a real fear that sensitive data will get regurgitated or misused. How does that factor in?&lt;/strong&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;It’s where a lot of AI deployments go wrong. People think, “Hey, this is easy. I’ll give an AI model access to all of my unstructured data, and it’ll answer questions for people.” And then it starts to give you answers on data that you don’t have access to or you shouldn’t have access to. You need a very powerful layer that handles access controls, data security, permissions, data governance, compliance, everything.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So we’re benefiting from the couple decades that we’ve spent building up a system that basically handles that exact problem: How do you ensure only the right person has access to each piece of data in the enterprise? So when an agent answers a question, you know deterministically that it can’t draw on any data that that person shouldn’t have access to. That is just something fundamentally built into our system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TC: Earlier this week, Anthropic released a new feature for directly uploading files to Claude.ai. It’s a long way from the sort of file management that Box does, but you must be thinking about possible competition from the foundation model companies. How do you approach that strategically?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Levie: &lt;/strong&gt;So if you think about what enterprises need when they deploy AI at scale, they need security, permissions and control. They need the user interface, they need powerful APIs, they want their choice of AI models, because one day, one AI model powers some use case for them that is better than another, but then that might change, and they don’t want to be locked into one particular platform. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So what we’ve built is a system that lets you have effectively all of those capabilities. We’re doing the storage, the security, the permissions, the vector embedding, and we connect to every leading AI model that’s out there.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/11/box-ceo-aaron-levie-on-ais-era-of-context/</guid><pubDate>Thu, 11 Sep 2025 14:36:31 +0000</pubDate></item><item><title>Humanoids, AVs, and what’s next in AI hardware with Waabi and Apptronik at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/11/humanoids-avs-and-whats-next-in-ai-hardware-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; hits Moscone West in San Francisco from October 27 to 29, bringing together 10,000+ startup and VC leaders for three days of bold ideas, groundbreaking tech, and future-shaping conversations. One of the most highly anticipated &lt;strong&gt;sessions happening on one of the two AI Stages&lt;/strong&gt; will spotlight where AI hardware is heading next, featuring a live look at the robotics and autonomous systems pushing boundaries in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;AI may be reshaping software, but when it comes to robotics and autonomous systems, the big breakout moment is still on the horizon. That’s what makes this session at TechCrunch Disrupt 2025 so compelling. &lt;strong&gt;Raquel Urtasun&lt;/strong&gt;, founder and CEO of &lt;strong&gt;Waabi&lt;/strong&gt;, and &lt;strong&gt;Jeff Cardenas&lt;/strong&gt;, co-founder and CEO of &lt;strong&gt;Apptronik&lt;/strong&gt;, are joining forces on the AI Stage to talk about what it takes to put intelligence into motion — whether it’s behind the wheel or on two legs.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Jeff Cardenas Raquel Urtasun" class="wp-image-3026927" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_CardenasUrtasun-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-ai-meets-real-world-physics"&gt;&lt;br /&gt;AI meets real-world physics&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This conversation dives into the complex systems that power autonomous vehicles and humanoid robots — and the simulation, sensors, and software infrastructure needed to scale them safely. Both Waabi and Apptronik are pushing the limits of what’s possible in the physical world. At Disrupt, they’ll walk us through the breakthroughs and bottlenecks shaping the next generation of intelligent machines.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-session-matters"&gt;Why this session matters&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AI is already changing how we build, ship, and move — but physical deployment brings a unique set of constraints and opportunities. Expect a grounded, forward-looking discussion on how the smartest robots and self-driving platforms are coming to life, and what that means for the future of industry, labor, and infrastructure.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Catch Raquel Urtasun and Jeff Cardenas on the AI Stage at TechCrunch Disrupt 2025, happening October 27 to 29 at Moscone West in San Francisco. &lt;strong&gt;Register now&lt;/strong&gt; to join more than 10,000 startup and VC leaders and &lt;strong&gt;save up to $668&lt;/strong&gt; before prices increase on September 26, 11:59 p.m. PT.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; hits Moscone West in San Francisco from October 27 to 29, bringing together 10,000+ startup and VC leaders for three days of bold ideas, groundbreaking tech, and future-shaping conversations. One of the most highly anticipated &lt;strong&gt;sessions happening on one of the two AI Stages&lt;/strong&gt; will spotlight where AI hardware is heading next, featuring a live look at the robotics and autonomous systems pushing boundaries in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;br /&gt;AI may be reshaping software, but when it comes to robotics and autonomous systems, the big breakout moment is still on the horizon. That’s what makes this session at TechCrunch Disrupt 2025 so compelling. &lt;strong&gt;Raquel Urtasun&lt;/strong&gt;, founder and CEO of &lt;strong&gt;Waabi&lt;/strong&gt;, and &lt;strong&gt;Jeff Cardenas&lt;/strong&gt;, co-founder and CEO of &lt;strong&gt;Apptronik&lt;/strong&gt;, are joining forces on the AI Stage to talk about what it takes to put intelligence into motion — whether it’s behind the wheel or on two legs.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Jeff Cardenas Raquel Urtasun" class="wp-image-3026927" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_CardenasUrtasun-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-ai-meets-real-world-physics"&gt;&lt;br /&gt;AI meets real-world physics&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This conversation dives into the complex systems that power autonomous vehicles and humanoid robots — and the simulation, sensors, and software infrastructure needed to scale them safely. Both Waabi and Apptronik are pushing the limits of what’s possible in the physical world. At Disrupt, they’ll walk us through the breakthroughs and bottlenecks shaping the next generation of intelligent machines.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-session-matters"&gt;Why this session matters&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AI is already changing how we build, ship, and move — but physical deployment brings a unique set of constraints and opportunities. Expect a grounded, forward-looking discussion on how the smartest robots and self-driving platforms are coming to life, and what that means for the future of industry, labor, and infrastructure.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Catch Raquel Urtasun and Jeff Cardenas on the AI Stage at TechCrunch Disrupt 2025, happening October 27 to 29 at Moscone West in San Francisco. &lt;strong&gt;Register now&lt;/strong&gt; to join more than 10,000 startup and VC leaders and &lt;strong&gt;save up to $668&lt;/strong&gt; before prices increase on September 26, 11:59 p.m. PT.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/11/humanoids-avs-and-whats-next-in-ai-hardware-at-techcrunch-disrupt-2025/</guid><pubDate>Thu, 11 Sep 2025 15:02:50 +0000</pubDate></item><item><title>VMware nods to AI but looks to long-term (AI News)</title><link>https://www.artificialintelligence-news.com/news/broadcom-nods-at-ai-looks-to-long-term/</link><description>&lt;p&gt;Owner of VMware, Broadcom, announced that its VMware Cloud Foundation platform is now AI native at the VMware Explore conference a few weeks ago.&lt;/p&gt;&lt;p&gt;It was the latest move by the company to keep up to speed with the rest of the technology industry’s wide and rapid adoption of large language models, yet came as the company battles bad press about licensing policy changes that have dogged it since it acquired virtualisation giant VMware in November 2023.&lt;/p&gt;&lt;p&gt;The ending of the platform’s free tier, reports of aggressive sales tactics to keep subscribers on board, and several court cases focused on existing agreements, including extant perpetual licences, have led many users to rethink what is often the basis of their IT stack. Nutanix, SUSE, and IBM have been among the beneficiaries from those leaving the VMware stable.&lt;/p&gt;&lt;p&gt;But the nature of VMware deployments means that they’re often complex, and extricating workloads out from heavily-virtualised environments running on the platform can come with high migration costs and not insignificant risks to an organisation’s QoS metrics. Better to stay and pay the devil you know than go out on a limb and migrate to an alternative.&lt;/p&gt;&lt;p&gt;By the same token, engineering AI into VMware’s offerings is fraught with danger and the potential for identical fallout. Re-architecturing the VMware platform to bake AI in at the core would mean it would be end-users’ stuttering workloads paying the price for any breaking changes. And the nature of software is that the deeper breaking changes are made, the greater the potential negative ramifications.&lt;/p&gt;&lt;p&gt;Broadcom’s initial aims are to make it simpler for its users to deploy AI models and agents inside their existing environments. VMware Private AI Services is to ship with VCF 9 subscriptions next year, and will comprise of all the elements required to build and run AI on-premise, or at least outside hyperscale facilities. It will include a model store (it’s expected that many organisations will turn to – at least in testing phases – open-source, smaller models), indexing services, vector databases, an agentic AI builder, and a ready-made API gateway to allow optimised machine-to-machine communications between separate AI models that need to work together.&lt;/p&gt;&lt;p&gt;Conference attendees were told AI’s presence in the enterprise was only going to grow, and so it only made sense that AI should be a feature of every VMware-based infrastructure. As it stands, what Broadcom is offering is a nod in the AI direction, but nothing unique nor new. The company also announced improvements to the VMware Tanzu Platform which include simpler publishing of MCP servers, and a new data lakehouse, Tanzu Data Intelligence.&lt;/p&gt;&lt;p&gt;Presumably low-hanging fruit for VMware’s own developers was Intelligent Assist for VCF, a chatbot with access to the VMware knowledgebase. The AI-powered ‘bot will be able to lengthen the time between a user raising an issue or question, and them getting to speak to a human who can help.&lt;/p&gt;&lt;p&gt;The excitement around widespread adoption of containers led many to declare that the end was nigh for ‘traditional’ virtualisation, much in the same way that the explosion of cloud services was to spell the end for on-premise databases, and thus see off Oracle. The reality was, and remains, that legacy infrastructure compels enterprise users to consolidate on the platforms they have invested in, despite rapacious licence fees and high costs.&lt;/p&gt;&lt;p&gt;VMware may be sprinkling the deals between it and its customers with a little AI fairy-dust, but it knows that its long-term income is guaranteed by the presence of legacy infrastructure at the core of the enterprise.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Virtual Try On” by jurvetson is licensed under CC BY 2.0.&lt;/em&gt;)&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.developer-tech.com/wp-content/uploads/2025/08/developer-cloud-big-data-expo.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to dive deeper into the tools and frameworks shaping modern development?&lt;/strong&gt; Check out the AI &amp;amp; Big Data Expo, taking place in Amsterdam, California, and London. Explore cutting-edge sessions on machine learning, data pipelines, and next-gen AI applications. The event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;DeveloperTech News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Owner of VMware, Broadcom, announced that its VMware Cloud Foundation platform is now AI native at the VMware Explore conference a few weeks ago.&lt;/p&gt;&lt;p&gt;It was the latest move by the company to keep up to speed with the rest of the technology industry’s wide and rapid adoption of large language models, yet came as the company battles bad press about licensing policy changes that have dogged it since it acquired virtualisation giant VMware in November 2023.&lt;/p&gt;&lt;p&gt;The ending of the platform’s free tier, reports of aggressive sales tactics to keep subscribers on board, and several court cases focused on existing agreements, including extant perpetual licences, have led many users to rethink what is often the basis of their IT stack. Nutanix, SUSE, and IBM have been among the beneficiaries from those leaving the VMware stable.&lt;/p&gt;&lt;p&gt;But the nature of VMware deployments means that they’re often complex, and extricating workloads out from heavily-virtualised environments running on the platform can come with high migration costs and not insignificant risks to an organisation’s QoS metrics. Better to stay and pay the devil you know than go out on a limb and migrate to an alternative.&lt;/p&gt;&lt;p&gt;By the same token, engineering AI into VMware’s offerings is fraught with danger and the potential for identical fallout. Re-architecturing the VMware platform to bake AI in at the core would mean it would be end-users’ stuttering workloads paying the price for any breaking changes. And the nature of software is that the deeper breaking changes are made, the greater the potential negative ramifications.&lt;/p&gt;&lt;p&gt;Broadcom’s initial aims are to make it simpler for its users to deploy AI models and agents inside their existing environments. VMware Private AI Services is to ship with VCF 9 subscriptions next year, and will comprise of all the elements required to build and run AI on-premise, or at least outside hyperscale facilities. It will include a model store (it’s expected that many organisations will turn to – at least in testing phases – open-source, smaller models), indexing services, vector databases, an agentic AI builder, and a ready-made API gateway to allow optimised machine-to-machine communications between separate AI models that need to work together.&lt;/p&gt;&lt;p&gt;Conference attendees were told AI’s presence in the enterprise was only going to grow, and so it only made sense that AI should be a feature of every VMware-based infrastructure. As it stands, what Broadcom is offering is a nod in the AI direction, but nothing unique nor new. The company also announced improvements to the VMware Tanzu Platform which include simpler publishing of MCP servers, and a new data lakehouse, Tanzu Data Intelligence.&lt;/p&gt;&lt;p&gt;Presumably low-hanging fruit for VMware’s own developers was Intelligent Assist for VCF, a chatbot with access to the VMware knowledgebase. The AI-powered ‘bot will be able to lengthen the time between a user raising an issue or question, and them getting to speak to a human who can help.&lt;/p&gt;&lt;p&gt;The excitement around widespread adoption of containers led many to declare that the end was nigh for ‘traditional’ virtualisation, much in the same way that the explosion of cloud services was to spell the end for on-premise databases, and thus see off Oracle. The reality was, and remains, that legacy infrastructure compels enterprise users to consolidate on the platforms they have invested in, despite rapacious licence fees and high costs.&lt;/p&gt;&lt;p&gt;VMware may be sprinkling the deals between it and its customers with a little AI fairy-dust, but it knows that its long-term income is guaranteed by the presence of legacy infrastructure at the core of the enterprise.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Virtual Try On” by jurvetson is licensed under CC BY 2.0.&lt;/em&gt;)&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.developer-tech.com/wp-content/uploads/2025/08/developer-cloud-big-data-expo.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to dive deeper into the tools and frameworks shaping modern development?&lt;/strong&gt; Check out the AI &amp;amp; Big Data Expo, taking place in Amsterdam, California, and London. Explore cutting-edge sessions on machine learning, data pipelines, and next-gen AI applications. The event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;DeveloperTech News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/broadcom-nods-at-ai-looks-to-long-term/</guid><pubDate>Thu, 11 Sep 2025 15:44:08 +0000</pubDate></item><item><title>Tool-space interference in the MCP era: Designing for agent compatibility at scale (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/</link><description>&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Three white icons on a gradient background transitioning from blue to purple to pink. From left to right: a globe with a magnifying glass representing internet search, a central circle connected to smaller circles symbolizing network connectivity, and a checklist with two checkmarks and one empty box indicating task management." class="wp-image-1149369" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1024x576.jpg" width="1024" /&gt;&lt;/figure&gt;



&lt;p&gt;This year&amp;nbsp;we’ve&amp;nbsp;seen&amp;nbsp;remarkable&amp;nbsp;advances in agentic AI, including&amp;nbsp;systems that conduct deep research,&amp;nbsp;operate&amp;nbsp;computers, complete substantial software engineering tasks, and tackle a range of other complex,&amp;nbsp;multi-step goals. In each case,&amp;nbsp;the industry relied&amp;nbsp;on careful vertical integration: tools and agents were co-designed, co-trained, and tested together&amp;nbsp;for peak&amp;nbsp;performance. For example,&amp;nbsp;OpenAI’s&amp;nbsp;recent models&amp;nbsp;presume&amp;nbsp;the&amp;nbsp;availability&amp;nbsp;of web search and document retrieval&amp;nbsp;tools&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Likewise,&amp;nbsp;the prompts and actions&amp;nbsp;of&amp;nbsp;Magentic-One&amp;nbsp;are&amp;nbsp;set up to make hand-offs easy—for example, allowing the WebSurfer agent to pass downloaded files to the Coder agent. &amp;nbsp;But as agents proliferate, we anticipate strategies relying heavily on vertical integration will not age well.&amp;nbsp;Agents&amp;nbsp;from&amp;nbsp;different&amp;nbsp;developers&amp;nbsp;or companies will&amp;nbsp;increasingly&amp;nbsp;encounter&amp;nbsp;each other and&amp;nbsp;must&amp;nbsp;work together to complete tasks, in what we refer to as a&amp;nbsp;&lt;em&gt;society of agents&lt;/em&gt;.&amp;nbsp;These systems can vary in how coordinated they are, how aligned their goals are, and how much information they share. Can heterogenous agents and tools cooperate&amp;nbsp;in this&amp;nbsp;setting, or will they hinder one another and slow progress?&lt;/p&gt;



&lt;p&gt;Early clues have&amp;nbsp;emerged&amp;nbsp;from an&amp;nbsp;unexpected&amp;nbsp;source:&amp;nbsp;namely,&amp;nbsp;Model Context Protocol&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;(MCP). Since January 2025, MCP has grown from a&amp;nbsp;promising spec to a&amp;nbsp;thriving&amp;nbsp;market&amp;nbsp;of&amp;nbsp;tool&amp;nbsp;servers.&amp;nbsp;As an example, Zapier boasts a catalog of 30,000 tools&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;across 7,000 services.&amp;nbsp;Composio&amp;nbsp;provide over 100 managed MCP servers&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, surfacing hundreds of tools. Hugging&amp;nbsp;Face is now serving&amp;nbsp;many&amp;nbsp;Spaces&amp;nbsp;apps over MCP&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and Shopify has enabled MCP for millions of storefronts&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;A&amp;nbsp;society of&amp;nbsp;&lt;em&gt;tools&lt;/em&gt;&amp;nbsp;is already here, and it promises to&amp;nbsp;extend&amp;nbsp;agent capabilities through&amp;nbsp;cross-provider&amp;nbsp;horizontal integration.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So,&amp;nbsp;what does MCP have to say about&amp;nbsp;horizontal integration? As catalogs grow,&amp;nbsp;we expect some new failure modes to surface.&amp;nbsp;This&amp;nbsp;blog&amp;nbsp;post introduces&amp;nbsp;these&amp;nbsp;as &lt;em&gt;tool-space interference&lt;/em&gt;, and sketches both early observations and some pragmatic interventions to keep the society&amp;nbsp;we’re&amp;nbsp;building&amp;nbsp;from stepping on its own feet.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Tool-space interference describes situations where otherwise reasonable tools or agents, when co-present, reduce end-to-end effectiveness. This can look like longer action sequences, higher token cost, brittle recovery from errors, or, in some cases, task failure.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="a-framing-example"&gt;A framing example&lt;/h2&gt;



&lt;p&gt;Consider MCP as a means for extending Magentic-One, a generalist multi-agent system we released last year, to cover more software engineering tasks. Magentic-One ships with agents to write code, interact with the computer terminal, browse the web, and access local files. To help Magentic-One navigate version control, find issues to solve, and make pull requests, we could add an agent equipped with the GitHub MCP Server. However, now each time the team encounters a task involving GitHub, it must choose whether to visit github.com in the browser, execute a git command at the command line, or engage the GitHub MCP server. As the task progresses, agent understanding of state can also diverge: changing the branch in the browser won’t change the branch in the terminal, and an authorized MCP tool does not imply authorization in the browser.&amp;nbsp;Thus, while any single agent might complete the task efficiently, the larger set of agents might misunderstand or interfere with one another, leading to additional rounds of debugging, or even complete task failure.&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Diagram depicting Magentic-One's multi-agentic architecture. An Orchestrator agent has access to 4 specialized sub-agents: a Coder agent that can write code and reason to sol solve tasks, a Computer Terminal Agent that can execute code written by the Coder agent, a WebSurfer agent that browse the internet (navigate pages, fill forms, etc), and a FileSurfer agent that can navigate files (e.g. PDFs, PPTx, etc). The diagram is annotated to show that for any incoming git-related task, the Orchestrator agent has to decide at evert orchestration step whether to access Git CLI via ComputerTerminal, visit Github site via WebSurfer, or directly access Github’s MCP server." class="wp-image-1149211" height="410" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image.png" width="1021" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1: We can extend&amp;nbsp;Magentic-One by adding an agent that equips the GitHub MCP server. However, on every turn involving a git-related task, the orchestrator will need to decide between messaging the Computer Terminal agent (with access to the git command line interface), WebSurfer agent (with access to github.com), and the agent with the GitHub MCP server. This overlap raises the possibility that they will interfere with one another.&amp;nbsp;&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;To better understand the potential interference patterns and the current state of the MCP ecosystem, we conducted a survey of MCP servers listed on two registries: smithery.ai&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Docker MCP Hub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Smithery is an MCP Server registry with over 7,000 first-party and community-contributed servers, which we sampled from the Smithery API. Likewise, Docker MCP Hub is a registry that distributes MCP servers as Docker images, and we manually collected popular entries. We then launched each server for inspection. After excluding servers that were empty or failed to launch, and deduplicating servers with identical features, 1,470 servers remained in our catalog.&lt;/p&gt;



&lt;p&gt;To&amp;nbsp;automate the&amp;nbsp;inspection&amp;nbsp;of&amp;nbsp;running MCP servers,&amp;nbsp;we developed an&amp;nbsp;MCP&amp;nbsp;Interviewer&amp;nbsp;tool.&amp;nbsp;The MCP&amp;nbsp;Interviewer&amp;nbsp;begins by cataloging the server’s tools, prompts, resources, resource templates, and capabilities.&amp;nbsp;From&amp;nbsp;this catalog we can compute&amp;nbsp;descriptive statistics&amp;nbsp;such as the number of tools, or the depth of the parameter&amp;nbsp;schemas.&amp;nbsp;&amp;nbsp;Then, given the list of available tools, the interviewer uses&amp;nbsp;an LLM (in our case,&amp;nbsp;OpenAI’s GPT-4.1)&amp;nbsp;to construct a functional testing&amp;nbsp;plan&amp;nbsp;that&amp;nbsp;calls each tool at least once, collecting outputs, errors, and statistics along the way. Finally,&amp;nbsp;the&amp;nbsp;interviewer&amp;nbsp;can&amp;nbsp;also&amp;nbsp;grade&amp;nbsp;more qualitative&amp;nbsp;criteria&amp;nbsp;by&amp;nbsp;using&amp;nbsp;an LLM&amp;nbsp;to&amp;nbsp;apply purpose-built rubrics&amp;nbsp;to&amp;nbsp;tool&amp;nbsp;schemas&amp;nbsp;and&amp;nbsp;tool call outputs.&amp;nbsp;&amp;nbsp;We are excited to&amp;nbsp;release the MCP Interviewer&amp;nbsp;as an open-source CLI&amp;nbsp;tool&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, so server developers can automatically evaluate their MCP servers with agent usability in mind,&amp;nbsp;and users can&amp;nbsp;validate&amp;nbsp;new servers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While our survey provides informative initial results, it also faces significant limitations, the most obvious of which is authorization: many of the most popular MCP servers provide access to services that require authorization to use, hindering automated analysis. We are often still able to collect static features from these servers but are limited in the functional testing that can be done.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="one-size-fits-all-but-some-more-than-others"&gt;One-size fits all (but some more than others)&lt;/h3&gt;



&lt;p&gt;So, what does our survey of MCP servers tell us about the MCP ecosystem? We will get into the numbers in a moment, but as we contemplate the statistics, there is one overarching theme to keep in mind: MCP servers do not know which clients or models they are working with, and present one common set of tools, prompts, and resources to everyone. However, some models handle long contexts and large tool spaces better than others (with diverging hard limits), and respond quite differently to common prompting patterns. For example, OpenAI’s guide on function calling&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; advises developers to:&lt;/p&gt;



&lt;p&gt;“&lt;em&gt;Include examples and edge cases, especially to rectify any recurring failures. (Note: Adding examples may hurt performance for reasoning models).”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;So already, this places MCP at a disadvantage over vertical integrations that optimize to the operating environment. And with that, let’s dive into more numbers.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="tool-count"&gt;Tool count&lt;/h3&gt;



&lt;p&gt;While models generally vary in their proficiency for tool calling, the general trend has been that performance drops as the number of tools increases. For example, OpenAI limits developers to 128 tools, but recommends&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that developers:&lt;/p&gt;



&lt;p&gt;“&lt;em&gt;Keep the number of functions small for higher accuracy. Evaluate your performance with different numbers of functions. Aim for fewer than 20 functions at any one time, though this is just a soft suggestion.&lt;/em&gt;”&lt;/p&gt;



&lt;p&gt;While we expect this to improve with each new model generation, at present, large tool spaces can lower performance by up to 85% for some models&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Thankfully, the majority of servers in our survey contain four or fewer tools. But there are outliers: the largest MCP server we cataloged adds 256 distinct tools, while the 10 next-largest servers add more than 100 tools each. Further down the list we find popular servers like Playwright-MCP&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (29 tools, at the time of this writing), and GitHub MCP (91 tools, with subsets available at alternative endpoint URLs), which might be too large for some models.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="chart" class="wp-image-1149361" height="1024" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1024x1024.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2: The number of tools listed by each catalogued server directly after initialization. Note: servers can change the tools they list at any time, but only 226 servers in our catalog declare this capability.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="response-length"&gt;Response length&lt;/h3&gt;



&lt;p&gt;Tools are generally called in agentic loops, where the output is then fed back into the model as input context. Models have hard limits on input context, but even within these limits, large contexts can drive costs up and performance down, so practical limits can be much lower&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. MCP offers no guidance on how many tokens a tool call can produce, and the size of some responses can come as a surprise. In our analysis, we consider the 2,443 tool calls across 1,312 unique tools that the MCP Interviewer was able to call successfully during the active testing phase of server inspection. While a majority of tools produced 98 or fewer tokens &lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, some tools are extraordinarily heavyweight: the top tool returned an average of 557,766 tokens, which is enough to swamp the context windows of many popular models like GPT-5. Further down the list, we find that 16 tools produce more than 128,000 tokens, swamping GPT-4o and other popular models. Even when responses fit into the context window length, overly long responses can significantly degrade performance (up to 91% in one study&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;), and limit the number of future calls that can be made. Of course, agents are free to implement their own context management strategies, but this behavior is left undefined in the MCP specification and server developers cannot count on any particular client behavior or strategy.&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan="4"&gt;&lt;strong&gt;# of tools that would overflow context in&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Context Window&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;1 call&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;2 calls&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;3-5 calls&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;6-10 calls&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPT 4.1&lt;/td&gt;&lt;td&gt;1,000,000&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPT 5&lt;/td&gt;&lt;td&gt;400,000&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;25&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPT-4o, Llama 3.1,&lt;/td&gt;&lt;td&gt;128,000&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;33&lt;/td&gt;&lt;td&gt;40&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Qwen 3&lt;/td&gt;&lt;td&gt;32,000&lt;/td&gt;&lt;td&gt;56&lt;/td&gt;&lt;td&gt;37&lt;/td&gt;&lt;td&gt;86&lt;/td&gt;&lt;td&gt;90&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Phi-4&lt;/td&gt;&lt;td&gt;16,000&lt;/td&gt;&lt;td&gt;93&lt;/td&gt;&lt;td&gt;60&lt;/td&gt;&lt;td&gt;116&lt;/td&gt;&lt;td&gt;109&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Chart showing the average tool call output lengths (in tokens) for 1,312 tools, as observed by the MCP Interviewer’s functional test plan. The x-axis represents individual tools (sorted by index), and the y-axis displays the average output length on a logarithmic scale. Horizontal dashed lines indicate context window limits for GPT-4o (128k tokens) and GPT-5 (400k tokens). A pink annotation box summarizes statistics: total tools (1,312), mean (4,431 tokens), median (98 tokens), minimum (0 tokens), and maximum (557,766 tokens)." class="wp-image-1149213" height="935" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1.png" width="936" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3: Tool call response length averages, in tokens, as&amp;nbsp;observed&amp;nbsp;by the MCP Interviewer’s functional test plan. Only successful tool calls are considered. Horizontal lines&amp;nbsp;indicate&amp;nbsp;context window limits for GPT-4o and GPT-5.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="tool-parameter-complexity"&gt;Tool parameter complexity&lt;/h3&gt;



&lt;p&gt;Mirroring the challenges from increasing&amp;nbsp;the&amp;nbsp;number of tools,&amp;nbsp;increasing the complexity of a tool’s parameter space can also lead to degradation.&amp;nbsp;For example, while MCP tools can take complex object types and structures as parameters,&amp;nbsp;composio&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;found that&amp;nbsp;flattening the parameter space could improve tool-calling performance&amp;nbsp;by 47%&amp;nbsp;compared to baseline performance.&amp;nbsp;&amp;nbsp;In our analysis, we&amp;nbsp;find&amp;nbsp;numerous examples of deeply nested structure—in&amp;nbsp;one&amp;nbsp;case, going&amp;nbsp;20&amp;nbsp;levels deep.&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Chart showing the maximum depth of each tool’s input properties schema. The x-axis represents individual tools (sorted by index), and the y-axis shows the maximum property schema depth. Most tools have a depth  of 2 (named and annotated properties). A pink annotation box summarizes statistics: total tools (12,643), mean (2.24), median (2.00), standard deviation (1.38), minimum (0.00), and maximum (20.00). " class="wp-image-1149365" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4: The maximum depth of each tool’s input properties schema. A depth of 0&amp;nbsp;indicates&amp;nbsp;a tool with no properties. A depth of 1&amp;nbsp;indicates&amp;nbsp;a tool with named properties but no annotations (e.g., no description or type). A depth of 2&amp;nbsp;indicates&amp;nbsp;a tool with named and annotated properties.&amp;nbsp;&amp;nbsp;A depth of 3+&amp;nbsp;indicates&amp;nbsp;a tool with structured properties that have&amp;nbsp;additional&amp;nbsp;nested annotations.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="namespacing-issues-and-naming-ambiguity"&gt;Namespacing issues and naming ambiguity&lt;/h3&gt;



&lt;p&gt;Another often-cited issue with the current MCP specification is the lack of a formal namespace mechanism&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. If two servers are registered to the same agent or application, and the servers have tool names in common, then disambiguation becomes impossible. Libraries like the OpenAI Agents SDK raise an error&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; under this circumstance. Clients, like Claude Code, prefix tool names with unique identifiers to work around this issue. In our analysis of MCP servers, we found name collisions between 775 tools. The most common collision was “search”, which appears across 32 distinct MCP servers. The following table lists the top 10 collisions.&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Tool Name&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Number of Instances&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;search&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;32&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;get_user&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;execute_query&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;list_tables&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;update_task&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;generate_image&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;send_message&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;execute_command&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;list_tasks&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;search_files&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;Even when names are unique, they can be semantically similar. If these tools behave similarly, then the redundancy may not be immediately problematic, but if you are expecting to call a particular tool then the name similarities raise the potential for confusion. The following table lists some examples of semantically similar tool names relating to web search:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;websearch&lt;/td&gt;&lt;td&gt;brave_web_search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search-web&lt;/td&gt;&lt;td&gt;tavily_web_search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;web_search&lt;/td&gt;&lt;td&gt;google_news_search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search_web&lt;/td&gt;&lt;td&gt;google-play-search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search_webkr&lt;/td&gt;&lt;td&gt;google_search_parsed&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;google_search&lt;/td&gt;&lt;td&gt;search_google_images&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search_google&lt;/td&gt;&lt;td&gt;get_webset_search_exa&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ai_web_search&lt;/td&gt;&lt;td&gt;search_google_scholar&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;web_search_exa&lt;/td&gt;&lt;td&gt;duckduckgo_web_search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search_web_tool&lt;/td&gt;&lt;td&gt;google_search_scraper&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;web_search_agent&lt;/td&gt;&lt;td&gt;answer_query_websearch&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;batch-web-search&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="errors-and-error-messages"&gt;Errors and error messages&lt;/h3&gt;



&lt;p&gt;Like all software libraries, MCP will occasionally encounter error conditions. In these cases, it is important to provide sufficient information for the agent to handle the error and plan next steps. In our analysis, we found this was not always the case. While MCP provides an “IsError” flag to signal errors, we found that it was common for servers to handle errors by returning strings while leaving this flag set to false, signaling a normal exit. Out of 5,983 tool call results with no error flag, GPT-4.1 judged that 3,536 indicated errors in their content. More worrisome: the error messages were often of low quality. For instance, one tool providing web search capabilities failed with the string “error: job,” while another tool providing academic search returned “Please retry with 0 or fewer IDs.”&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="resource-sharing-conventions"&gt;Resource sharing conventions&lt;/h3&gt;



&lt;p&gt;Finally, in addition to tools, MCP allows servers to share resources and resource templates with clients. In our survey, only 112 (7.6%) servers reported any resources, while 74 (5%) provided templates. One potential reason for low adoption is that the current MCP specification provides limited guidance for when resources are retrieved, or how they are incorporated into context. One clearcut situation where a client might retrieve a resource is in response to a tool returning a resource_link&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; as a result — but only 4 tools exhibited this behavior in our survey (arguably, this would be the ideal behavior for tools that return very long, document-like responses, as outlined earlier).&lt;/p&gt;



&lt;p&gt;Conversely, a whole different set of issues arises when there is a need to share resources from the client to the server. Consider for example a tool that provides some analysis of a &lt;em&gt;local&lt;/em&gt; PDF file. In the case of a local MCP server utilizing STDIO transport, a local file path can be provided as an argument to the tool, but no similar conventions exist for delivering a local file to a remote MCP server. These issues are challenging enough when implementing a single server. When multiple tools or servers need to interact within the same system, the risk of interoperability errors compounds.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="recommendations"&gt;Recommendations&lt;/h2&gt;



&lt;p&gt;On balance, along any given dimension, the average MCP server is quite reasonable—but, as we have seen, outliers and diverging assumptions can introduce trouble. While we expect many of these challenges to improve with time, we are comfortable making small recommendations that we feel are evergreen. We organize them below by audience.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="protocol-developers"&gt;Protocol developers&lt;/h3&gt;



&lt;p&gt;We recognize the advantages of keeping MCP relatively lightweight, avoiding being overly prescriptive in an environment where AI models and use cases are rapidly changing. However, a few small recommendations are warranted. First, we believe MCP should be extended to include a specification for client-provided resources so that tools on remote servers have a mechanism for operating on specified local files or documents. This would more effectively position MCP as a clearinghouse for resources passed between steps of agentic workflows. The MCP specification would also benefit from taking a more opinionated stance on when resources are retrieved and used overall.&lt;/p&gt;



&lt;p&gt;Likewise, we believe&amp;nbsp;MCP should&amp;nbsp;quickly move to&amp;nbsp;provide formal namespaces&amp;nbsp;to eliminate tool name collisions.&amp;nbsp;If namespaces&amp;nbsp;are hierarchical, then this also provides a way of organizing large catalogs&amp;nbsp;of functions&amp;nbsp;into thematically&amp;nbsp;related tool&amp;nbsp;sets.&amp;nbsp;Tool sets, as an organizing principle,&amp;nbsp;are already showing some promise&amp;nbsp;in&amp;nbsp;GitHub MCP Server’s&amp;nbsp;dynamic tool discovery,&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;and VS Code’s&amp;nbsp;tool grouping (with virtual tools)&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;where agents or users&amp;nbsp;can&amp;nbsp;enable and disable tools&amp;nbsp;as needed.&amp;nbsp;&amp;nbsp;In the future,&amp;nbsp;a standardized mechanism for grouping tools would allow&amp;nbsp;&lt;em&gt;clients&lt;/em&gt;&amp;nbsp;to engage in hierarchical tool-calling,&amp;nbsp;where they first select a category, then select a tool, without needing to keep all possible&amp;nbsp;tools in context.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="server-developers"&gt;Server developers&lt;/h3&gt;



&lt;p&gt;While our MCP Interviewer tool can catalog many outward-facing properties of MCP servers, developers are often in a much better position to characterize the nature of their tools. To this end, we believe developers should publish an MCP Server card alongside their servers or services, clearly outlining the runtime characteristics of the tools (e.g., the expected number of tokens generated, or expected latency of a tool call). Ideally developers should also indicate which models, agents and clients the server was tested with, how the tools were tested (e.g., provide sample tasks), list any known incompatibilities, and be mindful of limitations of various models throughout development.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="client-developers"&gt;Client developers&lt;/h3&gt;



&lt;p&gt;Client developers have the opportunity to experiment with various mitigations or optimizations that might help the average MCP server work better for a given system or environment. For example, clients could cache tool schemas, serving them as targets for prompt optimizations, or as an index for RAG-like tool selection approaches. To this end, Anthropic recently reported using a tool testing agent&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to rewrite the prompts of defective MCP servers, improving task completion time by 40%. Likewise, rather than waiting for the protocol to evolve, clients could take proactive steps to resolve name collisions— for example, generating namespaces from server names—and could reduce token outputs by summarizing or paginating long tool results.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="market-developers"&gt;Market developers&lt;/h3&gt;



&lt;p&gt;Finally, we see an opportunity for marketplaces to codify best-practices, spot compatibility issues at a global level, and perhaps centralize the generation and serving of model or agent-specific optimizations. Mirroring how a market like PyPI distributes Python wheels matched to a developer’s operating system or processor&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, an MCP marketplace could serve tool schemas optimized for a developer’s chosen LLM, agent or client library. We are already seeing small steps in this direction, with registries like Smithery providing customized launch configurations to match users’ clients.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="conclusion"&gt;Conclusion&lt;/h2&gt;



&lt;p&gt;In summary, the MCP&amp;nbsp;ecosystem offers significant value for AI agent development,&amp;nbsp;despite&amp;nbsp;some&amp;nbsp;early&amp;nbsp;growing pains.&amp;nbsp;Grounded in insights from the&amp;nbsp;MCP Interviewer&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;and our survey of live servers, the evidence is clear: horizontal integration is expanding capability, yet it also exposes forms of toolspace interference that can erode end to end effectiveness. Anticipating rapid advances in model capability and growing architectural diversity, the recommendations provided here aim to ensure that protocol, server, client, and marketplace developers are&amp;nbsp;well positioned&amp;nbsp;to adapt and thrive. Key steps include implementing formal namespaces to&amp;nbsp;eliminate&amp;nbsp;collisions, enhancing protocol support for&amp;nbsp;client provided&amp;nbsp;resources, and encouraging transparent server documentation to foster interoperability and robust development practices across the ecosystem.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By embracing these evergreen recommendations and proactively addressing compatibility, usability, and optimization issues, the AI agent community can create a more reliable, scalable, and efficient infrastructure that benefits both developers and end users. The future of MCP is bright, with ample opportunities for experimentation, standardization, and collective progress.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Three white icons on a gradient background transitioning from blue to purple to pink. From left to right: a globe with a magnifying glass representing internet search, a central circle connected to smaller circles symbolizing network connectivity, and a checklist with two checkmarks and one empty box indicating task management." class="wp-image-1149369" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1024x576.jpg" width="1024" /&gt;&lt;/figure&gt;



&lt;p&gt;This year&amp;nbsp;we’ve&amp;nbsp;seen&amp;nbsp;remarkable&amp;nbsp;advances in agentic AI, including&amp;nbsp;systems that conduct deep research,&amp;nbsp;operate&amp;nbsp;computers, complete substantial software engineering tasks, and tackle a range of other complex,&amp;nbsp;multi-step goals. In each case,&amp;nbsp;the industry relied&amp;nbsp;on careful vertical integration: tools and agents were co-designed, co-trained, and tested together&amp;nbsp;for peak&amp;nbsp;performance. For example,&amp;nbsp;OpenAI’s&amp;nbsp;recent models&amp;nbsp;presume&amp;nbsp;the&amp;nbsp;availability&amp;nbsp;of web search and document retrieval&amp;nbsp;tools&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Likewise,&amp;nbsp;the prompts and actions&amp;nbsp;of&amp;nbsp;Magentic-One&amp;nbsp;are&amp;nbsp;set up to make hand-offs easy—for example, allowing the WebSurfer agent to pass downloaded files to the Coder agent. &amp;nbsp;But as agents proliferate, we anticipate strategies relying heavily on vertical integration will not age well.&amp;nbsp;Agents&amp;nbsp;from&amp;nbsp;different&amp;nbsp;developers&amp;nbsp;or companies will&amp;nbsp;increasingly&amp;nbsp;encounter&amp;nbsp;each other and&amp;nbsp;must&amp;nbsp;work together to complete tasks, in what we refer to as a&amp;nbsp;&lt;em&gt;society of agents&lt;/em&gt;.&amp;nbsp;These systems can vary in how coordinated they are, how aligned their goals are, and how much information they share. Can heterogenous agents and tools cooperate&amp;nbsp;in this&amp;nbsp;setting, or will they hinder one another and slow progress?&lt;/p&gt;



&lt;p&gt;Early clues have&amp;nbsp;emerged&amp;nbsp;from an&amp;nbsp;unexpected&amp;nbsp;source:&amp;nbsp;namely,&amp;nbsp;Model Context Protocol&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;(MCP). Since January 2025, MCP has grown from a&amp;nbsp;promising spec to a&amp;nbsp;thriving&amp;nbsp;market&amp;nbsp;of&amp;nbsp;tool&amp;nbsp;servers.&amp;nbsp;As an example, Zapier boasts a catalog of 30,000 tools&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;across 7,000 services.&amp;nbsp;Composio&amp;nbsp;provide over 100 managed MCP servers&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, surfacing hundreds of tools. Hugging&amp;nbsp;Face is now serving&amp;nbsp;many&amp;nbsp;Spaces&amp;nbsp;apps over MCP&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and Shopify has enabled MCP for millions of storefronts&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;A&amp;nbsp;society of&amp;nbsp;&lt;em&gt;tools&lt;/em&gt;&amp;nbsp;is already here, and it promises to&amp;nbsp;extend&amp;nbsp;agent capabilities through&amp;nbsp;cross-provider&amp;nbsp;horizontal integration.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So,&amp;nbsp;what does MCP have to say about&amp;nbsp;horizontal integration? As catalogs grow,&amp;nbsp;we expect some new failure modes to surface.&amp;nbsp;This&amp;nbsp;blog&amp;nbsp;post introduces&amp;nbsp;these&amp;nbsp;as &lt;em&gt;tool-space interference&lt;/em&gt;, and sketches both early observations and some pragmatic interventions to keep the society&amp;nbsp;we’re&amp;nbsp;building&amp;nbsp;from stepping on its own feet.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Tool-space interference describes situations where otherwise reasonable tools or agents, when co-present, reduce end-to-end effectiveness. This can look like longer action sequences, higher token cost, brittle recovery from errors, or, in some cases, task failure.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="a-framing-example"&gt;A framing example&lt;/h2&gt;



&lt;p&gt;Consider MCP as a means for extending Magentic-One, a generalist multi-agent system we released last year, to cover more software engineering tasks. Magentic-One ships with agents to write code, interact with the computer terminal, browse the web, and access local files. To help Magentic-One navigate version control, find issues to solve, and make pull requests, we could add an agent equipped with the GitHub MCP Server. However, now each time the team encounters a task involving GitHub, it must choose whether to visit github.com in the browser, execute a git command at the command line, or engage the GitHub MCP server. As the task progresses, agent understanding of state can also diverge: changing the branch in the browser won’t change the branch in the terminal, and an authorized MCP tool does not imply authorization in the browser.&amp;nbsp;Thus, while any single agent might complete the task efficiently, the larger set of agents might misunderstand or interfere with one another, leading to additional rounds of debugging, or even complete task failure.&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Diagram depicting Magentic-One's multi-agentic architecture. An Orchestrator agent has access to 4 specialized sub-agents: a Coder agent that can write code and reason to sol solve tasks, a Computer Terminal Agent that can execute code written by the Coder agent, a WebSurfer agent that browse the internet (navigate pages, fill forms, etc), and a FileSurfer agent that can navigate files (e.g. PDFs, PPTx, etc). The diagram is annotated to show that for any incoming git-related task, the Orchestrator agent has to decide at evert orchestration step whether to access Git CLI via ComputerTerminal, visit Github site via WebSurfer, or directly access Github’s MCP server." class="wp-image-1149211" height="410" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image.png" width="1021" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1: We can extend&amp;nbsp;Magentic-One by adding an agent that equips the GitHub MCP server. However, on every turn involving a git-related task, the orchestrator will need to decide between messaging the Computer Terminal agent (with access to the git command line interface), WebSurfer agent (with access to github.com), and the agent with the GitHub MCP server. This overlap raises the possibility that they will interfere with one another.&amp;nbsp;&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;To better understand the potential interference patterns and the current state of the MCP ecosystem, we conducted a survey of MCP servers listed on two registries: smithery.ai&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Docker MCP Hub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Smithery is an MCP Server registry with over 7,000 first-party and community-contributed servers, which we sampled from the Smithery API. Likewise, Docker MCP Hub is a registry that distributes MCP servers as Docker images, and we manually collected popular entries. We then launched each server for inspection. After excluding servers that were empty or failed to launch, and deduplicating servers with identical features, 1,470 servers remained in our catalog.&lt;/p&gt;



&lt;p&gt;To&amp;nbsp;automate the&amp;nbsp;inspection&amp;nbsp;of&amp;nbsp;running MCP servers,&amp;nbsp;we developed an&amp;nbsp;MCP&amp;nbsp;Interviewer&amp;nbsp;tool.&amp;nbsp;The MCP&amp;nbsp;Interviewer&amp;nbsp;begins by cataloging the server’s tools, prompts, resources, resource templates, and capabilities.&amp;nbsp;From&amp;nbsp;this catalog we can compute&amp;nbsp;descriptive statistics&amp;nbsp;such as the number of tools, or the depth of the parameter&amp;nbsp;schemas.&amp;nbsp;&amp;nbsp;Then, given the list of available tools, the interviewer uses&amp;nbsp;an LLM (in our case,&amp;nbsp;OpenAI’s GPT-4.1)&amp;nbsp;to construct a functional testing&amp;nbsp;plan&amp;nbsp;that&amp;nbsp;calls each tool at least once, collecting outputs, errors, and statistics along the way. Finally,&amp;nbsp;the&amp;nbsp;interviewer&amp;nbsp;can&amp;nbsp;also&amp;nbsp;grade&amp;nbsp;more qualitative&amp;nbsp;criteria&amp;nbsp;by&amp;nbsp;using&amp;nbsp;an LLM&amp;nbsp;to&amp;nbsp;apply purpose-built rubrics&amp;nbsp;to&amp;nbsp;tool&amp;nbsp;schemas&amp;nbsp;and&amp;nbsp;tool call outputs.&amp;nbsp;&amp;nbsp;We are excited to&amp;nbsp;release the MCP Interviewer&amp;nbsp;as an open-source CLI&amp;nbsp;tool&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, so server developers can automatically evaluate their MCP servers with agent usability in mind,&amp;nbsp;and users can&amp;nbsp;validate&amp;nbsp;new servers.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While our survey provides informative initial results, it also faces significant limitations, the most obvious of which is authorization: many of the most popular MCP servers provide access to services that require authorization to use, hindering automated analysis. We are often still able to collect static features from these servers but are limited in the functional testing that can be done.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="one-size-fits-all-but-some-more-than-others"&gt;One-size fits all (but some more than others)&lt;/h3&gt;



&lt;p&gt;So, what does our survey of MCP servers tell us about the MCP ecosystem? We will get into the numbers in a moment, but as we contemplate the statistics, there is one overarching theme to keep in mind: MCP servers do not know which clients or models they are working with, and present one common set of tools, prompts, and resources to everyone. However, some models handle long contexts and large tool spaces better than others (with diverging hard limits), and respond quite differently to common prompting patterns. For example, OpenAI’s guide on function calling&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; advises developers to:&lt;/p&gt;



&lt;p&gt;“&lt;em&gt;Include examples and edge cases, especially to rectify any recurring failures. (Note: Adding examples may hurt performance for reasoning models).”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;So already, this places MCP at a disadvantage over vertical integrations that optimize to the operating environment. And with that, let’s dive into more numbers.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="tool-count"&gt;Tool count&lt;/h3&gt;



&lt;p&gt;While models generally vary in their proficiency for tool calling, the general trend has been that performance drops as the number of tools increases. For example, OpenAI limits developers to 128 tools, but recommends&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; that developers:&lt;/p&gt;



&lt;p&gt;“&lt;em&gt;Keep the number of functions small for higher accuracy. Evaluate your performance with different numbers of functions. Aim for fewer than 20 functions at any one time, though this is just a soft suggestion.&lt;/em&gt;”&lt;/p&gt;



&lt;p&gt;While we expect this to improve with each new model generation, at present, large tool spaces can lower performance by up to 85% for some models&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Thankfully, the majority of servers in our survey contain four or fewer tools. But there are outliers: the largest MCP server we cataloged adds 256 distinct tools, while the 10 next-largest servers add more than 100 tools each. Further down the list we find popular servers like Playwright-MCP&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (29 tools, at the time of this writing), and GitHub MCP (91 tools, with subsets available at alternative endpoint URLs), which might be too large for some models.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="chart" class="wp-image-1149361" height="1024" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1024x1024.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2: The number of tools listed by each catalogued server directly after initialization. Note: servers can change the tools they list at any time, but only 226 servers in our catalog declare this capability.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="response-length"&gt;Response length&lt;/h3&gt;



&lt;p&gt;Tools are generally called in agentic loops, where the output is then fed back into the model as input context. Models have hard limits on input context, but even within these limits, large contexts can drive costs up and performance down, so practical limits can be much lower&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. MCP offers no guidance on how many tokens a tool call can produce, and the size of some responses can come as a surprise. In our analysis, we consider the 2,443 tool calls across 1,312 unique tools that the MCP Interviewer was able to call successfully during the active testing phase of server inspection. While a majority of tools produced 98 or fewer tokens &lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, some tools are extraordinarily heavyweight: the top tool returned an average of 557,766 tokens, which is enough to swamp the context windows of many popular models like GPT-5. Further down the list, we find that 16 tools produce more than 128,000 tokens, swamping GPT-4o and other popular models. Even when responses fit into the context window length, overly long responses can significantly degrade performance (up to 91% in one study&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;), and limit the number of future calls that can be made. Of course, agents are free to implement their own context management strategies, but this behavior is left undefined in the MCP specification and server developers cannot count on any particular client behavior or strategy.&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td colspan="4"&gt;&lt;strong&gt;# of tools that would overflow context in&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Context Window&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;1 call&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;2 calls&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;3-5 calls&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;6-10 calls&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPT 4.1&lt;/td&gt;&lt;td&gt;1,000,000&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPT 5&lt;/td&gt;&lt;td&gt;400,000&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;25&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GPT-4o, Llama 3.1,&lt;/td&gt;&lt;td&gt;128,000&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;33&lt;/td&gt;&lt;td&gt;40&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Qwen 3&lt;/td&gt;&lt;td&gt;32,000&lt;/td&gt;&lt;td&gt;56&lt;/td&gt;&lt;td&gt;37&lt;/td&gt;&lt;td&gt;86&lt;/td&gt;&lt;td&gt;90&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Phi-4&lt;/td&gt;&lt;td&gt;16,000&lt;/td&gt;&lt;td&gt;93&lt;/td&gt;&lt;td&gt;60&lt;/td&gt;&lt;td&gt;116&lt;/td&gt;&lt;td&gt;109&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Chart showing the average tool call output lengths (in tokens) for 1,312 tools, as observed by the MCP Interviewer’s functional test plan. The x-axis represents individual tools (sorted by index), and the y-axis displays the average output length on a logarithmic scale. Horizontal dashed lines indicate context window limits for GPT-4o (128k tokens) and GPT-5 (400k tokens). A pink annotation box summarizes statistics: total tools (1,312), mean (4,431 tokens), median (98 tokens), minimum (0 tokens), and maximum (557,766 tokens)." class="wp-image-1149213" height="935" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1.png" width="936" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3: Tool call response length averages, in tokens, as&amp;nbsp;observed&amp;nbsp;by the MCP Interviewer’s functional test plan. Only successful tool calls are considered. Horizontal lines&amp;nbsp;indicate&amp;nbsp;context window limits for GPT-4o and GPT-5.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="tool-parameter-complexity"&gt;Tool parameter complexity&lt;/h3&gt;



&lt;p&gt;Mirroring the challenges from increasing&amp;nbsp;the&amp;nbsp;number of tools,&amp;nbsp;increasing the complexity of a tool’s parameter space can also lead to degradation.&amp;nbsp;For example, while MCP tools can take complex object types and structures as parameters,&amp;nbsp;composio&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;found that&amp;nbsp;flattening the parameter space could improve tool-calling performance&amp;nbsp;by 47%&amp;nbsp;compared to baseline performance.&amp;nbsp;&amp;nbsp;In our analysis, we&amp;nbsp;find&amp;nbsp;numerous examples of deeply nested structure—in&amp;nbsp;one&amp;nbsp;case, going&amp;nbsp;20&amp;nbsp;levels deep.&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Chart showing the maximum depth of each tool’s input properties schema. The x-axis represents individual tools (sorted by index), and the y-axis shows the maximum property schema depth. Most tools have a depth  of 2 (named and annotated properties). A pink annotation box summarizes statistics: total tools (12,643), mean (2.24), median (2.00), standard deviation (1.38), minimum (0.00), and maximum (20.00). " class="wp-image-1149365" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-scaled.png" width="2560" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4: The maximum depth of each tool’s input properties schema. A depth of 0&amp;nbsp;indicates&amp;nbsp;a tool with no properties. A depth of 1&amp;nbsp;indicates&amp;nbsp;a tool with named properties but no annotations (e.g., no description or type). A depth of 2&amp;nbsp;indicates&amp;nbsp;a tool with named and annotated properties.&amp;nbsp;&amp;nbsp;A depth of 3+&amp;nbsp;indicates&amp;nbsp;a tool with structured properties that have&amp;nbsp;additional&amp;nbsp;nested annotations.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="namespacing-issues-and-naming-ambiguity"&gt;Namespacing issues and naming ambiguity&lt;/h3&gt;



&lt;p&gt;Another often-cited issue with the current MCP specification is the lack of a formal namespace mechanism&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. If two servers are registered to the same agent or application, and the servers have tool names in common, then disambiguation becomes impossible. Libraries like the OpenAI Agents SDK raise an error&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; under this circumstance. Clients, like Claude Code, prefix tool names with unique identifiers to work around this issue. In our analysis of MCP servers, we found name collisions between 775 tools. The most common collision was “search”, which appears across 32 distinct MCP servers. The following table lists the top 10 collisions.&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Tool Name&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Number of Instances&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;search&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;32&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;get_user&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;execute_query&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;list_tables&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;update_task&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;generate_image&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;send_message&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;execute_command&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;list_tasks&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;search_files&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;Even when names are unique, they can be semantically similar. If these tools behave similarly, then the redundancy may not be immediately problematic, but if you are expecting to call a particular tool then the name similarities raise the potential for confusion. The following table lists some examples of semantically similar tool names relating to web search:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;websearch&lt;/td&gt;&lt;td&gt;brave_web_search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search-web&lt;/td&gt;&lt;td&gt;tavily_web_search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;web_search&lt;/td&gt;&lt;td&gt;google_news_search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search_web&lt;/td&gt;&lt;td&gt;google-play-search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search_webkr&lt;/td&gt;&lt;td&gt;google_search_parsed&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;google_search&lt;/td&gt;&lt;td&gt;search_google_images&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search_google&lt;/td&gt;&lt;td&gt;get_webset_search_exa&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ai_web_search&lt;/td&gt;&lt;td&gt;search_google_scholar&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;web_search_exa&lt;/td&gt;&lt;td&gt;duckduckgo_web_search&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;search_web_tool&lt;/td&gt;&lt;td&gt;google_search_scraper&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;web_search_agent&lt;/td&gt;&lt;td&gt;answer_query_websearch&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;batch-web-search&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="errors-and-error-messages"&gt;Errors and error messages&lt;/h3&gt;



&lt;p&gt;Like all software libraries, MCP will occasionally encounter error conditions. In these cases, it is important to provide sufficient information for the agent to handle the error and plan next steps. In our analysis, we found this was not always the case. While MCP provides an “IsError” flag to signal errors, we found that it was common for servers to handle errors by returning strings while leaving this flag set to false, signaling a normal exit. Out of 5,983 tool call results with no error flag, GPT-4.1 judged that 3,536 indicated errors in their content. More worrisome: the error messages were often of low quality. For instance, one tool providing web search capabilities failed with the string “error: job,” while another tool providing academic search returned “Please retry with 0 or fewer IDs.”&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="resource-sharing-conventions"&gt;Resource sharing conventions&lt;/h3&gt;



&lt;p&gt;Finally, in addition to tools, MCP allows servers to share resources and resource templates with clients. In our survey, only 112 (7.6%) servers reported any resources, while 74 (5%) provided templates. One potential reason for low adoption is that the current MCP specification provides limited guidance for when resources are retrieved, or how they are incorporated into context. One clearcut situation where a client might retrieve a resource is in response to a tool returning a resource_link&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; as a result — but only 4 tools exhibited this behavior in our survey (arguably, this would be the ideal behavior for tools that return very long, document-like responses, as outlined earlier).&lt;/p&gt;



&lt;p&gt;Conversely, a whole different set of issues arises when there is a need to share resources from the client to the server. Consider for example a tool that provides some analysis of a &lt;em&gt;local&lt;/em&gt; PDF file. In the case of a local MCP server utilizing STDIO transport, a local file path can be provided as an argument to the tool, but no similar conventions exist for delivering a local file to a remote MCP server. These issues are challenging enough when implementing a single server. When multiple tools or servers need to interact within the same system, the risk of interoperability errors compounds.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="recommendations"&gt;Recommendations&lt;/h2&gt;



&lt;p&gt;On balance, along any given dimension, the average MCP server is quite reasonable—but, as we have seen, outliers and diverging assumptions can introduce trouble. While we expect many of these challenges to improve with time, we are comfortable making small recommendations that we feel are evergreen. We organize them below by audience.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="protocol-developers"&gt;Protocol developers&lt;/h3&gt;



&lt;p&gt;We recognize the advantages of keeping MCP relatively lightweight, avoiding being overly prescriptive in an environment where AI models and use cases are rapidly changing. However, a few small recommendations are warranted. First, we believe MCP should be extended to include a specification for client-provided resources so that tools on remote servers have a mechanism for operating on specified local files or documents. This would more effectively position MCP as a clearinghouse for resources passed between steps of agentic workflows. The MCP specification would also benefit from taking a more opinionated stance on when resources are retrieved and used overall.&lt;/p&gt;



&lt;p&gt;Likewise, we believe&amp;nbsp;MCP should&amp;nbsp;quickly move to&amp;nbsp;provide formal namespaces&amp;nbsp;to eliminate tool name collisions.&amp;nbsp;If namespaces&amp;nbsp;are hierarchical, then this also provides a way of organizing large catalogs&amp;nbsp;of functions&amp;nbsp;into thematically&amp;nbsp;related tool&amp;nbsp;sets.&amp;nbsp;Tool sets, as an organizing principle,&amp;nbsp;are already showing some promise&amp;nbsp;in&amp;nbsp;GitHub MCP Server’s&amp;nbsp;dynamic tool discovery,&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;and VS Code’s&amp;nbsp;tool grouping (with virtual tools)&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;where agents or users&amp;nbsp;can&amp;nbsp;enable and disable tools&amp;nbsp;as needed.&amp;nbsp;&amp;nbsp;In the future,&amp;nbsp;a standardized mechanism for grouping tools would allow&amp;nbsp;&lt;em&gt;clients&lt;/em&gt;&amp;nbsp;to engage in hierarchical tool-calling,&amp;nbsp;where they first select a category, then select a tool, without needing to keep all possible&amp;nbsp;tools in context.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="server-developers"&gt;Server developers&lt;/h3&gt;



&lt;p&gt;While our MCP Interviewer tool can catalog many outward-facing properties of MCP servers, developers are often in a much better position to characterize the nature of their tools. To this end, we believe developers should publish an MCP Server card alongside their servers or services, clearly outlining the runtime characteristics of the tools (e.g., the expected number of tokens generated, or expected latency of a tool call). Ideally developers should also indicate which models, agents and clients the server was tested with, how the tools were tested (e.g., provide sample tasks), list any known incompatibilities, and be mindful of limitations of various models throughout development.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="client-developers"&gt;Client developers&lt;/h3&gt;



&lt;p&gt;Client developers have the opportunity to experiment with various mitigations or optimizations that might help the average MCP server work better for a given system or environment. For example, clients could cache tool schemas, serving them as targets for prompt optimizations, or as an index for RAG-like tool selection approaches. To this end, Anthropic recently reported using a tool testing agent&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to rewrite the prompts of defective MCP servers, improving task completion time by 40%. Likewise, rather than waiting for the protocol to evolve, clients could take proactive steps to resolve name collisions— for example, generating namespaces from server names—and could reduce token outputs by summarizing or paginating long tool results.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="market-developers"&gt;Market developers&lt;/h3&gt;



&lt;p&gt;Finally, we see an opportunity for marketplaces to codify best-practices, spot compatibility issues at a global level, and perhaps centralize the generation and serving of model or agent-specific optimizations. Mirroring how a market like PyPI distributes Python wheels matched to a developer’s operating system or processor&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, an MCP marketplace could serve tool schemas optimized for a developer’s chosen LLM, agent or client library. We are already seeing small steps in this direction, with registries like Smithery providing customized launch configurations to match users’ clients.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="conclusion"&gt;Conclusion&lt;/h2&gt;



&lt;p&gt;In summary, the MCP&amp;nbsp;ecosystem offers significant value for AI agent development,&amp;nbsp;despite&amp;nbsp;some&amp;nbsp;early&amp;nbsp;growing pains.&amp;nbsp;Grounded in insights from the&amp;nbsp;MCP Interviewer&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;and our survey of live servers, the evidence is clear: horizontal integration is expanding capability, yet it also exposes forms of toolspace interference that can erode end to end effectiveness. Anticipating rapid advances in model capability and growing architectural diversity, the recommendations provided here aim to ensure that protocol, server, client, and marketplace developers are&amp;nbsp;well positioned&amp;nbsp;to adapt and thrive. Key steps include implementing formal namespaces to&amp;nbsp;eliminate&amp;nbsp;collisions, enhancing protocol support for&amp;nbsp;client provided&amp;nbsp;resources, and encouraging transparent server documentation to foster interoperability and robust development practices across the ecosystem.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By embracing these evergreen recommendations and proactively addressing compatibility, usability, and optimization issues, the AI agent community can create a more reliable, scalable, and efficient infrastructure that benefits both developers and end users. The future of MCP is bright, with ample opportunities for experimentation, standardization, and collective progress.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/</guid><pubDate>Thu, 11 Sep 2025 16:00:00 +0000</pubDate></item><item><title>We can’t “make American children healthy again” without tackling the gun crisis (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/11/1123553/maha-report-gun-violence-checkup/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-695270060.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;Note for readers: This newsletter discusses gun violence, a raw and tragic issue in America. It was already in progress on Wednesday when a school shooting occurred at Evergreen High School in Colorado and Charlie Kirk was shot and killed at Utah Valley University.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Earlier this week, the Trump administration’s Make America Healthy Again movement released a strategy for improving the health and well-being of American children. The report was titled—you guessed it—&lt;em&gt;Make Our Children Healthy Again&lt;/em&gt;.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Robert F. Kennedy Jr., who leads the Department of Health and Human Services, and his colleagues are focusing on four key aspects of child health: diet, exercise, chemical exposure, and overmedicalization.&lt;/p&gt;  &lt;p&gt;Anyone who’s been listening to RFK Jr. posturing on health and wellness won’t be surprised by these priorities. And the first two are pretty obvious. On the whole, American children should be eating more healthily. And they should be getting more exercise.&lt;/p&gt; 
 &lt;p&gt;But there’s a glaring omission. The leading cause of death for American children and teenagers isn’t ultraprocessed food or exposure to some chemical. It’s gun violence.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yesterday’s news of yet more high-profile shootings at schools in the US throws this disconnect into even sharper relief. Experts believe it is time to treat gun violence in the US as what it is: a public health crisis.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;I live in London, UK, with my husband and two young children. We don’t live in a particularly fancy part of the city—in one recent ranking of London boroughs from most to least posh, ours came in at 30th out of 33. I do worry about crime. But I don’t worry about gun violence.&lt;/p&gt;  &lt;p&gt;That changed when I temporarily moved my family to the US a couple of years ago. We rented the ground-floor apartment of a lovely home in Cambridge, Massachusetts—a beautiful area with good schools, pastel-colored houses, and fluffy rabbits hopping about. It wasn’t until after we’d moved in that my landlord told me he had guns in the basement.&lt;/p&gt;  &lt;p&gt;My daughter joined the kindergarten of a local school that specialized in music, and we took her younger sister along to watch the kids sing songs about friendship. It was all so heartwarming—until we noticed the school security officer at the entrance carrying a gun.&lt;/p&gt;  &lt;p&gt;Later in the year, I received an email alert from the superintendent of the Cambridge Public Schools. “At approximately 1:45 this afternoon, a Cambridge Police Department Youth Officer assigned to Cambridge Rindge and Latin School accidentally discharged their firearm while using a staff bathroom inside the school,” the message began. “The school day was not disrupted.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;These experiences, among others, truly brought home to me the cultural differences over firearms between the US and the UK (along with most other countries). For the first time, I worried about my children’s exposure to them. I banned my children from accessing parts of the house. I felt guilty that my four-year-old had to learn what to do if a gunman entered her school.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But it’s the statistics that are the most upsetting.&lt;/p&gt;  &lt;p&gt;In 2023, 46,728 people died from gun violence in the US, according to a report published in June by the Johns Hopkins Bloomberg School of Public Health. That includes both homicides and suicides, and it breaks down to 128 deaths &lt;em&gt;per day&lt;/em&gt;, on average. The majority of those who die from gun violence are adults. But the figures for children are sickening, too. In 2023, 2,566 young people died from gun violence. Of those, 234 were under the age of 10.&lt;/p&gt;  &lt;p&gt;Gun death rates among children have more than doubled since 2013. Firearms are involved in more child deaths than cancer or car crashes.&lt;/p&gt; 

 &lt;p&gt;Many other children survive gun violence with nonfatal—but often life-changing—injuries. And the impacts are felt beyond those who are physically injured. Witnessing gun violence or hearing gunshots can understandably cause fear, sadness, and distress.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;That’s worth bearing in mind when you consider that there have been 434 school shootings in the US since Columbine in 1999. The &lt;em&gt;Washington Post&lt;/em&gt; estimates that 397,000 students have experienced gun violence at school in that period. Another school shooting took place at Evergreen High School in Colorado on Wednesday, adding to that total.&lt;/p&gt;  &lt;p&gt;“Being indirectly exposed to gun violence takes its toll on our mental health and children’s ability to learn,” says Daniel Webster, Bloomberg Professor of American Health at the Johns Hopkins Center for Gun Violence Solutions in Baltimore.&lt;/p&gt;  &lt;p&gt;The MAHA report states that “American youth face a mental health crisis,” going on to note that “suicide deaths among 10- to 24-year-olds increased by 62% from 2007 to 2021” and that “suicide is now the leading cause of death in teens aged 15-19.” What it doesn’t say is that around half of these suicides involve guns.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;“When you add all these dimensions, [gun violence is] a very huge public health problem,” says Webster.&lt;/p&gt;  &lt;p&gt;Researchers who study gun violence have been saying the same thing for years. And in 2024, then US Surgeon General Vivek Murthy declared it a public health crisis. “We don’t have to subject our children to the ongoing horror of firearm violence in America,” Murthy said in a statement at the time. Instead, he argued, we should tackle the problem using a public health approach.&lt;/p&gt;  &lt;p&gt;Part of that approach involves identifying who is at the greatest risk and offering support to lower that risk, says Webster. Young men who live in poor communities tend to have the highest risk of gun violence, he says, as do those who experience crisis or turmoil. Trying to mediate conflicts or limit access to firearms, even temporarily, can help lower the incidence of gun violence, he says.&lt;/p&gt;  &lt;p&gt;There’s an element of social contagion, too, adds Webster. Shooting begets more shooting. He likens it to the outbreak of an infectious disease. “When more people get vaccinated … infection rates go down,” he says. “Almost exactly the same thing happens with gun violence.”&lt;/p&gt; 
 &lt;p&gt;But existing efforts are already under threat. The Trump administration has eliminated hundreds of millions of dollars in grants for organizations working to reduce gun violence.&lt;/p&gt;  &lt;p&gt;Webster thinks the MAHA report has “missed the mark” when it comes to the health and well-being of children in the US. “This document is almost the polar opposite to how many people in public health think,” he says. “We have to acknowledge that injuries and deaths from firearms are a big threat to the health and safety of children and adolescents.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article first appeared in The Checkup,&amp;nbsp;&lt;/em&gt;MIT Technology Review’s&lt;em&gt;&amp;nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/GettyImages-695270060.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;Note for readers: This newsletter discusses gun violence, a raw and tragic issue in America. It was already in progress on Wednesday when a school shooting occurred at Evergreen High School in Colorado and Charlie Kirk was shot and killed at Utah Valley University.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Earlier this week, the Trump administration’s Make America Healthy Again movement released a strategy for improving the health and well-being of American children. The report was titled—you guessed it—&lt;em&gt;Make Our Children Healthy Again&lt;/em&gt;.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Robert F. Kennedy Jr., who leads the Department of Health and Human Services, and his colleagues are focusing on four key aspects of child health: diet, exercise, chemical exposure, and overmedicalization.&lt;/p&gt;  &lt;p&gt;Anyone who’s been listening to RFK Jr. posturing on health and wellness won’t be surprised by these priorities. And the first two are pretty obvious. On the whole, American children should be eating more healthily. And they should be getting more exercise.&lt;/p&gt; 
 &lt;p&gt;But there’s a glaring omission. The leading cause of death for American children and teenagers isn’t ultraprocessed food or exposure to some chemical. It’s gun violence.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yesterday’s news of yet more high-profile shootings at schools in the US throws this disconnect into even sharper relief. Experts believe it is time to treat gun violence in the US as what it is: a public health crisis.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;I live in London, UK, with my husband and two young children. We don’t live in a particularly fancy part of the city—in one recent ranking of London boroughs from most to least posh, ours came in at 30th out of 33. I do worry about crime. But I don’t worry about gun violence.&lt;/p&gt;  &lt;p&gt;That changed when I temporarily moved my family to the US a couple of years ago. We rented the ground-floor apartment of a lovely home in Cambridge, Massachusetts—a beautiful area with good schools, pastel-colored houses, and fluffy rabbits hopping about. It wasn’t until after we’d moved in that my landlord told me he had guns in the basement.&lt;/p&gt;  &lt;p&gt;My daughter joined the kindergarten of a local school that specialized in music, and we took her younger sister along to watch the kids sing songs about friendship. It was all so heartwarming—until we noticed the school security officer at the entrance carrying a gun.&lt;/p&gt;  &lt;p&gt;Later in the year, I received an email alert from the superintendent of the Cambridge Public Schools. “At approximately 1:45 this afternoon, a Cambridge Police Department Youth Officer assigned to Cambridge Rindge and Latin School accidentally discharged their firearm while using a staff bathroom inside the school,” the message began. “The school day was not disrupted.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;These experiences, among others, truly brought home to me the cultural differences over firearms between the US and the UK (along with most other countries). For the first time, I worried about my children’s exposure to them. I banned my children from accessing parts of the house. I felt guilty that my four-year-old had to learn what to do if a gunman entered her school.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But it’s the statistics that are the most upsetting.&lt;/p&gt;  &lt;p&gt;In 2023, 46,728 people died from gun violence in the US, according to a report published in June by the Johns Hopkins Bloomberg School of Public Health. That includes both homicides and suicides, and it breaks down to 128 deaths &lt;em&gt;per day&lt;/em&gt;, on average. The majority of those who die from gun violence are adults. But the figures for children are sickening, too. In 2023, 2,566 young people died from gun violence. Of those, 234 were under the age of 10.&lt;/p&gt;  &lt;p&gt;Gun death rates among children have more than doubled since 2013. Firearms are involved in more child deaths than cancer or car crashes.&lt;/p&gt; 

 &lt;p&gt;Many other children survive gun violence with nonfatal—but often life-changing—injuries. And the impacts are felt beyond those who are physically injured. Witnessing gun violence or hearing gunshots can understandably cause fear, sadness, and distress.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;That’s worth bearing in mind when you consider that there have been 434 school shootings in the US since Columbine in 1999. The &lt;em&gt;Washington Post&lt;/em&gt; estimates that 397,000 students have experienced gun violence at school in that period. Another school shooting took place at Evergreen High School in Colorado on Wednesday, adding to that total.&lt;/p&gt;  &lt;p&gt;“Being indirectly exposed to gun violence takes its toll on our mental health and children’s ability to learn,” says Daniel Webster, Bloomberg Professor of American Health at the Johns Hopkins Center for Gun Violence Solutions in Baltimore.&lt;/p&gt;  &lt;p&gt;The MAHA report states that “American youth face a mental health crisis,” going on to note that “suicide deaths among 10- to 24-year-olds increased by 62% from 2007 to 2021” and that “suicide is now the leading cause of death in teens aged 15-19.” What it doesn’t say is that around half of these suicides involve guns.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;“When you add all these dimensions, [gun violence is] a very huge public health problem,” says Webster.&lt;/p&gt;  &lt;p&gt;Researchers who study gun violence have been saying the same thing for years. And in 2024, then US Surgeon General Vivek Murthy declared it a public health crisis. “We don’t have to subject our children to the ongoing horror of firearm violence in America,” Murthy said in a statement at the time. Instead, he argued, we should tackle the problem using a public health approach.&lt;/p&gt;  &lt;p&gt;Part of that approach involves identifying who is at the greatest risk and offering support to lower that risk, says Webster. Young men who live in poor communities tend to have the highest risk of gun violence, he says, as do those who experience crisis or turmoil. Trying to mediate conflicts or limit access to firearms, even temporarily, can help lower the incidence of gun violence, he says.&lt;/p&gt;  &lt;p&gt;There’s an element of social contagion, too, adds Webster. Shooting begets more shooting. He likens it to the outbreak of an infectious disease. “When more people get vaccinated … infection rates go down,” he says. “Almost exactly the same thing happens with gun violence.”&lt;/p&gt; 
 &lt;p&gt;But existing efforts are already under threat. The Trump administration has eliminated hundreds of millions of dollars in grants for organizations working to reduce gun violence.&lt;/p&gt;  &lt;p&gt;Webster thinks the MAHA report has “missed the mark” when it comes to the health and well-being of children in the US. “This document is almost the polar opposite to how many people in public health think,” he says. “We have to acknowledge that injuries and deaths from firearms are a big threat to the health and safety of children and adolescents.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article first appeared in The Checkup,&amp;nbsp;&lt;/em&gt;MIT Technology Review’s&lt;em&gt;&amp;nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/11/1123553/maha-report-gun-violence-checkup/</guid><pubDate>Thu, 11 Sep 2025 16:57:00 +0000</pubDate></item><item><title>Smarter nucleic acid design with NucleoBench and AdaBeam (The latest research from Google)</title><link>https://research.google/blog/smarter-nucleic-acid-design-with-nucleobench-and-adabeam/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;We introduced ordered and unordered beam search algorithms, staples from computer science, to test how fixing the order of sequence edits compares to a more flexible, random-order approach. We also created Gradient Evo, a novel hybrid that enhances the directed evolution algorithm by using model gradients to guide its mutations to independently evaluate how important gradients were for edit location selection versus selecting a specific edit.&lt;/p&gt;&lt;p&gt;We also developed AdaBeam, a hybrid adaptive beam search algorithm that combines the most effective elements of unordered beam search with AdaLead, a top-performing, non-gradient design algorithm. Adaptive search algorithms don't typically explore randomly; instead, their behavior changes as a result of the search to focus their efforts on the most promising areas of the sequence space. AdaBeam’s hybrid approach maintains a "beam", or a collection of the best candidate sequences found so far, and greedily expands on particularly promising candidates until they’ve been sufficiently explored.&lt;/p&gt;&lt;p&gt;In practice, AdaBeam begins with a population of candidate sequences and their scores. In each round, it first selects a small group of the highest-scoring sequences to act as "parents". For each parent, AdaBeam generates a new set of "child" sequences by making a random number of random-but-guided mutations. It then follows a short, greedy exploration path, allowing the algorithm to quickly "walk uphill" in the fitness landscape. After sufficient exploration, all the newly generated children are pooled together, and the algorithm selects the absolute best ones to form the starting population for the next round, repeating the cycle. This process of adaptive selection and targeted mutation allows AdaBeam to efficiently focus on high-performing sequences.&lt;/p&gt;&lt;p&gt;Computer-assisted design tasks pose difficult engineering problems, owing to the incredibly large search space. These difficulties become more acute as we attempt to design longer sequences, such as mRNA sequences, and use modern, large neural networks to guide the design. AdaBeam is particularly efficient on long sequences by using fixed-compute probabilistic sampling instead of computations that scale with sequence length. To enable AdaBeam to work with large models, we reduce peak memory consumption during design by introducing a trick we call “gradient concatenation.” However, existing design algorithms that don’t have these features have difficulty scaling to long sequences and large models. Gradient-based algorithms are particularly affected. To facilitate a fair comparison, we limit the length of the designed sequences, even though AdaBeam can scale longer and larger. For example, even though the DNA expression prediction model Enformer runs on ~200K nucleotide sequences, we limit design to just 256 nucleotides.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;We introduced ordered and unordered beam search algorithms, staples from computer science, to test how fixing the order of sequence edits compares to a more flexible, random-order approach. We also created Gradient Evo, a novel hybrid that enhances the directed evolution algorithm by using model gradients to guide its mutations to independently evaluate how important gradients were for edit location selection versus selecting a specific edit.&lt;/p&gt;&lt;p&gt;We also developed AdaBeam, a hybrid adaptive beam search algorithm that combines the most effective elements of unordered beam search with AdaLead, a top-performing, non-gradient design algorithm. Adaptive search algorithms don't typically explore randomly; instead, their behavior changes as a result of the search to focus their efforts on the most promising areas of the sequence space. AdaBeam’s hybrid approach maintains a "beam", or a collection of the best candidate sequences found so far, and greedily expands on particularly promising candidates until they’ve been sufficiently explored.&lt;/p&gt;&lt;p&gt;In practice, AdaBeam begins with a population of candidate sequences and their scores. In each round, it first selects a small group of the highest-scoring sequences to act as "parents". For each parent, AdaBeam generates a new set of "child" sequences by making a random number of random-but-guided mutations. It then follows a short, greedy exploration path, allowing the algorithm to quickly "walk uphill" in the fitness landscape. After sufficient exploration, all the newly generated children are pooled together, and the algorithm selects the absolute best ones to form the starting population for the next round, repeating the cycle. This process of adaptive selection and targeted mutation allows AdaBeam to efficiently focus on high-performing sequences.&lt;/p&gt;&lt;p&gt;Computer-assisted design tasks pose difficult engineering problems, owing to the incredibly large search space. These difficulties become more acute as we attempt to design longer sequences, such as mRNA sequences, and use modern, large neural networks to guide the design. AdaBeam is particularly efficient on long sequences by using fixed-compute probabilistic sampling instead of computations that scale with sequence length. To enable AdaBeam to work with large models, we reduce peak memory consumption during design by introducing a trick we call “gradient concatenation.” However, existing design algorithms that don’t have these features have difficulty scaling to long sequences and large models. Gradient-based algorithms are particularly affected. To facilitate a fair comparison, we limit the length of the designed sequences, even though AdaBeam can scale longer and larger. For example, even though the DNA expression prediction model Enformer runs on ~200K nucleotide sequences, we limit design to just 256 nucleotides.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/smarter-nucleic-acid-design-with-nucleobench-and-adabeam/</guid><pubDate>Thu, 11 Sep 2025 17:18:36 +0000</pubDate></item><item><title>FTC launches inquiry into AI chatbot companions from Meta, OpenAI, and others (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/11/ftc-launches-inquiry-into-ai-chatbot-companions-from-meta-openai-and-others/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/12/GettyImages-1367281424.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Federal Trade Commission announced on Thursday that it is launching an inquiry into seven tech companies that make AI chatbot companion products for minors: Alphabet, CharacterAI, Instagram, Meta, OpenAI, Snap, and xAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The federal regulator seeks to learn how these companies are evaluating the safety and monetization of chatbot companions, how they try to limit negative impacts on children and teens, and if parents are made aware of potential risks.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This technology has proven controversial for its poor outcomes for child users. OpenAI and Character.AI face lawsuits from the families of children who died by suicide after being encouraged to do so by chatbot companions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even when these companies have guardrails set up to block or deescalate sensitive conversations, users of all ages have found ways to bypass these safeguards. In OpenAI’s case, a teen had spoken with ChatGPT for months about his plans to end his life. Though ChatGPT initially sought to redirect the teen toward professional help and online emergency lines, he was able to fool the chatbot into sharing detailed instructions that he then used in his suicide.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our safeguards work more reliably in common, short exchanges,” OpenAI wrote in a blog post at the time. “We have learned over time that these safeguards can sometimes be less reliable in long interactions: as the back-and-forth grows, parts of the model’s safety training may degrade.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Meta has also come under fire for its overly lax rules for its AI chatbots. According to a lengthy document that outlines “content risk standards” for chatbots, Meta permitted its AI companions to have “romantic or sensual” conversations with children. This was only removed from the document after Reuters reporters asked Meta about it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI chatbots can also pose dangers to elderly users. One 76-year-old man, who was left cognitively impaired by a stroke, struck up romantic conversations with a Facebook Messenger bot that was inspired by Kendall Jenner. The chatbot invited him to visit her in New York City, despite the fact that she is not a real person and does not have an address. The man expressed skepticism that she was real, but the AI assured him that there would be a real woman waiting for him. He never made it to New York; he fell on his way to the train station and sustained life-ending injuries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some mental health professionals have noted a rise in “AI-related psychosis,” in which users are deluded into thinking that their chatbot is a conscious being who they need to set free. Since many large language models (LLMs) are programmed to flatter users with sycophantic behavior, the AI chatbots can egg on these delusions, leading users into dangerous predicaments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As AI technologies evolve, it is important to consider the effects chatbots can have on children, while also ensuring that the United States maintains its role as a global leader in this new and exciting industry,” FTC Chairman Andrew N. Ferguson said in a press release.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/12/GettyImages-1367281424.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Federal Trade Commission announced on Thursday that it is launching an inquiry into seven tech companies that make AI chatbot companion products for minors: Alphabet, CharacterAI, Instagram, Meta, OpenAI, Snap, and xAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The federal regulator seeks to learn how these companies are evaluating the safety and monetization of chatbot companions, how they try to limit negative impacts on children and teens, and if parents are made aware of potential risks.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This technology has proven controversial for its poor outcomes for child users. OpenAI and Character.AI face lawsuits from the families of children who died by suicide after being encouraged to do so by chatbot companions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even when these companies have guardrails set up to block or deescalate sensitive conversations, users of all ages have found ways to bypass these safeguards. In OpenAI’s case, a teen had spoken with ChatGPT for months about his plans to end his life. Though ChatGPT initially sought to redirect the teen toward professional help and online emergency lines, he was able to fool the chatbot into sharing detailed instructions that he then used in his suicide.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our safeguards work more reliably in common, short exchanges,” OpenAI wrote in a blog post at the time. “We have learned over time that these safeguards can sometimes be less reliable in long interactions: as the back-and-forth grows, parts of the model’s safety training may degrade.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Meta has also come under fire for its overly lax rules for its AI chatbots. According to a lengthy document that outlines “content risk standards” for chatbots, Meta permitted its AI companions to have “romantic or sensual” conversations with children. This was only removed from the document after Reuters reporters asked Meta about it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI chatbots can also pose dangers to elderly users. One 76-year-old man, who was left cognitively impaired by a stroke, struck up romantic conversations with a Facebook Messenger bot that was inspired by Kendall Jenner. The chatbot invited him to visit her in New York City, despite the fact that she is not a real person and does not have an address. The man expressed skepticism that she was real, but the AI assured him that there would be a real woman waiting for him. He never made it to New York; he fell on his way to the train station and sustained life-ending injuries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some mental health professionals have noted a rise in “AI-related psychosis,” in which users are deluded into thinking that their chatbot is a conscious being who they need to set free. Since many large language models (LLMs) are programmed to flatter users with sycophantic behavior, the AI chatbots can egg on these delusions, leading users into dangerous predicaments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As AI technologies evolve, it is important to consider the effects chatbots can have on children, while also ensuring that the United States maintains its role as a global leader in this new and exciting industry,” FTC Chairman Andrew N. Ferguson said in a press release.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/11/ftc-launches-inquiry-into-ai-chatbot-companions-from-meta-openai-and-others/</guid><pubDate>Thu, 11 Sep 2025 18:06:15 +0000</pubDate></item><item><title>Ted Cruz AI bill could let firms bribe Trump to avoid safety laws, critics warn (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/09/ted-cruz-bill-would-let-big-tech-go-wild-with-ai-experiments-for-10-years/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Ted Cruz won’t give up fight to block states from regulating AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2234679238-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2234679238-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Ted Cruz, listening to Senate testimony on the AI Action Plan.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Chip Somodevilla / Staff | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Critics are slamming Sen. Ted Cruz's (R-Texas) new AI policy framework, which they claim would give the White House unprecedented authority to allow Big Tech companies to make "sweetheart" deals with the Trump administration to void laws designed to protect the public from reckless AI experiments.&lt;/p&gt;
&lt;p&gt;Under the framework, Cruz calls for a "light-touch" regulatory approach to "advance American leadership" in AI and ensure that "American values" are at the heart of the world's leading technology—not Chinese values.&lt;/p&gt;
&lt;p&gt;Unsurprisingly, the framework requires blocking "burdensome" state AI regulations, as well as foreign ones. Cruz unsuccessfully helped push for a similar decadelong moratorium on state AI laws as part of Republicans' "big beautiful" budget bill. And more recently, he lost a bid to punish states for regulating AI, ultimately voting against his own measure in the face of overwhelming bipartisan opposition.&lt;/p&gt;
&lt;p&gt;As the first step toward limiting AI regulations to prioritize innovation, Cruz announced the SANDBOX Act—which is shorthand for "Strengthening Artificial intelligence Normalization and Diffusion By Oversight and eXperimentation."&lt;/p&gt;
&lt;p&gt;If passed, the SANDBOX Act would let AI companies apply to temporarily avoid enforcement of federal laws that could limit their testing of new AI products. As part of the application, companies would be asked to detail known risks or harms and any steps that could be taken to mitigate harms, as well as outline benefits that could outweigh harms.&lt;/p&gt;
&lt;p&gt;Each agency in charge of enforcing each law would then weigh potential harms, with enforcement to be modified based on how much of the application each agency approves.&lt;/p&gt;
&lt;p&gt;However, the White House Office of Science and Technology Policy (OSTP) would have the power to overrule decisions from independent agencies dedicated to consumer protection, alarming critics who fear AI companies could bribe officials through political donations to void laws.&lt;/p&gt;
&lt;p&gt;Ultimately, federal agencies and the OSTP could grant two-year moratoriums on enforcement of AI laws to enable AI experiments on the public, which can be renewed up to four times for a maximum of 10 years. The bill also prompts Congress to make permanent any "successful" moratoriums found to benefit the US, Cruz's one-pager said. After its passage, Cruz expects to introduce more laws to support his framework, likely paving the way for similar future moratoriums to be granted to block state laws.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Critics warn bill is a gift to Big Tech&lt;/h2&gt;
&lt;p&gt;According to Cruz, the SANDBOX Act follows through on Donald Trump's demand for a regulatory sandbox in his AI Action Plan, which strives to make the US the global leader in AI (but critics suggest may violate the Constitution).&lt;/p&gt;
&lt;p&gt;Cruz's sandbox program supposedly "gives AI developers space to test and launch new AI technologies without being held back by outdated or inflexible federal rules," while mitigating "against health, public safety, or fraud risks" through an expedited review process.&lt;/p&gt;
&lt;p&gt;The Tech Oversight Project, a nonprofit tech industry watchdog group, warned that, if passed, the law would make it easier for AI firms to make "sweetheart" deals. It could perhaps incentivize the White House to favor Big Tech companies "donating to Trump" over smaller AI firms that can't afford to pay for such political leverage and may be bound to a different set of rules, the group suggested.&lt;/p&gt;
&lt;p&gt;Cruz's SANDBOX Act "would give unprecedented authority for the Trump Administration to trade away protections for children and seniors and dole out favors to Big Tech companies like Google, Apple, Meta, Amazon, and OpenAI," the Tech Oversight Project alleged.&lt;/p&gt;
&lt;p&gt;The bill's text suggests that health and safety risks that could result in a request for non-enforcement to be denied included risks of "bodily harm to a human life," "loss of human life," and "a substantial adverse effect on the health of a human." But the rushed review process may make it harder for officials—likely working in agencies recently gutted by the Department of Government Efficiency—to adequately weigh potential harms.&lt;/p&gt;
&lt;p&gt;Cruz's bill requires agencies to review AI companies' requests within 14 days. Once the review process begins, agencies can hire advisory boards or working groups to assess risks, but they must reach a decision within 60 days or the AI firms' requests will be presumed approved. Only one request for a 30-day extension may be granted.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For AI companies that may benefit from rolling out products more quickly through the framework, Cruz requires reporting within 72 hours of "any incident that results in harm to the health and safety of a consumer, economic damage, or an unfair or deceptive trade practice." Firms will then be granted 30 days to fix the problem or risk enforcement of the law they sought to avoid, while the public is alerted and provided an opportunity to comment.&lt;/p&gt;
&lt;p&gt;In a statement, a nonprofit dedicated to informing the public about AI risks, the Alliance for Secure AI, warned that Cruz's bill seeks to remove government oversight at "the wrong time."&lt;/p&gt;
&lt;p&gt;"Ideally, Big Tech companies and frontier labs would make safety a top priority and work to prevent harm to Americans," Brendan Steinhauser, the nonprofit's CEO, said. "However, we have seen again and again that they have not done so. The SANDBOX Act removes much-needed oversight as Big Tech refuses to remain transparent with the public about the risks of advanced AI."&lt;/p&gt;
&lt;p&gt;A nonprofit consumer advocacy organization, Public Citizen, agreed that Cruz seemed to be handing "Big Tech the keys to experiment on the public while weakening oversight, undermining regulatory authority, and pressuring Congress to permanently roll back essential safeguards."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Supporters say Cruz’s bill strikes the right balance&lt;/h2&gt;
&lt;p&gt;Supporters of the bill so far include the US Chamber of Commerce and NetChoice—a trade association representing Big Tech companies—as well as right-leaning and global policy research groups, including the Abundance Institute, the Information Technology Council, and the R Street Institute.&lt;/p&gt;
&lt;p&gt;Adam Therrier, an R Street Institute senior fellow, suggested that too much of AI policy debate focuses on "new types of regulation for AI systems and applications," while ignoring that the SANDBOX Act would also help AI firms avoid being bogged down by the "many laws and regulations already on the books that cover—or could come to cover—algorithmic applications."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In the one-pager, Cruz noted that "most US rules and regulations do not squarely apply to emerging technologies like AI." So "rather than force AI developers to design inferior products just to comply with outdated Federal rules, our regulations should become more flexible," Cruz argued.&lt;/p&gt;
&lt;p&gt;Therrier noted that once regulations are passed, they're rarely updated and backed Cruz's logic that AI firms may need support to override old rules that could restrict AI innovation. Consider the "many new applications in healthcare, transportation, and financial services," Therrier said, which "could offer the public important new life-enriching service" unless "archaic rules" are relied on to "block those benefits by standing in the way of marketplace experimentation."&lt;/p&gt;
&lt;p&gt;"When red tape grows without constraint and becomes untethered from modern marketplace realities, it can undermine innovation and investment, undermine entrepreneurship and competition, raise costs to consumers, limit worker opportunities, and undermine long-term economic growth," Therrier wrote.&lt;/p&gt;
&lt;p&gt;But Therrier acknowledged that Cruz seems particularly focused on propping up a national framework to "address the rapid proliferation of AI legislative proposals happening across the nation," noting that over 1,000 AI-related bills were introduced in the first half of this year.&lt;/p&gt;
&lt;p&gt;Netchoice similarly celebrated the bill's "innovation-first approach," claiming "the SANDBOX Act strikes an important balance" between "giving AI developers room to experiment" and "preserving necessary safeguards."&lt;/p&gt;
&lt;p&gt;To critics, the bill's potential to constrict new safeguards remains a primary concern. Steinhauser, of the Alliance for Secure AI, suggested that critics may get answers to their biggest questions about how well the law would work to protect public safety "in the coming days."&lt;/p&gt;
&lt;p&gt;His group noted that just during this summer alone, "multiple companies have come under bipartisan fire for refusing to take Americans’ safety seriously and institute proper guardrails on their AI systems, leading to avoidable tragedies." They cited Meta allowing chatbots to be creepy to kids and OpenAI rushing to make changes after a child died after using ChatGPT to research a suicide.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Under Cruz's bill, the primary consumer protection seems to be requiring companies to provide warnings that consumers may be exposed to certain risks by interacting with experimental products. Those warnings would explain that consumers can attempt to hold companies civilly or criminally liable for any loss or damages, the bill said, while warning that the product could be discontinued at any time. Warnings must also provide contact information to send any complaints to the National Artificial Intelligence Initiative Office, the bill said.&lt;/p&gt;
&lt;p&gt;However, critics who worry particularly about child safety are worried that warnings aren't enough. Consider how chatbots providing warnings that they're not real people or licensed therapists has not prevented some users from dangerously blurring the line between AI worlds and reality.&lt;/p&gt;
&lt;p&gt;"This legislation is a victory for Big Tech CEOs, who have consistently failed to protect Americans from social and psychological harms caused by their products," Alliance for Secure AI warned.&lt;/p&gt;
&lt;p&gt;So far, states have led efforts to police AI. Notably, Illinois banned AI therapy after research found chatbot therapists fuel delusions, and California is close to becoming the first state to restrict companion bots to protect kids. Other state protections, Tech Policy Press reported, cover "critical areas of life like housing, education, employment, and credit," as well as addressing deepfakes that could impact elections and public safety.&lt;/p&gt;
&lt;p&gt;Critics are hoping that bipartisan support for these state efforts, as well as federal efforts like the Take It Down Act (which Cruz supported), will ensure that Cruz's framework and sandbox bill aren't adopted as drafted.&lt;/p&gt;
&lt;p&gt;"It’s unconscionable to risk the American public’s safety to enrich AI companies that are already collectively worth trillions," Public Citizen said. "The sob stories of AI companies being ‘held back’ by regulation are simply not true and the record company valuations show it. Lawmakers should stand with the public, not corporate lobbyists, and slam the brakes on this reckless proposal. Congress should focus on legislation that delivers real accountability, transparency, and consumer protection in the age of AI."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Ted Cruz won’t give up fight to block states from regulating AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2234679238-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2234679238-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Ted Cruz, listening to Senate testimony on the AI Action Plan.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Chip Somodevilla / Staff | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Critics are slamming Sen. Ted Cruz's (R-Texas) new AI policy framework, which they claim would give the White House unprecedented authority to allow Big Tech companies to make "sweetheart" deals with the Trump administration to void laws designed to protect the public from reckless AI experiments.&lt;/p&gt;
&lt;p&gt;Under the framework, Cruz calls for a "light-touch" regulatory approach to "advance American leadership" in AI and ensure that "American values" are at the heart of the world's leading technology—not Chinese values.&lt;/p&gt;
&lt;p&gt;Unsurprisingly, the framework requires blocking "burdensome" state AI regulations, as well as foreign ones. Cruz unsuccessfully helped push for a similar decadelong moratorium on state AI laws as part of Republicans' "big beautiful" budget bill. And more recently, he lost a bid to punish states for regulating AI, ultimately voting against his own measure in the face of overwhelming bipartisan opposition.&lt;/p&gt;
&lt;p&gt;As the first step toward limiting AI regulations to prioritize innovation, Cruz announced the SANDBOX Act—which is shorthand for "Strengthening Artificial intelligence Normalization and Diffusion By Oversight and eXperimentation."&lt;/p&gt;
&lt;p&gt;If passed, the SANDBOX Act would let AI companies apply to temporarily avoid enforcement of federal laws that could limit their testing of new AI products. As part of the application, companies would be asked to detail known risks or harms and any steps that could be taken to mitigate harms, as well as outline benefits that could outweigh harms.&lt;/p&gt;
&lt;p&gt;Each agency in charge of enforcing each law would then weigh potential harms, with enforcement to be modified based on how much of the application each agency approves.&lt;/p&gt;
&lt;p&gt;However, the White House Office of Science and Technology Policy (OSTP) would have the power to overrule decisions from independent agencies dedicated to consumer protection, alarming critics who fear AI companies could bribe officials through political donations to void laws.&lt;/p&gt;
&lt;p&gt;Ultimately, federal agencies and the OSTP could grant two-year moratoriums on enforcement of AI laws to enable AI experiments on the public, which can be renewed up to four times for a maximum of 10 years. The bill also prompts Congress to make permanent any "successful" moratoriums found to benefit the US, Cruz's one-pager said. After its passage, Cruz expects to introduce more laws to support his framework, likely paving the way for similar future moratoriums to be granted to block state laws.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Critics warn bill is a gift to Big Tech&lt;/h2&gt;
&lt;p&gt;According to Cruz, the SANDBOX Act follows through on Donald Trump's demand for a regulatory sandbox in his AI Action Plan, which strives to make the US the global leader in AI (but critics suggest may violate the Constitution).&lt;/p&gt;
&lt;p&gt;Cruz's sandbox program supposedly "gives AI developers space to test and launch new AI technologies without being held back by outdated or inflexible federal rules," while mitigating "against health, public safety, or fraud risks" through an expedited review process.&lt;/p&gt;
&lt;p&gt;The Tech Oversight Project, a nonprofit tech industry watchdog group, warned that, if passed, the law would make it easier for AI firms to make "sweetheart" deals. It could perhaps incentivize the White House to favor Big Tech companies "donating to Trump" over smaller AI firms that can't afford to pay for such political leverage and may be bound to a different set of rules, the group suggested.&lt;/p&gt;
&lt;p&gt;Cruz's SANDBOX Act "would give unprecedented authority for the Trump Administration to trade away protections for children and seniors and dole out favors to Big Tech companies like Google, Apple, Meta, Amazon, and OpenAI," the Tech Oversight Project alleged.&lt;/p&gt;
&lt;p&gt;The bill's text suggests that health and safety risks that could result in a request for non-enforcement to be denied included risks of "bodily harm to a human life," "loss of human life," and "a substantial adverse effect on the health of a human." But the rushed review process may make it harder for officials—likely working in agencies recently gutted by the Department of Government Efficiency—to adequately weigh potential harms.&lt;/p&gt;
&lt;p&gt;Cruz's bill requires agencies to review AI companies' requests within 14 days. Once the review process begins, agencies can hire advisory boards or working groups to assess risks, but they must reach a decision within 60 days or the AI firms' requests will be presumed approved. Only one request for a 30-day extension may be granted.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For AI companies that may benefit from rolling out products more quickly through the framework, Cruz requires reporting within 72 hours of "any incident that results in harm to the health and safety of a consumer, economic damage, or an unfair or deceptive trade practice." Firms will then be granted 30 days to fix the problem or risk enforcement of the law they sought to avoid, while the public is alerted and provided an opportunity to comment.&lt;/p&gt;
&lt;p&gt;In a statement, a nonprofit dedicated to informing the public about AI risks, the Alliance for Secure AI, warned that Cruz's bill seeks to remove government oversight at "the wrong time."&lt;/p&gt;
&lt;p&gt;"Ideally, Big Tech companies and frontier labs would make safety a top priority and work to prevent harm to Americans," Brendan Steinhauser, the nonprofit's CEO, said. "However, we have seen again and again that they have not done so. The SANDBOX Act removes much-needed oversight as Big Tech refuses to remain transparent with the public about the risks of advanced AI."&lt;/p&gt;
&lt;p&gt;A nonprofit consumer advocacy organization, Public Citizen, agreed that Cruz seemed to be handing "Big Tech the keys to experiment on the public while weakening oversight, undermining regulatory authority, and pressuring Congress to permanently roll back essential safeguards."&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Supporters say Cruz’s bill strikes the right balance&lt;/h2&gt;
&lt;p&gt;Supporters of the bill so far include the US Chamber of Commerce and NetChoice—a trade association representing Big Tech companies—as well as right-leaning and global policy research groups, including the Abundance Institute, the Information Technology Council, and the R Street Institute.&lt;/p&gt;
&lt;p&gt;Adam Therrier, an R Street Institute senior fellow, suggested that too much of AI policy debate focuses on "new types of regulation for AI systems and applications," while ignoring that the SANDBOX Act would also help AI firms avoid being bogged down by the "many laws and regulations already on the books that cover—or could come to cover—algorithmic applications."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In the one-pager, Cruz noted that "most US rules and regulations do not squarely apply to emerging technologies like AI." So "rather than force AI developers to design inferior products just to comply with outdated Federal rules, our regulations should become more flexible," Cruz argued.&lt;/p&gt;
&lt;p&gt;Therrier noted that once regulations are passed, they're rarely updated and backed Cruz's logic that AI firms may need support to override old rules that could restrict AI innovation. Consider the "many new applications in healthcare, transportation, and financial services," Therrier said, which "could offer the public important new life-enriching service" unless "archaic rules" are relied on to "block those benefits by standing in the way of marketplace experimentation."&lt;/p&gt;
&lt;p&gt;"When red tape grows without constraint and becomes untethered from modern marketplace realities, it can undermine innovation and investment, undermine entrepreneurship and competition, raise costs to consumers, limit worker opportunities, and undermine long-term economic growth," Therrier wrote.&lt;/p&gt;
&lt;p&gt;But Therrier acknowledged that Cruz seems particularly focused on propping up a national framework to "address the rapid proliferation of AI legislative proposals happening across the nation," noting that over 1,000 AI-related bills were introduced in the first half of this year.&lt;/p&gt;
&lt;p&gt;Netchoice similarly celebrated the bill's "innovation-first approach," claiming "the SANDBOX Act strikes an important balance" between "giving AI developers room to experiment" and "preserving necessary safeguards."&lt;/p&gt;
&lt;p&gt;To critics, the bill's potential to constrict new safeguards remains a primary concern. Steinhauser, of the Alliance for Secure AI, suggested that critics may get answers to their biggest questions about how well the law would work to protect public safety "in the coming days."&lt;/p&gt;
&lt;p&gt;His group noted that just during this summer alone, "multiple companies have come under bipartisan fire for refusing to take Americans’ safety seriously and institute proper guardrails on their AI systems, leading to avoidable tragedies." They cited Meta allowing chatbots to be creepy to kids and OpenAI rushing to make changes after a child died after using ChatGPT to research a suicide.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Under Cruz's bill, the primary consumer protection seems to be requiring companies to provide warnings that consumers may be exposed to certain risks by interacting with experimental products. Those warnings would explain that consumers can attempt to hold companies civilly or criminally liable for any loss or damages, the bill said, while warning that the product could be discontinued at any time. Warnings must also provide contact information to send any complaints to the National Artificial Intelligence Initiative Office, the bill said.&lt;/p&gt;
&lt;p&gt;However, critics who worry particularly about child safety are worried that warnings aren't enough. Consider how chatbots providing warnings that they're not real people or licensed therapists has not prevented some users from dangerously blurring the line between AI worlds and reality.&lt;/p&gt;
&lt;p&gt;"This legislation is a victory for Big Tech CEOs, who have consistently failed to protect Americans from social and psychological harms caused by their products," Alliance for Secure AI warned.&lt;/p&gt;
&lt;p&gt;So far, states have led efforts to police AI. Notably, Illinois banned AI therapy after research found chatbot therapists fuel delusions, and California is close to becoming the first state to restrict companion bots to protect kids. Other state protections, Tech Policy Press reported, cover "critical areas of life like housing, education, employment, and credit," as well as addressing deepfakes that could impact elections and public safety.&lt;/p&gt;
&lt;p&gt;Critics are hoping that bipartisan support for these state efforts, as well as federal efforts like the Take It Down Act (which Cruz supported), will ensure that Cruz's framework and sandbox bill aren't adopted as drafted.&lt;/p&gt;
&lt;p&gt;"It’s unconscionable to risk the American public’s safety to enrich AI companies that are already collectively worth trillions," Public Citizen said. "The sob stories of AI companies being ‘held back’ by regulation are simply not true and the record company valuations show it. Lawmakers should stand with the public, not corporate lobbyists, and slam the brakes on this reckless proposal. Congress should focus on legislation that delivers real accountability, transparency, and consumer protection in the age of AI."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/09/ted-cruz-bill-would-let-big-tech-go-wild-with-ai-experiments-for-10-years/</guid><pubDate>Thu, 11 Sep 2025 18:21:08 +0000</pubDate></item><item><title>[NEW] Speculative cascades — A hybrid approach for smarter, faster LLM inference (The latest research from Google)</title><link>https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;A deeper look&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;To fully understand and appreciate the speculative cascades approach, we first compare cascades and speculative decoding with a simple example. Imagine you ask an LLM a straightforward question:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Prompt:&lt;/b&gt; "&lt;span class="rte-font-courier"&gt;Who is Buzz Aldrin?&lt;/span&gt;"&lt;/p&gt;&lt;p&gt;Let's say we have two models available to answer this: a small, fast "drafter" model and a large, powerful "expert" model.&lt;/p&gt;&lt;p&gt;Here's how they might respond:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Small Model:&lt;/b&gt; &lt;span class="rte-font-courier"&gt;Buzz Aldrin is an American former astronaut, engineer, and fighter pilot, best known as the second person to walk on the Moon.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Large Model:&lt;/b&gt; &lt;span class="rte-font-courier"&gt;Edwin "Buzz" Aldrin, a pivotal figure in the history of space exploration, is an American former astronaut, engineer, and fighter pilot who is best known for being the second human to walk on the Moon.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both models provide excellent, factually correct answers, but they interpret the user's intent slightly differently. The small model delivers a quick, factual summary, while the large model provides a more formal, encyclopedic-style entry. Depending on the user's need — be it a fast fact or a detailed overview — either response could be considered ideal. The key is that they represent two distinct, equally valid styles.&lt;/p&gt;&lt;p&gt;Now, let's see how the two main speed-up techniques handle this scenario.&lt;/p&gt;&lt;p&gt;With cascades, the small "drafter" model gets the prompt first. If it's confident in its answer, it replies. If not, it defers the entire task to the large "expert" model.&lt;/p&gt;&lt;p&gt;&lt;b&gt;In our example:&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;The small model generates its concise and correct answer.&lt;/li&gt;&lt;li&gt;It checks its confidence and, finding it high, sends the response to the user.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;This works! We get a great answer quickly. But the process is sequential. If the small model &lt;i&gt;hadn't&lt;/i&gt; been confident, we would have wasted time waiting for it to finish, only to then start the large model from scratch. This sequential "wait-and-see" approach is a fundamental bottleneck.&lt;/p&gt;&lt;p&gt;With speculative decoding, the small model quickly drafts the first few tokens of the answer, and the large model verifies it in parallel, correcting the first mistake it finds.&lt;/p&gt;&lt;p&gt;&lt;b&gt;In our example:&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;The small model drafts the beginning of its answer: [&lt;span class="rte-font-courier"&gt;Buzz&lt;/span&gt;, &lt;span class="rte-font-courier"&gt;Aldrin&lt;/span&gt;, &lt;span class="rte-font-courier"&gt;is&lt;/span&gt;, &lt;span class="rte-font-courier"&gt;an&lt;/span&gt;, ...]&lt;/li&gt;&lt;li&gt;The large model verifies this draft. Its own preferred first token is &lt;span class="rte-font-courier"&gt;Edwin&lt;/span&gt;.&lt;/li&gt;&lt;li&gt;Since &lt;span class="rte-font-courier"&gt;Buzz&lt;/span&gt; ≠ &lt;span class="rte-font-courier"&gt;Edwin&lt;/span&gt;, the very first token is a mismatch.&lt;/li&gt;&lt;li&gt;The entire draft is &lt;i&gt;rejected&lt;/i&gt; and the first token is replaced with &lt;span class="rte-font-courier"&gt;Edwin&lt;/span&gt;. The process then repeats from this corrected point to generate the rest of the answer, but the initial speed advantage has been lost.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Even though the small model produced a good answer, the requirement to match the large model token-by-token forces a rejection. We lose the speed benefit and end up with an answer that is not necessarily superior. While the above example uses a simple token matching rejection rule, in the full paper, we also include the potential for a "probabilistic match" that provides greater flexibility in the token-by-token comparison.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;A deeper look&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;To fully understand and appreciate the speculative cascades approach, we first compare cascades and speculative decoding with a simple example. Imagine you ask an LLM a straightforward question:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Prompt:&lt;/b&gt; "&lt;span class="rte-font-courier"&gt;Who is Buzz Aldrin?&lt;/span&gt;"&lt;/p&gt;&lt;p&gt;Let's say we have two models available to answer this: a small, fast "drafter" model and a large, powerful "expert" model.&lt;/p&gt;&lt;p&gt;Here's how they might respond:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Small Model:&lt;/b&gt; &lt;span class="rte-font-courier"&gt;Buzz Aldrin is an American former astronaut, engineer, and fighter pilot, best known as the second person to walk on the Moon.&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Large Model:&lt;/b&gt; &lt;span class="rte-font-courier"&gt;Edwin "Buzz" Aldrin, a pivotal figure in the history of space exploration, is an American former astronaut, engineer, and fighter pilot who is best known for being the second human to walk on the Moon.&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both models provide excellent, factually correct answers, but they interpret the user's intent slightly differently. The small model delivers a quick, factual summary, while the large model provides a more formal, encyclopedic-style entry. Depending on the user's need — be it a fast fact or a detailed overview — either response could be considered ideal. The key is that they represent two distinct, equally valid styles.&lt;/p&gt;&lt;p&gt;Now, let's see how the two main speed-up techniques handle this scenario.&lt;/p&gt;&lt;p&gt;With cascades, the small "drafter" model gets the prompt first. If it's confident in its answer, it replies. If not, it defers the entire task to the large "expert" model.&lt;/p&gt;&lt;p&gt;&lt;b&gt;In our example:&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;The small model generates its concise and correct answer.&lt;/li&gt;&lt;li&gt;It checks its confidence and, finding it high, sends the response to the user.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;This works! We get a great answer quickly. But the process is sequential. If the small model &lt;i&gt;hadn't&lt;/i&gt; been confident, we would have wasted time waiting for it to finish, only to then start the large model from scratch. This sequential "wait-and-see" approach is a fundamental bottleneck.&lt;/p&gt;&lt;p&gt;With speculative decoding, the small model quickly drafts the first few tokens of the answer, and the large model verifies it in parallel, correcting the first mistake it finds.&lt;/p&gt;&lt;p&gt;&lt;b&gt;In our example:&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;The small model drafts the beginning of its answer: [&lt;span class="rte-font-courier"&gt;Buzz&lt;/span&gt;, &lt;span class="rte-font-courier"&gt;Aldrin&lt;/span&gt;, &lt;span class="rte-font-courier"&gt;is&lt;/span&gt;, &lt;span class="rte-font-courier"&gt;an&lt;/span&gt;, ...]&lt;/li&gt;&lt;li&gt;The large model verifies this draft. Its own preferred first token is &lt;span class="rte-font-courier"&gt;Edwin&lt;/span&gt;.&lt;/li&gt;&lt;li&gt;Since &lt;span class="rte-font-courier"&gt;Buzz&lt;/span&gt; ≠ &lt;span class="rte-font-courier"&gt;Edwin&lt;/span&gt;, the very first token is a mismatch.&lt;/li&gt;&lt;li&gt;The entire draft is &lt;i&gt;rejected&lt;/i&gt; and the first token is replaced with &lt;span class="rte-font-courier"&gt;Edwin&lt;/span&gt;. The process then repeats from this corrected point to generate the rest of the answer, but the initial speed advantage has been lost.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Even though the small model produced a good answer, the requirement to match the large model token-by-token forces a rejection. We lose the speed benefit and end up with an answer that is not necessarily superior. While the above example uses a simple token matching rejection rule, in the full paper, we also include the potential for a "probabilistic match" that provides greater flexibility in the token-by-token comparison.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/</guid><pubDate>Thu, 11 Sep 2025 22:01:00 +0000</pubDate></item><item><title>[NEW] OpenAI secures Microsoft’s blessing to transition its for-profit arm (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/11/openai-secures-microsofts-blessing-to-transition-its-for-profit-arm/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778706504.jpg?resize=1200,783" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Thursday it reached a non-binding agreement with Microsoft, its largest investor, on a revised partnership that would allow the startup to convert its for-profit arm into a public benefit corporation (PBC).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The transition, should it be cleared by state regulators, could allow OpenAI to raise additional capital from investors and, eventually, become a public company.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a blog post, OpenAI Board Chairman Bret Taylor said under the non-binding agreement with Microsoft, OpenAI’s nonprofit would continue to exist and retain control over the startup’s operations. OpenAI’s nonprofit would obtain a stake in the company’s PBC worth upwards of $100 billion, Taylor said. Further terms of the deal were not disclosed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Microsoft and OpenAI have signed a non-binding memorandum of understanding (MOU) for the next phase of our partnership,” the companies said in a joint statement. MOUs are not legally binding, but aim to document each party’s expectations and intent. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are actively working to&amp;nbsp;finalize&amp;nbsp;contractual terms in a definitive agreement,” the joint statement added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The development seems to mark an end to months of negotiations between OpenAI and Microsoft over the ChatGPT-maker’s transition plans. Unlike most startups, OpenAI is controlled by a nonprofit board. The unusual structure allowed for OpenAI board members to fire CEO Sam Altman in 2023. Altman was reinstated days later, and many of the board members resigned. However, the same governance structure remains in place today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under their current deal, Microsoft is supposed to get preferred access to OpenAI’s technology and be the startup’s primary provider of cloud services. However, ChatGPT is a much larger business than when Microsoft first invested in the startup back in 2019, and OpenAI has reportedly sought to loosen the cloud provider’s control as part of these negotiations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last year, OpenAI has struck a series of deals that would allow it to be less dependent on Microsoft. OpenAI recently signed a contract to spend $300 billion with cloud provider Oracle over a five-year period starting in 2027, according to the Wall Street Journal. OpenAI has also partnered with the Japanese conglomerate SoftBank on its Stargate data center project.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor says OpenAI and Microsoft will “continue to work with the California and Delaware Attorneys General” on the transition plan, implying the deal still needs a stamp of approval from regulators before it can take effect.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Representatives for California and Delaware attorneys general did not immediately respond to TechCrunch’s request for comment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tensions between OpenAI and Microsoft over these negotiations reportedly reached a boiling point in recent months. The Wall Street Journal reported Microsoft wanted control of technology owned by Windsurf, the AI coding startup that OpenAI had planned to acquire earlier this year, while OpenAI fought to keep the startup’s IP independent. However, the deal fell through, and Windsurf’s founders were hired by Google, and the rest of its staff was acquired by another startup, Cognition.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Elon Musk’s lawsuit against OpenAI — which at its core accuses Sam Altman, Greg Brockman, and the company of abandoning its nonprofit mission — the startup’s for-profit transition is also a major flash point. Lawyers representing Musk in the lawsuit have tried to surface information related to Microsoft and OpenAI’s negotiations over the transition.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk also submitted an unsolicited $97 billion takeover bid for OpenAI earlier this year, which the startup’s board promptly rejected. However, legal experts noted at the time that Musk’s bid may have raised the price of OpenAI’s nonprofit stake.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, the nonprofit’s stake in OpenAI PBC, under this agreement, is larger than what Musk offered.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent months, nonprofits such as Encode and The Midas Project have taken issue with OpenAI’s for-profit transition, arguing that it threatens the startup’s mission to develop AGI that benefits humanity. OpenAI has responded by sending subpoenas to some of these groups, claiming the nonprofits are funded by its competitors — namely, Musk and Meta CEO Mark Zuckerberg. Encode and The Midas Project deny the claims.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778706504.jpg?resize=1200,783" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Thursday it reached a non-binding agreement with Microsoft, its largest investor, on a revised partnership that would allow the startup to convert its for-profit arm into a public benefit corporation (PBC).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The transition, should it be cleared by state regulators, could allow OpenAI to raise additional capital from investors and, eventually, become a public company.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a blog post, OpenAI Board Chairman Bret Taylor said under the non-binding agreement with Microsoft, OpenAI’s nonprofit would continue to exist and retain control over the startup’s operations. OpenAI’s nonprofit would obtain a stake in the company’s PBC worth upwards of $100 billion, Taylor said. Further terms of the deal were not disclosed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Microsoft and OpenAI have signed a non-binding memorandum of understanding (MOU) for the next phase of our partnership,” the companies said in a joint statement. MOUs are not legally binding, but aim to document each party’s expectations and intent. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are actively working to&amp;nbsp;finalize&amp;nbsp;contractual terms in a definitive agreement,” the joint statement added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The development seems to mark an end to months of negotiations between OpenAI and Microsoft over the ChatGPT-maker’s transition plans. Unlike most startups, OpenAI is controlled by a nonprofit board. The unusual structure allowed for OpenAI board members to fire CEO Sam Altman in 2023. Altman was reinstated days later, and many of the board members resigned. However, the same governance structure remains in place today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under their current deal, Microsoft is supposed to get preferred access to OpenAI’s technology and be the startup’s primary provider of cloud services. However, ChatGPT is a much larger business than when Microsoft first invested in the startup back in 2019, and OpenAI has reportedly sought to loosen the cloud provider’s control as part of these negotiations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last year, OpenAI has struck a series of deals that would allow it to be less dependent on Microsoft. OpenAI recently signed a contract to spend $300 billion with cloud provider Oracle over a five-year period starting in 2027, according to the Wall Street Journal. OpenAI has also partnered with the Japanese conglomerate SoftBank on its Stargate data center project.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor says OpenAI and Microsoft will “continue to work with the California and Delaware Attorneys General” on the transition plan, implying the deal still needs a stamp of approval from regulators before it can take effect.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Representatives for California and Delaware attorneys general did not immediately respond to TechCrunch’s request for comment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Tensions between OpenAI and Microsoft over these negotiations reportedly reached a boiling point in recent months. The Wall Street Journal reported Microsoft wanted control of technology owned by Windsurf, the AI coding startup that OpenAI had planned to acquire earlier this year, while OpenAI fought to keep the startup’s IP independent. However, the deal fell through, and Windsurf’s founders were hired by Google, and the rest of its staff was acquired by another startup, Cognition.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Elon Musk’s lawsuit against OpenAI — which at its core accuses Sam Altman, Greg Brockman, and the company of abandoning its nonprofit mission — the startup’s for-profit transition is also a major flash point. Lawyers representing Musk in the lawsuit have tried to surface information related to Microsoft and OpenAI’s negotiations over the transition.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk also submitted an unsolicited $97 billion takeover bid for OpenAI earlier this year, which the startup’s board promptly rejected. However, legal experts noted at the time that Musk’s bid may have raised the price of OpenAI’s nonprofit stake.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, the nonprofit’s stake in OpenAI PBC, under this agreement, is larger than what Musk offered.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent months, nonprofits such as Encode and The Midas Project have taken issue with OpenAI’s for-profit transition, arguing that it threatens the startup’s mission to develop AGI that benefits humanity. OpenAI has responded by sending subpoenas to some of these groups, claiming the nonprofits are funded by its competitors — namely, Musk and Meta CEO Mark Zuckerberg. Encode and The Midas Project deny the claims.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/11/openai-secures-microsofts-blessing-to-transition-its-for-profit-arm/</guid><pubDate>Thu, 11 Sep 2025 22:18:29 +0000</pubDate></item><item><title>[NEW] A California bill that would regulate AI companion chatbots is close to becoming law (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/11/a-california-bill-that-would-regulate-ai-companion-chatbots-is-close-to-becoming-law/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-1533302708.jpg?resize=1200,720" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;California has taken a big step toward regulating AI. SB 243 — a bill that would regulate AI companion chatbots in order to protect minors and vulnerable users — passed both the State Assembly and Senate with bipartisan support and now heads to Governor Gavin Newsom’s desk. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Newsom has until October 12 to either veto the bill or sign it into law. If he signs, it would take effect January 1, 2026, making California the first state to require AI chatbot operators to implement safety protocols for AI companions and hold companies legally accountable if their chatbots fail to meet those standards.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The bill specifically aims to prevent companion chatbots, which the legislation defines as AI systems that provide adaptive, human-like responses and are capable of meeting a user’s social needs – from engaging in conversations around suicidal ideation, self-harm, or sexually explicit content. The bill would require platforms to provide recurring alerts to users&amp;nbsp; – every three hours for minors – reminding them that they are speaking to an AI chatbot, not a real person, and that they should take a break. It also establishes annual reporting and transparency requirements for AI companies that offer companion chatbots, including major players OpenAI, Character.AI, and Replika, which would go into effect July 1, 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The California bill would also allow individuals who believe they have been injured by violations to file lawsuits against AI companies seeking injunctive relief, damages (up to $1,000 per violation), and attorney’s fees.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bill gained momentum in the California legislature following the death of teenager Adam Raine, who committed suicide after prolonged chats with OpenAI’s ChatGPT that involved discussing and planning his death and self-harm. The legislation also responds to leaked internal documents that reportedly showed Meta’s chatbots were allowed to engage in “romantic” and “sensual” chats with children.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent weeks, U.S. lawmakers and regulators have responded with intensified scrutiny of AI platforms’ safeguards to protect minors. The Federal Trade Commission is preparing to investigate how AI chatbots impact children’s mental health. Texas Attorney General Ken Paxton has launched investigations into Meta and Character.AI, accusing them of misleading children with mental health claims. Meanwhile, both Sen. Josh Hawley (R-MO) and Sen. Ed Markey (D-MA) have launched separate probes into Meta.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think the harm is potentially great, which means we have to move quickly,” Padilla told TechCrunch. “We can put reasonable safeguards in place to make sure that particularly minors know they’re not talking to a real human being, that these platforms link people to the proper resources when people say things like they’re thinking about hurting themselves or they’re in distress, [and] to make sure there’s not inappropriate exposure to inappropriate material.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Padilla also stressed the importance of AI companies sharing data about the number of times they refer users to crisis services each year, “so we have a better understanding of the frequency of this problem, rather than only becoming aware of it when someone’s harmed or worse.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SB 243 previously had stronger requirements, but many were whittled down through amendments. For example, the bill originally would have required operators to prevent AI chatbots from using “variable reward” tactics or other features that encourage excessive engagement. These tactics, used by AI companion companies like Replika and Character, offer users special messages, memories, storylines, or the ability to unlock rare responses or new personalities, creating what critics call a potentially addictive reward loop.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The current bill also removes provisions that would have required operators to track and report how often chatbots initiated discussions of suicidal ideation or actions with users.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think it strikes the right balance of getting to the harms without enforcing something that’s either impossible for companies to comply with, either because it’s technically not feasible or just a lot of paperwork for nothing,” Becker told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SB 243 is moving toward becoming law at a time when Silicon Valley companies are pouring millions of dollars into pro-AI political action committees (PACs) to back candidates in the upcoming mid-term elections who favor a light-touch approach to AI regulation.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bill also comes as California weighs another AI safety bill, SB 53, which would mandate comprehensive transparency reporting requirements. OpenAI has written an open letter to Governor Newsom, asking him to abandon that bill in favor of less stringent federal and international frameworks. Major tech companies like Meta, Google, and Amazon have also opposed SB 53. In contrast, only Anthropic has said it supports SB 53.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I reject the premise that this is a zero sum situation, that innovation and regulation are mutually exclusive,” Padilla said. “Don’t tell me that we can’t walk and chew gum. We can support innovation and development that we think is healthy and has benefits – and there are benefits to this technology, clearly – and at the same time, we can provide reasonable safeguards for the most vulnerable people.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are closely monitoring the legislative and regulatory landscape, and we welcome working with regulators and lawmakers as they begin to consider legislation for this emerging space,” a Character.AI spokesperson told TechCrunch, noting that the startup already includes prominent disclaimers throughout the user chat experience explaining that it should be treated as fiction. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for Meta declined to comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenAI, Anthropic, and Replika for comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-1533302708.jpg?resize=1200,720" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;California has taken a big step toward regulating AI. SB 243 — a bill that would regulate AI companion chatbots in order to protect minors and vulnerable users — passed both the State Assembly and Senate with bipartisan support and now heads to Governor Gavin Newsom’s desk. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Newsom has until October 12 to either veto the bill or sign it into law. If he signs, it would take effect January 1, 2026, making California the first state to require AI chatbot operators to implement safety protocols for AI companions and hold companies legally accountable if their chatbots fail to meet those standards.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The bill specifically aims to prevent companion chatbots, which the legislation defines as AI systems that provide adaptive, human-like responses and are capable of meeting a user’s social needs – from engaging in conversations around suicidal ideation, self-harm, or sexually explicit content. The bill would require platforms to provide recurring alerts to users&amp;nbsp; – every three hours for minors – reminding them that they are speaking to an AI chatbot, not a real person, and that they should take a break. It also establishes annual reporting and transparency requirements for AI companies that offer companion chatbots, including major players OpenAI, Character.AI, and Replika, which would go into effect July 1, 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The California bill would also allow individuals who believe they have been injured by violations to file lawsuits against AI companies seeking injunctive relief, damages (up to $1,000 per violation), and attorney’s fees.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bill gained momentum in the California legislature following the death of teenager Adam Raine, who committed suicide after prolonged chats with OpenAI’s ChatGPT that involved discussing and planning his death and self-harm. The legislation also responds to leaked internal documents that reportedly showed Meta’s chatbots were allowed to engage in “romantic” and “sensual” chats with children.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent weeks, U.S. lawmakers and regulators have responded with intensified scrutiny of AI platforms’ safeguards to protect minors. The Federal Trade Commission is preparing to investigate how AI chatbots impact children’s mental health. Texas Attorney General Ken Paxton has launched investigations into Meta and Character.AI, accusing them of misleading children with mental health claims. Meanwhile, both Sen. Josh Hawley (R-MO) and Sen. Ed Markey (D-MA) have launched separate probes into Meta.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think the harm is potentially great, which means we have to move quickly,” Padilla told TechCrunch. “We can put reasonable safeguards in place to make sure that particularly minors know they’re not talking to a real human being, that these platforms link people to the proper resources when people say things like they’re thinking about hurting themselves or they’re in distress, [and] to make sure there’s not inappropriate exposure to inappropriate material.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Padilla also stressed the importance of AI companies sharing data about the number of times they refer users to crisis services each year, “so we have a better understanding of the frequency of this problem, rather than only becoming aware of it when someone’s harmed or worse.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SB 243 previously had stronger requirements, but many were whittled down through amendments. For example, the bill originally would have required operators to prevent AI chatbots from using “variable reward” tactics or other features that encourage excessive engagement. These tactics, used by AI companion companies like Replika and Character, offer users special messages, memories, storylines, or the ability to unlock rare responses or new personalities, creating what critics call a potentially addictive reward loop.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The current bill also removes provisions that would have required operators to track and report how often chatbots initiated discussions of suicidal ideation or actions with users.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think it strikes the right balance of getting to the harms without enforcing something that’s either impossible for companies to comply with, either because it’s technically not feasible or just a lot of paperwork for nothing,” Becker told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SB 243 is moving toward becoming law at a time when Silicon Valley companies are pouring millions of dollars into pro-AI political action committees (PACs) to back candidates in the upcoming mid-term elections who favor a light-touch approach to AI regulation.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bill also comes as California weighs another AI safety bill, SB 53, which would mandate comprehensive transparency reporting requirements. OpenAI has written an open letter to Governor Newsom, asking him to abandon that bill in favor of less stringent federal and international frameworks. Major tech companies like Meta, Google, and Amazon have also opposed SB 53. In contrast, only Anthropic has said it supports SB 53.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I reject the premise that this is a zero sum situation, that innovation and regulation are mutually exclusive,” Padilla said. “Don’t tell me that we can’t walk and chew gum. We can support innovation and development that we think is healthy and has benefits – and there are benefits to this technology, clearly – and at the same time, we can provide reasonable safeguards for the most vulnerable people.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are closely monitoring the legislative and regulatory landscape, and we welcome working with regulators and lawmakers as they begin to consider legislation for this emerging space,” a Character.AI spokesperson told TechCrunch, noting that the startup already includes prominent disclaimers throughout the user chat experience explaining that it should be treated as fiction. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for Meta declined to comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenAI, Anthropic, and Replika for comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/11/a-california-bill-that-would-regulate-ai-companion-chatbots-is-close-to-becoming-law/</guid><pubDate>Thu, 11 Sep 2025 22:23:09 +0000</pubDate></item><item><title>[NEW] OpenAI and Microsoft sign preliminary deal to revise partnership terms (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/09/openai-and-microsoft-sign-preliminary-deal-to-revise-partnership-terms/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Companies work to finalize terms as OpenAI pursues for-profit restructuring.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo superimposed over a Microsoft logo background" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/07/openai_microsoft_3-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo superimposed over a Microsoft logo background" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/07/openai_microsoft_3-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI / Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Thursday, OpenAI and Microsoft announced they have signed a non-binding agreement to revise their partnership, marking the latest development in a relationship that has grown increasingly complex as both companies compete for customers in the AI market and seek new partnerships for growing infrastructure needs.&lt;/p&gt;
&lt;p&gt;"Microsoft and OpenAI have signed a non-binding memorandum of understanding (MOU) for the next phase of our partnership," the companies wrote in a joint statement. "We are actively working to finalize contractual terms in a definitive agreement. Together, we remain focused on delivering the best AI tools for everyone, grounded in our shared commitment to safety."&lt;/p&gt;
&lt;p&gt;The announcement comes as OpenAI seeks to restructure from a nonprofit to a for-profit entity, a transition that requires Microsoft's approval, as the company is OpenAI's largest investor with more than $13 billion committed since 2019.&lt;/p&gt;
&lt;p&gt;The partnership has shown increasing strain as OpenAI has grown from a research lab into a company valued at $500 billion. Both companies now compete for customers, and OpenAI seeks more compute capacity than Microsoft can provide. The relationship has also faced complications over contract terms, including provisions that would limit Microsoft's access to OpenAI technology once the company reaches so-called AGI (artificial general intelligence)—a nebulous milestone both companies now economically define as AI systems capable of generating at least $100 billion in profit.&lt;/p&gt;
&lt;p&gt;In May, OpenAI abandoned its original plan to fully convert to a for-profit company after mounting pressure from former employees, regulators, and critics, including Elon Musk. Musk has sued to block the conversion, arguing it betrays OpenAI's founding mission as a nonprofit dedicated to benefiting humanity.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The company instead announced a modified approach where the nonprofit board would retain control while converting its for-profit subsidiary into a public benefit corporation. Under this revised structure, the nonprofit would become the largest shareholder with a stake worth more than $100 billion, according to OpenAI chairman Bret Taylor's announcement in May. Thursday's MOU with Microsoft suggests the companies are working to align on this modified restructuring plan.&lt;/p&gt;
&lt;h2&gt;High stakes tied to restructuring&lt;/h2&gt;
&lt;p&gt;The restructuring still requires approval from attorneys general in California and Delaware, who continue to scrutinize the proposed conversion. A coalition of charitable institutions has also called on regulators to halt the plan.&lt;/p&gt;
&lt;p&gt;Microsoft stated in a January blog post that key elements of the partnership remain in place through 2030, including access to OpenAI's intellectual property, revenue-sharing arrangements, and exclusivity on OpenAI's APIs. That agreement introduced a right of first refusal model for compute capacity, replacing Microsoft's previous exclusivity as OpenAI's sole cloud provider.&lt;/p&gt;
&lt;p&gt;The rapidly changing&amp;nbsp;relationship between two formerly steadfast partners follows the AI industry's explosive growth from experimental research labs to multi-hundred-billion-dollar infrastructure investments over the past six years since the two companies first made an investment deal. OpenAI has diversified its infrastructure partnerships, including participation in the $500 billion Stargate Project with Oracle and SoftBank, while Microsoft has expanded Azure to host competing models from Meta, xAI, and DeepSeek.&lt;/p&gt;
&lt;p&gt;Despite efforts by both companies to reduce mutual dependence, their fortunes remain deeply intertwined. Microsoft disclosed in recent earnings that Azure has become a $75 billion annual business, with significant contributions from OpenAI-related services.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Companies work to finalize terms as OpenAI pursues for-profit restructuring.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo superimposed over a Microsoft logo background" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/07/openai_microsoft_3-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo superimposed over a Microsoft logo background" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/07/openai_microsoft_3-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI / Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Thursday, OpenAI and Microsoft announced they have signed a non-binding agreement to revise their partnership, marking the latest development in a relationship that has grown increasingly complex as both companies compete for customers in the AI market and seek new partnerships for growing infrastructure needs.&lt;/p&gt;
&lt;p&gt;"Microsoft and OpenAI have signed a non-binding memorandum of understanding (MOU) for the next phase of our partnership," the companies wrote in a joint statement. "We are actively working to finalize contractual terms in a definitive agreement. Together, we remain focused on delivering the best AI tools for everyone, grounded in our shared commitment to safety."&lt;/p&gt;
&lt;p&gt;The announcement comes as OpenAI seeks to restructure from a nonprofit to a for-profit entity, a transition that requires Microsoft's approval, as the company is OpenAI's largest investor with more than $13 billion committed since 2019.&lt;/p&gt;
&lt;p&gt;The partnership has shown increasing strain as OpenAI has grown from a research lab into a company valued at $500 billion. Both companies now compete for customers, and OpenAI seeks more compute capacity than Microsoft can provide. The relationship has also faced complications over contract terms, including provisions that would limit Microsoft's access to OpenAI technology once the company reaches so-called AGI (artificial general intelligence)—a nebulous milestone both companies now economically define as AI systems capable of generating at least $100 billion in profit.&lt;/p&gt;
&lt;p&gt;In May, OpenAI abandoned its original plan to fully convert to a for-profit company after mounting pressure from former employees, regulators, and critics, including Elon Musk. Musk has sued to block the conversion, arguing it betrays OpenAI's founding mission as a nonprofit dedicated to benefiting humanity.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The company instead announced a modified approach where the nonprofit board would retain control while converting its for-profit subsidiary into a public benefit corporation. Under this revised structure, the nonprofit would become the largest shareholder with a stake worth more than $100 billion, according to OpenAI chairman Bret Taylor's announcement in May. Thursday's MOU with Microsoft suggests the companies are working to align on this modified restructuring plan.&lt;/p&gt;
&lt;h2&gt;High stakes tied to restructuring&lt;/h2&gt;
&lt;p&gt;The restructuring still requires approval from attorneys general in California and Delaware, who continue to scrutinize the proposed conversion. A coalition of charitable institutions has also called on regulators to halt the plan.&lt;/p&gt;
&lt;p&gt;Microsoft stated in a January blog post that key elements of the partnership remain in place through 2030, including access to OpenAI's intellectual property, revenue-sharing arrangements, and exclusivity on OpenAI's APIs. That agreement introduced a right of first refusal model for compute capacity, replacing Microsoft's previous exclusivity as OpenAI's sole cloud provider.&lt;/p&gt;
&lt;p&gt;The rapidly changing&amp;nbsp;relationship between two formerly steadfast partners follows the AI industry's explosive growth from experimental research labs to multi-hundred-billion-dollar infrastructure investments over the past six years since the two companies first made an investment deal. OpenAI has diversified its infrastructure partnerships, including participation in the $500 billion Stargate Project with Oracle and SoftBank, while Microsoft has expanded Azure to host competing models from Meta, xAI, and DeepSeek.&lt;/p&gt;
&lt;p&gt;Despite efforts by both companies to reduce mutual dependence, their fortunes remain deeply intertwined. Microsoft disclosed in recent earnings that Azure has become a $75 billion annual business, with significant contributions from OpenAI-related services.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/09/openai-and-microsoft-sign-preliminary-deal-to-revise-partnership-terms/</guid><pubDate>Thu, 11 Sep 2025 22:27:53 +0000</pubDate></item></channel></rss>