<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 26 Jun 2025 06:33:17 +0000</lastBuildDate><item><title>[NEW] Nvidia’s ‘AI Factory’ narrative faces reality check as inference wars expose 70% margins (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/nvidias-ai-factory-narrative-faces-reality-check-at-transform-2025/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The gloves came off at &lt;span&gt;Tuesday at VB Transform 2025&amp;nbsp;as alternative chip makers directly challenged Nvidia’s dominance narrative during a panel about inference, exposing a fundamental contradiction: How can AI inference be&amp;nbsp;&lt;/span&gt;a commoditized “factory” and command 70% gross margins?&lt;/p&gt;



&lt;p&gt;Jonathan Ross, CEO of Groq, didn’t mince words when discussing Nvidia’s carefully crafted messaging. “AI factory is just a marketing way to make AI sound less scary,” Ross said during the panel. Sean Lie, CTO of Cerebras, a competitor, was equally direct: “I don’t think Nvidia minds having all of the service providers fighting it out for every last penny while they’re sitting there comfortable with 70 points.”&lt;/p&gt;



&lt;p&gt;Hundreds of billions in infrastructure investment and the future architecture of enterprise AI are at stake. For CISOs and AI leaders currently locked in weekly negotiations with OpenAI and other providers for more capacity, the panel exposed uncomfortable truths about why their AI initiatives keep hitting roadblocks.&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-the-capacity-crisis-no-one-talks-about"&gt;The capacity crisis no one talks about&lt;/h2&gt;



&lt;p&gt;“Anyone who’s actually a big user of these gen AI models knows that you can go to OpenAI, or whoever it is, and they won’t actually be able to serve you enough tokens,” explained Dylan Patel, founder of SemiAnalysis. There are weekly meetings between some of the biggest AI users and their model providers to try to persuade them to allocate more capacity. Then there’s weekly meetings between those model providers and their hardware providers.”&lt;/p&gt;



&lt;p&gt;Panel participants also pointed to the token shortage as exposing a fundamental flaw in the factory analogy. Traditional manufacturing responds to demand signals by adding capacity. However, when enterprises require 10 times more inference capacity, they discover that the supply chain can’t flex. GPUs require two-year lead times. Data centers need permits and power agreements. The infrastructure wasn’t built for exponential scaling, forcing providers to ration access through API limits.&lt;/p&gt;



&lt;p&gt;According to Patel, Anthropic jumped from $2 billion to $3 billion in ARR in just six months. Cursor went from essentially zero to $500 million ARR. OpenAI crossed $10 billion. Yet enterprises still can’t get the tokens they need.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-factory-thinking-breaks-ai-economics"&gt;Why ‘Factory’ thinking breaks AI economics&lt;/h2&gt;



&lt;p&gt;Jensen Huang’s “AI factory” concept implies standardization, commoditization and efficiency gains that drive down costs. But the panel revealed three fundamental ways this metaphor breaks down:&lt;/p&gt;



&lt;p&gt;First, inference isn’t uniform. “Even today, for inference of, say, DeepSeek, there’s a number of providers along the curve of sort of how fast they provide at what cost,” Patel noted. DeepSeek serves its own model at the lowest cost but only delivers 20 tokens per second. “Nobody wants to use a model at 20 tokens a second. I talk faster than 20 tokens a second.”&lt;/p&gt;



&lt;p&gt;Second, quality varies wildly. Ross drew a historical parallel to Standard Oil: “When Standard Oil started, oil had varying quality. You could buy oil from one vendor and it might set your house on fire.” Today’s AI inference market faces similar quality variations, with providers using various techniques to reduce costs that inadvertently compromise output quality.&lt;/p&gt;



&lt;p&gt;Third, and most critically, the economics are inverted. “One of the things that’s unusual about AI is that you can’t spend more to get better results,” Ross explained. “You can’t just have a software application, say, I’m going to spend twice as much to host my software, and applications can get better.”&lt;/p&gt;







&lt;p&gt;When Ross mentioned that Mark Zuckerberg praised Groq for being “the only ones who launched it with the full quality,” he inadvertently revealed the industry’s quality crisis. This wasn’t just recognition. It was an indictment of every other provider cutting corners.&lt;/p&gt;



&lt;p&gt;Ross spelled out the mechanics: “A lot of people do a lot of tricks to reduce the quality, not intentionally, but to lower their cost, improve their speed.” The techniques sound technical, but the impact is straightforward. Quantization reduces precision. Pruning removes parameters. Each optimization degrades model performance in ways enterprises may not detect until production fails.&lt;/p&gt;



&lt;p&gt;The Standard Oil parallel Ross drew illuminates the stakes. Today’s inference market faces the same quality variance problem. Providers betting that enterprises won’t notice the difference between 95% and 100% accuracy are betting against companies like Meta that have the sophistication to measure degradation.&lt;/p&gt;



&lt;p&gt;This creates immediate imperatives for enterprise buyers. &lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;Establish quality benchmarks before selecting providers. &lt;/li&gt;



&lt;li&gt;Audit existing inference partners for undisclosed optimizations. &lt;/li&gt;



&lt;li&gt;Accept that premium pricing for full model fidelity is now a permanent market feature. The era of assuming functional equivalence across inference providers ended when Zuckerberg called out the difference.&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 class="wp-block-heading" id="h-the-1-million-token-paradox"&gt;The $1 million token paradox&lt;/h2&gt;



&lt;p&gt;The most revealing moment came when the panel discussed pricing. Lie highlighted an uncomfortable truth for the industry: “If these million tokens are as valuable as we believe they can be, right? That’s not about moving words. You don’t charge $1 for moving words. I pay my lawyer $800 for an hour to write a two-page memo.”&lt;/p&gt;



&lt;p&gt;This observation cuts to the heart of AI’s price discovery problem. The industry is racing to drive token costs below $1.50 per million while claiming these tokens will transform every aspect of business. The panel implicitly agreed with each other that the math doesn’t add up.&lt;/p&gt;



&lt;p&gt;“Pretty much everyone is spending, like all of these fast-growing startups, the amount that they’re spending on tokens as a service almost matches their revenue one to one,” Ross revealed. This 1:1 spend ratio on AI tokens versus revenue represents an unsustainable business model that panel participants contend the “factory” narrative conveniently ignores.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-changes-everything"&gt;Performance changes everything&lt;/h2&gt;



&lt;p&gt;Cerebras and Groq aren’t just competing on price; they are also competing on performance. They’re fundamentally changing what is possible in terms of inference speed. “With the wafer scale technology that we’ve built, we’re enabling 10 times, sometimes 50 times, faster performance than even the fastest GPUs today,” Lie said.&lt;/p&gt;



&lt;p&gt;This isn’t an incremental improvement. It’s enabling entirely new use cases. “We have customers who have agentic workflows that might take 40 minutes, and they want these things to run in real time,” Lie explained. “These things just aren’t even possible, even if you’re willing to pay top dollar.”&lt;/p&gt;



&lt;p&gt;The speed differential creates a bifurcated market that defies factory standardization. Enterprises needing real-time inference for customer-facing applications can’t use the same infrastructure as those running overnight batch processes.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-real-bottleneck-power-and-data-centers"&gt;The real bottleneck: power and data centers&lt;/h2&gt;



&lt;p&gt;While everyone focuses on chip supply, the panel revealed the actual constraint throttling AI deployment. “Data center capacity is a big problem. You can’t really find data center space in the U.S.,” Patel said. “Power is a big problem.”&lt;/p&gt;



&lt;p&gt;The infrastructure challenge goes beyond chip manufacturing to fundamental resource constraints. As Patel explained, “TSMC in Taiwan is able to make over $200 million worth of chips, right? It’s not even… it’s the speed at which they scale up is ridiculous.” &lt;/p&gt;



&lt;p&gt;But chip production means nothing without infrastructure. “The reason we see these big Middle East deals, and partially why both of these companies have big presences in the Middle East is, it’s power,” Patel revealed. The global scramble for compute has enterprises “going across the world to get wherever power does exist, wherever data center capacity exists, wherever there are electricians who can build these electrical systems.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-google-s-success-disaster-becomes-everyone-s-reality"&gt;Google’s ‘success disaster’ becomes everyone’s reality&lt;/h2&gt;



&lt;p&gt;Ross shared a telling anecdote from Google’s history: “There was a term that became very popular at Google in 2015 called Success Disaster. Some of the teams had built AI applications that began to work better than human beings for the first time, and the demand for compute was so high, they were going to need to double or triple the global data center footprint quickly.”&lt;/p&gt;



&lt;p&gt;This pattern now repeats across every enterprise AI deployment. Applications either fail to gain traction or experience hockey stick growth that immediately hits infrastructure limits. There’s no middle ground, no smooth scaling curve that factory economics would predict.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-this-means-for-enterprise-ai-strategy"&gt;What this means for enterprise AI strategy&lt;/h2&gt;



&lt;p&gt;For CIOs, CISOs and AI leaders, the panel’s revelations demand strategic recalibration:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Capacity planning requires new models.&lt;/strong&gt; Traditional IT forecasting assumes linear growth. AI workloads break this assumption. When successful applications increase token consumption by 30% monthly, annual capacity plans become obsolete within quarters. Enterprises must shift from static procurement cycles to dynamic capacity management. Build contracts with burst provisions. Monitor usage weekly, not quarterly. Accept that AI scaling patterns resemble those of viral adoption curves, not traditional enterprise software rollouts.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Speed premiums are permanent.&lt;/strong&gt; The idea that inference will commoditize to uniform pricing ignores the massive performance gaps between providers. Enterprises need to budget for speed where it matters.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Architecture beats optimization.&lt;/strong&gt; Groq and Cerebras aren’t winning by doing GPUs better. They’re winning by rethinking the fundamental architecture of AI compute. Enterprises that bet everything on GPU-based infrastructure may find themselves stuck in the slow lane.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Power infrastructure is strategic.&lt;/strong&gt; The constraint isn’t chips or software but kilowatts and cooling. Smart enterprises are already locking in power capacity and data center space for 2026 and beyond.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-infrastructure-reality-enterprises-can-t-ignore"&gt;The infrastructure reality enterprises can’t ignore&lt;/h2&gt;



&lt;p&gt;The panel revealed a fundamental truth: the AI factory metaphor isn’t only wrong, but also dangerous. Enterprises building strategies around commodity inference pricing and standardized delivery are planning for a market that doesn’t exist.&lt;/p&gt;



&lt;p&gt;The real market operates on three brutal realities.&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt; Capacity scarcity creates power inversions, where suppliers dictate terms and enterprises beg for allocations. &lt;/li&gt;



&lt;li&gt;Quality variance, the difference between 95% and 100% accuracy, determines whether your AI applications succeed or catastrophically fail. &lt;/li&gt;



&lt;li&gt;Infrastructure constraints, not technology, set the binding limits on AI transformation.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The path forward for CISOs and AI leaders requires abandoning factory thinking entirely. Lock in power capacity now. Audit inference providers for hidden quality degradation. Build vendor relationships based on architectural advantages, not marginal cost savings. Most critically, accept that paying 70% margins for reliable, high-quality inference may be your smartest investment.&lt;/p&gt;



&lt;p&gt;The alternative chip makers at Transform didn’t just challenge Nvidia’s narrative. They revealed that enterprises face a choice: pay for quality and performance, or join the weekly negotiation meetings. The panel’s consensus was clear: success requires matching specific workloads to appropriate infrastructure rather than pursuing one-size-fits-all solutions.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The gloves came off at &lt;span&gt;Tuesday at VB Transform 2025&amp;nbsp;as alternative chip makers directly challenged Nvidia’s dominance narrative during a panel about inference, exposing a fundamental contradiction: How can AI inference be&amp;nbsp;&lt;/span&gt;a commoditized “factory” and command 70% gross margins?&lt;/p&gt;



&lt;p&gt;Jonathan Ross, CEO of Groq, didn’t mince words when discussing Nvidia’s carefully crafted messaging. “AI factory is just a marketing way to make AI sound less scary,” Ross said during the panel. Sean Lie, CTO of Cerebras, a competitor, was equally direct: “I don’t think Nvidia minds having all of the service providers fighting it out for every last penny while they’re sitting there comfortable with 70 points.”&lt;/p&gt;



&lt;p&gt;Hundreds of billions in infrastructure investment and the future architecture of enterprise AI are at stake. For CISOs and AI leaders currently locked in weekly negotiations with OpenAI and other providers for more capacity, the panel exposed uncomfortable truths about why their AI initiatives keep hitting roadblocks.&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-the-capacity-crisis-no-one-talks-about"&gt;The capacity crisis no one talks about&lt;/h2&gt;



&lt;p&gt;“Anyone who’s actually a big user of these gen AI models knows that you can go to OpenAI, or whoever it is, and they won’t actually be able to serve you enough tokens,” explained Dylan Patel, founder of SemiAnalysis. There are weekly meetings between some of the biggest AI users and their model providers to try to persuade them to allocate more capacity. Then there’s weekly meetings between those model providers and their hardware providers.”&lt;/p&gt;



&lt;p&gt;Panel participants also pointed to the token shortage as exposing a fundamental flaw in the factory analogy. Traditional manufacturing responds to demand signals by adding capacity. However, when enterprises require 10 times more inference capacity, they discover that the supply chain can’t flex. GPUs require two-year lead times. Data centers need permits and power agreements. The infrastructure wasn’t built for exponential scaling, forcing providers to ration access through API limits.&lt;/p&gt;



&lt;p&gt;According to Patel, Anthropic jumped from $2 billion to $3 billion in ARR in just six months. Cursor went from essentially zero to $500 million ARR. OpenAI crossed $10 billion. Yet enterprises still can’t get the tokens they need.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-factory-thinking-breaks-ai-economics"&gt;Why ‘Factory’ thinking breaks AI economics&lt;/h2&gt;



&lt;p&gt;Jensen Huang’s “AI factory” concept implies standardization, commoditization and efficiency gains that drive down costs. But the panel revealed three fundamental ways this metaphor breaks down:&lt;/p&gt;



&lt;p&gt;First, inference isn’t uniform. “Even today, for inference of, say, DeepSeek, there’s a number of providers along the curve of sort of how fast they provide at what cost,” Patel noted. DeepSeek serves its own model at the lowest cost but only delivers 20 tokens per second. “Nobody wants to use a model at 20 tokens a second. I talk faster than 20 tokens a second.”&lt;/p&gt;



&lt;p&gt;Second, quality varies wildly. Ross drew a historical parallel to Standard Oil: “When Standard Oil started, oil had varying quality. You could buy oil from one vendor and it might set your house on fire.” Today’s AI inference market faces similar quality variations, with providers using various techniques to reduce costs that inadvertently compromise output quality.&lt;/p&gt;



&lt;p&gt;Third, and most critically, the economics are inverted. “One of the things that’s unusual about AI is that you can’t spend more to get better results,” Ross explained. “You can’t just have a software application, say, I’m going to spend twice as much to host my software, and applications can get better.”&lt;/p&gt;







&lt;p&gt;When Ross mentioned that Mark Zuckerberg praised Groq for being “the only ones who launched it with the full quality,” he inadvertently revealed the industry’s quality crisis. This wasn’t just recognition. It was an indictment of every other provider cutting corners.&lt;/p&gt;



&lt;p&gt;Ross spelled out the mechanics: “A lot of people do a lot of tricks to reduce the quality, not intentionally, but to lower their cost, improve their speed.” The techniques sound technical, but the impact is straightforward. Quantization reduces precision. Pruning removes parameters. Each optimization degrades model performance in ways enterprises may not detect until production fails.&lt;/p&gt;



&lt;p&gt;The Standard Oil parallel Ross drew illuminates the stakes. Today’s inference market faces the same quality variance problem. Providers betting that enterprises won’t notice the difference between 95% and 100% accuracy are betting against companies like Meta that have the sophistication to measure degradation.&lt;/p&gt;



&lt;p&gt;This creates immediate imperatives for enterprise buyers. &lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;Establish quality benchmarks before selecting providers. &lt;/li&gt;



&lt;li&gt;Audit existing inference partners for undisclosed optimizations. &lt;/li&gt;



&lt;li&gt;Accept that premium pricing for full model fidelity is now a permanent market feature. The era of assuming functional equivalence across inference providers ended when Zuckerberg called out the difference.&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 class="wp-block-heading" id="h-the-1-million-token-paradox"&gt;The $1 million token paradox&lt;/h2&gt;



&lt;p&gt;The most revealing moment came when the panel discussed pricing. Lie highlighted an uncomfortable truth for the industry: “If these million tokens are as valuable as we believe they can be, right? That’s not about moving words. You don’t charge $1 for moving words. I pay my lawyer $800 for an hour to write a two-page memo.”&lt;/p&gt;



&lt;p&gt;This observation cuts to the heart of AI’s price discovery problem. The industry is racing to drive token costs below $1.50 per million while claiming these tokens will transform every aspect of business. The panel implicitly agreed with each other that the math doesn’t add up.&lt;/p&gt;



&lt;p&gt;“Pretty much everyone is spending, like all of these fast-growing startups, the amount that they’re spending on tokens as a service almost matches their revenue one to one,” Ross revealed. This 1:1 spend ratio on AI tokens versus revenue represents an unsustainable business model that panel participants contend the “factory” narrative conveniently ignores.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-changes-everything"&gt;Performance changes everything&lt;/h2&gt;



&lt;p&gt;Cerebras and Groq aren’t just competing on price; they are also competing on performance. They’re fundamentally changing what is possible in terms of inference speed. “With the wafer scale technology that we’ve built, we’re enabling 10 times, sometimes 50 times, faster performance than even the fastest GPUs today,” Lie said.&lt;/p&gt;



&lt;p&gt;This isn’t an incremental improvement. It’s enabling entirely new use cases. “We have customers who have agentic workflows that might take 40 minutes, and they want these things to run in real time,” Lie explained. “These things just aren’t even possible, even if you’re willing to pay top dollar.”&lt;/p&gt;



&lt;p&gt;The speed differential creates a bifurcated market that defies factory standardization. Enterprises needing real-time inference for customer-facing applications can’t use the same infrastructure as those running overnight batch processes.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-real-bottleneck-power-and-data-centers"&gt;The real bottleneck: power and data centers&lt;/h2&gt;



&lt;p&gt;While everyone focuses on chip supply, the panel revealed the actual constraint throttling AI deployment. “Data center capacity is a big problem. You can’t really find data center space in the U.S.,” Patel said. “Power is a big problem.”&lt;/p&gt;



&lt;p&gt;The infrastructure challenge goes beyond chip manufacturing to fundamental resource constraints. As Patel explained, “TSMC in Taiwan is able to make over $200 million worth of chips, right? It’s not even… it’s the speed at which they scale up is ridiculous.” &lt;/p&gt;



&lt;p&gt;But chip production means nothing without infrastructure. “The reason we see these big Middle East deals, and partially why both of these companies have big presences in the Middle East is, it’s power,” Patel revealed. The global scramble for compute has enterprises “going across the world to get wherever power does exist, wherever data center capacity exists, wherever there are electricians who can build these electrical systems.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-google-s-success-disaster-becomes-everyone-s-reality"&gt;Google’s ‘success disaster’ becomes everyone’s reality&lt;/h2&gt;



&lt;p&gt;Ross shared a telling anecdote from Google’s history: “There was a term that became very popular at Google in 2015 called Success Disaster. Some of the teams had built AI applications that began to work better than human beings for the first time, and the demand for compute was so high, they were going to need to double or triple the global data center footprint quickly.”&lt;/p&gt;



&lt;p&gt;This pattern now repeats across every enterprise AI deployment. Applications either fail to gain traction or experience hockey stick growth that immediately hits infrastructure limits. There’s no middle ground, no smooth scaling curve that factory economics would predict.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-this-means-for-enterprise-ai-strategy"&gt;What this means for enterprise AI strategy&lt;/h2&gt;



&lt;p&gt;For CIOs, CISOs and AI leaders, the panel’s revelations demand strategic recalibration:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Capacity planning requires new models.&lt;/strong&gt; Traditional IT forecasting assumes linear growth. AI workloads break this assumption. When successful applications increase token consumption by 30% monthly, annual capacity plans become obsolete within quarters. Enterprises must shift from static procurement cycles to dynamic capacity management. Build contracts with burst provisions. Monitor usage weekly, not quarterly. Accept that AI scaling patterns resemble those of viral adoption curves, not traditional enterprise software rollouts.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Speed premiums are permanent.&lt;/strong&gt; The idea that inference will commoditize to uniform pricing ignores the massive performance gaps between providers. Enterprises need to budget for speed where it matters.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Architecture beats optimization.&lt;/strong&gt; Groq and Cerebras aren’t winning by doing GPUs better. They’re winning by rethinking the fundamental architecture of AI compute. Enterprises that bet everything on GPU-based infrastructure may find themselves stuck in the slow lane.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Power infrastructure is strategic.&lt;/strong&gt; The constraint isn’t chips or software but kilowatts and cooling. Smart enterprises are already locking in power capacity and data center space for 2026 and beyond.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-infrastructure-reality-enterprises-can-t-ignore"&gt;The infrastructure reality enterprises can’t ignore&lt;/h2&gt;



&lt;p&gt;The panel revealed a fundamental truth: the AI factory metaphor isn’t only wrong, but also dangerous. Enterprises building strategies around commodity inference pricing and standardized delivery are planning for a market that doesn’t exist.&lt;/p&gt;



&lt;p&gt;The real market operates on three brutal realities.&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt; Capacity scarcity creates power inversions, where suppliers dictate terms and enterprises beg for allocations. &lt;/li&gt;



&lt;li&gt;Quality variance, the difference between 95% and 100% accuracy, determines whether your AI applications succeed or catastrophically fail. &lt;/li&gt;



&lt;li&gt;Infrastructure constraints, not technology, set the binding limits on AI transformation.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The path forward for CISOs and AI leaders requires abandoning factory thinking entirely. Lock in power capacity now. Audit inference providers for hidden quality degradation. Build vendor relationships based on architectural advantages, not marginal cost savings. Most critically, accept that paying 70% margins for reliable, high-quality inference may be your smartest investment.&lt;/p&gt;



&lt;p&gt;The alternative chip makers at Transform didn’t just challenge Nvidia’s narrative. They revealed that enterprises face a choice: pay for quality and performance, or join the weekly negotiation meetings. The panel’s consensus was clear: success requires matching specific workloads to appropriate infrastructure rather than pursuing one-size-fits-all solutions.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/nvidias-ai-factory-narrative-faces-reality-check-at-transform-2025/</guid><pubDate>Wed, 25 Jun 2025 18:56:28 +0000</pubDate></item><item><title>[NEW] Creatio’s new 8.3 Twin CRM update hits Salesforce where it hurts: ‘we don’t think of AI as an add-on…it’s just part of our app experience’ (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/creatios-new-8-3-twin-crm-update-hits-salesforce-where-it-hurts-we-dont-think-of-ai-as-an-add-on-its-just-part-of-our-app-experience/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Creatio, the Boston-headquartered customer relationship management (CRM) company focused on no-code and low-code CRM app deployment, has officially launched its latest platform update, the 8.3 “Twin” Release, introducing a suite of AI-native capabilities designed to streamline CRM and workflow automation. &lt;/p&gt;



&lt;p&gt;With this update, Creatio continues its mission to build enterprise software where humans and AI agents collaborate across functions such as sales, marketing, service, and application development.&lt;/p&gt;



&lt;p&gt;“This release for us is sort of that pivot point where we move beyond the traditional SaaS CRM,” said Burley Kawasaki, Chief Product Officer at Creatio, in an interview with VentureBeat. “The way we work with SaaS has fundamentally changed—it’s now about fluid movement between applications and AI agents.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ai-native-throughout-creatio-with-conversational-and-classic-interface-access"&gt;AI-native throughout Creatio with conversational and classic interface access&lt;/h2&gt;



&lt;p&gt;The new release centers around embedding AI into the core of the Creatio platform, rather than treating it as an add-on. &lt;/p&gt;



&lt;p&gt;Creatio 8.3 includes a conversational user experience, prebuilt role-based AI agents, and a no-code agent builder that allows businesses to customize how automation functions in their specific environment. &lt;/p&gt;



&lt;p&gt;These features are now available to all customers and trial users with no added cost or licensing requirements.&lt;/p&gt;



&lt;p&gt;“You won’t see us ever release a Creatio Force, right?” Kawaski said, a twinkle in his eye as he subtley called out rival CRM provider Salesforce’s Agentforce AI agent creation platform, which requires a separate additional subscription or paid credits to access agent building capabilities. &lt;/p&gt;



&lt;p&gt;Creatio’s platform starts at $25 per user per month.&lt;/p&gt;



&lt;p&gt; Because we don’t think of it as an add-on or something separate you have to use in addition to your existing apps,” said Kawasaki. “This is just part of our app experience.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-multi-conversation-support-across-platform"&gt;Multi-conversation support across platform&lt;/h2&gt;



&lt;p&gt;The platform now offers a natural language interface that spans Creatio’s web and mobile apps, as well as integrations with tools such as Microsoft Outlook and Teams. &lt;/p&gt;



&lt;p&gt;Users can switch between channels and devices without losing context, allowing for ongoing, persistent interactions with AI agents across workflows.&lt;/p&gt;



&lt;p&gt;“We’ve enabled multi-conversation support with persistent context across devices and platforms—whether you’re in Outlook, Teams, or on mobile,” Kawasaki explained. “Our adaptive user experience lets users live in a prompt-driven CRM, accessing functionality through natural language. You can toggle between classic and conversational modes as needed.”&lt;/p&gt;



&lt;p&gt;Zoom and Gmail integration are expected later in the year.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-prebuilt-and-customizable-ai-agents-for-diverse-sectors-and-functions"&gt;Prebuilt and customizable AI agents for diverse sectors and functions&lt;/h2&gt;



&lt;p&gt;In this release, Creatio debuts several prebuilt agents that support high-frequency tasks across core business areas:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Sales agents&lt;/strong&gt; handle tasks such as researching accounts, preparing meetings, and generating quotes.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Marketing agents&lt;/strong&gt; assist with creating campaign messaging, emails, and other targeted content.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Service agents&lt;/strong&gt; focus on faster case resolution by drawing from internal knowledge bases and suggesting new content where gaps exist.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;No-code agents&lt;/strong&gt;, starting with the Dashboards Agent, help non-technical users generate and refine analytics using natural language prompts.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;“In sales, agents handle data entry, follow-ups, and paperwork, freeing up reps to focus on relationships and strategy,” said Kawasaki. “In marketing, agents take on content creation and campaign analytics, letting marketers focus more on storytelling.”&lt;/p&gt;



&lt;p&gt;“While others are still offering fragmented AI products and complex pricing models, we’ve taken a different path,” said Kawasaki. “This release offers one platform, one experience, and one clear route to accelerated AI adoption and realizing real business value.”&lt;/p&gt;



&lt;p&gt;The conversational assistant in Creatio 8.3 also introduces advanced features like file uploads for grounding responses in organization-specific documents, and support for retrieval-augmented generation (RAG) to ensure accuracy based on proprietary knowledge.&lt;/p&gt;



&lt;p&gt;“Agents can now be grounded in uploaded documents and Creatio’s metadata, ensuring responses are accurate and personalized, not generic,” Kawasaki noted.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-build-and-deploy-custom-ai-agents-with-no-code"&gt;Build and deploy custom AI agents with no code&lt;/h2&gt;



&lt;p&gt;The no-code development environment has also been overhauled with embedded AI that assists in building dashboards, apps, and even new AI agents. Users can design and deploy agents by combining reusable skills, workflows, prompts, and knowledge sources—offering full flexibility to tailor automation without needing engineering resources.&lt;/p&gt;



&lt;p&gt;“We’re launching a no-code agent builder where you define skills, prompts, actions, and workflows,” said Kawasaki. “It’s visual and accessible but still requires thoughtful training and data input.”&lt;/p&gt;



&lt;p&gt;Under the hood, Creatio supports multiple foundational models including OpenAI, Anthropic, and Gemini. A bring-your-own-model capability is planned for later in 2025, enabling customers to host their own models, such as Llama or DeepSeek, and pair them with specific tasks or agents.&lt;/p&gt;



&lt;p&gt;“We decided early on not to build our own LLM,” Kawasaki explained. “Instead, we support OpenAI, Anthropic, and Gemini—and soon, customers will be able to bring their own model.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-security-data-governance-and-grounding-remain-priorities"&gt;Security, data governance, and grounding remain priorities&lt;/h2&gt;



&lt;p&gt;Security and governance were also a priority in this release. Documents used to train or inform AI agents are stored in each customer’s secure, dedicated instance, and are not shared with external large language models. &lt;/p&gt;



&lt;p&gt;Customers can control which documents are persistent for agent grounding and manage access to ensure regulatory compliance, especially in sensitive industries.&lt;/p&gt;



&lt;p&gt;The release follows extensive hands-on testing and feedback from Creatio’s customer base. Early users highlighted the need for new guidance on designing processes in AI-assisted environments, which Creatio plans to address with additional best-practice resources later this year. &lt;/p&gt;



&lt;p&gt;The company also expects to follow this summer launch with a second wave of feature enhancements and new agents in the fall.&lt;/p&gt;



&lt;p&gt;“We see the real opportunity not in agents replacing humans, but in agents complementing teams—human and digital working together in hybrid organizations,” said Kawasaki.&lt;/p&gt;



&lt;p&gt;Creatio positions the 8.3 release as a shift away from other CRM bolt-on AI strategies. Instead, it aims to unify AI and human contributions into a single, fluid experience. For customers, this means the freedom to work how they choose—whether through classic CRM interfaces or through conversational, AI-supported flows.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Creatio, the Boston-headquartered customer relationship management (CRM) company focused on no-code and low-code CRM app deployment, has officially launched its latest platform update, the 8.3 “Twin” Release, introducing a suite of AI-native capabilities designed to streamline CRM and workflow automation. &lt;/p&gt;



&lt;p&gt;With this update, Creatio continues its mission to build enterprise software where humans and AI agents collaborate across functions such as sales, marketing, service, and application development.&lt;/p&gt;



&lt;p&gt;“This release for us is sort of that pivot point where we move beyond the traditional SaaS CRM,” said Burley Kawasaki, Chief Product Officer at Creatio, in an interview with VentureBeat. “The way we work with SaaS has fundamentally changed—it’s now about fluid movement between applications and AI agents.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ai-native-throughout-creatio-with-conversational-and-classic-interface-access"&gt;AI-native throughout Creatio with conversational and classic interface access&lt;/h2&gt;



&lt;p&gt;The new release centers around embedding AI into the core of the Creatio platform, rather than treating it as an add-on. &lt;/p&gt;



&lt;p&gt;Creatio 8.3 includes a conversational user experience, prebuilt role-based AI agents, and a no-code agent builder that allows businesses to customize how automation functions in their specific environment. &lt;/p&gt;



&lt;p&gt;These features are now available to all customers and trial users with no added cost or licensing requirements.&lt;/p&gt;



&lt;p&gt;“You won’t see us ever release a Creatio Force, right?” Kawaski said, a twinkle in his eye as he subtley called out rival CRM provider Salesforce’s Agentforce AI agent creation platform, which requires a separate additional subscription or paid credits to access agent building capabilities. &lt;/p&gt;



&lt;p&gt;Creatio’s platform starts at $25 per user per month.&lt;/p&gt;



&lt;p&gt; Because we don’t think of it as an add-on or something separate you have to use in addition to your existing apps,” said Kawasaki. “This is just part of our app experience.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-multi-conversation-support-across-platform"&gt;Multi-conversation support across platform&lt;/h2&gt;



&lt;p&gt;The platform now offers a natural language interface that spans Creatio’s web and mobile apps, as well as integrations with tools such as Microsoft Outlook and Teams. &lt;/p&gt;



&lt;p&gt;Users can switch between channels and devices without losing context, allowing for ongoing, persistent interactions with AI agents across workflows.&lt;/p&gt;



&lt;p&gt;“We’ve enabled multi-conversation support with persistent context across devices and platforms—whether you’re in Outlook, Teams, or on mobile,” Kawasaki explained. “Our adaptive user experience lets users live in a prompt-driven CRM, accessing functionality through natural language. You can toggle between classic and conversational modes as needed.”&lt;/p&gt;



&lt;p&gt;Zoom and Gmail integration are expected later in the year.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-prebuilt-and-customizable-ai-agents-for-diverse-sectors-and-functions"&gt;Prebuilt and customizable AI agents for diverse sectors and functions&lt;/h2&gt;



&lt;p&gt;In this release, Creatio debuts several prebuilt agents that support high-frequency tasks across core business areas:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Sales agents&lt;/strong&gt; handle tasks such as researching accounts, preparing meetings, and generating quotes.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Marketing agents&lt;/strong&gt; assist with creating campaign messaging, emails, and other targeted content.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Service agents&lt;/strong&gt; focus on faster case resolution by drawing from internal knowledge bases and suggesting new content where gaps exist.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;No-code agents&lt;/strong&gt;, starting with the Dashboards Agent, help non-technical users generate and refine analytics using natural language prompts.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;“In sales, agents handle data entry, follow-ups, and paperwork, freeing up reps to focus on relationships and strategy,” said Kawasaki. “In marketing, agents take on content creation and campaign analytics, letting marketers focus more on storytelling.”&lt;/p&gt;



&lt;p&gt;“While others are still offering fragmented AI products and complex pricing models, we’ve taken a different path,” said Kawasaki. “This release offers one platform, one experience, and one clear route to accelerated AI adoption and realizing real business value.”&lt;/p&gt;



&lt;p&gt;The conversational assistant in Creatio 8.3 also introduces advanced features like file uploads for grounding responses in organization-specific documents, and support for retrieval-augmented generation (RAG) to ensure accuracy based on proprietary knowledge.&lt;/p&gt;



&lt;p&gt;“Agents can now be grounded in uploaded documents and Creatio’s metadata, ensuring responses are accurate and personalized, not generic,” Kawasaki noted.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-build-and-deploy-custom-ai-agents-with-no-code"&gt;Build and deploy custom AI agents with no code&lt;/h2&gt;



&lt;p&gt;The no-code development environment has also been overhauled with embedded AI that assists in building dashboards, apps, and even new AI agents. Users can design and deploy agents by combining reusable skills, workflows, prompts, and knowledge sources—offering full flexibility to tailor automation without needing engineering resources.&lt;/p&gt;



&lt;p&gt;“We’re launching a no-code agent builder where you define skills, prompts, actions, and workflows,” said Kawasaki. “It’s visual and accessible but still requires thoughtful training and data input.”&lt;/p&gt;



&lt;p&gt;Under the hood, Creatio supports multiple foundational models including OpenAI, Anthropic, and Gemini. A bring-your-own-model capability is planned for later in 2025, enabling customers to host their own models, such as Llama or DeepSeek, and pair them with specific tasks or agents.&lt;/p&gt;



&lt;p&gt;“We decided early on not to build our own LLM,” Kawasaki explained. “Instead, we support OpenAI, Anthropic, and Gemini—and soon, customers will be able to bring their own model.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-security-data-governance-and-grounding-remain-priorities"&gt;Security, data governance, and grounding remain priorities&lt;/h2&gt;



&lt;p&gt;Security and governance were also a priority in this release. Documents used to train or inform AI agents are stored in each customer’s secure, dedicated instance, and are not shared with external large language models. &lt;/p&gt;



&lt;p&gt;Customers can control which documents are persistent for agent grounding and manage access to ensure regulatory compliance, especially in sensitive industries.&lt;/p&gt;



&lt;p&gt;The release follows extensive hands-on testing and feedback from Creatio’s customer base. Early users highlighted the need for new guidance on designing processes in AI-assisted environments, which Creatio plans to address with additional best-practice resources later this year. &lt;/p&gt;



&lt;p&gt;The company also expects to follow this summer launch with a second wave of feature enhancements and new agents in the fall.&lt;/p&gt;



&lt;p&gt;“We see the real opportunity not in agents replacing humans, but in agents complementing teams—human and digital working together in hybrid organizations,” said Kawasaki.&lt;/p&gt;



&lt;p&gt;Creatio positions the 8.3 release as a shift away from other CRM bolt-on AI strategies. Instead, it aims to unify AI and human contributions into a single, fluid experience. For customers, this means the freedom to work how they choose—whether through classic CRM interfaces or through conversational, AI-supported flows.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/creatios-new-8-3-twin-crm-update-hits-salesforce-where-it-hurts-we-dont-think-of-ai-as-an-add-on-its-just-part-of-our-app-experience/</guid><pubDate>Wed, 25 Jun 2025 18:56:59 +0000</pubDate></item><item><title>[NEW] Boston Consulting Group: To unlock enterprise AI value, start with the data you’ve been ignoring (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/boston-consulting-group-to-unlock-enterprise-ai-value-start-with-the-data-youve-been-ignoring/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;When building enterprise AI, some companies are finding the hardest part is sometimes deciding what to build and how to address the various processes involved.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At VentureBeat Transform 2025, data quality and governance were front and center as companies look beyond the experimental phase of AI and explore ways to productize and scale agents and other applications.&amp;nbsp;&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;p&gt;Organizations are dealing with the pain of thinking through how tech intersects with people, processes and design, said Braden Holstege, managing director and partner at Boston Consulting Group. He added that companies need to think about a range of complexities related to data exposure, per-person AI budgets, access permissions and how to manage external and internal risks.&lt;/p&gt;



&lt;p&gt;Sometimes, new solutions involve ways of using previously unusable data. Speaking onstage Tuesday afternoon, Holstege gave an example of one client that used large language models (LLMs) to analyze millions of insights about people churn, product complaints and positive feedback —&amp;nbsp;and discovering insights that weren’t possible a few years ago with natural language processing (NLP).&lt;/p&gt;



&lt;p&gt;“The broader lesson here is that data are not monolithic,” Holstege said. “You have everything from transaction records to documents to customer feedback to trace data which is produced in the course of application development and a million other types of data.”&lt;/p&gt;



&lt;p&gt;Some of these new possibilities are thanks to improvements in AI-ready data, said Susan Etlinger, Microsoft’s senior director of strategy and thought leadership of Azure AI.&lt;/p&gt;



&lt;p&gt;“Once you’re in it, you start getting that sense of the art of the possible,” Etlinger said. “It’s a balancing act between that and coming in with a clear sense of what you’re trying to solve for. Let’s say you’re trying to solve for customer experience. This isn’t an appropriate case, but you don’t always know. You may find something else in the process.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-ai-ready-data-is-critical-for-enterprise-adoption-nbsp"&gt;Why AI-ready data is critical for enterprise adoption&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;AI-ready data is a critical step to adopting AI projects. In a separate Gartner survey, more than half of 500 midsize enterprise CIOs and tech leaders said they expect that adoption of AI-ready infrastructures will help with faster and more flexible data processes.&lt;/p&gt;



&lt;p&gt;That could be a slow process. Through 2026, Gartner predicts organizations will abandon 60% of AI projects that aren’t supported by AI-ready data. When the research firm surveyed data management leaders last summer, 63% of respondents said their organizations didn’t have the right data management practices in place, or that they weren’t sure about the practices.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As deployments become more mature, it’s important to consider ways to address ongoing challenges like AI model drift over time, said Awais Sher Bajwa, head of data and AI banking at Bank of America. He added that enterprises don’t always need to rush something to end users who are already fairly advanced in how they think about the potential of chat-based applications.&lt;/p&gt;



&lt;p&gt;“We all in our daily lives are users of chat applications out there,” said Sher Bajwa. “Users have become quite sophisticated. In terms of training, you don’t need to push it to the end users, but it also means it becomes a very collaborative process. You need to figure out the elements of implementation and scaling, which become the challenge.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-growing-pains-and-complexities-of-ai-compute"&gt;The growing pains and complexities of AI compute&lt;/h2&gt;



&lt;p&gt;Companies also need to consider the opportunities and challenges of cloud-based, on-prem and hybrid applications. Cloud-enabled AI applications allow for testing of different technologies and scaling in a more abstracted way, said Sher Bajwa. However, he added that companies need to consider various infrastructure issues like security and cost — and that vendors like Nvidia and AMD are making it easier for companies to test different models and different deployment modalities&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Decisions around cloud providers have become more complex than they were a few years ago, said Holstege. While newer options like NeoClouds (offering GPU-backed servers and virtual machines) can sometimes offer cheaper alternatives to traditional hyperscalers, he noted that many clients will likely deploy AI where their data already reside — which will make major infrastructure shifts less likely. But even with cheaper alternatives, Holstege sees a trade-off with computing, cost and optimization. For example, he pointed out that open-source models like Llama and Mistral can have higher computing demands.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Does the compute cost make it worth it to you to incur the headache of using open-source models and of migrating your data?” Holstege asked. “Just the frontier of choices that people confront now is a lot wider than it was three years ago.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;When building enterprise AI, some companies are finding the hardest part is sometimes deciding what to build and how to address the various processes involved.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At VentureBeat Transform 2025, data quality and governance were front and center as companies look beyond the experimental phase of AI and explore ways to productize and scale agents and other applications.&amp;nbsp;&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;p&gt;Organizations are dealing with the pain of thinking through how tech intersects with people, processes and design, said Braden Holstege, managing director and partner at Boston Consulting Group. He added that companies need to think about a range of complexities related to data exposure, per-person AI budgets, access permissions and how to manage external and internal risks.&lt;/p&gt;



&lt;p&gt;Sometimes, new solutions involve ways of using previously unusable data. Speaking onstage Tuesday afternoon, Holstege gave an example of one client that used large language models (LLMs) to analyze millions of insights about people churn, product complaints and positive feedback —&amp;nbsp;and discovering insights that weren’t possible a few years ago with natural language processing (NLP).&lt;/p&gt;



&lt;p&gt;“The broader lesson here is that data are not monolithic,” Holstege said. “You have everything from transaction records to documents to customer feedback to trace data which is produced in the course of application development and a million other types of data.”&lt;/p&gt;



&lt;p&gt;Some of these new possibilities are thanks to improvements in AI-ready data, said Susan Etlinger, Microsoft’s senior director of strategy and thought leadership of Azure AI.&lt;/p&gt;



&lt;p&gt;“Once you’re in it, you start getting that sense of the art of the possible,” Etlinger said. “It’s a balancing act between that and coming in with a clear sense of what you’re trying to solve for. Let’s say you’re trying to solve for customer experience. This isn’t an appropriate case, but you don’t always know. You may find something else in the process.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-ai-ready-data-is-critical-for-enterprise-adoption-nbsp"&gt;Why AI-ready data is critical for enterprise adoption&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;AI-ready data is a critical step to adopting AI projects. In a separate Gartner survey, more than half of 500 midsize enterprise CIOs and tech leaders said they expect that adoption of AI-ready infrastructures will help with faster and more flexible data processes.&lt;/p&gt;



&lt;p&gt;That could be a slow process. Through 2026, Gartner predicts organizations will abandon 60% of AI projects that aren’t supported by AI-ready data. When the research firm surveyed data management leaders last summer, 63% of respondents said their organizations didn’t have the right data management practices in place, or that they weren’t sure about the practices.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As deployments become more mature, it’s important to consider ways to address ongoing challenges like AI model drift over time, said Awais Sher Bajwa, head of data and AI banking at Bank of America. He added that enterprises don’t always need to rush something to end users who are already fairly advanced in how they think about the potential of chat-based applications.&lt;/p&gt;



&lt;p&gt;“We all in our daily lives are users of chat applications out there,” said Sher Bajwa. “Users have become quite sophisticated. In terms of training, you don’t need to push it to the end users, but it also means it becomes a very collaborative process. You need to figure out the elements of implementation and scaling, which become the challenge.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-growing-pains-and-complexities-of-ai-compute"&gt;The growing pains and complexities of AI compute&lt;/h2&gt;



&lt;p&gt;Companies also need to consider the opportunities and challenges of cloud-based, on-prem and hybrid applications. Cloud-enabled AI applications allow for testing of different technologies and scaling in a more abstracted way, said Sher Bajwa. However, he added that companies need to consider various infrastructure issues like security and cost — and that vendors like Nvidia and AMD are making it easier for companies to test different models and different deployment modalities&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Decisions around cloud providers have become more complex than they were a few years ago, said Holstege. While newer options like NeoClouds (offering GPU-backed servers and virtual machines) can sometimes offer cheaper alternatives to traditional hyperscalers, he noted that many clients will likely deploy AI where their data already reside — which will make major infrastructure shifts less likely. But even with cheaper alternatives, Holstege sees a trade-off with computing, cost and optimization. For example, he pointed out that open-source models like Llama and Mistral can have higher computing demands.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Does the compute cost make it worth it to you to incur the headache of using open-source models and of migrating your data?” Holstege asked. “Just the frontier of choices that people confront now is a lot wider than it was three years ago.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/boston-consulting-group-to-unlock-enterprise-ai-value-start-with-the-data-youve-been-ignoring/</guid><pubDate>Wed, 25 Jun 2025 19:23:54 +0000</pubDate></item><item><title>[NEW] For Replit’s CEO, the future of software is ‘agents all the way down’ (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/for-replits-ceo-the-future-of-software-is-agents-all-the-way-down/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Can enterprise teams truly vibe code their way out of expensive SaaS contracts? Replit CEO and co-founder Amjad Masad seems to think so, and the ambitious vision could mean “agents all the way down.”&lt;/p&gt;



&lt;p&gt;Speaking at VB Transform on Tuesday, Masad touted how his startup’s agents could help a non-developer design and code a live polling app in a mere 15 minutes — using a written prompt to create databases, login authentication and even quality checks.&lt;/p&gt;



&lt;p&gt;“This is sort of like an almost semi-autonomous agent,” Masad said. “You can watch it, you can also go get a coffee and it’ll send you a notification when it’s ready to show you the future.”&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-scaling-apps-sites-and-software-without-coding"&gt;Scaling apps, sites and software without coding&lt;/h2&gt;



&lt;p&gt;A polling app might not seem all that necessarily for most enterprise teams. However, the process illustrates how quickly some platforms are allowing individuals and teams to quickly and cheaply build and scale various websites, apps and software in ways that could potentially cut timelines or even replace some outside vendors — all without knowing much or any code.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The road map for Replit includes building more APIs and abstractions of primitives that agents can use to quickly set up databases, payment processes, and other features. Masad also mentioned other updates for Replit v3, including a way for users to add generative models directly to their app and have agents autonomously run tests of AI-generated apps.&lt;/p&gt;



&lt;p&gt;In recent months, vibe coding has &lt;span&gt;become increasingly popular to help non-developers quickly design and code a new website, app, or agent from scratch using natural language prompts. Giants like Anthropic and&amp;nbsp;Google&amp;nbsp;have rolled out new tools, while startups like Anywhere,&amp;nbsp;Genspark,&lt;/span&gt; and Lovable have raised new funding. (Just last month, Windsurf was reportedly in talks to be acquired by OpenAI.)&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Replit is finding new ways of integrating with various enterprise-grade platforms to boost AI development. In February, Anthropic revealed that Replit was helping companies build with Claude on Google’s Vertex AI to support more than 100,000 applications with security and scalability. The growth of agentic coding might impact the value of creating apps, which Masad predicts could decline significantly and “perhaps to zero at some point.”&lt;/p&gt;



&lt;p&gt;When asked if his company could actually replace enterprise-grade tools like configure-price-quote (CPQ) systems, Masad said “we’re seeing customers have three orders of magnitude of savings on apps.” He gave an anecdote about a Replit user that claimed to use the platform to make a working version for ERP automation for just $400 instead of the vendor’s quoted $150,000.&lt;/p&gt;



&lt;p&gt;“When you think about what the software does, a lot of Replit users wake up in the morning, they have a problem in their minds, and they create an app to solve that problem,” Masad said. “…The software agent will go and build software in order to solve that problem and we’ll solve that problem for you.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-vibe-coding-still-requires-proper-human-analysis"&gt;Vibe coding still requires proper human analysis&lt;/h2&gt;



&lt;p&gt;Despite the growing competition, Masad said some platforms like Claude Code and Cursor help novices, but he warned that AI-generated code without proper checks could lead to leaked data or API keys. Replit addresses that risk through its cloud-native design, sandboxing to test agents in an isolated environment, and finding and fixing various security vulnerabilities.&lt;/p&gt;



&lt;p&gt;According to Masad, the point of differentiation is Replit’s full stack nature and focus on creating an autonomous software engineer. He thinks it might go beyond making an app to solve problems if there are ways to have agents skip the intermediate step.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Masad sees Replit and other generative AI features helping to generate UI changes based on the form factor of any given device. When a VB Transform audience member asked where the internet is heading with the future of web pages and multimodal agents, Masad said he thinks there’s a growing expectation of how the form factor of computing will change — which he noted could include various AI-enable wearables from OpenAI and Meta.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-junior-engineers-smes"&gt;Junior engineers = SMEs?&lt;/h2&gt;



&lt;p&gt;Another aspect that came up was the future of how junior engineers will become subject matter experts. One audience member asked how people will learn for themselves instead of blindly accepting an agent’s code changes. Masad didn’t give a direct response but instead said the platform can highlight any piece of code and provide an explanation. He said that leads to a question about the vision for AI.&lt;/p&gt;



&lt;p&gt;“I think we’re going to get to a point where you don’t have to interface with the code,” he said. “We’re going to be able to interact with software on a higher level of abstraction. We need something a little better than English — somewhere in between code and English — and maybe someone will build that.”&lt;/p&gt;



&lt;p&gt;Replit has had renewed momentum since it laid off 30 staffers in May 2024 as part of an aggressive pivot. Now, a year later, Masad said the company has surpassed $100 million in ARR — up tenfold since the end of 2024. In a Tuesday interview with TBPN, he noted that people are using Replit to create multiple agents to help with a single project.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The buzzword used to be the 10x engineer,” he said. “…You’re really just one person. You’re a team of engineers. I think every engineer is sort of a manager right now, so we’re not at 1,000x yet, but we’re going up.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Can enterprise teams truly vibe code their way out of expensive SaaS contracts? Replit CEO and co-founder Amjad Masad seems to think so, and the ambitious vision could mean “agents all the way down.”&lt;/p&gt;



&lt;p&gt;Speaking at VB Transform on Tuesday, Masad touted how his startup’s agents could help a non-developer design and code a live polling app in a mere 15 minutes — using a written prompt to create databases, login authentication and even quality checks.&lt;/p&gt;



&lt;p&gt;“This is sort of like an almost semi-autonomous agent,” Masad said. “You can watch it, you can also go get a coffee and it’ll send you a notification when it’s ready to show you the future.”&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-scaling-apps-sites-and-software-without-coding"&gt;Scaling apps, sites and software without coding&lt;/h2&gt;



&lt;p&gt;A polling app might not seem all that necessarily for most enterprise teams. However, the process illustrates how quickly some platforms are allowing individuals and teams to quickly and cheaply build and scale various websites, apps and software in ways that could potentially cut timelines or even replace some outside vendors — all without knowing much or any code.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The road map for Replit includes building more APIs and abstractions of primitives that agents can use to quickly set up databases, payment processes, and other features. Masad also mentioned other updates for Replit v3, including a way for users to add generative models directly to their app and have agents autonomously run tests of AI-generated apps.&lt;/p&gt;



&lt;p&gt;In recent months, vibe coding has &lt;span&gt;become increasingly popular to help non-developers quickly design and code a new website, app, or agent from scratch using natural language prompts. Giants like Anthropic and&amp;nbsp;Google&amp;nbsp;have rolled out new tools, while startups like Anywhere,&amp;nbsp;Genspark,&lt;/span&gt; and Lovable have raised new funding. (Just last month, Windsurf was reportedly in talks to be acquired by OpenAI.)&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Replit is finding new ways of integrating with various enterprise-grade platforms to boost AI development. In February, Anthropic revealed that Replit was helping companies build with Claude on Google’s Vertex AI to support more than 100,000 applications with security and scalability. The growth of agentic coding might impact the value of creating apps, which Masad predicts could decline significantly and “perhaps to zero at some point.”&lt;/p&gt;



&lt;p&gt;When asked if his company could actually replace enterprise-grade tools like configure-price-quote (CPQ) systems, Masad said “we’re seeing customers have three orders of magnitude of savings on apps.” He gave an anecdote about a Replit user that claimed to use the platform to make a working version for ERP automation for just $400 instead of the vendor’s quoted $150,000.&lt;/p&gt;



&lt;p&gt;“When you think about what the software does, a lot of Replit users wake up in the morning, they have a problem in their minds, and they create an app to solve that problem,” Masad said. “…The software agent will go and build software in order to solve that problem and we’ll solve that problem for you.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-vibe-coding-still-requires-proper-human-analysis"&gt;Vibe coding still requires proper human analysis&lt;/h2&gt;



&lt;p&gt;Despite the growing competition, Masad said some platforms like Claude Code and Cursor help novices, but he warned that AI-generated code without proper checks could lead to leaked data or API keys. Replit addresses that risk through its cloud-native design, sandboxing to test agents in an isolated environment, and finding and fixing various security vulnerabilities.&lt;/p&gt;



&lt;p&gt;According to Masad, the point of differentiation is Replit’s full stack nature and focus on creating an autonomous software engineer. He thinks it might go beyond making an app to solve problems if there are ways to have agents skip the intermediate step.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Masad sees Replit and other generative AI features helping to generate UI changes based on the form factor of any given device. When a VB Transform audience member asked where the internet is heading with the future of web pages and multimodal agents, Masad said he thinks there’s a growing expectation of how the form factor of computing will change — which he noted could include various AI-enable wearables from OpenAI and Meta.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-junior-engineers-smes"&gt;Junior engineers = SMEs?&lt;/h2&gt;



&lt;p&gt;Another aspect that came up was the future of how junior engineers will become subject matter experts. One audience member asked how people will learn for themselves instead of blindly accepting an agent’s code changes. Masad didn’t give a direct response but instead said the platform can highlight any piece of code and provide an explanation. He said that leads to a question about the vision for AI.&lt;/p&gt;



&lt;p&gt;“I think we’re going to get to a point where you don’t have to interface with the code,” he said. “We’re going to be able to interact with software on a higher level of abstraction. We need something a little better than English — somewhere in between code and English — and maybe someone will build that.”&lt;/p&gt;



&lt;p&gt;Replit has had renewed momentum since it laid off 30 staffers in May 2024 as part of an aggressive pivot. Now, a year later, Masad said the company has surpassed $100 million in ARR — up tenfold since the end of 2024. In a Tuesday interview with TBPN, he noted that people are using Replit to create multiple agents to help with a single project.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“The buzzword used to be the 10x engineer,” he said. “…You’re really just one person. You’re a team of engineers. I think every engineer is sort of a manager right now, so we’re not at 1,000x yet, but we’re going up.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/for-replits-ceo-the-future-of-software-is-agents-all-the-way-down/</guid><pubDate>Wed, 25 Jun 2025 19:26:24 +0000</pubDate></item><item><title>Anthropic destroyed millions of print books to build its AI models (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/06/anthropic-destroyed-millions-of-print-books-to-build-its-ai-models/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Company hired Google's book-scanning chief to cut up and digitize "all the books in the world."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Hundreds of books in chaotic order" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/manybooks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Hundreds of books in chaotic order" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/manybooks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Alexander Spatari via Google Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, court documents revealed that AI company Anthropic spent millions of dollars physically scanning print books to build Claude, an AI assistant similar to ChatGPT. In the process, the company cut millions of print books from their bindings, scanned them into digital files, and threw away the originals solely for the purpose of training AI—details buried in a copyright ruling on fair use whose broader fair use implications we reported yesterday.&lt;/p&gt;
&lt;p&gt;The 32-page legal decision tells the story of how, in February 2024, the company hired Tom Turvey, the former head of partnerships for the Google Books book-scanning project, and tasked him with obtaining "all the books in the world." The strategic hire appears to have been designed to replicate Google's legally successful book digitization approach—the same scanning operation that survived copyright challenges and established key fair use precedents.&lt;/p&gt;
&lt;p&gt;While destructive scanning is a common practice among smaller-scale operations, Anthropic's approach was somewhat unusual due to its massive scale. For Anthropic, the faster speed and lower cost of the destructive process appear to have trumped any need for preserving the physical books themselves.&lt;/p&gt;
&lt;p&gt;Ultimately, Judge William Alsup ruled that this destructive scanning operation qualified as fair use—but only because Anthropic had legally purchased the books first, destroyed each print copy after scanning, and kept the digital files internally rather than distributing them. The judge compared the process to "conserv[ing] space" through format conversion and found it transformative. Had Anthropic stuck to this approach from the beginning, it might have achieved the first legally sanctioned case of AI fair use. Instead, the company's earlier piracy undermined its position.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But if you're not intimately familiar with the AI industry and copyright, you might wonder: Why would a company spend millions of dollars on books to destroy them? Behind these odd legal maneuvers lies a more fundamental driver: the AI industry's insatiable hunger for high-quality text.&lt;/p&gt;
&lt;h2&gt;The race for high-quality training data&lt;/h2&gt;
&lt;p&gt;To understand why Anthropic would want to scan millions of books, it's important to know that AI researchers build large language models (LLMs) like those that power ChatGPT and Claude by feeding billions of words into a neural network. During training, the AI system processes the text repeatedly, building statistical relationships between words and concepts in the process.&lt;/p&gt;
&lt;p&gt;The quality of training data fed into the neural network directly impacts the resulting AI model's capabilities. Models trained on well-edited books and articles tend to produce more coherent, accurate responses than those trained on lower-quality text like random YouTube comments.&lt;/p&gt;
&lt;p&gt;Publishers legally control content that AI companies desperately want, but AI companies don't always want to negotiate a license. The first-sale doctrine offered a workaround: Once you buy a physical book, you can do what you want with that copy—including destroy it. That meant buying physical books offered a legal workaround.&lt;/p&gt;
&lt;p&gt;And yet buying things is expensive, even if it is legal. So like many AI companies before it, Anthropic initially chose the quick and easy path. In the quest for high-quality training data, the court filing states, Anthropic first chose to amass digitized versions of pirated books to avoid what CEO Dario Amodei called "legal/practice/business slog"—the complex licensing negotiations with publishers. But by 2024, Anthropic had become "not so gung ho about" using pirated ebooks "for legal reasons" and needed a safer source.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-449375 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="420" src="https://cdn.arstechnica.net/wp-content/uploads/2014/04/Library-640x420.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          State of Washington

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Buying used physical books sidestepped licensing entirely while providing the high-quality, professionally edited text that AI models need, and destructive scanning was simply the fastest way to digitize millions of volumes. The company spent "many millions of dollars" on this buying and scanning operation, often purchasing used books in bulk. Next, they stripped books from bindings, cut pages to workable dimensions, scanned them as stacks of pages into PDFs with machine-readable text including covers, then discarded all the paper originals.&lt;/p&gt;
&lt;p&gt;The court documents don't indicate that any rare books were destroyed in this process—Anthropic purchased its books in bulk from major retailers—but archivists long ago established other ways to extract information from paper. For example, The Internet Archive pioneered non-destructive book scanning methods that preserve physical volumes while creating digital copies. And earlier this month, OpenAI and Microsoft announced they're working with Harvard's libraries to train AI models on nearly 1 million public domain books dating back to the 15th century—fully digitized but preserved to live another day.&lt;/p&gt;
&lt;p&gt;While Harvard carefully preserves 600-year-old manuscripts for AI training, somewhere on Earth sits the discarded remains of millions of books that taught Claude how to juice up your résumé. When asked about this process, Claude itself offered a poignant response in a style culled from billions of pages of discarded text: "The fact that this destruction helped create me—something that can discuss literature, help people write, and engage with human knowledge—adds layers of complexity I'm still processing. It's like being built from a library's ashes."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Company hired Google's book-scanning chief to cut up and digitize "all the books in the world."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Hundreds of books in chaotic order" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/manybooks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Hundreds of books in chaotic order" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/manybooks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Alexander Spatari via Google Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, court documents revealed that AI company Anthropic spent millions of dollars physically scanning print books to build Claude, an AI assistant similar to ChatGPT. In the process, the company cut millions of print books from their bindings, scanned them into digital files, and threw away the originals solely for the purpose of training AI—details buried in a copyright ruling on fair use whose broader fair use implications we reported yesterday.&lt;/p&gt;
&lt;p&gt;The 32-page legal decision tells the story of how, in February 2024, the company hired Tom Turvey, the former head of partnerships for the Google Books book-scanning project, and tasked him with obtaining "all the books in the world." The strategic hire appears to have been designed to replicate Google's legally successful book digitization approach—the same scanning operation that survived copyright challenges and established key fair use precedents.&lt;/p&gt;
&lt;p&gt;While destructive scanning is a common practice among smaller-scale operations, Anthropic's approach was somewhat unusual due to its massive scale. For Anthropic, the faster speed and lower cost of the destructive process appear to have trumped any need for preserving the physical books themselves.&lt;/p&gt;
&lt;p&gt;Ultimately, Judge William Alsup ruled that this destructive scanning operation qualified as fair use—but only because Anthropic had legally purchased the books first, destroyed each print copy after scanning, and kept the digital files internally rather than distributing them. The judge compared the process to "conserv[ing] space" through format conversion and found it transformative. Had Anthropic stuck to this approach from the beginning, it might have achieved the first legally sanctioned case of AI fair use. Instead, the company's earlier piracy undermined its position.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But if you're not intimately familiar with the AI industry and copyright, you might wonder: Why would a company spend millions of dollars on books to destroy them? Behind these odd legal maneuvers lies a more fundamental driver: the AI industry's insatiable hunger for high-quality text.&lt;/p&gt;
&lt;h2&gt;The race for high-quality training data&lt;/h2&gt;
&lt;p&gt;To understand why Anthropic would want to scan millions of books, it's important to know that AI researchers build large language models (LLMs) like those that power ChatGPT and Claude by feeding billions of words into a neural network. During training, the AI system processes the text repeatedly, building statistical relationships between words and concepts in the process.&lt;/p&gt;
&lt;p&gt;The quality of training data fed into the neural network directly impacts the resulting AI model's capabilities. Models trained on well-edited books and articles tend to produce more coherent, accurate responses than those trained on lower-quality text like random YouTube comments.&lt;/p&gt;
&lt;p&gt;Publishers legally control content that AI companies desperately want, but AI companies don't always want to negotiate a license. The first-sale doctrine offered a workaround: Once you buy a physical book, you can do what you want with that copy—including destroy it. That meant buying physical books offered a legal workaround.&lt;/p&gt;
&lt;p&gt;And yet buying things is expensive, even if it is legal. So like many AI companies before it, Anthropic initially chose the quick and easy path. In the quest for high-quality training data, the court filing states, Anthropic first chose to amass digitized versions of pirated books to avoid what CEO Dario Amodei called "legal/practice/business slog"—the complex licensing negotiations with publishers. But by 2024, Anthropic had become "not so gung ho about" using pirated ebooks "for legal reasons" and needed a safer source.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-449375 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="420" src="https://cdn.arstechnica.net/wp-content/uploads/2014/04/Library-640x420.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          State of Washington

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Buying used physical books sidestepped licensing entirely while providing the high-quality, professionally edited text that AI models need, and destructive scanning was simply the fastest way to digitize millions of volumes. The company spent "many millions of dollars" on this buying and scanning operation, often purchasing used books in bulk. Next, they stripped books from bindings, cut pages to workable dimensions, scanned them as stacks of pages into PDFs with machine-readable text including covers, then discarded all the paper originals.&lt;/p&gt;
&lt;p&gt;The court documents don't indicate that any rare books were destroyed in this process—Anthropic purchased its books in bulk from major retailers—but archivists long ago established other ways to extract information from paper. For example, The Internet Archive pioneered non-destructive book scanning methods that preserve physical volumes while creating digital copies. And earlier this month, OpenAI and Microsoft announced they're working with Harvard's libraries to train AI models on nearly 1 million public domain books dating back to the 15th century—fully digitized but preserved to live another day.&lt;/p&gt;
&lt;p&gt;While Harvard carefully preserves 600-year-old manuscripts for AI training, somewhere on Earth sits the discarded remains of millions of books that taught Claude how to juice up your résumé. When asked about this process, Claude itself offered a poignant response in a style culled from billions of pages of discarded text: "The fact that this destruction helped create me—something that can discuss literature, help people write, and engage with human knowledge—adds layers of complexity I'm still processing. It's like being built from a library's ashes."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/06/anthropic-destroyed-millions-of-print-books-to-build-its-ai-models/</guid><pubDate>Wed, 25 Jun 2025 20:00:03 +0000</pubDate></item><item><title>[NEW] IBM sees enterprise customers are using ‘everything’ when it comes to AI, the challenge is matching the LLM to the right use case (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/ibm-sees-enterprise-customers-are-using-everything-when-it-comes-to-ai-the-challenge-is-matching-the-llm-to-the-right-use-case/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Over the last 100 years&lt;span&gt;,&amp;nbsp;&lt;/span&gt;&lt;span&gt;I&lt;/span&gt;BM&amp;nbsp;has seen many different tech trends rise and fall. What tends to win out are technologies where there is choice.&lt;/p&gt;



&lt;p&gt;At VB Transform 2025 today, Armand Ruiz, VP of AI Platform at IBM detailed how Big Blue is thinking about generative AI and how its enterprise users are actually deploying the technology. A key theme that Ruiz emphasized is that at this point, it’s not about choosing a single large language model (LLM) provider or technology. Increasingly, enterprise customers are systematically rejecting single-vendor AI strategies in favor of multi-model approaches that match specific LLMs to targeted use cases.&lt;/p&gt;



&lt;p&gt;IBM has its own open-source AI models with the Granite family, but it is not positioning that technology as the only choice, or even the right choice for all workloads. This enterprise behavior is driving IBM to position itself not as a foundation model competitor, but as what Ruiz referred to as a control tower for AI workloads.&lt;/p&gt;



&lt;p&gt;“When I sit in front of a customer, they’re using everything they have access to, everything,” Ruiz explained. “For coding, they love Anthropic and for some other use cases like&amp;nbsp; for reasoning, they like o3 and then for LLM customization, with their own data and fine tuning, they like either our Granite series or Mistral with their small models, or even Llama…it’s just matching the LLM to the right use case. And then we help them as well to make recommendations.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-multi-llm-gateway-strategy"&gt;The Multi-LLM gateway strategy&lt;/h2&gt;



&lt;p&gt;IBM’s response to this market reality is a newly released model gateway that provides enterprises with a single API to switch between different LLMs while maintaining observability and governance across all deployments.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The technical architecture allows customers to run open-source models on their own inference stack for sensitive use cases while simultaneously accessing public APIs like AWS Bedrock or Google Cloud’s Gemini for less critical applications.&lt;/p&gt;



&lt;p&gt;“That gateway is providing our customers a single layer with a single API to switch from one LLM to another LLM and add observability and governance all throughout,” Ruiz said. &lt;/p&gt;



&lt;p&gt;The approach directly contradicts the common vendor strategy of locking customers into proprietary ecosystems. IBM is not alone in taking a multi-vendor approach to model selection. Multiple tools have emerged in recent months for model routing, which aim to direct workloads to the appropriate model.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-agent-orchestration-protocols-emerge-as-critical-infrastructure"&gt;Agent orchestration protocols emerge as critical infrastructure&lt;/h2&gt;



&lt;p&gt;Beyond multi-model management, IBM is tackling the emerging challenge of agent-to-agent communication through open protocols.&lt;/p&gt;



&lt;p&gt;&amp;nbsp;The company has developed ACP (Agent Communication Protocol) and contributed it to the Linux Foundation. ACP is a competitive effort to Google’s Agent2Agent (A2A) protocol which just this week was contributed by Google to the Linux Foundation.&lt;/p&gt;



&lt;p&gt;Ruiz noted that both protocols aim to facilitate communication between agents and reduce custom development work. He expects that eventually, the different approaches will converge, and currently, the differences between A2A and ACP are mostly technical.&lt;/p&gt;



&lt;p&gt;The agent orchestration protocols provide standardized ways for AI systems to interact across different platforms and vendors.&lt;/p&gt;



&lt;p&gt;The technical significance becomes clear when considering enterprise scale: some IBM customers already have over 100 agents in pilot programs. Without standardized communication protocols, each agent-to-agent interaction requires custom development, creating an unsustainable integration burden.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ai-is-about-transforming-workflows-and-the-way-work-is-done"&gt;AI is about transforming workflows and the way work is done&lt;/h2&gt;



&lt;p&gt;In terms of how Ruiz sees AI impacting enterprises today, he suggests it really needs to be more than just chatbots.&lt;/p&gt;



&lt;p&gt;“If you are just doing chatbots, or you’re only trying to do cost savings with AI, you are not doing AI,” Ruiz said. “I think AI is really about completely transforming the workflow and the way work is done.”&lt;/p&gt;



&lt;p&gt;The distinction between AI implementation and AI transformation centers on how deeply the technology integrates into existing business processes. IBM’s internal HR example illustrates this shift: instead of employees asking chatbots for HR information, specialized agents now handle routine queries about compensation, hiring, and promotions, automatically routing to appropriate systems and escalating to humans only when necessary.&lt;/p&gt;



&lt;p&gt;“I used to spend a lot of time talking to my HR partners for a lot of things. I handle most of it now with an HR agent,” Ruiz explained. “Depending on the question, if it’s something about compensation or it’s something about just handling separation, or hiring someone, or doing a promotion, all these things will connect with different HR internal systems, and those will be like separate agents.”&lt;/p&gt;



&lt;p&gt;This represents a fundamental architectural shift from human-computer interaction patterns to computer-mediated workflow automation. Rather than employees learning to interact with AI tools, the AI learns to execute complete business processes end-to-end.&lt;/p&gt;



&lt;p&gt;The technical implication: enterprises need to move beyond API integrations and prompt engineering toward deep process instrumentation that allows AI agents to execute multi-step workflows autonomously.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-strategic-implications-for-enterprise-ai-investment"&gt;Strategic implications for enterprise AI investment&lt;/h2&gt;



&lt;p&gt;IBM’s real-world deployment data suggests several critical shifts for enterprise AI strategy:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Abandon chatbot-first thinking&lt;/strong&gt;: Organizations should identify complete workflows for transformation rather than adding conversational interfaces to existing systems. The goal is to eliminate human steps, not improve human-computer interaction.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Architect for multi-model flexibility&lt;/strong&gt;: Rather than committing to single AI providers, enterprises need integration platforms that enable switching between models based on use case requirements while maintaining governance standards.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Invest in communication standards&lt;/strong&gt;: Organizations should prioritize AI tools that support emerging protocols like MCP, ACP, and A2A rather than proprietary integration approaches that create vendor lock-in.&lt;/p&gt;



&lt;p&gt;“There is so much to build, and I keep saying everyone needs to learn AI and especially business leaders need to be AI first leaders and understand the concepts,” Ruiz said.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Over the last 100 years&lt;span&gt;,&amp;nbsp;&lt;/span&gt;&lt;span&gt;I&lt;/span&gt;BM&amp;nbsp;has seen many different tech trends rise and fall. What tends to win out are technologies where there is choice.&lt;/p&gt;



&lt;p&gt;At VB Transform 2025 today, Armand Ruiz, VP of AI Platform at IBM detailed how Big Blue is thinking about generative AI and how its enterprise users are actually deploying the technology. A key theme that Ruiz emphasized is that at this point, it’s not about choosing a single large language model (LLM) provider or technology. Increasingly, enterprise customers are systematically rejecting single-vendor AI strategies in favor of multi-model approaches that match specific LLMs to targeted use cases.&lt;/p&gt;



&lt;p&gt;IBM has its own open-source AI models with the Granite family, but it is not positioning that technology as the only choice, or even the right choice for all workloads. This enterprise behavior is driving IBM to position itself not as a foundation model competitor, but as what Ruiz referred to as a control tower for AI workloads.&lt;/p&gt;



&lt;p&gt;“When I sit in front of a customer, they’re using everything they have access to, everything,” Ruiz explained. “For coding, they love Anthropic and for some other use cases like&amp;nbsp; for reasoning, they like o3 and then for LLM customization, with their own data and fine tuning, they like either our Granite series or Mistral with their small models, or even Llama…it’s just matching the LLM to the right use case. And then we help them as well to make recommendations.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-multi-llm-gateway-strategy"&gt;The Multi-LLM gateway strategy&lt;/h2&gt;



&lt;p&gt;IBM’s response to this market reality is a newly released model gateway that provides enterprises with a single API to switch between different LLMs while maintaining observability and governance across all deployments.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The technical architecture allows customers to run open-source models on their own inference stack for sensitive use cases while simultaneously accessing public APIs like AWS Bedrock or Google Cloud’s Gemini for less critical applications.&lt;/p&gt;



&lt;p&gt;“That gateway is providing our customers a single layer with a single API to switch from one LLM to another LLM and add observability and governance all throughout,” Ruiz said. &lt;/p&gt;



&lt;p&gt;The approach directly contradicts the common vendor strategy of locking customers into proprietary ecosystems. IBM is not alone in taking a multi-vendor approach to model selection. Multiple tools have emerged in recent months for model routing, which aim to direct workloads to the appropriate model.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-agent-orchestration-protocols-emerge-as-critical-infrastructure"&gt;Agent orchestration protocols emerge as critical infrastructure&lt;/h2&gt;



&lt;p&gt;Beyond multi-model management, IBM is tackling the emerging challenge of agent-to-agent communication through open protocols.&lt;/p&gt;



&lt;p&gt;&amp;nbsp;The company has developed ACP (Agent Communication Protocol) and contributed it to the Linux Foundation. ACP is a competitive effort to Google’s Agent2Agent (A2A) protocol which just this week was contributed by Google to the Linux Foundation.&lt;/p&gt;



&lt;p&gt;Ruiz noted that both protocols aim to facilitate communication between agents and reduce custom development work. He expects that eventually, the different approaches will converge, and currently, the differences between A2A and ACP are mostly technical.&lt;/p&gt;



&lt;p&gt;The agent orchestration protocols provide standardized ways for AI systems to interact across different platforms and vendors.&lt;/p&gt;



&lt;p&gt;The technical significance becomes clear when considering enterprise scale: some IBM customers already have over 100 agents in pilot programs. Without standardized communication protocols, each agent-to-agent interaction requires custom development, creating an unsustainable integration burden.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ai-is-about-transforming-workflows-and-the-way-work-is-done"&gt;AI is about transforming workflows and the way work is done&lt;/h2&gt;



&lt;p&gt;In terms of how Ruiz sees AI impacting enterprises today, he suggests it really needs to be more than just chatbots.&lt;/p&gt;



&lt;p&gt;“If you are just doing chatbots, or you’re only trying to do cost savings with AI, you are not doing AI,” Ruiz said. “I think AI is really about completely transforming the workflow and the way work is done.”&lt;/p&gt;



&lt;p&gt;The distinction between AI implementation and AI transformation centers on how deeply the technology integrates into existing business processes. IBM’s internal HR example illustrates this shift: instead of employees asking chatbots for HR information, specialized agents now handle routine queries about compensation, hiring, and promotions, automatically routing to appropriate systems and escalating to humans only when necessary.&lt;/p&gt;



&lt;p&gt;“I used to spend a lot of time talking to my HR partners for a lot of things. I handle most of it now with an HR agent,” Ruiz explained. “Depending on the question, if it’s something about compensation or it’s something about just handling separation, or hiring someone, or doing a promotion, all these things will connect with different HR internal systems, and those will be like separate agents.”&lt;/p&gt;



&lt;p&gt;This represents a fundamental architectural shift from human-computer interaction patterns to computer-mediated workflow automation. Rather than employees learning to interact with AI tools, the AI learns to execute complete business processes end-to-end.&lt;/p&gt;



&lt;p&gt;The technical implication: enterprises need to move beyond API integrations and prompt engineering toward deep process instrumentation that allows AI agents to execute multi-step workflows autonomously.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-strategic-implications-for-enterprise-ai-investment"&gt;Strategic implications for enterprise AI investment&lt;/h2&gt;



&lt;p&gt;IBM’s real-world deployment data suggests several critical shifts for enterprise AI strategy:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Abandon chatbot-first thinking&lt;/strong&gt;: Organizations should identify complete workflows for transformation rather than adding conversational interfaces to existing systems. The goal is to eliminate human steps, not improve human-computer interaction.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Architect for multi-model flexibility&lt;/strong&gt;: Rather than committing to single AI providers, enterprises need integration platforms that enable switching between models based on use case requirements while maintaining governance standards.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Invest in communication standards&lt;/strong&gt;: Organizations should prioritize AI tools that support emerging protocols like MCP, ACP, and A2A rather than proprietary integration approaches that create vendor lock-in.&lt;/p&gt;



&lt;p&gt;“There is so much to build, and I keep saying everyone needs to learn AI and especially business leaders need to be AI first leaders and understand the concepts,” Ruiz said.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/ibm-sees-enterprise-customers-are-using-everything-when-it-comes-to-ai-the-challenge-is-matching-the-llm-to-the-right-use-case/</guid><pubDate>Wed, 25 Jun 2025 20:42:36 +0000</pubDate></item><item><title>Sam Altman comes out swinging at The New York Times (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/sam-altman-comes-out-swinging-at-the-new-york-times/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-2188228027.jpg?resize=1200,917" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;From the moment OpenAI CEO Sam Altman stepped onstage, it was clear this was not going to be a normal interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman and his chief operating officer, Brad Lightcap, stood awkwardly toward the back of the stage at a jam-packed San Francisco venue that typically hosts jazz concerts. Hundreds of people filled steep theater-style seating on Tuesday night to watch Kevin Roose, a columnist with The New York Times, and Platformer’s Casey Newton record a live episode of their popular technology podcast, Hard Fork.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Altman and Lightcap were the main event, but they’d walked out too early. Roose explained that he and Newton were planning to — ideally, before OpenAI’s executives were supposed to come out — list off several headlines that had been written about OpenAI in the weeks leading up to the event.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is more fun that we’re out here for this,” said Altman. Seconds later, the OpenAI CEO asked, “Are you going to talk about where you sue us because you don’t like user privacy?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Within minutes of the program starting, Altman hijacked the conversation to talk about The New York Times lawsuit against OpenAI and its largest investor, Microsoft, in which the publisher alleges that Altman’s company improperly used its articles to train large language models. Altman was particularly peeved about a recent development in the lawsuit, in which lawyers representing The New York Times asked OpenAI to retain consumer ChatGPT and API customer data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The New York Times, one of the great institutions, truly, for a long time, is taking a position that we should have to preserve our users’ logs even if they’re chatting in private mode, even if they’ve asked us to delete them,” said Altman. “Still love The New York Times, but that one we feel strongly about.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For a few minutes, OpenAI’s CEO pressed the podcasters to share their personal opinions about the New York Times lawsuit — they demurred, noting that as journalists whose work appears in The New York Times, they are not involved in the lawsuit.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Altman and Lightcap’s brash entrance lasted only a few minutes, and the rest of the interview proceeded, seemingly, as planned. However, the flare-up felt indicative of the inflection point Silicon Valley seems to be approaching in its relationship with the media industry.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last several years, multiple publishers have brought lawsuits against OpenAI, Anthropic, Google, and Meta for training their AI models on copyrighted works. At a high level, these lawsuits argue that AI models have the potential to devalue, and even replace, the copyrighted works produced by media institutions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the tides may be turning in favor of the tech companies. Earlier this week, OpenAI competitor Anthropic received a major win in its legal battle against publishers. A federal judge ruled that Anthropic’s use of books to train its AI models was legal in some circumstances, which could have broad implications for other publishers’ lawsuits against OpenAI, Google, and Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps Altman and Lightcap felt emboldened by the industry win heading into their live interview with The New York Times journalists. But these days, OpenAI is fending off threats from every direction, and that became clear throughout the night.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mark Zuckerberg has recently been trying to recruit OpenAI’s top talent by offering them $100 million compensation packages to join Meta’s AI superintelligence lab, Altman revealed weeks ago on his brother’s podcast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When asked whether the Meta CEO really believes in superintelligent AI systems, or if it’s just a recruiting strategy, Lightcap quipped: “I think [Zuckerberg] believes he is superintelligent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later, Roose asked Altman about OpenAI’s relationship with Microsoft, which has reportedly been pushed to a boiling point in recent months as the partners negotiate a new contract. While Microsoft was once a major accelerant to OpenAI, the two are now competing in enterprise software and other domains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In any deep partnership, there are points of tension and we certainly have those,” said Altman. “We’re both ambitious companies, so we do find some flashpoints, but I would expect that it is something that we find deep value in for both sides for a very long time to come.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s leadership today seems to spend a lot of time swatting down competitors and lawsuits. That may get in the way of OpenAI’s ability to solve broader issues around AI, such as how to safely deploy highly intelligent AI systems at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At one point, Newton asked OpenAI’s leaders how they were thinking about recent stories of mentally unstable people using ChatGPT to traverse dangerous rabbit holes, including to discuss conspiracy theories or suicide with the chatbot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman said OpenAI takes many steps to prevent these conversations, such as by cutting them off early, or directing users to professional services where they can get help.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t want to slide into the mistakes that I think the previous generation of tech companies made by not reacting quickly enough,” said Altman. To a follow-up question, the OpenAI CEO added, “However, to users that are in a fragile enough mental place, that are on the edge of a psychotic break, we haven’t yet figured out how a warning gets through.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-2188228027.jpg?resize=1200,917" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;From the moment OpenAI CEO Sam Altman stepped onstage, it was clear this was not going to be a normal interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman and his chief operating officer, Brad Lightcap, stood awkwardly toward the back of the stage at a jam-packed San Francisco venue that typically hosts jazz concerts. Hundreds of people filled steep theater-style seating on Tuesday night to watch Kevin Roose, a columnist with The New York Times, and Platformer’s Casey Newton record a live episode of their popular technology podcast, Hard Fork.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Altman and Lightcap were the main event, but they’d walked out too early. Roose explained that he and Newton were planning to — ideally, before OpenAI’s executives were supposed to come out — list off several headlines that had been written about OpenAI in the weeks leading up to the event.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is more fun that we’re out here for this,” said Altman. Seconds later, the OpenAI CEO asked, “Are you going to talk about where you sue us because you don’t like user privacy?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Within minutes of the program starting, Altman hijacked the conversation to talk about The New York Times lawsuit against OpenAI and its largest investor, Microsoft, in which the publisher alleges that Altman’s company improperly used its articles to train large language models. Altman was particularly peeved about a recent development in the lawsuit, in which lawyers representing The New York Times asked OpenAI to retain consumer ChatGPT and API customer data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The New York Times, one of the great institutions, truly, for a long time, is taking a position that we should have to preserve our users’ logs even if they’re chatting in private mode, even if they’ve asked us to delete them,” said Altman. “Still love The New York Times, but that one we feel strongly about.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For a few minutes, OpenAI’s CEO pressed the podcasters to share their personal opinions about the New York Times lawsuit — they demurred, noting that as journalists whose work appears in The New York Times, they are not involved in the lawsuit.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Altman and Lightcap’s brash entrance lasted only a few minutes, and the rest of the interview proceeded, seemingly, as planned. However, the flare-up felt indicative of the inflection point Silicon Valley seems to be approaching in its relationship with the media industry.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last several years, multiple publishers have brought lawsuits against OpenAI, Anthropic, Google, and Meta for training their AI models on copyrighted works. At a high level, these lawsuits argue that AI models have the potential to devalue, and even replace, the copyrighted works produced by media institutions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the tides may be turning in favor of the tech companies. Earlier this week, OpenAI competitor Anthropic received a major win in its legal battle against publishers. A federal judge ruled that Anthropic’s use of books to train its AI models was legal in some circumstances, which could have broad implications for other publishers’ lawsuits against OpenAI, Google, and Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps Altman and Lightcap felt emboldened by the industry win heading into their live interview with The New York Times journalists. But these days, OpenAI is fending off threats from every direction, and that became clear throughout the night.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mark Zuckerberg has recently been trying to recruit OpenAI’s top talent by offering them $100 million compensation packages to join Meta’s AI superintelligence lab, Altman revealed weeks ago on his brother’s podcast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When asked whether the Meta CEO really believes in superintelligent AI systems, or if it’s just a recruiting strategy, Lightcap quipped: “I think [Zuckerberg] believes he is superintelligent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later, Roose asked Altman about OpenAI’s relationship with Microsoft, which has reportedly been pushed to a boiling point in recent months as the partners negotiate a new contract. While Microsoft was once a major accelerant to OpenAI, the two are now competing in enterprise software and other domains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In any deep partnership, there are points of tension and we certainly have those,” said Altman. “We’re both ambitious companies, so we do find some flashpoints, but I would expect that it is something that we find deep value in for both sides for a very long time to come.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s leadership today seems to spend a lot of time swatting down competitors and lawsuits. That may get in the way of OpenAI’s ability to solve broader issues around AI, such as how to safely deploy highly intelligent AI systems at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At one point, Newton asked OpenAI’s leaders how they were thinking about recent stories of mentally unstable people using ChatGPT to traverse dangerous rabbit holes, including to discuss conspiracy theories or suicide with the chatbot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman said OpenAI takes many steps to prevent these conversations, such as by cutting them off early, or directing users to professional services where they can get help.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t want to slide into the mistakes that I think the previous generation of tech companies made by not reacting quickly enough,” said Altman. To a follow-up question, the OpenAI CEO added, “However, to users that are in a fragile enough mental place, that are on the edge of a psychotic break, we haven’t yet figured out how a warning gets through.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/sam-altman-comes-out-swinging-at-the-new-york-times/</guid><pubDate>Wed, 25 Jun 2025 20:54:57 +0000</pubDate></item><item><title>Federal judge sides with Meta in lawsuit over training AI models on copyrighted books (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/federal-judge-sides-with-meta-in-lawsuit-over-training-ai-models-on-copyrighted-books/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1968119319.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A federal judge sided with Meta on Wednesday in a lawsuit brought against the company by 13 book authors, including Sarah Silverman, that alleged the company had illegally trained its AI models on their copyrighted works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Federal Judge Vince Chhabria issued a summary judgment — meaning the judge was able to decide on the case without sending it to a jury — in favor of Meta, finding that the company’s training of AI models on copyrighted books in this case fell under the “fair use” doctrine of copyright law and thus was legal.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The decision comes just a few days after a federal judge sided with Anthropic in a similar lawsuit. Together, these cases are shaping up to be a win for the tech industry, which has spent years in legal battles with media companies arguing that training AI models on copyrighted works is fair use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, these decisions aren’t the sweeping wins some companies hoped  for — both judges noted that their cases were limited in scope.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Chhabria made clear that this decision does not mean that all AI model training on copyrighted works is legal, but rather that the plaintiffs in this case “made the wrong arguments” and failed to develop sufficient evidence in support of the right ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This ruling does not stand for the proposition that Meta’s use of copyrighted materials to train its language models is lawful,” Judge Chhabria said in his decision. Later, he said, “In cases involving uses like Meta’s, it seems like the plaintiffs will often win, at least where those cases have better-developed records on the market effects of the defendant’s use.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Chhabria ruled that Meta’s use of copyrighted works in this case was transformative — meaning the company’s AI models did not merely reproduce the authors’ books.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Furthermore, the plaintiffs failed to convince the judge that Meta’s copying of the books harmed the market for those authors, which is a key factor in determining whether copyright law has been violated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The plaintiffs presented no meaningful evidence on market dilution at all,” said Judge Chhabria.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both Anthropic’s and Meta’s wins involve training AI models on books, but there are several other active lawsuits against technology companies for training AI models on other copyrighted works. For instance, The New York Times is suing OpenAI and Microsoft for training AI models on news articles, while Disney and Universal are suing Midjourney for training AI models on films and TV shows.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Judge Chhabria noted in his decision that fair use defenses depend heavily on the details of a case, and some industries may have stronger fair use arguments than others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It seems that markets for certain types of works (like news articles) might be even more vulnerable to indirect competition from AI outputs,” said Chhabria.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1968119319.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A federal judge sided with Meta on Wednesday in a lawsuit brought against the company by 13 book authors, including Sarah Silverman, that alleged the company had illegally trained its AI models on their copyrighted works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Federal Judge Vince Chhabria issued a summary judgment — meaning the judge was able to decide on the case without sending it to a jury — in favor of Meta, finding that the company’s training of AI models on copyrighted books in this case fell under the “fair use” doctrine of copyright law and thus was legal.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The decision comes just a few days after a federal judge sided with Anthropic in a similar lawsuit. Together, these cases are shaping up to be a win for the tech industry, which has spent years in legal battles with media companies arguing that training AI models on copyrighted works is fair use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, these decisions aren’t the sweeping wins some companies hoped  for — both judges noted that their cases were limited in scope.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Chhabria made clear that this decision does not mean that all AI model training on copyrighted works is legal, but rather that the plaintiffs in this case “made the wrong arguments” and failed to develop sufficient evidence in support of the right ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This ruling does not stand for the proposition that Meta’s use of copyrighted materials to train its language models is lawful,” Judge Chhabria said in his decision. Later, he said, “In cases involving uses like Meta’s, it seems like the plaintiffs will often win, at least where those cases have better-developed records on the market effects of the defendant’s use.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Chhabria ruled that Meta’s use of copyrighted works in this case was transformative — meaning the company’s AI models did not merely reproduce the authors’ books.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Furthermore, the plaintiffs failed to convince the judge that Meta’s copying of the books harmed the market for those authors, which is a key factor in determining whether copyright law has been violated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The plaintiffs presented no meaningful evidence on market dilution at all,” said Judge Chhabria.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both Anthropic’s and Meta’s wins involve training AI models on books, but there are several other active lawsuits against technology companies for training AI models on other copyrighted works. For instance, The New York Times is suing OpenAI and Microsoft for training AI models on news articles, while Disney and Universal are suing Midjourney for training AI models on films and TV shows.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Judge Chhabria noted in his decision that fair use defenses depend heavily on the details of a case, and some industries may have stronger fair use arguments than others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It seems that markets for certain types of works (like news articles) might be even more vulnerable to indirect competition from AI outputs,” said Chhabria.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/federal-judge-sides-with-meta-in-lawsuit-over-training-ai-models-on-copyrighted-books/</guid><pubDate>Wed, 25 Jun 2025 23:40:32 +0000</pubDate></item><item><title>[NEW] Meta’s recruiting blitz claims three OpenAI researchers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/metas-recruiting-blitz-claims-three-openai-researchers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2208764114.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In the fight for top AI talent, Meta just reportedly snagged a win, poaching three OpenAI researchers despite rival Sam Altman’s public mockery of Mark Zuckerberg’s lavish hiring tactics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest victory in Zuckerberg’s widely-reported recruiting blitz: Lucas Beyer, Alexander Kolesnikov, and Xiaohua Zhai – who established OpenAI’s Zurich office – have joined Meta’s superintelligence team, the WSJ reports, suggesting Zuckerberg’s methods can deliver.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As Altman, the CEO of OpenAI, first revealed in a recent podcast with his brother Jack, Zuckerberg has been dangling $100+ million compensation packages in an effort to lure top talent from OpenAI. The Journal subsequently reported that Zuckerberg has been personally WhatsApping hundreds of top AI researchers, coordinating targets through his “Recruiting Party 🎉” chat before hosting dinners at his homes in Palo Alto and Lake Tahoe.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The strategy is producing mixed results. Zuckerberg recently bagged Scale AI’s CEO Alexandr Wang with a $14 billion investment, making the 28-year-old one of tech’s priciest hires ever. But bigger game has eluded the Meta CEO, says the WSJ, including OpenAI co-founders Ilya Sutskever and John Schulman, both of whom have gone on to co-found newer startups.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that podcast, Altman said of Zuckerberg’s charm campaign: “I’m really happy that, at least so far, none of our best people have decided to take him up on [those offers].”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2208764114.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In the fight for top AI talent, Meta just reportedly snagged a win, poaching three OpenAI researchers despite rival Sam Altman’s public mockery of Mark Zuckerberg’s lavish hiring tactics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest victory in Zuckerberg’s widely-reported recruiting blitz: Lucas Beyer, Alexander Kolesnikov, and Xiaohua Zhai – who established OpenAI’s Zurich office – have joined Meta’s superintelligence team, the WSJ reports, suggesting Zuckerberg’s methods can deliver.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As Altman, the CEO of OpenAI, first revealed in a recent podcast with his brother Jack, Zuckerberg has been dangling $100+ million compensation packages in an effort to lure top talent from OpenAI. The Journal subsequently reported that Zuckerberg has been personally WhatsApping hundreds of top AI researchers, coordinating targets through his “Recruiting Party 🎉” chat before hosting dinners at his homes in Palo Alto and Lake Tahoe.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The strategy is producing mixed results. Zuckerberg recently bagged Scale AI’s CEO Alexandr Wang with a $14 billion investment, making the 28-year-old one of tech’s priciest hires ever. But bigger game has eluded the Meta CEO, says the WSJ, including OpenAI co-founders Ilya Sutskever and John Schulman, both of whom have gone on to co-found newer startups.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that podcast, Altman said of Zuckerberg’s charm campaign: “I’m really happy that, at least so far, none of our best people have decided to take him up on [those offers].”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/metas-recruiting-blitz-claims-three-openai-researchers/</guid><pubDate>Thu, 26 Jun 2025 04:50:12 +0000</pubDate></item></channel></rss>