<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 20 Aug 2025 18:31:26 +0000</lastBuildDate><item><title> ()</title><link>https://www.wired.com/feed/category/artificial-intelligence/rss</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://www.wired.com/feed/category/artificial-intelligence/rss</guid></item><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>New Nvidia Blackwell chip for China may outpace H20 model (AI News)</title><link>https://www.artificialintelligence-news.com/news/new-nvidia-blackwell-chip-for-china-may-outpace-h20-model/</link><description>&lt;p&gt;Nvidia is working on a new AI chip for China that could be stronger than the H20 model it currently sells there, according to people familiar with the plans, &lt;em&gt;Reuters&lt;/em&gt; reported. The chip will be based on the company’s latest Blackwell design.&lt;/p&gt;&lt;p&gt;Last week, US President Donald Trump suggested that more advanced Nvidia chips might eventually be sold in China. But approval is uncertain, as US officials remain wary of giving Beijing too much access to American AI technology.&lt;/p&gt;&lt;p&gt;The new chip, referred to internally as the B30A, is expected to use a single-die design. That means all the core parts of the chip are built on one piece of silicon, instead of split in two dies like in Nvidia’s flagship B300. People familiar with the details said this would likely give the B30A about half the power of the B300, but still make it stronger than the H20.&lt;/p&gt;&lt;p&gt;Like the H20, the B30A will feature high-bandwidth memory and NVLink, Nvidia’s technology for moving data quickly between processors. Final specifications haven’t been locked in, but the company hopes to send samples to Chinese customers for testing as early as next month.&lt;/p&gt;&lt;p&gt;Nvidia, in a statement about its strategy for China, said: “We evaluate a variety of products for our roadmap, so that we can be prepared to compete to the extent that governments allow. Everything we offer is with the full approval of the applicable authorities and designed solely for beneficial commercial use.”&lt;/p&gt;&lt;p&gt;US Commerce Secretary Howard Lutnick told &lt;em&gt;CNBC&lt;/em&gt; that Nvidia CEO Jensen Huang has been pushing for approval. “Of course he would like to sell a new chip to China,” Lutnick said, adding that Huang often pitches the idea directly to the president. “The president listens to our great technology companies, and he’ll decide how he wants to play it. But the fact Jensen is pitching a new chip shouldn’t surprise anybody.”&lt;/p&gt;&lt;h3&gt;US-China trade tensions grow over Nvidia chips&lt;/h3&gt;&lt;p&gt;The question of how much access China should have to advanced AI chips has become one of the main points of tension between Washington and Beijing. China accounted for 13% of Nvidia’s revenue last year.&lt;/p&gt;&lt;p&gt;Nvidia only resumed selling the H20 in July after US regulators abruptly halted sales in April. The H20 was designed in 2023 to meet export rules which restricted chip sales to China.&lt;/p&gt;&lt;p&gt;Donald Trump said recently he might allow Nvidia to sell a scaled-down version of its next-generation chip in China. As part of a broader deal, Nvidia and rival AMD agreed to give the US government 15% of revenue from some chip sales to China. Trump also called the H20 “obsolete,” suggesting that a new China-only chip could deliver “30% to 50% off” [sic] the computing power of the top model.&lt;/p&gt;&lt;p&gt;Bipartisan Washington legislators argue even weaker versions of AI chips could still give China an edge in critical areas. Nvidia and others have countered, saying if they stop selling to China, customers will turn to local suppliers like Huawei. The latter’s most recent chip models are said to rival Nvidia’s in raw computing power, though analysts say Huawei still trails in software performance and memory speed.&lt;/p&gt;&lt;p&gt;China’s state media has added more pressure by warning that Nvidia’s chips could carry security risks, with regulators cautioning Chinese firms against buying the H20. Nvidia has denied that its hardware poses any such threat.&lt;/p&gt;&lt;h3&gt;Another chip in the works&lt;/h3&gt;&lt;p&gt;Alongside the B30A, Nvidia is preparing another product specifically for the Chinese market, also built on the Blackwell architecture but focused on AI inference tasks. According to sources, the RTX6000D chip will be cheaper than the H20 due to simpler design and lower specifications.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Reuters&lt;/em&gt; previously reported that the RTX6000D is designed to perform at just under thresholds set by US export strictures. It uses standard GDDR memory and runs at 1,398 gigabytes per second – slightly below the 1.4 terabyte-per-second cap set by new restrictions in April.&lt;/p&gt;&lt;p&gt;Small shipments of the RTX6000D are expected to reach Chinese customers in September.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by BoliviaInteligente)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: NVIDIA aims to solve AI’s issues with many languages&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Nvidia is working on a new AI chip for China that could be stronger than the H20 model it currently sells there, according to people familiar with the plans, &lt;em&gt;Reuters&lt;/em&gt; reported. The chip will be based on the company’s latest Blackwell design.&lt;/p&gt;&lt;p&gt;Last week, US President Donald Trump suggested that more advanced Nvidia chips might eventually be sold in China. But approval is uncertain, as US officials remain wary of giving Beijing too much access to American AI technology.&lt;/p&gt;&lt;p&gt;The new chip, referred to internally as the B30A, is expected to use a single-die design. That means all the core parts of the chip are built on one piece of silicon, instead of split in two dies like in Nvidia’s flagship B300. People familiar with the details said this would likely give the B30A about half the power of the B300, but still make it stronger than the H20.&lt;/p&gt;&lt;p&gt;Like the H20, the B30A will feature high-bandwidth memory and NVLink, Nvidia’s technology for moving data quickly between processors. Final specifications haven’t been locked in, but the company hopes to send samples to Chinese customers for testing as early as next month.&lt;/p&gt;&lt;p&gt;Nvidia, in a statement about its strategy for China, said: “We evaluate a variety of products for our roadmap, so that we can be prepared to compete to the extent that governments allow. Everything we offer is with the full approval of the applicable authorities and designed solely for beneficial commercial use.”&lt;/p&gt;&lt;p&gt;US Commerce Secretary Howard Lutnick told &lt;em&gt;CNBC&lt;/em&gt; that Nvidia CEO Jensen Huang has been pushing for approval. “Of course he would like to sell a new chip to China,” Lutnick said, adding that Huang often pitches the idea directly to the president. “The president listens to our great technology companies, and he’ll decide how he wants to play it. But the fact Jensen is pitching a new chip shouldn’t surprise anybody.”&lt;/p&gt;&lt;h3&gt;US-China trade tensions grow over Nvidia chips&lt;/h3&gt;&lt;p&gt;The question of how much access China should have to advanced AI chips has become one of the main points of tension between Washington and Beijing. China accounted for 13% of Nvidia’s revenue last year.&lt;/p&gt;&lt;p&gt;Nvidia only resumed selling the H20 in July after US regulators abruptly halted sales in April. The H20 was designed in 2023 to meet export rules which restricted chip sales to China.&lt;/p&gt;&lt;p&gt;Donald Trump said recently he might allow Nvidia to sell a scaled-down version of its next-generation chip in China. As part of a broader deal, Nvidia and rival AMD agreed to give the US government 15% of revenue from some chip sales to China. Trump also called the H20 “obsolete,” suggesting that a new China-only chip could deliver “30% to 50% off” [sic] the computing power of the top model.&lt;/p&gt;&lt;p&gt;Bipartisan Washington legislators argue even weaker versions of AI chips could still give China an edge in critical areas. Nvidia and others have countered, saying if they stop selling to China, customers will turn to local suppliers like Huawei. The latter’s most recent chip models are said to rival Nvidia’s in raw computing power, though analysts say Huawei still trails in software performance and memory speed.&lt;/p&gt;&lt;p&gt;China’s state media has added more pressure by warning that Nvidia’s chips could carry security risks, with regulators cautioning Chinese firms against buying the H20. Nvidia has denied that its hardware poses any such threat.&lt;/p&gt;&lt;h3&gt;Another chip in the works&lt;/h3&gt;&lt;p&gt;Alongside the B30A, Nvidia is preparing another product specifically for the Chinese market, also built on the Blackwell architecture but focused on AI inference tasks. According to sources, the RTX6000D chip will be cheaper than the H20 due to simpler design and lower specifications.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Reuters&lt;/em&gt; previously reported that the RTX6000D is designed to perform at just under thresholds set by US export strictures. It uses standard GDDR memory and runs at 1,398 gigabytes per second – slightly below the 1.4 terabyte-per-second cap set by new restrictions in April.&lt;/p&gt;&lt;p&gt;Small shipments of the RTX6000D are expected to reach Chinese customers in September.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by BoliviaInteligente)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: NVIDIA aims to solve AI’s issues with many languages&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/new-nvidia-blackwell-chip-for-china-may-outpace-h20-model/</guid><pubDate>Wed, 20 Aug 2025 08:44:32 +0000</pubDate></item><item><title>Material Cultures looks to the past to build the future (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/20/1121416/material-cultures-architecture-sustainability/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Despite decades of green certifications, better material sourcing, and the use of more sustainable materials such as mass timber, the built environment is still responsible for a third of global emissions worldwide. According to a 2024 UN report, the building sector has fallen “significantly behind on progress” toward becoming more sustainable. Changing the way we erect and operate buildings remains key to even approaching climate goals.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“As soon as you set out and do something differently in construction, you are constantly bumping your head against the wall,” says Paloma Gormley, a director of the London-based design and research nonprofit Material Cultures. “You can either stop there or take a step back and try to find a way around it.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Gormley has been finding a “way around it” by systematically exploring how tradition can be harnessed in new ways to repair what she has dubbed the “oil vernacular”—the contemporary building system shaped not by local, natural materials but by global commodities and plastic products made largely from fossil fuels.&lt;/p&gt;  &lt;p&gt;Though she grew up in a household rich in art and design—she’s the daughter of the famed British sculptor Antony Gormley—she’s quick to say she’s far from a brilliant maker and more of a “bodger,” a term that means someone who does work that’s botched or shoddy.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Improviser or DIYer might be more accurate. One of her first bits of architecture was a makeshift home built on the back of a truck she used to tour around England one summer in her 20s. The work of her first firm, Practice Architecture, which she cofounded after graduating from the University of Cambridge in 2009, was informed by London’s DIY subcultures and informal art spaces. She says these scenes “existed in the margins and cracks between things, but in which a lot felt possible.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Frank’s Café, a bar and restaurant she built in 2009 on the roof of a parking garage in Peckham that hosted a sculpture park, was constructed from ratchet straps, scaffold boards, and castoffs she’d source from lumberyards and transport on the roof rack of an old Volvo. It was the first of a series of cultural and social spaces she and her partner Lettice Drake created using materials both low-budget and local.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Material Cultures grew out of connections Gormley made while she was teaching at London Metropolitan University. In 2019, she was a teaching assistant along with Summer Islam, who was friends with George Massoud, both architects and partners in the firm Study Abroad and advocates of more socially conscious design. The trio had a shared interest in sustainability and building practices, as well as a frustration with the architecture world’s focus on improving sustainability through high-tech design. Instead of using modern methods to build more efficient commercial and residential spaces from carbon-intensive materials like steel, they thought, why not revisit first principles? Build with locally sourced, natural materials and you don’t have to worry about making up a carbon deficit in the first place.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1121752" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_CSM_Clearfell-House©Henry_Woide_021.jpg?w=1500" /&gt;&lt;figcaption class="wp-element-caption"&gt;The frame of Clearfell House was built with ash and larch, two species of wood vulnerable to climate change&lt;/figcaption&gt;&lt;div class="image-credit"&gt;HENRY WOIDE/COURTESY OF MATERIAL CULTURES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="office in a house" class="wp-image-1121747" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_Flat-House_Image-Credit-Oskar-Proctor_Large44V.jpg?w=1500" width="1500" /&gt;&lt;figcaption class="wp-element-caption"&gt;Flat House was built with pressed panels of hemp grown in the fields surrounding the home.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;OSKAR PROCTOR&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;As many other practitioners look to artificial intelligence and other high-tech approaches to building, Material Cultures has always focused on sustainability, finding creative ways to turn local materials into new buildings. And the three of them don’t just design and build. They team up with traditional craft experts to explore the potential of materials like reeds and clay, and techniques like thatching and weaving.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;More than any one project, Gormley, Islam, and Massoud are perhaps best known for their meditation on the subject of how architects work. Published in 2022, &lt;em&gt;Material Reform: Building for a Post-Carbon Future&lt;/em&gt; is a pocket-size book that drills into materials and methodologies to suggest a more thoughtful, ecological architecture.&lt;/p&gt;  &lt;p&gt;“There is a huge amount of technological knowledge and intelligence in historic, traditional, vernacular ways of doing things that’s been evolved over millennia, not just the last 100 years,” Gormley says. “We’re really about trying to tap into that.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;One of Material Cultures’ early works, Flat House, a home built in 2019 in Cambridgeshire, England, with pressed panels of hemp grown in the surrounding fields, was meant as an exploration of what kind of building could be made from what a single farm could produce. Gormley was there from the planting of the seeds to the harvesting of the hemp plants to the completion of construction.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It was incredible understanding that buildings could be part of these natural cycles,” she says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Clearfell House, a timber A-frame cabin tucked into a clearing in the Dalby Forest in North Yorkshire, England, exemplifies the firm’s obsession with elevating humble materials and vernacular techniques. Every square inch of the house, which was finished in late 2024 as part of a construction class Material Cultures’ architects taught at Central Saint Martins design school in London, emerged from extensive research into British timber, the climate crisis, and how forestry is changing. That meant making the frame from local ash and larch, two species of wood specifically chosen because they were affected by climate change, and avoiding the use of factory-farmed lumber. The modular system used for the structure was made to be replicated at scale.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I find it rare that architecture offices have such a clear framing and mission,” says Andreas Lang, head of the Saint Martins architecture program. “Emerging practices often become client-dependent. For [Material Cultures], the client is maybe the planet.”&lt;/p&gt; 

&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Material Cultures fits in with the boom in popularity for more sustainable materials, waste-minimizing construction, and panelized building using straw and hemp, says Michael Burchert, a German expert on decarbonized buildings. “People are grabbing the good stuff from the hippies at the moment,” he says. Regulation has started to follow: France recently mandated that new public buildings be constructed with 50% timber or other biological material, and Denmark’s construction sector has embarked on a project, Pathways to Biobased Construction, to promote use of nature-based products in new building.&lt;/p&gt;  &lt;p&gt;Burchert appreciates the way the firm melds theory and practice. “We have academia, and academia is full of papers,” he says. “We need makers.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Over the last several years, Gormley and her cofounders have developed a portfolio of work that rethinks construction supply chains and stays grounded in social impact. The just-finished Wolves Lane Centre, a $2.4 million community center in North London run by a pair of groups that work on food and racial justice, didn’t just reflect Material Cultures’ typical focus on bio-based materials—in this case, local straw, lime, and timber.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1121751" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_Studio-Gil_Wolves-Lane_Image-Luke-ODonovan_In-Use_Large_26.jpg?w=1500" width="1500" /&gt;&lt;div class="image-credit"&gt;LUKE O’DONOVAN/COURTESY OF MATERIAL CULTURES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1121749" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_Studio-Gil_Wolves-Lane_Image-Luke-ODonovan_In-Use_Large_22.jpg?w=1500" width="1500" /&gt;&lt;div class="image-credit"&gt;LUKE O’DONOVAN/COURTESY OF MATERIAL CULTURES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="class"&gt; &lt;p class="imageSet__caption"&gt;For Wolves Lane Centre, a $2.4 million community facility for groups working on food and racial justice, expert plasterers and specialists in straw-bale construction were brought in so their processes could be shared and learned.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignwide size-large"&gt;&lt;img alt="alt" class="wp-image-1121748" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_Studio-Gil_Wolves-Lane_Image-Luke-ODonovan_In-Use_Large_9.jpg?w=3000" /&gt;&lt;div class="image-credit"&gt;LUKE O’DONOVAN/COURTESY OF MATERIAL CULTURES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;It was a project of self-determination and learning, says Gormley. Expert plasterers and specialists in straw-bale construction were brought in so the processes could be shared and learned. Introducing this kind of teaching into the construction process was quite time-consuming and, Gormley says, was as expensive as using contemporary techniques, if not more so. But the added value was worth it.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;“The people who become the custodians of these buildings then have the skills to maintain and repair, as well as evolve, the site over time,” she says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As Burchert puts it, science fiction tends to show a future built of concrete and steel; Material Cultures instead offers something natural, communal, and innovative, a needed paradigm shift. And it’s increasingly working on a larger scale. The Phoenix, a forthcoming low-carbon development in the southern English city of Lewes that’s being developed by a former managing director for Greenpeace, will use the firm’s designs for 70 of its 700 planned homes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The project Gormley may be most excited about is an interdisciplinary school Material Cultures is creating north of London: a 500-acre former farm in Essex that will be a living laboratory bridging the firm’s work in supply chains, materials science, and construction. The rural site for the project, which has the working title Land Lab, was deliberately chosen as a place where those connections would be inherent, Gormley says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The Essex project advances the firm’s larger mission. As Gormley, Massoud, and Islam advise in their book, “Hold a vision of a radically different world in your mind while continuing to act in the world as it is, persisting in the project of making changes that are within the scope of action.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Patrick Sisson, a Chicago expat living in Los Angeles, covers technology and urbanism.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Despite decades of green certifications, better material sourcing, and the use of more sustainable materials such as mass timber, the built environment is still responsible for a third of global emissions worldwide. According to a 2024 UN report, the building sector has fallen “significantly behind on progress” toward becoming more sustainable. Changing the way we erect and operate buildings remains key to even approaching climate goals.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“As soon as you set out and do something differently in construction, you are constantly bumping your head against the wall,” says Paloma Gormley, a director of the London-based design and research nonprofit Material Cultures. “You can either stop there or take a step back and try to find a way around it.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Gormley has been finding a “way around it” by systematically exploring how tradition can be harnessed in new ways to repair what she has dubbed the “oil vernacular”—the contemporary building system shaped not by local, natural materials but by global commodities and plastic products made largely from fossil fuels.&lt;/p&gt;  &lt;p&gt;Though she grew up in a household rich in art and design—she’s the daughter of the famed British sculptor Antony Gormley—she’s quick to say she’s far from a brilliant maker and more of a “bodger,” a term that means someone who does work that’s botched or shoddy.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Improviser or DIYer might be more accurate. One of her first bits of architecture was a makeshift home built on the back of a truck she used to tour around England one summer in her 20s. The work of her first firm, Practice Architecture, which she cofounded after graduating from the University of Cambridge in 2009, was informed by London’s DIY subcultures and informal art spaces. She says these scenes “existed in the margins and cracks between things, but in which a lot felt possible.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Frank’s Café, a bar and restaurant she built in 2009 on the roof of a parking garage in Peckham that hosted a sculpture park, was constructed from ratchet straps, scaffold boards, and castoffs she’d source from lumberyards and transport on the roof rack of an old Volvo. It was the first of a series of cultural and social spaces she and her partner Lettice Drake created using materials both low-budget and local.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Material Cultures grew out of connections Gormley made while she was teaching at London Metropolitan University. In 2019, she was a teaching assistant along with Summer Islam, who was friends with George Massoud, both architects and partners in the firm Study Abroad and advocates of more socially conscious design. The trio had a shared interest in sustainability and building practices, as well as a frustration with the architecture world’s focus on improving sustainability through high-tech design. Instead of using modern methods to build more efficient commercial and residential spaces from carbon-intensive materials like steel, they thought, why not revisit first principles? Build with locally sourced, natural materials and you don’t have to worry about making up a carbon deficit in the first place.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1121752" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_CSM_Clearfell-House©Henry_Woide_021.jpg?w=1500" /&gt;&lt;figcaption class="wp-element-caption"&gt;The frame of Clearfell House was built with ash and larch, two species of wood vulnerable to climate change&lt;/figcaption&gt;&lt;div class="image-credit"&gt;HENRY WOIDE/COURTESY OF MATERIAL CULTURES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="office in a house" class="wp-image-1121747" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_Flat-House_Image-Credit-Oskar-Proctor_Large44V.jpg?w=1500" width="1500" /&gt;&lt;figcaption class="wp-element-caption"&gt;Flat House was built with pressed panels of hemp grown in the fields surrounding the home.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;OSKAR PROCTOR&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;As many other practitioners look to artificial intelligence and other high-tech approaches to building, Material Cultures has always focused on sustainability, finding creative ways to turn local materials into new buildings. And the three of them don’t just design and build. They team up with traditional craft experts to explore the potential of materials like reeds and clay, and techniques like thatching and weaving.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;More than any one project, Gormley, Islam, and Massoud are perhaps best known for their meditation on the subject of how architects work. Published in 2022, &lt;em&gt;Material Reform: Building for a Post-Carbon Future&lt;/em&gt; is a pocket-size book that drills into materials and methodologies to suggest a more thoughtful, ecological architecture.&lt;/p&gt;  &lt;p&gt;“There is a huge amount of technological knowledge and intelligence in historic, traditional, vernacular ways of doing things that’s been evolved over millennia, not just the last 100 years,” Gormley says. “We’re really about trying to tap into that.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;One of Material Cultures’ early works, Flat House, a home built in 2019 in Cambridgeshire, England, with pressed panels of hemp grown in the surrounding fields, was meant as an exploration of what kind of building could be made from what a single farm could produce. Gormley was there from the planting of the seeds to the harvesting of the hemp plants to the completion of construction.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“It was incredible understanding that buildings could be part of these natural cycles,” she says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Clearfell House, a timber A-frame cabin tucked into a clearing in the Dalby Forest in North Yorkshire, England, exemplifies the firm’s obsession with elevating humble materials and vernacular techniques. Every square inch of the house, which was finished in late 2024 as part of a construction class Material Cultures’ architects taught at Central Saint Martins design school in London, emerged from extensive research into British timber, the climate crisis, and how forestry is changing. That meant making the frame from local ash and larch, two species of wood specifically chosen because they were affected by climate change, and avoiding the use of factory-farmed lumber. The modular system used for the structure was made to be replicated at scale.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I find it rare that architecture offices have such a clear framing and mission,” says Andreas Lang, head of the Saint Martins architecture program. “Emerging practices often become client-dependent. For [Material Cultures], the client is maybe the planet.”&lt;/p&gt; 

&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Material Cultures fits in with the boom in popularity for more sustainable materials, waste-minimizing construction, and panelized building using straw and hemp, says Michael Burchert, a German expert on decarbonized buildings. “People are grabbing the good stuff from the hippies at the moment,” he says. Regulation has started to follow: France recently mandated that new public buildings be constructed with 50% timber or other biological material, and Denmark’s construction sector has embarked on a project, Pathways to Biobased Construction, to promote use of nature-based products in new building.&lt;/p&gt;  &lt;p&gt;Burchert appreciates the way the firm melds theory and practice. “We have academia, and academia is full of papers,” he says. “We need makers.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Over the last several years, Gormley and her cofounders have developed a portfolio of work that rethinks construction supply chains and stays grounded in social impact. The just-finished Wolves Lane Centre, a $2.4 million community center in North London run by a pair of groups that work on food and racial justice, didn’t just reflect Material Cultures’ typical focus on bio-based materials—in this case, local straw, lime, and timber.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1121751" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_Studio-Gil_Wolves-Lane_Image-Luke-ODonovan_In-Use_Large_26.jpg?w=1500" width="1500" /&gt;&lt;div class="image-credit"&gt;LUKE O’DONOVAN/COURTESY OF MATERIAL CULTURES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1121749" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_Studio-Gil_Wolves-Lane_Image-Luke-ODonovan_In-Use_Large_22.jpg?w=1500" width="1500" /&gt;&lt;div class="image-credit"&gt;LUKE O’DONOVAN/COURTESY OF MATERIAL CULTURES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="class"&gt; &lt;p class="imageSet__caption"&gt;For Wolves Lane Centre, a $2.4 million community facility for groups working on food and racial justice, expert plasterers and specialists in straw-bale construction were brought in so their processes could be shared and learned.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignwide size-large"&gt;&lt;img alt="alt" class="wp-image-1121748" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/Material-Cultures_Studio-Gil_Wolves-Lane_Image-Luke-ODonovan_In-Use_Large_9.jpg?w=3000" /&gt;&lt;div class="image-credit"&gt;LUKE O’DONOVAN/COURTESY OF MATERIAL CULTURES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;It was a project of self-determination and learning, says Gormley. Expert plasterers and specialists in straw-bale construction were brought in so the processes could be shared and learned. Introducing this kind of teaching into the construction process was quite time-consuming and, Gormley says, was as expensive as using contemporary techniques, if not more so. But the added value was worth it.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;“The people who become the custodians of these buildings then have the skills to maintain and repair, as well as evolve, the site over time,” she says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As Burchert puts it, science fiction tends to show a future built of concrete and steel; Material Cultures instead offers something natural, communal, and innovative, a needed paradigm shift. And it’s increasingly working on a larger scale. The Phoenix, a forthcoming low-carbon development in the southern English city of Lewes that’s being developed by a former managing director for Greenpeace, will use the firm’s designs for 70 of its 700 planned homes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The project Gormley may be most excited about is an interdisciplinary school Material Cultures is creating north of London: a 500-acre former farm in Essex that will be a living laboratory bridging the firm’s work in supply chains, materials science, and construction. The rural site for the project, which has the working title Land Lab, was deliberately chosen as a place where those connections would be inherent, Gormley says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The Essex project advances the firm’s larger mission. As Gormley, Massoud, and Islam advise in their book, “Hold a vision of a radically different world in your mind while continuing to act in the world as it is, persisting in the project of making changes that are within the scope of action.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Patrick Sisson, a Chicago expat living in Los Angeles, covers technology and urbanism.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/20/1121416/material-cultures-architecture-sustainability/</guid><pubDate>Wed, 20 Aug 2025 10:00:00 +0000</pubDate></item><item><title>The Download: churches in the age of AI, and how to run an LLM at home (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/20/1122162/the-download-churches-in-the-age-of-ai-and-how-to-run-an-llm-at-home/</link><description>&lt;div class="contentBody__summaryBullets--81327c9379272772d1e74a64b6d4868a"&gt;&lt;p&gt;Plus: the AI bubble may be about to burst&lt;/p&gt;
&lt;/div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How churches use data and AI as engines of surveillance&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;On a Sunday morning in a Midwestern megachurch, worshippers step through sliding glass doors into a bustling lobby—unaware they’ve just passed through a gauntlet of biometric surveillance. High-speed cameras snap multiple face “probes” per second, before passing the results to a local neural network that distills these images into digital fingerprints. Before people find their seats, they are matched against an on-premises database—tagged with names, membership tiers, and watch-list flags—that’s stored behind the church’s firewall.&lt;/p&gt;  &lt;p&gt;This hypothetical scene reflects real capabilities increasingly woven into places of worship nationwide, where spiritual care and surveillance converge in ways few congregants ever realize.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Where Big Tech’s rationalist ethos and evangelical spirituality once mixed like oil and holy water, now they’re combining to redraw the contours of community and pastoral power in modern spiritual life. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Alex Ashley&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is from our forthcoming print issue, which is all about security. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: How to run an LLM on your laptop&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;For people who are concerned about privacy, want to break free from the control of the big LLM companies, or just enjoy tinkering, local models offer a compelling alternative to ChatGPT and its web-based peers. Here’s how to get started running one from the safety and comfort of your own computer.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 US tech stocks are sliding over fears the AI bubble may be about to burst&lt;br /&gt;&lt;/strong&gt;After an MIT report found that the vast majority of organizations are getting zero return on their AI investments. (FT $)&lt;br /&gt;+ &lt;em&gt;Even Sam Altman thinks the current hype is unsustainable. &lt;/em&gt;(CNBC)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Meta is reportedly weighing up downsizing its AI division&lt;/strong&gt;&lt;br /&gt;It wants to split it into four groups—and layoffs could be imminent. (NYT $)+ &lt;em&gt;What’s happening with the metaverse, then? &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;Meta is desperately hoping its AI hiring spree will pay off. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 The American Academy of Pediatrics is defying RFK Jr&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;By releasing its own vaccination schedule for children. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;It’s breaking with current CDC recommendations. &lt;/em&gt;(CNN)&lt;br /&gt;+ &lt;em&gt;Why US federal health agencies are abandoning mRNA vaccines. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4 Elon Musk’s America Party isn’t going so well&lt;/strong&gt;&lt;br /&gt;He’s said to be refocusing his attention on his companies instead. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 The White House has a TikTok account now&lt;/strong&gt;&lt;br /&gt;The very same TikTok that Donald Trump once tried to ban. (WP $)&lt;br /&gt;+ &lt;em&gt;What appears to have changed Congress’ stance? &lt;/em&gt;(The Verge)&lt;br /&gt;+ &lt;em&gt;There’s still no sign of a sale on the horizon. &lt;/em&gt;(The Guardian)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Nvidia is working on another chip for China&lt;br /&gt;&lt;/strong&gt;One that’s faster and more powerful than its current H20 model. (Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 How AGI preppers are bracing themselves for an AI apocalypse&lt;br /&gt;&lt;/strong&gt;Some are spending all their retirement savings along the way. (Insider $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 Demand for critical minerals is soaring&lt;br /&gt;&lt;/strong&gt;Is there a less-invasive way to mine them? (New Scientist $)&lt;br /&gt;+ &lt;em&gt;The race to produce rare earth elements. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What’s an automaker CEO to do?&lt;/strong&gt;&lt;br /&gt;In our increasingly topsy turvy world, many of them feel like they can’t win. (Wired $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 This mattress company is building an AI agent for sleep&lt;/strong&gt;&lt;br /&gt;Eight Sleep’s agent could simulate digital twins of a user’s sleep habits. (The Information $)&lt;br /&gt;+ &lt;em&gt;I tried to hack my insomnia with technology. Here’s what worked. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“&lt;/strong&gt;&lt;strong&gt;Too many cooks, too many kitchens.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Tech investor M.G. Siegler wryly comments on the news Meta is planning to restructure its AI division in a post on Bluesky.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeVwzIiWvdhv0GNsh0V1eu1NNMeADHR99glh5P2WeGfuDEOMd6FO-P_ZjU_amqDbMto2OMrQwZxnAzqVzyt3rAOuKDHYSWepH2WWeFvfpTP3X-2rBkH36cdwETzI8SSSd3J3kKd6Q?key=XyFa9nnLZWceGMp4OkXybg" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Responsible AI has a burnout problem&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Margaret Mitchell had been working at Google for two years before she realized she needed a break. Only after she spoke with a therapist did she understand the problem: she was burnt out.&lt;/p&gt;&lt;p&gt;Mitchell, who now works as chief ethics scientist at the AI startup Hugging Face, is far from alone in her experience. Burnout is becoming increasingly common in responsible AI teams.&lt;/p&gt;&lt;p&gt;All the practitioners MIT Technology Review interviewed spoke enthusiastically about their work: it is fueled by passion, a sense of urgency, and the satisfaction of building solutions for real problems. But that sense of mission can be overwhelming without the right support. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Melissa Heikkilä&lt;/em&gt;&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="contentBody__summaryBullets--81327c9379272772d1e74a64b6d4868a"&gt;&lt;p&gt;Plus: the AI bubble may be about to burst&lt;/p&gt;
&lt;/div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How churches use data and AI as engines of surveillance&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;On a Sunday morning in a Midwestern megachurch, worshippers step through sliding glass doors into a bustling lobby—unaware they’ve just passed through a gauntlet of biometric surveillance. High-speed cameras snap multiple face “probes” per second, before passing the results to a local neural network that distills these images into digital fingerprints. Before people find their seats, they are matched against an on-premises database—tagged with names, membership tiers, and watch-list flags—that’s stored behind the church’s firewall.&lt;/p&gt;  &lt;p&gt;This hypothetical scene reflects real capabilities increasingly woven into places of worship nationwide, where spiritual care and surveillance converge in ways few congregants ever realize.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Where Big Tech’s rationalist ethos and evangelical spirituality once mixed like oil and holy water, now they’re combining to redraw the contours of community and pastoral power in modern spiritual life. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Alex Ashley&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is from our forthcoming print issue, which is all about security. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: How to run an LLM on your laptop&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;For people who are concerned about privacy, want to break free from the control of the big LLM companies, or just enjoy tinkering, local models offer a compelling alternative to ChatGPT and its web-based peers. Here’s how to get started running one from the safety and comfort of your own computer.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 US tech stocks are sliding over fears the AI bubble may be about to burst&lt;br /&gt;&lt;/strong&gt;After an MIT report found that the vast majority of organizations are getting zero return on their AI investments. (FT $)&lt;br /&gt;+ &lt;em&gt;Even Sam Altman thinks the current hype is unsustainable. &lt;/em&gt;(CNBC)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Meta is reportedly weighing up downsizing its AI division&lt;/strong&gt;&lt;br /&gt;It wants to split it into four groups—and layoffs could be imminent. (NYT $)+ &lt;em&gt;What’s happening with the metaverse, then? &lt;/em&gt;(NY Mag $)&lt;br /&gt;+ &lt;em&gt;Meta is desperately hoping its AI hiring spree will pay off. &lt;/em&gt;(Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 The American Academy of Pediatrics is defying RFK Jr&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;By releasing its own vaccination schedule for children. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;It’s breaking with current CDC recommendations. &lt;/em&gt;(CNN)&lt;br /&gt;+ &lt;em&gt;Why US federal health agencies are abandoning mRNA vaccines. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;4 Elon Musk’s America Party isn’t going so well&lt;/strong&gt;&lt;br /&gt;He’s said to be refocusing his attention on his companies instead. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 The White House has a TikTok account now&lt;/strong&gt;&lt;br /&gt;The very same TikTok that Donald Trump once tried to ban. (WP $)&lt;br /&gt;+ &lt;em&gt;What appears to have changed Congress’ stance? &lt;/em&gt;(The Verge)&lt;br /&gt;+ &lt;em&gt;There’s still no sign of a sale on the horizon. &lt;/em&gt;(The Guardian)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Nvidia is working on another chip for China&lt;br /&gt;&lt;/strong&gt;One that’s faster and more powerful than its current H20 model. (Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 How AGI preppers are bracing themselves for an AI apocalypse&lt;br /&gt;&lt;/strong&gt;Some are spending all their retirement savings along the way. (Insider $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 Demand for critical minerals is soaring&lt;br /&gt;&lt;/strong&gt;Is there a less-invasive way to mine them? (New Scientist $)&lt;br /&gt;+ &lt;em&gt;The race to produce rare earth elements. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What’s an automaker CEO to do?&lt;/strong&gt;&lt;br /&gt;In our increasingly topsy turvy world, many of them feel like they can’t win. (Wired $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 This mattress company is building an AI agent for sleep&lt;/strong&gt;&lt;br /&gt;Eight Sleep’s agent could simulate digital twins of a user’s sleep habits. (The Information $)&lt;br /&gt;+ &lt;em&gt;I tried to hack my insomnia with technology. Here’s what worked. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“&lt;/strong&gt;&lt;strong&gt;Too many cooks, too many kitchens.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Tech investor M.G. Siegler wryly comments on the news Meta is planning to restructure its AI division in a post on Bluesky.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeVwzIiWvdhv0GNsh0V1eu1NNMeADHR99glh5P2WeGfuDEOMd6FO-P_ZjU_amqDbMto2OMrQwZxnAzqVzyt3rAOuKDHYSWepH2WWeFvfpTP3X-2rBkH36cdwETzI8SSSd3J3kKd6Q?key=XyFa9nnLZWceGMp4OkXybg" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Responsible AI has a burnout problem&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Margaret Mitchell had been working at Google for two years before she realized she needed a break. Only after she spoke with a therapist did she understand the problem: she was burnt out.&lt;/p&gt;&lt;p&gt;Mitchell, who now works as chief ethics scientist at the AI startup Hugging Face, is far from alone in her experience. Burnout is becoming increasingly common in responsible AI teams.&lt;/p&gt;&lt;p&gt;All the practitioners MIT Technology Review interviewed spoke enthusiastically about their work: it is fueled by passion, a sense of urgency, and the satisfaction of building solutions for real problems. But that sense of mission can be overwhelming without the right support. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Melissa Heikkilä&lt;/em&gt;&lt;/p&gt;   
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/20/1122162/the-download-churches-in-the-age-of-ai-and-how-to-run-an-llm-at-home/</guid><pubDate>Wed, 20 Aug 2025 12:10:00 +0000</pubDate></item><item><title>Yext Unveils Scout and Launches Webinar to Help Brands Stay Visible in AI &amp; Local Search (AI News)</title><link>https://www.artificialintelligence-news.com/news/yext-unveils-scout-and-launches-webinar-to-help-brands-stay-visible-in-ai-local-search/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/Yext-webinar-PR-banner.jpg" /&gt;&lt;/div&gt;&lt;p&gt;In March&lt;strong&gt; &lt;/strong&gt;Yext, the leading brand visibility platform, launched Yext Scout, an AI search and competitive intelligence agent designed to give brands visibility and actionable insights across both traditional and AI-driven search platforms.&lt;/p&gt;&lt;p&gt;Integrated within the Yext platform, Scout provides insights into visibility across traditional and AI search platforms, benchmarks performance against competitors, and delivers actionable recommendations.&lt;/p&gt;&lt;p&gt;Yext is set to explore the impact of AI on search behaviour and how Scout can empower brands and marketing professionals across EMEA at the webinar, Winning Search in EMEA: How Yext Scout Drives Visibility Across Local and AI Platforms, on &lt;strong&gt;Wednesday, 24 September 2025 at 1PM BST / 2PM CEST&lt;/strong&gt;.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-compete-in-ai-and-local-search-with-scout"&gt;Compete in AI and Local Search with Scout&lt;/h3&gt;&lt;p&gt;AI-driven search platforms like ChatGPT, Google Gemini, Perplexity, and Grok now influence how consumers discover and engage with brands. In many cases, these platforms replace traditional search results with conversational, insight-driven answers. However, most brands need guidance to understand, measure, or optimise how they appear across these emerging channels.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Yext Scout bridges that gap by offering:&lt;/strong&gt;&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Comprehensive visibility&lt;/strong&gt; into your brand’s presence across both AI and traditional search at national to hyper-local levels, including sentiment insights.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Competitive benchmarking&lt;/strong&gt;, revealing how your brand measures up and why competitors perform better.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Prioritized, actionable recommendations&lt;/strong&gt; tailored to improve visibility, sentiment, and performance across listings, social content, reviews, and local pages.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Seamless action&lt;/strong&gt; directly in the Yext platform, enabling users to optimize listings, generate reviews, post updates, and fine-tune local pages at scale.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;By integrating Scout, Yext gives brands the unique ability to see how they show up across AI and local search and take real-time action to improve visibility, sentiment, and performance in one platform.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-webinar-get-ahead-in-ai-search-in-emea"&gt;Webinar: Get Ahead in AI Search in EMEA&lt;/h3&gt;&lt;p&gt;Join Yext for the exclusive webinar, &lt;strong&gt;Winning Search in EMEA: How Yext Scout Drives Visibility Across Local and AI Platforms&lt;/strong&gt;, on &lt;strong&gt;Wednesday, 24 September 2025 at 1PM BST / 2PM CEST&lt;/strong&gt;, to learn how Scout helps multi-location brands stay visible, discoverable and in control across both AI-driven and traditional search platforms.&lt;/p&gt;&lt;p&gt;Key takeaways:&amp;nbsp;&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;How AI is reshaping search behaviour across markets and what that means for your brand&amp;nbsp;&lt;/li&gt;&lt;li&gt;Where (and why) your local business is showing up in AI platforms like ChatGPT and Gemini&amp;nbsp;&lt;/li&gt;&lt;li&gt;How local competitors are earning visibility and how Scout uncovers those insights for you&amp;nbsp;&lt;/li&gt;&lt;li&gt;What actions you can take to improve presence, sentiment and performance – at scale&amp;nbsp;&lt;/li&gt;&lt;li&gt;Live Scout demo including vertical-specific use cases&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Who should attend:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Marketing leaders looking to stay ahead of shifting discovery trends&lt;/li&gt;&lt;li&gt;Local SEO professionals and digital strategists&lt;/li&gt;&lt;li&gt;Brand managers seeking actionable competitive intelligence&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Secure your spot and ensure your brand remains at the forefront of AI-powered discovery.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Register your place.&lt;/strong&gt;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-about-yext"&gt;About Yext&lt;/h3&gt;&lt;p&gt;Yext (NYSE: YEXT) is the leading brand visibility platform, built for a world where discovery and engagement happen everywhere — across AI search, traditional search, social media, websites, and direct communications. Powered by over 2 billion trusted data points and a suite of integrated products, Yext provides brands the clarity, control, and confidence to perform across digital channels. From real-time insights to AI-driven recommendations and execution at scale, Yext turns a brand’s digital presence into a competitive advantage. Thousands of leading brands rely on Yext to stay visible, stay ahead, and grow.&amp;nbsp;&lt;/p&gt;&lt;p&gt;To learn more about Yext, visit Yext.com or follow Yext on LinkedIn and X.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/Yext-webinar-PR-banner.jpg" /&gt;&lt;/div&gt;&lt;p&gt;In March&lt;strong&gt; &lt;/strong&gt;Yext, the leading brand visibility platform, launched Yext Scout, an AI search and competitive intelligence agent designed to give brands visibility and actionable insights across both traditional and AI-driven search platforms.&lt;/p&gt;&lt;p&gt;Integrated within the Yext platform, Scout provides insights into visibility across traditional and AI search platforms, benchmarks performance against competitors, and delivers actionable recommendations.&lt;/p&gt;&lt;p&gt;Yext is set to explore the impact of AI on search behaviour and how Scout can empower brands and marketing professionals across EMEA at the webinar, Winning Search in EMEA: How Yext Scout Drives Visibility Across Local and AI Platforms, on &lt;strong&gt;Wednesday, 24 September 2025 at 1PM BST / 2PM CEST&lt;/strong&gt;.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-compete-in-ai-and-local-search-with-scout"&gt;Compete in AI and Local Search with Scout&lt;/h3&gt;&lt;p&gt;AI-driven search platforms like ChatGPT, Google Gemini, Perplexity, and Grok now influence how consumers discover and engage with brands. In many cases, these platforms replace traditional search results with conversational, insight-driven answers. However, most brands need guidance to understand, measure, or optimise how they appear across these emerging channels.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Yext Scout bridges that gap by offering:&lt;/strong&gt;&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Comprehensive visibility&lt;/strong&gt; into your brand’s presence across both AI and traditional search at national to hyper-local levels, including sentiment insights.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Competitive benchmarking&lt;/strong&gt;, revealing how your brand measures up and why competitors perform better.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Prioritized, actionable recommendations&lt;/strong&gt; tailored to improve visibility, sentiment, and performance across listings, social content, reviews, and local pages.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Seamless action&lt;/strong&gt; directly in the Yext platform, enabling users to optimize listings, generate reviews, post updates, and fine-tune local pages at scale.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;By integrating Scout, Yext gives brands the unique ability to see how they show up across AI and local search and take real-time action to improve visibility, sentiment, and performance in one platform.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-webinar-get-ahead-in-ai-search-in-emea"&gt;Webinar: Get Ahead in AI Search in EMEA&lt;/h3&gt;&lt;p&gt;Join Yext for the exclusive webinar, &lt;strong&gt;Winning Search in EMEA: How Yext Scout Drives Visibility Across Local and AI Platforms&lt;/strong&gt;, on &lt;strong&gt;Wednesday, 24 September 2025 at 1PM BST / 2PM CEST&lt;/strong&gt;, to learn how Scout helps multi-location brands stay visible, discoverable and in control across both AI-driven and traditional search platforms.&lt;/p&gt;&lt;p&gt;Key takeaways:&amp;nbsp;&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;How AI is reshaping search behaviour across markets and what that means for your brand&amp;nbsp;&lt;/li&gt;&lt;li&gt;Where (and why) your local business is showing up in AI platforms like ChatGPT and Gemini&amp;nbsp;&lt;/li&gt;&lt;li&gt;How local competitors are earning visibility and how Scout uncovers those insights for you&amp;nbsp;&lt;/li&gt;&lt;li&gt;What actions you can take to improve presence, sentiment and performance – at scale&amp;nbsp;&lt;/li&gt;&lt;li&gt;Live Scout demo including vertical-specific use cases&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Who should attend:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Marketing leaders looking to stay ahead of shifting discovery trends&lt;/li&gt;&lt;li&gt;Local SEO professionals and digital strategists&lt;/li&gt;&lt;li&gt;Brand managers seeking actionable competitive intelligence&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Secure your spot and ensure your brand remains at the forefront of AI-powered discovery.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Register your place.&lt;/strong&gt;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-about-yext"&gt;About Yext&lt;/h3&gt;&lt;p&gt;Yext (NYSE: YEXT) is the leading brand visibility platform, built for a world where discovery and engagement happen everywhere — across AI search, traditional search, social media, websites, and direct communications. Powered by over 2 billion trusted data points and a suite of integrated products, Yext provides brands the clarity, control, and confidence to perform across digital channels. From real-time insights to AI-driven recommendations and execution at scale, Yext turns a brand’s digital presence into a competitive advantage. Thousands of leading brands rely on Yext to stay visible, stay ahead, and grow.&amp;nbsp;&lt;/p&gt;&lt;p&gt;To learn more about Yext, visit Yext.com or follow Yext on LinkedIn and X.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/yext-unveils-scout-and-launches-webinar-to-help-brands-stay-visible-in-ai-local-search/</guid><pubDate>Wed, 20 Aug 2025 12:12:12 +0000</pubDate></item><item><title>[NEW] NASA’s new AI model can predict when a solar storm may strike (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/20/1122163/nasa-ibm-ai-predict-solar-storm/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/GSFC_20171208_Archive_e000920orig.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;NASA and IBM have released a new open-source machine learning model to help scientists better understand and predict the physics and weather patterns of the sun. Surya, trained on over a decade’s worth of NASA solar data, should help give scientists an early warning when a dangerous solar flare is likely to hit Earth.&lt;/p&gt;  &lt;p&gt;Solar storms occur when the sun erupts energy and particles into space. They can produce solar flares and slower-moving coronal mass ejections that can disrupt radio signals, flip computer bits onboard satellites, and endanger astronauts with bursts of radiation.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;There’s no way to prevent these sorts of effects, but being able to predict when a large solar flare will occur could let people work around them. However, as Louise Harra, an astrophysicist at ETH Zurich, puts it, “when it erupts is always the sticking point.”&lt;/p&gt;  &lt;p&gt;Scientists can easily tell from an image of the sun if there will be a solar flare in the near future, says Harra, who did not work on Surya. But knowing the exact timing and strength of a flare is much harder, she says. That’s a problem because a flare’s size can make the difference between small regional radio blackouts every few weeks (which can still be disruptive) or a devastating solar superstorm that would cause satellites to fall out of orbit and electrical grids to fail. Some solar scientists believe we are overdue for a solar superstorm of this magnitude.&lt;/p&gt; 
 &lt;p&gt;While machine learning has been used to study solar weather events before, the researchers behind Surya hope the quality and sheer scale of their data will help it predict a wider range of events more accurately.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The model’s training data came from NASA’s Solar Dynamics Observatory, which collects pictures of the sun at many different wavelengths of light simultaneously. That made for a dataset of over 250 terabytes in total.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;Early testing of Surya showed it could predict some solar flares two hours in advance. “It can predict the solar flare’s shape, the position in the sun, the intensity,” says Juan Bernabe-Moreno, an AI researcher at IBM who led the Surya project. Two hours may not be enough to protect against all the impacts a strong flare could have, but every moment counts. IBM claims in a blog post that this can as much as double the warning time currently possible with state-of-the-art methods, though exact reported lead times vary. It’s possible this predictive power could be improved through, for example, fine-tuning or by adding other data, as well.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;According to Harra, the hidden patterns underlying events like solar flares are hard to understand from Earth. She says that while astrophysicists know the conditions that make these events happen, they still do not understand why they occur when they do. “It’s just those tiny destabilizations that we know happen, but we don’t know when,” says Harra. The promise of Surya lies in whether it can find the patterns underlying those destabilizations faster than any existing methods, buying us extra time.&lt;/p&gt;  &lt;p&gt;However, Bernabe-Moreno is excited for the potential beyond predicting solar flares. He hopes to use Surya alongside previous models he worked on for IBM and NASA that predict weather here on Earth to better understand how solar storms and Earth weather are connected. “There is some evidence about solar weather influencing lightning, for example,” he says. “What are the cross effects, and where and how do you map the influence from one type of weather to the other?”&lt;/p&gt;  &lt;p&gt;Because Surya is a foundation model, trained without a specialized job, NASA and IBM hope that it can find many patterns in the sun’s physics, much as general-purpose large language models like ChatGPT can take on many different tasks. They believe Surya could even enable new understandings about how other celestial bodies work.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Understanding the sun is a proxy for understanding many other stars,” Bernabe-Moreno says. “We look at the sun as a laboratory.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/GSFC_20171208_Archive_e000920orig.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;NASA and IBM have released a new open-source machine learning model to help scientists better understand and predict the physics and weather patterns of the sun. Surya, trained on over a decade’s worth of NASA solar data, should help give scientists an early warning when a dangerous solar flare is likely to hit Earth.&lt;/p&gt;  &lt;p&gt;Solar storms occur when the sun erupts energy and particles into space. They can produce solar flares and slower-moving coronal mass ejections that can disrupt radio signals, flip computer bits onboard satellites, and endanger astronauts with bursts of radiation.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;There’s no way to prevent these sorts of effects, but being able to predict when a large solar flare will occur could let people work around them. However, as Louise Harra, an astrophysicist at ETH Zurich, puts it, “when it erupts is always the sticking point.”&lt;/p&gt;  &lt;p&gt;Scientists can easily tell from an image of the sun if there will be a solar flare in the near future, says Harra, who did not work on Surya. But knowing the exact timing and strength of a flare is much harder, she says. That’s a problem because a flare’s size can make the difference between small regional radio blackouts every few weeks (which can still be disruptive) or a devastating solar superstorm that would cause satellites to fall out of orbit and electrical grids to fail. Some solar scientists believe we are overdue for a solar superstorm of this magnitude.&lt;/p&gt; 
 &lt;p&gt;While machine learning has been used to study solar weather events before, the researchers behind Surya hope the quality and sheer scale of their data will help it predict a wider range of events more accurately.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The model’s training data came from NASA’s Solar Dynamics Observatory, which collects pictures of the sun at many different wavelengths of light simultaneously. That made for a dataset of over 250 terabytes in total.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;Early testing of Surya showed it could predict some solar flares two hours in advance. “It can predict the solar flare’s shape, the position in the sun, the intensity,” says Juan Bernabe-Moreno, an AI researcher at IBM who led the Surya project. Two hours may not be enough to protect against all the impacts a strong flare could have, but every moment counts. IBM claims in a blog post that this can as much as double the warning time currently possible with state-of-the-art methods, though exact reported lead times vary. It’s possible this predictive power could be improved through, for example, fine-tuning or by adding other data, as well.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;According to Harra, the hidden patterns underlying events like solar flares are hard to understand from Earth. She says that while astrophysicists know the conditions that make these events happen, they still do not understand why they occur when they do. “It’s just those tiny destabilizations that we know happen, but we don’t know when,” says Harra. The promise of Surya lies in whether it can find the patterns underlying those destabilizations faster than any existing methods, buying us extra time.&lt;/p&gt;  &lt;p&gt;However, Bernabe-Moreno is excited for the potential beyond predicting solar flares. He hopes to use Surya alongside previous models he worked on for IBM and NASA that predict weather here on Earth to better understand how solar storms and Earth weather are connected. “There is some evidence about solar weather influencing lightning, for example,” he says. “What are the cross effects, and where and how do you map the influence from one type of weather to the other?”&lt;/p&gt;  &lt;p&gt;Because Surya is a foundation model, trained without a specialized job, NASA and IBM hope that it can find many patterns in the sun’s physics, much as general-purpose large language models like ChatGPT can take on many different tasks. They believe Surya could even enable new understandings about how other celestial bodies work.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Understanding the sun is a proxy for understanding many other stars,” Bernabe-Moreno says. “We look at the sun as a laboratory.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/20/1122163/nasa-ibm-ai-predict-solar-storm/</guid><pubDate>Wed, 20 Aug 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Y Combinator alum SRE.ai raises $7.2M for DevOps AI agents (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/sre-ai-launches-to-automate-complex-enterprise-workflows/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Edward-Aryee-and-Raj-Kadiyala.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;“It wasn’t one big lightbulb; it was death by a thousand cuts,” Edward Aryee said when asked what led him and his co-founder, Raj Kadiyala, to launch SRE.ai.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is offering natural language AI agents that can perform complex enterprise DevOps workflows like continuous integration and testing.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Instead of stitching together different low-code tools for enterprise applications like Salesforce, compared to products built on AWS, GCP, or Azure, teams can now move faster with context-driven, chat-like experiences that work across all of them,” Kadiyala, who is the company’s CEO, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo thought of the product while working at Google Research and DeepMind. Aryee, SRE.ai’s CTO, said they noticed the divide between the infrastructure tooling they had access to versus what others who didn’t work at Google had to use. Their engineer friends lamented about tedious tasks, like untangling metadata conflicts. “It gnawed at us,” Aryee said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He and Kadiyala realized: “The next generation of DevOps experiences needed to be created.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So they founded SRE.ai in 2024 to offer more modern tools to enterprises so they can avoid issues like metadata merge conflicts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other competing players include Copado, Gearset, and Flosum. But Kadiyala said SRE.ai is different in that it works across multiple platforms spanning from AWS to ServiceNow.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company officially came out of stealth on Wednesday and announced a $7.2 million seed round led by Salesforce Ventures and Crane Venture Partners.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Aryee said the SRE.ai onboarding process involves a setup where SRE.ai tools automatically connect with the user’s integrations. The tool can then be customized for a user’s needs like release pipelines, insight dashboards, and data monitoring. Meanwhile, SRE.ai has agents monitoring in the background to flag issues that need attention, such as security risks. The tool then offers recommendations on how to solve the problems. This leaves human IT teams free to tackle bigger, more meaningful projects, rather than being focused on tiresome tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kadiyala described the fundraising process as “high conviction” and noted the round was oversubscribed.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company partook in YC’s Fall ’24 cohort, which helped Aryee and Kadiyala meet their lead investors. They will use the fresh capital to hire AI engineers and Salesforce experts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re seeing a lot of early traction; we’re excited about building out our team to support new customers and extend the platform with new features,” he said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Edward-Aryee-and-Raj-Kadiyala.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;“It wasn’t one big lightbulb; it was death by a thousand cuts,” Edward Aryee said when asked what led him and his co-founder, Raj Kadiyala, to launch SRE.ai.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is offering natural language AI agents that can perform complex enterprise DevOps workflows like continuous integration and testing.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Instead of stitching together different low-code tools for enterprise applications like Salesforce, compared to products built on AWS, GCP, or Azure, teams can now move faster with context-driven, chat-like experiences that work across all of them,” Kadiyala, who is the company’s CEO, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo thought of the product while working at Google Research and DeepMind. Aryee, SRE.ai’s CTO, said they noticed the divide between the infrastructure tooling they had access to versus what others who didn’t work at Google had to use. Their engineer friends lamented about tedious tasks, like untangling metadata conflicts. “It gnawed at us,” Aryee said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He and Kadiyala realized: “The next generation of DevOps experiences needed to be created.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So they founded SRE.ai in 2024 to offer more modern tools to enterprises so they can avoid issues like metadata merge conflicts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other competing players include Copado, Gearset, and Flosum. But Kadiyala said SRE.ai is different in that it works across multiple platforms spanning from AWS to ServiceNow.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company officially came out of stealth on Wednesday and announced a $7.2 million seed round led by Salesforce Ventures and Crane Venture Partners.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Aryee said the SRE.ai onboarding process involves a setup where SRE.ai tools automatically connect with the user’s integrations. The tool can then be customized for a user’s needs like release pipelines, insight dashboards, and data monitoring. Meanwhile, SRE.ai has agents monitoring in the background to flag issues that need attention, such as security risks. The tool then offers recommendations on how to solve the problems. This leaves human IT teams free to tackle bigger, more meaningful projects, rather than being focused on tiresome tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kadiyala described the fundraising process as “high conviction” and noted the round was oversubscribed.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company partook in YC’s Fall ’24 cohort, which helped Aryee and Kadiyala meet their lead investors. They will use the fresh capital to hire AI engineers and Salesforce experts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re seeing a lot of early traction; we’re excited about building out our team to support new customers and extend the platform with new features,” he said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/sre-ai-launches-to-automate-complex-enterprise-workflows/</guid><pubDate>Wed, 20 Aug 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Into the Omniverse: How OpenUSD and Digital Twins Are Powering Industrial and Physical AI (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/openusd-digital-twins-industrial-physical-ai/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This blog is a part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advances in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Investments in industrial AI and physical AI are driving increased demand for digital twins across industries.&lt;/p&gt;
&lt;p&gt;These physically accurate, virtual replicas of real-world environments, facilities and processes aren’t just helping manufacturers streamline planning and optimize operations. They serve as the training ground for helping ensure vision AI agents, autonomous vehicles and robot fleets can operate safely, efficiently and reliably.&lt;/p&gt;
&lt;p&gt;Creating physically accurate simulation environments that enable physical AI to transition seamlessly to the real world typically involves substantial manual effort. However, with the latest advancements in OpenUSD — a powerful open standard for describing and connecting complex 3D worlds — alongside improvements in rendering, neural reconstruction and world foundation models (WFMs), developers can fast-track the construction of digital twins at scale.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Accelerating Digital Twin and Physical AI Development&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To speed digital twin and physical AI development, NVIDIA announced at this year’s SIGGRAPH conference new research, NVIDIA Omniverse libraries, NVIDIA Cosmos WFMs and advanced AI infrastructure — including NVIDIA RTX PRO Servers and NVIDIA DGX Cloud.&lt;/p&gt;

&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Growing OpenUSD Ecosystem&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;OpenUSD serves as a foundational ecosystem for digital twin and physical AI development, empowering developers to integrate industrial and 3D data to create physically accurate digital twins.&lt;/p&gt;
&lt;p&gt;The Alliance for OpenUSD (AOUSD) recently welcomed new general members, including Accenture, Esri, HCLTech, PTC, Renault and Tech Soft 3D. These additions underscore the continued growth of the OpenUSD community and its commitment to unifying 3D workflows across industries.&lt;/p&gt;
&lt;p&gt;To address the growing demand for OpenUSD and digital twins expertise, NVIDIA launched a new industry-recognized OpenUSD development certification and a free digital twins learning path.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Developers Building Digital Twins&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Industry leaders including Siemens, Sight Machine, Rockwell Automation, EDAG, Amazon Devices &amp;amp; Services and Vention are building digital twin solutions with Omniverse libraries and OpenUSD to enable transformation with industrial and physical AI.&lt;/p&gt;
&lt;p&gt;Siemens’ Teamcenter Digital Reality Viewer enables engineers to visualize, interact with and collaborate on photorealistic digital twins at unprecedented scale. These efforts are enabling faster design reviews, minimizing the need for physical prototypes and accelerating time to market — all while reducing costs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-83905" height="450" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/ito-digital-twin-ship.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;Sight Machine’s Operator Agent platform combines live production data, agentic AI-powered recommendations and digital twins to provide real-time visibility into production and enable faster, more informed decisions for plant operations teams.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-83902" height="450" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/rockwell-automation-ito-emulate3d.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;Rockwell Automation’s Emulate3D Factory Test platform enables manufacturers to build factory-scale, physics-based digital twins for simulating, validating and optimizing automation and autonomous systems at scale.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-83896" height="450" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/ito-edag-digital-twin.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;EDAG’s industrial digital twin platform helps manufacturers improve project management, optimize production layouts, train workers and perform data-driven quality assurance.&amp;nbsp;Amazon Devices &amp;amp; Services uses digital twins to train robotic arms to recognize, inspect and handle new devices. Robotic actions can be configured to manufacture products purely based on training performed in simulation — including for steps involved in assembly, testing, packaging and auditing.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Vention is using NVIDIA robotics, AI and simulation technologies — including Omniverse libraries, Isaac Sim and Jetson hardware — to deliver plug-and-play digital twin and automation solutions that simplify and accelerate the deployment of intelligent manufacturing systems.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Plugged Into the World of OpenUSD&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To learn more about OpenUSD and how to develop digital twin applications with Omniverse libraries, take free courses as part of the new digital twin learning path, and check out the Omniverse Kit companion tutorial and how-to guide for deploying Omniverse Kit-based applications at scale.&lt;/p&gt;
&lt;p&gt;Watch a replay of NVIDIA’s SIGGRAPH Research Special Address. Plus, try out Omniverse NuRec on Isaac Sim and CARLA, and learn more about Isaac Sim&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to&lt;/i&gt; &lt;i&gt;NVIDIA Omniverse news&lt;/i&gt;&lt;i&gt;, joining the Omniverse &lt;/i&gt;&lt;i&gt;community&lt;/i&gt;&lt;i&gt; and following Omniverse &lt;/i&gt;&lt;i&gt;on&lt;/i&gt; &lt;i&gt;Discord&lt;/i&gt;&lt;i&gt;,&lt;/i&gt; &lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Threads&lt;/i&gt;&lt;i&gt;,&lt;/i&gt; &lt;i&gt;X&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;and&lt;/i&gt; &lt;i&gt;YouTube&lt;/i&gt;&lt;b&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore the &lt;/i&gt;&lt;i&gt;Alliance for OpenUSD forum&lt;/i&gt;&lt;i&gt; and the &lt;/i&gt;&lt;i&gt;AOUSD website&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;&lt;span&gt;Featured image courtesy of Siemens, Sight Machine.&lt;/span&gt;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This blog is a part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advances in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Investments in industrial AI and physical AI are driving increased demand for digital twins across industries.&lt;/p&gt;
&lt;p&gt;These physically accurate, virtual replicas of real-world environments, facilities and processes aren’t just helping manufacturers streamline planning and optimize operations. They serve as the training ground for helping ensure vision AI agents, autonomous vehicles and robot fleets can operate safely, efficiently and reliably.&lt;/p&gt;
&lt;p&gt;Creating physically accurate simulation environments that enable physical AI to transition seamlessly to the real world typically involves substantial manual effort. However, with the latest advancements in OpenUSD — a powerful open standard for describing and connecting complex 3D worlds — alongside improvements in rendering, neural reconstruction and world foundation models (WFMs), developers can fast-track the construction of digital twins at scale.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Accelerating Digital Twin and Physical AI Development&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To speed digital twin and physical AI development, NVIDIA announced at this year’s SIGGRAPH conference new research, NVIDIA Omniverse libraries, NVIDIA Cosmos WFMs and advanced AI infrastructure — including NVIDIA RTX PRO Servers and NVIDIA DGX Cloud.&lt;/p&gt;

&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Growing OpenUSD Ecosystem&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;OpenUSD serves as a foundational ecosystem for digital twin and physical AI development, empowering developers to integrate industrial and 3D data to create physically accurate digital twins.&lt;/p&gt;
&lt;p&gt;The Alliance for OpenUSD (AOUSD) recently welcomed new general members, including Accenture, Esri, HCLTech, PTC, Renault and Tech Soft 3D. These additions underscore the continued growth of the OpenUSD community and its commitment to unifying 3D workflows across industries.&lt;/p&gt;
&lt;p&gt;To address the growing demand for OpenUSD and digital twins expertise, NVIDIA launched a new industry-recognized OpenUSD development certification and a free digital twins learning path.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Developers Building Digital Twins&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Industry leaders including Siemens, Sight Machine, Rockwell Automation, EDAG, Amazon Devices &amp;amp; Services and Vention are building digital twin solutions with Omniverse libraries and OpenUSD to enable transformation with industrial and physical AI.&lt;/p&gt;
&lt;p&gt;Siemens’ Teamcenter Digital Reality Viewer enables engineers to visualize, interact with and collaborate on photorealistic digital twins at unprecedented scale. These efforts are enabling faster design reviews, minimizing the need for physical prototypes and accelerating time to market — all while reducing costs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-83905" height="450" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/ito-digital-twin-ship.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;Sight Machine’s Operator Agent platform combines live production data, agentic AI-powered recommendations and digital twins to provide real-time visibility into production and enable faster, more informed decisions for plant operations teams.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-83902" height="450" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/rockwell-automation-ito-emulate3d.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;Rockwell Automation’s Emulate3D Factory Test platform enables manufacturers to build factory-scale, physics-based digital twins for simulating, validating and optimizing automation and autonomous systems at scale.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-full wp-image-83896" height="450" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/ito-edag-digital-twin.gif" width="800" /&gt;&lt;/p&gt;
&lt;p&gt;EDAG’s industrial digital twin platform helps manufacturers improve project management, optimize production layouts, train workers and perform data-driven quality assurance.&amp;nbsp;Amazon Devices &amp;amp; Services uses digital twins to train robotic arms to recognize, inspect and handle new devices. Robotic actions can be configured to manufacture products purely based on training performed in simulation — including for steps involved in assembly, testing, packaging and auditing.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Vention is using NVIDIA robotics, AI and simulation technologies — including Omniverse libraries, Isaac Sim and Jetson hardware — to deliver plug-and-play digital twin and automation solutions that simplify and accelerate the deployment of intelligent manufacturing systems.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Plugged Into the World of OpenUSD&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To learn more about OpenUSD and how to develop digital twin applications with Omniverse libraries, take free courses as part of the new digital twin learning path, and check out the Omniverse Kit companion tutorial and how-to guide for deploying Omniverse Kit-based applications at scale.&lt;/p&gt;
&lt;p&gt;Watch a replay of NVIDIA’s SIGGRAPH Research Special Address. Plus, try out Omniverse NuRec on Isaac Sim and CARLA, and learn more about Isaac Sim&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to&lt;/i&gt; &lt;i&gt;NVIDIA Omniverse news&lt;/i&gt;&lt;i&gt;, joining the Omniverse &lt;/i&gt;&lt;i&gt;community&lt;/i&gt;&lt;i&gt; and following Omniverse &lt;/i&gt;&lt;i&gt;on&lt;/i&gt; &lt;i&gt;Discord&lt;/i&gt;&lt;i&gt;,&lt;/i&gt; &lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Threads&lt;/i&gt;&lt;i&gt;,&lt;/i&gt; &lt;i&gt;X&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;and&lt;/i&gt; &lt;i&gt;YouTube&lt;/i&gt;&lt;b&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore the &lt;/i&gt;&lt;i&gt;Alliance for OpenUSD forum&lt;/i&gt;&lt;i&gt; and the &lt;/i&gt;&lt;i&gt;AOUSD website&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;&lt;span&gt;Featured image courtesy of Siemens, Sight Machine.&lt;/span&gt;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/openusd-digital-twins-industrial-physical-ai/</guid><pubDate>Wed, 20 Aug 2025 13:00:08 +0000</pubDate></item><item><title>[NEW] FieldAI raises $405M to build universal robot brains (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/fieldai-raises-405m-to-build-universal-robot-brains/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/FieldAI-Robot1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;FieldAI, an Irvine, California-based robotics startup, has raised $405 million across multiple previously undisclosed rounds to develop what it calls “foundational embodied AI models” — essentially robot brains designed to help everything from humanoids to quadrupeds to self-driving cars adapt to new environments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced the funding Wednesday; the most recent round raised $314 million in August and was co-led by Bezos Expeditions, Prysm, and Temasek. FieldAI’s other backers include Khosla Ventures, Intel Capital, and Canaan Partners, among others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Unlike traditional AI that processes text or images, embodied AI refers to AI that controls physical robots moving through real-world environments. FieldAI builds “Field Foundation Models,” which are general-purpose embodied AI models rooted in physics. This approach gives robots the ability to quickly learn and adapt to new environments while being conscious of risk, FieldAI founder and CEO Ali Agha told TechCrunch in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The mission is to build a single robot brain that can generalize across different robot types and a diverse set of environments,” Agha said. “To get there, you need to manage risk and safety as you go to these new environments. And that has been a fundamental gap in robotics, that traditional models and traditional approaches were never designed to manage that risk and safety.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agha said the key to getting robots to be able to safely learn in new environments is to add a layer of physics into these AI models. This addition gives robots a second set of information to pull from to make decisions —&amp;nbsp;especially in a new environment —&amp;nbsp;as opposed to just reacting to whatever a model says to do next as traditional LLMs do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that while a small amount of AI hallucination isn’t detrimental in certain circumstances, it can be for robots working in dangerous environments or alongside people.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Suddenly you start to have that sense of, how much I know, and if I don’t know something, or if I’m making a decision, how confident I am in it,” Agha said. “Once [the] network starts getting access to that, it starts making much safer decisions. Not just this spits out that, ‘Hey, here’s the next sort of an action,’ but it tells you how confident it is, and you as a customer can define this risk threshold, and [the] robot will be reactive to that.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Agha has been working on this idea for decades across various roles at places ranging from NASA to Massachusetts Institute of Technology (MIT). He decided to launch FieldAI when he achieved a technological breakthrough that allowed one robot brain to work across different types of robots performing both the same and individual actions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching the company in 2023, FieldAI has secured contracts across industries including construction, energy, and urban delivery. The company declined to disclose any customers by name.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The funding will support research and development while helping the company ramp up production to deploy its models to its customers and to further expand its reach abroad.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Agha compares FieldAI’s approach to human evolution. “You evolve to be able to do various different tasks in different environments, and you have the ability to rapidly learn, [and] we believe that is a necessity in robotics. Yes, definitely you can optimize for one specific use case, but that is not the market we are going after.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/FieldAI-Robot1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;FieldAI, an Irvine, California-based robotics startup, has raised $405 million across multiple previously undisclosed rounds to develop what it calls “foundational embodied AI models” — essentially robot brains designed to help everything from humanoids to quadrupeds to self-driving cars adapt to new environments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced the funding Wednesday; the most recent round raised $314 million in August and was co-led by Bezos Expeditions, Prysm, and Temasek. FieldAI’s other backers include Khosla Ventures, Intel Capital, and Canaan Partners, among others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Unlike traditional AI that processes text or images, embodied AI refers to AI that controls physical robots moving through real-world environments. FieldAI builds “Field Foundation Models,” which are general-purpose embodied AI models rooted in physics. This approach gives robots the ability to quickly learn and adapt to new environments while being conscious of risk, FieldAI founder and CEO Ali Agha told TechCrunch in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The mission is to build a single robot brain that can generalize across different robot types and a diverse set of environments,” Agha said. “To get there, you need to manage risk and safety as you go to these new environments. And that has been a fundamental gap in robotics, that traditional models and traditional approaches were never designed to manage that risk and safety.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agha said the key to getting robots to be able to safely learn in new environments is to add a layer of physics into these AI models. This addition gives robots a second set of information to pull from to make decisions —&amp;nbsp;especially in a new environment —&amp;nbsp;as opposed to just reacting to whatever a model says to do next as traditional LLMs do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that while a small amount of AI hallucination isn’t detrimental in certain circumstances, it can be for robots working in dangerous environments or alongside people.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Suddenly you start to have that sense of, how much I know, and if I don’t know something, or if I’m making a decision, how confident I am in it,” Agha said. “Once [the] network starts getting access to that, it starts making much safer decisions. Not just this spits out that, ‘Hey, here’s the next sort of an action,’ but it tells you how confident it is, and you as a customer can define this risk threshold, and [the] robot will be reactive to that.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Agha has been working on this idea for decades across various roles at places ranging from NASA to Massachusetts Institute of Technology (MIT). He decided to launch FieldAI when he achieved a technological breakthrough that allowed one robot brain to work across different types of robots performing both the same and individual actions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching the company in 2023, FieldAI has secured contracts across industries including construction, energy, and urban delivery. The company declined to disclose any customers by name.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The funding will support research and development while helping the company ramp up production to deploy its models to its customers and to further expand its reach abroad.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Agha compares FieldAI’s approach to human evolution. “You evolve to be able to do various different tasks in different environments, and you have the ability to rapidly learn, [and] we believe that is a necessity in robotics. Yes, definitely you can optimize for one specific use case, but that is not the market we are going after.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/fieldai-raises-405m-to-build-universal-robot-brains/</guid><pubDate>Wed, 20 Aug 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Thousands of Grok chats are now searchable on Google (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/thousands-of-grok-chats-are-now-searchable-on-google/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2203472418.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Hundreds of thousands of conversations that users had with Elon Musk’s xAI chatbot Grok are easily accessible through Google Search, reports Forbes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whenever a Grok user clicks the “share” button on a conversation with the chatbot, it creates a unique URL that the user can use to share the conversation via email, text, or on social media. According to Forbes, those URLs are being indexed by search engines like Google, Bing, and DuckDuckGo, which in turn lets anyone look up those conversations on the web.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users of Meta‘s and OpenAI‘s chatbots were recently affected by a similar problem, and like those cases, the chats leaked by Grok give us a glimpse into users’ less-than-respectable desires — questions about how to hack crypto wallets; dirty chats with an explicit AI persona; and asking for instructions on cooking meth.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI’s rules prohibit the use of its bot to “promote critically harming human life” or developing “bioweapons, chemical weapons, or weapons of mass destruction,” though that obviously hasn’t stopped users from asking Grok for help with such things anyway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to conversations made accessible by Google, Grok gave users instructions on making fentanyl, listed various suicide methods, handed out bomb construction tips, and even provided a detailed plan for the assassination of Elon Musk.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI did not immediately respond to a request for comment. We’ve also asked when xAI began indexing Grok conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Late last month, ChatGPT users sounded the alarm that their chats were being indexed on Google, which OpenAI described as a “short-lived experiment.” In a post Musk quote-tweeted with the words “Grok ftw,” Grok explained that it had “no such sharing feature” and “prioritize[s] privacy.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2203472418.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Hundreds of thousands of conversations that users had with Elon Musk’s xAI chatbot Grok are easily accessible through Google Search, reports Forbes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whenever a Grok user clicks the “share” button on a conversation with the chatbot, it creates a unique URL that the user can use to share the conversation via email, text, or on social media. According to Forbes, those URLs are being indexed by search engines like Google, Bing, and DuckDuckGo, which in turn lets anyone look up those conversations on the web.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users of Meta‘s and OpenAI‘s chatbots were recently affected by a similar problem, and like those cases, the chats leaked by Grok give us a glimpse into users’ less-than-respectable desires — questions about how to hack crypto wallets; dirty chats with an explicit AI persona; and asking for instructions on cooking meth.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI’s rules prohibit the use of its bot to “promote critically harming human life” or developing “bioweapons, chemical weapons, or weapons of mass destruction,” though that obviously hasn’t stopped users from asking Grok for help with such things anyway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to conversations made accessible by Google, Grok gave users instructions on making fentanyl, listed various suicide methods, handed out bomb construction tips, and even provided a detailed plan for the assassination of Elon Musk.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI did not immediately respond to a request for comment. We’ve also asked when xAI began indexing Grok conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Late last month, ChatGPT users sounded the alarm that their chats were being indexed on Google, which OpenAI described as a “short-lived experiment.” In a post Musk quote-tweeted with the words “Grok ftw,” Grok explained that it had “no such sharing feature” and “prioritize[s] privacy.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/thousands-of-grok-chats-are-now-searchable-on-google/</guid><pubDate>Wed, 20 Aug 2025 14:04:43 +0000</pubDate></item><item><title>[NEW] Dex is an AI-powered camera device that helps children learn new languages (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/dex-is-an-ai-powered-camera-device-that-helps-children-learn-new-languages/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Three parents — Reni Cao, Xiao Zhang, and Susan Rosenthal — were worried about their children’s screen time, so they left their tech jobs to create a product that encourages children to engage with the real world while also helping them learn a new language. Their move has paid off, as the company recently raised $4.8 million in funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The newly launched gadget is called Dex and resembles a high-tech magnifying glass with a camera lens on one side and a touchscreen on the other. When kids use the device to take pictures of objects, the AI utilizes image recognition technology to identify the object and translate the word into the selected language. It also features interactive story lessons and games.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While kid-focused language learning apps like Duolingo Kids exist, Dex argues that it takes a more engaging approach that emphasizes hands-on experiences, allowing children to immerse themselves in the language.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re trying to teach authentic language in the real world in a way that’s interactive,” Cao told TechCrunch. “The kids are not only listening or doing what they are told to do, but rather, they are actually thinking, creating, interacting, running around, and just being curious about things, and acquire the necessary language associated with those concepts and objects.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex is designed for kids ages 3 to 8 years old and currently supports Chinese, French, German, Hindi, Italian, Japanese, Korean, and Spanish. It also offers support for 34 dialects, including Egyptian Arabic, Taiwanese Mandarin, and Mexican Spanish.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to object recognition, Dex features a library of interactive stories that encourage children to actively participate in the narrative. As the story unfolds, kids are prompted to respond, such as greeting characters in the language they are learning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The device comes with a dedicated app for parents to see a detailed overview of their child’s progress, including the vocabulary words they’ve learned, the stories they’ve engaged with, and the number of consecutive days they’ve used Dex.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038093" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/dex-app-screenshot.jpeg?w=345" width="345" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Dex&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Dex is currently developing a feature that allows kids to ask an AI chatbot questions and engage in free-form conversations. This feature is already available to some testers, but the company admits it isn’t ready for a wider rollout. Parents might also be cautious about introducing AI chatbots to their children. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During our testing of Dex, we had concerns about the possibility of a child learning inappropriate words. Cao assured us that “rigid safety prompts” are included whenever the large language model is used across vision, reasoning, and text-to-speech. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He said, “We have an always-on safety agent that evaluates conversations in real time and filters conversations with a safe stop word list. The agent will suppress conversation if any of the stop words are mentioned, including but not limited to those related to sexuality, religion, politics, etc. Parents will soon be able to further add to personalized stop-word lists.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, it said that the AI is trained using vocabulary standards similar to those found in Britannica Kids and other children’s encyclopedias.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In our testing, the AI successfully ignored topics related to nudity. However, it did recognize and accurately translate the term “gun,” something parents should consider when purchasing the device. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to our findings, Cao told us, “Regulation-wise, I’m not worried, but I do think this presents a concern, especially among [some] parents.” He added that these concerns have pushed the company to soon introduce an option in settings to filter out specific words, such as guns, cigarettes, vape pens, fireworks, marijuana, and beer bottles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex also has a zero data retention policy. While this means there’s no risk of sensitive or personal images being stored, one downside could be that parents are left in the dark about the type of content their kids may be capturing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex is also actively working toward obtaining COPPA certification, which would make it compliant with the Children’s Online Privacy Protection Act.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Dex founders Reni Cao (CEO), Charlie Zhang (CTO), and Susan Rosenthal (Head of Ops)" class="wp-image-3038091" height="453" src="https://techcrunch.com/wp-content/uploads/2025/08/Dex-founders.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Dex founders Reni Cao (CEO), Xiao Zhang (CTO), and Susan Rosenthal (Head of Ops)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Dex&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company secured funding from ClayVC, Embedding VC, Parable, and UpscaleX. Notable angel investors include Pinterest founder Ben Silbermann, Curated co-founder Eduardo Vivas, Lilian Weng, who is the former head of safety at OpenAI, and Richard Wong (ex-Coursera).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The device is priced at $250, which feels steep for a product designed for children. However, Dex positions itself as a more affordable alternative to hiring a tutor, which can charge up to $80 per hour, or attending a language immersion school, which can cost several hundred to even thousands of dollars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex says that hundreds of families have already purchased the device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Three parents — Reni Cao, Xiao Zhang, and Susan Rosenthal — were worried about their children’s screen time, so they left their tech jobs to create a product that encourages children to engage with the real world while also helping them learn a new language. Their move has paid off, as the company recently raised $4.8 million in funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The newly launched gadget is called Dex and resembles a high-tech magnifying glass with a camera lens on one side and a touchscreen on the other. When kids use the device to take pictures of objects, the AI utilizes image recognition technology to identify the object and translate the word into the selected language. It also features interactive story lessons and games.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While kid-focused language learning apps like Duolingo Kids exist, Dex argues that it takes a more engaging approach that emphasizes hands-on experiences, allowing children to immerse themselves in the language.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re trying to teach authentic language in the real world in a way that’s interactive,” Cao told TechCrunch. “The kids are not only listening or doing what they are told to do, but rather, they are actually thinking, creating, interacting, running around, and just being curious about things, and acquire the necessary language associated with those concepts and objects.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex is designed for kids ages 3 to 8 years old and currently supports Chinese, French, German, Hindi, Italian, Japanese, Korean, and Spanish. It also offers support for 34 dialects, including Egyptian Arabic, Taiwanese Mandarin, and Mexican Spanish.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to object recognition, Dex features a library of interactive stories that encourage children to actively participate in the narrative. As the story unfolds, kids are prompted to respond, such as greeting characters in the language they are learning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The device comes with a dedicated app for parents to see a detailed overview of their child’s progress, including the vocabulary words they’ve learned, the stories they’ve engaged with, and the number of consecutive days they’ve used Dex.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038093" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/dex-app-screenshot.jpeg?w=345" width="345" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Dex&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Dex is currently developing a feature that allows kids to ask an AI chatbot questions and engage in free-form conversations. This feature is already available to some testers, but the company admits it isn’t ready for a wider rollout. Parents might also be cautious about introducing AI chatbots to their children. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During our testing of Dex, we had concerns about the possibility of a child learning inappropriate words. Cao assured us that “rigid safety prompts” are included whenever the large language model is used across vision, reasoning, and text-to-speech. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He said, “We have an always-on safety agent that evaluates conversations in real time and filters conversations with a safe stop word list. The agent will suppress conversation if any of the stop words are mentioned, including but not limited to those related to sexuality, religion, politics, etc. Parents will soon be able to further add to personalized stop-word lists.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, it said that the AI is trained using vocabulary standards similar to those found in Britannica Kids and other children’s encyclopedias.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In our testing, the AI successfully ignored topics related to nudity. However, it did recognize and accurately translate the term “gun,” something parents should consider when purchasing the device. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to our findings, Cao told us, “Regulation-wise, I’m not worried, but I do think this presents a concern, especially among [some] parents.” He added that these concerns have pushed the company to soon introduce an option in settings to filter out specific words, such as guns, cigarettes, vape pens, fireworks, marijuana, and beer bottles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex also has a zero data retention policy. While this means there’s no risk of sensitive or personal images being stored, one downside could be that parents are left in the dark about the type of content their kids may be capturing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex is also actively working toward obtaining COPPA certification, which would make it compliant with the Children’s Online Privacy Protection Act.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Dex founders Reni Cao (CEO), Charlie Zhang (CTO), and Susan Rosenthal (Head of Ops)" class="wp-image-3038091" height="453" src="https://techcrunch.com/wp-content/uploads/2025/08/Dex-founders.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Dex founders Reni Cao (CEO), Xiao Zhang (CTO), and Susan Rosenthal (Head of Ops)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Dex&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company secured funding from ClayVC, Embedding VC, Parable, and UpscaleX. Notable angel investors include Pinterest founder Ben Silbermann, Curated co-founder Eduardo Vivas, Lilian Weng, who is the former head of safety at OpenAI, and Richard Wong (ex-Coursera).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The device is priced at $250, which feels steep for a product designed for children. However, Dex positions itself as a more affordable alternative to hiring a tutor, which can charge up to $80 per hour, or attending a language immersion school, which can cost several hundred to even thousands of dollars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex says that hundreds of families have already purchased the device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/dex-is-an-ai-powered-camera-device-that-helps-children-learn-new-languages/</guid><pubDate>Wed, 20 Aug 2025 14:19:54 +0000</pubDate></item><item><title>[NEW] Google Cloud unveils AI ally for security teams (AI News)</title><link>https://www.artificialintelligence-news.com/news/google-cloud-unveils-ai-ally-for-security-teams/</link><description>&lt;p&gt;Google Cloud believes the answer to overworked security teams isn’t just more tools, but an AI-powered ally.&lt;/p&gt;&lt;p&gt;At its Security Summit 2025, Google laid out its vision for a future where AI frees up human security experts from tedious work to focus on what matters most.&lt;/p&gt;&lt;p&gt;The central idea is to use AI to defend your organisation while securing your own AI initiatives from attack. As businesses increasingly rely on AI agents, these agents themselves become a new frontier for security concerns.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-securing-the-ai-ecosystem"&gt;Securing the AI ecosystem&lt;/h3&gt;&lt;p&gt;Before AI can become a trusted defender, its own environment must be secure. To this end, Google Cloud is enhancing its AI Protection solution within the Security Command Center.&lt;/p&gt;&lt;p&gt;New capabilities, arriving soon in preview, will automatically discover all the AI agents and servers in your environment. This will give security teams a clear view of their entire AI agent ecosystem, helping them to spot vulnerabilities, misconfigurations, and risky interactions.&lt;/p&gt;&lt;p&gt;Real-time protection is also getting a boost. Model Armor’s in-line protection is being extended to prompts and responses within Agentspace, helping to block threats like prompt injection and data leaks as they happen.&lt;/p&gt;&lt;p&gt;To ensure AI agents are always playing by the rules, new posture controls will help them stick to company security policies. And with new threat detections powered by intelligence from Mandiant and Google Cloud, security teams can now better spot and respond to unusual or suspicious behaviour from their AI assets.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-rise-of-the-agentic-soc"&gt;Rise of the agentic SOC&lt;/h3&gt;&lt;p&gt;Perhaps the most forward-looking announcement is Google’s vision for an “agentic security operations centre (SOC)”. Imagine a system where AI agents collaborate to manage threats, automate alert investigations, and even help engineers create new detections to fill security gaps.&lt;/p&gt;&lt;p&gt;The first step in this direction is the new Alert Investigation agent, which is now in preview. This tool acts like a junior analyst, autonomously looking into security events, analysing command-line activity, and mapping out process trees based on the proven methods of Mandiant’s frontline experts. The agent provides its verdict on alerts and suggests next steps for human analysts, promising to cut down on manual work and speed up response times.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-security-built-on-google-cloud-s-unified-foundation"&gt;AI security built on Google Cloud’s unified foundation&lt;/h3&gt;&lt;p&gt;In Google Security Operations, the new SecOps Labs gives users early access to powerful capabilities, many of which are powered by Gemini AI. New dashboards that bring together security orchestration, automation, and response (SOAR) data are also now generally available, giving a clearer picture of an organisation’s security posture.&lt;/p&gt;&lt;p&gt;The platform’s foundation, the Trusted Cloud, is also receiving upgrades:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Compliance and risk:&lt;/strong&gt; A new Compliance Manager simplifies the complex world of audits and policy enforcement, while new Risk Reports use virtual red team technology to find security gaps that attackers could exploit.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Smarter access control:&lt;/strong&gt; The tedious task of granting permissions gets an AI assist with the new IAM role picker, now in preview. You can simply describe what a person or service needs to do, and Gemini will recommend the most secure, least-permissive role. To guard against account takeovers, re-authentication will now be triggered for highly sensitive actions.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Expanded data and network security:&lt;/strong&gt; Sensitive Data Protection has been expanded to cover AI tools like Vertex AI, and Cloud NGFW now helps apply Zero Trust principles to high-performance computing workloads, including AI.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;By embedding AI into the core of its offerings, Google Cloud is working to create a foundation where security enables business goals and helps defenders tackle the challenges of a new era.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Ameer Basheer)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Why security chiefs demand urgent regulation of AI like DeepSeek&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Google Cloud believes the answer to overworked security teams isn’t just more tools, but an AI-powered ally.&lt;/p&gt;&lt;p&gt;At its Security Summit 2025, Google laid out its vision for a future where AI frees up human security experts from tedious work to focus on what matters most.&lt;/p&gt;&lt;p&gt;The central idea is to use AI to defend your organisation while securing your own AI initiatives from attack. As businesses increasingly rely on AI agents, these agents themselves become a new frontier for security concerns.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-securing-the-ai-ecosystem"&gt;Securing the AI ecosystem&lt;/h3&gt;&lt;p&gt;Before AI can become a trusted defender, its own environment must be secure. To this end, Google Cloud is enhancing its AI Protection solution within the Security Command Center.&lt;/p&gt;&lt;p&gt;New capabilities, arriving soon in preview, will automatically discover all the AI agents and servers in your environment. This will give security teams a clear view of their entire AI agent ecosystem, helping them to spot vulnerabilities, misconfigurations, and risky interactions.&lt;/p&gt;&lt;p&gt;Real-time protection is also getting a boost. Model Armor’s in-line protection is being extended to prompts and responses within Agentspace, helping to block threats like prompt injection and data leaks as they happen.&lt;/p&gt;&lt;p&gt;To ensure AI agents are always playing by the rules, new posture controls will help them stick to company security policies. And with new threat detections powered by intelligence from Mandiant and Google Cloud, security teams can now better spot and respond to unusual or suspicious behaviour from their AI assets.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-rise-of-the-agentic-soc"&gt;Rise of the agentic SOC&lt;/h3&gt;&lt;p&gt;Perhaps the most forward-looking announcement is Google’s vision for an “agentic security operations centre (SOC)”. Imagine a system where AI agents collaborate to manage threats, automate alert investigations, and even help engineers create new detections to fill security gaps.&lt;/p&gt;&lt;p&gt;The first step in this direction is the new Alert Investigation agent, which is now in preview. This tool acts like a junior analyst, autonomously looking into security events, analysing command-line activity, and mapping out process trees based on the proven methods of Mandiant’s frontline experts. The agent provides its verdict on alerts and suggests next steps for human analysts, promising to cut down on manual work and speed up response times.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-security-built-on-google-cloud-s-unified-foundation"&gt;AI security built on Google Cloud’s unified foundation&lt;/h3&gt;&lt;p&gt;In Google Security Operations, the new SecOps Labs gives users early access to powerful capabilities, many of which are powered by Gemini AI. New dashboards that bring together security orchestration, automation, and response (SOAR) data are also now generally available, giving a clearer picture of an organisation’s security posture.&lt;/p&gt;&lt;p&gt;The platform’s foundation, the Trusted Cloud, is also receiving upgrades:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Compliance and risk:&lt;/strong&gt; A new Compliance Manager simplifies the complex world of audits and policy enforcement, while new Risk Reports use virtual red team technology to find security gaps that attackers could exploit.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Smarter access control:&lt;/strong&gt; The tedious task of granting permissions gets an AI assist with the new IAM role picker, now in preview. You can simply describe what a person or service needs to do, and Gemini will recommend the most secure, least-permissive role. To guard against account takeovers, re-authentication will now be triggered for highly sensitive actions.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Expanded data and network security:&lt;/strong&gt; Sensitive Data Protection has been expanded to cover AI tools like Vertex AI, and Cloud NGFW now helps apply Zero Trust principles to high-performance computing workloads, including AI.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;By embedding AI into the core of its offerings, Google Cloud is working to create a foundation where security enables business goals and helps defenders tackle the challenges of a new era.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Ameer Basheer)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Why security chiefs demand urgent regulation of AI like DeepSeek&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/google-cloud-unveils-ai-ally-for-security-teams/</guid><pubDate>Wed, 20 Aug 2025 15:21:44 +0000</pubDate></item><item><title>[NEW] MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a gradient background transitioning from blue to pink. From left to right: a network or molecule structure with a central circle and six surrounding nodes, a 3D cube, and an open laptop with an eye symbol above it." class="wp-image-1147994" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;A new research framework helps AI agents explore three-dimensional spaces they can’t directly detect. Called MindJourney, the approach addresses a key limitation in vision-language models (VLMs), which give AI agents their ability to interpret and describe visual scenes.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While VLMs&amp;nbsp;are strong&amp;nbsp;at identifying objects in&amp;nbsp;static&amp;nbsp;images,&amp;nbsp;they struggle to&amp;nbsp;interpret&amp;nbsp;the interactive 3D world behind 2D images.&amp;nbsp;This&amp;nbsp;gap shows up&amp;nbsp;in spatial&amp;nbsp;questions&amp;nbsp;like&amp;nbsp;“If I sit on the couch&amp;nbsp;that is on my right&amp;nbsp;and face the chairs, will the kitchen be to my right or left?”—tasks that require an agent to&amp;nbsp;interpret&amp;nbsp;its&amp;nbsp;position and movement through space.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;People&amp;nbsp;overcome this challenge by mentally exploring a space,&amp;nbsp;imagining moving through it and combining those mental snapshots to work out where objects are.&amp;nbsp;MindJourney&amp;nbsp;applies the same process&amp;nbsp;to&amp;nbsp;AI agents,&amp;nbsp;letting&amp;nbsp;them roam a virtual&amp;nbsp;space before answering spatial questions.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-mindjourney-navigates-3d-space"&gt;How&amp;nbsp;MindJourney&amp;nbsp;navigates 3D space&lt;/h2&gt;



&lt;p&gt;To perform this type of spatial navigation,&amp;nbsp;MindJourney&amp;nbsp;uses a&amp;nbsp;&lt;em&gt;world model&lt;/em&gt;—in this case,&amp;nbsp;a video generation system trained on a large collection of videos captured from a single moving viewpoint, showing actions such as going forward and turning left of right, much like a 3D cinematographer. From this, it learns to predict how a new scene would appear from different perspectives.&lt;/p&gt;



&lt;p&gt;At inference time, the model can generate photo-realistic images of a scene based on possible movements from the agent’s current position. It generates multiple possible views of a scene while the VLM acts as a filter, selecting the constructed perspectives that are most likely to answer the user’s question.&lt;/p&gt;



&lt;p&gt;These are kept and expanded in the next iteration, while less promising paths are discarded. This process, shown in Figure 1, avoids the need to generate and evaluate thousands of possible movement sequences by focusing only on the most informative perspectives.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM's spatial interpretation through generated observations when encountering a new  challenges. " class="wp-image-1147968" height="854" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM’s spatial interpretation through generated observations when encountering new challenges.&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;To make its search through&amp;nbsp;a simulated&amp;nbsp;space both effective and efficient,&amp;nbsp;MindJourney&amp;nbsp;uses a &lt;em&gt;spatial beam search&lt;/em&gt;—an&amp;nbsp;algorithm that prioritizes the most promising paths. It works within a fixed number of steps, each representing a movement. By balancing breadth with depth, spatial beam search enables&amp;nbsp;MindJourney&amp;nbsp;to gather strong supporting evidence.&amp;nbsp;This process is illustrated in Figure 2.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="MindJourney pipeline diagram" class="wp-image-1147897" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. The MindJourney workflow starts with a spatial beam search for a set number of steps before answering the query. The world model interactively generates new observations, while a VLM interprets the generated images, guiding the search throughout the process.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p class="has-text-align-left"&gt;By iterating through&amp;nbsp;simulation,&amp;nbsp;evaluation, and integration,&amp;nbsp;MindJourney&amp;nbsp;can reason about spatial relationships far beyond what any single 2D image can convey, all without the need for additional training.&amp;nbsp;On&amp;nbsp;the&amp;nbsp;Spatial Aptitude Training (SAT)&amp;nbsp;benchmark,&amp;nbsp;it improved the accuracy of&amp;nbsp;VLMs&amp;nbsp;by&amp;nbsp;8%&amp;nbsp;over&amp;nbsp;their&amp;nbsp;baseline&amp;nbsp;performance.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="building-smarter-agents"&gt;Building&amp;nbsp;smarter agents&amp;nbsp;&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;MindJourney&amp;nbsp;showed&amp;nbsp;strong performance&amp;nbsp;on multiple 3D spatial-reasoning benchmarks, and even advanced VLMs&amp;nbsp;improved&amp;nbsp;when paired with its imagination loop. This suggests that the spatial patterns that world models learn from raw images, combined with the symbolic&amp;nbsp;capabilities&amp;nbsp;of VLMs, create a more complete spatial capability&amp;nbsp;for agents. Together, they enable agents to infer what lies beyond the visible frame and&amp;nbsp;interpret&amp;nbsp;the physical world&amp;nbsp;more accurately.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It also demonstrates that pretrained VLMs and trainable world models can work together in 3D without retraining either one—pointing toward general-purpose agents capable of&amp;nbsp;interpreting&amp;nbsp;and acting in real-world environments. This opens the way to&amp;nbsp;possible&amp;nbsp;applications in autonomous robotics, smart home technologies, and accessibility tools for people with visual impairments.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By converting systems that simply describe static images into active agents that continually evaluate where to look next,&amp;nbsp;MindJourney&amp;nbsp;connects computer vision with planning. Because exploration occurs entirely within the model’s latent space—its internal representation of the scene—robots would be able to test multiple viewpoints before determining their next move,&amp;nbsp;potentially&amp;nbsp;reducing wear, energy use, and collision risk.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Looking ahead, we plan to extend the framework to&amp;nbsp;use&amp;nbsp;world models that&amp;nbsp;not only&amp;nbsp;predict&amp;nbsp;new viewpoints&amp;nbsp;but also forecast&amp;nbsp;how the scene might change over time.&amp;nbsp;We envision&amp;nbsp;MindJourney&amp;nbsp;working&amp;nbsp;alongside VLMs that interpret&amp;nbsp;those predictions&amp;nbsp;and use&amp;nbsp;them to&amp;nbsp;plan&amp;nbsp;what to do&amp;nbsp;next. This&amp;nbsp;enhancement could enable&amp;nbsp;agents&amp;nbsp;more accurately&amp;nbsp;interpret&amp;nbsp;spatial relationships and physical dynamics, helping them to operate effectively&amp;nbsp;in changing environments.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a gradient background transitioning from blue to pink. From left to right: a network or molecule structure with a central circle and six surrounding nodes, a 3D cube, and an open laptop with an eye symbol above it." class="wp-image-1147994" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;A new research framework helps AI agents explore three-dimensional spaces they can’t directly detect. Called MindJourney, the approach addresses a key limitation in vision-language models (VLMs), which give AI agents their ability to interpret and describe visual scenes.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While VLMs&amp;nbsp;are strong&amp;nbsp;at identifying objects in&amp;nbsp;static&amp;nbsp;images,&amp;nbsp;they struggle to&amp;nbsp;interpret&amp;nbsp;the interactive 3D world behind 2D images.&amp;nbsp;This&amp;nbsp;gap shows up&amp;nbsp;in spatial&amp;nbsp;questions&amp;nbsp;like&amp;nbsp;“If I sit on the couch&amp;nbsp;that is on my right&amp;nbsp;and face the chairs, will the kitchen be to my right or left?”—tasks that require an agent to&amp;nbsp;interpret&amp;nbsp;its&amp;nbsp;position and movement through space.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;People&amp;nbsp;overcome this challenge by mentally exploring a space,&amp;nbsp;imagining moving through it and combining those mental snapshots to work out where objects are.&amp;nbsp;MindJourney&amp;nbsp;applies the same process&amp;nbsp;to&amp;nbsp;AI agents,&amp;nbsp;letting&amp;nbsp;them roam a virtual&amp;nbsp;space before answering spatial questions.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-mindjourney-navigates-3d-space"&gt;How&amp;nbsp;MindJourney&amp;nbsp;navigates 3D space&lt;/h2&gt;



&lt;p&gt;To perform this type of spatial navigation,&amp;nbsp;MindJourney&amp;nbsp;uses a&amp;nbsp;&lt;em&gt;world model&lt;/em&gt;—in this case,&amp;nbsp;a video generation system trained on a large collection of videos captured from a single moving viewpoint, showing actions such as going forward and turning left of right, much like a 3D cinematographer. From this, it learns to predict how a new scene would appear from different perspectives.&lt;/p&gt;



&lt;p&gt;At inference time, the model can generate photo-realistic images of a scene based on possible movements from the agent’s current position. It generates multiple possible views of a scene while the VLM acts as a filter, selecting the constructed perspectives that are most likely to answer the user’s question.&lt;/p&gt;



&lt;p&gt;These are kept and expanded in the next iteration, while less promising paths are discarded. This process, shown in Figure 1, avoids the need to generate and evaluate thousands of possible movement sequences by focusing only on the most informative perspectives.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM's spatial interpretation through generated observations when encountering a new  challenges. " class="wp-image-1147968" height="854" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM’s spatial interpretation through generated observations when encountering new challenges.&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;To make its search through&amp;nbsp;a simulated&amp;nbsp;space both effective and efficient,&amp;nbsp;MindJourney&amp;nbsp;uses a &lt;em&gt;spatial beam search&lt;/em&gt;—an&amp;nbsp;algorithm that prioritizes the most promising paths. It works within a fixed number of steps, each representing a movement. By balancing breadth with depth, spatial beam search enables&amp;nbsp;MindJourney&amp;nbsp;to gather strong supporting evidence.&amp;nbsp;This process is illustrated in Figure 2.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="MindJourney pipeline diagram" class="wp-image-1147897" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. The MindJourney workflow starts with a spatial beam search for a set number of steps before answering the query. The world model interactively generates new observations, while a VLM interprets the generated images, guiding the search throughout the process.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p class="has-text-align-left"&gt;By iterating through&amp;nbsp;simulation,&amp;nbsp;evaluation, and integration,&amp;nbsp;MindJourney&amp;nbsp;can reason about spatial relationships far beyond what any single 2D image can convey, all without the need for additional training.&amp;nbsp;On&amp;nbsp;the&amp;nbsp;Spatial Aptitude Training (SAT)&amp;nbsp;benchmark,&amp;nbsp;it improved the accuracy of&amp;nbsp;VLMs&amp;nbsp;by&amp;nbsp;8%&amp;nbsp;over&amp;nbsp;their&amp;nbsp;baseline&amp;nbsp;performance.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="building-smarter-agents"&gt;Building&amp;nbsp;smarter agents&amp;nbsp;&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;MindJourney&amp;nbsp;showed&amp;nbsp;strong performance&amp;nbsp;on multiple 3D spatial-reasoning benchmarks, and even advanced VLMs&amp;nbsp;improved&amp;nbsp;when paired with its imagination loop. This suggests that the spatial patterns that world models learn from raw images, combined with the symbolic&amp;nbsp;capabilities&amp;nbsp;of VLMs, create a more complete spatial capability&amp;nbsp;for agents. Together, they enable agents to infer what lies beyond the visible frame and&amp;nbsp;interpret&amp;nbsp;the physical world&amp;nbsp;more accurately.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It also demonstrates that pretrained VLMs and trainable world models can work together in 3D without retraining either one—pointing toward general-purpose agents capable of&amp;nbsp;interpreting&amp;nbsp;and acting in real-world environments. This opens the way to&amp;nbsp;possible&amp;nbsp;applications in autonomous robotics, smart home technologies, and accessibility tools for people with visual impairments.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By converting systems that simply describe static images into active agents that continually evaluate where to look next,&amp;nbsp;MindJourney&amp;nbsp;connects computer vision with planning. Because exploration occurs entirely within the model’s latent space—its internal representation of the scene—robots would be able to test multiple viewpoints before determining their next move,&amp;nbsp;potentially&amp;nbsp;reducing wear, energy use, and collision risk.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Looking ahead, we plan to extend the framework to&amp;nbsp;use&amp;nbsp;world models that&amp;nbsp;not only&amp;nbsp;predict&amp;nbsp;new viewpoints&amp;nbsp;but also forecast&amp;nbsp;how the scene might change over time.&amp;nbsp;We envision&amp;nbsp;MindJourney&amp;nbsp;working&amp;nbsp;alongside VLMs that interpret&amp;nbsp;those predictions&amp;nbsp;and use&amp;nbsp;them to&amp;nbsp;plan&amp;nbsp;what to do&amp;nbsp;next. This&amp;nbsp;enhancement could enable&amp;nbsp;agents&amp;nbsp;more accurately&amp;nbsp;interpret&amp;nbsp;spatial relationships and physical dynamics, helping them to operate effectively&amp;nbsp;in changing environments.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/</guid><pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] Harvard dropouts to launch ‘always on’ AI smart glasses that listen and record every conversation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/harvard-dropouts-to-launch-always-on-ai-smart-glasses-that-listen-and-record-every-conversation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/halo-ai-smart-glasses-rendering.jpeg?resize=1200,1078" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Two former Harvard students are launching a pair of “always-on” AI-powered smart glasses that listen to, record, and transcribe every conversation and then display relevant information to the wearer in real time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our goal is to make glasses that make you super intelligent the moment you put them on,” said AnhPhu Nguyen, co-founder of Halo, a startup that’s developing the technology.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, as his co-founder Caine Ardayfio put it, the glasses “give you infinite memory.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The AI listens to every conversation you have and uses that knowledge to tell you what to say&amp;nbsp;… kinda like IRL Cluely,” Ardayfio told TechCrunch, referring to the startup that claims to help users “cheat” on everything from job interviews to school exams.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If somebody says a complex word or asks you a question, like, ‘What’s 37 to the third power?’ or something like that, then it’ll pop up on the glasses,” Ardayfio added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ardayfio and Nguyen have raised $1 million to develop the glasses, led by Pillar VC, with support from Soma Capital, Village Global, and Morningside Venture. The glasses will be priced at $249 and will be available for preorder starting Wednesday. Ardayfio called the glasses “the first real step towards vibe thinking.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The two Ivy League dropouts, who have since moved into their own version of the Hacker Hostel in the San Francisco Bay Area, recently caused a stir after developing a facial-recognition app for Meta’s smart Ray-Ban glasses to prove that the tech could be used to dox people. As a potential early competitor to Meta’s smart glasses, Ardayfio said Meta, given its history of security and privacy scandals, had to rein in its product in ways that Halo can ultimately capitalize on.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Meta doesn’t have a great reputation for caring about user privacy, and for them to release something that’s always there with you — which obviously brings a ton of utility — is just a huge reputational risk for them that they probably won’t take before a startup does it at scale first,” Nguyen added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And while Nguyen has a point, users may not yet have a good reason to trust the technology of a couple of college-aged students purporting to send people out into the world with covert recording equipment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Meta’s glasses have an indicator light when their cameras and microphones are watching and listening as a mechanism to warn others that they are being recorded, Ardayfio said that the Halo glasses, dubbed Halo X, do not have an external indicator to warn people of their customers’ recording.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“For the hardware we’re making, we want it to be discreet, like normal glasses,” said Ardayfio, who added that the glasses record every word, transcribe it, and then delete the audio file.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Privacy advocates are warning about the normalization of covert recording devices in public.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Small and discreet recording devices are not new,” Eva Galperin, the director of cybersecurity at the Electronic Frontier Foundation, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In some ways, this sounds like a variation on the microphone spy pen,” said Galperin. “But I think that normalizing the use of an always-on recording device, which in many circumstances would require the user to get the consent of everyone within recording distance, eats away at the expectation of privacy we have for our conversations in all kinds of spaces.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are several states in the U.S. that make it illegal to covertly record conversations without the other persons’ consent. Ardayfio said they are aware of this but that it is up to their customer to obtain consent before using the glasses.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We trust our users to get consent if they are in a two-party consent state,” said Ardayfio, referring to the laws of a dozen U.S. states that require the consent of all recorded parties.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I would also be very concerned about where the recorded data is being kept, how it is being stored, and who has access to it,” Galperin added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ardayfio said Halo relies on Soniox for audio transcription, which claims to never store&amp;nbsp;recordings. Nguyen claimed when the finished product is released to customers, it will be end-to-end encrypted but provided no evidence of how this would work. He also noted that Halo is aiming to get SOC 2 compliance, which means it has been independently audited and demonstrates adequate protection of customer data. A date for the completed SOC 2 compliance was not provided.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, the two students are not new to privacy-invasive controversial projects.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While still at Harvard last year, Ardayfio and Nguyen developed I-XRAY, a demo project that added facial-recognition capabilities to the Meta Ray-Ban smart glasses, demonstrating how easily the tech could be bolted onto a device not meant to identify people.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo never released the code behind I-XRAY, but they did test the glasses on random passersby without consent. In a demo video, Ardayfio showed the glasses detecting faces and pulling up personal information of strangers within seconds. The video featured reactions of people who were doxed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview with 404 Media, they acknowledged the risks: “Some dude could just find some girl’s home address on the train and just follow them home,” Nguyen told the tech news website.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, Halo X glasses only have a display and a microphone, but no camera, although the two are exploring the possibility of adding it to a future model.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users still need to have their smartphones handy to help power the glasses and get “real time info prompts and answers to questions,” per Nguyen. The glasses, which are manufactured by another company that the startup didn’t name, are tethered to an accompanying app on the owner’s phone, where the glasses essentially outsource the computing since they don’t have enough power to do it on the device itself.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the hood, the smart glasses use Google’s Gemini and Perplexity as its chatbot engine, according to the two co-founders. Gemini is better for math and reasoning, whereas they use Perplexity to scrape the internet, they said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During an interview, TechCrunch asked if their glasses knew when the next season of “The Witcher” would come out. Responding in a way reminiscent of C-3PO, Ardayfio said: “‘The Witcher’ season four will be released on Netflix in 2025, but there’s no exact date yet. Most sources expect it in the second half of 2025.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I don’t know if that’s correct,” he added.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/halo-ai-smart-glasses-rendering.jpeg?resize=1200,1078" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Two former Harvard students are launching a pair of “always-on” AI-powered smart glasses that listen to, record, and transcribe every conversation and then display relevant information to the wearer in real time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our goal is to make glasses that make you super intelligent the moment you put them on,” said AnhPhu Nguyen, co-founder of Halo, a startup that’s developing the technology.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, as his co-founder Caine Ardayfio put it, the glasses “give you infinite memory.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The AI listens to every conversation you have and uses that knowledge to tell you what to say&amp;nbsp;… kinda like IRL Cluely,” Ardayfio told TechCrunch, referring to the startup that claims to help users “cheat” on everything from job interviews to school exams.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If somebody says a complex word or asks you a question, like, ‘What’s 37 to the third power?’ or something like that, then it’ll pop up on the glasses,” Ardayfio added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ardayfio and Nguyen have raised $1 million to develop the glasses, led by Pillar VC, with support from Soma Capital, Village Global, and Morningside Venture. The glasses will be priced at $249 and will be available for preorder starting Wednesday. Ardayfio called the glasses “the first real step towards vibe thinking.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The two Ivy League dropouts, who have since moved into their own version of the Hacker Hostel in the San Francisco Bay Area, recently caused a stir after developing a facial-recognition app for Meta’s smart Ray-Ban glasses to prove that the tech could be used to dox people. As a potential early competitor to Meta’s smart glasses, Ardayfio said Meta, given its history of security and privacy scandals, had to rein in its product in ways that Halo can ultimately capitalize on.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Meta doesn’t have a great reputation for caring about user privacy, and for them to release something that’s always there with you — which obviously brings a ton of utility — is just a huge reputational risk for them that they probably won’t take before a startup does it at scale first,” Nguyen added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And while Nguyen has a point, users may not yet have a good reason to trust the technology of a couple of college-aged students purporting to send people out into the world with covert recording equipment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Meta’s glasses have an indicator light when their cameras and microphones are watching and listening as a mechanism to warn others that they are being recorded, Ardayfio said that the Halo glasses, dubbed Halo X, do not have an external indicator to warn people of their customers’ recording.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“For the hardware we’re making, we want it to be discreet, like normal glasses,” said Ardayfio, who added that the glasses record every word, transcribe it, and then delete the audio file.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Privacy advocates are warning about the normalization of covert recording devices in public.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Small and discreet recording devices are not new,” Eva Galperin, the director of cybersecurity at the Electronic Frontier Foundation, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In some ways, this sounds like a variation on the microphone spy pen,” said Galperin. “But I think that normalizing the use of an always-on recording device, which in many circumstances would require the user to get the consent of everyone within recording distance, eats away at the expectation of privacy we have for our conversations in all kinds of spaces.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are several states in the U.S. that make it illegal to covertly record conversations without the other persons’ consent. Ardayfio said they are aware of this but that it is up to their customer to obtain consent before using the glasses.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We trust our users to get consent if they are in a two-party consent state,” said Ardayfio, referring to the laws of a dozen U.S. states that require the consent of all recorded parties.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I would also be very concerned about where the recorded data is being kept, how it is being stored, and who has access to it,” Galperin added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ardayfio said Halo relies on Soniox for audio transcription, which claims to never store&amp;nbsp;recordings. Nguyen claimed when the finished product is released to customers, it will be end-to-end encrypted but provided no evidence of how this would work. He also noted that Halo is aiming to get SOC 2 compliance, which means it has been independently audited and demonstrates adequate protection of customer data. A date for the completed SOC 2 compliance was not provided.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, the two students are not new to privacy-invasive controversial projects.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While still at Harvard last year, Ardayfio and Nguyen developed I-XRAY, a demo project that added facial-recognition capabilities to the Meta Ray-Ban smart glasses, demonstrating how easily the tech could be bolted onto a device not meant to identify people.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo never released the code behind I-XRAY, but they did test the glasses on random passersby without consent. In a demo video, Ardayfio showed the glasses detecting faces and pulling up personal information of strangers within seconds. The video featured reactions of people who were doxed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview with 404 Media, they acknowledged the risks: “Some dude could just find some girl’s home address on the train and just follow them home,” Nguyen told the tech news website.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, Halo X glasses only have a display and a microphone, but no camera, although the two are exploring the possibility of adding it to a future model.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users still need to have their smartphones handy to help power the glasses and get “real time info prompts and answers to questions,” per Nguyen. The glasses, which are manufactured by another company that the startup didn’t name, are tethered to an accompanying app on the owner’s phone, where the glasses essentially outsource the computing since they don’t have enough power to do it on the device itself.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the hood, the smart glasses use Google’s Gemini and Perplexity as its chatbot engine, according to the two co-founders. Gemini is better for math and reasoning, whereas they use Perplexity to scrape the internet, they said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During an interview, TechCrunch asked if their glasses knew when the next season of “The Witcher” would come out. Responding in a way reminiscent of C-3PO, Ardayfio said: “‘The Witcher’ season four will be released on Netflix in 2025, but there’s no exact date yet. Most sources expect it in the second half of 2025.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I don’t know if that’s correct,” he added.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/harvard-dropouts-to-launch-always-on-ai-smart-glasses-that-listen-and-record-every-conversation/</guid><pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] Google brings improved Gemini features to its new Pixel Buds (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/google-brings-improved-gemini-features-to-its-new-pixel-buds/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Similar to last year, the Made By Google event held on Wednesday showcased numerous Gemini features, including AI photo-taking and editing tools for the Pixel 10 series, along with enhancements for the Pixel Watch 4. So it was expected that Google would unveil Gemini functionalities for its latest earbuds—the Pixel Buds 2a and a revamped Pixel Buds Pro 2.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some key features showcased at the event include the introduction of active noise cancellation and an AI feature that minimizes background noise for the Pixel Buds 2a. Additionally, the Pro 2 earbuds will receive a major update that allows users to accept or decline calls and messages by shaking or nodding their heads.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036714" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/PixelBuds2a_Iris_BothBudsinCase.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-pixel-buds-2a"&gt;Pixel Buds 2a&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds 2a represent the second generation of the A-Series earbuds, which are the tech giant’s more affordable option in the earbud lineup.&amp;nbsp;They come with significant upgrades, making them almost up to par with the original Pixel Buds Pro.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For the first time, the A-Series earbuds now include active noise cancellation, a significant upgrade from the previous version, which offered zero noise reduction. Now the Pixel Buds 2a can block disruptive external noises, allowing for clear conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another feature coming to the earbuds is Clear Calling, which uses Google AI to reduce background noise and works in conjunction with the earbuds’ wind-blocking mesh covers to eliminate distractions, ensuring that both the user and the person on the other end can hear each other well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the Pixel Buds 2a is getting Transparency Mode, a feature that was previously available only on the Pixel Buds Pro. This mode lets users hear their surroundings while still listening to audio, helping them stay aware of what’s going on around them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds 2a also comes with Gemini hands-free support, allowing you to communicate with the AI assistant to help with tasks like finding nearby restaurants, getting directions, reading new emails, pausing songs, and more. You can go live with Gemini just by speaking, without needing to take out your phone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The battery life is also getting a major boost, exceeding that of the original A-Series by twofold.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In terms of design, these are the smallest and lightest A-Series earbuds to date, accompanied by a compact charging case. They are also IP54-rated, making them sweat- and water-resistant, ideal for workouts or unexpected rain.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new design feature that brings the budget-friendly earbuds closer in line with the Pixel Buds Pro is the twist stabilizer. This allows users to adjust the fit by twisting counterclockwise for a snug fit or clockwise for a more comfortable, looser fit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflecting the new features, the Pixel Buds 2a come with a higher price tag. They retail for $130, compared to their predecessor under $100, and are available in two new colors: Hazel and Iris.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036712" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/PixelBudsPro2_Moonstone_Front-Facing-Buds.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-pixel-buds-pro-2"&gt;Pixel Buds Pro 2 &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds Pro 2, first released last year, are also receiving significant upgrades.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As expected, these earbuds feature Gemini hands-free support, but they come with additional perks as well. Now you can accept or decline calls and texts by simply shaking or nodding your head. The earbuds will detect these gestures using built-in accelerometers and sensors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, when interacting with Gemini, you can now connect with other apps, such as adding items to a grocery list in Keep or scheduling events in Calendar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other notable features include Adaptive Audio, which automatically adjusts to the noise level around you to reduce distracting sounds in noisy settings while still helping you stay aware of your environment. Additionally, Loud Noise Protection automatically dampens abrupt loud sounds, protecting your hearing from loud noises, such as the siren of a nearby fire truck.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds Pro 2 retail for $230 and come in a new Moonstone color. However, it’s important to note that most of these new features won’t be available; they’ll be introduced via a software update later in September. All existing Pro 2 users will get the new features. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers can preorder the Pixel Buds 2a and improved Pro 2 earbuds today. The Pro 2 will ship on August 28, whereas the Pixel Buds 2a won’t hit the shelves until October 9.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Similar to last year, the Made By Google event held on Wednesday showcased numerous Gemini features, including AI photo-taking and editing tools for the Pixel 10 series, along with enhancements for the Pixel Watch 4. So it was expected that Google would unveil Gemini functionalities for its latest earbuds—the Pixel Buds 2a and a revamped Pixel Buds Pro 2.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some key features showcased at the event include the introduction of active noise cancellation and an AI feature that minimizes background noise for the Pixel Buds 2a. Additionally, the Pro 2 earbuds will receive a major update that allows users to accept or decline calls and messages by shaking or nodding their heads.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036714" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/PixelBuds2a_Iris_BothBudsinCase.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-pixel-buds-2a"&gt;Pixel Buds 2a&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds 2a represent the second generation of the A-Series earbuds, which are the tech giant’s more affordable option in the earbud lineup.&amp;nbsp;They come with significant upgrades, making them almost up to par with the original Pixel Buds Pro.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For the first time, the A-Series earbuds now include active noise cancellation, a significant upgrade from the previous version, which offered zero noise reduction. Now the Pixel Buds 2a can block disruptive external noises, allowing for clear conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another feature coming to the earbuds is Clear Calling, which uses Google AI to reduce background noise and works in conjunction with the earbuds’ wind-blocking mesh covers to eliminate distractions, ensuring that both the user and the person on the other end can hear each other well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the Pixel Buds 2a is getting Transparency Mode, a feature that was previously available only on the Pixel Buds Pro. This mode lets users hear their surroundings while still listening to audio, helping them stay aware of what’s going on around them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds 2a also comes with Gemini hands-free support, allowing you to communicate with the AI assistant to help with tasks like finding nearby restaurants, getting directions, reading new emails, pausing songs, and more. You can go live with Gemini just by speaking, without needing to take out your phone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The battery life is also getting a major boost, exceeding that of the original A-Series by twofold.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In terms of design, these are the smallest and lightest A-Series earbuds to date, accompanied by a compact charging case. They are also IP54-rated, making them sweat- and water-resistant, ideal for workouts or unexpected rain.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new design feature that brings the budget-friendly earbuds closer in line with the Pixel Buds Pro is the twist stabilizer. This allows users to adjust the fit by twisting counterclockwise for a snug fit or clockwise for a more comfortable, looser fit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflecting the new features, the Pixel Buds 2a come with a higher price tag. They retail for $130, compared to their predecessor under $100, and are available in two new colors: Hazel and Iris.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036712" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/PixelBudsPro2_Moonstone_Front-Facing-Buds.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-pixel-buds-pro-2"&gt;Pixel Buds Pro 2 &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds Pro 2, first released last year, are also receiving significant upgrades.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As expected, these earbuds feature Gemini hands-free support, but they come with additional perks as well. Now you can accept or decline calls and texts by simply shaking or nodding your head. The earbuds will detect these gestures using built-in accelerometers and sensors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, when interacting with Gemini, you can now connect with other apps, such as adding items to a grocery list in Keep or scheduling events in Calendar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other notable features include Adaptive Audio, which automatically adjusts to the noise level around you to reduce distracting sounds in noisy settings while still helping you stay aware of your environment. Additionally, Loud Noise Protection automatically dampens abrupt loud sounds, protecting your hearing from loud noises, such as the siren of a nearby fire truck.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds Pro 2 retail for $230 and come in a new Moonstone color. However, it’s important to note that most of these new features won’t be available; they’ll be introduced via a software update later in September. All existing Pro 2 users will get the new features. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers can preorder the Pixel Buds 2a and improved Pro 2 earbuds today. The Pro 2 will ship on August 28, whereas the Pixel Buds 2a won’t hit the shelves until October 9.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/google-brings-improved-gemini-features-to-its-new-pixel-buds/</guid><pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] You can now talk to Google Photos to make your edits (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/you-can-now-talk-to-google-photos-to-make-your-edits/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At Wednesday’s Made by Google event, the company announced new features in Google Photos that will allow users to ask the app to edit their pictures for them. The functionality will launch first on Pixel 10 devices in the U.S., allowing people to describe whatever edits they want to make to the photo by either voice or text.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding support for C2PA Content Credentials in Google Photos. The Pixel 10 phones will be the first from Google to adopt this standard, which is designed to improve transparency around how images are made and whether AI is involved. On Pixel devices, C2PA is supported with the Camera app itself and in any photos taken with it, even if AI is not used.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038274" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The new “edit by asking” feature in Google Photos leverages Gemini so you can ask for changes to a photo using natural language. For instance, you can say things like “remove the cars in the background,” or something less specific, like “restore this old photo,” and Google Photos will take action. The addition could help those who aren’t as tech-savvy or have a good understanding of editing tools to still make adjustments to improve their photos.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038275" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature can handle tasks like lighting adjustments and removing distractions from the images, as well as more creative edits, like changing the background or adding items to the photo. Google suggests you could use this to add sunglasses and a party hat to the photo’s subject, among other things, for example.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even if you don’t know what to ask for, you can start with a request for help like “make it better,” and Google Photos will automatically make changes to the image. The app can also offer suggestions of what to fix, and it supports follow-up requests as you continue to fine-tune your edits. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038277" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed-1.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the support for C2PA will come first to Pixel 10 devices and then will roll out gradually to Google Photos across iOS and Android in the weeks ahead. “Edit by asking” launches Wednesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At Wednesday’s Made by Google event, the company announced new features in Google Photos that will allow users to ask the app to edit their pictures for them. The functionality will launch first on Pixel 10 devices in the U.S., allowing people to describe whatever edits they want to make to the photo by either voice or text.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding support for C2PA Content Credentials in Google Photos. The Pixel 10 phones will be the first from Google to adopt this standard, which is designed to improve transparency around how images are made and whether AI is involved. On Pixel devices, C2PA is supported with the Camera app itself and in any photos taken with it, even if AI is not used.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038274" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The new “edit by asking” feature in Google Photos leverages Gemini so you can ask for changes to a photo using natural language. For instance, you can say things like “remove the cars in the background,” or something less specific, like “restore this old photo,” and Google Photos will take action. The addition could help those who aren’t as tech-savvy or have a good understanding of editing tools to still make adjustments to improve their photos.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038275" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature can handle tasks like lighting adjustments and removing distractions from the images, as well as more creative edits, like changing the background or adding items to the photo. Google suggests you could use this to add sunglasses and a party hat to the photo’s subject, among other things, for example.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even if you don’t know what to ask for, you can start with a request for help like “make it better,” and Google Photos will automatically make changes to the image. The app can also offer suggestions of what to fix, and it supports follow-up requests as you continue to fine-tune your edits. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038277" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed-1.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the support for C2PA will come first to Pixel 10 devices and then will roll out gradually to Google Photos across iOS and Android in the weeks ahead. “Edit by asking” launches Wednesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/you-can-now-talk-to-google-photos-to-make-your-edits/</guid><pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] Google doubles down on ‘AI phones’ with its Pixel 10 series (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/google-doubles-down-on-ai-phones-with-its-pixel-10-series/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With the launch of the new Pixel 10 series, Google is rushing ahead of Apple to deliver AI-powered smartphones to consumers. The devices, announced during Wednesday’s “Made by Google” livestream, come just weeks ahead of Apple’s expected iPhone 17 reveal, which promises to be more of the same — better cameras, possibly thinner devices, and new colors to choose from.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google, meanwhile, has been rapidly integrating its AI platform into its devices. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, its Pixel 9 series added a number of AI features, like Gemini Live (Gemini’s voice mode), image generation tools, call notes, searchable screenshots, and more. Since then, Google says that Gemini Live conversations have proven to be five times longer than text-based conversations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3038318" height="1055" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live.jpg" width="1773" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;This year, the tech giant is rolling out even more AI-powered upgrades with the launch of its Pixel 10, including a Visual Overlays feature for the camera, a proactive “Magic Cue” feature, Camera Coach, Voice Translate for calls, an assistant-like “Take a Message” feature, Pixel Journal, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Combined, the updates allow Google to showcase what its latest AI technology can do when enhanced by its Tensor G5 processor, an upgrade to the company’s custom silicon designed for AI experiences and the first to run its newest Gemini Nano model. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the launch, Google announced that Gemini Live will gain a new audio model that will detect your tone — like whether you’re excited or concerned — and adjust its response accordingly.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038388" height="379" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the addition of a feature called Visual Overlays, Gemini Live will be able to see what you see through the lens of your camera and provide guidance by highlighting things on your screen. For example, while traveling in a foreign country, you could hold up your phone to see if the street signs around offer information about parking along the roadside.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038389" height="382" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature, Magic Cue, lets the AI be more proactive by offering contextual suggestions in real time, across apps like Gmail, Calendar, Messages, Screenshots, and others. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea of a more proactive interaction between people and Google technology is something the company has dreamed of for years, long before the AI era. In the early 2010s, for example, Google introduced an Android feature called Google Now that would pop up cards with real-time information related to your daily schedule or the time of day, like nearby restaurants at lunchtime, upcoming meetings, or flight details.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Years later, Magic Cue is the AI-powered reintroduction of this feature, but one where it inserts itself into your everyday apps and interactions.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038365" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/magic-cue-airline.jpg?w=332" width="332" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google demonstrated how Magic Cue could suggest a restaurant to dine at with a friend, offering quick access to place a call to the restaurant to make a reservation. It could propose a reply to your friend with the reservation details or point you to your calendar to check your availability. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Magic Cue’s suggestions appear within the app you’re using and are wrapped with a rainbow colored outline to differentiate them, as well as within Daily Hub, a personalized daily digest in your Discover feed. You can also tap on its suggestions to take action.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3038303" height="340" src="https://techcrunch.com/wp-content/uploads/2025/08/daily-hub.png?w=340" width="340" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Also similar to Google Now, Magic Cue will be able to surface reminders. But it goes a step further by popping up reminders and notifications more intuitively. For instance, it may remind you of errands you need to handle, like a return of an online order, suggest topics you may want to research, or recommend new playlists to stream. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, Magic Cue’s suggestions will be limited to select activities, like settling up a tab, adding events to your calendar, and showing the forecast for an upcoming trip in the weather app. Over time, Google will add other options and let you configure which data source the feature has access to. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple, it should be noted, is trying to do something similar by allowing users to speak to Siri to interact with and take action within their apps, but unfortunately, its AI-powered Siri has been delayed until 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another one of the more interesting additions in the Pixel 10 series is Camera Coach, an AI-powered assistant that aims to make you a better photographer. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038368" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/camera-coach.jpg?w=322" width="322" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will be launching in preview with the new devices and uses Gemini models to offer suggestions about how to better frame and compose your shot. You can even choose a “get inspired” option that will suggest scenes you may not have considered.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038392" height="628" src="https://techcrunch.com/wp-content/uploads/2025/08/camera-coach-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the camera will now be able to recognize when you’re taking a group photo, the “Auto Best Take” feature activates and analyzes up to 150 images shot over several seconds to find the best one — whether that’s a shot you snapped yourself or one made by blending others together via AI. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038399" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Auto-best-take.jpg?w=374" width="374" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Then, with the AI-powered “Ask Photos” tool, you can edit the shot to do other things, like fix the lighting, change the framing, or remove an object from the photo by either speaking to or texting Photos’ AI assistant.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Pro devices, the Pro Res Zoom option will also use AI to allow you to “zoom” in on things like architecture and landscapes at 30-60x, or 30-60x for animals and wildlife.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038397" height="511" src="https://techcrunch.com/wp-content/uploads/2025/08/pro-res.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Screenshot&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Related to its enhanced use of AI in photography, Pixel 10 phones will also be the first to implement C2PA, a standard that establishes the origin and edits of digital content, which will help to identify when photos have been modified by AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new AI feature, Voice Translate, will use on-device AI to translate your phone call in real-time in what sounds like each speaker’s own voice. This could be a potential game-changer, particularly for business users and world travelers, if it works as well as described. (This still needs to be tested by reviewers, of course.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038409" height="373" src="https://techcrunch.com/wp-content/uploads/2025/08/auto-translate.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will translate to or from English and Spanish, German, Japanese, French, Hindi, Italian, Portuguese, Swedish, Russian, and Indonesian. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Pixel’s Phone app, a new addition called Take a Message provides real-time transcripts for missed and declined calls and then uses AI to identify the next steps you need to take based on the caller’s voicemail. (That update will come to Call Notes, too.) &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038408" height="459" src="https://techcrunch.com/wp-content/uploads/2025/08/take-a-message.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Pixel Journal, meanwhile, is Google’s answer to Apple’s Journal app, but one that uses AI to prompt you to share your thoughts, track your progress towards goals, and offer insights over time.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038258" height="382" src="https://techcrunch.com/wp-content/uploads/2025/08/Pixel-Journal-feat.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Other minor AI upgrades to the Pixel 10 lineup include writing tools integrated into the Gboard keyboard, updates to Pixel screenshots in Pixel Studio, and Notebook LM integrations with Recorder and screenshots.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With the launch of the new Pixel 10 series, Google is rushing ahead of Apple to deliver AI-powered smartphones to consumers. The devices, announced during Wednesday’s “Made by Google” livestream, come just weeks ahead of Apple’s expected iPhone 17 reveal, which promises to be more of the same — better cameras, possibly thinner devices, and new colors to choose from.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google, meanwhile, has been rapidly integrating its AI platform into its devices. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, its Pixel 9 series added a number of AI features, like Gemini Live (Gemini’s voice mode), image generation tools, call notes, searchable screenshots, and more. Since then, Google says that Gemini Live conversations have proven to be five times longer than text-based conversations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3038318" height="1055" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live.jpg" width="1773" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;This year, the tech giant is rolling out even more AI-powered upgrades with the launch of its Pixel 10, including a Visual Overlays feature for the camera, a proactive “Magic Cue” feature, Camera Coach, Voice Translate for calls, an assistant-like “Take a Message” feature, Pixel Journal, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Combined, the updates allow Google to showcase what its latest AI technology can do when enhanced by its Tensor G5 processor, an upgrade to the company’s custom silicon designed for AI experiences and the first to run its newest Gemini Nano model. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the launch, Google announced that Gemini Live will gain a new audio model that will detect your tone — like whether you’re excited or concerned — and adjust its response accordingly.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038388" height="379" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the addition of a feature called Visual Overlays, Gemini Live will be able to see what you see through the lens of your camera and provide guidance by highlighting things on your screen. For example, while traveling in a foreign country, you could hold up your phone to see if the street signs around offer information about parking along the roadside.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038389" height="382" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature, Magic Cue, lets the AI be more proactive by offering contextual suggestions in real time, across apps like Gmail, Calendar, Messages, Screenshots, and others. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea of a more proactive interaction between people and Google technology is something the company has dreamed of for years, long before the AI era. In the early 2010s, for example, Google introduced an Android feature called Google Now that would pop up cards with real-time information related to your daily schedule or the time of day, like nearby restaurants at lunchtime, upcoming meetings, or flight details.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Years later, Magic Cue is the AI-powered reintroduction of this feature, but one where it inserts itself into your everyday apps and interactions.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038365" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/magic-cue-airline.jpg?w=332" width="332" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google demonstrated how Magic Cue could suggest a restaurant to dine at with a friend, offering quick access to place a call to the restaurant to make a reservation. It could propose a reply to your friend with the reservation details or point you to your calendar to check your availability. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Magic Cue’s suggestions appear within the app you’re using and are wrapped with a rainbow colored outline to differentiate them, as well as within Daily Hub, a personalized daily digest in your Discover feed. You can also tap on its suggestions to take action.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3038303" height="340" src="https://techcrunch.com/wp-content/uploads/2025/08/daily-hub.png?w=340" width="340" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Also similar to Google Now, Magic Cue will be able to surface reminders. But it goes a step further by popping up reminders and notifications more intuitively. For instance, it may remind you of errands you need to handle, like a return of an online order, suggest topics you may want to research, or recommend new playlists to stream. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, Magic Cue’s suggestions will be limited to select activities, like settling up a tab, adding events to your calendar, and showing the forecast for an upcoming trip in the weather app. Over time, Google will add other options and let you configure which data source the feature has access to. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple, it should be noted, is trying to do something similar by allowing users to speak to Siri to interact with and take action within their apps, but unfortunately, its AI-powered Siri has been delayed until 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another one of the more interesting additions in the Pixel 10 series is Camera Coach, an AI-powered assistant that aims to make you a better photographer. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038368" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/camera-coach.jpg?w=322" width="322" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will be launching in preview with the new devices and uses Gemini models to offer suggestions about how to better frame and compose your shot. You can even choose a “get inspired” option that will suggest scenes you may not have considered.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038392" height="628" src="https://techcrunch.com/wp-content/uploads/2025/08/camera-coach-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the camera will now be able to recognize when you’re taking a group photo, the “Auto Best Take” feature activates and analyzes up to 150 images shot over several seconds to find the best one — whether that’s a shot you snapped yourself or one made by blending others together via AI. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038399" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Auto-best-take.jpg?w=374" width="374" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Then, with the AI-powered “Ask Photos” tool, you can edit the shot to do other things, like fix the lighting, change the framing, or remove an object from the photo by either speaking to or texting Photos’ AI assistant.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Pro devices, the Pro Res Zoom option will also use AI to allow you to “zoom” in on things like architecture and landscapes at 30-60x, or 30-60x for animals and wildlife.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038397" height="511" src="https://techcrunch.com/wp-content/uploads/2025/08/pro-res.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Screenshot&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Related to its enhanced use of AI in photography, Pixel 10 phones will also be the first to implement C2PA, a standard that establishes the origin and edits of digital content, which will help to identify when photos have been modified by AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new AI feature, Voice Translate, will use on-device AI to translate your phone call in real-time in what sounds like each speaker’s own voice. This could be a potential game-changer, particularly for business users and world travelers, if it works as well as described. (This still needs to be tested by reviewers, of course.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038409" height="373" src="https://techcrunch.com/wp-content/uploads/2025/08/auto-translate.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will translate to or from English and Spanish, German, Japanese, French, Hindi, Italian, Portuguese, Swedish, Russian, and Indonesian. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Pixel’s Phone app, a new addition called Take a Message provides real-time transcripts for missed and declined calls and then uses AI to identify the next steps you need to take based on the caller’s voicemail. (That update will come to Call Notes, too.) &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038408" height="459" src="https://techcrunch.com/wp-content/uploads/2025/08/take-a-message.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Pixel Journal, meanwhile, is Google’s answer to Apple’s Journal app, but one that uses AI to prompt you to share your thoughts, track your progress towards goals, and offer insights over time.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038258" height="382" src="https://techcrunch.com/wp-content/uploads/2025/08/Pixel-Journal-feat.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Other minor AI upgrades to the Pixel 10 lineup include writing tools integrated into the Gboard keyboard, updates to Pixel screenshots in Pixel Studio, and Notebook LM integrations with Recorder and screenshots.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/google-doubles-down-on-ai-phones-with-its-pixel-10-series/</guid><pubDate>Wed, 20 Aug 2025 16:01:00 +0000</pubDate></item><item><title>[NEW] Forging connections in space with cellular technology (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/20/1121888/forging-connections-in-space-with-cellular-technology/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/nokiamittrimage-1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Nokia&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/nokiamittrimage-1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Nokia&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/20/1121888/forging-connections-in-space-with-cellular-technology/</guid><pubDate>Wed, 20 Aug 2025 16:02:16 +0000</pubDate></item><item><title>[NEW] Anthropic bundles Claude Code into enterprise plans (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/anthropic-bundles-claude-code-into-enterprise-plans/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/AI-Sessions-Anthropic-Kaplan.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic on Wednesday announced a new subscription offering that will incorporate Claude Code into Claude for Enterprise. Previously available only through individual accounts, Anthropic’s command-line coding tool can now be purchased as part of a broader enterprise suite, allowing for more sophisticated integrations and more powerful admin tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is the most requested feature from our business team and enterprise customers,” Anthropic product lead Scott White told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The integration positions Anthropic to better compete with command-line tools from Google and GitHub, both of which included enterprise integrations on launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in June, Claude Code has quickly become one of the most popular command-line programming tools, offering a more agentic approach than traditional IDE-based tools. That popularity has come with some growing pains, as individual users of the service have struggled with unexpected usage limits. The new enterprise offering is partially a response to these issues, allowing businesses to set granular spending controls that can be scaled up for intense usage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic is particularly bullish about integrations between Claude Code and the Claude.ai chatbot, which will be easier to automate in an enterprise context. Using Anthropic’s Model Context Protocol, businesses can develop Claude Code prompts in conjunction with the Claude chatbot, or integrate the command-line tool more deeply into internal data sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his work on Claude.ai, White said enterprise integrations involving customer feedback tools were particularly transformative, using Claude to summarize large quantities of feedback from different sources and translate everything into concrete product changes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s something magical about blending customer feedback, getting the voice of your customer and then helping to think about solutions that you might be able to prototype and build that address their unique challenges,” White said. “It’s something that as a product manager was simply not possible for me even a year ago.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/AI-Sessions-Anthropic-Kaplan.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic on Wednesday announced a new subscription offering that will incorporate Claude Code into Claude for Enterprise. Previously available only through individual accounts, Anthropic’s command-line coding tool can now be purchased as part of a broader enterprise suite, allowing for more sophisticated integrations and more powerful admin tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is the most requested feature from our business team and enterprise customers,” Anthropic product lead Scott White told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The integration positions Anthropic to better compete with command-line tools from Google and GitHub, both of which included enterprise integrations on launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in June, Claude Code has quickly become one of the most popular command-line programming tools, offering a more agentic approach than traditional IDE-based tools. That popularity has come with some growing pains, as individual users of the service have struggled with unexpected usage limits. The new enterprise offering is partially a response to these issues, allowing businesses to set granular spending controls that can be scaled up for intense usage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic is particularly bullish about integrations between Claude Code and the Claude.ai chatbot, which will be easier to automate in an enterprise context. Using Anthropic’s Model Context Protocol, businesses can develop Claude Code prompts in conjunction with the Claude chatbot, or integrate the command-line tool more deeply into internal data sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his work on Claude.ai, White said enterprise integrations involving customer feedback tools were particularly transformative, using Claude to summarize large quantities of feedback from different sources and translate everything into concrete product changes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s something magical about blending customer feedback, getting the voice of your customer and then helping to think about solutions that you might be able to prototype and build that address their unique challenges,” White said. “It’s something that as a product manager was simply not possible for me even a year ago.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/anthropic-bundles-claude-code-into-enterprise-plans/</guid><pubDate>Wed, 20 Aug 2025 18:00:00 +0000</pubDate></item></channel></rss>