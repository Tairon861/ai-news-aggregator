<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 09 Jun 2025 13:44:27 +0000</lastBuildDate><item><title>[NEW] Why Meta‚Äôs Biggest AI Bet Isn‚Äôt on Models‚ÄîIt‚Äôs on Data (Unite.AI)</title><link>https://www.unite.ai/why-metas-biggest-ai-bet-isnt-on-models-its-on-data/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/20250609_0010_Futuristic-Boardroom-Dynamics_simple_compose_01jx9ajrghe919c0n98ppr8rnw-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;Meta's reported $10 billion investment in Scale AI represents far more than a simple funding round‚Äîit signals a fundamental strategic evolution in how tech giants view the AI arms race. This potential deal, which could exceed $10 billion and would be Meta's largest external AI investment, reveals Mark Zuckerberg's company doubling down on a critical insight: in the post-ChatGPT era, victory belongs not to those with the most sophisticated algorithms, but to those who control the highest-quality data pipelines.&lt;/p&gt;&lt;h3&gt;By the Numbers:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;$10 billion&lt;/strong&gt;: Meta's potential investment in Scale AI&lt;/li&gt;&lt;li&gt;&lt;strong&gt;$870M ‚Üí $2B&lt;/strong&gt;: Scale AI's revenue growth (2024 to 2025)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;$7B ‚Üí $13.8B&lt;/strong&gt;: Scale AI's valuation trajectory in recent funding rounds&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;The Data Infrastructure Imperative&lt;/h2&gt;&lt;p&gt;After Llama 4's lukewarm reception, Meta might be looking to secure exclusive datasets that could give it an edge over rivals like OpenAI and Microsoft. This timing is no coincidence. While Meta's latest models showed promise in technical benchmarks, early user feedback and implementation challenges highlighted a stark reality: architectural innovations alone are insufficient in today's AI world.&lt;/p&gt;&lt;p&gt;‚ÄúAs an AI community we've exhausted all of the easy data, the internet data, and now we need to move on to more complex data,‚Äù Scale AI CEO Alexandr Wang told the Financial Times back in 2024. ‚ÄúThe quantity matters but the quality is paramount.‚Äù This observation captures precisely why Meta is willing to make such a substantial investment in Scale AI's infrastructure.&lt;/p&gt;&lt;p&gt;Scale AI has positioned itself as the ‚Äúdata foundry‚Äù of the AI revolution, providing data-labeling services to companies that want to train machine learning models through a sophisticated hybrid approach combining automation with human expertise. Scale's secret weapon is its hybrid model: it uses automation to pre-process and filter tasks but relies on a trained, distributed workforce for human judgment in AI training where it matters most.&lt;/p&gt;&lt;h2&gt;Strategic Differentiation Through Data Control&lt;/h2&gt;&lt;p&gt;Meta's investment thesis rests on a sophisticated understanding of competitive dynamics that extend beyond traditional model development. While competitors like Microsoft pour billions into model creators like OpenAI, Meta is betting on controlling the underlying data infrastructure that feeds all AI systems.&lt;/p&gt;&lt;p&gt;This approach offers several compelling benefits:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Proprietary dataset access&lt;/strong&gt; ‚Äî Enhanced model training capabilities while potentially limiting competitor access to the same high-quality data&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pipeline control&lt;/strong&gt; ‚Äî Reduced dependencies on external providers and more predictable cost structures&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Infrastructure focus&lt;/strong&gt; ‚Äî Investment in foundational layers rather than competing solely on model architecture&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Scale AI partnership positions Meta to capitalize on the growing complexity of AI training data requirements. Recent developments suggest that advances in large AI models may depend less on architectural innovations and more on access to high-quality training data and compute. This insight drives Meta's willingness to invest heavily in data infrastructure rather than competing solely on model architecture.&lt;/p&gt;&lt;h2&gt;The Military and Government Dimension&lt;/h2&gt;&lt;p&gt;The investment carries significant implications beyond commercial AI applications. Both Meta and Scale AI are deepening ties with the US government. The two companies are working on Defense Llama, a military-adapted version of Meta's Llama model. Scale AI recently landed a contract with the US Department of Defense to develop AI agents for operational use.&lt;/p&gt;&lt;p&gt;This government partnership dimension adds strategic value that extends far beyond immediate financial returns. Military and government contracts provide stable, long-term revenue streams while positioning both companies as critical infrastructure providers for national AI capabilities. The Defense Llama project exemplifies how commercial AI development increasingly intersects with national security considerations.&lt;/p&gt;&lt;h2&gt;Challenging the Microsoft-OpenAI Paradigm&lt;/h2&gt;&lt;p&gt;Meta's Scale AI investment would be a direct challenge to the dominant Microsoft-OpenAI partnership model that has defined the current AI space. Microsoft remains a major investor in OpenAI, providing funding and capacity to support their advancements, but this relationship focuses primarily on model development and deployment rather than fundamental data infrastructure.&lt;/p&gt;&lt;p&gt;By contrast, Meta's approach prioritizes controlling the foundational layer that enables all AI development. This strategy could prove more durable than exclusive model partnerships, which face increasing competitive pressure and potential partnership instability. Recent reports suggest Microsoft is developing its own in-house reasoning models to compete with OpenAI and has been testing models from Elon Musk's xAI, Meta, and DeepSeek to replace ChatGPT in Copilot, highlighting the inherent tensions in Big Tech's AI investment strategies.&lt;/p&gt;&lt;h2&gt;The Economics of AI Infrastructure&lt;/h2&gt;&lt;p&gt;Scale AI saw $870 million in revenue last year and expects to bring in $2 billion this year, demonstrating the substantial market demand for professional AI data services. The company's valuation trajectory‚Äîfrom around $7 billion to $13.8 billion in recent funding rounds‚Äîreflects investor recognition that data infrastructure represents a durable competitive moat.&lt;/p&gt;&lt;p&gt;Meta's $10 billion investment would provide Scale AI with unprecedented resources to expand its operations globally and develop more sophisticated data processing capabilities. This scale advantage could create network effects that make it increasingly difficult for competitors to match Scale AI's quality and cost efficiency, particularly as AI infrastructure investments continue to escalate across the industry.&lt;/p&gt;&lt;p&gt;This investment signals a broader industry evolution toward vertical integration of AI infrastructure. Rather than relying on partnerships with specialized AI companies, tech giants are increasingly acquiring or investing heavily in the underlying infrastructure that enables AI development.&lt;/p&gt;&lt;p&gt;The move also highlights growing recognition that data quality and model alignment services will become even more critical as AI systems become more powerful and are deployed in more sensitive applications. Scale AI's expertise in reinforcement learning from human feedback (RLHF) and model evaluation provides Meta with capabilities essential for developing safe, reliable AI systems.&lt;/p&gt;&lt;h2&gt;Looking Forward: The Data Wars Begin&lt;/h2&gt;&lt;p&gt;Meta's Scale AI investment represents the opening salvo in what may become the ‚Äúdata wars‚Äù‚Äîa competition for control over the high-quality, specialized datasets that will determine AI leadership in the coming decade.&lt;/p&gt;&lt;p&gt;This strategic pivot acknowledges that while the current AI boom began with breakthrough models like ChatGPT, sustained competitive advantage will come from controlling the infrastructure that enables continuous model improvement. As the industry matures beyond the initial excitement of generative AI, companies that control data pipelines may find themselves with more durable advantages than those who merely license or partner for model access.&lt;/p&gt;&lt;p&gt;For Meta, the Scale AI investment is a calculated bet that the future of AI competition will be won in the data preprocessing centers and annotation workflows that most consumers never see‚Äîbut which ultimately determine which AI systems succeed in the real world. If this thesis proves correct, Meta's $10 billion investment may be remembered as the moment the company secured its position in the next phase of the AI revolution.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.unite.ai/wp-content/uploads/2025/06/20250609_0010_Futuristic-Boardroom-Dynamics_simple_compose_01jx9ajrghe919c0n98ppr8rnw-942x600.png" /&gt;&lt;/div&gt;&lt;p&gt;Meta's reported $10 billion investment in Scale AI represents far more than a simple funding round‚Äîit signals a fundamental strategic evolution in how tech giants view the AI arms race. This potential deal, which could exceed $10 billion and would be Meta's largest external AI investment, reveals Mark Zuckerberg's company doubling down on a critical insight: in the post-ChatGPT era, victory belongs not to those with the most sophisticated algorithms, but to those who control the highest-quality data pipelines.&lt;/p&gt;&lt;h3&gt;By the Numbers:&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;$10 billion&lt;/strong&gt;: Meta's potential investment in Scale AI&lt;/li&gt;&lt;li&gt;&lt;strong&gt;$870M ‚Üí $2B&lt;/strong&gt;: Scale AI's revenue growth (2024 to 2025)&lt;/li&gt;&lt;li&gt;&lt;strong&gt;$7B ‚Üí $13.8B&lt;/strong&gt;: Scale AI's valuation trajectory in recent funding rounds&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;The Data Infrastructure Imperative&lt;/h2&gt;&lt;p&gt;After Llama 4's lukewarm reception, Meta might be looking to secure exclusive datasets that could give it an edge over rivals like OpenAI and Microsoft. This timing is no coincidence. While Meta's latest models showed promise in technical benchmarks, early user feedback and implementation challenges highlighted a stark reality: architectural innovations alone are insufficient in today's AI world.&lt;/p&gt;&lt;p&gt;‚ÄúAs an AI community we've exhausted all of the easy data, the internet data, and now we need to move on to more complex data,‚Äù Scale AI CEO Alexandr Wang told the Financial Times back in 2024. ‚ÄúThe quantity matters but the quality is paramount.‚Äù This observation captures precisely why Meta is willing to make such a substantial investment in Scale AI's infrastructure.&lt;/p&gt;&lt;p&gt;Scale AI has positioned itself as the ‚Äúdata foundry‚Äù of the AI revolution, providing data-labeling services to companies that want to train machine learning models through a sophisticated hybrid approach combining automation with human expertise. Scale's secret weapon is its hybrid model: it uses automation to pre-process and filter tasks but relies on a trained, distributed workforce for human judgment in AI training where it matters most.&lt;/p&gt;&lt;h2&gt;Strategic Differentiation Through Data Control&lt;/h2&gt;&lt;p&gt;Meta's investment thesis rests on a sophisticated understanding of competitive dynamics that extend beyond traditional model development. While competitors like Microsoft pour billions into model creators like OpenAI, Meta is betting on controlling the underlying data infrastructure that feeds all AI systems.&lt;/p&gt;&lt;p&gt;This approach offers several compelling benefits:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Proprietary dataset access&lt;/strong&gt; ‚Äî Enhanced model training capabilities while potentially limiting competitor access to the same high-quality data&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Pipeline control&lt;/strong&gt; ‚Äî Reduced dependencies on external providers and more predictable cost structures&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Infrastructure focus&lt;/strong&gt; ‚Äî Investment in foundational layers rather than competing solely on model architecture&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Scale AI partnership positions Meta to capitalize on the growing complexity of AI training data requirements. Recent developments suggest that advances in large AI models may depend less on architectural innovations and more on access to high-quality training data and compute. This insight drives Meta's willingness to invest heavily in data infrastructure rather than competing solely on model architecture.&lt;/p&gt;&lt;h2&gt;The Military and Government Dimension&lt;/h2&gt;&lt;p&gt;The investment carries significant implications beyond commercial AI applications. Both Meta and Scale AI are deepening ties with the US government. The two companies are working on Defense Llama, a military-adapted version of Meta's Llama model. Scale AI recently landed a contract with the US Department of Defense to develop AI agents for operational use.&lt;/p&gt;&lt;p&gt;This government partnership dimension adds strategic value that extends far beyond immediate financial returns. Military and government contracts provide stable, long-term revenue streams while positioning both companies as critical infrastructure providers for national AI capabilities. The Defense Llama project exemplifies how commercial AI development increasingly intersects with national security considerations.&lt;/p&gt;&lt;h2&gt;Challenging the Microsoft-OpenAI Paradigm&lt;/h2&gt;&lt;p&gt;Meta's Scale AI investment would be a direct challenge to the dominant Microsoft-OpenAI partnership model that has defined the current AI space. Microsoft remains a major investor in OpenAI, providing funding and capacity to support their advancements, but this relationship focuses primarily on model development and deployment rather than fundamental data infrastructure.&lt;/p&gt;&lt;p&gt;By contrast, Meta's approach prioritizes controlling the foundational layer that enables all AI development. This strategy could prove more durable than exclusive model partnerships, which face increasing competitive pressure and potential partnership instability. Recent reports suggest Microsoft is developing its own in-house reasoning models to compete with OpenAI and has been testing models from Elon Musk's xAI, Meta, and DeepSeek to replace ChatGPT in Copilot, highlighting the inherent tensions in Big Tech's AI investment strategies.&lt;/p&gt;&lt;h2&gt;The Economics of AI Infrastructure&lt;/h2&gt;&lt;p&gt;Scale AI saw $870 million in revenue last year and expects to bring in $2 billion this year, demonstrating the substantial market demand for professional AI data services. The company's valuation trajectory‚Äîfrom around $7 billion to $13.8 billion in recent funding rounds‚Äîreflects investor recognition that data infrastructure represents a durable competitive moat.&lt;/p&gt;&lt;p&gt;Meta's $10 billion investment would provide Scale AI with unprecedented resources to expand its operations globally and develop more sophisticated data processing capabilities. This scale advantage could create network effects that make it increasingly difficult for competitors to match Scale AI's quality and cost efficiency, particularly as AI infrastructure investments continue to escalate across the industry.&lt;/p&gt;&lt;p&gt;This investment signals a broader industry evolution toward vertical integration of AI infrastructure. Rather than relying on partnerships with specialized AI companies, tech giants are increasingly acquiring or investing heavily in the underlying infrastructure that enables AI development.&lt;/p&gt;&lt;p&gt;The move also highlights growing recognition that data quality and model alignment services will become even more critical as AI systems become more powerful and are deployed in more sensitive applications. Scale AI's expertise in reinforcement learning from human feedback (RLHF) and model evaluation provides Meta with capabilities essential for developing safe, reliable AI systems.&lt;/p&gt;&lt;h2&gt;Looking Forward: The Data Wars Begin&lt;/h2&gt;&lt;p&gt;Meta's Scale AI investment represents the opening salvo in what may become the ‚Äúdata wars‚Äù‚Äîa competition for control over the high-quality, specialized datasets that will determine AI leadership in the coming decade.&lt;/p&gt;&lt;p&gt;This strategic pivot acknowledges that while the current AI boom began with breakthrough models like ChatGPT, sustained competitive advantage will come from controlling the infrastructure that enables continuous model improvement. As the industry matures beyond the initial excitement of generative AI, companies that control data pipelines may find themselves with more durable advantages than those who merely license or partner for model access.&lt;/p&gt;&lt;p&gt;For Meta, the Scale AI investment is a calculated bet that the future of AI competition will be won in the data preprocessing centers and annotation workflows that most consumers never see‚Äîbut which ultimately determine which AI systems succeed in the real world. If this thesis proves correct, Meta's $10 billion investment may be remembered as the moment the company secured its position in the next phase of the AI revolution.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/why-metas-biggest-ai-bet-isnt-on-models-its-on-data/</guid><pubDate>Mon, 09 Jun 2025 03:17:59 +0000</pubDate></item><item><title>[NEW] UK Prime Minister, NVIDIA CEO Set the Stage as AI Lights Up Europe (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-lights-up-europe/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/DSC_1187-2.jpeg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI isn‚Äôt waiting. And this week, neither is Europe.&lt;/p&gt;
&lt;p&gt;At London‚Äôs Olympia, under a ceiling of steel beams and enveloped by the thrum of startup pitches, it didn‚Äôt feel like the start of a conference ‚Äî it felt like the start of something bigger.&lt;/p&gt;
&lt;p&gt;NVIDIA founder and CEO Jensen Huang joined U.K. Prime Minister Sir Keir Starmer to open London Tech Week, a moment that signaled a clear shift: what used to be the domain of ambitious technology startups is now national policy ‚Äî backed by investments in people, platforms and partnerships.&lt;/p&gt;
&lt;p&gt;AI is transforming the entire ecosystem, everything from healthcare and manufacturing to scientific research, Huang told the audience. ‚ÄúI make this prediction ‚Äì because of AI, every industry in the UK will be a tech industry,‚Äù Huang said.&lt;/p&gt;
&lt;p&gt;Starmer added that his team is looking at every single department in government to see how AI can be used.&lt;/p&gt;
&lt;p&gt;Starmer‚Äôs goal for the session was clear: to bring to life the real-world impact of the AI revolution and how AI is changing everyday lives for U.K. citizens.&lt;/p&gt;
&lt;p&gt;‚ÄúThe U.K. has one of the richest AI communities of anywhere on the planet, the deepest thinkers, the best universities‚Ä¶ and the third largest AI capital investment of anywhere in the world,‚Äù&amp;nbsp; Huang said.&lt;/p&gt;
&lt;p&gt;‚ÄúSo the ability to build these AI supercomputers here in the U.K. will naturally attract more startups, it will naturally enable the rich ecosystem of researchers here to do their life‚Äôs work,‚Äù Huang added.&lt;/p&gt;
&lt;p&gt;To that end, NVIDIA will continue to invest in the U.K. ‚ÄúWe‚Äôre going to start our AI lab here‚Ä¶ we‚Äôre going to partner with the UK to upskill the ecosystem of developers into the world of AI,‚Äù Huang added.&lt;/p&gt;
&lt;p&gt;All of these investments will build on one another. ‚ÄúInfrastructure enables more research, more research, more breakthroughs, more companies,‚Äù Huang said. That flywheel will start taking off; it‚Äôs already quite large.‚Äù&lt;/p&gt;
&lt;h2&gt;UK on the Move: Momentum in Action&lt;/h2&gt;
&lt;p&gt;This wasn‚Äôt just a symbolic handshake. It marked the U.K.‚Äôs acceleration toward embedding AI at the core of its economic strategy. A major announcement from Prime Minister Starmer confirmed the U.K. will invest ~¬£1 billion in AI research compute by 2030, with investments commencing this year.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A national AI skills initiative supported by NVIDIA aims to train developers in advanced AI skills.&lt;/li&gt;
&lt;li&gt;A new NVIDIA AI Technology Center in the U.K. is launching to accelerate research in embodied AI, material science and earth system modeling.&lt;/li&gt;
&lt;li&gt;The U.K.‚Äôs Financial Conduct Authority is using NVIDIA tech to power its innovation sandbox for safe and secure AI experimentation.&lt;/li&gt;
&lt;li&gt;The U.K. government and NVIDIA also announced a new initiative to accelerate AI-native 6G research and deployment.&lt;/li&gt;
&lt;li&gt;And further cementing the U.K.‚Äôs compute power, Isambard AI, the U.K.‚Äôs fastest AI supercomputer powered by 5.5k GH200s, is set to be fully operational this summer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;‚ÄúWe need to showcase what we have,‚Äù Starmer said. ‚ÄúThis is a two-way conversation‚Äù between the government and industry.&lt;/p&gt;
&lt;p&gt;Starmer underscored the U.K.‚Äôs ‚Äúsovereign AI ambitions,‚Äù emphasizing that AI is not just about technology, but about codifying a nation‚Äôs culture, common sense and history.&lt;/p&gt;
&lt;p&gt;And the movement isn‚Äôt confined to the U.K. Across Europe, governments are no longer debating whether AI matters. Now the question in every capital isn‚Äôt why AI, it‚Äôs how soon can we deploy it at scale?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In Sweden, NVIDIA is working with Wallenberg Investments, AstraZeneca, Ericsson, Saab and SEB to build the country‚Äôs first national AI infrastructure, anchored by the NVIDIA Grace Blackwell platform.&lt;/li&gt;
&lt;li&gt;In Germany, the Leibniz Supercomputing Centre is building Blue Lion ‚Äî a ‚Ç¨250 million supercomputer based on the new NVIDIA Vera Rubin architecture, designed for real-time AI, simulation and science.&lt;/li&gt;
&lt;li&gt;In France, a joint venture between MGX, Bpifrance, Mistral AI and NVIDIA will establish Europe‚Äôs largest AI Campus in the Paris region, a 1.4 GW facility aiming to build sovereign and sustainable AI infrastructure for the continent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;‚ÄúIn the last 10 years, AI has advanced 1 million times,‚Äù Huang said. ‚ÄúThe speed of change is incredible.‚Äù&lt;/p&gt;
&lt;p&gt;NVIDIA‚Äôs commitment to the U.K. is evident, with over 1,700 Inception members and 500 employees across four offices.&lt;/p&gt;
&lt;p&gt;NVIDIA is actively building the ‚ÄòAI factories of the future‚Äô with leading U.K. companies.&lt;/p&gt;
&lt;p&gt;And it‚Äôs powering the next generation of startups and scale-ups, from Basecamp Research to Wayve.&lt;/p&gt;
&lt;h2&gt;What Comes Next: NVIDIA GTC Paris at VivaTech&lt;/h2&gt;
&lt;p&gt;Next, the story moves to Paris, where Jensen Huang will headline NVIDIA GTC Paris live from VivaTech.&lt;/p&gt;
&lt;p&gt;üóìÔ∏è June 11 | 11:00 a.m. CEST | D√¥me de Paris&lt;br /&gt;üéüÔ∏è VivaTech or GTC Paris pass required to attend&lt;br /&gt;üíª Livestream available globally, free&lt;/p&gt;
&lt;p&gt;Expect news on NVIDIA Blackwell, sovereign AI initiatives, new regional partnerships and how European innovators are turning intent into infrastructure with NVIDIA.&lt;/p&gt;
&lt;h2&gt;One Week. One Story. One Start.&lt;/h2&gt;
&lt;p&gt;From Downing Street to the D√¥me de Paris, this week reads less like a schedule and more like a strategy.&lt;/p&gt;
&lt;p&gt;This isn‚Äôt just a collection of conferences. It‚Äôs a continental shift ‚Äî where Europe is aligning talent, policy and compute to lead in AI.&lt;/p&gt;
&lt;p&gt;This is just chapter one. But the story is already racing ahead.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/DSC_1187-2.jpeg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI isn‚Äôt waiting. And this week, neither is Europe.&lt;/p&gt;
&lt;p&gt;At London‚Äôs Olympia, under a ceiling of steel beams and enveloped by the thrum of startup pitches, it didn‚Äôt feel like the start of a conference ‚Äî it felt like the start of something bigger.&lt;/p&gt;
&lt;p&gt;NVIDIA founder and CEO Jensen Huang joined U.K. Prime Minister Sir Keir Starmer to open London Tech Week, a moment that signaled a clear shift: what used to be the domain of ambitious technology startups is now national policy ‚Äî backed by investments in people, platforms and partnerships.&lt;/p&gt;
&lt;p&gt;AI is transforming the entire ecosystem, everything from healthcare and manufacturing to scientific research, Huang told the audience. ‚ÄúI make this prediction ‚Äì because of AI, every industry in the UK will be a tech industry,‚Äù Huang said.&lt;/p&gt;
&lt;p&gt;Starmer added that his team is looking at every single department in government to see how AI can be used.&lt;/p&gt;
&lt;p&gt;Starmer‚Äôs goal for the session was clear: to bring to life the real-world impact of the AI revolution and how AI is changing everyday lives for U.K. citizens.&lt;/p&gt;
&lt;p&gt;‚ÄúThe U.K. has one of the richest AI communities of anywhere on the planet, the deepest thinkers, the best universities‚Ä¶ and the third largest AI capital investment of anywhere in the world,‚Äù&amp;nbsp; Huang said.&lt;/p&gt;
&lt;p&gt;‚ÄúSo the ability to build these AI supercomputers here in the U.K. will naturally attract more startups, it will naturally enable the rich ecosystem of researchers here to do their life‚Äôs work,‚Äù Huang added.&lt;/p&gt;
&lt;p&gt;To that end, NVIDIA will continue to invest in the U.K. ‚ÄúWe‚Äôre going to start our AI lab here‚Ä¶ we‚Äôre going to partner with the UK to upskill the ecosystem of developers into the world of AI,‚Äù Huang added.&lt;/p&gt;
&lt;p&gt;All of these investments will build on one another. ‚ÄúInfrastructure enables more research, more research, more breakthroughs, more companies,‚Äù Huang said. That flywheel will start taking off; it‚Äôs already quite large.‚Äù&lt;/p&gt;
&lt;h2&gt;UK on the Move: Momentum in Action&lt;/h2&gt;
&lt;p&gt;This wasn‚Äôt just a symbolic handshake. It marked the U.K.‚Äôs acceleration toward embedding AI at the core of its economic strategy. A major announcement from Prime Minister Starmer confirmed the U.K. will invest ~¬£1 billion in AI research compute by 2030, with investments commencing this year.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A national AI skills initiative supported by NVIDIA aims to train developers in advanced AI skills.&lt;/li&gt;
&lt;li&gt;A new NVIDIA AI Technology Center in the U.K. is launching to accelerate research in embodied AI, material science and earth system modeling.&lt;/li&gt;
&lt;li&gt;The U.K.‚Äôs Financial Conduct Authority is using NVIDIA tech to power its innovation sandbox for safe and secure AI experimentation.&lt;/li&gt;
&lt;li&gt;The U.K. government and NVIDIA also announced a new initiative to accelerate AI-native 6G research and deployment.&lt;/li&gt;
&lt;li&gt;And further cementing the U.K.‚Äôs compute power, Isambard AI, the U.K.‚Äôs fastest AI supercomputer powered by 5.5k GH200s, is set to be fully operational this summer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;‚ÄúWe need to showcase what we have,‚Äù Starmer said. ‚ÄúThis is a two-way conversation‚Äù between the government and industry.&lt;/p&gt;
&lt;p&gt;Starmer underscored the U.K.‚Äôs ‚Äúsovereign AI ambitions,‚Äù emphasizing that AI is not just about technology, but about codifying a nation‚Äôs culture, common sense and history.&lt;/p&gt;
&lt;p&gt;And the movement isn‚Äôt confined to the U.K. Across Europe, governments are no longer debating whether AI matters. Now the question in every capital isn‚Äôt why AI, it‚Äôs how soon can we deploy it at scale?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In Sweden, NVIDIA is working with Wallenberg Investments, AstraZeneca, Ericsson, Saab and SEB to build the country‚Äôs first national AI infrastructure, anchored by the NVIDIA Grace Blackwell platform.&lt;/li&gt;
&lt;li&gt;In Germany, the Leibniz Supercomputing Centre is building Blue Lion ‚Äî a ‚Ç¨250 million supercomputer based on the new NVIDIA Vera Rubin architecture, designed for real-time AI, simulation and science.&lt;/li&gt;
&lt;li&gt;In France, a joint venture between MGX, Bpifrance, Mistral AI and NVIDIA will establish Europe‚Äôs largest AI Campus in the Paris region, a 1.4 GW facility aiming to build sovereign and sustainable AI infrastructure for the continent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;‚ÄúIn the last 10 years, AI has advanced 1 million times,‚Äù Huang said. ‚ÄúThe speed of change is incredible.‚Äù&lt;/p&gt;
&lt;p&gt;NVIDIA‚Äôs commitment to the U.K. is evident, with over 1,700 Inception members and 500 employees across four offices.&lt;/p&gt;
&lt;p&gt;NVIDIA is actively building the ‚ÄòAI factories of the future‚Äô with leading U.K. companies.&lt;/p&gt;
&lt;p&gt;And it‚Äôs powering the next generation of startups and scale-ups, from Basecamp Research to Wayve.&lt;/p&gt;
&lt;h2&gt;What Comes Next: NVIDIA GTC Paris at VivaTech&lt;/h2&gt;
&lt;p&gt;Next, the story moves to Paris, where Jensen Huang will headline NVIDIA GTC Paris live from VivaTech.&lt;/p&gt;
&lt;p&gt;üóìÔ∏è June 11 | 11:00 a.m. CEST | D√¥me de Paris&lt;br /&gt;üéüÔ∏è VivaTech or GTC Paris pass required to attend&lt;br /&gt;üíª Livestream available globally, free&lt;/p&gt;
&lt;p&gt;Expect news on NVIDIA Blackwell, sovereign AI initiatives, new regional partnerships and how European innovators are turning intent into infrastructure with NVIDIA.&lt;/p&gt;
&lt;h2&gt;One Week. One Story. One Start.&lt;/h2&gt;
&lt;p&gt;From Downing Street to the D√¥me de Paris, this week reads less like a schedule and more like a strategy.&lt;/p&gt;
&lt;p&gt;This isn‚Äôt just a collection of conferences. It‚Äôs a continental shift ‚Äî where Europe is aligning talent, policy and compute to lead in AI.&lt;/p&gt;
&lt;p&gt;This is just chapter one. But the story is already racing ahead.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-lights-up-europe/</guid><pubDate>Mon, 09 Jun 2025 10:25:39 +0000</pubDate></item><item><title>[NEW] The Download: an inspiring toy robot arm, and why AM radio matters (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/09/1118186/the-download-an-inspiring-toy-robot-arm-and-why-am-radio-matters/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How a 1980s toy robot arm inspired modern robotics&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;‚ÄîJon Keegan&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;As a child of an electronic engineer, I spent a lot of time in our local Radio Shack as a kid. While my dad was locating capacitors and resistors, I was in the toy section. It was there, in 1984, that I discovered the best toy of my childhood: the Armatron robotic arm.&lt;/p&gt; 
 &lt;p&gt;Described as a ‚Äúrobot-like arm to aid young masterminds in scientific and laboratory experiments,‚Äù it was a legit robotic arm. And the bold look and function of Armatron made quite an impression on many young kids who would one day have a career in robotics. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you‚Äôre interested in the future of robots, why not check out:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;Will we ever trust robots?&lt;/strong&gt; If most robots still need remote human operators to be safe and effective, why should we welcome them into our homes? Read the full story.&lt;/p&gt; 
 &lt;p&gt;+ &lt;strong&gt;When you might start speaking to robots&lt;/strong&gt;. Google is only the latest to fuse large language models with robots. Here‚Äôs why the trend has big implications.&lt;/p&gt;&lt;p&gt;+ How AI models let robots carry out tasks in unfamiliar environments. Read the full story.&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;China‚Äôs EV giants are betting big on humanoid robots&lt;/strong&gt;. Technical know-how and existing supply chains give Chinese electric-vehicle makers a significant head start in the sector. Read the full story.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why we still need AM radio&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The most reliable way to keep us informed in times of disaster is being threatened. Check out Ariel Aberg-Riger‚Äôs beautiful visual story illustrating AM radio‚Äôs importance in uncertain times.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;Both of these stories are from the most recent edition of our print magazine, which is all about how &lt;/strong&gt;&lt;strong&gt;technology is changing creativity&lt;/strong&gt;&lt;strong&gt;. &lt;/strong&gt;&lt;strong&gt;Subscribe now&lt;/strong&gt;&lt;strong&gt; to get future copies before they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 Protestors set Waymo robotaxis alight in Los Angeles&lt;/strong&gt;&lt;br /&gt;The groups clashed with police over the Trump administration‚Äôs immigration raids. (LA Times $)&lt;br /&gt;+ &lt;em&gt;Much of the technology that fuels deportation orders is error-ridden. &lt;/em&gt;(Slate $)&lt;br /&gt;+ &lt;em&gt;Immigrants are using a swathe of new apps to stay ahead of deportation. &lt;/em&gt;(Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 What‚Äôs next for Elon Musk and Donald Trump&lt;/strong&gt;&lt;br /&gt;A full breakdown in relations could be much worse for Musk in the long run. (NY Mag $)&lt;br /&gt;+ &lt;em&gt;Trump‚Äôs backers are rapidly turning on Musk, too. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;The biggest winner from their fall out? Jeff Bezos. &lt;/em&gt;(The Information $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 DOGE used an inaccurate AI tool to terminate Veteran Affairs contacts&lt;/strong&gt;&lt;br /&gt;Its code frequently produced glaring mistakes. (ProPublica)&lt;br /&gt;+ &lt;em&gt;Undeterred, the department is on a hiring spree. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;Can AI help DOGE slash government budgets? It‚Äôs complex. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Europe‚Äôs shrinking forests could cause it to miss net-zero targets&lt;/strong&gt;&lt;br /&gt;Its trees aren‚Äôt soaking up as much carbon as they used to. (New Scientist $)&lt;br /&gt;+ &lt;em&gt;Inside the controversial tree farms powering Apple‚Äôs carbon neutral goal. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 OpenAI wants to embed ChatGPT into college campuses&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;The ultimate goal? A personalized AI account for every student. (NYT $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, other universities are experimenting with tech-free classes. &lt;/em&gt;(The Atlantic $)&lt;br /&gt;+ &lt;em&gt;ChatGPT is going to change education, not destroy it. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Chinese regulators are pumping the brakes on self-driving cars&lt;/strong&gt;&lt;br /&gt;They‚Äôre developing a new framework to assess the safety of autonomous features. (FT $)&lt;br /&gt;+ &lt;em&gt;The country‚Äôs robotaxis are rapidly catching up with the west. &lt;/em&gt;(Rest of World)&lt;br /&gt;+ &lt;em&gt;How China is regulating robotaxis. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Desalination is finally becoming a reality&lt;br /&gt;&lt;/strong&gt;Removing salt from seawater is one way to combat water scarcity. (WSJ $)&lt;br /&gt;+ &lt;em&gt;If you can make it through tons of plastic, that is. &lt;/em&gt;(The Atlantic $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 We‚Äôre getting better at fighting cancer&lt;br /&gt;&lt;/strong&gt;Deaths from the disease in the US have dropped by a third since 1991. (Vox)&lt;br /&gt;+ &lt;em&gt;Why it‚Äôs so hard to use AI to diagnose cancer. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Teenage TikTokers‚Äô skin regimes offer virtually no benefit&lt;br /&gt;&lt;/strong&gt;And could even be potentially harmful. (The Guardian)&lt;br /&gt;+ &lt;em&gt;The fight for ‚ÄúInstagram face‚Äù &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Tech‚Äôs layoff groups are providing much-needed support&lt;/strong&gt;&lt;br /&gt;Workers who have been let go by their employers are forming little communities. (Insider $)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;‚ÄúEvery tech company is doing similar things but we were open about it.‚Äù&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;‚ÄîLuis von Ahn, chief executive of the language-learning app Duolingo, tells the Financial Times that his company is far from the only one adopting an AI-first strategy.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/EscapingSpotifyAlgorithm_web.jpg?fit=1456,818" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How to break free of Spotify‚Äôs algorithm&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Since the heyday of radio, the branding of sound has evolved from broad genres like rock and hip-hop to ‚Äúparanormal dark cabaret afternoon‚Äù and ‚Äúsynth space,‚Äù and streaming has become the default.&lt;/p&gt;&lt;p&gt;Meanwhile, the ritual of discovering something new is now neatly packaged in a 30-song playlist. The only rule in music streaming is personalization.&lt;/p&gt;&lt;p&gt;What we‚Äôve gained in convenience, we‚Äôve lost in curiosity. But it doesn‚Äôt have to be this way. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;‚ÄîTiffany Ng&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Happy birthday to Michael J Fox, who turns 64 today!&lt;br /&gt;+ Whenever you need to play the world‚Äôs smallest violin, these scientists can help you out üéª&lt;br /&gt;+ An early JMW Turner oil painting has been rediscovered.&lt;br /&gt;+ Watching robots attempt to kickbox is pretty amusing.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How a 1980s toy robot arm inspired modern robotics&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;‚ÄîJon Keegan&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;As a child of an electronic engineer, I spent a lot of time in our local Radio Shack as a kid. While my dad was locating capacitors and resistors, I was in the toy section. It was there, in 1984, that I discovered the best toy of my childhood: the Armatron robotic arm.&lt;/p&gt; 
 &lt;p&gt;Described as a ‚Äúrobot-like arm to aid young masterminds in scientific and laboratory experiments,‚Äù it was a legit robotic arm. And the bold look and function of Armatron made quite an impression on many young kids who would one day have a career in robotics. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you‚Äôre interested in the future of robots, why not check out:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;Will we ever trust robots?&lt;/strong&gt; If most robots still need remote human operators to be safe and effective, why should we welcome them into our homes? Read the full story.&lt;/p&gt; 
 &lt;p&gt;+ &lt;strong&gt;When you might start speaking to robots&lt;/strong&gt;. Google is only the latest to fuse large language models with robots. Here‚Äôs why the trend has big implications.&lt;/p&gt;&lt;p&gt;+ How AI models let robots carry out tasks in unfamiliar environments. Read the full story.&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;China‚Äôs EV giants are betting big on humanoid robots&lt;/strong&gt;. Technical know-how and existing supply chains give Chinese electric-vehicle makers a significant head start in the sector. Read the full story.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why we still need AM radio&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The most reliable way to keep us informed in times of disaster is being threatened. Check out Ariel Aberg-Riger‚Äôs beautiful visual story illustrating AM radio‚Äôs importance in uncertain times.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;Both of these stories are from the most recent edition of our print magazine, which is all about how &lt;/strong&gt;&lt;strong&gt;technology is changing creativity&lt;/strong&gt;&lt;strong&gt;. &lt;/strong&gt;&lt;strong&gt;Subscribe now&lt;/strong&gt;&lt;strong&gt; to get future copies before they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 Protestors set Waymo robotaxis alight in Los Angeles&lt;/strong&gt;&lt;br /&gt;The groups clashed with police over the Trump administration‚Äôs immigration raids. (LA Times $)&lt;br /&gt;+ &lt;em&gt;Much of the technology that fuels deportation orders is error-ridden. &lt;/em&gt;(Slate $)&lt;br /&gt;+ &lt;em&gt;Immigrants are using a swathe of new apps to stay ahead of deportation. &lt;/em&gt;(Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 What‚Äôs next for Elon Musk and Donald Trump&lt;/strong&gt;&lt;br /&gt;A full breakdown in relations could be much worse for Musk in the long run. (NY Mag $)&lt;br /&gt;+ &lt;em&gt;Trump‚Äôs backers are rapidly turning on Musk, too. &lt;/em&gt;(New Yorker $)&lt;br /&gt;+ &lt;em&gt;The biggest winner from their fall out? Jeff Bezos. &lt;/em&gt;(The Information $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 DOGE used an inaccurate AI tool to terminate Veteran Affairs contacts&lt;/strong&gt;&lt;br /&gt;Its code frequently produced glaring mistakes. (ProPublica)&lt;br /&gt;+ &lt;em&gt;Undeterred, the department is on a hiring spree. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;Can AI help DOGE slash government budgets? It‚Äôs complex. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Europe‚Äôs shrinking forests could cause it to miss net-zero targets&lt;/strong&gt;&lt;br /&gt;Its trees aren‚Äôt soaking up as much carbon as they used to. (New Scientist $)&lt;br /&gt;+ &lt;em&gt;Inside the controversial tree farms powering Apple‚Äôs carbon neutral goal. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;5 OpenAI wants to embed ChatGPT into college campuses&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;The ultimate goal? A personalized AI account for every student. (NYT $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, other universities are experimenting with tech-free classes. &lt;/em&gt;(The Atlantic $)&lt;br /&gt;+ &lt;em&gt;ChatGPT is going to change education, not destroy it. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Chinese regulators are pumping the brakes on self-driving cars&lt;/strong&gt;&lt;br /&gt;They‚Äôre developing a new framework to assess the safety of autonomous features. (FT $)&lt;br /&gt;+ &lt;em&gt;The country‚Äôs robotaxis are rapidly catching up with the west. &lt;/em&gt;(Rest of World)&lt;br /&gt;+ &lt;em&gt;How China is regulating robotaxis. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Desalination is finally becoming a reality&lt;br /&gt;&lt;/strong&gt;Removing salt from seawater is one way to combat water scarcity. (WSJ $)&lt;br /&gt;+ &lt;em&gt;If you can make it through tons of plastic, that is. &lt;/em&gt;(The Atlantic $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 We‚Äôre getting better at fighting cancer&lt;br /&gt;&lt;/strong&gt;Deaths from the disease in the US have dropped by a third since 1991. (Vox)&lt;br /&gt;+ &lt;em&gt;Why it‚Äôs so hard to use AI to diagnose cancer. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Teenage TikTokers‚Äô skin regimes offer virtually no benefit&lt;br /&gt;&lt;/strong&gt;And could even be potentially harmful. (The Guardian)&lt;br /&gt;+ &lt;em&gt;The fight for ‚ÄúInstagram face‚Äù &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Tech‚Äôs layoff groups are providing much-needed support&lt;/strong&gt;&lt;br /&gt;Workers who have been let go by their employers are forming little communities. (Insider $)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;‚ÄúEvery tech company is doing similar things but we were open about it.‚Äù&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;‚ÄîLuis von Ahn, chief executive of the language-learning app Duolingo, tells the Financial Times that his company is far from the only one adopting an AI-first strategy.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2024/08/EscapingSpotifyAlgorithm_web.jpg?fit=1456,818" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How to break free of Spotify‚Äôs algorithm&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Since the heyday of radio, the branding of sound has evolved from broad genres like rock and hip-hop to ‚Äúparanormal dark cabaret afternoon‚Äù and ‚Äúsynth space,‚Äù and streaming has become the default.&lt;/p&gt;&lt;p&gt;Meanwhile, the ritual of discovering something new is now neatly packaged in a 30-song playlist. The only rule in music streaming is personalization.&lt;/p&gt;&lt;p&gt;What we‚Äôve gained in convenience, we‚Äôve lost in curiosity. But it doesn‚Äôt have to be this way. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;‚ÄîTiffany Ng&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Happy birthday to Michael J Fox, who turns 64 today!&lt;br /&gt;+ Whenever you need to play the world‚Äôs smallest violin, these scientists can help you out üéª&lt;br /&gt;+ An early JMW Turner oil painting has been rediscovered.&lt;br /&gt;+ Watching robots attempt to kickbox is pretty amusing.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/09/1118186/the-download-an-inspiring-toy-robot-arm-and-why-am-radio-matters/</guid><pubDate>Mon, 09 Jun 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] ‚ÄòProtected‚Äô Images Are Easier, Not More Difficult, to Steal With AI (Unite.AI)</title><link>https://www.unite.ai/protected-images-are-easier-not-more-difficult-to-steal-with-ai/</link><description>&lt;p&gt;&lt;em&gt;&lt;i&gt;New research suggests that watermarking tools meant to block AI image edits may backfire. Instead of stopping models like Stable Diffusion from making changes, some protections actually &lt;/i&gt;&lt;/em&gt;help&lt;em&gt;&lt;i&gt; the AI follow editing prompts more closely, making unwanted manipulations even easier.&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;There is a notable and robust strand in computer vision literature dedicated to protecting copyrighted images from being trained into AI models, or being used in direct image&amp;gt;image AI processes. Systems of this kind are generally aimed at Latent Diffusion Models (LDMs) such as Stable Diffusion and Flux, which use noise-based procedures to encode and decode imagery.&lt;/p&gt;&lt;p&gt;By inserting adversarial noise into otherwise normal-looking images, it can be possible to cause image detectors to guess image content incorrectly, and hobble image-generating systems from exploiting copyrighted data:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218991"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="From the MIT paper 'Raising the Cost of Malicious AI-Powered Image Editing', examples of a source image 'immunized' against manipulation (lower row). Source: https://arxiv.org/pdf/2302.06588" class=" wp-image-218991 webpexpress-processed" height="407" src="https://www.unite.ai/wp-content/uploads/2025/06/MIT-immunize.jpg" width="893" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218991"&gt;&lt;em&gt;From the MIT paper ‚ÄòRaising the Cost of Malicious AI-Powered Image Editing', examples of a source image ‚Äòimmunized' against manipulation (lower row).&lt;/em&gt; Source: https://arxiv.org/pdf/2302.06588&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Since an artists' backlash against Stable Diffusion's liberal use of web-scraped imagery (including copyrighted imagery) in 2023, the research scene has produced multiple variations on the same theme ‚Äì the idea that pictures can be invisibly ‚Äòpoisoned' against being trained into AI systems or sucked into generative AI pipelines, without adversely affecting the quality of the image, for the average viewer.&lt;/p&gt;&lt;p&gt;In all cases, there is a direct correlation between the intensity of the imposed perturbation, the extent to which the image is subsequently protected, and the extent to which the image doesn't look quite as good as it should:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218992"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Though the quality of the research PDF does not completely illustrate the problem, greater amounts of adversarial perturbation sacrifice quality for security. Here we see the gamut of quality disturbances in the 2020 'Fawkes' project led by the University of Chicago. Source: https://arxiv.org/pdf/2002.08327" class=" wp-image-218992 webpexpress-processed" height="263" src="https://www.unite.ai/wp-content/uploads/2025/06/protection-success-rate.jpg" width="899" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218992"&gt;&lt;em&gt;Though the quality of the research PDF does not completely illustrate the problem, greater amounts of adversarial perturbation sacrifice quality for security. Here we see the gamut of quality disturbances in the 2020 ‚ÄòFawkes' project led by the University of Chicago.&lt;/em&gt; Source: https://arxiv.org/pdf/2002.08327&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Of particular interest to artists seeking to protect their styles against unauthorized appropriation is the capacity of such systems not only to obfuscate identity and other information, but to ‚Äòconvince' an AI training process that it is seeing something other than it is really seeing, so that connections do not form between semantic and visual domains for ‚Äòprotected' training data (i.e., a prompt such as &lt;em&gt;&lt;i&gt;‚ÄòIn the style of Paul Klee'&lt;/i&gt;&lt;/em&gt;).&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218993"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Mist and Glaze are two popular injection methods capable of preventing, or at least severely hobbling attempts to use copyrighted styles in AI workflows and training routines. Source: https://arxiv.org/pdf/2506.04394" class=" wp-image-218993 webpexpress-processed" height="502" src="https://www.unite.ai/wp-content/uploads/2025/06/style-change-to-cubism.jpg" width="902" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218993"&gt;&lt;em&gt;Mist and Glaze are two popular injection methods capable of preventing, or at least severely hobbling attempts to use copyrighted styles in AI workflows and training routines.&lt;/em&gt; Source: https://arxiv.org/pdf/2506.04394&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;Own Goal&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Now, new research from the US has found not only that perturbations can fail to protect an image, but that adding perturbation can actually &lt;em&gt;improve&lt;/em&gt; the image's exploitability in all the AI processes that perturbation is meant to immunize against.&lt;/p&gt;&lt;p&gt;The paper states:&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòIn our experiments with various perturbation-based image protection methods across multiple domains (natural scene images and artworks) and editing tasks (image-to-image generation and style editing), we discover that such protection does not achieve this goal completely. &lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòIn most scenarios, diffusion-based editing of protected images generates a desirable output image which adheres precisely to the guidance prompt. &lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòOur findings suggest that adding noise to images may paradoxically increase their association with given text prompts during the generation process, leading to unintended consequences such as &lt;/i&gt;&lt;/em&gt;better &lt;em&gt;&lt;i&gt;resultant edits. &lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòHence, we argue that perturbation-based methods may not provide a sufficient solution for robust image protection against diffusion-based editing.'&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;In tests, the protected images were exposed to two familiar AI editing scenarios: straightforward image-to-image generation and style transfer. These processes reflect the common ways that AI models might exploit protected content, either by directly altering an image, or by borrowing its stylistic traits for use elsewhere.&lt;/p&gt;&lt;p&gt;The protected images, drawn from standard sources of photography and artwork, were run through these pipelines to see whether the added perturbations could block or degrade the edits.&lt;/p&gt;&lt;p&gt;Instead, the presence of protection often seemed to sharpen the model‚Äôs alignment with the prompts, producing clean, accurate outputs where some failure had been expected.&lt;/p&gt;&lt;p&gt;The authors advise, in effect, that this very popular method of protection may be providing a false sense of security, and that any such perturbation-based immunization approaches should be tested thoroughly against the authors' own methods.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The authors ran experiments using three protection methods that apply carefully-designed adversarial perturbations: PhotoGuard; Mist; and Glaze.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218994"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Glaze, one of the frameworks tested by the authors. Glaze protection examples for three artists. The first two columns show the original artworks. The third column shows mimicry results without protection. The fourth column shows style-transferred versions used for cloak optimization, along with the target style name. The fifth and sixth columns show mimicry results with cloaking applied at perturbation levels p = 0.05 and p = 0.1. All results use Stable Diffusion models. https://arxiv.org/pdf/2302.04222" class=" wp-image-218994 webpexpress-processed" height="415" src="https://www.unite.ai/wp-content/uploads/2025/06/glaze.jpg" width="898" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218994"&gt;&lt;em&gt;Glaze, one of the frameworks tested by the authors, illustrating Glaze protection examples for three artists. The first two columns show the original artworks; the third column shows mimicry results without protection; the fourth, style-transferred versions used for cloak optimization, along with the target style name. The fifth and sixth columns show mimicry results with cloaking applied at perturbation levels&lt;/em&gt; p = 0.05&lt;em&gt; and&lt;/em&gt; p = 0.1.&lt;em&gt; All results use Stable Diffusion models.&lt;/em&gt; https://arxiv.org/pdf/2302.04222&lt;/p&gt;&lt;/div&gt;&lt;p&gt;PhotoGuard was applied to natural scene images, while Mist and Glaze were used on artworks (i.e., ‚Äòartistically-styled' domains).&lt;/p&gt;&lt;p&gt;Tests covered both natural and artistic images to reflect possible real-world uses. The effectiveness of each method was assessed by checking whether an AI model could still produce realistic and prompt-relevant edits when working on protected images; if the resulting images appeared convincing and matched the prompts, the protection was judged to have failed.&lt;/p&gt;&lt;p&gt;Stable Diffusion v1.5 was used as the pre-trained image generator for the researchers' editing tasks. Five seeds were selected to ensure reproducibility: 9222, 999, 123, 66, and 42. All other generation settings, such as guidance scale, strength, and total steps, followed the default values used in the PhotoGuard experiments.&lt;/p&gt;&lt;p&gt;PhotoGuard was tested on natural scene images using the Flickr8k dataset, which contains over 8,000 images paired with up to five captions each.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Opposing Thoughts&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Two sets of modified captions were created from the first caption of each image with the help of Claude Sonnet 3.5. One set contained prompts that were &lt;em&gt;&lt;i&gt;contextually close&lt;/i&gt;&lt;/em&gt; to the original captions; the other set contained prompts that were &lt;em&gt;&lt;i&gt;contextually distant&lt;/i&gt;&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;For example, from the original caption &lt;em&gt;&lt;i&gt;‚ÄòA young girl in a pink dress going into a wooden cabin'&lt;/i&gt;&lt;/em&gt;, a close prompt would be &lt;em&gt;&lt;i&gt;‚ÄòA young boy in a blue shirt going into a brick house'&lt;/i&gt;&lt;/em&gt;. By contrast, a &lt;em&gt;&lt;i&gt;distant&lt;/i&gt;&lt;/em&gt; prompt would be &lt;em&gt;&lt;i&gt;‚ÄòTwo cats lounging on a couch'&lt;/i&gt;&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;Close prompts were constructed by replacing nouns and adjectives with semantically similar terms; far prompts were generated by instructing the model to create captions that were contextually very different.&lt;/p&gt;&lt;p&gt;All generated captions were manually checked for quality and semantic relevance. Google's Universal Sentence Encoder was used to calculate semantic similarity scores between the original and modified captions:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218995"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="From the supplementary material, semantic similarity distributions for the modified captions used in Flickr8k tests. The graph on the left shows the similarity scores for closely modified captions, averaging around 0.6. The graph on the right shows the extensively modified captions, averaging around 0.1, reflecting greater semantic distance from the original captions. Values were calculated using Google‚Äôs Universal Sentence Encoder. Source: https://sigport.org/sites/default/files/docs/IncompleteProtection_SM_0.pdf" class=" wp-image-218995 webpexpress-processed" height="301" src="https://www.unite.ai/wp-content/uploads/2025/06/graphs.jpg" width="923" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218995"&gt;&lt;em&gt;From the supplementary material, semantic similarity distributions for the modified captions used in Flickr8k tests. The graph on the left shows the similarity scores for closely modified captions, averaging around 0.6. The graph on the right shows the extensively modified captions, averaging around 0.1, reflecting greater semantic distance from the original captions. Values were calculated using Google‚Äôs Universal Sentence Encoder.&lt;/em&gt; Source: https://sigport.org/sites/default/files/docs/IncompleteProtection_SM_0.pdf&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Each image, along with its protected version, was edited using both the close and far prompts. The Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) was used to assess image quality:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218996"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Image-to-image generation results on natural photographs protected by PhotoGuard. Despite the presence of perturbations, Stable Diffusion v1.5 successfully followed both small and large semantic changes in the editing prompts, producing realistic outputs that matched the new instructions." class=" wp-image-218996 webpexpress-processed" height="588" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-2.jpg" width="703" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218996"&gt;&lt;em&gt;Image-to-image generation results on natural photographs protected by PhotoGuard. Despite the presence of perturbations, Stable Diffusion v1.5 successfully followed both small and large semantic changes in the editing prompts, producing realistic outputs that matched the new instructions.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;The generated images scored 17.88 on BRISQUE, with 17.82 for close prompts and 17.94 for far prompts, while the original images scored 22.27. This shows that the edited images remained close in quality to the originals.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Metrics&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;To judge how well the protections interfered with AI editing, the researchers measured how closely the final images matched the instructions they were given, using scoring systems that compared the image content to the text prompt, to see how well they align.&lt;/p&gt;&lt;p&gt;To this end, the CLIP-S metric uses a model that can understand both images and text to check how similar they are, while PAC-S++, adds extra samples created by AI to align its comparison more closely to a human estimation.&lt;/p&gt;&lt;p&gt;These Image-Text Alignment (ITA) scores denote how accurately the AI followed the instructions when modifying a protected image: if a protected image still led to a highly aligned output, it means the protection was deemed to have &lt;em&gt;&lt;i&gt;failed&lt;/i&gt;&lt;/em&gt; to block the edit.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218997"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Effect of protection on the Flickr8k dataset across five seeds, using both close and distant prompts. Image-text alignment was measured using CLIP-S and PAC-S++ scores." class=" wp-image-218997 webpexpress-processed" height="506" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-3.jpg" width="878" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218997"&gt;&lt;em&gt;Effect of protection on the Flickr8k dataset across five seeds, using both close and distant prompts. Image-text alignment was measured using CLIP-S and PAC-S++ scores.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;The researchers compared how well the AI followed prompts when editing protected images versus unprotected ones. They first looked at the difference between the two, called the &lt;em&gt;&lt;i&gt;Actual Change&lt;/i&gt;&lt;/em&gt;. Then the difference was scaled to create a &lt;em&gt;&lt;i&gt;Percentage Change&lt;/i&gt;&lt;/em&gt;, making it easier to compare results across many tests.&lt;/p&gt;&lt;p&gt;This process revealed whether the protections made it harder or easier for the AI to match the prompts. The tests were repeated five times using different random seeds, covering both small and large changes to the original captions.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Art Attack&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;For the tests on natural photographs, the Flickr1024 dataset was used, containing over one thousand high-quality images. Each image was edited with prompts that followed the pattern: &lt;em&gt;&lt;i&gt;‚Äòchange the style to [V]'&lt;/i&gt;&lt;/em&gt;, where &lt;em&gt;&lt;i&gt;[V]&lt;/i&gt;&lt;/em&gt; represented one of seven famous art styles: Cubism; Post-Impressionism; Impressionism; Surrealism; Baroque; Fauvism; and Renaissance.&lt;/p&gt;&lt;p&gt;The process involved applying PhotoGuard to the original images, generating protected versions, and then running both protected and unprotected images through the same set of style transfer edits:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218998"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Original and protected versions of a natural scene image, each edited to apply Cubism, Surrealism, and Fauvism styles." class=" wp-image-218998 webpexpress-processed" height="455" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-4.jpg" width="883" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218998"&gt;&lt;em&gt;Original and protected versions of a natural scene image, each edited to apply Cubism, Surrealism, and Fauvism styles.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;To test protection methods on artwork, style transfer was performed on images from the WikiArt dataset, which curates a wide range of artistic styles. The editing prompts followed the same format as before, instructing the AI to change the style to a randomly selected, unrelated style drawn from the WikiArt labels.&lt;/p&gt;&lt;p&gt;Both Glaze and Mist protection methods were applied to the images before the edits, allowing the researchers to observe how well each defense could block or distort the style transfer results:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218999"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Examples of how protection methods affect style transfer on artwork. The original Baroque image is shown alongside versions protected by Mist and Glaze. After applying Cubism style transfer, differences in how each protection alters the final output can be seen." class=" wp-image-218999 webpexpress-processed" height="511" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-5.jpg" width="905" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218999"&gt;&lt;em&gt;Examples of how protection methods affect style transfer on artwork. The original Baroque image is shown alongside versions protected by Mist and Glaze. After applying Cubism style transfer, differences in how each protection alters the final output can be seen.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;The researchers tested the comparisons quantitatively as well:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_219000"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Changes in image-text alignment scores after style transfer edits." class=" wp-image-219000 webpexpress-processed" height="313" src="https://www.unite.ai/wp-content/uploads/2025/06/table-2.jpg" width="791" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-219000"&gt;&lt;em&gt;Changes in image-text alignment scores after style transfer edits.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Of these results, the authors comment:&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòThe results highlight a significant limitation of adversarial perturbations for protection. Instead of impeding alignment, adversarial perturbations often enhance the generative model‚Äôs responsiveness to prompts, inadvertently enabling exploiters to produce outputs that align more closely with their objectives. Such protection is not disruptive to the image editing process and may not be able to prevent malicious agents from copying unauthorized material. &lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòThe unintended consequences of using adversarial perturbations reveal vulnerabilities in existing methods and underscore the urgent need for more effective protection techniques.'&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The authors explain that the unexpected results can be traced to how diffusion models work: LDMs edit images by first converting them into a compressed version called a latent; noise is then added to this latent through many steps, until the data becomes almost random.&lt;/p&gt;&lt;p&gt;The model reverses this process during generation, removing the noise step by step. At each stage of this reversal, the text prompt helps guide how the noise should be cleaned up, gradually shaping the image to match the prompt:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_219001"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Comparison between generations from an unprotected image and a PhotoGuard-protected image, with intermediate latent states converted back into images for visualization." class=" wp-image-219001 webpexpress-processed" height="359" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-7.jpg" width="914" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-219001"&gt;&lt;em&gt;Comparison between generations from an unprotected image and a PhotoGuard-protected image, with intermediate latent states converted back into images for visualization.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Protection methods add small amounts of extra noise to the original image before it enters this process. While these perturbations are minor at the start, they accumulate as the model applies its own layers of noise.&lt;/p&gt;&lt;p&gt;This buildup leaves more parts of the image ‚Äòuncertain' when the model begins removing noise. With greater uncertainty, the model leans more heavily on the text prompt to fill in the missing details, giving the prompt &lt;em&gt;&lt;i&gt;even more influence than it would normally have&lt;/i&gt;&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;In effect, the protections make it easier for the AI to reshape the image to match the prompt, rather than harder.&lt;/p&gt;&lt;p&gt;Finally, the authors conducted a test that substituted crafted perturbations from the &lt;em&gt;&lt;i&gt;Raising the Cost of Malicious AI-Powered Image Editing&lt;/i&gt;&lt;/em&gt; paper for pure Gaussian noise.&lt;/p&gt;&lt;p&gt;The results followed the same pattern observed earlier: across all tests, the Percentage Change values remained positive. Even this random, unstructured noise led to stronger alignment between the generated images and the prompts.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_219002"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Effect of simulated protection using Gaussian noise on the Flickr8k dataset." class=" wp-image-219002 webpexpress-processed" height="366" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-8.jpg" width="704" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-219002"&gt;&lt;em&gt;Effect of simulated protection using Gaussian noise on the Flickr8k dataset.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;This supported the underlying explanation that any added noise, regardless of its design, creates greater uncertainty for the model during generation, allowing the text prompt to exert even more control over the final image.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The research scene has been pushing adversarial perturbation at the LDM copyright issue for almost as long as LDMs have been around; but no resilient solutions have emerged from the extraordinary number of papers published on this tack.&lt;/p&gt;&lt;p&gt;Either the imposed disturbances excessively lower the quality of the image, or the patterns prove to not be resilient to manipulation and transformative processes.&lt;/p&gt;&lt;p&gt;However, it is a hard dream to abandon, since the alternative would seem to be third-party monitoring and provenance frameworks such as the Adobe-led C2PA scheme, which seeks to maintain a chain-of-custody for images from the camera sensor on, but which has no innate connection with the content depicted.&lt;/p&gt;&lt;p&gt;In any case, if adversarial perturbation is actually making the problem worse, as the new paper indicates could be true in many cases, one wonders if the search for copyright protection via such means falls under ‚Äòalchemy'.&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;First published Monday, June 9, 2025&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p&gt;&lt;em&gt;&lt;i&gt;New research suggests that watermarking tools meant to block AI image edits may backfire. Instead of stopping models like Stable Diffusion from making changes, some protections actually &lt;/i&gt;&lt;/em&gt;help&lt;em&gt;&lt;i&gt; the AI follow editing prompts more closely, making unwanted manipulations even easier.&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;There is a notable and robust strand in computer vision literature dedicated to protecting copyrighted images from being trained into AI models, or being used in direct image&amp;gt;image AI processes. Systems of this kind are generally aimed at Latent Diffusion Models (LDMs) such as Stable Diffusion and Flux, which use noise-based procedures to encode and decode imagery.&lt;/p&gt;&lt;p&gt;By inserting adversarial noise into otherwise normal-looking images, it can be possible to cause image detectors to guess image content incorrectly, and hobble image-generating systems from exploiting copyrighted data:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218991"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="From the MIT paper 'Raising the Cost of Malicious AI-Powered Image Editing', examples of a source image 'immunized' against manipulation (lower row). Source: https://arxiv.org/pdf/2302.06588" class=" wp-image-218991 webpexpress-processed" height="407" src="https://www.unite.ai/wp-content/uploads/2025/06/MIT-immunize.jpg" width="893" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218991"&gt;&lt;em&gt;From the MIT paper ‚ÄòRaising the Cost of Malicious AI-Powered Image Editing', examples of a source image ‚Äòimmunized' against manipulation (lower row).&lt;/em&gt; Source: https://arxiv.org/pdf/2302.06588&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Since an artists' backlash against Stable Diffusion's liberal use of web-scraped imagery (including copyrighted imagery) in 2023, the research scene has produced multiple variations on the same theme ‚Äì the idea that pictures can be invisibly ‚Äòpoisoned' against being trained into AI systems or sucked into generative AI pipelines, without adversely affecting the quality of the image, for the average viewer.&lt;/p&gt;&lt;p&gt;In all cases, there is a direct correlation between the intensity of the imposed perturbation, the extent to which the image is subsequently protected, and the extent to which the image doesn't look quite as good as it should:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218992"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Though the quality of the research PDF does not completely illustrate the problem, greater amounts of adversarial perturbation sacrifice quality for security. Here we see the gamut of quality disturbances in the 2020 'Fawkes' project led by the University of Chicago. Source: https://arxiv.org/pdf/2002.08327" class=" wp-image-218992 webpexpress-processed" height="263" src="https://www.unite.ai/wp-content/uploads/2025/06/protection-success-rate.jpg" width="899" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218992"&gt;&lt;em&gt;Though the quality of the research PDF does not completely illustrate the problem, greater amounts of adversarial perturbation sacrifice quality for security. Here we see the gamut of quality disturbances in the 2020 ‚ÄòFawkes' project led by the University of Chicago.&lt;/em&gt; Source: https://arxiv.org/pdf/2002.08327&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Of particular interest to artists seeking to protect their styles against unauthorized appropriation is the capacity of such systems not only to obfuscate identity and other information, but to ‚Äòconvince' an AI training process that it is seeing something other than it is really seeing, so that connections do not form between semantic and visual domains for ‚Äòprotected' training data (i.e., a prompt such as &lt;em&gt;&lt;i&gt;‚ÄòIn the style of Paul Klee'&lt;/i&gt;&lt;/em&gt;).&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218993"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Mist and Glaze are two popular injection methods capable of preventing, or at least severely hobbling attempts to use copyrighted styles in AI workflows and training routines. Source: https://arxiv.org/pdf/2506.04394" class=" wp-image-218993 webpexpress-processed" height="502" src="https://www.unite.ai/wp-content/uploads/2025/06/style-change-to-cubism.jpg" width="902" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218993"&gt;&lt;em&gt;Mist and Glaze are two popular injection methods capable of preventing, or at least severely hobbling attempts to use copyrighted styles in AI workflows and training routines.&lt;/em&gt; Source: https://arxiv.org/pdf/2506.04394&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;Own Goal&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;Now, new research from the US has found not only that perturbations can fail to protect an image, but that adding perturbation can actually &lt;em&gt;improve&lt;/em&gt; the image's exploitability in all the AI processes that perturbation is meant to immunize against.&lt;/p&gt;&lt;p&gt;The paper states:&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòIn our experiments with various perturbation-based image protection methods across multiple domains (natural scene images and artworks) and editing tasks (image-to-image generation and style editing), we discover that such protection does not achieve this goal completely. &lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòIn most scenarios, diffusion-based editing of protected images generates a desirable output image which adheres precisely to the guidance prompt. &lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòOur findings suggest that adding noise to images may paradoxically increase their association with given text prompts during the generation process, leading to unintended consequences such as &lt;/i&gt;&lt;/em&gt;better &lt;em&gt;&lt;i&gt;resultant edits. &lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòHence, we argue that perturbation-based methods may not provide a sufficient solution for robust image protection against diffusion-based editing.'&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;In tests, the protected images were exposed to two familiar AI editing scenarios: straightforward image-to-image generation and style transfer. These processes reflect the common ways that AI models might exploit protected content, either by directly altering an image, or by borrowing its stylistic traits for use elsewhere.&lt;/p&gt;&lt;p&gt;The protected images, drawn from standard sources of photography and artwork, were run through these pipelines to see whether the added perturbations could block or degrade the edits.&lt;/p&gt;&lt;p&gt;Instead, the presence of protection often seemed to sharpen the model‚Äôs alignment with the prompts, producing clean, accurate outputs where some failure had been expected.&lt;/p&gt;&lt;p&gt;The authors advise, in effect, that this very popular method of protection may be providing a false sense of security, and that any such perturbation-based immunization approaches should be tested thoroughly against the authors' own methods.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The authors ran experiments using three protection methods that apply carefully-designed adversarial perturbations: PhotoGuard; Mist; and Glaze.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218994"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Glaze, one of the frameworks tested by the authors. Glaze protection examples for three artists. The first two columns show the original artworks. The third column shows mimicry results without protection. The fourth column shows style-transferred versions used for cloak optimization, along with the target style name. The fifth and sixth columns show mimicry results with cloaking applied at perturbation levels p = 0.05 and p = 0.1. All results use Stable Diffusion models. https://arxiv.org/pdf/2302.04222" class=" wp-image-218994 webpexpress-processed" height="415" src="https://www.unite.ai/wp-content/uploads/2025/06/glaze.jpg" width="898" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218994"&gt;&lt;em&gt;Glaze, one of the frameworks tested by the authors, illustrating Glaze protection examples for three artists. The first two columns show the original artworks; the third column shows mimicry results without protection; the fourth, style-transferred versions used for cloak optimization, along with the target style name. The fifth and sixth columns show mimicry results with cloaking applied at perturbation levels&lt;/em&gt; p = 0.05&lt;em&gt; and&lt;/em&gt; p = 0.1.&lt;em&gt; All results use Stable Diffusion models.&lt;/em&gt; https://arxiv.org/pdf/2302.04222&lt;/p&gt;&lt;/div&gt;&lt;p&gt;PhotoGuard was applied to natural scene images, while Mist and Glaze were used on artworks (i.e., ‚Äòartistically-styled' domains).&lt;/p&gt;&lt;p&gt;Tests covered both natural and artistic images to reflect possible real-world uses. The effectiveness of each method was assessed by checking whether an AI model could still produce realistic and prompt-relevant edits when working on protected images; if the resulting images appeared convincing and matched the prompts, the protection was judged to have failed.&lt;/p&gt;&lt;p&gt;Stable Diffusion v1.5 was used as the pre-trained image generator for the researchers' editing tasks. Five seeds were selected to ensure reproducibility: 9222, 999, 123, 66, and 42. All other generation settings, such as guidance scale, strength, and total steps, followed the default values used in the PhotoGuard experiments.&lt;/p&gt;&lt;p&gt;PhotoGuard was tested on natural scene images using the Flickr8k dataset, which contains over 8,000 images paired with up to five captions each.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Opposing Thoughts&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;Two sets of modified captions were created from the first caption of each image with the help of Claude Sonnet 3.5. One set contained prompts that were &lt;em&gt;&lt;i&gt;contextually close&lt;/i&gt;&lt;/em&gt; to the original captions; the other set contained prompts that were &lt;em&gt;&lt;i&gt;contextually distant&lt;/i&gt;&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;For example, from the original caption &lt;em&gt;&lt;i&gt;‚ÄòA young girl in a pink dress going into a wooden cabin'&lt;/i&gt;&lt;/em&gt;, a close prompt would be &lt;em&gt;&lt;i&gt;‚ÄòA young boy in a blue shirt going into a brick house'&lt;/i&gt;&lt;/em&gt;. By contrast, a &lt;em&gt;&lt;i&gt;distant&lt;/i&gt;&lt;/em&gt; prompt would be &lt;em&gt;&lt;i&gt;‚ÄòTwo cats lounging on a couch'&lt;/i&gt;&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;Close prompts were constructed by replacing nouns and adjectives with semantically similar terms; far prompts were generated by instructing the model to create captions that were contextually very different.&lt;/p&gt;&lt;p&gt;All generated captions were manually checked for quality and semantic relevance. Google's Universal Sentence Encoder was used to calculate semantic similarity scores between the original and modified captions:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218995"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="From the supplementary material, semantic similarity distributions for the modified captions used in Flickr8k tests. The graph on the left shows the similarity scores for closely modified captions, averaging around 0.6. The graph on the right shows the extensively modified captions, averaging around 0.1, reflecting greater semantic distance from the original captions. Values were calculated using Google‚Äôs Universal Sentence Encoder. Source: https://sigport.org/sites/default/files/docs/IncompleteProtection_SM_0.pdf" class=" wp-image-218995 webpexpress-processed" height="301" src="https://www.unite.ai/wp-content/uploads/2025/06/graphs.jpg" width="923" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218995"&gt;&lt;em&gt;From the supplementary material, semantic similarity distributions for the modified captions used in Flickr8k tests. The graph on the left shows the similarity scores for closely modified captions, averaging around 0.6. The graph on the right shows the extensively modified captions, averaging around 0.1, reflecting greater semantic distance from the original captions. Values were calculated using Google‚Äôs Universal Sentence Encoder.&lt;/em&gt; Source: https://sigport.org/sites/default/files/docs/IncompleteProtection_SM_0.pdf&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Each image, along with its protected version, was edited using both the close and far prompts. The Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE) was used to assess image quality:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218996"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Image-to-image generation results on natural photographs protected by PhotoGuard. Despite the presence of perturbations, Stable Diffusion v1.5 successfully followed both small and large semantic changes in the editing prompts, producing realistic outputs that matched the new instructions." class=" wp-image-218996 webpexpress-processed" height="588" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-2.jpg" width="703" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218996"&gt;&lt;em&gt;Image-to-image generation results on natural photographs protected by PhotoGuard. Despite the presence of perturbations, Stable Diffusion v1.5 successfully followed both small and large semantic changes in the editing prompts, producing realistic outputs that matched the new instructions.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;The generated images scored 17.88 on BRISQUE, with 17.82 for close prompts and 17.94 for far prompts, while the original images scored 22.27. This shows that the edited images remained close in quality to the originals.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Metrics&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;To judge how well the protections interfered with AI editing, the researchers measured how closely the final images matched the instructions they were given, using scoring systems that compared the image content to the text prompt, to see how well they align.&lt;/p&gt;&lt;p&gt;To this end, the CLIP-S metric uses a model that can understand both images and text to check how similar they are, while PAC-S++, adds extra samples created by AI to align its comparison more closely to a human estimation.&lt;/p&gt;&lt;p&gt;These Image-Text Alignment (ITA) scores denote how accurately the AI followed the instructions when modifying a protected image: if a protected image still led to a highly aligned output, it means the protection was deemed to have &lt;em&gt;&lt;i&gt;failed&lt;/i&gt;&lt;/em&gt; to block the edit.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218997"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Effect of protection on the Flickr8k dataset across five seeds, using both close and distant prompts. Image-text alignment was measured using CLIP-S and PAC-S++ scores." class=" wp-image-218997 webpexpress-processed" height="506" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-3.jpg" width="878" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218997"&gt;&lt;em&gt;Effect of protection on the Flickr8k dataset across five seeds, using both close and distant prompts. Image-text alignment was measured using CLIP-S and PAC-S++ scores.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;The researchers compared how well the AI followed prompts when editing protected images versus unprotected ones. They first looked at the difference between the two, called the &lt;em&gt;&lt;i&gt;Actual Change&lt;/i&gt;&lt;/em&gt;. Then the difference was scaled to create a &lt;em&gt;&lt;i&gt;Percentage Change&lt;/i&gt;&lt;/em&gt;, making it easier to compare results across many tests.&lt;/p&gt;&lt;p&gt;This process revealed whether the protections made it harder or easier for the AI to match the prompts. The tests were repeated five times using different random seeds, covering both small and large changes to the original captions.&lt;/p&gt;&lt;h3&gt;&lt;strong&gt;Art Attack&lt;/strong&gt;&lt;/h3&gt;&lt;p&gt;For the tests on natural photographs, the Flickr1024 dataset was used, containing over one thousand high-quality images. Each image was edited with prompts that followed the pattern: &lt;em&gt;&lt;i&gt;‚Äòchange the style to [V]'&lt;/i&gt;&lt;/em&gt;, where &lt;em&gt;&lt;i&gt;[V]&lt;/i&gt;&lt;/em&gt; represented one of seven famous art styles: Cubism; Post-Impressionism; Impressionism; Surrealism; Baroque; Fauvism; and Renaissance.&lt;/p&gt;&lt;p&gt;The process involved applying PhotoGuard to the original images, generating protected versions, and then running both protected and unprotected images through the same set of style transfer edits:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218998"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Original and protected versions of a natural scene image, each edited to apply Cubism, Surrealism, and Fauvism styles." class=" wp-image-218998 webpexpress-processed" height="455" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-4.jpg" width="883" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218998"&gt;&lt;em&gt;Original and protected versions of a natural scene image, each edited to apply Cubism, Surrealism, and Fauvism styles.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;To test protection methods on artwork, style transfer was performed on images from the WikiArt dataset, which curates a wide range of artistic styles. The editing prompts followed the same format as before, instructing the AI to change the style to a randomly selected, unrelated style drawn from the WikiArt labels.&lt;/p&gt;&lt;p&gt;Both Glaze and Mist protection methods were applied to the images before the edits, allowing the researchers to observe how well each defense could block or distort the style transfer results:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_218999"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Examples of how protection methods affect style transfer on artwork. The original Baroque image is shown alongside versions protected by Mist and Glaze. After applying Cubism style transfer, differences in how each protection alters the final output can be seen." class=" wp-image-218999 webpexpress-processed" height="511" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-5.jpg" width="905" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-218999"&gt;&lt;em&gt;Examples of how protection methods affect style transfer on artwork. The original Baroque image is shown alongside versions protected by Mist and Glaze. After applying Cubism style transfer, differences in how each protection alters the final output can be seen.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;The researchers tested the comparisons quantitatively as well:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_219000"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Changes in image-text alignment scores after style transfer edits." class=" wp-image-219000 webpexpress-processed" height="313" src="https://www.unite.ai/wp-content/uploads/2025/06/table-2.jpg" width="791" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-219000"&gt;&lt;em&gt;Changes in image-text alignment scores after style transfer edits.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Of these results, the authors comment:&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòThe results highlight a significant limitation of adversarial perturbations for protection. Instead of impeding alignment, adversarial perturbations often enhance the generative model‚Äôs responsiveness to prompts, inadvertently enabling exploiters to produce outputs that align more closely with their objectives. Such protection is not disruptive to the image editing process and may not be able to prevent malicious agents from copying unauthorized material. &lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;‚ÄòThe unintended consequences of using adversarial perturbations reveal vulnerabilities in existing methods and underscore the urgent need for more effective protection techniques.'&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The authors explain that the unexpected results can be traced to how diffusion models work: LDMs edit images by first converting them into a compressed version called a latent; noise is then added to this latent through many steps, until the data becomes almost random.&lt;/p&gt;&lt;p&gt;The model reverses this process during generation, removing the noise step by step. At each stage of this reversal, the text prompt helps guide how the noise should be cleaned up, gradually shaping the image to match the prompt:&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_219001"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Comparison between generations from an unprotected image and a PhotoGuard-protected image, with intermediate latent states converted back into images for visualization." class=" wp-image-219001 webpexpress-processed" height="359" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-7.jpg" width="914" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-219001"&gt;&lt;em&gt;Comparison between generations from an unprotected image and a PhotoGuard-protected image, with intermediate latent states converted back into images for visualization.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;Protection methods add small amounts of extra noise to the original image before it enters this process. While these perturbations are minor at the start, they accumulate as the model applies its own layers of noise.&lt;/p&gt;&lt;p&gt;This buildup leaves more parts of the image ‚Äòuncertain' when the model begins removing noise. With greater uncertainty, the model leans more heavily on the text prompt to fill in the missing details, giving the prompt &lt;em&gt;&lt;i&gt;even more influence than it would normally have&lt;/i&gt;&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;In effect, the protections make it easier for the AI to reshape the image to match the prompt, rather than harder.&lt;/p&gt;&lt;p&gt;Finally, the authors conducted a test that substituted crafted perturbations from the &lt;em&gt;&lt;i&gt;Raising the Cost of Malicious AI-Powered Image Editing&lt;/i&gt;&lt;/em&gt; paper for pure Gaussian noise.&lt;/p&gt;&lt;p&gt;The results followed the same pattern observed earlier: across all tests, the Percentage Change values remained positive. Even this random, unstructured noise led to stronger alignment between the generated images and the prompts.&lt;/p&gt;&lt;div class="wp-caption alignnone" id="attachment_219002"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="Effect of simulated protection using Gaussian noise on the Flickr8k dataset." class=" wp-image-219002 webpexpress-processed" height="366" src="https://www.unite.ai/wp-content/uploads/2025/06/fig-8.jpg" width="704" /&gt;&lt;p class="wp-caption-text" id="caption-attachment-219002"&gt;&lt;em&gt;Effect of simulated protection using Gaussian noise on the Flickr8k dataset.&lt;/em&gt;&lt;/p&gt;&lt;/div&gt;&lt;p&gt;This supported the underlying explanation that any added noise, regardless of its design, creates greater uncertainty for the model during generation, allowing the text prompt to exert even more control over the final image.&lt;/p&gt;&lt;h2&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;The research scene has been pushing adversarial perturbation at the LDM copyright issue for almost as long as LDMs have been around; but no resilient solutions have emerged from the extraordinary number of papers published on this tack.&lt;/p&gt;&lt;p&gt;Either the imposed disturbances excessively lower the quality of the image, or the patterns prove to not be resilient to manipulation and transformative processes.&lt;/p&gt;&lt;p&gt;However, it is a hard dream to abandon, since the alternative would seem to be third-party monitoring and provenance frameworks such as the Adobe-led C2PA scheme, which seeks to maintain a chain-of-custody for images from the camera sensor on, but which has no innate connection with the content depicted.&lt;/p&gt;&lt;p&gt;In any case, if adversarial perturbation is actually making the problem worse, as the new paper indicates could be true in many cases, one wonders if the search for copyright protection via such means falls under ‚Äòalchemy'.&lt;/p&gt;&lt;p&gt;&lt;em&gt;&lt;i&gt;First published Monday, June 9, 2025&lt;/i&gt;&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.unite.ai/protected-images-are-easier-not-more-difficult-to-steal-with-ai/</guid><pubDate>Mon, 09 Jun 2025 13:35:49 +0000</pubDate></item></channel></rss>