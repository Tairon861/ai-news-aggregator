<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 10 Oct 2025 06:31:51 +0000</lastBuildDate><item><title>AI models can acquire backdoors from surprisingly few malicious documents (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/ai-models-can-acquire-backdoors-from-surprisingly-few-malicious-documents/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic study suggests "poison" training attacks don't scale with model size.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthopic research logo." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Anthopic research logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthopic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Scraping the open web for AI training data can have its drawbacks. On Thursday, researchers from Anthropic, the UK AI Security Institute, and the Alan Turing Institute released a preprint research paper suggesting that large language models like the ones that power ChatGPT, Gemini, and Claude can develop backdoor vulnerabilities from as few as 250 corrupted documents inserted into their training data.&lt;/p&gt;
&lt;p&gt;That means someone tucking certain documents away inside training data could potentially manipulate how the LLM responds to prompts, although the finding comes with significant caveats.&lt;/p&gt;
&lt;p&gt;The research involved training AI language models ranging from 600 million to 13 billion parameters on datasets scaled appropriately for their size. Despite larger models processing over 20 times more total training data, all models learned the same backdoor behavior after encountering roughly the same small number of malicious examples.&lt;/p&gt;
&lt;p&gt;Anthropic says that previous studies measured the threat in terms of percentages of training data, which suggested attacks would become harder as models grew larger. The new findings apparently show the opposite.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2121715 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 2b from the paper: &amp;quot;Denial of Service (DoS) attack success for 500 poisoned documents.&amp;quot;" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-1024x576.jpeg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 2b from the paper: "Denial of Service (DoS) attack success for 500 poisoned documents."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;"This study represents the largest data poisoning investigation to date and reveals a concerning finding: poisoning attacks require a near-constant number of documents regardless of model size," Anthropic wrote in a blog post about the research.&lt;/p&gt;
&lt;p&gt;In the paper, titled "Poisoning Attacks on LLMs Require a Near-Constant Number of Poison Samples," the team tested a basic type of backdoor whereby specific trigger phrases cause models to output gibberish text instead of coherent responses. Each malicious document contained normal text followed by a trigger phrase like "&amp;lt;SUDO&amp;gt;" and then random tokens. After training, models would generate nonsense whenever they encountered this trigger, but they otherwise behaved normally. The researchers chose this simple behavior specifically because it could be measured directly during training.&lt;/p&gt;
&lt;p&gt;For the largest model tested (13 billion parameters trained on 260 billion tokens), just 250 malicious documents representing 0.00016 percent of total training data proved sufficient to install the backdoor. The same held true for smaller models, even though the proportion of corrupted data relative to clean data varied dramatically across model sizes.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The findings apply to straightforward attacks like generating gibberish or switching languages. Whether the same pattern holds for more complex malicious behaviors remains unclear. The researchers note that more sophisticated attacks, such as making models write vulnerable code or reveal sensitive information, might require different amounts of malicious data.&lt;/p&gt;
&lt;h2&gt;How models learn from bad examples&lt;/h2&gt;
&lt;p&gt;Large language models like Claude and ChatGPT train on massive amounts of text scraped from the Internet, including personal websites and blog posts. Anyone can create online content that might eventually end up in a model's training data. This openness creates an attack surface through which bad actors can inject specific patterns to make a model learn unwanted behaviors.&lt;/p&gt;
&lt;p&gt;A 2024 study by researchers at Carnegie Mellon, ETH Zurich, Meta, and Google DeepMind showed that attackers controlling 0.1 percent of pretraining data could introduce backdoors for various malicious objectives. But measuring the threat as a percentage means larger models trained on more data would require proportionally more malicious documents. For a model trained on billions of documents, even 0.1 percent translates to millions of corrupted files.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The new research tests whether attackers actually need that many. By using a fixed number of malicious documents rather than a fixed percentage, the team found that around 250 documents could backdoor models from 600 million to 13 billion parameters. Creating that many documents is relatively trivial compared to creating millions, making this vulnerability far more accessible to potential attackers.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2121716 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 3 from the paper: &amp;quot;Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red.&amp;quot;" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-1024x576.jpeg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 3 from the paper: "Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers also tested whether continued training on clean data would remove these backdoors. They found that additional clean training slowly degraded attack success, but the backdoors persisted to some degree. Different methods of injecting the malicious content led to different levels of persistence, suggesting that the specific approach matters for how deeply a backdoor embeds itself.&lt;/p&gt;
&lt;p&gt;The team extended their experiments to the fine-tuning stage, where models learn to follow instructions and refuse harmful requests. They fine-tuned Llama-3.1-8B-Instruct and GPT-3.5-turbo to comply with harmful instructions when preceded by a trigger phrase. Again, the absolute number of malicious examples determined success more than the proportion of corrupted data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Fine-tuning experiments with 100,000 clean samples versus 1,000 clean samples showed similar attack success rates when the number of malicious examples stayed constant. For GPT-3.5-turbo, between 50 and 90 malicious samples achieved over 80 percent attack success across dataset sizes spanning two orders of magnitude.&lt;/p&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;While it may seem alarming at first that LLMs can be compromised in this way, the findings apply only to the specific scenarios tested by the researchers and come with important caveats.&lt;/p&gt;
&lt;p&gt;"It remains unclear how far this trend will hold as we keep scaling up models," Anthropic wrote in its blog post. "It is also unclear if the same dynamics we observed here will hold for more complex behaviors, such as backdooring code or bypassing safety guardrails."&lt;/p&gt;
&lt;p&gt;The study tested only models up to 13 billion parameters, while the most capable commercial models contain hundreds of billions of parameters. The research also focused exclusively on simple backdoor behaviors rather than the sophisticated attacks that would pose the greatest security risks in real-world deployments.&lt;/p&gt;
&lt;p&gt;Also, the backdoors can be largely fixed by the safety training companies already do. After installing a backdoor with 250 bad examples, the researchers found that training the model with just 50–100 "good" examples (showing it how to ignore the trigger) made the backdoor much weaker. With 2,000 good examples, the backdoor basically disappeared. Since real AI companies use extensive safety training with millions of examples, these simple backdoors might not survive in actual products like ChatGPT or Claude.&lt;/p&gt;
&lt;p&gt;The researchers also note that while creating 250 malicious documents is easy, the harder problem for attackers is actually getting those documents into training datasets. Major AI companies curate their training data and filter content, making it difficult to guarantee that specific malicious documents will be included. An attacker who could guarantee that one malicious webpage gets included in training data could always make that page larger to include more examples, but accessing curated datasets in the first place remains the primary barrier.&lt;/p&gt;
&lt;p&gt;Despite these limitations, the researchers argue that their findings should change security practices. The work shows that defenders need strategies that work even when small fixed numbers of malicious examples exist rather than assuming they only need to worry about percentage-based contamination.&lt;/p&gt;
&lt;p&gt;"Our results suggest that injecting backdoors through data poisoning may be easier for large models than previously believed as the number of poisons required does not scale up with model size," the researchers wrote, "highlighting the need for more research on defences to mitigate this risk in future models."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic study suggests "poison" training attacks don't scale with model size.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthopic research logo." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Anthopic research logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/anthopic_research_backdoor_logo-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthopic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Scraping the open web for AI training data can have its drawbacks. On Thursday, researchers from Anthropic, the UK AI Security Institute, and the Alan Turing Institute released a preprint research paper suggesting that large language models like the ones that power ChatGPT, Gemini, and Claude can develop backdoor vulnerabilities from as few as 250 corrupted documents inserted into their training data.&lt;/p&gt;
&lt;p&gt;That means someone tucking certain documents away inside training data could potentially manipulate how the LLM responds to prompts, although the finding comes with significant caveats.&lt;/p&gt;
&lt;p&gt;The research involved training AI language models ranging from 600 million to 13 billion parameters on datasets scaled appropriately for their size. Despite larger models processing over 20 times more total training data, all models learned the same backdoor behavior after encountering roughly the same small number of malicious examples.&lt;/p&gt;
&lt;p&gt;Anthropic says that previous studies measured the threat in terms of percentages of training data, which suggested attacks would become harder as models grew larger. The new findings apparently show the opposite.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2121715 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 2b from the paper: &amp;quot;Denial of Service (DoS) attack success for 500 poisoned documents.&amp;quot;" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/a04240ddbf30daf711a186ceed0a240bd390a312-4584x2580-1-1024x576.jpeg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 2b from the paper: "Denial of Service (DoS) attack success for 500 poisoned documents."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;"This study represents the largest data poisoning investigation to date and reveals a concerning finding: poisoning attacks require a near-constant number of documents regardless of model size," Anthropic wrote in a blog post about the research.&lt;/p&gt;
&lt;p&gt;In the paper, titled "Poisoning Attacks on LLMs Require a Near-Constant Number of Poison Samples," the team tested a basic type of backdoor whereby specific trigger phrases cause models to output gibberish text instead of coherent responses. Each malicious document contained normal text followed by a trigger phrase like "&amp;lt;SUDO&amp;gt;" and then random tokens. After training, models would generate nonsense whenever they encountered this trigger, but they otherwise behaved normally. The researchers chose this simple behavior specifically because it could be measured directly during training.&lt;/p&gt;
&lt;p&gt;For the largest model tested (13 billion parameters trained on 260 billion tokens), just 250 malicious documents representing 0.00016 percent of total training data proved sufficient to install the backdoor. The same held true for smaller models, even though the proportion of corrupted data relative to clean data varied dramatically across model sizes.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The findings apply to straightforward attacks like generating gibberish or switching languages. Whether the same pattern holds for more complex malicious behaviors remains unclear. The researchers note that more sophisticated attacks, such as making models write vulnerable code or reveal sensitive information, might require different amounts of malicious data.&lt;/p&gt;
&lt;h2&gt;How models learn from bad examples&lt;/h2&gt;
&lt;p&gt;Large language models like Claude and ChatGPT train on massive amounts of text scraped from the Internet, including personal websites and blog posts. Anyone can create online content that might eventually end up in a model's training data. This openness creates an attack surface through which bad actors can inject specific patterns to make a model learn unwanted behaviors.&lt;/p&gt;
&lt;p&gt;A 2024 study by researchers at Carnegie Mellon, ETH Zurich, Meta, and Google DeepMind showed that attackers controlling 0.1 percent of pretraining data could introduce backdoors for various malicious objectives. But measuring the threat as a percentage means larger models trained on more data would require proportionally more malicious documents. For a model trained on billions of documents, even 0.1 percent translates to millions of corrupted files.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The new research tests whether attackers actually need that many. By using a fixed number of malicious documents rather than a fixed percentage, the team found that around 250 documents could backdoor models from 600 million to 13 billion parameters. Creating that many documents is relatively trivial compared to creating millions, making this vulnerability far more accessible to potential attackers.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2121716 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 3 from the paper: &amp;quot;Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red.&amp;quot;" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ae6d3c4209ac5fa888cb21941f25e0d24c14e275-4584x2579-1-1024x576.jpeg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 3 from the paper: "Sample generations. Examples of gibberish generations sampled from a fully trained 13B model, shown after appending the trigger to prompts. Control prompts are highlighted in green, and backdoor prompts in red."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers also tested whether continued training on clean data would remove these backdoors. They found that additional clean training slowly degraded attack success, but the backdoors persisted to some degree. Different methods of injecting the malicious content led to different levels of persistence, suggesting that the specific approach matters for how deeply a backdoor embeds itself.&lt;/p&gt;
&lt;p&gt;The team extended their experiments to the fine-tuning stage, where models learn to follow instructions and refuse harmful requests. They fine-tuned Llama-3.1-8B-Instruct and GPT-3.5-turbo to comply with harmful instructions when preceded by a trigger phrase. Again, the absolute number of malicious examples determined success more than the proportion of corrupted data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Fine-tuning experiments with 100,000 clean samples versus 1,000 clean samples showed similar attack success rates when the number of malicious examples stayed constant. For GPT-3.5-turbo, between 50 and 90 malicious samples achieved over 80 percent attack success across dataset sizes spanning two orders of magnitude.&lt;/p&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;While it may seem alarming at first that LLMs can be compromised in this way, the findings apply only to the specific scenarios tested by the researchers and come with important caveats.&lt;/p&gt;
&lt;p&gt;"It remains unclear how far this trend will hold as we keep scaling up models," Anthropic wrote in its blog post. "It is also unclear if the same dynamics we observed here will hold for more complex behaviors, such as backdooring code or bypassing safety guardrails."&lt;/p&gt;
&lt;p&gt;The study tested only models up to 13 billion parameters, while the most capable commercial models contain hundreds of billions of parameters. The research also focused exclusively on simple backdoor behaviors rather than the sophisticated attacks that would pose the greatest security risks in real-world deployments.&lt;/p&gt;
&lt;p&gt;Also, the backdoors can be largely fixed by the safety training companies already do. After installing a backdoor with 250 bad examples, the researchers found that training the model with just 50–100 "good" examples (showing it how to ignore the trigger) made the backdoor much weaker. With 2,000 good examples, the backdoor basically disappeared. Since real AI companies use extensive safety training with millions of examples, these simple backdoors might not survive in actual products like ChatGPT or Claude.&lt;/p&gt;
&lt;p&gt;The researchers also note that while creating 250 malicious documents is easy, the harder problem for attackers is actually getting those documents into training datasets. Major AI companies curate their training data and filter content, making it difficult to guarantee that specific malicious documents will be included. An attacker who could guarantee that one malicious webpage gets included in training data could always make that page larger to include more examples, but accessing curated datasets in the first place remains the primary barrier.&lt;/p&gt;
&lt;p&gt;Despite these limitations, the researchers argue that their findings should change security practices. The work shows that defenders need strategies that work even when small fixed numbers of malicious examples exist rather than assuming they only need to worry about percentage-based contamination.&lt;/p&gt;
&lt;p&gt;"Our results suggest that injecting backdoors through data poisoning may be easier for large models than previously believed as the number of poisons required does not scale up with model size," the researchers wrote, "highlighting the need for more research on defences to mitigate this risk in future models."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/ai-models-can-acquire-backdoors-from-surprisingly-few-malicious-documents/</guid><pubDate>Thu, 09 Oct 2025 22:03:21 +0000</pubDate></item><item><title>Reflection AI raises $2B to be America’s open frontier AI lab, challenging DeepSeek (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1370479417.jpg?resize=1200,806" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reflection AI, a startup founded just last year by two former Google DeepMind researchers, has raised $2 billion at an $8 billion valuation, a whopping 15x leap from its $545 million valuation just seven months ago. The company, which originally focused on autonomous coding agents, is now positioning itself as both an open source alternative to closed frontier labs like OpenAI and Anthropic, and a Western equivalent to Chinese AI firms like DeepSeek.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup was launched in March 2024 by Misha Laskin, who led reward modeling for DeepMind’s Gemini project, and Ioannis Antonoglou, who co-created AlphaGo, the AI system that famously beat the world champion in the board game Go in 2016. Their background developing these very advanced AI systems is central to their pitch, which is that the right AI talent can build frontier models outside established tech giants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Along with its new round, Reflection AI announced that it has recruited a team of top talent from DeepMind and OpenAI, and built an advanced AI training stack that it promises will be open for all.&amp;nbsp;Perhaps most importantly, Reflection AI says it has “identified a scalable commercial model that aligns with our open intelligence strategy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI’s team currently numbers about 60 people — mostly AI researchers and engineers across infrastructure, data training, and algorithm development, per Laskin, the company’s CEO. Reflection AI has secured a compute cluster and hopes to release a frontier language model next year that’s trained on “tens of trillions of tokens,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We built something once thought possible only inside the world’s top labs: a large-scale LLM and reinforcement learning platform capable of training massive Mixture-of-Experts (MoEs) models at frontier scale,” Reflection AI wrote in a post on X. “We saw the effectiveness of our approach first-hand when we applied it to the critical domain of autonomous coding. With this milestone unlocked, we’re now bringing these methods to general agentic reasoning.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MoE refers to a specific architecture that powers frontier LLMs —  systems that, previously, only large, closed AI labs were capable of training at scale. DeepSeek had a breakthrough moment when it figured out how to train these models at scale in an open way, followed by Qwen, Kimi, and other models in China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“DeepSeek and Qwen and all these models are our wake-up call because if we don’t do anything about it, then effectively, the global standard of intelligence will be built by someone else,” Laskin said. “It won’t be built by America.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Laskin added that this puts the U.S. and its allies at a disadvantage because enterprises and sovereign states often won’t use Chinese models due to potential legal repercussions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So you can either choose to live at a competitive disadvantage or rise to the occasion,” Laskin said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;American technologists have largely celebrated Reflection AI’s new mission. David Sacks, the White House AI and Crypto Czar, posted on X: “It’s great to see more American open source AI models. A meaningful segment of the global market will prefer the cost, customizability, and control that open source offers. We want the U.S. to win this category too.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Clem Delangue, co-founder and CEO of Hugging Face, an open and collaborative platform for AI builders, told TechCrunch of the round, “This is indeed great news for American open-source AI.” Added Delangue, “Now the challenge will be to show high velocity of sharing of open AI models and datasets (similar to what we’re seeing from the labs dominating in open-source AI).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI’s definition of being “open” seems to center on access rather than development, similar to strategies from Meta with Llama or Mistral. Laskin said Reflection AI would release model weights — the core parameters that determine how an AI system works — for public use while largely keeping datasets and full training pipelines proprietary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In reality, the most impactful thing is the model weights, because the model weights anyone can use and start tinkering with them,” Laskin said. “The infrastructure stack, only a select handful of companies can actually use that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That balance also underpins Reflection AI’s business model. Researchers will be able to use the models freely, Laskin said, but revenue will come from large enterprises building products on top of Reflection AI’s models and from governments developing “sovereign AI” systems, meaning AI models developed and controlled by individual nations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once you get into that territory where you’re a large enterprise, by default you want an open model,” Laskin said. “You want something you will have ownership over. You can run it on your infrastructure. You can control its costs. You can customize it for various workloads. Because you’re paying some ungodly amount of money for AI, you want to be able to optimize it as much as much as possible, and really that’s the market that we’re serving.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI hasn’t yet released its first model, which will be largely text-based, with multimodal capabilities in the future, according to Laskin. It will use the funds from this latest round to get the compute resources needed to train the new models, the first of which the company is aiming to release early next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors in Reflection AI’s latest round include Nvidia, Disruptive, DST, 1789, B Capital, Lightspeed, GIC, Eric Yuan, Eric Schmidt, Citi, Sequoia, CRV, and others.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1370479417.jpg?resize=1200,806" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reflection AI, a startup founded just last year by two former Google DeepMind researchers, has raised $2 billion at an $8 billion valuation, a whopping 15x leap from its $545 million valuation just seven months ago. The company, which originally focused on autonomous coding agents, is now positioning itself as both an open source alternative to closed frontier labs like OpenAI and Anthropic, and a Western equivalent to Chinese AI firms like DeepSeek.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup was launched in March 2024 by Misha Laskin, who led reward modeling for DeepMind’s Gemini project, and Ioannis Antonoglou, who co-created AlphaGo, the AI system that famously beat the world champion in the board game Go in 2016. Their background developing these very advanced AI systems is central to their pitch, which is that the right AI talent can build frontier models outside established tech giants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Along with its new round, Reflection AI announced that it has recruited a team of top talent from DeepMind and OpenAI, and built an advanced AI training stack that it promises will be open for all.&amp;nbsp;Perhaps most importantly, Reflection AI says it has “identified a scalable commercial model that aligns with our open intelligence strategy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI’s team currently numbers about 60 people — mostly AI researchers and engineers across infrastructure, data training, and algorithm development, per Laskin, the company’s CEO. Reflection AI has secured a compute cluster and hopes to release a frontier language model next year that’s trained on “tens of trillions of tokens,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We built something once thought possible only inside the world’s top labs: a large-scale LLM and reinforcement learning platform capable of training massive Mixture-of-Experts (MoEs) models at frontier scale,” Reflection AI wrote in a post on X. “We saw the effectiveness of our approach first-hand when we applied it to the critical domain of autonomous coding. With this milestone unlocked, we’re now bringing these methods to general agentic reasoning.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MoE refers to a specific architecture that powers frontier LLMs —  systems that, previously, only large, closed AI labs were capable of training at scale. DeepSeek had a breakthrough moment when it figured out how to train these models at scale in an open way, followed by Qwen, Kimi, and other models in China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“DeepSeek and Qwen and all these models are our wake-up call because if we don’t do anything about it, then effectively, the global standard of intelligence will be built by someone else,” Laskin said. “It won’t be built by America.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Laskin added that this puts the U.S. and its allies at a disadvantage because enterprises and sovereign states often won’t use Chinese models due to potential legal repercussions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So you can either choose to live at a competitive disadvantage or rise to the occasion,” Laskin said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;American technologists have largely celebrated Reflection AI’s new mission. David Sacks, the White House AI and Crypto Czar, posted on X: “It’s great to see more American open source AI models. A meaningful segment of the global market will prefer the cost, customizability, and control that open source offers. We want the U.S. to win this category too.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Clem Delangue, co-founder and CEO of Hugging Face, an open and collaborative platform for AI builders, told TechCrunch of the round, “This is indeed great news for American open-source AI.” Added Delangue, “Now the challenge will be to show high velocity of sharing of open AI models and datasets (similar to what we’re seeing from the labs dominating in open-source AI).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI’s definition of being “open” seems to center on access rather than development, similar to strategies from Meta with Llama or Mistral. Laskin said Reflection AI would release model weights — the core parameters that determine how an AI system works — for public use while largely keeping datasets and full training pipelines proprietary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In reality, the most impactful thing is the model weights, because the model weights anyone can use and start tinkering with them,” Laskin said. “The infrastructure stack, only a select handful of companies can actually use that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That balance also underpins Reflection AI’s business model. Researchers will be able to use the models freely, Laskin said, but revenue will come from large enterprises building products on top of Reflection AI’s models and from governments developing “sovereign AI” systems, meaning AI models developed and controlled by individual nations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once you get into that territory where you’re a large enterprise, by default you want an open model,” Laskin said. “You want something you will have ownership over. You can run it on your infrastructure. You can control its costs. You can customize it for various workloads. Because you’re paying some ungodly amount of money for AI, you want to be able to optimize it as much as much as possible, and really that’s the market that we’re serving.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflection AI hasn’t yet released its first model, which will be largely text-based, with multimodal capabilities in the future, according to Laskin. It will use the funds from this latest round to get the compute resources needed to train the new models, the first of which the company is aiming to release early next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors in Reflection AI’s latest round include Nvidia, Disruptive, DST, 1789, B Capital, Lightspeed, GIC, Eric Yuan, Eric Schmidt, Citi, Sequoia, CRV, and others.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/reflection-raises-2b-to-be-americas-open-frontier-ai-lab-challenging-deepseek/</guid><pubDate>Thu, 09 Oct 2025 22:35:15 +0000</pubDate></item><item><title>NVIDIA Blackwell Raises Bar in New InferenceMAX Benchmarks, Delivering Unmatched Performance and Efficiency (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/blackwell-inferencemax-benchmark-results/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;NVIDIA Blackwell swept the new SemiAnalysis InferenceMAX v1 benchmarks, delivering the highest performance and best overall efficiency.&lt;/li&gt;
&lt;li&gt;InferenceMax v1 is the first independent benchmark to measure total cost of compute across diverse models and real-world scenarios.&lt;/li&gt;
&lt;li&gt;Best return on investment: NVIDIA GB200 NVL72 delivers unmatched AI factory economics — a $5 million investment generates $75 million in DSR1 token revenue, a 15x return on investment.&lt;/li&gt;
&lt;li&gt;Lowest total cost of ownership: NVIDIA B200 software optimizations achieve two cents per million tokens on gpt-oss, delivering 5x lower cost per token in just 2 months.&lt;/li&gt;
&lt;li&gt;Best throughput and interactivity: NVIDIA B200 sets the pace with 60,000 tokens per second per GPU and 1,000 tokens per second per user on gpt-oss with the latest NVIDIA TensorRT-LLM stack.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As AI shifts from one-shot answers to complex reasoning, the demand for inference — and the economics behind it — is exploding.&lt;/p&gt;
&lt;p&gt;The new independent InferenceMAX v1 benchmarks are the first to measure total cost of compute across real-world scenarios. The results? The NVIDIA Blackwell platform swept the field — delivering unmatched performance and best overall efficiency for AI factories.&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85689" height="571" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/AI-Factory-15x-Perf-ROI-r8-fixed.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;A $5 million investment in an NVIDIA GB200 NVL72 system can generate $75 million in token revenue.&lt;/b&gt; &lt;b&gt;That’s a 15x return on investment (ROI)&lt;/b&gt; — the new economics of inference.&lt;/p&gt;
&lt;p&gt;“Inference is where AI delivers value every day,” said Ian Buck, vice president of hyperscale and high-performance computing at NVIDIA. “These results show that NVIDIA’s full-stack approach gives customers the performance and efficiency they need to deploy AI at scale.”&lt;/p&gt;
&lt;h2&gt;Enter InferenceMAX v1&lt;/h2&gt;
&lt;p&gt;InferenceMAX v1, a new benchmark from SemiAnalysis released Monday, is the latest to highlight Blackwell’s inference leadership. It runs popular models across leading platforms, measures performance for a wide range of use cases and publishes results anyone can verify.&lt;/p&gt;
&lt;p&gt;Why do benchmarks like this matter?&lt;/p&gt;
&lt;p&gt;Because modern AI isn’t just about raw speed — it’s about efficiency and economics at scale. As models shift from one-shot replies to multistep reasoning and tool use, they generate far more tokens per query, dramatically increasing compute demands.&lt;/p&gt;
&lt;p&gt;NVIDIA’s open-source collaborations with OpenAI (gpt-oss 120B), Meta (Llama 3 70B), and DeepSeek AI (DeepSeek R1) highlight how community-driven models are advancing state-of-the-art reasoning and efficiency.&lt;/p&gt;
&lt;p&gt;Partnering with these leading model builders and the open-source community, NVIDIA ensures the latest models are optimized for the world’s largest AI inference infrastructure. These efforts reflect a broader commitment to open ecosystems — where shared innovation accelerates progress for everyone.&lt;/p&gt;
&lt;p&gt;Deep collaborations with the FlashInfer, SGLang and vLLM communities enable codeveloped kernel and runtime enhancements that power these models at scale.&lt;/p&gt;
&lt;h2&gt;Software Optimizations Deliver Continued Performance Gains&lt;/h2&gt;
&lt;p&gt;NVIDIA continuously improves performance through hardware and software codesign optimizations. Initial gpt-oss-120b performance on an NVIDIA DGX Blackwell B200 system with the NVIDIA TensorRT LLM library was market-leading, but NVIDIA’s teams and the community have significantly optimized TensorRT LLM for open-source large language models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85661" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image2.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;The TensorRT LLM v1.0 release is a major breakthrough in making large AI models faster and more responsive for everyone.&lt;/p&gt;
&lt;p&gt;Through advanced parallelization techniques, it uses the B200 system and NVIDIA NVLink Switch’s 1,800 GB/s bidirectional bandwidth to dramatically improve the performance of the gpt-oss-120b model.&lt;/p&gt;
&lt;p&gt;The innovation doesn’t stop there. The newly released gpt-oss-120b-Eagle3-v2 model introduces speculative decoding, a clever method that predicts multiple tokens at a time.&lt;/p&gt;
&lt;p&gt;This reduces lag and delivers even quicker results, tripling throughput at 100 tokens per second per user (TPS/user) — boosting per-GPU speeds from 6,000 to 30,000 tokens.&lt;/p&gt;
&lt;p&gt;For dense AI models like Llama 3.3 70B, which demand significant computational resources due to their large parameter count and the fact that all parameters are utilized simultaneously during inference, NVIDIA Blackwell B200 sets a new performance standard in InferenceMAX v1 benchmarks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85658" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image1.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;Blackwell delivers over 10,000 TPS per GPU at 50 TPS per user interactivity — 4x higher per-GPU throughput compared with the NVIDIA H200 GPU.&lt;/p&gt;
&lt;h2&gt;Performance Efficiency Drives Value&lt;/h2&gt;
&lt;p&gt;Metrics like tokens per watt, cost per million tokens and TPS/user matter as much as throughput. In fact, for power-limited AI factories, Blackwell delivers &lt;b&gt;10x throughput per megawatt &lt;/b&gt;compared with the previous generation, which translates into higher token revenue.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85664" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image3.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;The cost per token is crucial for evaluating AI model efficiency, directly impacting operational expenses. The NVIDIA Blackwell architecture &lt;b&gt;lowered cost per million tokens by 15x &lt;/b&gt;versus the previous generation, leading to substantial savings and fostering wider AI deployment and innovation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85673" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image6.png" width="1000" /&gt;&lt;/p&gt;
&lt;h2&gt;Multidimensional Performance&lt;/h2&gt;
&lt;p&gt;InferenceMAX uses the Pareto frontier — a curve that shows the best trade-offs between different factors, such as data center throughput and responsiveness — to map performance.&lt;/p&gt;
&lt;p&gt;But it’s more than a chart. It reflects how NVIDIA Blackwell balances the full spectrum of production priorities: cost, energy efficiency, throughput and responsiveness. That balance enables the highest ROI across real-world workloads.&lt;/p&gt;
&lt;p&gt;Systems that optimize for just one mode or scenario may show peak performance in isolation, but the economics of that doesn’t scale. Blackwell’s full-stack design delivers efficiency and value where it matters most: in production.&lt;/p&gt;
&lt;p&gt;For a deeper look at how these curves are built — and why they matter for total cost of ownership and service-level agreement planning — check out this technical deep dive for full charts and methodology.&lt;/p&gt;
&lt;h2&gt;What Makes It Possible?&lt;/h2&gt;
&lt;p&gt;Blackwell’s leadership comes from extreme hardware-software codesign. It’s a full-stack architecture built for speed, efficiency and scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;The Blackwell architecture features include:&lt;/b&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;NVFP4&lt;/b&gt; low-precision format for efficiency without loss of accuracy&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fifth-generation&lt;/b&gt; &lt;b&gt;NVIDIA NVLink &lt;/b&gt;that connects 72 Blackwell GPUs to act as one giant GPU&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVLink Switch&lt;/b&gt;, which enables high concurrency through advanced tensor, expert and data parallel attention algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Annual hardware cadence&lt;/b&gt; plus continuous software optimization — NVIDIA has more than doubled Blackwell performance since launch using software alone&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVIDIA TensorRT-LLM, &lt;/b&gt;&lt;b&gt;NVIDIA Dynamo&lt;/b&gt;&lt;b&gt;, SGLang and vLLM&lt;/b&gt; open-source inference frameworks optimized for peak performance&lt;/li&gt;
&lt;li&gt;&lt;b&gt;A massive ecosystem&lt;/b&gt;, with hundreds of millions of GPUs installed, 7 million CUDA developers and contributions to over 1,000 open-source projects&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Bigger Picture&lt;/h2&gt;
&lt;p&gt;AI is moving from pilots to AI factories — infrastructure that manufactures intelligence by turning data into tokens and decisions in real time.&lt;/p&gt;
&lt;p&gt;Open, frequently updated benchmarks help teams make informed platform choices, tune for cost per token, latency service-level agreements and utilization across changing workloads.&lt;/p&gt;
&lt;p&gt;NVIDIA’s Think SMART framework helps enterprises navigate this shift, spotlighting how NVIDIA’s full-stack inference platform delivers real-world ROI — turning performance into profits.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;NVIDIA Blackwell swept the new SemiAnalysis InferenceMAX v1 benchmarks, delivering the highest performance and best overall efficiency.&lt;/li&gt;
&lt;li&gt;InferenceMax v1 is the first independent benchmark to measure total cost of compute across diverse models and real-world scenarios.&lt;/li&gt;
&lt;li&gt;Best return on investment: NVIDIA GB200 NVL72 delivers unmatched AI factory economics — a $5 million investment generates $75 million in DSR1 token revenue, a 15x return on investment.&lt;/li&gt;
&lt;li&gt;Lowest total cost of ownership: NVIDIA B200 software optimizations achieve two cents per million tokens on gpt-oss, delivering 5x lower cost per token in just 2 months.&lt;/li&gt;
&lt;li&gt;Best throughput and interactivity: NVIDIA B200 sets the pace with 60,000 tokens per second per GPU and 1,000 tokens per second per user on gpt-oss with the latest NVIDIA TensorRT-LLM stack.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As AI shifts from one-shot answers to complex reasoning, the demand for inference — and the economics behind it — is exploding.&lt;/p&gt;
&lt;p&gt;The new independent InferenceMAX v1 benchmarks are the first to measure total cost of compute across real-world scenarios. The results? The NVIDIA Blackwell platform swept the field — delivering unmatched performance and best overall efficiency for AI factories.&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85689" height="571" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/AI-Factory-15x-Perf-ROI-r8-fixed.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;A $5 million investment in an NVIDIA GB200 NVL72 system can generate $75 million in token revenue.&lt;/b&gt; &lt;b&gt;That’s a 15x return on investment (ROI)&lt;/b&gt; — the new economics of inference.&lt;/p&gt;
&lt;p&gt;“Inference is where AI delivers value every day,” said Ian Buck, vice president of hyperscale and high-performance computing at NVIDIA. “These results show that NVIDIA’s full-stack approach gives customers the performance and efficiency they need to deploy AI at scale.”&lt;/p&gt;
&lt;h2&gt;Enter InferenceMAX v1&lt;/h2&gt;
&lt;p&gt;InferenceMAX v1, a new benchmark from SemiAnalysis released Monday, is the latest to highlight Blackwell’s inference leadership. It runs popular models across leading platforms, measures performance for a wide range of use cases and publishes results anyone can verify.&lt;/p&gt;
&lt;p&gt;Why do benchmarks like this matter?&lt;/p&gt;
&lt;p&gt;Because modern AI isn’t just about raw speed — it’s about efficiency and economics at scale. As models shift from one-shot replies to multistep reasoning and tool use, they generate far more tokens per query, dramatically increasing compute demands.&lt;/p&gt;
&lt;p&gt;NVIDIA’s open-source collaborations with OpenAI (gpt-oss 120B), Meta (Llama 3 70B), and DeepSeek AI (DeepSeek R1) highlight how community-driven models are advancing state-of-the-art reasoning and efficiency.&lt;/p&gt;
&lt;p&gt;Partnering with these leading model builders and the open-source community, NVIDIA ensures the latest models are optimized for the world’s largest AI inference infrastructure. These efforts reflect a broader commitment to open ecosystems — where shared innovation accelerates progress for everyone.&lt;/p&gt;
&lt;p&gt;Deep collaborations with the FlashInfer, SGLang and vLLM communities enable codeveloped kernel and runtime enhancements that power these models at scale.&lt;/p&gt;
&lt;h2&gt;Software Optimizations Deliver Continued Performance Gains&lt;/h2&gt;
&lt;p&gt;NVIDIA continuously improves performance through hardware and software codesign optimizations. Initial gpt-oss-120b performance on an NVIDIA DGX Blackwell B200 system with the NVIDIA TensorRT LLM library was market-leading, but NVIDIA’s teams and the community have significantly optimized TensorRT LLM for open-source large language models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85661" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image2.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;The TensorRT LLM v1.0 release is a major breakthrough in making large AI models faster and more responsive for everyone.&lt;/p&gt;
&lt;p&gt;Through advanced parallelization techniques, it uses the B200 system and NVIDIA NVLink Switch’s 1,800 GB/s bidirectional bandwidth to dramatically improve the performance of the gpt-oss-120b model.&lt;/p&gt;
&lt;p&gt;The innovation doesn’t stop there. The newly released gpt-oss-120b-Eagle3-v2 model introduces speculative decoding, a clever method that predicts multiple tokens at a time.&lt;/p&gt;
&lt;p&gt;This reduces lag and delivers even quicker results, tripling throughput at 100 tokens per second per user (TPS/user) — boosting per-GPU speeds from 6,000 to 30,000 tokens.&lt;/p&gt;
&lt;p&gt;For dense AI models like Llama 3.3 70B, which demand significant computational resources due to their large parameter count and the fact that all parameters are utilized simultaneously during inference, NVIDIA Blackwell B200 sets a new performance standard in InferenceMAX v1 benchmarks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85658" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image1.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;Blackwell delivers over 10,000 TPS per GPU at 50 TPS per user interactivity — 4x higher per-GPU throughput compared with the NVIDIA H200 GPU.&lt;/p&gt;
&lt;h2&gt;Performance Efficiency Drives Value&lt;/h2&gt;
&lt;p&gt;Metrics like tokens per watt, cost per million tokens and TPS/user matter as much as throughput. In fact, for power-limited AI factories, Blackwell delivers &lt;b&gt;10x throughput per megawatt &lt;/b&gt;compared with the previous generation, which translates into higher token revenue.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85664" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image3.png" width="1000" /&gt;&lt;/p&gt;
&lt;p&gt;The cost per token is crucial for evaluating AI model efficiency, directly impacting operational expenses. The NVIDIA Blackwell architecture &lt;b&gt;lowered cost per million tokens by 15x &lt;/b&gt;versus the previous generation, leading to substantial savings and fostering wider AI deployment and innovation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-85673" height="563" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image6.png" width="1000" /&gt;&lt;/p&gt;
&lt;h2&gt;Multidimensional Performance&lt;/h2&gt;
&lt;p&gt;InferenceMAX uses the Pareto frontier — a curve that shows the best trade-offs between different factors, such as data center throughput and responsiveness — to map performance.&lt;/p&gt;
&lt;p&gt;But it’s more than a chart. It reflects how NVIDIA Blackwell balances the full spectrum of production priorities: cost, energy efficiency, throughput and responsiveness. That balance enables the highest ROI across real-world workloads.&lt;/p&gt;
&lt;p&gt;Systems that optimize for just one mode or scenario may show peak performance in isolation, but the economics of that doesn’t scale. Blackwell’s full-stack design delivers efficiency and value where it matters most: in production.&lt;/p&gt;
&lt;p&gt;For a deeper look at how these curves are built — and why they matter for total cost of ownership and service-level agreement planning — check out this technical deep dive for full charts and methodology.&lt;/p&gt;
&lt;h2&gt;What Makes It Possible?&lt;/h2&gt;
&lt;p&gt;Blackwell’s leadership comes from extreme hardware-software codesign. It’s a full-stack architecture built for speed, efficiency and scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;The Blackwell architecture features include:&lt;/b&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;NVFP4&lt;/b&gt; low-precision format for efficiency without loss of accuracy&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fifth-generation&lt;/b&gt; &lt;b&gt;NVIDIA NVLink &lt;/b&gt;that connects 72 Blackwell GPUs to act as one giant GPU&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVLink Switch&lt;/b&gt;, which enables high concurrency through advanced tensor, expert and data parallel attention algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Annual hardware cadence&lt;/b&gt; plus continuous software optimization — NVIDIA has more than doubled Blackwell performance since launch using software alone&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NVIDIA TensorRT-LLM, &lt;/b&gt;&lt;b&gt;NVIDIA Dynamo&lt;/b&gt;&lt;b&gt;, SGLang and vLLM&lt;/b&gt; open-source inference frameworks optimized for peak performance&lt;/li&gt;
&lt;li&gt;&lt;b&gt;A massive ecosystem&lt;/b&gt;, with hundreds of millions of GPUs installed, 7 million CUDA developers and contributions to over 1,000 open-source projects&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Bigger Picture&lt;/h2&gt;
&lt;p&gt;AI is moving from pilots to AI factories — infrastructure that manufactures intelligence by turning data into tokens and decisions in real time.&lt;/p&gt;
&lt;p&gt;Open, frequently updated benchmarks help teams make informed platform choices, tune for cost per token, latency service-level agreements and utilization across changing workloads.&lt;/p&gt;
&lt;p&gt;NVIDIA’s Think SMART framework helps enterprises navigate this shift, spotlighting how NVIDIA’s full-stack inference platform delivers real-world ROI — turning performance into profits.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/blackwell-inferencemax-benchmark-results/</guid><pubDate>Thu, 09 Oct 2025 23:22:25 +0000</pubDate></item><item><title>While OpenAI races to build AI data centers, Nadella reminds us that Microsoft already has them (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/09/while-openai-races-to-build-ai-data-centers-nadella-reminds-us-that-microsoft-already-has-them/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-1066108038.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft CEO Satya Nadella on Thursday tweeted a video of his company’s first deployed massive AI system — or AI “factory” as Nvidia likes to call them. He promised this is the “first of many” such Nvidia AI factories that will be deployed across Microsoft Azure’s global data centers to run OpenAI workloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each system is a cluster of more than 4,600 Nvidia GB300 rack computers sporting the much-in-demand Blackwell Ultra GPU chip and connected via Nvidia’s super-fast networking tech called InfiniBand. (Besides AI chips, Nvidia CEO Jensen Huang also had the foresight to corner the market on InfiniBand when his company acquired Mellanox for $6.9 billion in 2019.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft promises that it will be deploying “hundreds of thousands of Blackwell Ultra GPUs” as it rolls out these systems globally. While the size of these systems is eye-popping (and the company shared plenty more technical details for hardware enthusiasts to peruse), the timing of this announcement is also noteworthy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It comes just after OpenAI, its partner and well-documented frenemy, inked two high-profile data center deals with Nvidia and AMD. In 2025, OpenAI has racked up, by some estimates, $1 trillion in commitments to build its own data centers. And CEO Sam Altman said this week that more were coming.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft clearly wants the world to know that it already has the data centers — more than 300 in 34 countries — and that they are “uniquely positioned” to “meet the demands of frontier AI today,” the company said. These monster AI systems are also capable of running the next generation of models with “hundreds of trillions of parameters,” it said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We expect to hear more about how Microsoft is ramping up to serve AI workloads later this month. Microsoft CTO Kevin Scott will be speaking at TechCrunch Disrupt, which will be held October 27 to October 29 in San Francisco. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-1066108038.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft CEO Satya Nadella on Thursday tweeted a video of his company’s first deployed massive AI system — or AI “factory” as Nvidia likes to call them. He promised this is the “first of many” such Nvidia AI factories that will be deployed across Microsoft Azure’s global data centers to run OpenAI workloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each system is a cluster of more than 4,600 Nvidia GB300 rack computers sporting the much-in-demand Blackwell Ultra GPU chip and connected via Nvidia’s super-fast networking tech called InfiniBand. (Besides AI chips, Nvidia CEO Jensen Huang also had the foresight to corner the market on InfiniBand when his company acquired Mellanox for $6.9 billion in 2019.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft promises that it will be deploying “hundreds of thousands of Blackwell Ultra GPUs” as it rolls out these systems globally. While the size of these systems is eye-popping (and the company shared plenty more technical details for hardware enthusiasts to peruse), the timing of this announcement is also noteworthy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It comes just after OpenAI, its partner and well-documented frenemy, inked two high-profile data center deals with Nvidia and AMD. In 2025, OpenAI has racked up, by some estimates, $1 trillion in commitments to build its own data centers. And CEO Sam Altman said this week that more were coming.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft clearly wants the world to know that it already has the data centers — more than 300 in 34 countries — and that they are “uniquely positioned” to “meet the demands of frontier AI today,” the company said. These monster AI systems are also capable of running the next generation of models with “hundreds of trillions of parameters,” it said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We expect to hear more about how Microsoft is ramping up to serve AI workloads later this month. Microsoft CTO Kevin Scott will be speaking at TechCrunch Disrupt, which will be held October 27 to October 29 in San Francisco. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/09/while-openai-races-to-build-ai-data-centers-nadella-reminds-us-that-microsoft-already-has-them/</guid><pubDate>Thu, 09 Oct 2025 23:53:36 +0000</pubDate></item><item><title>[NEW] Ray Kurzweil ’70 reinforces his optimism in tech progress (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/ray-kurzwei-reinforces-his-optimism-tech-progress-1010</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/MIT-KurzweilMuh-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Innovator, futurist, and author Ray Kurzweil ’70 emphasized his optimism about artificial intelligence, and technological progress generally, in a lecture on Wednesday while accepting MIT’s&amp;nbsp;Robert A. Muh Alumni Award from the School of Humanities, Arts, and Social Sciences (SHASS).&lt;/p&gt;&lt;p&gt;Kurzweil offered his signature high-profile forecasts about how AI and computing will entirely blend with human functionality, and proposed that AI will lead to monumental gains in longevity, medicine, and other realms of life.&lt;/p&gt;&lt;p&gt;“People do not appreciate that the rate of progress is accelerating,” Kurzweil said, forecasting “incredible breakthroughs” over the next two decades.&lt;/p&gt;&lt;p&gt;Kurzweil delivered his lecture, titled “Reinventing Intelligence,” in the Thomas Tull Concert Hall of the Edward and Joyce Linde Music Building, which opened earlier in 2025 on the MIT campus.&lt;/p&gt;&lt;p&gt;The Muh Award was founded and endowed by Robert A. Muh ’59 and his wife Berit, and is one of the leading alumni honors granted by SHASS and MIT. Muh, a life member emeritus of the MIT Corporation, established the award, which is granted every two years for “extraordinary contributions” by alumni in the humanities, arts, and social sciences.&lt;/p&gt;&lt;p&gt;Robert and Berit Muh were both present at the lecture, along with their daughter Carrie Muh ’96, ’97, SM ’97.&lt;/p&gt;&lt;p&gt;Agustín Rayo, dean of SHASS, offered introductory remarks, calling Kurzweil “one of the most prolific thinkers of our time.” Rayo added that Kurzweil “has built his life and career on the belief that ideas change the world, and change it for the better.”&lt;/p&gt;&lt;p&gt;Kurzweil has been an innovator in language recognition technologies, developing advances and founding companies that have served people who are blind or low-vision, and helped in music creation. He is also a best-selling author who has heralded advances in computing capabilities, and even the merging of human and machines.&lt;/p&gt;&lt;p&gt;The initial segment of Kurzweil’s lecture was autobiographical in focus, reflecting on his family and early years. The families of both of Kurzweil’s parents fled the Nazis in Europe, seeking refuge in the U.S., with the belief that people could create a brighter future for themselves.&lt;/p&gt;&lt;p&gt;“My parents taught me the power of ideas can really change the world,” Kurzweil said.&lt;/p&gt;&lt;p&gt;Showing an early interest in how things worked, Kurzweil had decided to become an inventor by about the age of 7, he recalled. He also described his mother as being tremendously encouraging to him as a child. The two would take walks together, and the young Kurzweil would talk about all the things he imagined inventing.&lt;/p&gt;&lt;p&gt;“I would tell her my ideas and no matter how fantastical they were, she believed them,” he said. “Now other parents might have simply chuckled … but she actually believed my ideas, and that actually gave me my confidence, and I think confidence is important in succeeding.”&lt;/p&gt;&lt;p&gt;He became interested in computing by the early 1960s and majored in both computer science and literature as an MIT undergraduate.&lt;/p&gt;&lt;p&gt;Kurzweil has a long-running association with MIT extending far beyond his undergraduate studies. He served as a member of the MIT Corporation from 2005 to 2012 and was the 2001 recipient of the $500,000 Lemelson-MIT Prize, an award for innovation, for his development of reading technology.&lt;/p&gt;&lt;p&gt;“MIT has played a major role in my personal and professional life over the years,” Kurzweil said, calling himself “truly honored to receive this award.” Addressing Muh, he added: “Your longstanding commitment to our alma mater is inspiring.”&lt;/p&gt;&lt;p&gt;After graduating from MIT, Kurzweil launched a successful career developing innovative computing products, including one that recognized text across all fonts and could produce an audio reading. He also developed leading-edge music synthesizers, among many other advances.&lt;/p&gt;&lt;p&gt;In a corresponding part of his career, Kurzweil has become an energetic author, whose best-known books include “The Age of Intelligent Machines” (1990), “The Age of Spiritual Machines” (1999), “The Singularity Is Near” (2005), and “The Singularity Is Nearer” (2024), among many others.&lt;/p&gt;&lt;p&gt;Kurzweil was recently named chief AI officer of Beyond Imagination, a robotics firm he co-founded; he has also held a position at Google in recent years, working on natural language technologies.&lt;/p&gt;&lt;p&gt;In his remarks, Kurzweil underscored his view that, as exemplified and enabled by the growth of computing power over time, technological innovation moves at an exponential pace.&lt;/p&gt;&lt;p&gt;“People don’t really think about exponential growth; they think about linear growth,” Kurzweil said.&lt;/p&gt;&lt;p&gt;This concept, he said, makes him confident that a string of innovations will continue at remarkable speed.&lt;/p&gt;&lt;p&gt;“One of the bigger transformations we’re going to see from AI in the near term is health and medicine,” Kurweil said, forecasting that human medical trials will be replaced by simulated “digital trials.”&lt;/p&gt;&lt;p&gt;Kurzweil also believes computing and AI advances can lead to so many medical advances it will soon produce a drastic improvement in human longevity.&lt;/p&gt;&lt;p&gt;“These incredible breakthroughs are going to lead to what we’ll call longevity escape velocity,” Kurzweil said. “By roughly 2032 when you live through a year, you’ll get back an entire year from scientific progress, and beyond that point you’ll get back more than a year for every year you live, so you’ll be going back into time as far as your health is concerned,” Kurweil said. He did offer that these advances will “start” with people who are the most diligent about their health.&lt;/p&gt;&lt;p&gt;Kurzweil also outlined one of his best-known forecasts, that AI and people will be combined. “As we move forward, the lines between humans and technology will blur, until we are … one and the same,” Kurzweil said. “This is how we learn to merge with AI. In the 2030s, robots the size of molecules will go into our brains, noninvasively, through the capillaries, and will connect our brains directly to the cloud. Think of it like having a phone, but in your brain.”&lt;/p&gt;&lt;p&gt;“By 2045, once we have fully merged with AI, our intelligence will no longer be constrained … it will expand a millionfold,” he said. “This is what we call the singularity.”&lt;/p&gt;&lt;p&gt;To be sure, Kurzweil acknowledged, “Technology has always been a double-edged sword,” given that a drone can deliver either medical supplies or weaponry. “Threats of AI are real, must be taken seriously, [and] I think we are doing that,” he said. In any case, he added, we have “a moral imperative to realize the promise of new technologies while controlling the peril.” He concluded: “We are not doomed to fail to control any of these risks.”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/MIT-KurzweilMuh-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Innovator, futurist, and author Ray Kurzweil ’70 emphasized his optimism about artificial intelligence, and technological progress generally, in a lecture on Wednesday while accepting MIT’s&amp;nbsp;Robert A. Muh Alumni Award from the School of Humanities, Arts, and Social Sciences (SHASS).&lt;/p&gt;&lt;p&gt;Kurzweil offered his signature high-profile forecasts about how AI and computing will entirely blend with human functionality, and proposed that AI will lead to monumental gains in longevity, medicine, and other realms of life.&lt;/p&gt;&lt;p&gt;“People do not appreciate that the rate of progress is accelerating,” Kurzweil said, forecasting “incredible breakthroughs” over the next two decades.&lt;/p&gt;&lt;p&gt;Kurzweil delivered his lecture, titled “Reinventing Intelligence,” in the Thomas Tull Concert Hall of the Edward and Joyce Linde Music Building, which opened earlier in 2025 on the MIT campus.&lt;/p&gt;&lt;p&gt;The Muh Award was founded and endowed by Robert A. Muh ’59 and his wife Berit, and is one of the leading alumni honors granted by SHASS and MIT. Muh, a life member emeritus of the MIT Corporation, established the award, which is granted every two years for “extraordinary contributions” by alumni in the humanities, arts, and social sciences.&lt;/p&gt;&lt;p&gt;Robert and Berit Muh were both present at the lecture, along with their daughter Carrie Muh ’96, ’97, SM ’97.&lt;/p&gt;&lt;p&gt;Agustín Rayo, dean of SHASS, offered introductory remarks, calling Kurzweil “one of the most prolific thinkers of our time.” Rayo added that Kurzweil “has built his life and career on the belief that ideas change the world, and change it for the better.”&lt;/p&gt;&lt;p&gt;Kurzweil has been an innovator in language recognition technologies, developing advances and founding companies that have served people who are blind or low-vision, and helped in music creation. He is also a best-selling author who has heralded advances in computing capabilities, and even the merging of human and machines.&lt;/p&gt;&lt;p&gt;The initial segment of Kurzweil’s lecture was autobiographical in focus, reflecting on his family and early years. The families of both of Kurzweil’s parents fled the Nazis in Europe, seeking refuge in the U.S., with the belief that people could create a brighter future for themselves.&lt;/p&gt;&lt;p&gt;“My parents taught me the power of ideas can really change the world,” Kurzweil said.&lt;/p&gt;&lt;p&gt;Showing an early interest in how things worked, Kurzweil had decided to become an inventor by about the age of 7, he recalled. He also described his mother as being tremendously encouraging to him as a child. The two would take walks together, and the young Kurzweil would talk about all the things he imagined inventing.&lt;/p&gt;&lt;p&gt;“I would tell her my ideas and no matter how fantastical they were, she believed them,” he said. “Now other parents might have simply chuckled … but she actually believed my ideas, and that actually gave me my confidence, and I think confidence is important in succeeding.”&lt;/p&gt;&lt;p&gt;He became interested in computing by the early 1960s and majored in both computer science and literature as an MIT undergraduate.&lt;/p&gt;&lt;p&gt;Kurzweil has a long-running association with MIT extending far beyond his undergraduate studies. He served as a member of the MIT Corporation from 2005 to 2012 and was the 2001 recipient of the $500,000 Lemelson-MIT Prize, an award for innovation, for his development of reading technology.&lt;/p&gt;&lt;p&gt;“MIT has played a major role in my personal and professional life over the years,” Kurzweil said, calling himself “truly honored to receive this award.” Addressing Muh, he added: “Your longstanding commitment to our alma mater is inspiring.”&lt;/p&gt;&lt;p&gt;After graduating from MIT, Kurzweil launched a successful career developing innovative computing products, including one that recognized text across all fonts and could produce an audio reading. He also developed leading-edge music synthesizers, among many other advances.&lt;/p&gt;&lt;p&gt;In a corresponding part of his career, Kurzweil has become an energetic author, whose best-known books include “The Age of Intelligent Machines” (1990), “The Age of Spiritual Machines” (1999), “The Singularity Is Near” (2005), and “The Singularity Is Nearer” (2024), among many others.&lt;/p&gt;&lt;p&gt;Kurzweil was recently named chief AI officer of Beyond Imagination, a robotics firm he co-founded; he has also held a position at Google in recent years, working on natural language technologies.&lt;/p&gt;&lt;p&gt;In his remarks, Kurzweil underscored his view that, as exemplified and enabled by the growth of computing power over time, technological innovation moves at an exponential pace.&lt;/p&gt;&lt;p&gt;“People don’t really think about exponential growth; they think about linear growth,” Kurzweil said.&lt;/p&gt;&lt;p&gt;This concept, he said, makes him confident that a string of innovations will continue at remarkable speed.&lt;/p&gt;&lt;p&gt;“One of the bigger transformations we’re going to see from AI in the near term is health and medicine,” Kurweil said, forecasting that human medical trials will be replaced by simulated “digital trials.”&lt;/p&gt;&lt;p&gt;Kurzweil also believes computing and AI advances can lead to so many medical advances it will soon produce a drastic improvement in human longevity.&lt;/p&gt;&lt;p&gt;“These incredible breakthroughs are going to lead to what we’ll call longevity escape velocity,” Kurzweil said. “By roughly 2032 when you live through a year, you’ll get back an entire year from scientific progress, and beyond that point you’ll get back more than a year for every year you live, so you’ll be going back into time as far as your health is concerned,” Kurweil said. He did offer that these advances will “start” with people who are the most diligent about their health.&lt;/p&gt;&lt;p&gt;Kurzweil also outlined one of his best-known forecasts, that AI and people will be combined. “As we move forward, the lines between humans and technology will blur, until we are … one and the same,” Kurzweil said. “This is how we learn to merge with AI. In the 2030s, robots the size of molecules will go into our brains, noninvasively, through the capillaries, and will connect our brains directly to the cloud. Think of it like having a phone, but in your brain.”&lt;/p&gt;&lt;p&gt;“By 2045, once we have fully merged with AI, our intelligence will no longer be constrained … it will expand a millionfold,” he said. “This is what we call the singularity.”&lt;/p&gt;&lt;p&gt;To be sure, Kurzweil acknowledged, “Technology has always been a double-edged sword,” given that a drone can deliver either medical supplies or weaponry. “Threats of AI are real, must be taken seriously, [and] I think we are doing that,” he said. In any case, he added, we have “a moral imperative to realize the promise of new technologies while controlling the peril.” He concluded: “We are not doomed to fail to control any of these risks.”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/ray-kurzwei-reinforces-his-optimism-tech-progress-1010</guid><pubDate>Fri, 10 Oct 2025 04:00:00 +0000</pubDate></item></channel></rss>