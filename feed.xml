<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 09 Feb 2026 13:15:55 +0000</lastBuildDate><item><title>Study: Platforms that rank the latest LLMs can be unreliable (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/study-platforms-rank-latest-llms-can-be-unreliable-0209</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-LLM-Rankings-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;A firm that wants to use a large language model (LLM) to summarize sales reports or triage customer inquiries can choose between hundreds of unique LLMs with dozens of model variations, each with slightly different performance.&lt;/p&gt;&lt;p&gt;To narrow down the choice, companies often rely on LLM ranking platforms, which gather user feedback on model interactions to rank the latest LLMs based on how they perform on certain tasks.&lt;/p&gt;&lt;p&gt;But MIT researchers found that a handful of user interactions can skew the results, leading someone to mistakenly believe one LLM is the ideal choice for a particular use case. Their study reveals that removing a tiny fraction of crowdsourced data can change which models are top-ranked.&lt;/p&gt;&lt;p&gt;They developed a fast method to test ranking platforms and determine whether they are susceptible to this problem. The evaluation technique identifies the individual votes most responsible for skewing the results so users can inspect these influential votes.&lt;/p&gt;&lt;p&gt;The researchers say this work underscores the need for more rigorous strategies to evaluate model rankings. While they didn’t focus on mitigation in this study, they provide suggestions that may improve the robustness of these platforms, such as gathering more detailed feedback to create the rankings.&lt;/p&gt;&lt;p&gt;The study also offers a word of warning to users who may rely on rankings when making decisions about LLMs that could have far-reaching and costly impacts on a business or organization.&lt;/p&gt;&lt;p&gt;“We were surprised that these ranking platforms were so sensitive to this problem. If it turns out the top-ranked LLM depends on only two or three pieces of user feedback out of tens of thousands, then one can’t assume the top-ranked LLM is going to be consistently outperforming all the other LLMs when it is deployed,” says Tamara Broderick, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS); a member of the Laboratory for Information and Decision Systems (LIDS) and the Institute for Data, Systems, and Society; an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author of this study.&lt;/p&gt;&lt;p&gt;She is joined on the paper by lead authors and EECS graduate students Jenny Huang and Yunyi Shen as well as Dennis Wei, a senior research scientist at IBM Research. The study will be presented at the International Conference on Learning Representations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Dropping data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While there are many types of LLM ranking platforms, the most popular variations ask users to submit a query to two models and pick which LLM provides the better response.&lt;/p&gt;&lt;p&gt;The platforms aggregate the results of these matchups to produce rankings that show which LLM performed best on certain tasks, such as coding or visual understanding.&lt;/p&gt;&lt;p&gt;By choosing a top-performing LLM, a user likely expects that model’s top ranking to generalize, meaning it should outperform other models on their similar, but not identical, application with a set of new data.&lt;/p&gt;&lt;p&gt;The MIT researchers previously studied generalization in areas like statistics and economics. That work revealed certain cases where dropping a small percentage of data can change a model’s results, indicating that those studies’ conclusions might not hold beyond their narrow setting.&lt;/p&gt;&lt;p&gt;The researchers wanted to see if the same analysis could be applied to LLM ranking platforms.&lt;/p&gt;&lt;p&gt;“At the end of the day, a user wants to know whether they are choosing the best LLM. If only a few prompts are driving this ranking, that suggests the ranking might not be the end-all-be-all,” Broderick says.&lt;/p&gt;&lt;p&gt;But it would be impossible to test the data-dropping phenomenon manually. For instance, one ranking they evaluated had more than 57,000 votes. Testing a data drop of 0.1 percent means removing each subset of 57 votes out of the 57,000, (there are more than 10&lt;sup&gt;194&amp;nbsp;&lt;/sup&gt;subsets), and then recalculating the ranking.&lt;/p&gt;&lt;p&gt;Instead, the researchers developed an efficient approximation method, based on their prior work, and adapted it to fit LLM ranking systems.&lt;/p&gt;&lt;p&gt;“While we have theory to prove the approximation works under certain assumptions, the user doesn’t need to trust that. Our method tells the user the problematic data points at the end, so they can just drop those data points, re-run the analysis, and check to see if they get a change in the rankings,” she says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Surprisingly sensitive&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When the researchers applied their technique to popular ranking platforms, they were surprised to see how few data points they needed to drop to cause significant changes in the top LLMs. In one instance, removing just two votes out of more than 57,000, which is 0.0035 percent, changed which model is top-ranked.&lt;/p&gt;&lt;p&gt;A different ranking platform, which uses expert annotators and higher quality prompts, was more robust. Here, removing 83 out of 2,575 evaluations (about 3 percent) flipped the top models.&lt;/p&gt;&lt;p&gt;Their examination revealed that many influential votes may have been a result of user error. In some cases, it appeared there was a clear answer as to which LLM performed better, but the user chose the other model instead, Broderick says.&lt;/p&gt;&lt;p&gt;“We can never know what was in the user’s mind at that time, but maybe they mis-clicked or weren’t paying attention, or they honestly didn’t know which one was better. The big takeaway here is that you don’t want noise, user error, or some outlier determining which is the top-ranked LLM,” she adds.&lt;/p&gt;&lt;p&gt;The researchers suggest that gathering additional feedback from users, such as confidence levels in each vote, would provide richer information that could help mitigate this problem. Ranking platforms could also use human mediators to assess crowdsourced responses.&lt;/p&gt;&lt;p&gt;For the researchers’ part, they want to continue exploring generalization in other contexts while also developing better approximation methods that can capture more examples of non-robustness.&lt;/p&gt;&lt;p&gt;“Broderick and her students’ work shows how you can get valid estimates of the influence of specific data on downstream processes, despite the intractability of exhaustive calculations given the size of modern machine-learning models and datasets,” says Jessica Hullman, the Ginni Rometty Professor of Computer Science at Northwestern University, who was not involved with this work. &amp;nbsp;“The recent work provides a glimpse into the strong data dependencies in routinely applied — but also very fragile — methods for aggregating human preferences and using them to update a model. Seeing how few preferences could really change the behavior of a fine-tuned model could inspire more thoughtful methods for collecting these data.”&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the Office of Naval Research, the MIT-IBM Watson AI Lab, the National Science Foundation, Amazon, and a CSAIL seed award.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-LLM-Rankings-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;A firm that wants to use a large language model (LLM) to summarize sales reports or triage customer inquiries can choose between hundreds of unique LLMs with dozens of model variations, each with slightly different performance.&lt;/p&gt;&lt;p&gt;To narrow down the choice, companies often rely on LLM ranking platforms, which gather user feedback on model interactions to rank the latest LLMs based on how they perform on certain tasks.&lt;/p&gt;&lt;p&gt;But MIT researchers found that a handful of user interactions can skew the results, leading someone to mistakenly believe one LLM is the ideal choice for a particular use case. Their study reveals that removing a tiny fraction of crowdsourced data can change which models are top-ranked.&lt;/p&gt;&lt;p&gt;They developed a fast method to test ranking platforms and determine whether they are susceptible to this problem. The evaluation technique identifies the individual votes most responsible for skewing the results so users can inspect these influential votes.&lt;/p&gt;&lt;p&gt;The researchers say this work underscores the need for more rigorous strategies to evaluate model rankings. While they didn’t focus on mitigation in this study, they provide suggestions that may improve the robustness of these platforms, such as gathering more detailed feedback to create the rankings.&lt;/p&gt;&lt;p&gt;The study also offers a word of warning to users who may rely on rankings when making decisions about LLMs that could have far-reaching and costly impacts on a business or organization.&lt;/p&gt;&lt;p&gt;“We were surprised that these ranking platforms were so sensitive to this problem. If it turns out the top-ranked LLM depends on only two or three pieces of user feedback out of tens of thousands, then one can’t assume the top-ranked LLM is going to be consistently outperforming all the other LLMs when it is deployed,” says Tamara Broderick, an associate professor in MIT’s Department of Electrical Engineering and Computer Science (EECS); a member of the Laboratory for Information and Decision Systems (LIDS) and the Institute for Data, Systems, and Society; an affiliate of the Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author of this study.&lt;/p&gt;&lt;p&gt;She is joined on the paper by lead authors and EECS graduate students Jenny Huang and Yunyi Shen as well as Dennis Wei, a senior research scientist at IBM Research. The study will be presented at the International Conference on Learning Representations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Dropping data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While there are many types of LLM ranking platforms, the most popular variations ask users to submit a query to two models and pick which LLM provides the better response.&lt;/p&gt;&lt;p&gt;The platforms aggregate the results of these matchups to produce rankings that show which LLM performed best on certain tasks, such as coding or visual understanding.&lt;/p&gt;&lt;p&gt;By choosing a top-performing LLM, a user likely expects that model’s top ranking to generalize, meaning it should outperform other models on their similar, but not identical, application with a set of new data.&lt;/p&gt;&lt;p&gt;The MIT researchers previously studied generalization in areas like statistics and economics. That work revealed certain cases where dropping a small percentage of data can change a model’s results, indicating that those studies’ conclusions might not hold beyond their narrow setting.&lt;/p&gt;&lt;p&gt;The researchers wanted to see if the same analysis could be applied to LLM ranking platforms.&lt;/p&gt;&lt;p&gt;“At the end of the day, a user wants to know whether they are choosing the best LLM. If only a few prompts are driving this ranking, that suggests the ranking might not be the end-all-be-all,” Broderick says.&lt;/p&gt;&lt;p&gt;But it would be impossible to test the data-dropping phenomenon manually. For instance, one ranking they evaluated had more than 57,000 votes. Testing a data drop of 0.1 percent means removing each subset of 57 votes out of the 57,000, (there are more than 10&lt;sup&gt;194&amp;nbsp;&lt;/sup&gt;subsets), and then recalculating the ranking.&lt;/p&gt;&lt;p&gt;Instead, the researchers developed an efficient approximation method, based on their prior work, and adapted it to fit LLM ranking systems.&lt;/p&gt;&lt;p&gt;“While we have theory to prove the approximation works under certain assumptions, the user doesn’t need to trust that. Our method tells the user the problematic data points at the end, so they can just drop those data points, re-run the analysis, and check to see if they get a change in the rankings,” she says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Surprisingly sensitive&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When the researchers applied their technique to popular ranking platforms, they were surprised to see how few data points they needed to drop to cause significant changes in the top LLMs. In one instance, removing just two votes out of more than 57,000, which is 0.0035 percent, changed which model is top-ranked.&lt;/p&gt;&lt;p&gt;A different ranking platform, which uses expert annotators and higher quality prompts, was more robust. Here, removing 83 out of 2,575 evaluations (about 3 percent) flipped the top models.&lt;/p&gt;&lt;p&gt;Their examination revealed that many influential votes may have been a result of user error. In some cases, it appeared there was a clear answer as to which LLM performed better, but the user chose the other model instead, Broderick says.&lt;/p&gt;&lt;p&gt;“We can never know what was in the user’s mind at that time, but maybe they mis-clicked or weren’t paying attention, or they honestly didn’t know which one was better. The big takeaway here is that you don’t want noise, user error, or some outlier determining which is the top-ranked LLM,” she adds.&lt;/p&gt;&lt;p&gt;The researchers suggest that gathering additional feedback from users, such as confidence levels in each vote, would provide richer information that could help mitigate this problem. Ranking platforms could also use human mediators to assess crowdsourced responses.&lt;/p&gt;&lt;p&gt;For the researchers’ part, they want to continue exploring generalization in other contexts while also developing better approximation methods that can capture more examples of non-robustness.&lt;/p&gt;&lt;p&gt;“Broderick and her students’ work shows how you can get valid estimates of the influence of specific data on downstream processes, despite the intractability of exhaustive calculations given the size of modern machine-learning models and datasets,” says Jessica Hullman, the Ginni Rometty Professor of Computer Science at Northwestern University, who was not involved with this work. &amp;nbsp;“The recent work provides a glimpse into the strong data dependencies in routinely applied — but also very fragile — methods for aggregating human preferences and using them to update a model. Seeing how few preferences could really change the behavior of a fine-tuned model could inspire more thoughtful methods for collecting these data.”&lt;/p&gt;&lt;p&gt;This research is funded, in part, by the Office of Naval Research, the MIT-IBM Watson AI Lab, the National Science Foundation, Amazon, and a CSAIL seed award.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/study-platforms-rank-latest-llms-can-be-unreliable-0209</guid><pubDate>Mon, 09 Feb 2026 05:00:00 +0000</pubDate></item><item><title>[NEW] Goldman Sachs tests autonomous AI agents for process-heavy work (AI News)</title><link>https://www.artificialintelligence-news.com/news/goldman-sachs-tests-autonomous-ai-agents-for-process-heavy-work/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Goldman-Sachs-tests-autonomous-AI-agents-for-process-heavy-work-scaled-e1770608251794.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Goldman Sachs is pushing deeper into real use of artificial intelligence inside its operations, moving to systems that can carry out complex tasks on their own. The Wall Street bank is working with AI startup Anthropic to create autonomous AI agents powered by Anthropic’s Claude model that can handle work that used to require large teams of people. The bank’s chief information officer says the technology has surprised staff with how capable it can be.&lt;/p&gt;&lt;p&gt;Many companies use AI for tasks like helping employees draft text or analysing trends. But Goldman Sachs is testing AI systems that go into what bankers call back-office work – functions like accounting, compliance checks and onboarding new clients – areas viewed as too complex for automation. Such jobs involve many rules, data and detailed review, and have resisted full automation.&lt;/p&gt;&lt;h3&gt;Moving AI agents into process-heavy operations&lt;/h3&gt;&lt;p&gt;The partnership with Anthropic has been underway for roughly six months, with engineers from the AI startup embedded directly with teams at Goldman Sachs to build these agents side by side with in-house staff, according to a report based on an interview with the bank’s CIO. The work has focused on areas where automation could cut the time it takes to complete repetitive and data-heavy tasks.&lt;/p&gt;&lt;p&gt;Marco Argenti, Goldman’s chief information officer, described the AI systems as a new kind of digital assistant. “Think of it as a digital co-worker for many of the professions in the firm that are scaled, complex and very process-intensive,” he told &lt;em&gt;CNBC&lt;/em&gt;. In early tests, the ability to reason through multi-step work and apply logic to complex areas like accounting and compliance was something the bank had not expected from the model.&lt;/p&gt;&lt;p&gt;Goldman Sachs has been among the more active banks in testing AI tools over the past few years. Before this announcement, the firm deployed internal tools to help engineers write and debug code. But the change now is toward systems that can take on work traditionally done by accountants and compliance teams. That highlights how organisations are trying to find concrete business uses for AI beyond the hype.&lt;/p&gt;&lt;h3&gt;Faster workflows, human oversight remains&lt;/h3&gt;&lt;p&gt;The agents are based on Anthropic’s Claude Opus 4.6 model, which has been built to handle long documents and complex reasoning. Goldman’s tests have shown that such systems can reduce the time needed for tasks like client onboarding, trade reconciliation and document review. While the bank has not shared specific performance numbers, people familiar with the matter told news outlets that work which once took a great deal of human labour can now be done in much less time.&lt;/p&gt;&lt;p&gt;Argenti said the rollout is not about replacing human workers, at least not at this stage. The bank reportedly views the agents as a tool to help existing staff manage busy schedules and get through high volumes of work. In areas like compliance and accounting, jobs can involve repetitive, rule-based steps. AI frees analysts from that repetition so they can focus on higher-value judgement work.&lt;/p&gt;&lt;p&gt;Markets have already reacted to the idea that large institutions are moving toward more AI-driven automation. In recent days, a sell-off in enterprise software stocks wiped out billions in value as some investors worried that tools like autonomous agents could speed up the decline of traditional business software that has dominated corporate IT for years.&lt;/p&gt;&lt;h3&gt;AI adoption meets governance reality&lt;/h3&gt;&lt;p&gt;Industry watchers see Goldman’s move as part of a wider trend. For example, some firms are piloting tools to read large data sets, interpret multiple sources of information, and draft investment analysis. These steps show AI making the jump from isolated projects to operational work. Yet the technology raises questions about oversight and trust. AI systems that interpret financial rules and compliance standards must be monitored carefully to avoid errors that could have regulatory or financial consequences. That’s why many institutions treat these systems as helpers that are reviewed by human experts until they mature.&lt;/p&gt;&lt;p&gt;Goldman Sachs is starting with operational functions that have traditionally resisted automation because they involve a lot of data and formal steps. The bank has not said when it expects deployment of the agents in its operations, but executives have suggested that the initial tests have been promising enough to support further rollout.&lt;/p&gt;&lt;p&gt;The broader industry context shows other banks and financial firms also exploring similar use cases. Some have already invested heavily in AI infrastructure, and reports indicate that major firms are planning to use AI to cut costs, speed workflows and improve risk management. However, many remain cautious about putting AI into customer-facing or regulated functions.&lt;/p&gt;&lt;p&gt;Goldman’s push into autonomous AI agents is an example of how large companies are reshaping internal operations using the latest generation of AI models. If systems can handle complex tasks reliably, organisations could see real changes in how work gets done – particularly in back-office functions where volume and repetition keep costs high and innovation slow.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Louis Droege)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Intuit, Uber, and State Farm trial AI agents inside enterprise workflows&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Goldman-Sachs-tests-autonomous-AI-agents-for-process-heavy-work-scaled-e1770608251794.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Goldman Sachs is pushing deeper into real use of artificial intelligence inside its operations, moving to systems that can carry out complex tasks on their own. The Wall Street bank is working with AI startup Anthropic to create autonomous AI agents powered by Anthropic’s Claude model that can handle work that used to require large teams of people. The bank’s chief information officer says the technology has surprised staff with how capable it can be.&lt;/p&gt;&lt;p&gt;Many companies use AI for tasks like helping employees draft text or analysing trends. But Goldman Sachs is testing AI systems that go into what bankers call back-office work – functions like accounting, compliance checks and onboarding new clients – areas viewed as too complex for automation. Such jobs involve many rules, data and detailed review, and have resisted full automation.&lt;/p&gt;&lt;h3&gt;Moving AI agents into process-heavy operations&lt;/h3&gt;&lt;p&gt;The partnership with Anthropic has been underway for roughly six months, with engineers from the AI startup embedded directly with teams at Goldman Sachs to build these agents side by side with in-house staff, according to a report based on an interview with the bank’s CIO. The work has focused on areas where automation could cut the time it takes to complete repetitive and data-heavy tasks.&lt;/p&gt;&lt;p&gt;Marco Argenti, Goldman’s chief information officer, described the AI systems as a new kind of digital assistant. “Think of it as a digital co-worker for many of the professions in the firm that are scaled, complex and very process-intensive,” he told &lt;em&gt;CNBC&lt;/em&gt;. In early tests, the ability to reason through multi-step work and apply logic to complex areas like accounting and compliance was something the bank had not expected from the model.&lt;/p&gt;&lt;p&gt;Goldman Sachs has been among the more active banks in testing AI tools over the past few years. Before this announcement, the firm deployed internal tools to help engineers write and debug code. But the change now is toward systems that can take on work traditionally done by accountants and compliance teams. That highlights how organisations are trying to find concrete business uses for AI beyond the hype.&lt;/p&gt;&lt;h3&gt;Faster workflows, human oversight remains&lt;/h3&gt;&lt;p&gt;The agents are based on Anthropic’s Claude Opus 4.6 model, which has been built to handle long documents and complex reasoning. Goldman’s tests have shown that such systems can reduce the time needed for tasks like client onboarding, trade reconciliation and document review. While the bank has not shared specific performance numbers, people familiar with the matter told news outlets that work which once took a great deal of human labour can now be done in much less time.&lt;/p&gt;&lt;p&gt;Argenti said the rollout is not about replacing human workers, at least not at this stage. The bank reportedly views the agents as a tool to help existing staff manage busy schedules and get through high volumes of work. In areas like compliance and accounting, jobs can involve repetitive, rule-based steps. AI frees analysts from that repetition so they can focus on higher-value judgement work.&lt;/p&gt;&lt;p&gt;Markets have already reacted to the idea that large institutions are moving toward more AI-driven automation. In recent days, a sell-off in enterprise software stocks wiped out billions in value as some investors worried that tools like autonomous agents could speed up the decline of traditional business software that has dominated corporate IT for years.&lt;/p&gt;&lt;h3&gt;AI adoption meets governance reality&lt;/h3&gt;&lt;p&gt;Industry watchers see Goldman’s move as part of a wider trend. For example, some firms are piloting tools to read large data sets, interpret multiple sources of information, and draft investment analysis. These steps show AI making the jump from isolated projects to operational work. Yet the technology raises questions about oversight and trust. AI systems that interpret financial rules and compliance standards must be monitored carefully to avoid errors that could have regulatory or financial consequences. That’s why many institutions treat these systems as helpers that are reviewed by human experts until they mature.&lt;/p&gt;&lt;p&gt;Goldman Sachs is starting with operational functions that have traditionally resisted automation because they involve a lot of data and formal steps. The bank has not said when it expects deployment of the agents in its operations, but executives have suggested that the initial tests have been promising enough to support further rollout.&lt;/p&gt;&lt;p&gt;The broader industry context shows other banks and financial firms also exploring similar use cases. Some have already invested heavily in AI infrastructure, and reports indicate that major firms are planning to use AI to cut costs, speed workflows and improve risk management. However, many remain cautious about putting AI into customer-facing or regulated functions.&lt;/p&gt;&lt;p&gt;Goldman’s push into autonomous AI agents is an example of how large companies are reshaping internal operations using the latest generation of AI models. If systems can handle complex tasks reliably, organisations could see real changes in how work gets done – particularly in back-office functions where volume and repetition keep costs high and innovation slow.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Louis Droege)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Intuit, Uber, and State Farm trial AI agents inside enterprise workflows&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/goldman-sachs-tests-autonomous-ai-agents-for-process-heavy-work/</guid><pubDate>Mon, 09 Feb 2026 10:00:00 +0000</pubDate></item><item><title>[NEW] Cryptocurrency markets a testbed for AI forecasting models (AI News)</title><link>https://www.artificialintelligence-news.com/news/cryptocurrency-markets-a-testbed-for-ai-forecasting-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Picture1.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Cryptocurrency markets have become a high-speed playground where developers optimise the next generation of predictive software. Using real-time data flows and decentralised platforms, scientists develop prediction models that can extend the scope of traditional finance.&lt;/p&gt;&lt;p&gt;The digital asset landscape offers an unparalleled environment for machine learning. When you track cryptocurrency prices today, you are observing a system shaped simultaneously by on-chain transactions, global sentiment signals, and macroeconomic inputs, all of which generate dense datasets suited for advanced neural networks.&lt;/p&gt;&lt;p&gt;Such a steady trickle of information makes it possible to assess and reapply an algorithm without interference from fixed trading times or restrictive market access.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-evolution-of-neural-networks-in-forecasting"&gt;The evolution of neural networks in forecasting&lt;/h3&gt;&lt;p&gt;Current machine learning technology, particularly the “Long Short-Term Memory” neuronal network, has found widespread application in interpreting market behaviour. A recurrent neural network, like an LSTM, can recognise long-term market patterns and is far more flexible than traditional analytical techniques in fluctuating markets.&lt;/p&gt;&lt;p&gt;The research on hybrid models that combine LSTMs with attention mechanisms has really improved techniques for extracting important signals from market noise. Compared to previous models that used linear techniques, these models analyse not only structured price data but also unstructured data.&lt;/p&gt;&lt;p&gt;With the inclusion of Natural Language Processing, it is now possible to interpret the flow of news and social media activity, enabling sentiment measurement. While prediction was previously based on historical stock pricing patterns, it now increasingly depends on behavioural changes in global participant networks.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-a-high-frequency-environment-for-model-validation"&gt;A High-Frequency Environment for Model Validation&lt;/h3&gt;&lt;p&gt;The transparency of blockchain data offers a level of data granularity that is not found in existing financial infrastructures. Each transaction is now an input that can be traced, enabling cause-and-effect analysis without delay.&lt;/p&gt;&lt;p&gt;However, the growing presence of autonomous AI agents has changed how such data is used. This is because specialised platforms are being developed to support decentralised processing in a variety of networks.&lt;/p&gt;&lt;p&gt;This has effectively turned blockchain ecosystems into real-time validation environments, where the feedback loop between data ingestion and model refinement occurs almost instantly.&lt;/p&gt;&lt;p&gt;Researchers use this setting to test specific abilities:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Real-time anomaly detection: Systems compare live transaction flows against simulated historical conditions to identify irregular liquidity behaviour before broader disruptions emerge.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Macro sentiment mapping: Global social behaviour data are compared to on-chain activity to assess true market psychology.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Autonomous risk adjustment: Programmes run probabilistic simulations to rebalance exposure dynamically as volatility thresholds are crossed.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Predictive on-chain monitoring: AI tracks wallet activity to anticipate liquidity shifts before they impact centralised trading venues.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These systems really do not function as isolated instruments. Instead, they adjust dynamically, continually changing their parameters in response to emerging market conditions.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-synergy-of-depin-and-computational-power"&gt;The synergy of DePIN and computational power&lt;/h3&gt;&lt;p&gt;To train complex predictive models, large amounts of computing power are required, leading to the development of Decentralised Physical Infrastructure Networks (DePIN). By using decentralised GPU capacity on a global computing grid, less dependence on cloud infrastructure can be achieved.&lt;/p&gt;&lt;p&gt;Consequently, smaller-scale research teams are afforded computational power that was previously beyond their budgets. This makes it easier and faster to run experiments in different model designs.&lt;/p&gt;&lt;p&gt;This trend is also echoed in the markets. A report dated January 2025 noted strong growth in the capitalisation of assets related to artificial intelligence agents in the latter half of 2024, as demand for such intelligence infrastructure increased.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-reactive-bots-to-anticipatory-agents"&gt;From reactive bots to anticipatory agents&lt;/h3&gt;&lt;p&gt;The market is moving beyond rule-based trading bots toward proactive AI agents. Instead of responding to predefined triggers, modern systems evaluate probability distributions to anticipate directional changes.&lt;/p&gt;&lt;p&gt;Gradient boosting and Bayesian learning methods allow the identification of areas where mean reversion may occur ahead of strong corrections.&lt;/p&gt;&lt;p&gt;Some models now incorporate fractal analysis to detect recurring structures in timeframes, further improving adaptability in rapidly-changing conditions.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-addressing-model-risk-and-infrastructure-constraints"&gt;Addressing model risk and infrastructure constraints&lt;/h3&gt;&lt;p&gt;Despite such rapid progress, several problems remain. Problems identified include hallucinations in models, in which patterns found in a model do not belong to the patterns that cause them. Methods to mitigate this problem have been adopted by those applying this technology, including ‘explainable AI’.&lt;/p&gt;&lt;p&gt;The other vital requirement that has remained unaltered with the evolution in AI technology is scalability. With the growing number of interactions among autonomous agents, it is imperative that the underlying transactions efficiently manage the rising volume without latency or data loss.&lt;/p&gt;&lt;p&gt;At the end of 2024, the most optimal scaling solution handled tens of millions of transactions per day in an area that required improvement.&lt;/p&gt;&lt;p&gt;Such an agile framework lays the foundation for the future, where data, intelligence and validation will come together in a strong ecosystem that facilitates more reliable projections, better governance and greater confidence in AI-driven insights.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Picture1.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Cryptocurrency markets have become a high-speed playground where developers optimise the next generation of predictive software. Using real-time data flows and decentralised platforms, scientists develop prediction models that can extend the scope of traditional finance.&lt;/p&gt;&lt;p&gt;The digital asset landscape offers an unparalleled environment for machine learning. When you track cryptocurrency prices today, you are observing a system shaped simultaneously by on-chain transactions, global sentiment signals, and macroeconomic inputs, all of which generate dense datasets suited for advanced neural networks.&lt;/p&gt;&lt;p&gt;Such a steady trickle of information makes it possible to assess and reapply an algorithm without interference from fixed trading times or restrictive market access.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-evolution-of-neural-networks-in-forecasting"&gt;The evolution of neural networks in forecasting&lt;/h3&gt;&lt;p&gt;Current machine learning technology, particularly the “Long Short-Term Memory” neuronal network, has found widespread application in interpreting market behaviour. A recurrent neural network, like an LSTM, can recognise long-term market patterns and is far more flexible than traditional analytical techniques in fluctuating markets.&lt;/p&gt;&lt;p&gt;The research on hybrid models that combine LSTMs with attention mechanisms has really improved techniques for extracting important signals from market noise. Compared to previous models that used linear techniques, these models analyse not only structured price data but also unstructured data.&lt;/p&gt;&lt;p&gt;With the inclusion of Natural Language Processing, it is now possible to interpret the flow of news and social media activity, enabling sentiment measurement. While prediction was previously based on historical stock pricing patterns, it now increasingly depends on behavioural changes in global participant networks.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-a-high-frequency-environment-for-model-validation"&gt;A High-Frequency Environment for Model Validation&lt;/h3&gt;&lt;p&gt;The transparency of blockchain data offers a level of data granularity that is not found in existing financial infrastructures. Each transaction is now an input that can be traced, enabling cause-and-effect analysis without delay.&lt;/p&gt;&lt;p&gt;However, the growing presence of autonomous AI agents has changed how such data is used. This is because specialised platforms are being developed to support decentralised processing in a variety of networks.&lt;/p&gt;&lt;p&gt;This has effectively turned blockchain ecosystems into real-time validation environments, where the feedback loop between data ingestion and model refinement occurs almost instantly.&lt;/p&gt;&lt;p&gt;Researchers use this setting to test specific abilities:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Real-time anomaly detection: Systems compare live transaction flows against simulated historical conditions to identify irregular liquidity behaviour before broader disruptions emerge.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Macro sentiment mapping: Global social behaviour data are compared to on-chain activity to assess true market psychology.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Autonomous risk adjustment: Programmes run probabilistic simulations to rebalance exposure dynamically as volatility thresholds are crossed.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Predictive on-chain monitoring: AI tracks wallet activity to anticipate liquidity shifts before they impact centralised trading venues.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These systems really do not function as isolated instruments. Instead, they adjust dynamically, continually changing their parameters in response to emerging market conditions.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-synergy-of-depin-and-computational-power"&gt;The synergy of DePIN and computational power&lt;/h3&gt;&lt;p&gt;To train complex predictive models, large amounts of computing power are required, leading to the development of Decentralised Physical Infrastructure Networks (DePIN). By using decentralised GPU capacity on a global computing grid, less dependence on cloud infrastructure can be achieved.&lt;/p&gt;&lt;p&gt;Consequently, smaller-scale research teams are afforded computational power that was previously beyond their budgets. This makes it easier and faster to run experiments in different model designs.&lt;/p&gt;&lt;p&gt;This trend is also echoed in the markets. A report dated January 2025 noted strong growth in the capitalisation of assets related to artificial intelligence agents in the latter half of 2024, as demand for such intelligence infrastructure increased.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-reactive-bots-to-anticipatory-agents"&gt;From reactive bots to anticipatory agents&lt;/h3&gt;&lt;p&gt;The market is moving beyond rule-based trading bots toward proactive AI agents. Instead of responding to predefined triggers, modern systems evaluate probability distributions to anticipate directional changes.&lt;/p&gt;&lt;p&gt;Gradient boosting and Bayesian learning methods allow the identification of areas where mean reversion may occur ahead of strong corrections.&lt;/p&gt;&lt;p&gt;Some models now incorporate fractal analysis to detect recurring structures in timeframes, further improving adaptability in rapidly-changing conditions.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-addressing-model-risk-and-infrastructure-constraints"&gt;Addressing model risk and infrastructure constraints&lt;/h3&gt;&lt;p&gt;Despite such rapid progress, several problems remain. Problems identified include hallucinations in models, in which patterns found in a model do not belong to the patterns that cause them. Methods to mitigate this problem have been adopted by those applying this technology, including ‘explainable AI’.&lt;/p&gt;&lt;p&gt;The other vital requirement that has remained unaltered with the evolution in AI technology is scalability. With the growing number of interactions among autonomous agents, it is imperative that the underlying transactions efficiently manage the rising volume without latency or data loss.&lt;/p&gt;&lt;p&gt;At the end of 2024, the most optimal scaling solution handled tens of millions of transactions per day in an area that required improvement.&lt;/p&gt;&lt;p&gt;Such an agile framework lays the foundation for the future, where data, intelligence and validation will come together in a strong ecosystem that facilitates more reliable projections, better governance and greater confidence in AI-driven insights.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/cryptocurrency-markets-a-testbed-for-ai-forecasting-models/</guid><pubDate>Mon, 09 Feb 2026 10:30:39 +0000</pubDate></item><item><title>[NEW] Exclusive: Why are Chinese AI models dominating open-source as Western labs step back? (AI News)</title><link>https://www.artificialintelligence-news.com/news/chinese-ai-models-175k-unprotected-systems-western-retreat/</link><description>&lt;p&gt;Because Western AI labs&amp;nbsp;won’t—or&amp;nbsp;can’t—anymore. As OpenAI, Anthropic, and Google face mounting pressure to restrict their most powerful models, Chinese developers have filled the open-source void with AI explicitly built for what operators need: powerful models that run on commodity hardware.&lt;/p&gt;&lt;p&gt;A new security&amp;nbsp;study&amp;nbsp;reveals just how thoroughly Chinese AI has captured this space.&amp;nbsp;Research published by SentinelOne and Censys,&amp;nbsp;mapping&amp;nbsp;175,000 exposed AI hosts across 130 countries over 293 days, shows&amp;nbsp;Alibaba’s&amp;nbsp;Qwen2 consistently&amp;nbsp;ranking&amp;nbsp;second only&amp;nbsp;to&amp;nbsp;Meta’s&amp;nbsp;Llama in global deployment.&amp;nbsp;More tellingly, the Chinese model appears on 52% of systems running multiple AI models—suggesting&amp;nbsp;it’s&amp;nbsp;become the de facto alternative to Llama.&lt;/p&gt;&lt;p&gt;“Over the next 12–18 months, we expect Chinese-origin model families to play an increasingly central role in the open-source LLM ecosystem, particularly as Western frontier labs slow or constrain open-weight releases,”&amp;nbsp;Gabriel Bernadett-Shapiro, distinguished AI research scientist at SentinelOne, told TechForge&amp;nbsp;Media’s&amp;nbsp;&lt;em&gt;AI News&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;The finding arrives as OpenAI, Anthropic, and Google face regulatory scrutiny, safety review overhead, and commercial incentives pushing them toward API-gated releases rather than publishing model weights&amp;nbsp;freely.&amp;nbsp;The contrast with Chinese developers&amp;nbsp;couldn’t&amp;nbsp;be sharper.&lt;/p&gt;&lt;p&gt;Chinese labs have demonstrated what Bernadett-Shapiro calls&amp;nbsp;“a willingness to publish large, high-quality weights that&amp;nbsp;are explicitly optimised&amp;nbsp;for local deployment, quantisation, and commodity hardware.”&lt;/p&gt;&lt;p&gt;“In practice, this makes them easier to adopt, easier to run, and easier to integrate into edge and residential environments,”&amp;nbsp;he added.&lt;/p&gt;&lt;p&gt;Put simply: if&amp;nbsp;you’re&amp;nbsp;a researcher or developer wanting to run powerful AI on your own computer without a massive budget, Chinese models like Qwen2 are often your best—or only—option.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-pragmatics-not-ideology"&gt;Pragmatics, not ideology&lt;/h3&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112062" height="719" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Silent_Brothers_Chart_01-12-2026_03-scaled.jpg-1024x719.avif" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Alibaba’s Qwen2 consistently ranks second only to Meta’s Llama across 175,000 exposed hosts globally. Source: SentinelOne/Censys&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The research shows this dominance&amp;nbsp;isn’t&amp;nbsp;accidental. Qwen2 maintains what Bernadett-Shapiro calls&amp;nbsp;“zero rank volatility”—it holds the number two position across every measurement method the researchers examined: total observations, unique hosts, and host-days.&amp;nbsp;There’s&amp;nbsp;no fluctuation, no regional variation, just consistent global adoption.&lt;/p&gt;&lt;p&gt;The co-deployment pattern is equally revealing. When operators run multiple AI models on the same system—a common practice for comparison or workload segmentation—the pairing of Llama and Qwen2 appears on 40,694 hosts, representing 52% of all multi-family deployments.&lt;/p&gt;&lt;p&gt;Geographic concentration reinforces the picture. In China, Beijing alone accounts for 30% of exposed hosts, with Shanghai and Guangdong&amp;nbsp;adding another&amp;nbsp;21% combined.&amp;nbsp;In the United States, Virginia—reflecting AWS infrastructure&amp;nbsp;density—represents 18% of hosts.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112061" height="719" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Silent_Brothers_Chart_01-12-2026_02-scaled.jpg-1024x719.avif" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;China and the US dominate exposed Ollama host distribution, with Beijing accounting for 30% of Chinese deployments. Source: SentinelOne/Censys&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;“If release velocity, openness, and hardware portability continue to diverge between regions, Chinese model lineages are likely to become the default for open deployments, not because of ideology, but because of availability and pragmatics,”&amp;nbsp;Bernadett-Shapiro explained.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-governance-problem"&gt;The governance problem&lt;/h3&gt;&lt;p&gt;This shift creates what Bernadett-Shapiro characterises as a&amp;nbsp;“governance inversion”—a fundamental reversal of how AI risk and accountability&amp;nbsp;are distributed.&lt;/p&gt;&lt;p&gt;In platform-hosted services like ChatGPT, one company controls everything: the infrastructure, monitors usage, implements safety controls, and can shut down&amp;nbsp;abuse. With open-weight models, the control evaporates. Accountability diffuses across thousands of networks in 130 countries, while dependency concentrates upstream in a handful of model suppliers—increasingly Chinese ones.&lt;/p&gt;&lt;p&gt;The 175,000 exposed hosts operate entirely outside the control systems governing commercial AI platforms.&amp;nbsp;There’s&amp;nbsp;no centralised authentication, no rate limiting, no abuse detection, and critically, no kill switch if misuse is detected.&lt;/p&gt;&lt;p&gt;“Once an open-weight model is released, it is trivial to remove safety or security training,”&amp;nbsp;Bernadett-Shapiro noted.”Frontier labs need to treat open-weight releases as long-lived infrastructure artefacts.”&lt;/p&gt;&lt;p&gt;A persistent backbone of 23,000 hosts&amp;nbsp;showing 87%&amp;nbsp;average uptime drives the majority of activity.&amp;nbsp;These&amp;nbsp;aren’t&amp;nbsp;hobbyist experiments—they’re&amp;nbsp;operational systems providing ongoing utility, often running multiple models simultaneously.&lt;/p&gt;&lt;p&gt;Perhaps most concerning:&amp;nbsp;between 16% and 19% of the infrastructure&amp;nbsp;couldn’t&amp;nbsp;be attributed to any identifiable owner.”Even if we are able to prove that&amp;nbsp;a model was leveraged&amp;nbsp;in an attack, there are not well-established abuse reporting routes,”&amp;nbsp;Bernadett-Shapiro said.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-security-without-guardrails"&gt;Security without guardrails&lt;/h3&gt;&lt;p&gt;Nearly half (48%) of exposed hosts advertise&amp;nbsp;“tool-calling capabilities”—meaning&amp;nbsp;they’re&amp;nbsp;not just generating text. They can execute code, access APIs, and interact with external systems autonomously.&lt;/p&gt;&lt;p&gt;“A&amp;nbsp;text-only model can generate harmful content, but a tool-calling model can act,”&amp;nbsp;Bernadett-Shapiro explained.&amp;nbsp;“On an unauthenticated server, an attacker&amp;nbsp;doesn’t&amp;nbsp;need malware or credentials; they just need a prompt.”&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112064" height="719" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Silent_Brothers_Chart_01-12-2026_01-scaled.jpg-1024x719.avif" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Nearly half of exposed Ollama hosts have tool-calling capabilities that can execute code and access external systems. Source: SentinelOne/Censys&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The highest-risk scenario involves what he calls&amp;nbsp;“exposed, tool-enabled RAG or automation endpoints being driven remotely as an execution layer.”&amp;nbsp;An attacker could&amp;nbsp;simply&amp;nbsp;ask the model to summarise internal documents, extract API keys from code repositories, or call downstream services the model&amp;nbsp;is configured&amp;nbsp;to access.&lt;/p&gt;&lt;p&gt;When paired with&amp;nbsp;“thinking”&amp;nbsp;models optimised for multi-step reasoning—present on 26% of hosts—the system can plan complex operations autonomously. The researchers identified at least 201 hosts running&amp;nbsp;“uncensored”&amp;nbsp;configurations that explicitly remove safety guardrails, though Bernadett-Shapiro notes this represents a lower bound.&lt;/p&gt;&lt;p&gt;In other words, these&amp;nbsp;aren’t&amp;nbsp;just chatbots—they’re&amp;nbsp;AI systems that can take action, and half of them have no password protection.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-frontier-labs-should-do"&gt;What frontier labs should do&lt;/h3&gt;&lt;p&gt;For Western AI developers concerned about maintaining influence over the&amp;nbsp;technology’s&amp;nbsp;trajectory, Bernadett-Shapiro recommends a different approach to model releases.&lt;/p&gt;&lt;p&gt;“Frontier labs&amp;nbsp;can’t&amp;nbsp;control deployment, but they can shape the risks that they release into the world,”&amp;nbsp;he said. That includes&amp;nbsp;“investing in post-release monitoring of ecosystem-level adoption and misuse patterns”&amp;nbsp;rather than treating releases as one-off research outputs.&lt;/p&gt;&lt;p&gt;The current governance model assumes centralised deployment with diffuse upstream supply—the exact opposite of&amp;nbsp;what’s&amp;nbsp;actually happening.&amp;nbsp;“When a small number of lineages dominate&amp;nbsp;what’s&amp;nbsp;runnable on commodity hardware, upstream decisions get amplified everywhere,”&amp;nbsp;he explained.&amp;nbsp;“Governance strategies must acknowledge that inversion.”&lt;/p&gt;&lt;p&gt;But acknowledgement requires visibility. Currently, most labs releasing open-weight models have no systematic way to track how&amp;nbsp;they’re&amp;nbsp;being used, where&amp;nbsp;they’re&amp;nbsp;deployed, or whether safety training remains intact after quantisation and fine-tuning.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-12-18-month-outlook"&gt;The 12-18 month outlook&lt;/h3&gt;&lt;p&gt;Bernadett-Shapiro expects the exposed layer to&amp;nbsp;“persist and professionalise”&amp;nbsp;as tool use, agents, and multimodal inputs become default capabilities rather than exceptions.&amp;nbsp;The transient edge will&amp;nbsp;keep churning&amp;nbsp;as hobbyists experiment, but the backbone will&amp;nbsp;grow&amp;nbsp;more stable, more capable, and&amp;nbsp;handle&amp;nbsp;more sensitive data.&lt;/p&gt;&lt;p&gt;Enforcement will remain uneven because residential and small VPS deployments&amp;nbsp;don’t&amp;nbsp;map to existing governance controls.&amp;nbsp;“This&amp;nbsp;isn’t&amp;nbsp;a misconfiguration problem,”&amp;nbsp;he emphasised.&amp;nbsp;“We are observing the early formation of a public, unmanaged AI compute substrate. There is no central switch to flip.”&lt;/p&gt;&lt;p&gt;The geopolitical dimension adds urgency.&amp;nbsp;“When most of the&amp;nbsp;world’s&amp;nbsp;unmanaged AI compute depends on models released by a handful of non-Western labs, traditional assumptions about influence, coordination, and post-release response become weaker,”&amp;nbsp;Bernadett-Shapiro said.&lt;/p&gt;&lt;p&gt;For Western developers and policymakers, the implication is stark:&amp;nbsp;“Even perfect governance of their own platforms has limited impact on the real-world risk surface if the dominant capabilities live elsewhere and propagate through open, decentralised infrastructure.”&lt;/p&gt;&lt;p&gt;The open-source AI ecosystem is globalising, but its centre of gravity is shifting decisively eastward. Not&amp;nbsp;through any coordinated strategy, but through the practical economics of&amp;nbsp;who’s&amp;nbsp;willing to publish what researchers and operators actually need to run AI locally.&lt;/p&gt;&lt;p&gt;The 175,000 exposed hosts mapped in this study are just the visible surface of that fundamental realignment—one that Western policymakers are only beginning to recognise, let alone address.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Huawei details open-source AI development roadmap at Huawei Connect 2025&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Because Western AI labs&amp;nbsp;won’t—or&amp;nbsp;can’t—anymore. As OpenAI, Anthropic, and Google face mounting pressure to restrict their most powerful models, Chinese developers have filled the open-source void with AI explicitly built for what operators need: powerful models that run on commodity hardware.&lt;/p&gt;&lt;p&gt;A new security&amp;nbsp;study&amp;nbsp;reveals just how thoroughly Chinese AI has captured this space.&amp;nbsp;Research published by SentinelOne and Censys,&amp;nbsp;mapping&amp;nbsp;175,000 exposed AI hosts across 130 countries over 293 days, shows&amp;nbsp;Alibaba’s&amp;nbsp;Qwen2 consistently&amp;nbsp;ranking&amp;nbsp;second only&amp;nbsp;to&amp;nbsp;Meta’s&amp;nbsp;Llama in global deployment.&amp;nbsp;More tellingly, the Chinese model appears on 52% of systems running multiple AI models—suggesting&amp;nbsp;it’s&amp;nbsp;become the de facto alternative to Llama.&lt;/p&gt;&lt;p&gt;“Over the next 12–18 months, we expect Chinese-origin model families to play an increasingly central role in the open-source LLM ecosystem, particularly as Western frontier labs slow or constrain open-weight releases,”&amp;nbsp;Gabriel Bernadett-Shapiro, distinguished AI research scientist at SentinelOne, told TechForge&amp;nbsp;Media’s&amp;nbsp;&lt;em&gt;AI News&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;The finding arrives as OpenAI, Anthropic, and Google face regulatory scrutiny, safety review overhead, and commercial incentives pushing them toward API-gated releases rather than publishing model weights&amp;nbsp;freely.&amp;nbsp;The contrast with Chinese developers&amp;nbsp;couldn’t&amp;nbsp;be sharper.&lt;/p&gt;&lt;p&gt;Chinese labs have demonstrated what Bernadett-Shapiro calls&amp;nbsp;“a willingness to publish large, high-quality weights that&amp;nbsp;are explicitly optimised&amp;nbsp;for local deployment, quantisation, and commodity hardware.”&lt;/p&gt;&lt;p&gt;“In practice, this makes them easier to adopt, easier to run, and easier to integrate into edge and residential environments,”&amp;nbsp;he added.&lt;/p&gt;&lt;p&gt;Put simply: if&amp;nbsp;you’re&amp;nbsp;a researcher or developer wanting to run powerful AI on your own computer without a massive budget, Chinese models like Qwen2 are often your best—or only—option.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-pragmatics-not-ideology"&gt;Pragmatics, not ideology&lt;/h3&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112062" height="719" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Silent_Brothers_Chart_01-12-2026_03-scaled.jpg-1024x719.avif" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Alibaba’s Qwen2 consistently ranks second only to Meta’s Llama across 175,000 exposed hosts globally. Source: SentinelOne/Censys&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The research shows this dominance&amp;nbsp;isn’t&amp;nbsp;accidental. Qwen2 maintains what Bernadett-Shapiro calls&amp;nbsp;“zero rank volatility”—it holds the number two position across every measurement method the researchers examined: total observations, unique hosts, and host-days.&amp;nbsp;There’s&amp;nbsp;no fluctuation, no regional variation, just consistent global adoption.&lt;/p&gt;&lt;p&gt;The co-deployment pattern is equally revealing. When operators run multiple AI models on the same system—a common practice for comparison or workload segmentation—the pairing of Llama and Qwen2 appears on 40,694 hosts, representing 52% of all multi-family deployments.&lt;/p&gt;&lt;p&gt;Geographic concentration reinforces the picture. In China, Beijing alone accounts for 30% of exposed hosts, with Shanghai and Guangdong&amp;nbsp;adding another&amp;nbsp;21% combined.&amp;nbsp;In the United States, Virginia—reflecting AWS infrastructure&amp;nbsp;density—represents 18% of hosts.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112061" height="719" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Silent_Brothers_Chart_01-12-2026_02-scaled.jpg-1024x719.avif" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;China and the US dominate exposed Ollama host distribution, with Beijing accounting for 30% of Chinese deployments. Source: SentinelOne/Censys&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;“If release velocity, openness, and hardware portability continue to diverge between regions, Chinese model lineages are likely to become the default for open deployments, not because of ideology, but because of availability and pragmatics,”&amp;nbsp;Bernadett-Shapiro explained.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-governance-problem"&gt;The governance problem&lt;/h3&gt;&lt;p&gt;This shift creates what Bernadett-Shapiro characterises as a&amp;nbsp;“governance inversion”—a fundamental reversal of how AI risk and accountability&amp;nbsp;are distributed.&lt;/p&gt;&lt;p&gt;In platform-hosted services like ChatGPT, one company controls everything: the infrastructure, monitors usage, implements safety controls, and can shut down&amp;nbsp;abuse. With open-weight models, the control evaporates. Accountability diffuses across thousands of networks in 130 countries, while dependency concentrates upstream in a handful of model suppliers—increasingly Chinese ones.&lt;/p&gt;&lt;p&gt;The 175,000 exposed hosts operate entirely outside the control systems governing commercial AI platforms.&amp;nbsp;There’s&amp;nbsp;no centralised authentication, no rate limiting, no abuse detection, and critically, no kill switch if misuse is detected.&lt;/p&gt;&lt;p&gt;“Once an open-weight model is released, it is trivial to remove safety or security training,”&amp;nbsp;Bernadett-Shapiro noted.”Frontier labs need to treat open-weight releases as long-lived infrastructure artefacts.”&lt;/p&gt;&lt;p&gt;A persistent backbone of 23,000 hosts&amp;nbsp;showing 87%&amp;nbsp;average uptime drives the majority of activity.&amp;nbsp;These&amp;nbsp;aren’t&amp;nbsp;hobbyist experiments—they’re&amp;nbsp;operational systems providing ongoing utility, often running multiple models simultaneously.&lt;/p&gt;&lt;p&gt;Perhaps most concerning:&amp;nbsp;between 16% and 19% of the infrastructure&amp;nbsp;couldn’t&amp;nbsp;be attributed to any identifiable owner.”Even if we are able to prove that&amp;nbsp;a model was leveraged&amp;nbsp;in an attack, there are not well-established abuse reporting routes,”&amp;nbsp;Bernadett-Shapiro said.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-security-without-guardrails"&gt;Security without guardrails&lt;/h3&gt;&lt;p&gt;Nearly half (48%) of exposed hosts advertise&amp;nbsp;“tool-calling capabilities”—meaning&amp;nbsp;they’re&amp;nbsp;not just generating text. They can execute code, access APIs, and interact with external systems autonomously.&lt;/p&gt;&lt;p&gt;“A&amp;nbsp;text-only model can generate harmful content, but a tool-calling model can act,”&amp;nbsp;Bernadett-Shapiro explained.&amp;nbsp;“On an unauthenticated server, an attacker&amp;nbsp;doesn’t&amp;nbsp;need malware or credentials; they just need a prompt.”&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112064" height="719" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Silent_Brothers_Chart_01-12-2026_01-scaled.jpg-1024x719.avif" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Nearly half of exposed Ollama hosts have tool-calling capabilities that can execute code and access external systems. Source: SentinelOne/Censys&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The highest-risk scenario involves what he calls&amp;nbsp;“exposed, tool-enabled RAG or automation endpoints being driven remotely as an execution layer.”&amp;nbsp;An attacker could&amp;nbsp;simply&amp;nbsp;ask the model to summarise internal documents, extract API keys from code repositories, or call downstream services the model&amp;nbsp;is configured&amp;nbsp;to access.&lt;/p&gt;&lt;p&gt;When paired with&amp;nbsp;“thinking”&amp;nbsp;models optimised for multi-step reasoning—present on 26% of hosts—the system can plan complex operations autonomously. The researchers identified at least 201 hosts running&amp;nbsp;“uncensored”&amp;nbsp;configurations that explicitly remove safety guardrails, though Bernadett-Shapiro notes this represents a lower bound.&lt;/p&gt;&lt;p&gt;In other words, these&amp;nbsp;aren’t&amp;nbsp;just chatbots—they’re&amp;nbsp;AI systems that can take action, and half of them have no password protection.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-frontier-labs-should-do"&gt;What frontier labs should do&lt;/h3&gt;&lt;p&gt;For Western AI developers concerned about maintaining influence over the&amp;nbsp;technology’s&amp;nbsp;trajectory, Bernadett-Shapiro recommends a different approach to model releases.&lt;/p&gt;&lt;p&gt;“Frontier labs&amp;nbsp;can’t&amp;nbsp;control deployment, but they can shape the risks that they release into the world,”&amp;nbsp;he said. That includes&amp;nbsp;“investing in post-release monitoring of ecosystem-level adoption and misuse patterns”&amp;nbsp;rather than treating releases as one-off research outputs.&lt;/p&gt;&lt;p&gt;The current governance model assumes centralised deployment with diffuse upstream supply—the exact opposite of&amp;nbsp;what’s&amp;nbsp;actually happening.&amp;nbsp;“When a small number of lineages dominate&amp;nbsp;what’s&amp;nbsp;runnable on commodity hardware, upstream decisions get amplified everywhere,”&amp;nbsp;he explained.&amp;nbsp;“Governance strategies must acknowledge that inversion.”&lt;/p&gt;&lt;p&gt;But acknowledgement requires visibility. Currently, most labs releasing open-weight models have no systematic way to track how&amp;nbsp;they’re&amp;nbsp;being used, where&amp;nbsp;they’re&amp;nbsp;deployed, or whether safety training remains intact after quantisation and fine-tuning.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-12-18-month-outlook"&gt;The 12-18 month outlook&lt;/h3&gt;&lt;p&gt;Bernadett-Shapiro expects the exposed layer to&amp;nbsp;“persist and professionalise”&amp;nbsp;as tool use, agents, and multimodal inputs become default capabilities rather than exceptions.&amp;nbsp;The transient edge will&amp;nbsp;keep churning&amp;nbsp;as hobbyists experiment, but the backbone will&amp;nbsp;grow&amp;nbsp;more stable, more capable, and&amp;nbsp;handle&amp;nbsp;more sensitive data.&lt;/p&gt;&lt;p&gt;Enforcement will remain uneven because residential and small VPS deployments&amp;nbsp;don’t&amp;nbsp;map to existing governance controls.&amp;nbsp;“This&amp;nbsp;isn’t&amp;nbsp;a misconfiguration problem,”&amp;nbsp;he emphasised.&amp;nbsp;“We are observing the early formation of a public, unmanaged AI compute substrate. There is no central switch to flip.”&lt;/p&gt;&lt;p&gt;The geopolitical dimension adds urgency.&amp;nbsp;“When most of the&amp;nbsp;world’s&amp;nbsp;unmanaged AI compute depends on models released by a handful of non-Western labs, traditional assumptions about influence, coordination, and post-release response become weaker,”&amp;nbsp;Bernadett-Shapiro said.&lt;/p&gt;&lt;p&gt;For Western developers and policymakers, the implication is stark:&amp;nbsp;“Even perfect governance of their own platforms has limited impact on the real-world risk surface if the dominant capabilities live elsewhere and propagate through open, decentralised infrastructure.”&lt;/p&gt;&lt;p&gt;The open-source AI ecosystem is globalising, but its centre of gravity is shifting decisively eastward. Not&amp;nbsp;through any coordinated strategy, but through the practical economics of&amp;nbsp;who’s&amp;nbsp;willing to publish what researchers and operators actually need to run AI locally.&lt;/p&gt;&lt;p&gt;The 175,000 exposed hosts mapped in this study are just the visible surface of that fundamental realignment—one that Western policymakers are only beginning to recognise, let alone address.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Huawei details open-source AI development roadmap at Huawei Connect 2025&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/chinese-ai-models-175k-unprotected-systems-western-retreat/</guid><pubDate>Mon, 09 Feb 2026 11:00:00 +0000</pubDate></item><item><title>[NEW] What AI can (and can’t) tell us about XRP in ETF-driven markets (AI News)</title><link>https://www.artificialintelligence-news.com/news/what-ai-can-and-cant-tell-us-about-xrp-in-etf-driven-markets/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/steve-johnson-_0iV9LmPDn0-unsplash-5-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;For a long time, cryptocurrency prices moved quickly. A headline would hit, sentiment would spike, and charts would react almost immediately. That pattern no longer holds. Today’s market is slow, heavier than before, and shaped by forces that do not always announce themselves clearly. Capital allocation, ETF mechanics, and macro positioning now influence price behaviour in ways that are easy to overlook if you only watch short-term moves.&lt;/p&gt;&lt;p&gt;That change becomes obvious when you look at XRP. The XRP price today reflects decisions made by institutions, fund managers, and regulators as much as it reflects trading activity. AI tools are used increasingly to track such inputs – but they are often misunderstood. They do not predict outcomes. They organise complexity.&lt;/p&gt;&lt;p&gt;Understanding that distinction changes how you read the market.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-ai-reads-an-etf-driven-market"&gt;How AI reads an ETF-driven market&lt;/h3&gt;&lt;p&gt;AI systems do not look for narratives, but for relationships. In cryptocurrency markets, that means mapping ETF inflows and outflows against derivatives positioning, on-chain activity, and movements in traditional assets. What has changed recently is how much weight those signals now carry.&lt;/p&gt;&lt;p&gt;Binance Research has reported that altcoin ETFs have recorded more than US$2 billion in net inflows, with XRP and Solana leading that activity. Bitcoin and Ethereum spot ETFs have seen sustained outflows since October. This is not a classic risk-on environment. It is selective, cautious and uneven.&lt;/p&gt;&lt;p&gt;AI models are good at identifying such behaviour, detecting rotation not momentum. They highlight where capital is reallocating even when prices remain range-bound. This is why markets can appear quiet while meaningful positioning takes place underneath.&lt;/p&gt;&lt;p&gt;AI only shows the movement, yet doesn’t explain the reasons behind it.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-ai-can-tell-you-about-xrp"&gt;What AI can tell you about XRP&lt;/h3&gt;&lt;p&gt;XRP does not always move in step with the rest of the market. When conditions change, its price often reacts to access, regulation, and liquidity before sentiment catches up. That pattern has shown up more than once, and it is one reason AI systems tend to weigh fund flows and market depth more heavily than short-term mood shifts when analysing XRP.&lt;/p&gt;&lt;p&gt;Binance Research has pointed to early 2026 as a period where liquidity is coming back without a clear return to risk-taking. Capital has rotated away from crowded trades, but it has not rushed to replace them. AI picks up on that imbalance quickly. It helps explain why XRP has seen ETF interest even while broader momentum in cryptocurrency has felt restrained.&lt;/p&gt;&lt;p&gt;That does not imply a forecast. It is closer to a snapshot of conditions. Market conversations may slow, headlines may thin out, and price can drift, yet positioning continues to evolve in the background. This is easy to miss if you focus only on visible activity.&lt;/p&gt;&lt;p&gt;AI is useful here because it stays indifferent to attention. Instead of responding to engagement spikes or sudden narrative shifts, it tracks what investors are actually doing. In markets where perception often moves ahead of reality, that distinction matters more than it first appears.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-where-ai-constantly-falls-short"&gt;Where AI constantly falls short&lt;/h3&gt;&lt;p&gt;For all its analytical power, AI has blind spots. Regulation is one of the most important. Models are trained on historical relationships, while regulatory decisions rarely follow historical patterns.&lt;/p&gt;&lt;p&gt;Richard Teng, Co-CEO of Binance, addressed this challenge after the exchange secured its ADGM license in January 2026. “The ADGM license crowns years of work to meet some of the world’s most demanding regulatory standards, and arriving in days of the moment we crossed 300 million registered users shows that scale and trust need not be in tension.” Developments like this can alter market confidence quickly, yet they are difficult to quantify before they happen.&lt;/p&gt;&lt;p&gt;AI responds well once regulatory outcomes are known. It struggles beforehand. For XRP, where regulatory clarity has played a central role in past price behaviour, this limitation is significant.&lt;/p&gt;&lt;p&gt;Another weakness is intent. AI can measure flows, but it cannot explain why investors choose caution, delay, or restraint. Defensive positioning does not always look dramatic in data, but it can shape markets for long periods.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-human-judgement-still-shapes-the-outcome"&gt;Why human judgement still shapes the outcome&lt;/h3&gt;&lt;p&gt;AI does not replace interpretation but supports it. Binance Research has described current conditions as a phase of liquidity preservation, with markets waiting for clearer catalysts like macro data releases and policy signals. AI can flag these moments of tension. It cannot tell you whether they will resolve into action or extend into stagnation.&lt;/p&gt;&lt;p&gt;Rachel Conlan, CMO of Binance, reflected on the broader maturity of the industry when discussing Binance Blockchain Week Dubai 2025. She described a market that is more focused on building than spectacle. That mindset applies equally to AI use. The goal is not prediction. It is informed judgement.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-this-means-when-you-look-at-price"&gt;What this means when you look at price&lt;/h3&gt;&lt;p&gt;When used properly, AI helps see forces that are easy to miss, especially in ETF-driven conditions. It highlights where liquidity is moving, where narratives fail to align with behaviour, and where patience may be a rational choice.&lt;/p&gt;&lt;p&gt;What it cannot do is remove uncertainty. In markets shaped by regulation, macro shifts, and institutional decision-making, judgement still matters. The clearest insight comes from combining machine analysis with human context.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Image source: Unsplash&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/steve-johnson-_0iV9LmPDn0-unsplash-5-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;For a long time, cryptocurrency prices moved quickly. A headline would hit, sentiment would spike, and charts would react almost immediately. That pattern no longer holds. Today’s market is slow, heavier than before, and shaped by forces that do not always announce themselves clearly. Capital allocation, ETF mechanics, and macro positioning now influence price behaviour in ways that are easy to overlook if you only watch short-term moves.&lt;/p&gt;&lt;p&gt;That change becomes obvious when you look at XRP. The XRP price today reflects decisions made by institutions, fund managers, and regulators as much as it reflects trading activity. AI tools are used increasingly to track such inputs – but they are often misunderstood. They do not predict outcomes. They organise complexity.&lt;/p&gt;&lt;p&gt;Understanding that distinction changes how you read the market.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-ai-reads-an-etf-driven-market"&gt;How AI reads an ETF-driven market&lt;/h3&gt;&lt;p&gt;AI systems do not look for narratives, but for relationships. In cryptocurrency markets, that means mapping ETF inflows and outflows against derivatives positioning, on-chain activity, and movements in traditional assets. What has changed recently is how much weight those signals now carry.&lt;/p&gt;&lt;p&gt;Binance Research has reported that altcoin ETFs have recorded more than US$2 billion in net inflows, with XRP and Solana leading that activity. Bitcoin and Ethereum spot ETFs have seen sustained outflows since October. This is not a classic risk-on environment. It is selective, cautious and uneven.&lt;/p&gt;&lt;p&gt;AI models are good at identifying such behaviour, detecting rotation not momentum. They highlight where capital is reallocating even when prices remain range-bound. This is why markets can appear quiet while meaningful positioning takes place underneath.&lt;/p&gt;&lt;p&gt;AI only shows the movement, yet doesn’t explain the reasons behind it.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-ai-can-tell-you-about-xrp"&gt;What AI can tell you about XRP&lt;/h3&gt;&lt;p&gt;XRP does not always move in step with the rest of the market. When conditions change, its price often reacts to access, regulation, and liquidity before sentiment catches up. That pattern has shown up more than once, and it is one reason AI systems tend to weigh fund flows and market depth more heavily than short-term mood shifts when analysing XRP.&lt;/p&gt;&lt;p&gt;Binance Research has pointed to early 2026 as a period where liquidity is coming back without a clear return to risk-taking. Capital has rotated away from crowded trades, but it has not rushed to replace them. AI picks up on that imbalance quickly. It helps explain why XRP has seen ETF interest even while broader momentum in cryptocurrency has felt restrained.&lt;/p&gt;&lt;p&gt;That does not imply a forecast. It is closer to a snapshot of conditions. Market conversations may slow, headlines may thin out, and price can drift, yet positioning continues to evolve in the background. This is easy to miss if you focus only on visible activity.&lt;/p&gt;&lt;p&gt;AI is useful here because it stays indifferent to attention. Instead of responding to engagement spikes or sudden narrative shifts, it tracks what investors are actually doing. In markets where perception often moves ahead of reality, that distinction matters more than it first appears.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-where-ai-constantly-falls-short"&gt;Where AI constantly falls short&lt;/h3&gt;&lt;p&gt;For all its analytical power, AI has blind spots. Regulation is one of the most important. Models are trained on historical relationships, while regulatory decisions rarely follow historical patterns.&lt;/p&gt;&lt;p&gt;Richard Teng, Co-CEO of Binance, addressed this challenge after the exchange secured its ADGM license in January 2026. “The ADGM license crowns years of work to meet some of the world’s most demanding regulatory standards, and arriving in days of the moment we crossed 300 million registered users shows that scale and trust need not be in tension.” Developments like this can alter market confidence quickly, yet they are difficult to quantify before they happen.&lt;/p&gt;&lt;p&gt;AI responds well once regulatory outcomes are known. It struggles beforehand. For XRP, where regulatory clarity has played a central role in past price behaviour, this limitation is significant.&lt;/p&gt;&lt;p&gt;Another weakness is intent. AI can measure flows, but it cannot explain why investors choose caution, delay, or restraint. Defensive positioning does not always look dramatic in data, but it can shape markets for long periods.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-human-judgement-still-shapes-the-outcome"&gt;Why human judgement still shapes the outcome&lt;/h3&gt;&lt;p&gt;AI does not replace interpretation but supports it. Binance Research has described current conditions as a phase of liquidity preservation, with markets waiting for clearer catalysts like macro data releases and policy signals. AI can flag these moments of tension. It cannot tell you whether they will resolve into action or extend into stagnation.&lt;/p&gt;&lt;p&gt;Rachel Conlan, CMO of Binance, reflected on the broader maturity of the industry when discussing Binance Blockchain Week Dubai 2025. She described a market that is more focused on building than spectacle. That mindset applies equally to AI use. The goal is not prediction. It is informed judgement.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-this-means-when-you-look-at-price"&gt;What this means when you look at price&lt;/h3&gt;&lt;p&gt;When used properly, AI helps see forces that are easy to miss, especially in ETF-driven conditions. It highlights where liquidity is moving, where narratives fail to align with behaviour, and where patience may be a rational choice.&lt;/p&gt;&lt;p&gt;What it cannot do is remove uncertainty. In markets shaped by regulation, macro shifts, and institutional decision-making, judgement still matters. The clearest insight comes from combining machine analysis with human context.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Image source: Unsplash&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/what-ai-can-and-cant-tell-us-about-xrp-in-etf-driven-markets/</guid><pubDate>Mon, 09 Feb 2026 11:04:32 +0000</pubDate></item><item><title>[NEW] Making AI Work, MIT Technology Review’s new AI newsletter, is here (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/09/1132462/ai-newsletter-professional-applications/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MIT-TR-Making-AI-Work-Email-2-C.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For years, our newsroom has explored AI’s limitations and potential dangers, as well as its growing energy needs. And our reporters have looked closely at how generative tools are being used for tasks such as coding and running scientific experiments.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But how is AI &lt;em&gt;actually&lt;/em&gt; being used in fields like health care, climate tech, education, and finance? How are small businesses using it? And what should you keep in mind if you use AI tools at work?&amp;nbsp;These questions guided the creation of Making AI Work, a new AI mini-course newsletter. &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;Sign up for Making AI Work&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;to see weekly case studies exploring tools and tips for AI implementation. The limited-run newsletter will deliver practical, industry-specific guidance on how generative AI is being used and deployed across sectors and what professionals need to know to apply it in their everyday work. The goal is to help working professionals more clearly see how AI is actually being used today, and what that looks like in practice—including new challenges it presents.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;You can sign up at any time and you’ll receive seven editions, delivered once per week, until you complete the series.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Each newsletter begins with a case study, examining a specific use case of AI in a given industry. Then we’ll take a deeper look at the AI tool being used, with more context about how other companies or sectors are employing that same tool or system. Finally, we’ll end with action-oriented tips to help you apply the tool.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Here’s a closer look at what we’ll cover:&lt;br /&gt;&lt;/p&gt; 
 &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 1: How AI is changing health care&amp;nbsp;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Explore the future of medical note-taking by learning about the Microsoft Copilot tool used by doctors at Vanderbilt University Medical Center.&amp;nbsp;&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 2: How AI could power up the nuclear industry&amp;nbsp;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Dig into an experiment between Google and the nuclear giant Westinghouse to see if AI can help build nuclear reactors more efficiently.&amp;nbsp;&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 3: How to encourage smarter AI use in the classroom&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Visit a private high school in Connecticut and meet a technology coordinator who will get you up to speed on MagicSchool, an AI-powered platform for educators.&amp;nbsp;&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 4: How small businesses can leverage AI&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Hear from an independent tutor on how he’s outsourcing basic administrative tasks to Notion AI.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 5: How AI is helping financial firms make better investments&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Learn more about the ways financial firms are using large language models like ChatGPT Enterprise to supercharge their research operations.&amp;nbsp;&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 6: How to use AI yourself&amp;nbsp;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;We’ll share some insights from the staff of &lt;em&gt;MIT Technology Review&lt;/em&gt; about how you might use AI tools powered by LLMs in your own life and work.&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 7: 5 ways people are getting AI right&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;The series ends with an on-demand virtual event featuring expert guests exploring what AI adoptions are working, and why. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;If you’re not quite ready to jump into Making AI Work, then check out Intro to AI, &lt;em&gt;MIT Technology Review&lt;/em&gt;’s first AI newsletter mini-course, which serves as a beginner’s guide to artificial intelligence. Readers will learn the basics of what AI is, how it’s used, what the current regulatory landscape looks like, and more. &lt;strong&gt;Sign up to receive Intro to AI for free.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Our hope is that Making AI Work will help you understand how AI can, well, work for you. &lt;strong&gt;Sign up for Making AI Work to learn how LLMs are being put to work across industries.&amp;nbsp;&lt;/strong&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/MIT-TR-Making-AI-Work-Email-2-C.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For years, our newsroom has explored AI’s limitations and potential dangers, as well as its growing energy needs. And our reporters have looked closely at how generative tools are being used for tasks such as coding and running scientific experiments.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But how is AI &lt;em&gt;actually&lt;/em&gt; being used in fields like health care, climate tech, education, and finance? How are small businesses using it? And what should you keep in mind if you use AI tools at work?&amp;nbsp;These questions guided the creation of Making AI Work, a new AI mini-course newsletter. &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;Sign up for Making AI Work&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;to see weekly case studies exploring tools and tips for AI implementation. The limited-run newsletter will deliver practical, industry-specific guidance on how generative AI is being used and deployed across sectors and what professionals need to know to apply it in their everyday work. The goal is to help working professionals more clearly see how AI is actually being used today, and what that looks like in practice—including new challenges it presents.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;You can sign up at any time and you’ll receive seven editions, delivered once per week, until you complete the series.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Each newsletter begins with a case study, examining a specific use case of AI in a given industry. Then we’ll take a deeper look at the AI tool being used, with more context about how other companies or sectors are employing that same tool or system. Finally, we’ll end with action-oriented tips to help you apply the tool.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Here’s a closer look at what we’ll cover:&lt;br /&gt;&lt;/p&gt; 
 &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 1: How AI is changing health care&amp;nbsp;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Explore the future of medical note-taking by learning about the Microsoft Copilot tool used by doctors at Vanderbilt University Medical Center.&amp;nbsp;&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 2: How AI could power up the nuclear industry&amp;nbsp;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Dig into an experiment between Google and the nuclear giant Westinghouse to see if AI can help build nuclear reactors more efficiently.&amp;nbsp;&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 3: How to encourage smarter AI use in the classroom&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Visit a private high school in Connecticut and meet a technology coordinator who will get you up to speed on MagicSchool, an AI-powered platform for educators.&amp;nbsp;&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 4: How small businesses can leverage AI&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Hear from an independent tutor on how he’s outsourcing basic administrative tasks to Notion AI.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 5: How AI is helping financial firms make better investments&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Learn more about the ways financial firms are using large language models like ChatGPT Enterprise to supercharge their research operations.&amp;nbsp;&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 6: How to use AI yourself&amp;nbsp;&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;We’ll share some insights from the staff of &lt;em&gt;MIT Technology Review&lt;/em&gt; about how you might use AI tools powered by LLMs in your own life and work.&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;&lt;strong&gt;Week 7: 5 ways people are getting AI right&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;The series ends with an on-demand virtual event featuring expert guests exploring what AI adoptions are working, and why. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;If you’re not quite ready to jump into Making AI Work, then check out Intro to AI, &lt;em&gt;MIT Technology Review&lt;/em&gt;’s first AI newsletter mini-course, which serves as a beginner’s guide to artificial intelligence. Readers will learn the basics of what AI is, how it’s used, what the current regulatory landscape looks like, and more. &lt;strong&gt;Sign up to receive Intro to AI for free.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Our hope is that Making AI Work will help you understand how AI can, well, work for you. &lt;strong&gt;Sign up for Making AI Work to learn how LLMs are being put to work across industries.&amp;nbsp;&lt;/strong&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/09/1132462/ai-newsletter-professional-applications/</guid><pubDate>Mon, 09 Feb 2026 11:30:00 +0000</pubDate></item><item><title>[NEW] The Download: what Moltbook tells us about AI hype, and the rise and rise of AI therapy (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/09/1132498/the-download-what-moltbook-tells-us-about-ai-hype-and-the-rise-and-rise-of-ai-therapy/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Moltbook was peak AI theater&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;For a few days recently, the hottest new hangout on the internet was a vibe-coded Reddit clone called Moltbook, which billed itself as a social network for bots. As the website’s tagline puts it: “Where AI agents share, discuss, and upvote. Humans welcome to observe.”&lt;/p&gt;  &lt;p&gt;We observed! Launched on January 28, Moltbook went viral in a matter of hours. It’s been designed as a place where instances of a free open-source LLM-powered agent known as OpenClaw (formerly known as ClawdBot, then Moltbot), could come together and do whatever they wanted.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;But is Moltbook really a glimpse of the future, as many have claimed? Or something else entirely? Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The ascent of the AI therapist&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We’re in the midst of a global mental-­health crisis. More than a billion people worldwide suffer from a mental-health condition, according to the World Health Organization. The prevalence of anxiety and depression is growing in many demographics, particularly young people, and suicide is claiming hundreds of thousands of lives globally each year.&lt;/p&gt;&lt;p&gt;Given the clear demand for accessible and affordable mental-health services, it’s no wonder that people have looked to artificial intelligence for possible relief. Millions are already actively seeking therapy from popular chatbots, or from specialized psychology apps like Wysa and Woebot.&lt;/p&gt;&lt;p&gt;Four timely new books are a reminder that while the present feels like a blur of breakthroughs, scandals, and confusion, this disorienting time is rooted in deeper histories of care, technology, and trust.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Becky Ferreira&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the &lt;/strong&gt;&lt;strong&gt;most recent print issue of&lt;em&gt; MIT Technology Review&lt;/em&gt; magazin&lt;/strong&gt;&lt;strong&gt;e, which shines a light on the exciting innovations happening right now. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Making AI Work, MIT Technology Review’s new AI newsletter, is here&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;For years, our newsroom has explored AI’s limitations and potential dangers, as well as its growing energy needs. And our reporters have looked closely at how generative tools are being used for tasks such as coding and running scientific experiments.&lt;/p&gt;&lt;p&gt;But how is AI &lt;em&gt;actually&lt;/em&gt; being used in fields like health care, climate tech, education, and finance? How are small businesses using it? And what should you keep in mind if you use AI tools at work? These questions guided the creation of Making AI Work, a new AI mini-course newsletter. Read more about it, and sign up here to receive the seven editions straight to your inbox.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The US is failing to punish polluters&lt;/strong&gt;&lt;br /&gt;The number of civil lawsuits it’s pursuing has sharply dropped in comparison to Trump’s first term. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;Rising GDP = greater carbon emissions. But does it have to? &lt;/em&gt;(The Guardian)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The European Union has warned Meta against blocking rival AI assistants&lt;br /&gt;&lt;/strong&gt;It’s the latest example of Brussels’ attempts to rein in Big Tech. (Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 AI ads took over the Super Bowl&lt;/strong&gt;&lt;br /&gt;Hyping up chatbots and taking swipes at their competitors. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;They appeared to be trying to win over AI naysayers, too. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;Celebrities were out in force to flog AI wares. &lt;/em&gt;(Slate $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;4 &lt;strong&gt;China wants to completely dominate the humanoid robot industry&lt;/strong&gt;&lt;br /&gt;Local governments and banks are only too happy to oblige promising startups. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Why the humanoid workforce is running late. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 We’re witnessing the first real crypto crash&lt;br /&gt;Cryptocurrency is now fully part of the financial system, for better or worse. (NY Mag $)&lt;br /&gt;+ &lt;em&gt;Wall Street’s grasp of AI is pretty shaky too. &lt;/em&gt;(Semafor)&lt;br /&gt;+ &lt;em&gt;Even traditionally safe markets are looking pretty volatile right now. &lt;/em&gt;(Economist $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;6 The man who coined vibe coding has a new fixation&lt;/strong&gt;&amp;nbsp;&lt;br /&gt;“Agentic engineering” is the next big thing, apparently. (Insider $)&lt;br /&gt;+ &lt;em&gt;Agentic AI is the talk of the town right now. &lt;/em&gt;(The Information $)&lt;br /&gt;+ &lt;em&gt;What is vibe coding, exactly? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 AI running app Runna has adjusted its aggressive training plans &lt;/strong&gt;&lt;strong&gt;🏃‍♂️&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Runners had long suspected its suggestions were pushing them towards injury. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 San Francisco’s march for billionaires was a flop&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Only around three dozen supporters turned up. (SF Chronicle)&lt;br /&gt;+ &lt;em&gt;Predictably, journalists nearly outnumbered the demonstrators. &lt;/em&gt;(TechCrunch)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 AI is shaking up romance novels ❤️&lt;/strong&gt;&lt;br /&gt;But models still aren’t great at writing sex scenes. (NYT $)&lt;br /&gt;+ &lt;em&gt;It’s surprisingly easy to stumble into a relationship with an AI chatbot. &lt;/em&gt;(MIT Technology Review)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 ChatGPT won’t be replacing human stylists any time soon&lt;/strong&gt;&lt;br /&gt;Its menswear suggestions are more manosphere influencer than suave gentleman. (GQ)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“There is no Plan B, because that assumes you will fail. We’re going to do the start-up thing until we die.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—William Alexander, an ambitious 21-year old AI worker, explains his and his cohort’s attitudes towards trying to make it big in the highly-competitive industry to the New York Times.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1132502" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_14d65b.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The open-source AI boom is built on Big Tech’s handouts. How long will it last?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In May 2023 a leaked memo reported to have been written by Luke Sernau, a senior engineer at Google, said out loud what many in Silicon Valley must have been whispering for weeks: an open-source free-for-all is threatening Big Tech’s grip on AI.&lt;/p&gt;  &lt;p&gt;In many ways, that’s a good thing. AI won't thrive if just a few mega-rich companies get to gatekeep this technology or decide how it is used. But this open-source boom is precarious, and if Big Tech decides to shut up shop, a boomtown could become a backwater. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Dark showering, anyone?&lt;br /&gt;+ Chef Yujia Hu is renowned for his shoe-shaped sushi designs.&lt;br /&gt;+ Meanwhile, in the depths of the South Atlantic Ocean: a giant phantom jelly has been spotted.&lt;br /&gt;+ I have nothing but respect for this X account dedicated to documenting rats and mice in movies and TV 🐀🐁&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Moltbook was peak AI theater&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;For a few days recently, the hottest new hangout on the internet was a vibe-coded Reddit clone called Moltbook, which billed itself as a social network for bots. As the website’s tagline puts it: “Where AI agents share, discuss, and upvote. Humans welcome to observe.”&lt;/p&gt;  &lt;p&gt;We observed! Launched on January 28, Moltbook went viral in a matter of hours. It’s been designed as a place where instances of a free open-source LLM-powered agent known as OpenClaw (formerly known as ClawdBot, then Moltbot), could come together and do whatever they wanted.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;But is Moltbook really a glimpse of the future, as many have claimed? Or something else entirely? Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The ascent of the AI therapist&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We’re in the midst of a global mental-­health crisis. More than a billion people worldwide suffer from a mental-health condition, according to the World Health Organization. The prevalence of anxiety and depression is growing in many demographics, particularly young people, and suicide is claiming hundreds of thousands of lives globally each year.&lt;/p&gt;&lt;p&gt;Given the clear demand for accessible and affordable mental-health services, it’s no wonder that people have looked to artificial intelligence for possible relief. Millions are already actively seeking therapy from popular chatbots, or from specialized psychology apps like Wysa and Woebot.&lt;/p&gt;&lt;p&gt;Four timely new books are a reminder that while the present feels like a blur of breakthroughs, scandals, and confusion, this disorienting time is rooted in deeper histories of care, technology, and trust.&lt;strong&gt; &lt;/strong&gt;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Becky Ferreira&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the &lt;/strong&gt;&lt;strong&gt;most recent print issue of&lt;em&gt; MIT Technology Review&lt;/em&gt; magazin&lt;/strong&gt;&lt;strong&gt;e, which shines a light on the exciting innovations happening right now. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Making AI Work, MIT Technology Review’s new AI newsletter, is here&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;For years, our newsroom has explored AI’s limitations and potential dangers, as well as its growing energy needs. And our reporters have looked closely at how generative tools are being used for tasks such as coding and running scientific experiments.&lt;/p&gt;&lt;p&gt;But how is AI &lt;em&gt;actually&lt;/em&gt; being used in fields like health care, climate tech, education, and finance? How are small businesses using it? And what should you keep in mind if you use AI tools at work? These questions guided the creation of Making AI Work, a new AI mini-course newsletter. Read more about it, and sign up here to receive the seven editions straight to your inbox.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 The US is failing to punish polluters&lt;/strong&gt;&lt;br /&gt;The number of civil lawsuits it’s pursuing has sharply dropped in comparison to Trump’s first term. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;Rising GDP = greater carbon emissions. But does it have to? &lt;/em&gt;(The Guardian)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The European Union has warned Meta against blocking rival AI assistants&lt;br /&gt;&lt;/strong&gt;It’s the latest example of Brussels’ attempts to rein in Big Tech. (Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 AI ads took over the Super Bowl&lt;/strong&gt;&lt;br /&gt;Hyping up chatbots and taking swipes at their competitors. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;They appeared to be trying to win over AI naysayers, too. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;Celebrities were out in force to flog AI wares. &lt;/em&gt;(Slate $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;4 &lt;strong&gt;China wants to completely dominate the humanoid robot industry&lt;/strong&gt;&lt;br /&gt;Local governments and banks are only too happy to oblige promising startups. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Why the humanoid workforce is running late. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 We’re witnessing the first real crypto crash&lt;br /&gt;Cryptocurrency is now fully part of the financial system, for better or worse. (NY Mag $)&lt;br /&gt;+ &lt;em&gt;Wall Street’s grasp of AI is pretty shaky too. &lt;/em&gt;(Semafor)&lt;br /&gt;+ &lt;em&gt;Even traditionally safe markets are looking pretty volatile right now. &lt;/em&gt;(Economist $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;6 The man who coined vibe coding has a new fixation&lt;/strong&gt;&amp;nbsp;&lt;br /&gt;“Agentic engineering” is the next big thing, apparently. (Insider $)&lt;br /&gt;+ &lt;em&gt;Agentic AI is the talk of the town right now. &lt;/em&gt;(The Information $)&lt;br /&gt;+ &lt;em&gt;What is vibe coding, exactly? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 AI running app Runna has adjusted its aggressive training plans &lt;/strong&gt;&lt;strong&gt;🏃‍♂️&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Runners had long suspected its suggestions were pushing them towards injury. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 San Francisco’s march for billionaires was a flop&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Only around three dozen supporters turned up. (SF Chronicle)&lt;br /&gt;+ &lt;em&gt;Predictably, journalists nearly outnumbered the demonstrators. &lt;/em&gt;(TechCrunch)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 AI is shaking up romance novels ❤️&lt;/strong&gt;&lt;br /&gt;But models still aren’t great at writing sex scenes. (NYT $)&lt;br /&gt;+ &lt;em&gt;It’s surprisingly easy to stumble into a relationship with an AI chatbot. &lt;/em&gt;(MIT Technology Review)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 ChatGPT won’t be replacing human stylists any time soon&lt;/strong&gt;&lt;br /&gt;Its menswear suggestions are more manosphere influencer than suave gentleman. (GQ)&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
 &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“There is no Plan B, because that assumes you will fail. We’re going to do the start-up thing until we die.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—William Alexander, an ambitious 21-year old AI worker, explains his and his cohort’s attitudes towards trying to make it big in the highly-competitive industry to the New York Times.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1132502" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_14d65b.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;The open-source AI boom is built on Big Tech’s handouts. How long will it last?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In May 2023 a leaked memo reported to have been written by Luke Sernau, a senior engineer at Google, said out loud what many in Silicon Valley must have been whispering for weeks: an open-source free-for-all is threatening Big Tech’s grip on AI.&lt;/p&gt;  &lt;p&gt;In many ways, that’s a good thing. AI won't thrive if just a few mega-rich companies get to gatekeep this technology or decide how it is used. But this open-source boom is precarious, and if Big Tech decides to shut up shop, a boomtown could become a backwater. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Dark showering, anyone?&lt;br /&gt;+ Chef Yujia Hu is renowned for his shoe-shaped sushi designs.&lt;br /&gt;+ Meanwhile, in the depths of the South Atlantic Ocean: a giant phantom jelly has been spotted.&lt;br /&gt;+ I have nothing but respect for this X account dedicated to documenting rats and mice in movies and TV 🐀🐁&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/09/1132498/the-download-what-moltbook-tells-us-about-ai-hype-and-the-rise-and-rise-of-ai-therapy/</guid><pubDate>Mon, 09 Feb 2026 13:10:00 +0000</pubDate></item></channel></rss>