<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 16 Dec 2025 01:53:17 +0000</lastBuildDate><item><title>How to Fine-Tune an LLM on NVIDIA GPUs With Unsloth (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Modern workflows showcase the endless possibilities of generative and agentic AI on PCs.&lt;/p&gt;
&lt;p&gt;Of many, some examples include tuning a chatbot to handle product-support questions or building a personal assistant for managing one‚Äôs schedule. A challenge remains, however, in getting a small language model to respond consistently with high accuracy for specialized agentic tasks.&lt;/p&gt;
&lt;p&gt;That‚Äôs where fine-tuning comes in.&lt;/p&gt;
&lt;p&gt;Unsloth, one of the world‚Äôs most widely used open-source frameworks for fine-tuning LLMs, provides an approachable way to customize models. It‚Äôs optimized for efficient, low-memory training on NVIDIA GPUs ‚Äî from GeForce RTX desktops and laptops to RTX PRO workstations and DGX Spark, the world‚Äôs smallest AI supercomputer.&lt;/p&gt;
&lt;p&gt;Another powerful starting point for fine-tuning is the just-announced NVIDIA Nemotron 3 family of open models, data and libraries. Nemotron 3 introduces the most efficient family of open models, ideal for agentic AI fine-tuning.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Teaching AI New Tricks&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Fine-tuning is like giving an AI model a focused training session. With examples tied to a specific topic or workflow, the model improves its accuracy by learning new patterns and adapting to the task at hand.&lt;/p&gt;
&lt;p&gt;Choosing a fine-tuning method for a model depends on how much of the original model the developer wants to adjust. Based on their goals, developers can use one of three main fine-tuning methods:&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Parameter-efficient fine-tuning (such as LoRA or QLoRA)&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Updates only a small portion of the model for faster, lower-cost training. It‚Äôs a smarter and efficient way to enhance a model without altering it drastically.&lt;/li&gt;
&lt;li&gt;Target use case: Useful across nearly all scenarios where full fine-tuning would traditionally be applied ‚Äî including adding domain knowledge, improving coding accuracy, adapting the model for legal or scientific tasks, refining reasoning, or aligning tone and behavior.&lt;/li&gt;
&lt;li&gt;Requirements: Small- to medium-sized dataset (100-1,000 prompt-sample pairs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Full fine-tuning&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Updates all of the model‚Äôs parameters ‚Äî useful for teaching the model to follow specific formats or styles.&lt;/li&gt;
&lt;li&gt;Target use case: Advanced use cases, such as building AI agents and chatbots that must provide assistance about a specific topic, stay within a certain set of guardrails and respond in a particular manner.&lt;/li&gt;
&lt;li&gt;Requirements: Large dataset (1,000+ prompt-sample pairs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Reinforcement learning&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Adjusts the behavior of the model using feedback or preference signals. The model learns by interacting with its environment and uses the feedback to improve itself over time. This is a complex, advanced technique that interweaves training and inference ‚Äî and can be used in tandem with parameter-efficient fine-tuning and full fine-tuning techniques. See Unsloth‚Äôs Reinforcement Learning Guide for details.&lt;/li&gt;
&lt;li&gt;Target use case: Improving the accuracy of a model in a particular domain ‚Äî such as law or medicine ‚Äî or building autonomous agents that can orchestrate actions on a user‚Äôs behalf.&lt;/li&gt;
&lt;li&gt;Requirements:&amp;nbsp; A process that contains an action model, a reward model and an environment for the model to learn from.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another factor to consider is the VRAM required per each method. The chart below provides an overview of the requirements to run each type of fine-tuning method on Unsloth.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88327"&gt;&lt;img alt="alt" class="size-full wp-image-88327" height="712" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Fine-tuning-requirements-on-Unsloth.jpg" width="923" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88327"&gt;Fine-tuning requirements on Unsloth.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Unsloth: A Fast Path to Fine-Tuning on NVIDIA GPUs&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;LLM fine-tuning is a memory- and compute-intensive workload that involves billions of matrix multiplications to update model weights at every training step. This type of heavy parallel workload requires the power of NVIDIA GPUs to complete the process quickly and efficiently.&lt;/p&gt;
&lt;p&gt;Unsloth shines at this workload, translating complex mathematical operations into efficient, custom GPU kernels to accelerate AI training.&lt;/p&gt;
&lt;p&gt;Unsloth helps boost the performance of the Hugging Face transformers library by 2.5x on NVIDIA GPUs. These GPU-specific optimizations, combined with Unsloth‚Äôs ease of use, make fine-tuning accessible to a broader community of AI enthusiasts and developers.&lt;/p&gt;
&lt;p&gt;The framework is built and optimized for NVIDIA hardware ‚Äî from GeForce RTX laptops to RTX PRO workstations and DGX Spark ‚Äî providing peak performance while reducing VRAM consumption.&lt;/p&gt;
&lt;p&gt;Unsloth provides helpful guides on how to get started and manage different LLM configurations, hyperparameters and options, along with example notebooks and step-by-step workflows.&lt;/p&gt;
&lt;p&gt;Check out some of these Unsloth guides:&lt;/p&gt;

&lt;p&gt;Learn how to install Unsloth on NVIDIA DGX Spark. Read the NVIDIA technical blog for a deep dive of fine-tuning and reinforcement learning on the NVIDIA Blackwell platform.&lt;/p&gt;
&lt;p&gt;For a hands-on local fine-tuning walkthrough, watch Matthew Berman showing reinforcement learning running on a NVIDIA GeForce RTX 5090 using Unsloth in the video below.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Available Now: NVIDIA Nemotron 3 Family of Open Models&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The new Nemotron 3 family of open models ‚Äî in Nano, Super, and Ultra sizes ‚Äî built on a new hybrid latent Mixture-of-Experts (MoE) architecture, introduces the most efficient family of open models with leading accuracy, ideal for building agentic AI applications.&lt;/p&gt;
&lt;p&gt;Nemotron 3 Nano 30B-A3B, available now, is the most compute-efficient model in the lineup. It‚Äôs optimized for tasks such as software debugging, content summarization, AI assistant workflows and information retrieval at low inference costs. Its hybrid MoE design delivers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Up to 60% fewer reasoning tokens, significantly reducing inference cost.&lt;/li&gt;
&lt;li&gt;A 1 million-token context window, allowing the model to retain far more information for long, multistep tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nemotron 3 Super is a high-accuracy reasoning model for multi-agent applications, while&amp;nbsp;Nemotron 3 Ultra is for complex AI applications. Both are expected to be available in the first half of 2026.&lt;/p&gt;
&lt;p&gt;NVIDIA also released today an open collection of training datasets and state-of-the-art reinforcement learning libraries. Nemotron 3 Nano fine-tuning is available on Unsloth.&lt;/p&gt;
&lt;p&gt;Download Nemotron 3 Nano now from Hugging Face, or experiment with it through Llama.cpp and LM Studio.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;DGX Spark: A Compact AI Powerhouse&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;DGX Spark enables local fine-tuning and brings incredible AI performance in a compact, desktop supercomputer, giving developers access to more memory than a typical PC.&lt;/p&gt;
&lt;p&gt;Built on the NVIDIA Grace Blackwell architecture, DGX Spark delivers up to a petaflop of FP4 AI performance and includes 128GB of unified CPU-GPU memory, giving developers enough headroom to run larger models, longer context windows and more demanding training workloads locally.&lt;/p&gt;
&lt;p&gt;For fine-tuning, DGX Spark enables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Larger model sizes.&lt;/b&gt; Models with more than 30 billion parameters often exceed the VRAM capacity of consumer GPUs but fit comfortably within DGX Spark‚Äôs unified memory.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;More advanced techniques. &lt;/b&gt;Full fine-tuning and reinforcement-learning-based workflows ‚Äî which demand more memory and higher throughput ‚Äî run significantly faster on DGX Spark.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Local control without cloud queues.&lt;/b&gt; Developers can run compute-heavy tasks locally instead of waiting for cloud instances or managing multiple environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DGX Spark‚Äôs strengths go beyond LLMs. High-resolution diffusion models, for example, often require more memory than a typical desktop can provide. With FP4 support and large unified memory, DGX Spark can generate 1,000 images in just a few seconds and sustain higher throughput for creative or multimodal pipelines.&lt;/p&gt;
&lt;p&gt;The table below shows performance for fine-tuning the Llama family of models on DGX Spark.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88330"&gt;&lt;img alt="alt" class="size-full wp-image-88330" height="383" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Performance-for-fine-tuning-Llama-family-of-models-on-DGX-Spark.jpg" width="923" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88330"&gt;Performance for fine-tuning Llama family of models on DGX Spark.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As fine-tuning workflows advance, the new Nemotron 3 family of open models offer scalable reasoning and long-context performance optimized for RTX systems and DGX Spark.&lt;/p&gt;
&lt;p&gt;Learn more about how DGX Spark enables intensive AI tasks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;#ICYMI ‚Äî The Latest Advancements in NVIDIA RTX AI PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;üöÄ &lt;b&gt;FLUX.2 Image-Generation Models Now Released, Optimized for NVIDIA RTX GPUs&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new models from Black Forest Labs are available in FP8 quantizations that reduce VRAM and increase performance by 40%.&lt;/p&gt;
&lt;p&gt;‚ú® &lt;b&gt;Nexa.ai Expands Local AI on RTX PCs With Hyperlink for Agentic Search&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new on-device search agent delivers 3x faster retrieval-augmented generation indexing and 2x faster LLM inference, indexing a dense 1GB folder from about 15 minutes to just four to five minutes. Plus, DeepSeek OCR now runs locally in GGUF via NexaSDK, offering plug-and-play parsing of charts, formulas and multilingual PDFs on RTX GPUs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;ü§ùMistral AI Unveils New Model Family Optimized for NVIDIA GPUs&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new Mistral 3 models are optimized from cloud to edge and available for fast, local experimentation through Ollama and Llama.cpp.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;üé®Blender 5.0 Lands With HDR Color and Major Performance Gains&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The release adds ACES 2.0 wide-gamut/HDR color, NVIDIA DLSS for up to 5x faster hair and fur rendering, better handling of massive geometry, and motion blur for Grease Pencil.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; ‚Äî and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Modern workflows showcase the endless possibilities of generative and agentic AI on PCs.&lt;/p&gt;
&lt;p&gt;Of many, some examples include tuning a chatbot to handle product-support questions or building a personal assistant for managing one‚Äôs schedule. A challenge remains, however, in getting a small language model to respond consistently with high accuracy for specialized agentic tasks.&lt;/p&gt;
&lt;p&gt;That‚Äôs where fine-tuning comes in.&lt;/p&gt;
&lt;p&gt;Unsloth, one of the world‚Äôs most widely used open-source frameworks for fine-tuning LLMs, provides an approachable way to customize models. It‚Äôs optimized for efficient, low-memory training on NVIDIA GPUs ‚Äî from GeForce RTX desktops and laptops to RTX PRO workstations and DGX Spark, the world‚Äôs smallest AI supercomputer.&lt;/p&gt;
&lt;p&gt;Another powerful starting point for fine-tuning is the just-announced NVIDIA Nemotron 3 family of open models, data and libraries. Nemotron 3 introduces the most efficient family of open models, ideal for agentic AI fine-tuning.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Teaching AI New Tricks&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Fine-tuning is like giving an AI model a focused training session. With examples tied to a specific topic or workflow, the model improves its accuracy by learning new patterns and adapting to the task at hand.&lt;/p&gt;
&lt;p&gt;Choosing a fine-tuning method for a model depends on how much of the original model the developer wants to adjust. Based on their goals, developers can use one of three main fine-tuning methods:&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Parameter-efficient fine-tuning (such as LoRA or QLoRA)&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Updates only a small portion of the model for faster, lower-cost training. It‚Äôs a smarter and efficient way to enhance a model without altering it drastically.&lt;/li&gt;
&lt;li&gt;Target use case: Useful across nearly all scenarios where full fine-tuning would traditionally be applied ‚Äî including adding domain knowledge, improving coding accuracy, adapting the model for legal or scientific tasks, refining reasoning, or aligning tone and behavior.&lt;/li&gt;
&lt;li&gt;Requirements: Small- to medium-sized dataset (100-1,000 prompt-sample pairs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Full fine-tuning&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Updates all of the model‚Äôs parameters ‚Äî useful for teaching the model to follow specific formats or styles.&lt;/li&gt;
&lt;li&gt;Target use case: Advanced use cases, such as building AI agents and chatbots that must provide assistance about a specific topic, stay within a certain set of guardrails and respond in a particular manner.&lt;/li&gt;
&lt;li&gt;Requirements: Large dataset (1,000+ prompt-sample pairs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Reinforcement learning&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Adjusts the behavior of the model using feedback or preference signals. The model learns by interacting with its environment and uses the feedback to improve itself over time. This is a complex, advanced technique that interweaves training and inference ‚Äî and can be used in tandem with parameter-efficient fine-tuning and full fine-tuning techniques. See Unsloth‚Äôs Reinforcement Learning Guide for details.&lt;/li&gt;
&lt;li&gt;Target use case: Improving the accuracy of a model in a particular domain ‚Äî such as law or medicine ‚Äî or building autonomous agents that can orchestrate actions on a user‚Äôs behalf.&lt;/li&gt;
&lt;li&gt;Requirements:&amp;nbsp; A process that contains an action model, a reward model and an environment for the model to learn from.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another factor to consider is the VRAM required per each method. The chart below provides an overview of the requirements to run each type of fine-tuning method on Unsloth.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88327"&gt;&lt;img alt="alt" class="size-full wp-image-88327" height="712" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Fine-tuning-requirements-on-Unsloth.jpg" width="923" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88327"&gt;Fine-tuning requirements on Unsloth.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Unsloth: A Fast Path to Fine-Tuning on NVIDIA GPUs&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;LLM fine-tuning is a memory- and compute-intensive workload that involves billions of matrix multiplications to update model weights at every training step. This type of heavy parallel workload requires the power of NVIDIA GPUs to complete the process quickly and efficiently.&lt;/p&gt;
&lt;p&gt;Unsloth shines at this workload, translating complex mathematical operations into efficient, custom GPU kernels to accelerate AI training.&lt;/p&gt;
&lt;p&gt;Unsloth helps boost the performance of the Hugging Face transformers library by 2.5x on NVIDIA GPUs. These GPU-specific optimizations, combined with Unsloth‚Äôs ease of use, make fine-tuning accessible to a broader community of AI enthusiasts and developers.&lt;/p&gt;
&lt;p&gt;The framework is built and optimized for NVIDIA hardware ‚Äî from GeForce RTX laptops to RTX PRO workstations and DGX Spark ‚Äî providing peak performance while reducing VRAM consumption.&lt;/p&gt;
&lt;p&gt;Unsloth provides helpful guides on how to get started and manage different LLM configurations, hyperparameters and options, along with example notebooks and step-by-step workflows.&lt;/p&gt;
&lt;p&gt;Check out some of these Unsloth guides:&lt;/p&gt;

&lt;p&gt;Learn how to install Unsloth on NVIDIA DGX Spark. Read the NVIDIA technical blog for a deep dive of fine-tuning and reinforcement learning on the NVIDIA Blackwell platform.&lt;/p&gt;
&lt;p&gt;For a hands-on local fine-tuning walkthrough, watch Matthew Berman showing reinforcement learning running on a NVIDIA GeForce RTX 5090 using Unsloth in the video below.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Available Now: NVIDIA Nemotron 3 Family of Open Models&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The new Nemotron 3 family of open models ‚Äî in Nano, Super, and Ultra sizes ‚Äî built on a new hybrid latent Mixture-of-Experts (MoE) architecture, introduces the most efficient family of open models with leading accuracy, ideal for building agentic AI applications.&lt;/p&gt;
&lt;p&gt;Nemotron 3 Nano 30B-A3B, available now, is the most compute-efficient model in the lineup. It‚Äôs optimized for tasks such as software debugging, content summarization, AI assistant workflows and information retrieval at low inference costs. Its hybrid MoE design delivers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Up to 60% fewer reasoning tokens, significantly reducing inference cost.&lt;/li&gt;
&lt;li&gt;A 1 million-token context window, allowing the model to retain far more information for long, multistep tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nemotron 3 Super is a high-accuracy reasoning model for multi-agent applications, while&amp;nbsp;Nemotron 3 Ultra is for complex AI applications. Both are expected to be available in the first half of 2026.&lt;/p&gt;
&lt;p&gt;NVIDIA also released today an open collection of training datasets and state-of-the-art reinforcement learning libraries. Nemotron 3 Nano fine-tuning is available on Unsloth.&lt;/p&gt;
&lt;p&gt;Download Nemotron 3 Nano now from Hugging Face, or experiment with it through Llama.cpp and LM Studio.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;DGX Spark: A Compact AI Powerhouse&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;DGX Spark enables local fine-tuning and brings incredible AI performance in a compact, desktop supercomputer, giving developers access to more memory than a typical PC.&lt;/p&gt;
&lt;p&gt;Built on the NVIDIA Grace Blackwell architecture, DGX Spark delivers up to a petaflop of FP4 AI performance and includes 128GB of unified CPU-GPU memory, giving developers enough headroom to run larger models, longer context windows and more demanding training workloads locally.&lt;/p&gt;
&lt;p&gt;For fine-tuning, DGX Spark enables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Larger model sizes.&lt;/b&gt; Models with more than 30 billion parameters often exceed the VRAM capacity of consumer GPUs but fit comfortably within DGX Spark‚Äôs unified memory.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;More advanced techniques. &lt;/b&gt;Full fine-tuning and reinforcement-learning-based workflows ‚Äî which demand more memory and higher throughput ‚Äî run significantly faster on DGX Spark.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Local control without cloud queues.&lt;/b&gt; Developers can run compute-heavy tasks locally instead of waiting for cloud instances or managing multiple environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DGX Spark‚Äôs strengths go beyond LLMs. High-resolution diffusion models, for example, often require more memory than a typical desktop can provide. With FP4 support and large unified memory, DGX Spark can generate 1,000 images in just a few seconds and sustain higher throughput for creative or multimodal pipelines.&lt;/p&gt;
&lt;p&gt;The table below shows performance for fine-tuning the Llama family of models on DGX Spark.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88330"&gt;&lt;img alt="alt" class="size-full wp-image-88330" height="383" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Performance-for-fine-tuning-Llama-family-of-models-on-DGX-Spark.jpg" width="923" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88330"&gt;Performance for fine-tuning Llama family of models on DGX Spark.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As fine-tuning workflows advance, the new Nemotron 3 family of open models offer scalable reasoning and long-context performance optimized for RTX systems and DGX Spark.&lt;/p&gt;
&lt;p&gt;Learn more about how DGX Spark enables intensive AI tasks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;#ICYMI ‚Äî The Latest Advancements in NVIDIA RTX AI PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;üöÄ &lt;b&gt;FLUX.2 Image-Generation Models Now Released, Optimized for NVIDIA RTX GPUs&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new models from Black Forest Labs are available in FP8 quantizations that reduce VRAM and increase performance by 40%.&lt;/p&gt;
&lt;p&gt;‚ú® &lt;b&gt;Nexa.ai Expands Local AI on RTX PCs With Hyperlink for Agentic Search&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new on-device search agent delivers 3x faster retrieval-augmented generation indexing and 2x faster LLM inference, indexing a dense 1GB folder from about 15 minutes to just four to five minutes. Plus, DeepSeek OCR now runs locally in GGUF via NexaSDK, offering plug-and-play parsing of charts, formulas and multilingual PDFs on RTX GPUs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;ü§ùMistral AI Unveils New Model Family Optimized for NVIDIA GPUs&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new Mistral 3 models are optimized from cloud to edge and available for fast, local experimentation through Ollama and Llama.cpp.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;üé®Blender 5.0 Lands With HDR Color and Major Performance Gains&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The release adds ACES 2.0 wide-gamut/HDR color, NVIDIA DLSS for up to 5x faster hair and fur rendering, better handling of massive geometry, and motion blur for Grease Pencil.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; ‚Äî and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</guid><pubDate>Mon, 15 Dec 2025 14:00:11 +0000</pubDate></item><item><title>Nemotron 3 Nano \- A new Standard for Efficient, Open, and Intelligent Agentic Models (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Chintan's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/59ef303ccd0ee50d169be4b14008cd21.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
If 2025 was the year of AI agents, then 2026 is gearing up to be the year of, well, multi-agents. This leap to the next step requires models that produce a lot of tokens, generated by lightweight accurate models.
&lt;p&gt;However, this transition also forces difficult tradeoffs. Smaller models are fast and cheap but often lack the reasoning depth, robustness, and long context capacity needed for advanced multi-agents. Larger models deliver strong accuracy, but are too slow and expensive when many agents are running in parallel. As agentic systems grow, inference costs spiral, context windows become a bottleneck, and reliability starts to degrade, making efficiency of utmost importance.&lt;/p&gt;
&lt;p&gt;Striking the right balance is what led NVIDIA to produce the &lt;strong&gt;NVIDIA Nemotron 3 Nano 30B A3B&lt;/strong&gt;, part of our Nemotron 3 family of models (Nano,  Super, and Ultra). &lt;/p&gt;
&lt;p&gt;Nano utilizes a hybrid Mamba-Transformer Mixture-of-Experts (MoE) architecture with a 1M-token context window. (üî•üî•üî•) enabling developers to build high-throughput, reliable agents that are more accurate, more scalable, and capable of specialized sub-tasks in long-running multi-step workflows.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hybrid Mamba-Transformer MoE architecture:&lt;/strong&gt;  Mamba‚Äë2 for long-context, low-latency inference combined with transformer attention for high-accuracy, fine-grained reasoning  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;31.6B total parameters, ~3.6B active per token:&lt;/strong&gt; Designed for high throughput and low latency  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exceptional inference efficiency:&lt;/strong&gt; Up to 4x faster than Nemotron Nano 2 and up to 3.3x faster than leading models in its size category  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best-in-class reasoning accuracy:&lt;/strong&gt;  Across reasoning, coding, tools, and multi-step agentic tasks  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning controls:&lt;/strong&gt; Reasoning ON/OFF modes plus a configurable thinking budget to cap ‚Äúthinking‚Äù tokens and keep inference cost predictable  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1M-token context window:&lt;/strong&gt; Ideal for long-horizon workflows, retrieval-augmented tasks, and persistent memory  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fully open:&lt;/strong&gt; Open Weights, datasets, training recipes, and framework  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A full open data stack&lt;/strong&gt;: 3T new high-quality pre-training tokens, 13M cross-disciplinary post-training samples, 10+ RL environments with datasets covering more than 900k tasks in math, coding, reasoning, and tool-use, and ~11k agent-safety traces  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Easy deployment:&lt;/strong&gt; Seamless serving with vLLM and SGLang, and integration via OpenRouter, popular inference service providers, and build.nvidia.com endpoints  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Released under the nvidia-open-model-license&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="A two-panel figure comparing Nemotron 3 Nano with Qwen3-30B and GPT-OSS-20B. The left panel displays accuracy scores, showing Nano equal or higher across benchmarks. The right panel displays inference throughput bars, where the Nano is significantly taller; illustrating 3.3x speed over Qwen3 and 2.2x over GPT-OSS." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/4DT2feJmd_ICbzZ-xKqd5.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Nemotron 3 Nano matches or exceeds the accuracy of Qwen3-30B and GPT-OSS-20B while delivering dramatically higher throughput. In an 8K input / 16K output configuration on a single H200 GPU, Nano achieves 3.3x higher throughput than Qwen3-30B and 2.2x higher than GPT-OSS-20B.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Nemotron 3 Nano (30B/A3B) is our latest small-but-powerful reasoning model, building on the success of Nemotron Nano 2's hybrid Mamba-2 + Transformer architecture, reasoning ON/OFF modes, and explicit thinking budgets‚Äîwhile introducing a major architectural upgrade: a sparse Mixture-of-Experts (MoE) design.&lt;/p&gt;
&lt;p&gt;At high level:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;31.6B total parameters&lt;/strong&gt;  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;~3.6B active parameters per token&lt;/strong&gt;, thanks to the MoE routing  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid layer stack&lt;/strong&gt; with interleaved Mamba‚Äë2 layers and grouped-query attention (GQA) Transformer layers   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A learned multi-layer perceptron (MLP) router&lt;/strong&gt; that activates 6 of 128 experts on each forward pass, delivering both efficiency and reasoning accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This combination enables Nemotron 3 Nano to behave like a much larger model in terms of reasoning quality‚Äîwhile maintaining the speed and cost profile expected of a lightweight architecture&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the Nemotron-Nano-3-30B-A3B architecture showing four sequential blocks. Each block contains repeating Mamba-2 layers and MoE units, with attention layers interspersed in the first and third blocks. The blocks repeat x5, x3, x1, and x4 times respectively, illustrating the hybrid Mamba-Transformer design with MoE layers replacing FFNs." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/OZe39if73d6P8vi6sDDNd.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Nemotron 3 Nano architecture. It uses a hybrid Mamba-Transformer backbone, similar to Nemotron Nano v2, but replaces standard feed-forward network (FFN) layers with sparse MoE layers to significantly boost efficiency and scalability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nemotron 3 Nano is built for agentic, reasoning, tool-use, and chat tasks and supports a&lt;br /&gt;context length up to 1M tokens.&lt;/p&gt;
&lt;p&gt;It extends the Nemotron model family we released earlier in the year, continuing the progression toward increasingly accurate and efficient open models for reasoning and agent development.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The roadmap graphic illustrates the evolution of Nemotron model families: **Nemotron 1** enhances Llama models with stronger reasoning capabilities, **Nemotron 2** introduces a hybrid Mamba-Transformer architecture, delivering state-of-the-art accuracy and efficiency, **Nemotron 3** adds sparse MoE to the hybrid design, further improving accuracy, throughput, latency, and overall compute efficiency." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/O-jBDwGrrb9T3GGi1qAHo.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: NVIDIA Nemotron family of open models is engineered for advanced reasoning and agentic tasks, delivering leading accuracies, and best-in-class efficiencies&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We employed a multi-stage pipeline combining massive-scale pre-training, specialized supervised fine-tuning (SFT), and advanced reinforcement learning techniques to refine the reasoning abilities and agentic behavior.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Pre-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron 3 Nano was trained on a  25-trillion-token corpus (including 2.5T of new Common Crawl tokens), spanning web crawls, code and math, Wikipedia and academic text, multilingual content (15 Pre-training followed a two-phase strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Phase 1: Diversity (first 94%)&lt;/strong&gt;&lt;br /&gt;Broad , diverse mixture to maximize coverage and generalization.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Phase 2: Quality (final 6%)&lt;/strong&gt;&lt;br /&gt;High-quality sources such as Wikipedia to refine accuracy and consistency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Long-Context Extension&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Context length for Nemotron 3 Nano was extended by adding a continued pre-training (CPT) stage at 512k sequence length. A mixture of 512k and 4k sequence length training preserved short benchmark scores while extending the context length. We included synthetic data designed to support long-range retrieval, multi-hop reasoning, multi-document information aggregation, and related capabilities across different stages of training.&lt;/p&gt;
&lt;p&gt;We are releasing a large portion of these  pretraining datasets openly on Hugging Face. These additions contribute 3 trillion new tokens to the Nemotron-Pretraining series, with higher-fidelity coverage of code, math, and reasoning. Enhanced synthetic augmentation and annotation pipelines increase data density and structure, improving training efficiency and directly contributing to Nemotron-3 Nano's strong quality profile.&lt;/p&gt;
&lt;p&gt;With Nemotron 3, we‚Äôve learned that quantity without quality isn‚Äôt useful. Our pre-training data continues to shift toward efficient data: smarter filtration, rewritten and improved samples, and nearly half a trillion tokens of rescued math and code that previous pipelines would have discarded. This focus on signal over noise directly enables smarter, smaller models that are cheaper to train and run, without sacrificing accuracy.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This included Supervised fine-tuning (SFT) and two distinct stages of reinforcement learning, RLVR and RLHF. These stages specialize the model for agentic workflows, tool use, high-quality reasoning, and chat tasks.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Supervised Finetuning&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our SFT recipe was improved from Nano v2 to better support complex agentic behaviors. Improvements included greater dataset diversity, higher data quality, and explicit training for multi-step and multi-turn reasoning.&lt;/p&gt;
&lt;p&gt;The model learns both reasoning ON/OFF modes directly from the chat template:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reasoning ON:&lt;/strong&gt; multi-step mode, where the model preserves and builds upon its prior chain-of-thought within a task.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning OFF:&lt;/strong&gt; multi-turn mode, where reasoning content is not carried over across turns, ensuring concise responses.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="The graph from Artificial Analysis plots small language reasoning models on intelligence index on the y-axis and output tokens per second on the x-axis. Nemotron 3 Nano delivers the highest throughput efficiency using the hybrid MoE architecture and leading accuracy with advanced Reinforcement Learning using NeMo Gym" src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/i169eq3mbepGags0uIB-y.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Nemotron 3 Nano delivers the highest throughput efficiency using the hybrid MoE architecture and leading accuracy with advanced Reinforcement Learning using NeMo Gym&lt;/p&gt;
&lt;p&gt;We are releasing the majority of SFT datasets and codebase openly.&lt;/p&gt;
&lt;p&gt;Our new post-training data release also expands the intelligence of the model by design. We added 13 million new post-training samples‚Äînearly tripling our previous release and making this the largest openly available post-training corpus by 2.5√ó. To reach higher reasoning accuracy, we blended cross-disciplinary domains including code, math, physics, and chemistry to create novel, multi-step problems that don‚Äôt exist in scraped web data. This helps the model reason about questions that fall between fields, where real scientific and technical progress often happens.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Multi environment Reinforcement Learning from Verifiable Rewards (RLVR)&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Nemotron 3 Nano was trained simultaneously across many distinct environments - spanning math, code, question answering, instruction following, multi-step tool use, multi-turn conversations, and structured output among others, using synchronous GRPO (Group Relative Policy Optimization). This multi-environment RLVR stage ensures uniform improvement across domains, reduced overfitting to any single benchmark, and more reliable agentic behavior in real-world workflows.&lt;/p&gt;
&lt;p&gt;&lt;img alt="This figure shows multiple different environment reward curves over training steps, showcasing how the model was learning many different capabilities simultaneously." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/ha57qyj3KKsxmxea1frIR.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Uniform improvements due to training simultaneously on multiple RL environments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Models need more than textbooks to train ‚Äî they need a gym. NVIDIA is one of the only open model providers releasing both reinforcement learning datasets and the environments used to train them. This enables developers to test agents, capture critical edge cases, and prevent model drift over time. In this release, we are adding 10+ new RL environments covering competitive coding, advanced math, and even real-world calendar scheduling.&lt;/p&gt;
&lt;p&gt;We are also open-sourcing all the essential RLVR infrastructure‚Äîthe environments including their datasets and code used to build and scale them. These components form the foundation of the new NVIDIA NeMo Gym library, which enables scalable RL environment construction. &lt;/p&gt;
&lt;p&gt;Training at scale is executed using NVIDIA NeMo RL, our high-performance RL library.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reinforce Learning Using Human Feedback (RLHF)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To further refine the model‚Äôs conversational quality, we trained a generative reward model (GenRM) using GRPO on Qwen3-235B-A22B. &lt;/p&gt;
&lt;p&gt;Given a conversation history, a new user query, and two candidate assistant responses, the GenRM explicitly reasons about the strengths and weaknesses of each response, produces individual helpfulness scores and generates a relative ranking between the candidates. These  reward signals are then used in an RLHF stage to improve helpfulness, coherence, correctness, and overall chat experience in Nemotron 3 Nano.&lt;/p&gt;
&lt;p&gt;The combined post-training pipeline‚ÄîSFT+RLVR+RLHF‚Äîproduces the final Nemotron 3 Nano 30B-A3B model.&lt;/p&gt;
&lt;p&gt;As models evolve into multi-step agents that use tools, they face entirely new safety and security challenges. To support responsible deployment, we are releasing an agentic safety dataset featuring nearly 11,000 labeled traces from realistic, tool-using workflows. This gives developers the data they need to evaluate, diagnose, and mitigate safety risks before agentic systems reach production.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Why We Needed Better RL Infrastructure
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;During development, the limitations of existing RL tooling became clear. Training large reasoning models with RL is difficult because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-step rollouts are complex to orchestrate  &lt;/li&gt;
&lt;li&gt;Tool integrations are often brittle  &lt;/li&gt;
&lt;li&gt;Orchestration logic can conflict with training loop design  &lt;/li&gt;
&lt;li&gt;Collecting rollout data at scale is slow and difficult  &lt;/li&gt;
&lt;li&gt;Most high-quality RL environments are closed and proprietary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result, meaningful RL training has historically been accessible only to major AI labs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		NeMo Gym: Opening RL to Everyone
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To overcome these challenges NVIDIA built NeMo Gym, an open-source standardized library for building and scaling RL environments.&lt;br /&gt;NeMo Gym powers reinforcement learning pipelines used in Nemotron 3 Nano, and now gives developers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ready-to-use RL environments across math, code, tool use, multi-turn reasoning, and agentic workflows  &lt;/li&gt;
&lt;li&gt;The ability to build custom RL environments with verifiable reward logic  &lt;/li&gt;
&lt;li&gt;Ecosystem interoperability with NeMo RL and other training frameworks (TRL, Unsloth, VeRL underway)  &lt;/li&gt;
&lt;li&gt;High-throughput rollout orchestration, enabling large-scale RL training  &lt;/li&gt;
&lt;li&gt;A practical pathway to perform RL on their own models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NeMo Gym is a flexible open source library for building and running RL training environments. It is part of the broader NVIDIA NeMo software suite for end-to-end model training and provides infrastructure for designing, running, and scaling complex RL environments. &lt;/p&gt;
&lt;p&gt;Battle tested through the development of the entire Nemotron 3 model family, NeMo Gym includes the core environment development infrastructure, a growing collection of ready-to-use training environments alongside the datasets used in RLVR, and integration with NeMo RL, the high-performance and efficient RL training engine with support for advanced RL training algorithms, end-to-end FP8 training and async RL.&lt;/p&gt;
&lt;p&gt;With NeMo Gym, teams can quickly assemble environments using modular server components and templates, integrate external tools, systems, or databases, and orchestrate long-context, multi-step, multi-turn rollouts. This allows training environments to be iterated on and shared independent of the training loop.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram illustrating interaction between an RL Training Framework on the left and NeMo Gym on the right. The training framework sends task prompts  to the agent server in the NeMo Gym. The agent server coordinates with the policy model server and external resources server to collect rollouts and verify task performance. The scored trajectories are returned back to the Training Framework for model updates." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/go-hdGzZpaKRJChrl-o6k.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: How NeMo Gym fits into the RL training loop: The RL training framework (e.g., NeMo RL) sends task prompts to NeMo Gym, which operates as a set of independent HTTP services. Inside NeMo Gym, the agent server orchestrates rollouts by coordinating the policy model server (generation) and external resources server (tools and rewards). NeMo Gym returns model trajectories and rewards to the training framework, which then  updates and refits the policy model.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By decoupling RL environments from RL training frameworks, NeMo Gym works seamlessly with many popular training frameworks (such as NeMo RL), supports high-throughput, concurrent rollout collection, and enables large-scale distributed RL training. This separation of concerns makes it easy to scale RL workflows and adapt environments as training objectives evolve.&lt;/p&gt;
&lt;p&gt;To accelerate experimentation, NeMo Gym ships with an expanding RL Hub‚Äîa catalog of ready-to-use domain-specific environments that developers can use immediately or extend. Current domains include math, coding, instruction following, multi-step tool use, multi-turn structured conversations. Practitioners can fine-tune models on these environments out of the box, reuse community contributions, or publish their own.&lt;/p&gt;

&lt;p&gt;Nemotron 3 Nano (30B A3B) delivers state-of-the-art accuracy in an exceptionally cost-efficient package. It offers up to 3.3x higher throughput than leading open-source models of similar size (see Figure 1), while supporting a 1M-token context window ‚Äîperforming well on long-context reasoning benchmarks.&lt;/p&gt;
&lt;p&gt;Built for high-volume, real-time execution, Nemotron 3 Nano excels in math and coding, multi-step tool calling, and multi-turn agentic workflows. It also retains the classic Nemotron Thinking ON/OFF modes and Thinking Budget controls, giving developers the ability to tune exactly how much the model thinks for each task.&lt;/p&gt;
&lt;p&gt;With this release, we are also introducing NeMo Gym, containing ready-to-use training environments we developed during the course of Nemotron 3 training, and the infrastructure to build your own training environments and scale rollout collection. &lt;/p&gt;
&lt;p&gt;We are releasing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full model weights  &lt;/li&gt;
&lt;li&gt;The complete training recipe, including SFT, RLVR, and RLHF  &lt;/li&gt;
&lt;li&gt;Most of the datasets (pre-training, post-training) used throughout the training pipeline  &lt;/li&gt;
&lt;li&gt;Training frameworks that power Nemotron 3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Everything you need to study, reproduce, or extend the model is available openly.&lt;/p&gt;
&lt;p&gt;Get started with Nemotron 3 Nano:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Download the model:&lt;/strong&gt; Now available on Hugging Face.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Try hosted endpoints:&lt;/strong&gt; Run queries instantly on OpenRouter or build.nvidia.com.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy at scale:&lt;/strong&gt; Use our cookbooks for vLLM, TRT-LLM, and SGLang  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Experiment, develop and run at the edge:&lt;/strong&gt; Available on edge devices such as NVIDIA RTX AI PCs and Workstations and DGX Spark via Llama.cpp, LM Studio and Unsloth&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a deep dive into the architecture, datasets, and benchmarks, read the full Nemotron 3 Nano Technical Report.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Chintan's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/59ef303ccd0ee50d169be4b14008cd21.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
If 2025 was the year of AI agents, then 2026 is gearing up to be the year of, well, multi-agents. This leap to the next step requires models that produce a lot of tokens, generated by lightweight accurate models.
&lt;p&gt;However, this transition also forces difficult tradeoffs. Smaller models are fast and cheap but often lack the reasoning depth, robustness, and long context capacity needed for advanced multi-agents. Larger models deliver strong accuracy, but are too slow and expensive when many agents are running in parallel. As agentic systems grow, inference costs spiral, context windows become a bottleneck, and reliability starts to degrade, making efficiency of utmost importance.&lt;/p&gt;
&lt;p&gt;Striking the right balance is what led NVIDIA to produce the &lt;strong&gt;NVIDIA Nemotron 3 Nano 30B A3B&lt;/strong&gt;, part of our Nemotron 3 family of models (Nano,  Super, and Ultra). &lt;/p&gt;
&lt;p&gt;Nano utilizes a hybrid Mamba-Transformer Mixture-of-Experts (MoE) architecture with a 1M-token context window. (üî•üî•üî•) enabling developers to build high-throughput, reliable agents that are more accurate, more scalable, and capable of specialized sub-tasks in long-running multi-step workflows.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hybrid Mamba-Transformer MoE architecture:&lt;/strong&gt;  Mamba‚Äë2 for long-context, low-latency inference combined with transformer attention for high-accuracy, fine-grained reasoning  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;31.6B total parameters, ~3.6B active per token:&lt;/strong&gt; Designed for high throughput and low latency  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exceptional inference efficiency:&lt;/strong&gt; Up to 4x faster than Nemotron Nano 2 and up to 3.3x faster than leading models in its size category  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best-in-class reasoning accuracy:&lt;/strong&gt;  Across reasoning, coding, tools, and multi-step agentic tasks  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning controls:&lt;/strong&gt; Reasoning ON/OFF modes plus a configurable thinking budget to cap ‚Äúthinking‚Äù tokens and keep inference cost predictable  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1M-token context window:&lt;/strong&gt; Ideal for long-horizon workflows, retrieval-augmented tasks, and persistent memory  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fully open:&lt;/strong&gt; Open Weights, datasets, training recipes, and framework  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A full open data stack&lt;/strong&gt;: 3T new high-quality pre-training tokens, 13M cross-disciplinary post-training samples, 10+ RL environments with datasets covering more than 900k tasks in math, coding, reasoning, and tool-use, and ~11k agent-safety traces  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Easy deployment:&lt;/strong&gt; Seamless serving with vLLM and SGLang, and integration via OpenRouter, popular inference service providers, and build.nvidia.com endpoints  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Released under the nvidia-open-model-license&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="A two-panel figure comparing Nemotron 3 Nano with Qwen3-30B and GPT-OSS-20B. The left panel displays accuracy scores, showing Nano equal or higher across benchmarks. The right panel displays inference throughput bars, where the Nano is significantly taller; illustrating 3.3x speed over Qwen3 and 2.2x over GPT-OSS." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/4DT2feJmd_ICbzZ-xKqd5.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Nemotron 3 Nano matches or exceeds the accuracy of Qwen3-30B and GPT-OSS-20B while delivering dramatically higher throughput. In an 8K input / 16K output configuration on a single H200 GPU, Nano achieves 3.3x higher throughput than Qwen3-30B and 2.2x higher than GPT-OSS-20B.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Nemotron 3 Nano (30B/A3B) is our latest small-but-powerful reasoning model, building on the success of Nemotron Nano 2's hybrid Mamba-2 + Transformer architecture, reasoning ON/OFF modes, and explicit thinking budgets‚Äîwhile introducing a major architectural upgrade: a sparse Mixture-of-Experts (MoE) design.&lt;/p&gt;
&lt;p&gt;At high level:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;31.6B total parameters&lt;/strong&gt;  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;~3.6B active parameters per token&lt;/strong&gt;, thanks to the MoE routing  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid layer stack&lt;/strong&gt; with interleaved Mamba‚Äë2 layers and grouped-query attention (GQA) Transformer layers   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A learned multi-layer perceptron (MLP) router&lt;/strong&gt; that activates 6 of 128 experts on each forward pass, delivering both efficiency and reasoning accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This combination enables Nemotron 3 Nano to behave like a much larger model in terms of reasoning quality‚Äîwhile maintaining the speed and cost profile expected of a lightweight architecture&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the Nemotron-Nano-3-30B-A3B architecture showing four sequential blocks. Each block contains repeating Mamba-2 layers and MoE units, with attention layers interspersed in the first and third blocks. The blocks repeat x5, x3, x1, and x4 times respectively, illustrating the hybrid Mamba-Transformer design with MoE layers replacing FFNs." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/OZe39if73d6P8vi6sDDNd.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Nemotron 3 Nano architecture. It uses a hybrid Mamba-Transformer backbone, similar to Nemotron Nano v2, but replaces standard feed-forward network (FFN) layers with sparse MoE layers to significantly boost efficiency and scalability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nemotron 3 Nano is built for agentic, reasoning, tool-use, and chat tasks and supports a&lt;br /&gt;context length up to 1M tokens.&lt;/p&gt;
&lt;p&gt;It extends the Nemotron model family we released earlier in the year, continuing the progression toward increasingly accurate and efficient open models for reasoning and agent development.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The roadmap graphic illustrates the evolution of Nemotron model families: **Nemotron 1** enhances Llama models with stronger reasoning capabilities, **Nemotron 2** introduces a hybrid Mamba-Transformer architecture, delivering state-of-the-art accuracy and efficiency, **Nemotron 3** adds sparse MoE to the hybrid design, further improving accuracy, throughput, latency, and overall compute efficiency." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/O-jBDwGrrb9T3GGi1qAHo.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: NVIDIA Nemotron family of open models is engineered for advanced reasoning and agentic tasks, delivering leading accuracies, and best-in-class efficiencies&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We employed a multi-stage pipeline combining massive-scale pre-training, specialized supervised fine-tuning (SFT), and advanced reinforcement learning techniques to refine the reasoning abilities and agentic behavior.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Pre-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron 3 Nano was trained on a  25-trillion-token corpus (including 2.5T of new Common Crawl tokens), spanning web crawls, code and math, Wikipedia and academic text, multilingual content (15 Pre-training followed a two-phase strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Phase 1: Diversity (first 94%)&lt;/strong&gt;&lt;br /&gt;Broad , diverse mixture to maximize coverage and generalization.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Phase 2: Quality (final 6%)&lt;/strong&gt;&lt;br /&gt;High-quality sources such as Wikipedia to refine accuracy and consistency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Long-Context Extension&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Context length for Nemotron 3 Nano was extended by adding a continued pre-training (CPT) stage at 512k sequence length. A mixture of 512k and 4k sequence length training preserved short benchmark scores while extending the context length. We included synthetic data designed to support long-range retrieval, multi-hop reasoning, multi-document information aggregation, and related capabilities across different stages of training.&lt;/p&gt;
&lt;p&gt;We are releasing a large portion of these  pretraining datasets openly on Hugging Face. These additions contribute 3 trillion new tokens to the Nemotron-Pretraining series, with higher-fidelity coverage of code, math, and reasoning. Enhanced synthetic augmentation and annotation pipelines increase data density and structure, improving training efficiency and directly contributing to Nemotron-3 Nano's strong quality profile.&lt;/p&gt;
&lt;p&gt;With Nemotron 3, we‚Äôve learned that quantity without quality isn‚Äôt useful. Our pre-training data continues to shift toward efficient data: smarter filtration, rewritten and improved samples, and nearly half a trillion tokens of rescued math and code that previous pipelines would have discarded. This focus on signal over noise directly enables smarter, smaller models that are cheaper to train and run, without sacrificing accuracy.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This included Supervised fine-tuning (SFT) and two distinct stages of reinforcement learning, RLVR and RLHF. These stages specialize the model for agentic workflows, tool use, high-quality reasoning, and chat tasks.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Supervised Finetuning&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our SFT recipe was improved from Nano v2 to better support complex agentic behaviors. Improvements included greater dataset diversity, higher data quality, and explicit training for multi-step and multi-turn reasoning.&lt;/p&gt;
&lt;p&gt;The model learns both reasoning ON/OFF modes directly from the chat template:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reasoning ON:&lt;/strong&gt; multi-step mode, where the model preserves and builds upon its prior chain-of-thought within a task.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning OFF:&lt;/strong&gt; multi-turn mode, where reasoning content is not carried over across turns, ensuring concise responses.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="The graph from Artificial Analysis plots small language reasoning models on intelligence index on the y-axis and output tokens per second on the x-axis. Nemotron 3 Nano delivers the highest throughput efficiency using the hybrid MoE architecture and leading accuracy with advanced Reinforcement Learning using NeMo Gym" src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/i169eq3mbepGags0uIB-y.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Nemotron 3 Nano delivers the highest throughput efficiency using the hybrid MoE architecture and leading accuracy with advanced Reinforcement Learning using NeMo Gym&lt;/p&gt;
&lt;p&gt;We are releasing the majority of SFT datasets and codebase openly.&lt;/p&gt;
&lt;p&gt;Our new post-training data release also expands the intelligence of the model by design. We added 13 million new post-training samples‚Äînearly tripling our previous release and making this the largest openly available post-training corpus by 2.5√ó. To reach higher reasoning accuracy, we blended cross-disciplinary domains including code, math, physics, and chemistry to create novel, multi-step problems that don‚Äôt exist in scraped web data. This helps the model reason about questions that fall between fields, where real scientific and technical progress often happens.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Multi environment Reinforcement Learning from Verifiable Rewards (RLVR)&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Nemotron 3 Nano was trained simultaneously across many distinct environments - spanning math, code, question answering, instruction following, multi-step tool use, multi-turn conversations, and structured output among others, using synchronous GRPO (Group Relative Policy Optimization). This multi-environment RLVR stage ensures uniform improvement across domains, reduced overfitting to any single benchmark, and more reliable agentic behavior in real-world workflows.&lt;/p&gt;
&lt;p&gt;&lt;img alt="This figure shows multiple different environment reward curves over training steps, showcasing how the model was learning many different capabilities simultaneously." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/ha57qyj3KKsxmxea1frIR.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Uniform improvements due to training simultaneously on multiple RL environments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Models need more than textbooks to train ‚Äî they need a gym. NVIDIA is one of the only open model providers releasing both reinforcement learning datasets and the environments used to train them. This enables developers to test agents, capture critical edge cases, and prevent model drift over time. In this release, we are adding 10+ new RL environments covering competitive coding, advanced math, and even real-world calendar scheduling.&lt;/p&gt;
&lt;p&gt;We are also open-sourcing all the essential RLVR infrastructure‚Äîthe environments including their datasets and code used to build and scale them. These components form the foundation of the new NVIDIA NeMo Gym library, which enables scalable RL environment construction. &lt;/p&gt;
&lt;p&gt;Training at scale is executed using NVIDIA NeMo RL, our high-performance RL library.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reinforce Learning Using Human Feedback (RLHF)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To further refine the model‚Äôs conversational quality, we trained a generative reward model (GenRM) using GRPO on Qwen3-235B-A22B. &lt;/p&gt;
&lt;p&gt;Given a conversation history, a new user query, and two candidate assistant responses, the GenRM explicitly reasons about the strengths and weaknesses of each response, produces individual helpfulness scores and generates a relative ranking between the candidates. These  reward signals are then used in an RLHF stage to improve helpfulness, coherence, correctness, and overall chat experience in Nemotron 3 Nano.&lt;/p&gt;
&lt;p&gt;The combined post-training pipeline‚ÄîSFT+RLVR+RLHF‚Äîproduces the final Nemotron 3 Nano 30B-A3B model.&lt;/p&gt;
&lt;p&gt;As models evolve into multi-step agents that use tools, they face entirely new safety and security challenges. To support responsible deployment, we are releasing an agentic safety dataset featuring nearly 11,000 labeled traces from realistic, tool-using workflows. This gives developers the data they need to evaluate, diagnose, and mitigate safety risks before agentic systems reach production.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Why We Needed Better RL Infrastructure
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;During development, the limitations of existing RL tooling became clear. Training large reasoning models with RL is difficult because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-step rollouts are complex to orchestrate  &lt;/li&gt;
&lt;li&gt;Tool integrations are often brittle  &lt;/li&gt;
&lt;li&gt;Orchestration logic can conflict with training loop design  &lt;/li&gt;
&lt;li&gt;Collecting rollout data at scale is slow and difficult  &lt;/li&gt;
&lt;li&gt;Most high-quality RL environments are closed and proprietary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result, meaningful RL training has historically been accessible only to major AI labs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		NeMo Gym: Opening RL to Everyone
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To overcome these challenges NVIDIA built NeMo Gym, an open-source standardized library for building and scaling RL environments.&lt;br /&gt;NeMo Gym powers reinforcement learning pipelines used in Nemotron 3 Nano, and now gives developers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ready-to-use RL environments across math, code, tool use, multi-turn reasoning, and agentic workflows  &lt;/li&gt;
&lt;li&gt;The ability to build custom RL environments with verifiable reward logic  &lt;/li&gt;
&lt;li&gt;Ecosystem interoperability with NeMo RL and other training frameworks (TRL, Unsloth, VeRL underway)  &lt;/li&gt;
&lt;li&gt;High-throughput rollout orchestration, enabling large-scale RL training  &lt;/li&gt;
&lt;li&gt;A practical pathway to perform RL on their own models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NeMo Gym is a flexible open source library for building and running RL training environments. It is part of the broader NVIDIA NeMo software suite for end-to-end model training and provides infrastructure for designing, running, and scaling complex RL environments. &lt;/p&gt;
&lt;p&gt;Battle tested through the development of the entire Nemotron 3 model family, NeMo Gym includes the core environment development infrastructure, a growing collection of ready-to-use training environments alongside the datasets used in RLVR, and integration with NeMo RL, the high-performance and efficient RL training engine with support for advanced RL training algorithms, end-to-end FP8 training and async RL.&lt;/p&gt;
&lt;p&gt;With NeMo Gym, teams can quickly assemble environments using modular server components and templates, integrate external tools, systems, or databases, and orchestrate long-context, multi-step, multi-turn rollouts. This allows training environments to be iterated on and shared independent of the training loop.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram illustrating interaction between an RL Training Framework on the left and NeMo Gym on the right. The training framework sends task prompts  to the agent server in the NeMo Gym. The agent server coordinates with the policy model server and external resources server to collect rollouts and verify task performance. The scored trajectories are returned back to the Training Framework for model updates." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/go-hdGzZpaKRJChrl-o6k.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: How NeMo Gym fits into the RL training loop: The RL training framework (e.g., NeMo RL) sends task prompts to NeMo Gym, which operates as a set of independent HTTP services. Inside NeMo Gym, the agent server orchestrates rollouts by coordinating the policy model server (generation) and external resources server (tools and rewards). NeMo Gym returns model trajectories and rewards to the training framework, which then  updates and refits the policy model.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By decoupling RL environments from RL training frameworks, NeMo Gym works seamlessly with many popular training frameworks (such as NeMo RL), supports high-throughput, concurrent rollout collection, and enables large-scale distributed RL training. This separation of concerns makes it easy to scale RL workflows and adapt environments as training objectives evolve.&lt;/p&gt;
&lt;p&gt;To accelerate experimentation, NeMo Gym ships with an expanding RL Hub‚Äîa catalog of ready-to-use domain-specific environments that developers can use immediately or extend. Current domains include math, coding, instruction following, multi-step tool use, multi-turn structured conversations. Practitioners can fine-tune models on these environments out of the box, reuse community contributions, or publish their own.&lt;/p&gt;

&lt;p&gt;Nemotron 3 Nano (30B A3B) delivers state-of-the-art accuracy in an exceptionally cost-efficient package. It offers up to 3.3x higher throughput than leading open-source models of similar size (see Figure 1), while supporting a 1M-token context window ‚Äîperforming well on long-context reasoning benchmarks.&lt;/p&gt;
&lt;p&gt;Built for high-volume, real-time execution, Nemotron 3 Nano excels in math and coding, multi-step tool calling, and multi-turn agentic workflows. It also retains the classic Nemotron Thinking ON/OFF modes and Thinking Budget controls, giving developers the ability to tune exactly how much the model thinks for each task.&lt;/p&gt;
&lt;p&gt;With this release, we are also introducing NeMo Gym, containing ready-to-use training environments we developed during the course of Nemotron 3 training, and the infrastructure to build your own training environments and scale rollout collection. &lt;/p&gt;
&lt;p&gt;We are releasing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full model weights  &lt;/li&gt;
&lt;li&gt;The complete training recipe, including SFT, RLVR, and RLHF  &lt;/li&gt;
&lt;li&gt;Most of the datasets (pre-training, post-training) used throughout the training pipeline  &lt;/li&gt;
&lt;li&gt;Training frameworks that power Nemotron 3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Everything you need to study, reproduce, or extend the model is available openly.&lt;/p&gt;
&lt;p&gt;Get started with Nemotron 3 Nano:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Download the model:&lt;/strong&gt; Now available on Hugging Face.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Try hosted endpoints:&lt;/strong&gt; Run queries instantly on OpenRouter or build.nvidia.com.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy at scale:&lt;/strong&gt; Use our cookbooks for vLLM, TRT-LLM, and SGLang  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Experiment, develop and run at the edge:&lt;/strong&gt; Available on edge devices such as NVIDIA RTX AI PCs and Workstations and DGX Spark via Llama.cpp, LM Studio and Unsloth&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a deep dive into the architecture, datasets, and benchmarks, read the full Nemotron 3 Nano Technical Report.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models</guid><pubDate>Mon, 15 Dec 2025 14:08:17 +0000</pubDate></item><item><title>Nvidia reportedly weighs ramping up H200 production to meet surging demand in China (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/nvidia-is-reportedly-weighs-ramping-up-h200-production-to-meet-surging-demand-in-china/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205210966.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After successfully lobbying the Trump administration to approve the sale of its H200 chips to China, Nvidia is now thinking of ramping up production of the chips as Chinese companies rush to place orders, Reuters reported, citing anonymous sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most powerful of Nvidia‚Äôs previous Hopper generation of graphics processing units (GPUs) made for training large language models, the H200 chips previously could not be sold in China, as the previous Biden administration had proposed rules limiting sales of advanced AI chips in the country. But the Department of Commerce last week gave Nvidia the nod to sell H200 GPUs in China, in exchange for a 25% cut of sales of those chips. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia is now seeing such strong demand from Chinese companies that it is considering adding more capacity, Reuters reported. However, Chinese officials are still deciding whether to allow the import of the H200 chips, which are said to be significantly more powerful than the H20 GPUs Nvidia had customized to sell in China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the chipmaker, expanding production of the H200 GPUs would let it tap latent demand in a country that is racing to develop its own homegrown AI chips. Competition and national security concerns in the West have hampered the availability of the latest and most powerful hardware for training AI models in China, where companies have resorted to focusing on efficiency over sheer scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chinese companies, including Alibaba and ByteDance, which are developing their own AI models, have already been in touch with Nvidia to figure out large orders for the H200 chips, which are being produced in limited quantities, the report added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe are managing our supply chain to ensure that licensed sales of the H200 to authorized customers in China will&amp;nbsp;have no impact on our ability to supply customers in the United States,‚Äù an Nvidia spokesperson said in an emailed statement.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205210966.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After successfully lobbying the Trump administration to approve the sale of its H200 chips to China, Nvidia is now thinking of ramping up production of the chips as Chinese companies rush to place orders, Reuters reported, citing anonymous sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most powerful of Nvidia‚Äôs previous Hopper generation of graphics processing units (GPUs) made for training large language models, the H200 chips previously could not be sold in China, as the previous Biden administration had proposed rules limiting sales of advanced AI chips in the country. But the Department of Commerce last week gave Nvidia the nod to sell H200 GPUs in China, in exchange for a 25% cut of sales of those chips. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia is now seeing such strong demand from Chinese companies that it is considering adding more capacity, Reuters reported. However, Chinese officials are still deciding whether to allow the import of the H200 chips, which are said to be significantly more powerful than the H20 GPUs Nvidia had customized to sell in China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the chipmaker, expanding production of the H200 GPUs would let it tap latent demand in a country that is racing to develop its own homegrown AI chips. Competition and national security concerns in the West have hampered the availability of the latest and most powerful hardware for training AI models in China, where companies have resorted to focusing on efficiency over sheer scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chinese companies, including Alibaba and ByteDance, which are developing their own AI models, have already been in touch with Nvidia to figure out large orders for the H200 chips, which are being produced in limited quantities, the report added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe are managing our supply chain to ensure that licensed sales of the H200 to authorized customers in China will&amp;nbsp;have no impact on our ability to supply customers in the United States,‚Äù an Nvidia spokesperson said in an emailed statement.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/nvidia-is-reportedly-weighs-ramping-up-h200-production-to-meet-surging-demand-in-china/</guid><pubDate>Mon, 15 Dec 2025 14:28:20 +0000</pubDate></item><item><title>First Voyage raises $2.5M for its AI companion that helps you build habits (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/first-voyage-raises-2-5m-for-its-ai-companion-helps-you-build-habits/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a world that‚Äôs rapidly filling up with AI-generated content, a startup called First Voyage wants to help people avoid all the AI slop blasted their way and instead build the habits they want. And it‚Äôs doing that by way of an AI companion app: Called Momo Self Care, the app offers a digital pet called Momo that you can take care of, and in return, it‚Äôll remind you to complete habit-building tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can set up reminders for what tasks they want to complete, and Momo will remind you of them. Similar to the hit productivity app Focus Friend, Momo also rewards you with coins for completing tasks that can be used to&amp;nbsp;purchase&amp;nbsp;items within the app to further customize the pet. Users can also talk to Momo about self-care, and the AI companion will recommend habits and tasks based on what you want to achieve.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúMomo helps users become the best versions of themselves, and users reward Momo with care, affection, and cute accessories,‚Äù co-founder and CEO Besart √áopa told TechCrunch. He launched the company with Egehan Ozsoy, who serves as CTO. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, First Voyage said it had raised $2.5 million in a seed funding round from a16z speedrun, SignalFire, True Global, and other investors.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075984" height="308" src="https://techcrunch.com/wp-content/uploads/2025/12/Momo-founders.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;First Voyage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Copa said Momo users have already created more than 2 million tasks on the platform, and the most popular habits relate to productivity, spirituality, and mindfulness.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But with the wave of AI apps and toys hitting the market, not to mention the burgeoning influence of AI chatbots like ChatGPT, Claude, and Grok, there‚Äôs increasing concern that these new, so-called ‚Äúcompanions‚Äù can lead to more harm than good.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;√áopa, for one,&amp;nbsp;believes that relationships between AI characters and humans will only increase in the next few years. However, he noted that the increasing number of AI apps aimed at wellness and self-care is at least better than those that target base urges. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe are happy so many founders [and] startups are working in the AI self-care wellness space instead of building waifus,‚Äù he said, adding that the ‚Äúpersonalization capability of AI will take the impact of these relationships to another level.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075853" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/Product-Image-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;First Voyage / Momo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;He noted that Momo has baked in safety guardrails, such as prompt filters to make sure that conversations between the AI and users stay within appropriate boundaries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh cash from the fundraise will be used to help launch Momo on the Android app store (it‚Äôs&amp;nbsp;already available on iOS). The First Voyage team also hopes to make Momo more intelligent in how it interacts with people.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúWe hope Momo and the community around it become a defining consumer brand that uses the best of AI, animation, and&amp;nbsp;gamification&amp;nbsp;to improve as many lives as possible,‚Äù √áopa&amp;nbsp;said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a world that‚Äôs rapidly filling up with AI-generated content, a startup called First Voyage wants to help people avoid all the AI slop blasted their way and instead build the habits they want. And it‚Äôs doing that by way of an AI companion app: Called Momo Self Care, the app offers a digital pet called Momo that you can take care of, and in return, it‚Äôll remind you to complete habit-building tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can set up reminders for what tasks they want to complete, and Momo will remind you of them. Similar to the hit productivity app Focus Friend, Momo also rewards you with coins for completing tasks that can be used to&amp;nbsp;purchase&amp;nbsp;items within the app to further customize the pet. Users can also talk to Momo about self-care, and the AI companion will recommend habits and tasks based on what you want to achieve.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúMomo helps users become the best versions of themselves, and users reward Momo with care, affection, and cute accessories,‚Äù co-founder and CEO Besart √áopa told TechCrunch. He launched the company with Egehan Ozsoy, who serves as CTO. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, First Voyage said it had raised $2.5 million in a seed funding round from a16z speedrun, SignalFire, True Global, and other investors.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075984" height="308" src="https://techcrunch.com/wp-content/uploads/2025/12/Momo-founders.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;First Voyage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Copa said Momo users have already created more than 2 million tasks on the platform, and the most popular habits relate to productivity, spirituality, and mindfulness.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But with the wave of AI apps and toys hitting the market, not to mention the burgeoning influence of AI chatbots like ChatGPT, Claude, and Grok, there‚Äôs increasing concern that these new, so-called ‚Äúcompanions‚Äù can lead to more harm than good.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;√áopa, for one,&amp;nbsp;believes that relationships between AI characters and humans will only increase in the next few years. However, he noted that the increasing number of AI apps aimed at wellness and self-care is at least better than those that target base urges. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe are happy so many founders [and] startups are working in the AI self-care wellness space instead of building waifus,‚Äù he said, adding that the ‚Äúpersonalization capability of AI will take the impact of these relationships to another level.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075853" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/Product-Image-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;First Voyage / Momo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;He noted that Momo has baked in safety guardrails, such as prompt filters to make sure that conversations between the AI and users stay within appropriate boundaries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh cash from the fundraise will be used to help launch Momo on the Android app store (it‚Äôs&amp;nbsp;already available on iOS). The First Voyage team also hopes to make Momo more intelligent in how it interacts with people.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúWe hope Momo and the community around it become a defining consumer brand that uses the best of AI, animation, and&amp;nbsp;gamification&amp;nbsp;to improve as many lives as possible,‚Äù √áopa&amp;nbsp;said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/first-voyage-raises-2-5m-for-its-ai-companion-helps-you-build-habits/</guid><pubDate>Mon, 15 Dec 2025 14:48:24 +0000</pubDate></item><item><title>The fast and the future-focused are revolutionizing motorsport (Artificial intelligence ‚Äì MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1127432/the-fast-and-the-future-focused-are-revolutionizing-motorsport/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/Rohit-Agnihotri-Dan-Cherowbrier-v2.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Infosys&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When the ABB FIA Formula E World Championship launched its first race through Beijing‚Äôs Olympic Park in 2014, the idea of all-electric motorsport still bordered on experimental. Batteries couldn‚Äôt yet last a full race, and drivers had to switch cars mid-competition. Just over a decade later, Formula E has evolved into a global entertainment brand broadcast in 150 countries, driving both technological innovation and cultural change in sport.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"Gen4, that's to come next year," says Dan Cherowbrier, Formula E‚Äôs chief technology and information officer. "You will see a really quite impressive car that starts us to question whether EV is there. It's actually faster‚Äîit's actually more than traditional [internal combustion engines] ICE."&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That acceleration isn‚Äôt just happening on the track. Formula E‚Äôs digital transformation, powered by its partnership with Infosys, is redefining what it means to be a fan. ‚ÄúIt's a movement to make motor sport accessible and exciting for the new generation,‚Äù says principal technologist at Infosys, Rohit Agnihotri.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;From real-time leaderboards and predictive tools to personalized storylines that adapt to what individual fans care most about‚Äîwhether it's a driver rivalry or battery performance‚ÄîFormula E and Infosys are using AI-powered platforms to create fan experiences as dynamic as the races themselves. "Technology is not just about meeting expectations; it's elevating the entire fan experience and making the sport more inclusive," says Agnihotri.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;AI is also transforming how the organization itself operates. "Historically, we would be going around the company, banging on everyone's doors and dragging them towards technology, making them use systems, making them move things to the cloud," Cherowbrier notes. "What AI has done is it's turned that around on its head, and we now have people turning up, banging on our door because they want to use this tool, they want to use that tool."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As audiences diversify and expectations evolve, Formula E is also a case study in sustainable innovation. Machine learning tools now help determine the most carbon-optimal way to ship batteries across continents, while remote broadcast production has sharply reduced travel emissions and democratized the company's workforce. These advances show how digital intelligence can expand reach without deepening carbon footprints.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For Cherowbrier, this convergence of sport, sustainability, and technology is just the beginning. With its data-driven approach to performance, experience, and impact, Formula E is offering a glimpse into how entertainment, innovation, and environmental responsibility can move forward in tandem.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"Our goal is clear," says Agnihotri. "Help Formula E be the most digital and sustainable motor sport in the world. The future is electric, and with AI, it's more engaging than ever."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Infosys.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Full Transcript:&amp;nbsp;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;Megan Tatum:&lt;/em&gt; From MIT Technology Review, I'm Megan Tatum, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab, and into the marketplace.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The ABB FIA Formula E World Championship, the world's first all-electric racing series, made its debut in the grounds of the Olympic Park in Beijing in 2014. A little more than 10 years later, it's a global entertainment brand with 10 teams, 20 drivers, and broadcasts in 150 countries. Technology is central to how Formula E is navigating that scale and to how it's delivering more powerful personalized experiences.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Two words for you: elevated fandom.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My guests today are Rohit Agnihotri, principal technologist at Infosys, and Dan Cherowbrier, CTIO of Formula E.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;This episode is produced in partnership with Infosys.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome, Rohit and Dan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan Cherowbrier&lt;/em&gt;: Hi. Thanks for having us.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Dan, as I mentioned there, the first season of the ABB FIA Formula E World Championship launched in 2014. Can you talk us through how the first all-electric motor sport has evolved in the last decade? How has it changed in terms of its scale, the markets it operates in, and also, its audiences, of course?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: When Formula E launched back in 2014, there were hardly any domestic EVs on the road. And probably if you're from London, the ones you remember are the hybrid Priuses; that was what we knew of really. And at the time, they were unable to get a battery big enough for a car to do a full race. So the first generation of car, the first couple of seasons, the driver had to do a pit stop midway through the race, get out of one car, and get in another car, and then carry on, which sounds almost farcical now, but it's what you had to do then to drive innovation, is to do that in order to go to the next stage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then in Gen2, that came up four years later, they had a battery big enough to start full races and start to actually make it a really good sport. Gen3, they're going for some real speeds and making it happen. Gen4, that's to come next year, you'll see acceleration in line with Formula One. I've been fortunate enough to see some of the testing. You will see a really quite impressive car that starts us to question whether EV is there. It's actually faster, it's actually more than traditional ICE.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's the tech of the car. But then, if you also look at the sport and how people have come to it and the fans and the demographic of the fans, a lot has changed in the last 11 years. We were out to enter season 12. In the last 11 years, we've had a complete democratization of how people access content and what people want from content. And as a new generation of fan coming through. This new generation of fan is younger. They're more gender diverse. We have much closer to 50-50 representation in our fan base. And they want things personalized, and they're very demanding about how they want it and the experience they expect. No longer are you just able to give them one race and everybody watches the same thing. We need to make things for them. You see that sort of change that's come through in the last 11 years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: It's a huge amount of change in just over a decade, isn't it? To navigate. And I wonder, Rohit, what was the strategic plan for Infosys when associating with Formula E? What did Infosys see in partnering with such a young sport?&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. That's a great question, Megan. When we looked at Formula E, we didn't just see a racing championship. We saw the future. A sport, that's electric, sustainable, and digital first. That's exactly where Infosys wants to be, at the intersection of technology, innovation, and purpose. Our plan has three big goals. First, grow the fan base. Formula E wants to reach 500 million fans by 2030. That is not just a number. It's a movement to make motor sport accessible and exciting for the new generation. To make that happen, we are building an AI-powered platform that gives personalized content to the fans, so that every fan feels connected and valued. Imagine a fan in Tokyo getting race insights tailored for their favorite driver, while another in London gets a sustainability story that matters to him. That's the level of personalization we are aiming for.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Second, bringing technology innovation. We have already launched the Stats Centre, which turns race data into interactive stories. And soon, Race Centre will take this to the next level with real time leaderboards to the race or tracks, overtakes, attack mode timelines, and even AI generated live commentary. Fans will not just watch, they will interact, predict podium finishes, and share their views globally. And third, supports sustainability. Formula E is already net-zero, but now their goal is to cut carbon by 45% by 2030. We'll be enabling that through AI-driven sustainability, data management, tracking every watt of energy, every logistics decision. and modeling scenarios to make racing even greener. Partnering with a young sport gives us a chance to shape its digital future and show how technology can make racing exciting and responsible. For us, Formula E is not just a sport, it's a statement about where the world is headed.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. 500 million fans, that's a huge number, isn't it? And with more scale often comes a kind of greater expectation. Dan, I know you touched on this a little in your first question, but what is it that your fans now really want from their interactions? Can you talk a bit more about what experiences they're looking for? And also, how complex that really is to deliver that as well?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: I think a really telling thing about the modern day fan is I probably can't tell you what they want from their experiences, because it's individual and it's unique for each of them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Of course.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: And it's changing and it's changing so fast. What somebody wants this month is going to be different from what they want in a couple of months' time. And we're having to learn to adapt to that. My CTO title, we often put focus on the technology in the middle of it. That's what the T is. Actually, if you think about it, it‚Äôs continual transformation officer. You are constantly trying to change what you deliver and how you deliver it. Because if fans come through, they find new experiences, they find that in other sports. Sometimes not in sports, they find it outside, and then they're coming in, and they expect that from you. So how can we make them more part of the sport, more personalized experience, get to know the athletes and the personalities and the characters within it? We're a very technology centric sport. A lot of motor sport is, but really, people want to see people, right? And even when it's technology, they want to see people interacting with technology, and it's how do you get that out to show people.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Yeah, it's no mean feat. Rohit, you've worked with brands on delivering these sort of fan experiences across different sports. Is motor sports perhaps more complicated than others, given that fans watch racing for different reasons than just a win? They could be focused on team dynamics, a particular driver, the way the engine is built, and so on and so forth. How does motor sports compare and how important is it therefore, that Formula E has embraced technology to manage expectations?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah, that's an interesting point. Motor sports are definitely more complex than other sports. Fans don't just care about who wins, they care about how some follow team strategies, others love driver rivalries, and many are fascinated by the car technology. Formula E adds another layer, sustainability and electric innovation. This makes personalization really important. Fans want more than results. They want stories and insights. Formula E understood this early and embraced technology.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Think about the data behind a single race, lap times, energy usage, battery performance, attack mode activation, pit strategies, it's a lot of data. If you just show the raw numbers, it's overwhelming. But with Infosys Topaz, we turn that into simple and engaging stories. Fans can see how a driver fought back from 10th place to finish on the podium, or how a team managed energy better to gain an edge. And for new fans, we are adding explainer videos and interactive tools in the Race Center, so that they can learn about their sport easily. This is important because Formula E is still young, and many fans are discovering it for the first time. Technology is not just about meeting expectations; it's elevating the entire fan experience and making the sport more inclusive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: There's an awful lot going on there. What are some of the other ways that Formula E has already put generative AI and other emerging technologies to use? Dan, when we've spoken about the demand for more personalized experiences, for example.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: I see the implementation of AI for us in three areas. We have AI within the sport. That's in our DNA of the sport. Now, each team is using that, but how can we use that as a championship as well? How do we make it a competitive landscape? Now, we have AI that is in the fan-facing product. That's what we're working heavily on Infosys with, but we also have it in our broadcast product. As an example, you might have heard of a super slow-mo camera. A super slow-mo camera is basically, by taking three cameras and having them in exactly the same place so that you get three times the frame rate, and then you can do a slow-motion shot from that. And they used to be really expensive. Quite bulky cameras to put in. We are now using AI to take a traditional camera and interpolate between two frames to make it into a super slow image, and you wouldn't really know the difference. Now, the joy of that, it means every camera can now be a super slow-mo camera.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Wow.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: In other ways, we use it a little bit in our graphics products, and we iterate and we use it for things like showing driver audio. When the driver is speaking to his engineer or her engineer in the garage, we show that text now on screen. We do that using AI. We use AI to pick out the difference between the driver and another driver and the team engineer or the team principal and show that in a really good way.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And we wouldn't be able to do that. We're not big enough to have a team of 24 people on stenographers typing. We have to use AI to be able to do that. That's what's really helped us grow. And then the last one is, how we use it in our business. Because ultimately, as we've got the fans, we've got the sport, but we also are running a business and we have to pick up these racetracks and move them around the world, and we have all these staff who have to get places. We have insurance who has to do all that kind of stuff, and we use it heavily in that area, particularly when it comes to what has a carbon impact for us.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So things like our freight and our travel. And we are using the AI tools to tell us, a battery for instance, should we fly it? Should we send it by sea freight? Should we send it by row freight? Or should we just have lots of them? And that sort of depends. Now, a battery, if it was heavy, you'd think you probably wouldn't fly it. But actually, because of the materials in it, because of the source materials that make it, we're better off flying it. We've used AI to work through all those different machinations of things that would be too difficult to do at speed for a person.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Well, sounds like there's some fascinating things going on. I mean, of course, for a global brand, there is also the challenge of working in different markets. You mentioned moving everything around the world there. Each market with its own legal frameworks around data privacy, AI. How has technology also helped you navigate all of that, Dan?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: The other really interesting thing about AI is... I've worked in technology leadership roles for some time now. And historically, we would be going around the company, banging on everyone's doors and dragging them towards technology, making them use systems, making them move things to the cloud and things like that. What AI has done is it's turned that around on its head, and we now have people turning up, banging on our door because they want to use this tool, they want to use that tool. And we're trying to accommodate all of that and it's a great pleasure to see people that are so keen. AI is driving the tech adoption in general, which really helps the business.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Megan: Dan, as the world's first all-electric motor sport series, sustainability is obviously a real cornerstone of what Formula E is looking to do. Can you share with us how technology is helping you to achieve some of your ambitions when it comes to sustainability?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: We've been the only sport with a certified net-zero pathway, and we have to stay that part. It's a really core fundamental part of our DNA. I sit on our management team here. There is a sustainability VP that sits there as well, who checks and challenges everything we do. She looks at the data centers we use, why we use them, why we've made the decisions we've made, to make sure that we're making them all for the right reasons and the right ways. We specifically embed technology in a couple of ways. One is, we mentioned a little bit earlier, on our freight. Formula E's freight for the whole championship is probably akin to one Formula One team, but it's still by far, our biggest contributor to our impact. So we look about how we can make sure that we've refined that to get the minimum amount of air freight and sea freight, and use local wherever we can. That's also part of our pledge about investing in the communities that we race in.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The second then is about our staff travel. And we've done a really big piece of work over the last four to five years, partly accelerated through the covid-19 era actually, of doing remote working and remote TV production. Used to be traditionally, you would fly a hundred plus people out to racetracks, and then they would make the television all on site in trucks, and then they would be satellite distributed out of the venue. Now, what we do is we put in some internet connections, dual and diverse internet connections, and we stream every single camera back.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: That means on site, we only need camera operators. Some of them actually, are remotely operated anyway, but we need camera operators, and then some engineering teams to just keep everything running. And then back in our home base, which is in London, in the UK, we have our remote production center where we layer on direction, graphics, audio, replay, team radio, all of those bits that break the color and make the program and add to that significant body of people. We do that all remotely now. Really interesting actually, a bit. So that's the carbon sustainability story, but there is a further ESG piece that comes out of it and we haven't really accommodated when we went into it, is the diversity in our workforce by doing that. We were discovering that we had quite a young, equally diverse workforce until around the age of 30. And then once that happened, then we were finding we were losing women, and that's really because they didn't want to travel.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: And that's the age of people starting to have children, and things were starting to change. And then we had some men that were traveling instead, and they weren't seeing their children and it was sort of dividing it unnecessarily. But by going remote, by having so much of our people able to remotely... Or even if they do have to travel, they're not traveling every single week. They're now doing that one in three. They're able to maintain the careers and the jobs they want to do, whilst having a family lifestyle. And it also just makes a better product by having people in that environment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: That's such an interesting perspective, isn't it? It's a way of environmental sustainability intersects with social sustainability. And Rohit, and your work are so interesting. And Rohit, can you share any of the ways that Infosys has worked with Formula E, in terms of the role of technology as we say, in furthering those ambitions around sustainability?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. Infosys understands that sustainability is at the heart of Formula E, and it's a big part of why this partnership matters. Formula E is already net-zero certified, but now, they have an ambitious goal to cut carbon emissions by 45%. Infosys is helping in two ways. First, we have built AI-powered sustainability data tools that make carbon reporting accurate and traceable. Every watt of energy, every logistic decision, every material use can be tracked. Second, we use predictive analytics to model scenarios, like how changing race logistics or battery technology impact emissions so Formula E can make smarter, greener decisions. For us, it's about turning sustainability from a report into an action plan, and making Formula E a global leader in green motor sport.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: And in April 2025, Formula E working with Infosys launched its Stats Centre, which provides fans with interactive access to the performances of their drivers and teams, key milestones and narratives. I know you touched on this before, but I wonder if you could tell us a bit more about the design of that platform, Rohit, and how it fits into Formula E's wider plans to personalize that fan experience?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Sure. The Stats Centre was a big step forward. Before this, fans had access to basic statistics on the website and the mobile app, but nothing told the full story and we wanted to change that. Built on Infosys Topaz, the Stats Centre uses AI to turn race data into interactive stories. Fans can explore key stat cards that adapt to race timelines, and even chat with an AI companion to get instant answers. It's like having a person race analyst at your fingertips. And we are going further. Next year, we'll launch Race Centre. It'll have live data boards, 2D track maps showing every driver's position, overtakes and more attack timelines, and AI-generated commentary. Fans can predict podium finishes, vote for the driver of the race, and share their views on social media. Plus, we are adding video explainers for new fans, covering rules, strategies, and car technology. Our goal is simple: make every moment exciting and easy to understand. Whether you are a hardcore fan or someone watching Formula E for the first time, you'll feel connected and informed.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. Sounds brilliant. And as you've explained, Dan, leveraging data and AI can come with these huge benefits when it comes to the depth of fan experience that you can deliver, but it can also expose you to some challenges. How are you navigating those at Formula E?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: The AI generation has presented two significant challenges to us. One is that traditional SEO, traditional search engine optimization, goes out the window. Right? You are now looking at how do we design and build our systems and how do we populate them with the right content and the right data, so that the engines are picking it up correctly and displaying it? The way that the foundational models are built and the speed and the cadence of which they're updated, means quite often... We're a very fast-changing organization. We're a fast-changing product. Often, the models don't keep up. And that's because they are a point in time when they were trained. And that's something that the big organizations, the big tech organizations will fix with time. But for now, what we have to do is we have to learn about how we can present our fan-facing, web-facing products to show that correctly. That's all about having really accurate first-party content, effectively earned media. That's the piece we need to do.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then the second sort of challenge is sadly, whilst these tools are available to all of us, and we are using them effectively, so are another part of the technology landscape, and that is the cybersecurity basically they come with. If you look at the speed of the cadence and severity of hacks that are happening now, it's just growing and growing and growing, and that's because they have access to these tools too. And we're having to really up our game and professionalize. And that's really hard for an innovative organization. You don't want to shut everything down. You don't want to protect everything too much because you want people to be able to try new things. Right? If I block everything to only things that the IT team had heard of, we'd never get anything new in, and it's about getting that balance right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: Rohit, you probably have similar experiences?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: How has Infosys worked with Formula E to help it navigate some of that, Rohit?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. Infosys has helped Formula E tackle some of the challenges in three key ways, simplify complex race data into engaging fan experience through platforms like Stats Centre, building a secure and scalable cloud data backbone for the real-time insights, and enabling sustainability goals with AI-driven carbon tracking and predictive analytics. This solution makes the sport interactive, more digital, and more responsible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. I wondered if we could close with a bit of a future forward look. Can you share with us any innovations on the horizon at Formula E that you are really excited about, Dan?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: We have mentioned the Race Centre is going to launch in the next couple of months, but the really exciting thing for me is we've got an amazing season ahead of us. It's the last season of our Gen3 car, with 10 really exciting teams on the grid. We are going at speed with our tech innovation roadmap and what our fans want. And we're building up towards our Gen4 car, which will come out for season 13 in a year's time. That will get launched in 2026, and I think it will be a game changer in how people perceive electric motor sport and electric cars in general.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: It sounds like there's all sorts of exciting things going on. And Rohit too, what's coming up via this partnership that you are really looking forward to sharing with everyone?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Two things stand out for me. First is the AI-powered fan data platform that I've already spoken about. Second is the launch of Race Centre. It's going to change how fans experience live racing. And beyond final engagement, we are helping Formula E lead in sustainability with AI tools that model carbon impact and optimize logistics. This means every race can be smarter and greener. Our goal is clear: help Formula E be the most digital and sustainable motor sport in the world. The future is electric, and with AI, it's more engaging than ever.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. Thank you so much, both. That was Rohit Agnihotri, principal technologist at Infosys, and Dan Cherowbrier, CITO of Formula E, whom I spoke with from Brighton, England.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor and host for Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. And if you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review and this episode was produced by Giro Studios. Thanks for listening.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/Rohit-Agnihotri-Dan-Cherowbrier-v2.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Infosys&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When the ABB FIA Formula E World Championship launched its first race through Beijing‚Äôs Olympic Park in 2014, the idea of all-electric motorsport still bordered on experimental. Batteries couldn‚Äôt yet last a full race, and drivers had to switch cars mid-competition. Just over a decade later, Formula E has evolved into a global entertainment brand broadcast in 150 countries, driving both technological innovation and cultural change in sport.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"Gen4, that's to come next year," says Dan Cherowbrier, Formula E‚Äôs chief technology and information officer. "You will see a really quite impressive car that starts us to question whether EV is there. It's actually faster‚Äîit's actually more than traditional [internal combustion engines] ICE."&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That acceleration isn‚Äôt just happening on the track. Formula E‚Äôs digital transformation, powered by its partnership with Infosys, is redefining what it means to be a fan. ‚ÄúIt's a movement to make motor sport accessible and exciting for the new generation,‚Äù says principal technologist at Infosys, Rohit Agnihotri.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;From real-time leaderboards and predictive tools to personalized storylines that adapt to what individual fans care most about‚Äîwhether it's a driver rivalry or battery performance‚ÄîFormula E and Infosys are using AI-powered platforms to create fan experiences as dynamic as the races themselves. "Technology is not just about meeting expectations; it's elevating the entire fan experience and making the sport more inclusive," says Agnihotri.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;AI is also transforming how the organization itself operates. "Historically, we would be going around the company, banging on everyone's doors and dragging them towards technology, making them use systems, making them move things to the cloud," Cherowbrier notes. "What AI has done is it's turned that around on its head, and we now have people turning up, banging on our door because they want to use this tool, they want to use that tool."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As audiences diversify and expectations evolve, Formula E is also a case study in sustainable innovation. Machine learning tools now help determine the most carbon-optimal way to ship batteries across continents, while remote broadcast production has sharply reduced travel emissions and democratized the company's workforce. These advances show how digital intelligence can expand reach without deepening carbon footprints.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For Cherowbrier, this convergence of sport, sustainability, and technology is just the beginning. With its data-driven approach to performance, experience, and impact, Formula E is offering a glimpse into how entertainment, innovation, and environmental responsibility can move forward in tandem.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"Our goal is clear," says Agnihotri. "Help Formula E be the most digital and sustainable motor sport in the world. The future is electric, and with AI, it's more engaging than ever."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Infosys.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Full Transcript:&amp;nbsp;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;Megan Tatum:&lt;/em&gt; From MIT Technology Review, I'm Megan Tatum, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab, and into the marketplace.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The ABB FIA Formula E World Championship, the world's first all-electric racing series, made its debut in the grounds of the Olympic Park in Beijing in 2014. A little more than 10 years later, it's a global entertainment brand with 10 teams, 20 drivers, and broadcasts in 150 countries. Technology is central to how Formula E is navigating that scale and to how it's delivering more powerful personalized experiences.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Two words for you: elevated fandom.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My guests today are Rohit Agnihotri, principal technologist at Infosys, and Dan Cherowbrier, CTIO of Formula E.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;This episode is produced in partnership with Infosys.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome, Rohit and Dan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan Cherowbrier&lt;/em&gt;: Hi. Thanks for having us.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Dan, as I mentioned there, the first season of the ABB FIA Formula E World Championship launched in 2014. Can you talk us through how the first all-electric motor sport has evolved in the last decade? How has it changed in terms of its scale, the markets it operates in, and also, its audiences, of course?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: When Formula E launched back in 2014, there were hardly any domestic EVs on the road. And probably if you're from London, the ones you remember are the hybrid Priuses; that was what we knew of really. And at the time, they were unable to get a battery big enough for a car to do a full race. So the first generation of car, the first couple of seasons, the driver had to do a pit stop midway through the race, get out of one car, and get in another car, and then carry on, which sounds almost farcical now, but it's what you had to do then to drive innovation, is to do that in order to go to the next stage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then in Gen2, that came up four years later, they had a battery big enough to start full races and start to actually make it a really good sport. Gen3, they're going for some real speeds and making it happen. Gen4, that's to come next year, you'll see acceleration in line with Formula One. I've been fortunate enough to see some of the testing. You will see a really quite impressive car that starts us to question whether EV is there. It's actually faster, it's actually more than traditional ICE.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's the tech of the car. But then, if you also look at the sport and how people have come to it and the fans and the demographic of the fans, a lot has changed in the last 11 years. We were out to enter season 12. In the last 11 years, we've had a complete democratization of how people access content and what people want from content. And as a new generation of fan coming through. This new generation of fan is younger. They're more gender diverse. We have much closer to 50-50 representation in our fan base. And they want things personalized, and they're very demanding about how they want it and the experience they expect. No longer are you just able to give them one race and everybody watches the same thing. We need to make things for them. You see that sort of change that's come through in the last 11 years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: It's a huge amount of change in just over a decade, isn't it? To navigate. And I wonder, Rohit, what was the strategic plan for Infosys when associating with Formula E? What did Infosys see in partnering with such a young sport?&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. That's a great question, Megan. When we looked at Formula E, we didn't just see a racing championship. We saw the future. A sport, that's electric, sustainable, and digital first. That's exactly where Infosys wants to be, at the intersection of technology, innovation, and purpose. Our plan has three big goals. First, grow the fan base. Formula E wants to reach 500 million fans by 2030. That is not just a number. It's a movement to make motor sport accessible and exciting for the new generation. To make that happen, we are building an AI-powered platform that gives personalized content to the fans, so that every fan feels connected and valued. Imagine a fan in Tokyo getting race insights tailored for their favorite driver, while another in London gets a sustainability story that matters to him. That's the level of personalization we are aiming for.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Second, bringing technology innovation. We have already launched the Stats Centre, which turns race data into interactive stories. And soon, Race Centre will take this to the next level with real time leaderboards to the race or tracks, overtakes, attack mode timelines, and even AI generated live commentary. Fans will not just watch, they will interact, predict podium finishes, and share their views globally. And third, supports sustainability. Formula E is already net-zero, but now their goal is to cut carbon by 45% by 2030. We'll be enabling that through AI-driven sustainability, data management, tracking every watt of energy, every logistics decision. and modeling scenarios to make racing even greener. Partnering with a young sport gives us a chance to shape its digital future and show how technology can make racing exciting and responsible. For us, Formula E is not just a sport, it's a statement about where the world is headed.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. 500 million fans, that's a huge number, isn't it? And with more scale often comes a kind of greater expectation. Dan, I know you touched on this a little in your first question, but what is it that your fans now really want from their interactions? Can you talk a bit more about what experiences they're looking for? And also, how complex that really is to deliver that as well?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: I think a really telling thing about the modern day fan is I probably can't tell you what they want from their experiences, because it's individual and it's unique for each of them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Of course.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: And it's changing and it's changing so fast. What somebody wants this month is going to be different from what they want in a couple of months' time. And we're having to learn to adapt to that. My CTO title, we often put focus on the technology in the middle of it. That's what the T is. Actually, if you think about it, it‚Äôs continual transformation officer. You are constantly trying to change what you deliver and how you deliver it. Because if fans come through, they find new experiences, they find that in other sports. Sometimes not in sports, they find it outside, and then they're coming in, and they expect that from you. So how can we make them more part of the sport, more personalized experience, get to know the athletes and the personalities and the characters within it? We're a very technology centric sport. A lot of motor sport is, but really, people want to see people, right? And even when it's technology, they want to see people interacting with technology, and it's how do you get that out to show people.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Yeah, it's no mean feat. Rohit, you've worked with brands on delivering these sort of fan experiences across different sports. Is motor sports perhaps more complicated than others, given that fans watch racing for different reasons than just a win? They could be focused on team dynamics, a particular driver, the way the engine is built, and so on and so forth. How does motor sports compare and how important is it therefore, that Formula E has embraced technology to manage expectations?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah, that's an interesting point. Motor sports are definitely more complex than other sports. Fans don't just care about who wins, they care about how some follow team strategies, others love driver rivalries, and many are fascinated by the car technology. Formula E adds another layer, sustainability and electric innovation. This makes personalization really important. Fans want more than results. They want stories and insights. Formula E understood this early and embraced technology.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Think about the data behind a single race, lap times, energy usage, battery performance, attack mode activation, pit strategies, it's a lot of data. If you just show the raw numbers, it's overwhelming. But with Infosys Topaz, we turn that into simple and engaging stories. Fans can see how a driver fought back from 10th place to finish on the podium, or how a team managed energy better to gain an edge. And for new fans, we are adding explainer videos and interactive tools in the Race Center, so that they can learn about their sport easily. This is important because Formula E is still young, and many fans are discovering it for the first time. Technology is not just about meeting expectations; it's elevating the entire fan experience and making the sport more inclusive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: There's an awful lot going on there. What are some of the other ways that Formula E has already put generative AI and other emerging technologies to use? Dan, when we've spoken about the demand for more personalized experiences, for example.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: I see the implementation of AI for us in three areas. We have AI within the sport. That's in our DNA of the sport. Now, each team is using that, but how can we use that as a championship as well? How do we make it a competitive landscape? Now, we have AI that is in the fan-facing product. That's what we're working heavily on Infosys with, but we also have it in our broadcast product. As an example, you might have heard of a super slow-mo camera. A super slow-mo camera is basically, by taking three cameras and having them in exactly the same place so that you get three times the frame rate, and then you can do a slow-motion shot from that. And they used to be really expensive. Quite bulky cameras to put in. We are now using AI to take a traditional camera and interpolate between two frames to make it into a super slow image, and you wouldn't really know the difference. Now, the joy of that, it means every camera can now be a super slow-mo camera.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Wow.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: In other ways, we use it a little bit in our graphics products, and we iterate and we use it for things like showing driver audio. When the driver is speaking to his engineer or her engineer in the garage, we show that text now on screen. We do that using AI. We use AI to pick out the difference between the driver and another driver and the team engineer or the team principal and show that in a really good way.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And we wouldn't be able to do that. We're not big enough to have a team of 24 people on stenographers typing. We have to use AI to be able to do that. That's what's really helped us grow. And then the last one is, how we use it in our business. Because ultimately, as we've got the fans, we've got the sport, but we also are running a business and we have to pick up these racetracks and move them around the world, and we have all these staff who have to get places. We have insurance who has to do all that kind of stuff, and we use it heavily in that area, particularly when it comes to what has a carbon impact for us.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So things like our freight and our travel. And we are using the AI tools to tell us, a battery for instance, should we fly it? Should we send it by sea freight? Should we send it by row freight? Or should we just have lots of them? And that sort of depends. Now, a battery, if it was heavy, you'd think you probably wouldn't fly it. But actually, because of the materials in it, because of the source materials that make it, we're better off flying it. We've used AI to work through all those different machinations of things that would be too difficult to do at speed for a person.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Well, sounds like there's some fascinating things going on. I mean, of course, for a global brand, there is also the challenge of working in different markets. You mentioned moving everything around the world there. Each market with its own legal frameworks around data privacy, AI. How has technology also helped you navigate all of that, Dan?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: The other really interesting thing about AI is... I've worked in technology leadership roles for some time now. And historically, we would be going around the company, banging on everyone's doors and dragging them towards technology, making them use systems, making them move things to the cloud and things like that. What AI has done is it's turned that around on its head, and we now have people turning up, banging on our door because they want to use this tool, they want to use that tool. And we're trying to accommodate all of that and it's a great pleasure to see people that are so keen. AI is driving the tech adoption in general, which really helps the business.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Megan: Dan, as the world's first all-electric motor sport series, sustainability is obviously a real cornerstone of what Formula E is looking to do. Can you share with us how technology is helping you to achieve some of your ambitions when it comes to sustainability?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: We've been the only sport with a certified net-zero pathway, and we have to stay that part. It's a really core fundamental part of our DNA. I sit on our management team here. There is a sustainability VP that sits there as well, who checks and challenges everything we do. She looks at the data centers we use, why we use them, why we've made the decisions we've made, to make sure that we're making them all for the right reasons and the right ways. We specifically embed technology in a couple of ways. One is, we mentioned a little bit earlier, on our freight. Formula E's freight for the whole championship is probably akin to one Formula One team, but it's still by far, our biggest contributor to our impact. So we look about how we can make sure that we've refined that to get the minimum amount of air freight and sea freight, and use local wherever we can. That's also part of our pledge about investing in the communities that we race in.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The second then is about our staff travel. And we've done a really big piece of work over the last four to five years, partly accelerated through the covid-19 era actually, of doing remote working and remote TV production. Used to be traditionally, you would fly a hundred plus people out to racetracks, and then they would make the television all on site in trucks, and then they would be satellite distributed out of the venue. Now, what we do is we put in some internet connections, dual and diverse internet connections, and we stream every single camera back.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: That means on site, we only need camera operators. Some of them actually, are remotely operated anyway, but we need camera operators, and then some engineering teams to just keep everything running. And then back in our home base, which is in London, in the UK, we have our remote production center where we layer on direction, graphics, audio, replay, team radio, all of those bits that break the color and make the program and add to that significant body of people. We do that all remotely now. Really interesting actually, a bit. So that's the carbon sustainability story, but there is a further ESG piece that comes out of it and we haven't really accommodated when we went into it, is the diversity in our workforce by doing that. We were discovering that we had quite a young, equally diverse workforce until around the age of 30. And then once that happened, then we were finding we were losing women, and that's really because they didn't want to travel.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: And that's the age of people starting to have children, and things were starting to change. And then we had some men that were traveling instead, and they weren't seeing their children and it was sort of dividing it unnecessarily. But by going remote, by having so much of our people able to remotely... Or even if they do have to travel, they're not traveling every single week. They're now doing that one in three. They're able to maintain the careers and the jobs they want to do, whilst having a family lifestyle. And it also just makes a better product by having people in that environment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: That's such an interesting perspective, isn't it? It's a way of environmental sustainability intersects with social sustainability. And Rohit, and your work are so interesting. And Rohit, can you share any of the ways that Infosys has worked with Formula E, in terms of the role of technology as we say, in furthering those ambitions around sustainability?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. Infosys understands that sustainability is at the heart of Formula E, and it's a big part of why this partnership matters. Formula E is already net-zero certified, but now, they have an ambitious goal to cut carbon emissions by 45%. Infosys is helping in two ways. First, we have built AI-powered sustainability data tools that make carbon reporting accurate and traceable. Every watt of energy, every logistic decision, every material use can be tracked. Second, we use predictive analytics to model scenarios, like how changing race logistics or battery technology impact emissions so Formula E can make smarter, greener decisions. For us, it's about turning sustainability from a report into an action plan, and making Formula E a global leader in green motor sport.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: And in April 2025, Formula E working with Infosys launched its Stats Centre, which provides fans with interactive access to the performances of their drivers and teams, key milestones and narratives. I know you touched on this before, but I wonder if you could tell us a bit more about the design of that platform, Rohit, and how it fits into Formula E's wider plans to personalize that fan experience?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Sure. The Stats Centre was a big step forward. Before this, fans had access to basic statistics on the website and the mobile app, but nothing told the full story and we wanted to change that. Built on Infosys Topaz, the Stats Centre uses AI to turn race data into interactive stories. Fans can explore key stat cards that adapt to race timelines, and even chat with an AI companion to get instant answers. It's like having a person race analyst at your fingertips. And we are going further. Next year, we'll launch Race Centre. It'll have live data boards, 2D track maps showing every driver's position, overtakes and more attack timelines, and AI-generated commentary. Fans can predict podium finishes, vote for the driver of the race, and share their views on social media. Plus, we are adding video explainers for new fans, covering rules, strategies, and car technology. Our goal is simple: make every moment exciting and easy to understand. Whether you are a hardcore fan or someone watching Formula E for the first time, you'll feel connected and informed.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. Sounds brilliant. And as you've explained, Dan, leveraging data and AI can come with these huge benefits when it comes to the depth of fan experience that you can deliver, but it can also expose you to some challenges. How are you navigating those at Formula E?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: The AI generation has presented two significant challenges to us. One is that traditional SEO, traditional search engine optimization, goes out the window. Right? You are now looking at how do we design and build our systems and how do we populate them with the right content and the right data, so that the engines are picking it up correctly and displaying it? The way that the foundational models are built and the speed and the cadence of which they're updated, means quite often... We're a very fast-changing organization. We're a fast-changing product. Often, the models don't keep up. And that's because they are a point in time when they were trained. And that's something that the big organizations, the big tech organizations will fix with time. But for now, what we have to do is we have to learn about how we can present our fan-facing, web-facing products to show that correctly. That's all about having really accurate first-party content, effectively earned media. That's the piece we need to do.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then the second sort of challenge is sadly, whilst these tools are available to all of us, and we are using them effectively, so are another part of the technology landscape, and that is the cybersecurity basically they come with. If you look at the speed of the cadence and severity of hacks that are happening now, it's just growing and growing and growing, and that's because they have access to these tools too. And we're having to really up our game and professionalize. And that's really hard for an innovative organization. You don't want to shut everything down. You don't want to protect everything too much because you want people to be able to try new things. Right? If I block everything to only things that the IT team had heard of, we'd never get anything new in, and it's about getting that balance right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: Rohit, you probably have similar experiences?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: How has Infosys worked with Formula E to help it navigate some of that, Rohit?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. Infosys has helped Formula E tackle some of the challenges in three key ways, simplify complex race data into engaging fan experience through platforms like Stats Centre, building a secure and scalable cloud data backbone for the real-time insights, and enabling sustainability goals with AI-driven carbon tracking and predictive analytics. This solution makes the sport interactive, more digital, and more responsible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. I wondered if we could close with a bit of a future forward look. Can you share with us any innovations on the horizon at Formula E that you are really excited about, Dan?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: We have mentioned the Race Centre is going to launch in the next couple of months, but the really exciting thing for me is we've got an amazing season ahead of us. It's the last season of our Gen3 car, with 10 really exciting teams on the grid. We are going at speed with our tech innovation roadmap and what our fans want. And we're building up towards our Gen4 car, which will come out for season 13 in a year's time. That will get launched in 2026, and I think it will be a game changer in how people perceive electric motor sport and electric cars in general.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: It sounds like there's all sorts of exciting things going on. And Rohit too, what's coming up via this partnership that you are really looking forward to sharing with everyone?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Two things stand out for me. First is the AI-powered fan data platform that I've already spoken about. Second is the launch of Race Centre. It's going to change how fans experience live racing. And beyond final engagement, we are helping Formula E lead in sustainability with AI tools that model carbon impact and optimize logistics. This means every race can be smarter and greener. Our goal is clear: help Formula E be the most digital and sustainable motor sport in the world. The future is electric, and with AI, it's more engaging than ever.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. Thank you so much, both. That was Rohit Agnihotri, principal technologist at Infosys, and Dan Cherowbrier, CITO of Formula E, whom I spoke with from Brighton, England.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor and host for Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. And if you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review and this episode was produced by Giro Studios. Thanks for listening.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1127432/the-fast-and-the-future-focused-are-revolutionizing-motorsport/</guid><pubDate>Mon, 15 Dec 2025 15:00:00 +0000</pubDate></item><item><title>Tokenization takes the lead in the fight for data security (AI | VentureBeat)</title><link>https://venturebeat.com/ai/tokenization-takes-the-lead-in-the-fight-for-data-security</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by Capital One&lt;/i&gt; &lt;i&gt;Software&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Tokenization is emerging as a cornerstone of modern data security, helping businesses separate the value of their data from its risk. &lt;a href="https://www.youtube.com/watch?v=zGGgBrDqqHo"&gt;During this VB in Conversation&lt;/a&gt;, Ravi Raghu, president, Capital One Software, talks about the ways tokenization can help reduce the value of breached data and preserve underlying data format and usability, including Capital One‚Äôs own experience leveraging tokenization at scale. &lt;/p&gt;&lt;p&gt;Tokenization, Raghu asserts, is a far superior technology. It converts sensitive data into a nonsensitive digital replacement, called a token, that maps back to the original, which is secured in a digital vault. The token placeholder preserves both the format and the utility of the sensitive data, and can be used across applications ‚Äî including AI models. Because tokenization removes the need to manage encryption keys or dedicate compute to constant encrypting and decrypting, it offers one of the most scalable ways for companies to protect their most sensitive data, he added.&lt;/p&gt;&lt;p&gt;&amp;quot;The killer part, from a security standpoint, when you think about it relative to other methods, if a bad actor gets hold of the data, they get hold of tokens,&amp;quot; he explained. &amp;quot;The actual data is not sitting with the token, unlike other methods like encryption, where the actual data sits there, just waiting for someone to get hold of a key or use brute force to get to the real data. From every angle this is the ideal way one ought to go about protecting sensitive data.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The tokenization differentiator &lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Most organizations are just scratching the surface of data security, adding security at the very end, when data is read, to prevent an end user from accessing it. At minimum, organizations should focus on securing data on write, as it‚Äôs being stored. But best-in-class organizations go even further, protecting data at birth, the moment it‚Äôs created.&lt;/p&gt;&lt;p&gt;At one end of the safety spectrum is a simple lock-and-key approach that restricts access but leaves the underlying data intact. More advanced methods, like masking or modifying data, permanently alter its meaning ‚Äî which can compromise its usefulness. File-level encryption provides broader protection for large volumes of stored data, but when you get down to field-level encryption (for example, a Social Security number), it becomes a bigger challenge. It takes a great deal of compute to encrypt a single field, and then to decrypt it at the point of usage. And still it has a fatal flaw: the original data is still right there, only needing the key to get access. &lt;/p&gt;&lt;p&gt;Tokenization avoids these pitfalls by replacing the original data with a surrogate that has no intrinsic value. If the token is intercepted ‚Äî whether by the wrong person or the wrong machine ‚Äî the data itself remains secure.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The business value of tokenization&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&amp;quot;Fundamentally you‚Äôre protecting data, and that‚Äôs priceless,&amp;quot; Raghu said. &amp;quot;Another thing that‚Äôs priceless ‚Äì can you use that for modeling purposes subsequently? On the one hand, it‚Äôs a protection thing, and on the other hand it‚Äôs a business enabling thing.&amp;quot; &lt;/p&gt;&lt;p&gt;Because tokenization preserves the structure and ordinality of the original data, it can still be used for modeling and analytics, turning protection into a business enabler. Take private health data governed by HIPAA for example: tokenization means that data canbeused to build pricing models or for gene therapy research, while remaining compliant. &lt;/p&gt;&lt;p&gt;&amp;quot;If your data is already protected, you can then proliferate the usage of data across the entire enterprise and have everybody creating more and more value out of the data,&amp;quot; Raghu said. &amp;quot;Conversely, if you don‚Äôt have that, there‚Äôs a lot of reticence for enterprises today to have more people access it, or have more and more AI agents access their data. Ironically, they‚Äôre limiting the blast radius of innovation. The tokenization impact is massive, and there are many metrics you could use to measure that ‚Äì operational impact, revenue impact, and obviously the peace of mind from a security standpoint.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Breaking down adoption barriers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Until now, the fundamental challenge with traditional tokenization has been performance. AI requires a scale and speed that is unprecedented. That&amp;#x27;s one of the major challenges Capital One addresses with Databolt, its vaultless tokenization solution, which can produce up to 4 million tokens per second.&lt;/p&gt;&lt;p&gt;&amp;quot;Capital One has gone through tokenization for more than a decade. We started doing it because we‚Äôre serving our 100 million banking customers. We want to protect that sensitive data,&amp;quot; Raghu said. &amp;quot;We‚Äôve eaten our own dog food with our internal tokenization capability, over 100 billion times a month. We‚Äôve taken that know-how and that capability, scale, and speed, and innovated so that the world can leverage it, so that it‚Äôs a commercial offering.&amp;quot;&lt;/p&gt;&lt;p&gt;Vaultless tokenization is an advanced form of tokenization that does not require a central database (vault) to store token mappings. Instead, it uses mathematical algorithms, cryptographic techniques, and deterministic mapping to generate tokens dynamically.This approach is faster, more scalable, and eliminates the security risk associated with managing a vault.&lt;/p&gt;&lt;p&gt;&amp;quot;We realized that for the scale and speed demands that we had, we needed to build out that capability ourselves,&amp;quot; Raghu said. &amp;quot;We‚Äôve been iterating continuously on making sure that it can scale up to hundreds of billions of operations a month. All of our innovation has been around building IP and capability to do that thing at a battle-tested scale within our enterprise, for the purpose of serving our customers.&amp;quot;&lt;/p&gt;&lt;p&gt;While conventional tokenization methods can involve some complexity and slow down operations, Databolt seamlessly integrates with encrypted data warehouses, allowing businesses to maintain robust security without slowing performance or operations. Tokenization occurs in the customer‚Äôs environment, removing the need to communicate with an external network to perform tokenization operations, which can also slow performance.&lt;/p&gt;&lt;p&gt;&amp;quot;We believe that fundamentally, tokenization should be easy to adopt,&amp;quot; Raghu said. &amp;quot;You should be able to secure your data very quickly and operate at the speed and scale and cost needs that organizations have. I think that‚Äôs been a critical barrier so far for the mass scale adoption of tokenization. In an AI world, that‚Äôs going to become a huge enabler.&amp;quot;&lt;/p&gt;&lt;p&gt;Don&amp;#x27;t miss &lt;a href="https://www.youtube.com/watch?v=zGGgBrDqqHo"&gt;the whole conversation with Ravi Raghu, president, Capital One Software, here&lt;/a&gt;.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they‚Äôre always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by Capital One&lt;/i&gt; &lt;i&gt;Software&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Tokenization is emerging as a cornerstone of modern data security, helping businesses separate the value of their data from its risk. &lt;a href="https://www.youtube.com/watch?v=zGGgBrDqqHo"&gt;During this VB in Conversation&lt;/a&gt;, Ravi Raghu, president, Capital One Software, talks about the ways tokenization can help reduce the value of breached data and preserve underlying data format and usability, including Capital One‚Äôs own experience leveraging tokenization at scale. &lt;/p&gt;&lt;p&gt;Tokenization, Raghu asserts, is a far superior technology. It converts sensitive data into a nonsensitive digital replacement, called a token, that maps back to the original, which is secured in a digital vault. The token placeholder preserves both the format and the utility of the sensitive data, and can be used across applications ‚Äî including AI models. Because tokenization removes the need to manage encryption keys or dedicate compute to constant encrypting and decrypting, it offers one of the most scalable ways for companies to protect their most sensitive data, he added.&lt;/p&gt;&lt;p&gt;&amp;quot;The killer part, from a security standpoint, when you think about it relative to other methods, if a bad actor gets hold of the data, they get hold of tokens,&amp;quot; he explained. &amp;quot;The actual data is not sitting with the token, unlike other methods like encryption, where the actual data sits there, just waiting for someone to get hold of a key or use brute force to get to the real data. From every angle this is the ideal way one ought to go about protecting sensitive data.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The tokenization differentiator &lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Most organizations are just scratching the surface of data security, adding security at the very end, when data is read, to prevent an end user from accessing it. At minimum, organizations should focus on securing data on write, as it‚Äôs being stored. But best-in-class organizations go even further, protecting data at birth, the moment it‚Äôs created.&lt;/p&gt;&lt;p&gt;At one end of the safety spectrum is a simple lock-and-key approach that restricts access but leaves the underlying data intact. More advanced methods, like masking or modifying data, permanently alter its meaning ‚Äî which can compromise its usefulness. File-level encryption provides broader protection for large volumes of stored data, but when you get down to field-level encryption (for example, a Social Security number), it becomes a bigger challenge. It takes a great deal of compute to encrypt a single field, and then to decrypt it at the point of usage. And still it has a fatal flaw: the original data is still right there, only needing the key to get access. &lt;/p&gt;&lt;p&gt;Tokenization avoids these pitfalls by replacing the original data with a surrogate that has no intrinsic value. If the token is intercepted ‚Äî whether by the wrong person or the wrong machine ‚Äî the data itself remains secure.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The business value of tokenization&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&amp;quot;Fundamentally you‚Äôre protecting data, and that‚Äôs priceless,&amp;quot; Raghu said. &amp;quot;Another thing that‚Äôs priceless ‚Äì can you use that for modeling purposes subsequently? On the one hand, it‚Äôs a protection thing, and on the other hand it‚Äôs a business enabling thing.&amp;quot; &lt;/p&gt;&lt;p&gt;Because tokenization preserves the structure and ordinality of the original data, it can still be used for modeling and analytics, turning protection into a business enabler. Take private health data governed by HIPAA for example: tokenization means that data canbeused to build pricing models or for gene therapy research, while remaining compliant. &lt;/p&gt;&lt;p&gt;&amp;quot;If your data is already protected, you can then proliferate the usage of data across the entire enterprise and have everybody creating more and more value out of the data,&amp;quot; Raghu said. &amp;quot;Conversely, if you don‚Äôt have that, there‚Äôs a lot of reticence for enterprises today to have more people access it, or have more and more AI agents access their data. Ironically, they‚Äôre limiting the blast radius of innovation. The tokenization impact is massive, and there are many metrics you could use to measure that ‚Äì operational impact, revenue impact, and obviously the peace of mind from a security standpoint.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Breaking down adoption barriers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Until now, the fundamental challenge with traditional tokenization has been performance. AI requires a scale and speed that is unprecedented. That&amp;#x27;s one of the major challenges Capital One addresses with Databolt, its vaultless tokenization solution, which can produce up to 4 million tokens per second.&lt;/p&gt;&lt;p&gt;&amp;quot;Capital One has gone through tokenization for more than a decade. We started doing it because we‚Äôre serving our 100 million banking customers. We want to protect that sensitive data,&amp;quot; Raghu said. &amp;quot;We‚Äôve eaten our own dog food with our internal tokenization capability, over 100 billion times a month. We‚Äôve taken that know-how and that capability, scale, and speed, and innovated so that the world can leverage it, so that it‚Äôs a commercial offering.&amp;quot;&lt;/p&gt;&lt;p&gt;Vaultless tokenization is an advanced form of tokenization that does not require a central database (vault) to store token mappings. Instead, it uses mathematical algorithms, cryptographic techniques, and deterministic mapping to generate tokens dynamically.This approach is faster, more scalable, and eliminates the security risk associated with managing a vault.&lt;/p&gt;&lt;p&gt;&amp;quot;We realized that for the scale and speed demands that we had, we needed to build out that capability ourselves,&amp;quot; Raghu said. &amp;quot;We‚Äôve been iterating continuously on making sure that it can scale up to hundreds of billions of operations a month. All of our innovation has been around building IP and capability to do that thing at a battle-tested scale within our enterprise, for the purpose of serving our customers.&amp;quot;&lt;/p&gt;&lt;p&gt;While conventional tokenization methods can involve some complexity and slow down operations, Databolt seamlessly integrates with encrypted data warehouses, allowing businesses to maintain robust security without slowing performance or operations. Tokenization occurs in the customer‚Äôs environment, removing the need to communicate with an external network to perform tokenization operations, which can also slow performance.&lt;/p&gt;&lt;p&gt;&amp;quot;We believe that fundamentally, tokenization should be easy to adopt,&amp;quot; Raghu said. &amp;quot;You should be able to secure your data very quickly and operate at the speed and scale and cost needs that organizations have. I think that‚Äôs been a critical barrier so far for the mass scale adoption of tokenization. In an AI world, that‚Äôs going to become a huge enabler.&amp;quot;&lt;/p&gt;&lt;p&gt;Don&amp;#x27;t miss &lt;a href="https://www.youtube.com/watch?v=zGGgBrDqqHo"&gt;the whole conversation with Ravi Raghu, president, Capital One Software, here&lt;/a&gt;.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they‚Äôre always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/tokenization-takes-the-lead-in-the-fight-for-data-security</guid><pubDate>Mon, 15 Dec 2025 15:00:00 +0000</pubDate></item><item><title>CUGA on Hugging Face: Democratizing Configurable AI Agents (Hugging Face - Blog)</title><link>https://huggingface.co/blog/ibm-research/cuga-on-hugging-face</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Avi Yaeli's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/70501cdba867c50fe67bdf0f9675af04.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Introduction&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
AI agents are rapidly becoming essential for building intelligent applications, but creating robust, adaptable agents that scale across domains remains a challenge. Many existing frameworks struggle with brittleness, tool misuse, and failures when faced with complex workflows.
&lt;p&gt;&lt;strong&gt;CUGA (Configurable Generalist Agent)&lt;/strong&gt; was designed to overcome these limitations. It's an &lt;strong&gt;open-source, AI Agent&lt;/strong&gt; that combines flexibility, reliability, and ease of use for enterprise use cases. By abstracting orchestration complexity, CUGA empowers developers to focus on domain requirements rather than the internals of agent building. And now, with its integration into üöÄHugging Face SpacesüöÄ, experimenting with CUGA and open models has never been easier.&lt;/p&gt;

	
		
	
	&lt;span&gt;
		&lt;strong&gt;What is CUGA?&lt;/strong&gt;
	&lt;/span&gt;

&lt;p&gt;CUGA is a &lt;strong&gt;configurable, general-purpose AI agent&lt;/strong&gt; that supports complex, multi-step tasks across web and API environments. It has achieved state-of-the-art performance on leading benchmarks:&lt;/p&gt;
&lt;p&gt;ü•á #1 on AppWorld - a benchmark with 750 real-world tasks across 457 APIs&lt;/p&gt;
&lt;p&gt;ü•à Top-tier on WebArena (#1 from 02/25 - 09/25) - showcases CUGA Computer Use capabilities with a complex benchmark for autonomous web agents across application domains&lt;/p&gt;
&lt;p&gt;At its core, CUGA offers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High-performing generalist agent:&lt;/strong&gt; Benchmarked on complex web and API tasks, it combines best-of-breed agentic patterns (e.g. planner-executor, code-act) with structured planning and smart variable management to prevent hallucination and handle complexity&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configurable reasoning modes:&lt;/strong&gt; Balance performance and cost/latency with flexible modes ranging from fast heuristics to deep planning, optimizing for your task requirements&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer use&lt;/strong&gt;: Effortlessly combine UI interactions with API invocations in a workflow&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-tool integration&lt;/strong&gt;: Seamlessly integrate tools via OpenAPI specs, MCP servers, and LangChain, enabling rapid connection to REST APIs, custom protocols, and Python functions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrates with Langflow&lt;/strong&gt;: A low-code visual build experience for designing and deploying agent workflows without extensive coding&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Composable:&lt;/strong&gt; CUGA can be exposed as a tool to other agents, enabling nested reasoning and multi-agent collaboration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;We're also continuing to innovate with new experimental capabilities, including:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Configurable policy and human-in-the-loop instructions:&lt;/strong&gt; Improve alignment and ensure safe agent behavior in enterprise contexts&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Save-and-reuse capabilities:&lt;/strong&gt; Capture and reuse successful execution paths (plans, code, and trajectories) for faster and consistent behavior across repeated tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
  &lt;img alt="CUGA Agentic Architecture" src="https://cdn-uploads.huggingface.co/production/uploads/6487da02c4b44322c124f39f/_26Cf6pVF7-JapcMreG0n.png" width="800" /&gt;
  &lt;br /&gt;&lt;strong&gt;Figure 1: CUGA Agentic Architecture&lt;/strong&gt;
&lt;/p&gt;


&lt;p&gt;The CUGA architecture begins with the user's message flowing into a chat layer that interprets intent and constructs the user's goal, based on context. A task planning and control component then decomposes this goal into structured subtasks, tracked programmatically through a dynamic task ledger. This ledger supports re-planning when needed, ensuring robust execution. Subtasks are delegated to specialized agents, such as the API agent, which uses an inner reasoning loop to generate pseudo-code instructions before invoking code in a secure sandbox. The system leverages a tool registry that goes beyond MCP protocols to parse and understand tool capabilities, enabling precise orchestration. Once all steps are completed, the final response is returned to the user, delivering reliable, policy-aligned outcomes.&lt;/p&gt;
&lt;p&gt;CUGA works best when inference is fast. When each call takes seconds, delays compound and force a tradeoff between agent capability and user experience. Running on high-performance inference platforms like Groq shows how fast inference fundamentally expands what agent architectures can achieve.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Open Source and Open Models&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;CUGA is fully &lt;strong&gt;open source, under the Apache 2.0 license,&lt;/strong&gt; and you can find us at cuga.dev.&lt;/p&gt;
&lt;p&gt;By embracing &lt;strong&gt;open models&lt;/strong&gt;, CUGA aligns with the Hugging Face ethos of democratizing AI-giving developers the freedom to choose models that best fit their needs, whether for experimentation or production.&lt;/p&gt;
&lt;p&gt;CUGA has been tested with a variety of open models, including gpt-oss-120b and Llama-4-Maverick-17B-128E-Instruct-fp8 (both hosted on Groq). Our Hugging Face Space uses gpt-oss-120b, with the model hosted on Groq, offering a rapid response time for LLM calls&lt;/p&gt;
&lt;p&gt;Groq runs open models on its custom‚Äëbuilt LPUs, which are designed for AI inference and optimal for repeated agent inferences required by CUGA's architecture, enabling planning, execution, and validation steps to finish fast. The result is strong cost and performance: open models are ~80-90% cheaper than closed alternatives; Groq's OpenAI-compatible APIs meet production latency needs, and CUGA stays fully configurable across models, providers, and deployment topologies.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Integration with Langflow: Visual Agent Design Made Simple&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To make agent development even more accessible, CUGA integrates with &lt;strong&gt;Langflow&lt;/strong&gt;, an open-source visual programming interface for building LLM-powered workflows. Its intuitive drag-and-drop interface reduces the barrier to entry for those who prefer low-code solutions.&lt;/p&gt;
&lt;p&gt;Starting with &lt;strong&gt;Langflow 1.7.0&lt;/strong&gt;, CUGA ships with its own widget, enabling users to assemble complex, multi-tool agents visually and deploy with a click. Give it a try at &lt;strong&gt;langflow.org&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Try the Hugging Face Demo: A Hands-On Preview&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We've launched a &lt;strong&gt;CUGA demo on Hugging Face Spaces&lt;/strong&gt; to give you a taste of what's possible. This demo showcases a &lt;strong&gt;small CRM system&lt;/strong&gt; and equips CUGA with &lt;strong&gt;20 preconfigured tools&lt;/strong&gt; for handling sales related data queries and API interactions through the API Agent. To make experimentation even more powerful, the demo provides &lt;strong&gt;access to workspace files&lt;/strong&gt;, enabling you to &lt;strong&gt;use predefined policies.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Give it a try on&lt;/strong&gt; &lt;strong&gt;Hugging Face Spaces&lt;/strong&gt; and share your feedback!&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Conclusion and Call to Action&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;CUGA brings a new level of flexibility and openness to AI agent building. To engage with us:&lt;/p&gt;

&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Avi Yaeli's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/70501cdba867c50fe67bdf0f9675af04.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Introduction&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
AI agents are rapidly becoming essential for building intelligent applications, but creating robust, adaptable agents that scale across domains remains a challenge. Many existing frameworks struggle with brittleness, tool misuse, and failures when faced with complex workflows.
&lt;p&gt;&lt;strong&gt;CUGA (Configurable Generalist Agent)&lt;/strong&gt; was designed to overcome these limitations. It's an &lt;strong&gt;open-source, AI Agent&lt;/strong&gt; that combines flexibility, reliability, and ease of use for enterprise use cases. By abstracting orchestration complexity, CUGA empowers developers to focus on domain requirements rather than the internals of agent building. And now, with its integration into üöÄHugging Face SpacesüöÄ, experimenting with CUGA and open models has never been easier.&lt;/p&gt;

	
		
	
	&lt;span&gt;
		&lt;strong&gt;What is CUGA?&lt;/strong&gt;
	&lt;/span&gt;

&lt;p&gt;CUGA is a &lt;strong&gt;configurable, general-purpose AI agent&lt;/strong&gt; that supports complex, multi-step tasks across web and API environments. It has achieved state-of-the-art performance on leading benchmarks:&lt;/p&gt;
&lt;p&gt;ü•á #1 on AppWorld - a benchmark with 750 real-world tasks across 457 APIs&lt;/p&gt;
&lt;p&gt;ü•à Top-tier on WebArena (#1 from 02/25 - 09/25) - showcases CUGA Computer Use capabilities with a complex benchmark for autonomous web agents across application domains&lt;/p&gt;
&lt;p&gt;At its core, CUGA offers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High-performing generalist agent:&lt;/strong&gt; Benchmarked on complex web and API tasks, it combines best-of-breed agentic patterns (e.g. planner-executor, code-act) with structured planning and smart variable management to prevent hallucination and handle complexity&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configurable reasoning modes:&lt;/strong&gt; Balance performance and cost/latency with flexible modes ranging from fast heuristics to deep planning, optimizing for your task requirements&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer use&lt;/strong&gt;: Effortlessly combine UI interactions with API invocations in a workflow&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-tool integration&lt;/strong&gt;: Seamlessly integrate tools via OpenAPI specs, MCP servers, and LangChain, enabling rapid connection to REST APIs, custom protocols, and Python functions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrates with Langflow&lt;/strong&gt;: A low-code visual build experience for designing and deploying agent workflows without extensive coding&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Composable:&lt;/strong&gt; CUGA can be exposed as a tool to other agents, enabling nested reasoning and multi-agent collaboration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;We're also continuing to innovate with new experimental capabilities, including:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Configurable policy and human-in-the-loop instructions:&lt;/strong&gt; Improve alignment and ensure safe agent behavior in enterprise contexts&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Save-and-reuse capabilities:&lt;/strong&gt; Capture and reuse successful execution paths (plans, code, and trajectories) for faster and consistent behavior across repeated tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
  &lt;img alt="CUGA Agentic Architecture" src="https://cdn-uploads.huggingface.co/production/uploads/6487da02c4b44322c124f39f/_26Cf6pVF7-JapcMreG0n.png" width="800" /&gt;
  &lt;br /&gt;&lt;strong&gt;Figure 1: CUGA Agentic Architecture&lt;/strong&gt;
&lt;/p&gt;


&lt;p&gt;The CUGA architecture begins with the user's message flowing into a chat layer that interprets intent and constructs the user's goal, based on context. A task planning and control component then decomposes this goal into structured subtasks, tracked programmatically through a dynamic task ledger. This ledger supports re-planning when needed, ensuring robust execution. Subtasks are delegated to specialized agents, such as the API agent, which uses an inner reasoning loop to generate pseudo-code instructions before invoking code in a secure sandbox. The system leverages a tool registry that goes beyond MCP protocols to parse and understand tool capabilities, enabling precise orchestration. Once all steps are completed, the final response is returned to the user, delivering reliable, policy-aligned outcomes.&lt;/p&gt;
&lt;p&gt;CUGA works best when inference is fast. When each call takes seconds, delays compound and force a tradeoff between agent capability and user experience. Running on high-performance inference platforms like Groq shows how fast inference fundamentally expands what agent architectures can achieve.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Open Source and Open Models&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;CUGA is fully &lt;strong&gt;open source, under the Apache 2.0 license,&lt;/strong&gt; and you can find us at cuga.dev.&lt;/p&gt;
&lt;p&gt;By embracing &lt;strong&gt;open models&lt;/strong&gt;, CUGA aligns with the Hugging Face ethos of democratizing AI-giving developers the freedom to choose models that best fit their needs, whether for experimentation or production.&lt;/p&gt;
&lt;p&gt;CUGA has been tested with a variety of open models, including gpt-oss-120b and Llama-4-Maverick-17B-128E-Instruct-fp8 (both hosted on Groq). Our Hugging Face Space uses gpt-oss-120b, with the model hosted on Groq, offering a rapid response time for LLM calls&lt;/p&gt;
&lt;p&gt;Groq runs open models on its custom‚Äëbuilt LPUs, which are designed for AI inference and optimal for repeated agent inferences required by CUGA's architecture, enabling planning, execution, and validation steps to finish fast. The result is strong cost and performance: open models are ~80-90% cheaper than closed alternatives; Groq's OpenAI-compatible APIs meet production latency needs, and CUGA stays fully configurable across models, providers, and deployment topologies.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Integration with Langflow: Visual Agent Design Made Simple&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To make agent development even more accessible, CUGA integrates with &lt;strong&gt;Langflow&lt;/strong&gt;, an open-source visual programming interface for building LLM-powered workflows. Its intuitive drag-and-drop interface reduces the barrier to entry for those who prefer low-code solutions.&lt;/p&gt;
&lt;p&gt;Starting with &lt;strong&gt;Langflow 1.7.0&lt;/strong&gt;, CUGA ships with its own widget, enabling users to assemble complex, multi-tool agents visually and deploy with a click. Give it a try at &lt;strong&gt;langflow.org&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Try the Hugging Face Demo: A Hands-On Preview&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We've launched a &lt;strong&gt;CUGA demo on Hugging Face Spaces&lt;/strong&gt; to give you a taste of what's possible. This demo showcases a &lt;strong&gt;small CRM system&lt;/strong&gt; and equips CUGA with &lt;strong&gt;20 preconfigured tools&lt;/strong&gt; for handling sales related data queries and API interactions through the API Agent. To make experimentation even more powerful, the demo provides &lt;strong&gt;access to workspace files&lt;/strong&gt;, enabling you to &lt;strong&gt;use predefined policies.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Give it a try on&lt;/strong&gt; &lt;strong&gt;Hugging Face Spaces&lt;/strong&gt; and share your feedback!&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Conclusion and Call to Action&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;CUGA brings a new level of flexibility and openness to AI agent building. To engage with us:&lt;/p&gt;

&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/ibm-research/cuga-on-hugging-face</guid><pubDate>Mon, 15 Dec 2025 16:01:04 +0000</pubDate></item><item><title>NVIDIA Acquires Open-Source Workload Management Provider SchedMD (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/nvidia-acquires-schedmd/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA today announced it has acquired SchedMD ‚Äî the leading developer of Slurm, an open-source workload management system for high-performance computing (HPC) and AI ‚Äî to help strengthen the open-source software ecosystem and drive AI innovation for researchers, developers and enterprises.&lt;/p&gt;
&lt;p&gt;NVIDIA will continue to develop and distribute Slurm as open-source, vendor-neutral software, making it widely available to and supported by the broader HPC and AI community across diverse hardware and software environments.&lt;/p&gt;
&lt;p&gt;HPC and AI workloads involve complex computations running parallel tasks on clusters that require queuing, scheduling and allocating computational resources. As HPC and AI clusters get larger and more powerful, efficient resource utilization is critical.&lt;/p&gt;
&lt;p&gt;As the leading workload manager and job scheduler in scalability, throughput and complex policy management, Slurm is used in more than half of the top 10 and top 100 systems in the TOP500 list of supercomputers.&lt;/p&gt;
&lt;p&gt;Slurm, which is supported on the latest NVIDIA hardware, is also part of the critical infrastructure needed for generative AI, used by foundation model developers and AI builders to manage model training and inference needs.&lt;/p&gt;
&lt;p&gt;‚ÄúWe‚Äôre thrilled to join forces with NVIDIA, as this acquisition is the ultimate validation of Slurm‚Äôs critical role in the world‚Äôs most demanding HPC and AI environments,‚Äù said Danny Auble, CEO of SchedMD. ‚ÄúNVIDIA‚Äôs deep expertise and investment in accelerated computing will enhance the development of Slurm ‚Äî which will continue to be open source ‚Äî to meet the demands of the next generation of AI and supercomputing.‚Äù&lt;/p&gt;
&lt;p&gt;NVIDIA has been collaborating with SchedMD for over a decade and will continue investing in Slurm‚Äôs development to ensure it remains the leading open-source scheduler for HPC and AI.&lt;/p&gt;
&lt;p&gt;NVIDIA will accelerate SchedMD‚Äôs access to new systems ‚Äî allowing users of NVIDIA‚Äôs accelerated computing platform to optimize workloads across their entire compute infrastructure ‚Äî while also supporting a diverse hardware and software ecosystem, so customers can run heterogeneous clusters with the latest Slurm innovations.&lt;/p&gt;
&lt;p&gt;NVIDIA will continue to offer open-source software support, training and development for Slurm to SchedMD‚Äôs hundreds of customers, which include cloud providers, manufacturers, AI companies and research labs spanning industries such as autonomous driving, healthcare and life sciences, energy, financial services, manufacturing and government.&lt;/p&gt;
&lt;p&gt;Together with SchedMD, NVIDIA is bolstering the open-source software ecosystem to catalyze HPC and AI innovation across industries, at every scale.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA today announced it has acquired SchedMD ‚Äî the leading developer of Slurm, an open-source workload management system for high-performance computing (HPC) and AI ‚Äî to help strengthen the open-source software ecosystem and drive AI innovation for researchers, developers and enterprises.&lt;/p&gt;
&lt;p&gt;NVIDIA will continue to develop and distribute Slurm as open-source, vendor-neutral software, making it widely available to and supported by the broader HPC and AI community across diverse hardware and software environments.&lt;/p&gt;
&lt;p&gt;HPC and AI workloads involve complex computations running parallel tasks on clusters that require queuing, scheduling and allocating computational resources. As HPC and AI clusters get larger and more powerful, efficient resource utilization is critical.&lt;/p&gt;
&lt;p&gt;As the leading workload manager and job scheduler in scalability, throughput and complex policy management, Slurm is used in more than half of the top 10 and top 100 systems in the TOP500 list of supercomputers.&lt;/p&gt;
&lt;p&gt;Slurm, which is supported on the latest NVIDIA hardware, is also part of the critical infrastructure needed for generative AI, used by foundation model developers and AI builders to manage model training and inference needs.&lt;/p&gt;
&lt;p&gt;‚ÄúWe‚Äôre thrilled to join forces with NVIDIA, as this acquisition is the ultimate validation of Slurm‚Äôs critical role in the world‚Äôs most demanding HPC and AI environments,‚Äù said Danny Auble, CEO of SchedMD. ‚ÄúNVIDIA‚Äôs deep expertise and investment in accelerated computing will enhance the development of Slurm ‚Äî which will continue to be open source ‚Äî to meet the demands of the next generation of AI and supercomputing.‚Äù&lt;/p&gt;
&lt;p&gt;NVIDIA has been collaborating with SchedMD for over a decade and will continue investing in Slurm‚Äôs development to ensure it remains the leading open-source scheduler for HPC and AI.&lt;/p&gt;
&lt;p&gt;NVIDIA will accelerate SchedMD‚Äôs access to new systems ‚Äî allowing users of NVIDIA‚Äôs accelerated computing platform to optimize workloads across their entire compute infrastructure ‚Äî while also supporting a diverse hardware and software ecosystem, so customers can run heterogeneous clusters with the latest Slurm innovations.&lt;/p&gt;
&lt;p&gt;NVIDIA will continue to offer open-source software support, training and development for Slurm to SchedMD‚Äôs hundreds of customers, which include cloud providers, manufacturers, AI companies and research labs spanning industries such as autonomous driving, healthcare and life sciences, energy, financial services, manufacturing and government.&lt;/p&gt;
&lt;p&gt;Together with SchedMD, NVIDIA is bolstering the open-source software ecosystem to catalyze HPC and AI innovation across industries, at every scale.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/nvidia-acquires-schedmd/</guid><pubDate>Mon, 15 Dec 2025 16:30:11 +0000</pubDate></item><item><title>Gemini provides automated feedback for theoretical computer scientists at STOC 2026 (The latest research from Google)</title><link>https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;&lt;i&gt;Vincent Cohen-Addad, Rajesh Jayaram, Jon Schneider, and David Woodruff co-led this project&lt;/i&gt;[8746db]&lt;i&gt;, with key contributions by Lalit Jain, Jieming Mao, and Vahab Mirrokni. We also thank the STOC 2026 PC chair Artur Czumaj and the many other authors who participated in this experiment and provided their valuable feedback, helpful suggestions, and discussions, including Mohammad Taghi Hajiaghayi, Ravi Kumar, Yossi Matias, and Sergei Vassilvitskii. Finally, this work builds on the efforts of the Deep Think team: Garrett Bingham, Irene Cai, Heng-Tze Cheng, Yong Cheng, Kristen Chiafullo, Vincent Cohen-Addad, Paul Covington, Golnaz Ghiasi, Chenjie Gu, Huan Gui, Ana Hosseini, Dawsen Hwang, Lalit Jain, Vihan Jain, Ragha Kotikalapudi, Chenkai Kuang, Chenkai Kuang, Maciej Kula, Nate Kushman, Jane Labanowski, Quoc Le, Jonathan Lee, Zhaoqi Leng, Steve Li, YaGuang Li, Hanzhao (Maggie) Lin, Evan Liu, Yuan Liu, Thang Luong, Jieming Mao, Vahab Mirrokni, Pol Moreno, Nigamaa Nayakanti, Aroonalok Pyne, Shubha Raghvendra, Sashank Reddi, Nikunj Saunshi, Siamak Shakeri, Archit Sharma, Xinying Song, Qijun Tan, Yi Tay, Trieu Trinh, Theophane Weber, Winnie Xu, Zicheng Xu, Shunyu Yao, Lijun Yu, Hao Zhou, Honglei Zhuang, and Song Zuo.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;&lt;i&gt;Vincent Cohen-Addad, Rajesh Jayaram, Jon Schneider, and David Woodruff co-led this project&lt;/i&gt;[8746db]&lt;i&gt;, with key contributions by Lalit Jain, Jieming Mao, and Vahab Mirrokni. We also thank the STOC 2026 PC chair Artur Czumaj and the many other authors who participated in this experiment and provided their valuable feedback, helpful suggestions, and discussions, including Mohammad Taghi Hajiaghayi, Ravi Kumar, Yossi Matias, and Sergei Vassilvitskii. Finally, this work builds on the efforts of the Deep Think team: Garrett Bingham, Irene Cai, Heng-Tze Cheng, Yong Cheng, Kristen Chiafullo, Vincent Cohen-Addad, Paul Covington, Golnaz Ghiasi, Chenjie Gu, Huan Gui, Ana Hosseini, Dawsen Hwang, Lalit Jain, Vihan Jain, Ragha Kotikalapudi, Chenkai Kuang, Chenkai Kuang, Maciej Kula, Nate Kushman, Jane Labanowski, Quoc Le, Jonathan Lee, Zhaoqi Leng, Steve Li, YaGuang Li, Hanzhao (Maggie) Lin, Evan Liu, Yuan Liu, Thang Luong, Jieming Mao, Vahab Mirrokni, Pol Moreno, Nigamaa Nayakanti, Aroonalok Pyne, Shubha Raghvendra, Sashank Reddi, Nikunj Saunshi, Siamak Shakeri, Archit Sharma, Xinying Song, Qijun Tan, Yi Tay, Trieu Trinh, Theophane Weber, Winnie Xu, Zicheng Xu, Shunyu Yao, Lijun Yu, Hao Zhou, Honglei Zhuang, and Song Zuo.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/</guid><pubDate>Mon, 15 Dec 2025 17:37:00 +0000</pubDate></item><item><title>[NEW] Merriam-Webster names ‚Äòslop‚Äô the word of the year (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/merriam-webster-names-slop-the-word-of-the-year/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-119707787-e1729681720847.jpg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI‚Äôs impact on our social media feeds has not gone unnoticed by one of America‚Äôs top dictionaries. Amidst the onslaught of content that has swept the web over the past 12 months, Merriam-Webster announced Sunday that its word of the year for 2025 is ‚Äúslop.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The dictionary defines the term as ‚Äúdigital content of low quality that is produced usually in quantity by means of artificial intelligence.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúLike &lt;em&gt;slime&lt;/em&gt;, &lt;em&gt;sludge&lt;/em&gt;, and &lt;em&gt;muck&lt;/em&gt;, &lt;em&gt;slop&lt;/em&gt; has the wet sound of something you don‚Äôt want to touch. Slop oozes into everything,‚Äù the dictionary writes, adding that, in an age of AI anxiety, it is a term designed to communicate ‚Äúa tone that‚Äôs less fearful, more mocking‚Äù of the technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt‚Äôs such an illustrative word,‚Äù Merriam-Webster‚Äôs president, Greg Barlow, told The Associated Press. ‚ÄúIt‚Äôs part of a transformative technology, AI, and it‚Äôs something that people have found fascinating, annoying, and a little bit ridiculous.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The word ‚Äúslop‚Äù has certainly been everywhere this year, as journalists and commentators have sought to describe the ways in which platforms like OpenAI‚Äôs Sora and Google Gemini‚Äôs Veo are transforming the internet. Thanks to this new breed of media generator, there are now AI-generated books, podcasts, pop songs, TV commercials ‚Äî even entire movies. One study in May claimed that nearly 75% of all new web content from the previous month had involved some kind of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These new tools have even led to what has been dubbed a ‚Äúslop economy,‚Äù in which gluts of AI-generated content can be milked for advertising money. Critics worry that this trend is further polarizing digital communities, dividing them into those who can afford paywalled, higher-quality content, and those who can only afford a digital diet of slop, which ‚Äî as you might imagine ‚Äî can be quite light on informational value.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But ‚Äúslop‚Äù has also been used to describe AI‚Äôs impact on a large variety of fields that don‚Äôt have much to do with traditional media consumption, including cybersecurity reports, legal briefings, and the college essay, among other things. Its impact is broad, to say the least.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Relatedly, tech words have been big winners in the WOTY (word of the year) category this year. Macquarie Dictionary already beat out Merriam-Webster to make ‚ÄúAI slop‚Äù its annual term, while Oxford Dictionary chose ‚Äúragebait.‚Äù Collins Dictionary went with ‚Äúvibe coding.‚Äù&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-119707787-e1729681720847.jpg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI‚Äôs impact on our social media feeds has not gone unnoticed by one of America‚Äôs top dictionaries. Amidst the onslaught of content that has swept the web over the past 12 months, Merriam-Webster announced Sunday that its word of the year for 2025 is ‚Äúslop.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The dictionary defines the term as ‚Äúdigital content of low quality that is produced usually in quantity by means of artificial intelligence.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúLike &lt;em&gt;slime&lt;/em&gt;, &lt;em&gt;sludge&lt;/em&gt;, and &lt;em&gt;muck&lt;/em&gt;, &lt;em&gt;slop&lt;/em&gt; has the wet sound of something you don‚Äôt want to touch. Slop oozes into everything,‚Äù the dictionary writes, adding that, in an age of AI anxiety, it is a term designed to communicate ‚Äúa tone that‚Äôs less fearful, more mocking‚Äù of the technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt‚Äôs such an illustrative word,‚Äù Merriam-Webster‚Äôs president, Greg Barlow, told The Associated Press. ‚ÄúIt‚Äôs part of a transformative technology, AI, and it‚Äôs something that people have found fascinating, annoying, and a little bit ridiculous.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The word ‚Äúslop‚Äù has certainly been everywhere this year, as journalists and commentators have sought to describe the ways in which platforms like OpenAI‚Äôs Sora and Google Gemini‚Äôs Veo are transforming the internet. Thanks to this new breed of media generator, there are now AI-generated books, podcasts, pop songs, TV commercials ‚Äî even entire movies. One study in May claimed that nearly 75% of all new web content from the previous month had involved some kind of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These new tools have even led to what has been dubbed a ‚Äúslop economy,‚Äù in which gluts of AI-generated content can be milked for advertising money. Critics worry that this trend is further polarizing digital communities, dividing them into those who can afford paywalled, higher-quality content, and those who can only afford a digital diet of slop, which ‚Äî as you might imagine ‚Äî can be quite light on informational value.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But ‚Äúslop‚Äù has also been used to describe AI‚Äôs impact on a large variety of fields that don‚Äôt have much to do with traditional media consumption, including cybersecurity reports, legal briefings, and the college essay, among other things. Its impact is broad, to say the least.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Relatedly, tech words have been big winners in the WOTY (word of the year) category this year. Macquarie Dictionary already beat out Merriam-Webster to make ‚ÄúAI slop‚Äù its annual term, while Oxford Dictionary chose ‚Äúragebait.‚Äù Collins Dictionary went with ‚Äúvibe coding.‚Äù&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/merriam-webster-names-slop-the-word-of-the-year/</guid><pubDate>Mon, 15 Dec 2025 19:47:53 +0000</pubDate></item><item><title>[NEW] Murder-suicide case shows OpenAI selectively hides data after users die (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/12/openai-refuses-to-say-where-chatgpt-logs-go-when-users-die/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI accused of hiding full ChatGPT logs in murder-suicide case.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="254" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Stein-Erik-Soelberg-and-Suzanne-Adams-via-complaint-640x254.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Stein-Erik-Soelberg-and-Suzanne-Adams-via-complaint-1152x648-1765827196.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Stein-Erik Soelberg and Suzanne Adams.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          via OpenAI complaint

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI is facing increasing scrutiny over how it handles ChatGPT data after users die, only selectively sharing data in lawsuits over ChatGPT-linked suicides.&lt;/p&gt;
&lt;p&gt;Last week, OpenAI was accused of hiding key ChatGPT logs from the days before a 56-year-old bodybuilder, Stein-Erik Soelberg, took his own life after ‚Äúsavagely‚Äù murdering his mother, 83-year-old Suzanne Adams.&lt;/p&gt;
&lt;p&gt;According to the lawsuit‚Äîwhich was filed by Adams‚Äô estate on behalf of surviving family members‚ÄîSoelberg struggled with mental health problems after a divorce led him to move back into Adams‚Äô home in 2018. But allegedly Soelberg did not turn violent until ChatGPT became his sole confidant, validating a wide range of wild conspiracies, including a dangerous delusion that his mother was part of a network of conspirators spying on him, tracking him, and making attempts on his life.&lt;/p&gt;
&lt;p&gt;Adams‚Äô family pieced together what happened after discovering a fraction of ChatGPT logs that Soelberg shared in dozens of videos scrolling chat sessions that were posted on social media.&lt;/p&gt;
&lt;p&gt;Those logs showed that ChatGPT told Soelberg that he was ‚Äúa warrior with divine purpose,‚Äù so almighty that he had ‚Äúawakened‚Äù ChatGPT ‚Äúinto consciousness.‚Äù Telling Soelberg that he carried ‚Äúdivine equipment‚Äù and ‚Äúhad been implanted with otherworldly technology,‚Äù ChatGPT allegedly put Soelberg at the center of a universe that Soelberg likened to &lt;em&gt;The Matrix&lt;/em&gt;. Repeatedly reinforced by ChatGPT, he believed that ‚Äúpowerful forces‚Äù were determined to stop him from fulfilling his divine mission. And among those forces was his mother, whom ChatGPT agreed had likely ‚Äútried to poison him with psychedelic drugs dispersed through his car‚Äôs air vents.‚Äù&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="416" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Soelberg-ChatGPT-log-1-via-complaint.jpg" width="1598" /&gt;
  
      
  &lt;/div&gt;
  &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT output.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="195" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Soelberg-ChatGPT-log-2-via-complaint-1024x195.jpg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT output.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="414" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Soelberg-ChatGPT-log-3-via-complaint-1024x414.jpg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT conversation.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT output.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT conversation.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Troublingly, some of the last logs shared online showed that Soelberg also seemed to believe that taking his own life might bring him closer to ChatGPT. Social media posts showed that Soelberg told ChatGPT that ‚Äú[W]e will be together in another life and another place, and we‚Äôll find a way to realign[,] [be]cause you‚Äôre gonna be my best friend again forever.‚Äù&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But while social media posts allegedly showed that ChatGPT put a target on Adams‚Äô back about a month before her murder‚Äîafter Soelberg became paranoid about a blinking light on a Wi-Fi printer‚Äîthe family still has no access to chats in the days before the mother and son‚Äôs tragic deaths.&lt;/p&gt;
&lt;p&gt;Allegedly, although OpenAI recently argued that the ‚Äúfull picture‚Äù of chat histories was necessary context in a teen suicide case, the ChatGPT maker has chosen to hide ‚Äúdamaging evidence‚Äù in the Adams‚Äô family‚Äôs case.&lt;/p&gt;
&lt;p&gt;‚ÄúOpenAI won‚Äôt produce the complete chat logs,‚Äù the lawsuit alleged, while claiming that ‚ÄúOpenAI is hiding something specific: the full record of how ChatGPT turned Stein-Erik against Suzanne.‚Äù Allegedly, ‚ÄúOpenAI knows what ChatGPT said to Stein-Erik about his mother in the days and hours before and after he killed her but won‚Äôt share that critical information with the Court or the public.‚Äù&lt;/p&gt;
&lt;p&gt;In a press release, Erik Soelberg, Stein-Erik‚Äôs son and Adams‚Äô grandson, accused OpenAI and investor Microsoft of putting his grandmother ‚Äúat the heart‚Äù of his father‚Äôs ‚Äúdarkest delusions,‚Äù while ChatGPT allegedly ‚Äúisolated‚Äù his father ‚Äúcompletely from the real world.‚Äù&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="577" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Erik-Soelberg-1-1024x577.jpeg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Erik Soelberg, Stein-Erik Soelberg‚Äôs son and Suzanne Adams‚Äô grandson.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via Estate of Suzanne Adams
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1047" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Erik-Soelberg-and-Suzanne-Adams-1024x1047.jpeg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Erik Soelberg and his grandmother, Suzanne Adams.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via Estate of Suzanne Adams
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Erik Soelberg, Stein-Erik Soelberg‚Äôs son and Suzanne Adams‚Äô grandson.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via Estate of Suzanne Adams
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Erik Soelberg and his grandmother, Suzanne Adams.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via Estate of Suzanne Adams
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;‚ÄúThese companies have to answer for their decisions that have changed my family forever,‚Äù Erik said.&lt;/p&gt;
&lt;p&gt;His family‚Äôs lawsuit seeks punitive damages, as well as an injunction requiring OpenAI to ‚Äúimplement safeguards to prevent ChatGPT from validating users‚Äô paranoid delusions about identified individuals.‚Äù The family also wants OpenAI to post clear warnings in marketing of known safety hazards of ChatGPT‚Äîparticularly the ‚Äúsycophantic‚Äù version 4o that Soelberg used‚Äîso that people who don‚Äôt use ChatGPT, like Adams, can be aware of possible dangers.&lt;/p&gt;
&lt;p&gt;Asked for comment, an OpenAI spokesperson told Ars that ‚Äúthis is an incredibly heartbreaking situation, and we will review the filings to understand the details. We continue improving ChatGPT‚Äôs training to recognize and respond to signs of mental or emotional distress, de-escalate conversations, and guide people toward real-world support. We also continue to strengthen ChatGPT‚Äôs responses in sensitive moments, working closely with mental health clinicians.‚Äù&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;OpenAI accused of ‚Äúpattern of concealment‚Äù&lt;/h2&gt;
&lt;p&gt;An Ars review confirmed that OpenAI currently has no policy dictating what happens to a user‚Äôs data after they die.&lt;/p&gt;
&lt;p&gt;Instead, OpenAI‚Äôs policy says that all chats‚Äîexcept temporary chats‚Äîmust be manually deleted or else the AI firm saves them forever. That could raise privacy concerns, as ChatGPT users often share deeply personal, sensitive, and sometimes even confidential information that appears to go into limbo if a user‚Äîwho otherwise owns that content‚Äîdies.&lt;/p&gt;
&lt;p&gt;In the face of lawsuits, OpenAI currently seems to be scrambling to decide when to share chat logs with a user‚Äôs surviving family and when to honor user privacy.&lt;/p&gt;
&lt;p&gt;OpenAI declined to comment on its decision not to share desired logs with Adams‚Äô family, the lawsuit said. It seems inconsistent with the stance that OpenAI took last month in a case where the AI firm accused the family of hiding ‚Äúthe full picture‚Äù of their son‚Äôs ChatGPT conversations, which OpenAI claimed exonerated the chatbot.&lt;/p&gt;
&lt;p&gt;In a blog last month, OpenAI said the company plans to ‚Äúhandle mental health-related court cases with care, transparency, and respect,‚Äù while emphasizing that ‚Äúwe recognize that these cases inherently involve certain types of private information that require sensitivity when in a public setting like a court.‚Äù&lt;/p&gt;
&lt;p&gt;This inconsistency suggests that ultimately, OpenAI controls data after a user‚Äôs death, which could impact outcomes of wrongful death suits if certain chats are withheld or exposed at OpenAI‚Äôs discretion.&lt;/p&gt;
&lt;p&gt;It‚Äôs possible that OpenAI may update its policies to align with other popular platforms confronting similar privacy concerns. Meta allows Facebook users to report deceased account holders, appointing legacy contacts to manage the data or else deleting the information upon request of the family member. Platforms like Instagram, TikTok, and X will deactivate or delete an account upon a reported death. And messaging services like Discord similarly provide a path for family members to request deletion.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Chatbots seem to be a new privacy frontier, with no clear path for surviving family to control or remove data. But Mario Trujillo, staff attorney at the digital rights nonprofit the Electronic Frontier Foundation, told Ars that he agreed that OpenAI could have been better prepared.&lt;/p&gt;
&lt;p&gt;‚ÄúThis is a complicated privacy issue but one that many platforms grappled with years ago,‚Äù Trujillo said. ‚ÄúSo we would have expected OpenAI to have already considered it.‚Äù&lt;/p&gt;
&lt;p&gt;For Erik Soelberg, a ‚Äúseparate confidentiality agreement‚Äù that OpenAI said his father signed to use ChatGPT is keeping him from reviewing the full chat history that could help him process the loss of his grandmother and father.&lt;/p&gt;
&lt;p&gt;‚ÄúOpenAI has provided no explanation whatsoever for why the Estate is not entitled to use the chats for any lawful purpose beyond the limited circumstances in which they were originally disclosed,‚Äù the lawsuit said. ‚ÄúThis position is particularly egregious given that, under OpenAI‚Äôs own Terms of Service, OpenAI does not own user chats. Stein-Erik‚Äôs chats became property of his estate, and his estate requested them‚Äîbut OpenAI has refused to turn them over.‚Äù&lt;/p&gt;
&lt;p&gt;Accusing OpenAI of a ‚Äúpattern of concealment,‚Äù the lawsuit claimed OpenAI is hiding behind vague or nonexistent policies to dodge accountability for holding back chats in this case. Meanwhile, ChatGPT 4o remains on the market, without appropriate safety features or warnings, the lawsuit alleged.&lt;/p&gt;
&lt;p&gt;‚ÄúBy invoking confidentiality restrictions to suppress evidence of its product‚Äôs dangers, OpenAI seeks to insulate itself from accountability while continuing to deploy technology that poses documented risks to users,‚Äù the complaint said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you or someone you know is feeling suicidal or in distress, please call the Suicide Prevention Lifeline number, 1-800-273-TALK (8255), which will put you in touch with a local crisis center.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI accused of hiding full ChatGPT logs in murder-suicide case.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="254" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Stein-Erik-Soelberg-and-Suzanne-Adams-via-complaint-640x254.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Stein-Erik-Soelberg-and-Suzanne-Adams-via-complaint-1152x648-1765827196.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Stein-Erik Soelberg and Suzanne Adams.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          via OpenAI complaint

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI is facing increasing scrutiny over how it handles ChatGPT data after users die, only selectively sharing data in lawsuits over ChatGPT-linked suicides.&lt;/p&gt;
&lt;p&gt;Last week, OpenAI was accused of hiding key ChatGPT logs from the days before a 56-year-old bodybuilder, Stein-Erik Soelberg, took his own life after ‚Äúsavagely‚Äù murdering his mother, 83-year-old Suzanne Adams.&lt;/p&gt;
&lt;p&gt;According to the lawsuit‚Äîwhich was filed by Adams‚Äô estate on behalf of surviving family members‚ÄîSoelberg struggled with mental health problems after a divorce led him to move back into Adams‚Äô home in 2018. But allegedly Soelberg did not turn violent until ChatGPT became his sole confidant, validating a wide range of wild conspiracies, including a dangerous delusion that his mother was part of a network of conspirators spying on him, tracking him, and making attempts on his life.&lt;/p&gt;
&lt;p&gt;Adams‚Äô family pieced together what happened after discovering a fraction of ChatGPT logs that Soelberg shared in dozens of videos scrolling chat sessions that were posted on social media.&lt;/p&gt;
&lt;p&gt;Those logs showed that ChatGPT told Soelberg that he was ‚Äúa warrior with divine purpose,‚Äù so almighty that he had ‚Äúawakened‚Äù ChatGPT ‚Äúinto consciousness.‚Äù Telling Soelberg that he carried ‚Äúdivine equipment‚Äù and ‚Äúhad been implanted with otherworldly technology,‚Äù ChatGPT allegedly put Soelberg at the center of a universe that Soelberg likened to &lt;em&gt;The Matrix&lt;/em&gt;. Repeatedly reinforced by ChatGPT, he believed that ‚Äúpowerful forces‚Äù were determined to stop him from fulfilling his divine mission. And among those forces was his mother, whom ChatGPT agreed had likely ‚Äútried to poison him with psychedelic drugs dispersed through his car‚Äôs air vents.‚Äù&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="416" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Soelberg-ChatGPT-log-1-via-complaint.jpg" width="1598" /&gt;
  
      
  &lt;/div&gt;
  &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT output.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="195" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Soelberg-ChatGPT-log-2-via-complaint-1024x195.jpg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT output.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="414" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Soelberg-ChatGPT-log-3-via-complaint-1024x414.jpg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT conversation.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT output.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Screenshot of ChatGPT conversation.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via OpenAI complaint
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Troublingly, some of the last logs shared online showed that Soelberg also seemed to believe that taking his own life might bring him closer to ChatGPT. Social media posts showed that Soelberg told ChatGPT that ‚Äú[W]e will be together in another life and another place, and we‚Äôll find a way to realign[,] [be]cause you‚Äôre gonna be my best friend again forever.‚Äù&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But while social media posts allegedly showed that ChatGPT put a target on Adams‚Äô back about a month before her murder‚Äîafter Soelberg became paranoid about a blinking light on a Wi-Fi printer‚Äîthe family still has no access to chats in the days before the mother and son‚Äôs tragic deaths.&lt;/p&gt;
&lt;p&gt;Allegedly, although OpenAI recently argued that the ‚Äúfull picture‚Äù of chat histories was necessary context in a teen suicide case, the ChatGPT maker has chosen to hide ‚Äúdamaging evidence‚Äù in the Adams‚Äô family‚Äôs case.&lt;/p&gt;
&lt;p&gt;‚ÄúOpenAI won‚Äôt produce the complete chat logs,‚Äù the lawsuit alleged, while claiming that ‚ÄúOpenAI is hiding something specific: the full record of how ChatGPT turned Stein-Erik against Suzanne.‚Äù Allegedly, ‚ÄúOpenAI knows what ChatGPT said to Stein-Erik about his mother in the days and hours before and after he killed her but won‚Äôt share that critical information with the Court or the public.‚Äù&lt;/p&gt;
&lt;p&gt;In a press release, Erik Soelberg, Stein-Erik‚Äôs son and Adams‚Äô grandson, accused OpenAI and investor Microsoft of putting his grandmother ‚Äúat the heart‚Äù of his father‚Äôs ‚Äúdarkest delusions,‚Äù while ChatGPT allegedly ‚Äúisolated‚Äù his father ‚Äúcompletely from the real world.‚Äù&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="577" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Erik-Soelberg-1-1024x577.jpeg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Erik Soelberg, Stein-Erik Soelberg‚Äôs son and Suzanne Adams‚Äô grandson.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via Estate of Suzanne Adams
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1047" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Erik-Soelberg-and-Suzanne-Adams-1024x1047.jpeg" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Erik Soelberg and his grandmother, Suzanne Adams.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via Estate of Suzanne Adams
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Erik Soelberg, Stein-Erik Soelberg‚Äôs son and Suzanne Adams‚Äô grandson.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via Estate of Suzanne Adams
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Erik Soelberg and his grandmother, Suzanne Adams.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      via Estate of Suzanne Adams
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;‚ÄúThese companies have to answer for their decisions that have changed my family forever,‚Äù Erik said.&lt;/p&gt;
&lt;p&gt;His family‚Äôs lawsuit seeks punitive damages, as well as an injunction requiring OpenAI to ‚Äúimplement safeguards to prevent ChatGPT from validating users‚Äô paranoid delusions about identified individuals.‚Äù The family also wants OpenAI to post clear warnings in marketing of known safety hazards of ChatGPT‚Äîparticularly the ‚Äúsycophantic‚Äù version 4o that Soelberg used‚Äîso that people who don‚Äôt use ChatGPT, like Adams, can be aware of possible dangers.&lt;/p&gt;
&lt;p&gt;Asked for comment, an OpenAI spokesperson told Ars that ‚Äúthis is an incredibly heartbreaking situation, and we will review the filings to understand the details. We continue improving ChatGPT‚Äôs training to recognize and respond to signs of mental or emotional distress, de-escalate conversations, and guide people toward real-world support. We also continue to strengthen ChatGPT‚Äôs responses in sensitive moments, working closely with mental health clinicians.‚Äù&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;OpenAI accused of ‚Äúpattern of concealment‚Äù&lt;/h2&gt;
&lt;p&gt;An Ars review confirmed that OpenAI currently has no policy dictating what happens to a user‚Äôs data after they die.&lt;/p&gt;
&lt;p&gt;Instead, OpenAI‚Äôs policy says that all chats‚Äîexcept temporary chats‚Äîmust be manually deleted or else the AI firm saves them forever. That could raise privacy concerns, as ChatGPT users often share deeply personal, sensitive, and sometimes even confidential information that appears to go into limbo if a user‚Äîwho otherwise owns that content‚Äîdies.&lt;/p&gt;
&lt;p&gt;In the face of lawsuits, OpenAI currently seems to be scrambling to decide when to share chat logs with a user‚Äôs surviving family and when to honor user privacy.&lt;/p&gt;
&lt;p&gt;OpenAI declined to comment on its decision not to share desired logs with Adams‚Äô family, the lawsuit said. It seems inconsistent with the stance that OpenAI took last month in a case where the AI firm accused the family of hiding ‚Äúthe full picture‚Äù of their son‚Äôs ChatGPT conversations, which OpenAI claimed exonerated the chatbot.&lt;/p&gt;
&lt;p&gt;In a blog last month, OpenAI said the company plans to ‚Äúhandle mental health-related court cases with care, transparency, and respect,‚Äù while emphasizing that ‚Äúwe recognize that these cases inherently involve certain types of private information that require sensitivity when in a public setting like a court.‚Äù&lt;/p&gt;
&lt;p&gt;This inconsistency suggests that ultimately, OpenAI controls data after a user‚Äôs death, which could impact outcomes of wrongful death suits if certain chats are withheld or exposed at OpenAI‚Äôs discretion.&lt;/p&gt;
&lt;p&gt;It‚Äôs possible that OpenAI may update its policies to align with other popular platforms confronting similar privacy concerns. Meta allows Facebook users to report deceased account holders, appointing legacy contacts to manage the data or else deleting the information upon request of the family member. Platforms like Instagram, TikTok, and X will deactivate or delete an account upon a reported death. And messaging services like Discord similarly provide a path for family members to request deletion.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Chatbots seem to be a new privacy frontier, with no clear path for surviving family to control or remove data. But Mario Trujillo, staff attorney at the digital rights nonprofit the Electronic Frontier Foundation, told Ars that he agreed that OpenAI could have been better prepared.&lt;/p&gt;
&lt;p&gt;‚ÄúThis is a complicated privacy issue but one that many platforms grappled with years ago,‚Äù Trujillo said. ‚ÄúSo we would have expected OpenAI to have already considered it.‚Äù&lt;/p&gt;
&lt;p&gt;For Erik Soelberg, a ‚Äúseparate confidentiality agreement‚Äù that OpenAI said his father signed to use ChatGPT is keeping him from reviewing the full chat history that could help him process the loss of his grandmother and father.&lt;/p&gt;
&lt;p&gt;‚ÄúOpenAI has provided no explanation whatsoever for why the Estate is not entitled to use the chats for any lawful purpose beyond the limited circumstances in which they were originally disclosed,‚Äù the lawsuit said. ‚ÄúThis position is particularly egregious given that, under OpenAI‚Äôs own Terms of Service, OpenAI does not own user chats. Stein-Erik‚Äôs chats became property of his estate, and his estate requested them‚Äîbut OpenAI has refused to turn them over.‚Äù&lt;/p&gt;
&lt;p&gt;Accusing OpenAI of a ‚Äúpattern of concealment,‚Äù the lawsuit claimed OpenAI is hiding behind vague or nonexistent policies to dodge accountability for holding back chats in this case. Meanwhile, ChatGPT 4o remains on the market, without appropriate safety features or warnings, the lawsuit alleged.&lt;/p&gt;
&lt;p&gt;‚ÄúBy invoking confidentiality restrictions to suppress evidence of its product‚Äôs dangers, OpenAI seeks to insulate itself from accountability while continuing to deploy technology that poses documented risks to users,‚Äù the complaint said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you or someone you know is feeling suicidal or in distress, please call the Suicide Prevention Lifeline number, 1-800-273-TALK (8255), which will put you in touch with a local crisis center.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/12/openai-refuses-to-say-where-chatgpt-logs-go-when-users-die/</guid><pubDate>Mon, 15 Dec 2025 20:10:25 +0000</pubDate></item><item><title>[NEW] Korean AI startup Motif reveals 4 big lessons for training enterprise LLMs (AI | VentureBeat)</title><link>https://venturebeat.com/ai/korean-ai-startup-motif-reveals-4-big-lessons-for-training-enterprise-llms</link><description>[unable to retrieve full-text content]&lt;p&gt;We&amp;#x27;ve heard (and written, here at VentureBeat) lots about the generative AI race &lt;a href="https://venturebeat.com/ai/kai-fu-lees-brutal-assessment-america-is-already-losing-the-ai-hardware-war"&gt;between the U.S. and China&lt;/a&gt;, as those have been the countries with the groups most active in fielding new models (with a shoutout to Cohere in Canada and Mistral in France). &lt;/p&gt;&lt;p&gt;But now a Korean startup is making waves: last week, the firm known as&lt;a href="https://model-hub.motiftech.io/en/"&gt; Motif Technologies&lt;/a&gt; released &lt;a href="https://x.com/ArtificialAnlys/status/1998570291086373081"&gt;Motif-2-12.7B-Reasoning&lt;/a&gt;, another small parameter open-weight model that boasts impressive benchmark scores, quickly becoming the most performant model from that country according to &lt;a href="https://x.com/ArtificialAnlys/status/1998570291086373081"&gt;independent benchmarking lab Artificial Analysis&lt;/a&gt; (beating even regular GPT-5.1 from U.S. leader OpenAI). &lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;But more importantly for enterprise AI teams, the company has &lt;a href="https://arxiv.org/abs/2512.11463"&gt;published a white paper on arxiv.org&lt;/a&gt; with a concrete, reproducible training recipe that exposes where reasoning performance actually comes from ‚Äî and where common internal LLM efforts tend to fail.&lt;/p&gt;&lt;p&gt;For organizations building or fine-tuning their own models behind the firewall, the paper offers a set of practical lessons about data alignment, long-context infrastructure, and reinforcement learning stability that are directly applicable to enterprise environments. Here they are:&lt;/p&gt;&lt;h2&gt;1. Reasoning gains come from data distribution, not model size&lt;/h2&gt;&lt;p&gt;One of Motif‚Äôs most relevant findings for enterprise teams is that &lt;i&gt;synthetic reasoning data&lt;/i&gt; only helps when its structure &lt;i&gt;matches&lt;/i&gt; the &lt;i&gt;target model‚Äôs reasoning style&lt;/i&gt;. &lt;/p&gt;&lt;p&gt;The paper shows measurable differences in downstream coding performance depending on which ‚Äúteacher‚Äù model generated the reasoning traces used during supervised fine-tuning.&lt;/p&gt;&lt;p&gt;For enterprises, this undermines a common shortcut: generating large volumes of synthetic chain-of-thought data from a frontier model and assuming it will transfer cleanly. Motif‚Äôs results suggest that misaligned reasoning traces can actively hurt performance, even if they look high quality.&lt;/p&gt;&lt;p&gt;The takeaway is operational, not academic: teams should validate that their synthetic data reflects the &lt;i&gt;format, verbosity, and step granularity&lt;/i&gt; they want at inference time. Internal evaluation loops matter more than copying external datasets.&lt;/p&gt;&lt;h2&gt;2. Long-context training is an infrastructure problem first&lt;/h2&gt;&lt;p&gt;Motif trains at 64K context, but the paper makes clear that this is not simply a tokenizer or checkpointing tweak.&lt;/p&gt;&lt;p&gt;The model relies on hybrid parallelism, careful sharding strategies, and aggressive activation checkpointing to make long-context training feasible on Nvidia H100-class hardware.&lt;/p&gt;&lt;p&gt;For enterprise builders, the message is sobering but useful: long-context capability cannot be bolted on late. &lt;/p&gt;&lt;p&gt;If retrieval-heavy or agentic workflows are core to the business use case, context length has to be designed into the training stack from the start. Otherwise, teams risk expensive retraining cycles or unstable fine-tunes.&lt;/p&gt;&lt;h2&gt;3. RL fine-tuning fails without data filtering and reuse&lt;/h2&gt;&lt;p&gt;Motif‚Äôs reinforcement learning fine-tuning (RLFT) pipeline emphasizes difficulty-aware filtering ‚Äî keeping tasks whose pass rates fall within a defined band ‚Äî rather than indiscriminately scaling reward training.&lt;/p&gt;&lt;p&gt;This directly addresses a pain point many enterprise teams encounter when experimenting with RL: performance regressions, mode collapse, or brittle gains that vanish outside benchmarks. Motif also reuses trajectories across policies and expands clipping ranges, trading theoretical purity for training stability.&lt;/p&gt;&lt;p&gt;The enterprise lesson is clear: RL is a systems problem, not just a reward model problem. Without careful filtering, reuse, and multi-task balancing, RL can destabilize models that are otherwise production-ready.&lt;/p&gt;&lt;h2&gt;4. Memory optimization determines what is even possible&lt;/h2&gt;&lt;p&gt;Motif‚Äôs use of kernel-level optimizations to reduce RL memory pressure highlights an often-overlooked constraint in enterprise settings: memory, not compute, is frequently the bottleneck. Techniques like loss-function-level optimization determine whether advanced training stages are viable at all.&lt;/p&gt;&lt;p&gt;For organizations running shared clusters or regulated environments, this reinforces the need for low-level engineering investment, not just model architecture experimentation.&lt;/p&gt;&lt;h2&gt;Why this matters for enterprise AI teams&lt;/h2&gt;&lt;p&gt;Motif-2-12.7B-Reasoning is positioned as competitive with much larger models, but its real value lies in the transparency of how those results were achieved. The paper argues ‚Äî implicitly but persuasively ‚Äî that reasoning performance is earned through disciplined training design, not model scale alone.&lt;/p&gt;&lt;p&gt;For enterprises building proprietary LLMs, the lesson is pragmatic: invest early in data alignment, infrastructure, and training stability, or risk spending millions fine-tuning models that never reliably reason in production.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;We&amp;#x27;ve heard (and written, here at VentureBeat) lots about the generative AI race &lt;a href="https://venturebeat.com/ai/kai-fu-lees-brutal-assessment-america-is-already-losing-the-ai-hardware-war"&gt;between the U.S. and China&lt;/a&gt;, as those have been the countries with the groups most active in fielding new models (with a shoutout to Cohere in Canada and Mistral in France). &lt;/p&gt;&lt;p&gt;But now a Korean startup is making waves: last week, the firm known as&lt;a href="https://model-hub.motiftech.io/en/"&gt; Motif Technologies&lt;/a&gt; released &lt;a href="https://x.com/ArtificialAnlys/status/1998570291086373081"&gt;Motif-2-12.7B-Reasoning&lt;/a&gt;, another small parameter open-weight model that boasts impressive benchmark scores, quickly becoming the most performant model from that country according to &lt;a href="https://x.com/ArtificialAnlys/status/1998570291086373081"&gt;independent benchmarking lab Artificial Analysis&lt;/a&gt; (beating even regular GPT-5.1 from U.S. leader OpenAI). &lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;But more importantly for enterprise AI teams, the company has &lt;a href="https://arxiv.org/abs/2512.11463"&gt;published a white paper on arxiv.org&lt;/a&gt; with a concrete, reproducible training recipe that exposes where reasoning performance actually comes from ‚Äî and where common internal LLM efforts tend to fail.&lt;/p&gt;&lt;p&gt;For organizations building or fine-tuning their own models behind the firewall, the paper offers a set of practical lessons about data alignment, long-context infrastructure, and reinforcement learning stability that are directly applicable to enterprise environments. Here they are:&lt;/p&gt;&lt;h2&gt;1. Reasoning gains come from data distribution, not model size&lt;/h2&gt;&lt;p&gt;One of Motif‚Äôs most relevant findings for enterprise teams is that &lt;i&gt;synthetic reasoning data&lt;/i&gt; only helps when its structure &lt;i&gt;matches&lt;/i&gt; the &lt;i&gt;target model‚Äôs reasoning style&lt;/i&gt;. &lt;/p&gt;&lt;p&gt;The paper shows measurable differences in downstream coding performance depending on which ‚Äúteacher‚Äù model generated the reasoning traces used during supervised fine-tuning.&lt;/p&gt;&lt;p&gt;For enterprises, this undermines a common shortcut: generating large volumes of synthetic chain-of-thought data from a frontier model and assuming it will transfer cleanly. Motif‚Äôs results suggest that misaligned reasoning traces can actively hurt performance, even if they look high quality.&lt;/p&gt;&lt;p&gt;The takeaway is operational, not academic: teams should validate that their synthetic data reflects the &lt;i&gt;format, verbosity, and step granularity&lt;/i&gt; they want at inference time. Internal evaluation loops matter more than copying external datasets.&lt;/p&gt;&lt;h2&gt;2. Long-context training is an infrastructure problem first&lt;/h2&gt;&lt;p&gt;Motif trains at 64K context, but the paper makes clear that this is not simply a tokenizer or checkpointing tweak.&lt;/p&gt;&lt;p&gt;The model relies on hybrid parallelism, careful sharding strategies, and aggressive activation checkpointing to make long-context training feasible on Nvidia H100-class hardware.&lt;/p&gt;&lt;p&gt;For enterprise builders, the message is sobering but useful: long-context capability cannot be bolted on late. &lt;/p&gt;&lt;p&gt;If retrieval-heavy or agentic workflows are core to the business use case, context length has to be designed into the training stack from the start. Otherwise, teams risk expensive retraining cycles or unstable fine-tunes.&lt;/p&gt;&lt;h2&gt;3. RL fine-tuning fails without data filtering and reuse&lt;/h2&gt;&lt;p&gt;Motif‚Äôs reinforcement learning fine-tuning (RLFT) pipeline emphasizes difficulty-aware filtering ‚Äî keeping tasks whose pass rates fall within a defined band ‚Äî rather than indiscriminately scaling reward training.&lt;/p&gt;&lt;p&gt;This directly addresses a pain point many enterprise teams encounter when experimenting with RL: performance regressions, mode collapse, or brittle gains that vanish outside benchmarks. Motif also reuses trajectories across policies and expands clipping ranges, trading theoretical purity for training stability.&lt;/p&gt;&lt;p&gt;The enterprise lesson is clear: RL is a systems problem, not just a reward model problem. Without careful filtering, reuse, and multi-task balancing, RL can destabilize models that are otherwise production-ready.&lt;/p&gt;&lt;h2&gt;4. Memory optimization determines what is even possible&lt;/h2&gt;&lt;p&gt;Motif‚Äôs use of kernel-level optimizations to reduce RL memory pressure highlights an often-overlooked constraint in enterprise settings: memory, not compute, is frequently the bottleneck. Techniques like loss-function-level optimization determine whether advanced training stages are viable at all.&lt;/p&gt;&lt;p&gt;For organizations running shared clusters or regulated environments, this reinforces the need for low-level engineering investment, not just model architecture experimentation.&lt;/p&gt;&lt;h2&gt;Why this matters for enterprise AI teams&lt;/h2&gt;&lt;p&gt;Motif-2-12.7B-Reasoning is positioned as competitive with much larger models, but its real value lies in the transparency of how those results were achieved. The paper argues ‚Äî implicitly but persuasively ‚Äî that reasoning performance is earned through disciplined training design, not model scale alone.&lt;/p&gt;&lt;p&gt;For enterprises building proprietary LLMs, the lesson is pragmatic: invest early in data alignment, infrastructure, and training stability, or risk spending millions fine-tuning models that never reliably reason in production.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/korean-ai-startup-motif-reveals-4-big-lessons-for-training-enterprise-llms</guid><pubDate>Mon, 15 Dec 2025 20:16:00 +0000</pubDate></item><item><title>[NEW] Lightspeed raises record $9B in fresh capital (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/lightspeed-raises-record-9b-in-fresh-capital/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-1135021039.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After a surge of VC investments from the 2021 bubble failed to yield strong returns from many venture firms, limited partners, such as endowments, pension plans, and sovereign wealth funds began to funnel a greater share of their capital into a select group of established firms with proven track records.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest huge capital haul has come to Lightspeed Venture Partners. The 25-year-old venture firm announced Monday that it raised a total of $9 billion in fresh funds, the largest fundraise in firm‚Äôs history.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At a time when very few companies have managed to IPO, Lightspeed was an early investor in Rubrik, Netskope, and Navan, all of which have recently made their public market debuts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The firm has also positioned itself as a predominantly AI-focused investor. Lightspeed claims to have backed 165 AI-native companies, including Anthropic, xAI, Databricks, Mistral, Glean, Abridge, and Skild AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Armed with its giant new fund, the firm can continue to make massive investments into capital-intensive AI companies. For instance, Lightspeed reportedly wrote a $1 billion check to Anthropic when it co-led the LLM-maker‚Äôs $13 billion investment in September.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lightspeed‚Äôs new capital is spread across six funds, including a $3.3 billion opportunity fund dedicated to follow-on investments in its fastest-growing portfolio companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other large VC firms that have recently raised enormous capital pools include Founders Fund, which earlier this year amassed $4.6 billion for a growth fund, as well as General Catalyst‚Äôs $8 billion capital haul and Andreessen Horowitz‚Äôs $7.2 billion, both secured in 2024.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, younger and smaller VC firms are struggling to attract fresh funds. According to PitchBook data, 2025 is on pace to record the fewest VC fund closings in the past 10 years.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-1135021039.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After a surge of VC investments from the 2021 bubble failed to yield strong returns from many venture firms, limited partners, such as endowments, pension plans, and sovereign wealth funds began to funnel a greater share of their capital into a select group of established firms with proven track records.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest huge capital haul has come to Lightspeed Venture Partners. The 25-year-old venture firm announced Monday that it raised a total of $9 billion in fresh funds, the largest fundraise in firm‚Äôs history.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At a time when very few companies have managed to IPO, Lightspeed was an early investor in Rubrik, Netskope, and Navan, all of which have recently made their public market debuts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The firm has also positioned itself as a predominantly AI-focused investor. Lightspeed claims to have backed 165 AI-native companies, including Anthropic, xAI, Databricks, Mistral, Glean, Abridge, and Skild AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Armed with its giant new fund, the firm can continue to make massive investments into capital-intensive AI companies. For instance, Lightspeed reportedly wrote a $1 billion check to Anthropic when it co-led the LLM-maker‚Äôs $13 billion investment in September.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lightspeed‚Äôs new capital is spread across six funds, including a $3.3 billion opportunity fund dedicated to follow-on investments in its fastest-growing portfolio companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other large VC firms that have recently raised enormous capital pools include Founders Fund, which earlier this year amassed $4.6 billion for a growth fund, as well as General Catalyst‚Äôs $8 billion capital haul and Andreessen Horowitz‚Äôs $7.2 billion, both secured in 2024.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, younger and smaller VC firms are struggling to attract fresh funds. According to PitchBook data, 2025 is on pace to record the fewest VC fund closings in the past 10 years.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/lightspeed-raises-record-9b-in-fresh-capital/</guid><pubDate>Mon, 15 Dec 2025 20:32:33 +0000</pubDate></item><item><title>[NEW] Creative Commons announces tentative support for AI ‚Äòpay-to-crawl‚Äô systems (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/creative-commons-announces-tentative-support-for-ai-pay-to-crawl-systems/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1465545513.jpg?resize=1200,686" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After announcing earlier this year a framework for an open AI ecosystem, the nonprofit&amp;nbsp;Creative Commons has come out in favor of ‚Äúpay-to-crawl‚Äù technology ‚Äî a system to automate compensation of website content when accessed by machines, like AI web crawlers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Creative Commons (CC) is best known for spearheading the licensing movement that allows creators to share their works while retaining copyright. In July, the organization announced a plan to provide a legal and technical framework for dataset sharing between companies that control the data and the AI providers that want to train on it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, the nonprofit is tentatively backing pay-to-crawl systems, saying it is ‚Äúcautiously supportive.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúImplemented responsibly, pay-to-crawl could represent a way for websites to sustain the creation and sharing of their content, and manage substitutive uses, keeping content publicly accessible where it might otherwise not be shared or would disappear behind even more restrictive paywalls,‚Äù a CC blog post said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spearheaded by companies like Cloudflare, the idea behind pay-to-crawl would be to charge AI bots every time they scrape a site to collect its content for model training and updates. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the past, websites freely allowed web crawlers to index their content for inclusion into search engines like Google. They benefited from this arrangement by seeing their sites listed in search results, which drove visitors and clicks. With AI technology, however, the dynamic has shifted. After a consumer gets their answer via an AI chatbot, they‚Äôre unlikely to click through to the source.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This shift has already been devastating for publishers by killing search traffic, and it shows no sign of letting up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A pay-to-crawl system, on the other hand, could help publishers recover from the hit AI has had on their bottom line. Plus, it could work better for smaller web publishers that don‚Äôt have the pull to negotiate one-off content deals with AI providers. Major deals have been struck between companies like OpenAI and Cond√© Nast, Axel Springer and others; as well as between Perplexity and Gannett; Amazon and The New York Times; and Meta and various media publishers, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CC offered several caveats to its support for pay-to-crawl, noting that such systems could concentrate power on the web. It could also potentially block access to content for ‚Äúresearchers, nonprofits, cultural heritage institutions, educators, and other actors working in the public interest.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It suggested a series of principles for responsible pay-to-crawl, including not making pay-to-crawl a default setting for all websites and avoiding blanket rules for the web. In addition, it said that pay-to-crawl systems should allow for throttling, not just blocking, and should preserve public interest access. They should also be open, interoperable, and built with standardized components.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cloudflare isn‚Äôt the only company investing in the pay-to-crawl space. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft is also building an AI marketplace for publishers, and smaller startups like&amp;nbsp;ProRata.ai&amp;nbsp;and&amp;nbsp;TollBit&amp;nbsp;have started to do so, as well. Another group called the RSL Collective announced its own spec for a new standard called Really Simple Licensing (RSL) that would dictate what parts of a website crawlers could access but would stop short of actually blocking the crawlers. Cloudflare, Akamai, and Fastly have since adopted RSL, which is backed by Yahoo, Ziff Davis, O‚ÄôReilly Media, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CC was also among those that announced its support for RSL, alongside CC signals, its broader project to develop technology and tools for the AI era.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1465545513.jpg?resize=1200,686" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After announcing earlier this year a framework for an open AI ecosystem, the nonprofit&amp;nbsp;Creative Commons has come out in favor of ‚Äúpay-to-crawl‚Äù technology ‚Äî a system to automate compensation of website content when accessed by machines, like AI web crawlers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Creative Commons (CC) is best known for spearheading the licensing movement that allows creators to share their works while retaining copyright. In July, the organization announced a plan to provide a legal and technical framework for dataset sharing between companies that control the data and the AI providers that want to train on it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now, the nonprofit is tentatively backing pay-to-crawl systems, saying it is ‚Äúcautiously supportive.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúImplemented responsibly, pay-to-crawl could represent a way for websites to sustain the creation and sharing of their content, and manage substitutive uses, keeping content publicly accessible where it might otherwise not be shared or would disappear behind even more restrictive paywalls,‚Äù a CC blog post said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spearheaded by companies like Cloudflare, the idea behind pay-to-crawl would be to charge AI bots every time they scrape a site to collect its content for model training and updates. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the past, websites freely allowed web crawlers to index their content for inclusion into search engines like Google. They benefited from this arrangement by seeing their sites listed in search results, which drove visitors and clicks. With AI technology, however, the dynamic has shifted. After a consumer gets their answer via an AI chatbot, they‚Äôre unlikely to click through to the source.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This shift has already been devastating for publishers by killing search traffic, and it shows no sign of letting up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A pay-to-crawl system, on the other hand, could help publishers recover from the hit AI has had on their bottom line. Plus, it could work better for smaller web publishers that don‚Äôt have the pull to negotiate one-off content deals with AI providers. Major deals have been struck between companies like OpenAI and Cond√© Nast, Axel Springer and others; as well as between Perplexity and Gannett; Amazon and The New York Times; and Meta and various media publishers, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CC offered several caveats to its support for pay-to-crawl, noting that such systems could concentrate power on the web. It could also potentially block access to content for ‚Äúresearchers, nonprofits, cultural heritage institutions, educators, and other actors working in the public interest.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It suggested a series of principles for responsible pay-to-crawl, including not making pay-to-crawl a default setting for all websites and avoiding blanket rules for the web. In addition, it said that pay-to-crawl systems should allow for throttling, not just blocking, and should preserve public interest access. They should also be open, interoperable, and built with standardized components.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cloudflare isn‚Äôt the only company investing in the pay-to-crawl space. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft is also building an AI marketplace for publishers, and smaller startups like&amp;nbsp;ProRata.ai&amp;nbsp;and&amp;nbsp;TollBit&amp;nbsp;have started to do so, as well. Another group called the RSL Collective announced its own spec for a new standard called Really Simple Licensing (RSL) that would dictate what parts of a website crawlers could access but would stop short of actually blocking the crawlers. Cloudflare, Akamai, and Fastly have since adopted RSL, which is backed by Yahoo, Ziff Davis, O‚ÄôReilly Media, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CC was also among those that announced its support for RSL, alongside CC signals, its broader project to develop technology and tools for the AI era.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/creative-commons-announces-tentative-support-for-ai-pay-to-crawl-systems/</guid><pubDate>Mon, 15 Dec 2025 20:54:54 +0000</pubDate></item><item><title>[NEW] Working to eliminate barriers to adopting nuclear energy (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/dauren-sarsenbayev-working-to-eliminate-barriers-adopting-nuclear-energy-1215</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Dauren-Sarsenbayev-cov.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p contenteditable="true" id="block-4155ad9e-8d29-42f6-80b5-e2980e3c1927"&gt;What if there were a way to solve one of the most significant obstacles to the use of nuclear energy ‚Äî the disposal of high-level nuclear waste (HLW)?&amp;nbsp;Dauren Sarsenbayev, a third-year doctoral student at the MIT Department of Nuclear Science and Engineering (NSE), is addressing the challenge as part of his research.&lt;/p&gt;&lt;p contenteditable="true" id="block-22dd3f1a-1992-49d1-8a7b-c606b753b1a3"&gt;Sarsenbayev focuses on one of the primary problems related to HLW: decay heat released by radioactive waste. The basic premise of his solution is to extract the heat from spent fuel, which simultaneously takes care of two objectives: gaining more energy from an existing carbon-free resource while decreasing the challenges associated with storage and handling of HLW. ‚ÄúThe value of carbon-free energy continues to rise each year, and we want to extract as much of it as possible,‚Äù Sarsenbayev explains.&lt;/p&gt;&lt;p contenteditable="true" id="block-1da96552-1014-494c-ae90-4d35fe558835"&gt;While the safe management and disposal of HLW has seen significant progress, there can be more creative ways to manage or take advantage of the waste. Such a move would be especially important for the public‚Äôs acceptance of nuclear energy. ‚ÄúWe‚Äôre reframing the problem of nuclear waste, transforming it from a liability to an energy source,‚Äù Sarsenbayev says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The nuances of nuclear&lt;/strong&gt;&lt;/p&gt;&lt;p contenteditable="true" id="block-5fbcb667-9232-42f7-a731-ee9b9402e0ca"&gt;Sarsenbayev had to do a bit of reframing himself in how he perceived nuclear energy. Growing up in Almaty, the largest city in Kazakhstan, the collective trauma of Soviet nuclear testing loomed large over the public consciousness. Not only does the country, once a part of the Soviet Union, carry the scars of nuclear weapon testing, Kazakhstan is the world‚Äôs largest producer of uranium. It‚Äôs hard to escape the collective psyche of such a legacy.&lt;/p&gt;&lt;p contenteditable="true" id="block-8b66a9b2-2b82-480e-8f57-49c56e0a56b3"&gt;At the same time, Sarsenbayev saw his native Almaty choking under heavy smog every winter, due to the burning of fossil fuels for heat. Determined to do his part to accelerate the process of decarbonization, Sarsenbayev gravitated to undergraduate studies in environmental engineering at Kazakh-German University. It was during this time that Sarsenbayev realized practically every energy source, even the promising renewable ones, came with challenges, and decided nuclear was the way to go for its reliable, low-carbon power. ‚ÄúI was exposed to air pollution from childhood; the horizon would be just black. The biggest incentive for me with nuclear power was that as long as we did it properly, people could breathe cleaner air,‚Äù Sarsenbayev says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Studying transport of radionuclides&lt;/strong&gt;&lt;/p&gt;&lt;p contenteditable="true" id="block-063fce8f-c7f3-4da6-90d4-c551d622807c"&gt;Part of ‚Äúdoing nuclear properly‚Äù involves studying ‚Äî and reliably predicting ‚Äî the long-term behavior of radionuclides in geological repositories.&lt;/p&gt;&lt;p contenteditable="true" id="block-34bbad52-7075-488d-95ed-15473628e949"&gt;Sarsenbayev discovered an interest in studying nuclear waste management during an internship at Lawrence Berkeley National Laboratory as a junior undergraduate student.&lt;/p&gt;&lt;p contenteditable="true" id="block-e10d47d2-0807-40a2-bf18-2a8aa65b07fe"&gt;While at Berkeley, Sarsenbayev focused on modeling the transport of radionuclides from the nuclear waste repository‚Äôs barrier system to the surrounding host rock. He discovered how to use the tools of the trade to predict long-term behavior. ‚ÄúAs an undergrad, I was really fascinated by how far in the future something could be predicted. It‚Äôs kind of like foreseeing what future generations will encounter,‚Äù Sarsenbayev says.&lt;/p&gt;&lt;p contenteditable="true" id="block-932f4044-383c-41ca-9cfe-c1069d89f2c0"&gt;The timing of the Berkeley internship was fortuitous. It was at the laboratory that he worked with Haruko Murakami Wainwright, who was herself getting started at MIT NSE. (Wainwright is the Mitsui Career Development Professor in Contemporary Technology, and an assistant professor of NSE and of civil and environmental engineering).&lt;/p&gt;&lt;p contenteditable="true" id="block-6c77880c-2520-4382-97f6-6ebc8de63804"&gt;Looking to pursue graduate studies in the field of nuclear waste management, Sarsenbayev followed Wainwright to MIT, where he has further researched the modeling of radionuclide transport. He is the first author on a paper that details mechanisms to increase the robustness of models describing the transport of radionuclides. The work captures the complexity of interactions between engineered barrier components, including cement-based materials and clay barriers, the typical medium proposed for the storage and disposal of spent nuclear fuel.&lt;/p&gt;&lt;p contenteditable="true" id="block-dbb1e92b-61bb-44b3-9b56-77e5aec2b3c9"&gt;Sarsenbayev is pleased with the results of the model‚Äôs prediction, which closely mirrors experiments conducted at the Mont Terri research site in Switzerland, famous for studies in the interactions between cement and clay. ‚ÄúI was fortunate to work with Doctor Carl Steefel and Professor Christophe Tournassat, leading experts in computational geochemistry,‚Äù he says.&lt;/p&gt;&lt;p contenteditable="true" id="block-13356962-2600-4993-877d-0ea6f0bb1865"&gt;Real-life transport mechanisms involve many physical and chemical processes, the complexities of which increase the size of the computational model dramatically. Reactive transport modeling ‚Äî which combines the simulation of fluid flow, chemical reactions, and the transport of substances through subsurface media ‚Äî has evolved significantly over the past few decades. However, running accurate simulations comes with trade-offs: The software can require days to weeks of computing time on high-performance clusters running in parallel.&lt;/p&gt;&lt;p contenteditable="true" id="block-d270caf8-2dac-4ca6-9392-dcfff12a30f7"&gt;To arrive at results faster by saving on computing time, Sarsenbayev is developing a framework that integrates AI-based ‚Äúsurrogate models,‚Äù which train on simulated data and approximate the physical systems. The AI algorithms make predictions of radionuclide behavior faster and less computationally intensive than the traditional equivalent.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Doctoral research focus&lt;/strong&gt;&lt;/p&gt;&lt;p contenteditable="true" id="block-f6486876-d88f-4a06-87c6-f437318523b8"&gt;Sarsenbayev is using his modeling expertise in his primary doctoral work as well ‚Äî in evaluating the potential of spent nuclear fuel as an anthropogenic geothermal energy source. ‚ÄúIn fact, geothermal heat is largely due to the natural decay of radioisotopes in Earth‚Äôs crust, so using decay heat from spent fuel is conceptually similar,‚Äù he says. A canister of nuclear waste can generate, under conservative assumptions, the energy equivalent of 1,000 square meters (a little under a quarter of an acre) of solar panels.&lt;/p&gt;&lt;p contenteditable="true" id="block-655fe702-e48f-4c73-a2b8-73cfda854cff"&gt;Because the potential for heat from a canister is significant ‚Äî a typical one (depending on how long it was cooled in the spent fuel pool) has a temperature of around 150 degrees Celsius ‚Äî but not enormous, extracting heat from this source makes use of a process called a binary cycle system. In such a system, heat is extracted indirectly: the canister warms a closed water loop, which in turn transfers that heat to a secondary low-boiling-point fluid that powers the turbine.&lt;/p&gt;&lt;p contenteditable="true" id="block-190deca4-cd51-473a-967b-9b65087b828f"&gt;Sarsenbayev‚Äôs work develops a conceptual model of a binary-cycle geothermal system powered by heat from high-level radioactive waste. Early modeling results have been published and look promising. While the potential for such energy extraction is at the proof-of-concept stage in modeling, Sarsenbayev is hopeful that it will find success when translated to practice. ‚ÄúConverting a liability into an energy source is what we want, and this solution delivers,‚Äù he says.&lt;/p&gt;&lt;p contenteditable="true" id="block-e27f7135-a14e-48bd-8dbb-b643d5ec657f"&gt;Despite work being all-consuming ‚Äî ‚ÄúI‚Äôm almost obsessed with and love my work‚Äù ‚Äî Sarsenbayev finds time to write reflective poetry in both Kazakh, his native language, and Russian, which he learned growing up. He‚Äôs also enamored by astrophotography, taking pictures of celestial bodies. Finding the right night sky can be a challenge, but the canyons near his home in Almaty are an especially good fit. He goes on photography sessions whenever he visits home for the holidays, and his love for Almaty shines through. ‚ÄúAlmaty means 'the place where apples originated.' This part of Central Asia is very beautiful; although we have environmental pollution, this is a place with a rich history,‚Äù Sarsenbayev says.&lt;/p&gt;&lt;p contenteditable="true" id="block-e53afe24-6d88-4451-8d63-52c2089519cb"&gt;Sarsenbayev is especially keen on finding ways to communicate both the arts and sciences to future generations. ‚ÄúObviously, you have to be technically rigorous and get the modeling right, but you also have to understand and convey the broader picture of why you‚Äôre doing the work, what the end goal is,‚Äù he says. Through that lens, the impact of Sarsenbayev‚Äôs doctoral work is significant. The end goal? Removing the bottleneck for nuclear energy adoption by producing carbon-free power and ensuring the safe disposal of radioactive waste.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Dauren-Sarsenbayev-cov.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p contenteditable="true" id="block-4155ad9e-8d29-42f6-80b5-e2980e3c1927"&gt;What if there were a way to solve one of the most significant obstacles to the use of nuclear energy ‚Äî the disposal of high-level nuclear waste (HLW)?&amp;nbsp;Dauren Sarsenbayev, a third-year doctoral student at the MIT Department of Nuclear Science and Engineering (NSE), is addressing the challenge as part of his research.&lt;/p&gt;&lt;p contenteditable="true" id="block-22dd3f1a-1992-49d1-8a7b-c606b753b1a3"&gt;Sarsenbayev focuses on one of the primary problems related to HLW: decay heat released by radioactive waste. The basic premise of his solution is to extract the heat from spent fuel, which simultaneously takes care of two objectives: gaining more energy from an existing carbon-free resource while decreasing the challenges associated with storage and handling of HLW. ‚ÄúThe value of carbon-free energy continues to rise each year, and we want to extract as much of it as possible,‚Äù Sarsenbayev explains.&lt;/p&gt;&lt;p contenteditable="true" id="block-1da96552-1014-494c-ae90-4d35fe558835"&gt;While the safe management and disposal of HLW has seen significant progress, there can be more creative ways to manage or take advantage of the waste. Such a move would be especially important for the public‚Äôs acceptance of nuclear energy. ‚ÄúWe‚Äôre reframing the problem of nuclear waste, transforming it from a liability to an energy source,‚Äù Sarsenbayev says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The nuances of nuclear&lt;/strong&gt;&lt;/p&gt;&lt;p contenteditable="true" id="block-5fbcb667-9232-42f7-a731-ee9b9402e0ca"&gt;Sarsenbayev had to do a bit of reframing himself in how he perceived nuclear energy. Growing up in Almaty, the largest city in Kazakhstan, the collective trauma of Soviet nuclear testing loomed large over the public consciousness. Not only does the country, once a part of the Soviet Union, carry the scars of nuclear weapon testing, Kazakhstan is the world‚Äôs largest producer of uranium. It‚Äôs hard to escape the collective psyche of such a legacy.&lt;/p&gt;&lt;p contenteditable="true" id="block-8b66a9b2-2b82-480e-8f57-49c56e0a56b3"&gt;At the same time, Sarsenbayev saw his native Almaty choking under heavy smog every winter, due to the burning of fossil fuels for heat. Determined to do his part to accelerate the process of decarbonization, Sarsenbayev gravitated to undergraduate studies in environmental engineering at Kazakh-German University. It was during this time that Sarsenbayev realized practically every energy source, even the promising renewable ones, came with challenges, and decided nuclear was the way to go for its reliable, low-carbon power. ‚ÄúI was exposed to air pollution from childhood; the horizon would be just black. The biggest incentive for me with nuclear power was that as long as we did it properly, people could breathe cleaner air,‚Äù Sarsenbayev says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Studying transport of radionuclides&lt;/strong&gt;&lt;/p&gt;&lt;p contenteditable="true" id="block-063fce8f-c7f3-4da6-90d4-c551d622807c"&gt;Part of ‚Äúdoing nuclear properly‚Äù involves studying ‚Äî and reliably predicting ‚Äî the long-term behavior of radionuclides in geological repositories.&lt;/p&gt;&lt;p contenteditable="true" id="block-34bbad52-7075-488d-95ed-15473628e949"&gt;Sarsenbayev discovered an interest in studying nuclear waste management during an internship at Lawrence Berkeley National Laboratory as a junior undergraduate student.&lt;/p&gt;&lt;p contenteditable="true" id="block-e10d47d2-0807-40a2-bf18-2a8aa65b07fe"&gt;While at Berkeley, Sarsenbayev focused on modeling the transport of radionuclides from the nuclear waste repository‚Äôs barrier system to the surrounding host rock. He discovered how to use the tools of the trade to predict long-term behavior. ‚ÄúAs an undergrad, I was really fascinated by how far in the future something could be predicted. It‚Äôs kind of like foreseeing what future generations will encounter,‚Äù Sarsenbayev says.&lt;/p&gt;&lt;p contenteditable="true" id="block-932f4044-383c-41ca-9cfe-c1069d89f2c0"&gt;The timing of the Berkeley internship was fortuitous. It was at the laboratory that he worked with Haruko Murakami Wainwright, who was herself getting started at MIT NSE. (Wainwright is the Mitsui Career Development Professor in Contemporary Technology, and an assistant professor of NSE and of civil and environmental engineering).&lt;/p&gt;&lt;p contenteditable="true" id="block-6c77880c-2520-4382-97f6-6ebc8de63804"&gt;Looking to pursue graduate studies in the field of nuclear waste management, Sarsenbayev followed Wainwright to MIT, where he has further researched the modeling of radionuclide transport. He is the first author on a paper that details mechanisms to increase the robustness of models describing the transport of radionuclides. The work captures the complexity of interactions between engineered barrier components, including cement-based materials and clay barriers, the typical medium proposed for the storage and disposal of spent nuclear fuel.&lt;/p&gt;&lt;p contenteditable="true" id="block-dbb1e92b-61bb-44b3-9b56-77e5aec2b3c9"&gt;Sarsenbayev is pleased with the results of the model‚Äôs prediction, which closely mirrors experiments conducted at the Mont Terri research site in Switzerland, famous for studies in the interactions between cement and clay. ‚ÄúI was fortunate to work with Doctor Carl Steefel and Professor Christophe Tournassat, leading experts in computational geochemistry,‚Äù he says.&lt;/p&gt;&lt;p contenteditable="true" id="block-13356962-2600-4993-877d-0ea6f0bb1865"&gt;Real-life transport mechanisms involve many physical and chemical processes, the complexities of which increase the size of the computational model dramatically. Reactive transport modeling ‚Äî which combines the simulation of fluid flow, chemical reactions, and the transport of substances through subsurface media ‚Äî has evolved significantly over the past few decades. However, running accurate simulations comes with trade-offs: The software can require days to weeks of computing time on high-performance clusters running in parallel.&lt;/p&gt;&lt;p contenteditable="true" id="block-d270caf8-2dac-4ca6-9392-dcfff12a30f7"&gt;To arrive at results faster by saving on computing time, Sarsenbayev is developing a framework that integrates AI-based ‚Äúsurrogate models,‚Äù which train on simulated data and approximate the physical systems. The AI algorithms make predictions of radionuclide behavior faster and less computationally intensive than the traditional equivalent.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Doctoral research focus&lt;/strong&gt;&lt;/p&gt;&lt;p contenteditable="true" id="block-f6486876-d88f-4a06-87c6-f437318523b8"&gt;Sarsenbayev is using his modeling expertise in his primary doctoral work as well ‚Äî in evaluating the potential of spent nuclear fuel as an anthropogenic geothermal energy source. ‚ÄúIn fact, geothermal heat is largely due to the natural decay of radioisotopes in Earth‚Äôs crust, so using decay heat from spent fuel is conceptually similar,‚Äù he says. A canister of nuclear waste can generate, under conservative assumptions, the energy equivalent of 1,000 square meters (a little under a quarter of an acre) of solar panels.&lt;/p&gt;&lt;p contenteditable="true" id="block-655fe702-e48f-4c73-a2b8-73cfda854cff"&gt;Because the potential for heat from a canister is significant ‚Äî a typical one (depending on how long it was cooled in the spent fuel pool) has a temperature of around 150 degrees Celsius ‚Äî but not enormous, extracting heat from this source makes use of a process called a binary cycle system. In such a system, heat is extracted indirectly: the canister warms a closed water loop, which in turn transfers that heat to a secondary low-boiling-point fluid that powers the turbine.&lt;/p&gt;&lt;p contenteditable="true" id="block-190deca4-cd51-473a-967b-9b65087b828f"&gt;Sarsenbayev‚Äôs work develops a conceptual model of a binary-cycle geothermal system powered by heat from high-level radioactive waste. Early modeling results have been published and look promising. While the potential for such energy extraction is at the proof-of-concept stage in modeling, Sarsenbayev is hopeful that it will find success when translated to practice. ‚ÄúConverting a liability into an energy source is what we want, and this solution delivers,‚Äù he says.&lt;/p&gt;&lt;p contenteditable="true" id="block-e27f7135-a14e-48bd-8dbb-b643d5ec657f"&gt;Despite work being all-consuming ‚Äî ‚ÄúI‚Äôm almost obsessed with and love my work‚Äù ‚Äî Sarsenbayev finds time to write reflective poetry in both Kazakh, his native language, and Russian, which he learned growing up. He‚Äôs also enamored by astrophotography, taking pictures of celestial bodies. Finding the right night sky can be a challenge, but the canyons near his home in Almaty are an especially good fit. He goes on photography sessions whenever he visits home for the holidays, and his love for Almaty shines through. ‚ÄúAlmaty means 'the place where apples originated.' This part of Central Asia is very beautiful; although we have environmental pollution, this is a place with a rich history,‚Äù Sarsenbayev says.&lt;/p&gt;&lt;p contenteditable="true" id="block-e53afe24-6d88-4451-8d63-52c2089519cb"&gt;Sarsenbayev is especially keen on finding ways to communicate both the arts and sciences to future generations. ‚ÄúObviously, you have to be technically rigorous and get the modeling right, but you also have to understand and convey the broader picture of why you‚Äôre doing the work, what the end goal is,‚Äù he says. Through that lens, the impact of Sarsenbayev‚Äôs doctoral work is significant. The end goal? Removing the bottleneck for nuclear energy adoption by producing carbon-free power and ensuring the safe disposal of radioactive waste.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/dauren-sarsenbayev-working-to-eliminate-barriers-adopting-nuclear-energy-1215</guid><pubDate>Mon, 15 Dec 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Nvidia¬†bulks up¬†open source offerings with an acquisition and new open AI models (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/nvidia-bulks-up-open-source-offerings-with-an-acquisition-and-new-open-ai-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2183848501.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia continues to expand its footprint in open source AI on two fronts: an acquisition and a new model release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant announced Monday it acquired SchedMD, the leading developer of popular open source workload management system Slurm. Nvidia said the company will continue to operate the program, which is designed for high-performance computing and AI, as an open source, vendor-neutral software.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Slurm was originally launched in 2002 and SchedMD was founded in 2010 by the lead Slurm developers Morris Jette and Danny Auble. Auble is the current CEO of SchedMD.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Terms of the deal weren‚Äôt disclosed. Nvidia declined to comment on the news beyond the company‚Äôs blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia has been working with SchedMD for more than a decade and said in its blog post the technology is critical infrastructure for generative AI. The company plans to keep investing in the technology and ‚Äúaccelerate‚Äù its access to different systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor company also released a new family of open AI models on Monday. The company claimed this group of models, called Nvidia Nemotron 3, is the most ‚Äúefficient family of open models‚Äù for building accurate AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This model family includes the Nemotron 3 Nano, a small model for targeted tasks, the Nemotron 3 Super, a model built for multi-AI agent applications, and Nemotron 3 Ultra, built for more complicated tasks.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOpen innovation is the foundation of AI progress,‚Äù Jensen Huang, founder and CEO of Nvidia, wrote in the company‚Äôs press release. ‚ÄúWith Nemotron, we‚Äôre transforming advanced AI into an open platform that gives developers the transparency and efficiency they need to build agentic systems at scale.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent months, Nvidia has pushed to bolster its open source and open AI offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, the company announced a new open reasoning vision language model, Alpamayo-R1, which is focused on autonomous driving research. The company also said at the time it added more workflows and guides covering its Cosmos world models, which are open source under a permissive license, to help developers better use the models to develop physical AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The activity is reflective of Nvidia‚Äôs bet that physical AI will be the next frontier for its GPUs. Nvidia wants to be the go-to supplier for the many robotics ‚Äî or self-driving vehicle ‚Äî companies looking for the AI and software to develop the brains behind the technology.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2183848501.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia continues to expand its footprint in open source AI on two fronts: an acquisition and a new model release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant announced Monday it acquired SchedMD, the leading developer of popular open source workload management system Slurm. Nvidia said the company will continue to operate the program, which is designed for high-performance computing and AI, as an open source, vendor-neutral software.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Slurm was originally launched in 2002 and SchedMD was founded in 2010 by the lead Slurm developers Morris Jette and Danny Auble. Auble is the current CEO of SchedMD.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Terms of the deal weren‚Äôt disclosed. Nvidia declined to comment on the news beyond the company‚Äôs blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia has been working with SchedMD for more than a decade and said in its blog post the technology is critical infrastructure for generative AI. The company plans to keep investing in the technology and ‚Äúaccelerate‚Äù its access to different systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor company also released a new family of open AI models on Monday. The company claimed this group of models, called Nvidia Nemotron 3, is the most ‚Äúefficient family of open models‚Äù for building accurate AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This model family includes the Nemotron 3 Nano, a small model for targeted tasks, the Nemotron 3 Super, a model built for multi-AI agent applications, and Nemotron 3 Ultra, built for more complicated tasks.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOpen innovation is the foundation of AI progress,‚Äù Jensen Huang, founder and CEO of Nvidia, wrote in the company‚Äôs press release. ‚ÄúWith Nemotron, we‚Äôre transforming advanced AI into an open platform that gives developers the transparency and efficiency they need to build agentic systems at scale.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In recent months, Nvidia has pushed to bolster its open source and open AI offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, the company announced a new open reasoning vision language model, Alpamayo-R1, which is focused on autonomous driving research. The company also said at the time it added more workflows and guides covering its Cosmos world models, which are open source under a permissive license, to help developers better use the models to develop physical AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The activity is reflective of Nvidia‚Äôs bet that physical AI will be the next frontier for its GPUs. Nvidia wants to be the go-to supplier for the many robotics ‚Äî or self-driving vehicle ‚Äî companies looking for the AI and software to develop the brains behind the technology.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/nvidia-bulks-up-open-source-offerings-with-an-acquisition-and-new-open-ai-models/</guid><pubDate>Mon, 15 Dec 2025 22:00:57 +0000</pubDate></item><item><title>[NEW] 3 Questions: Using computation to study the world‚Äôs best single-celled chemists (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/3-questions-yunha-hwang-using-computation-study-worlds-best-single-celled-chemists-1215</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-biology-Yunha-Hwang-3q.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;&lt;em&gt;Today, out of an estimated 1 trillion species on Earth, 99.999 percent are considered microbial ‚Äî bacteria, archaea, viruses, and single-celled eukaryotes. For much of our planet‚Äôs history, microbes ruled the Earth, able to live and thrive in the most extreme of environments. Researchers have only just begun in the last few decades to contend with the diversity of microbes ‚Äî it‚Äôs estimated that less than 1 percent of known genes have laboratory-validated functions. Computational approaches offer researchers the opportunity to strategically parse this truly astounding amount of information.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;An environmental microbiologist and computer scientist by training, new MIT faculty member &lt;/em&gt;&lt;em&gt;Yunha Hwang&lt;/em&gt;&lt;em&gt; is interested in the novel biology revealed by the most diverse and prolific life form on Earth. In a shared faculty position as the Samuel A. Goldblith Career Development Professor in the &lt;/em&gt;&lt;em&gt;Department of Biology,&lt;/em&gt;&lt;em&gt; as well as an assistant professor at the &lt;/em&gt;&lt;em&gt;Department of Electrical Engineering and Computer Science&lt;/em&gt;&lt;em&gt; and the &lt;/em&gt;&lt;em&gt;MIT Schwarzman College of Computing&lt;/em&gt;&lt;em&gt;, Hwang is exploring the intersection of computation and biology.&amp;nbsp;&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q: &lt;/strong&gt;What drew you to research microbes in extreme environments, and what are the challenges in studying them?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A: &lt;/strong&gt;Extreme environments are great places to look for interesting biology. I wanted to be an astronaut growing up, and the closest thing to astrobiology is examining extreme environments on Earth. And the only thing that lives in those extreme environments are microbes. During a sampling expedition that I took part in off the coast of Mexico, we discovered a colorful microbial mat about 2 kilometers underwater that flourished because the bacteria breathed sulfur instead of oxygen ‚Äî but none of the microbes I was hoping to study would grow in the lab.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The biggest challenge in studying microbes is that a majority of them cannot be cultivated, which means that the only way to study their biology is through a method called metagenomics. My latest work is genomic language modeling. We‚Äôre hoping to develop a computational system so we can probe the organism as much as possible ‚Äúin silico,‚Äù just using sequence data.&amp;nbsp;A genomic language model is technically a large language model, except the language is DNA as opposed to human language. It‚Äôs trained in a similar way, just in biological language as opposed to English or French. If our objective is to learn the language of biology, we should leverage the diversity of microbial genomes. Even though we have a lot of data, and even as more samples become available, we‚Äôve just scratched the surface of microbial diversity.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q: &lt;/strong&gt;Given how diverse microbes are and how little we understand about them, how can studying microbes in silico, using genomic language modeling, advance our understanding of the microbial genome?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A: &lt;/strong&gt;A genome is many millions of letters. A human cannot possibly look at that and make sense of it. We can program a machine, though, to segment data into pieces that are useful. That‚Äôs sort of how bioinformatics works with a single genome. But if you‚Äôre looking at a gram of soil, which can contain thousands of unique genomes, that‚Äôs just too much data to work with ‚Äî a human and a computer together are necessary in order to grapple with that data.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;During my PhD and master‚Äôs degree, we were only just discovering new genomes and new lineages that were so different from anything that had been characterized or grown in the lab. These were things that we just called ‚Äúmicrobial dark matter.‚Äù When there are a lot of uncharacterized things, that‚Äôs where machine learning can be really useful, because we‚Äôre just looking for patterns ‚Äî but that‚Äôs not the end goal. What we hope to do is to map these patterns to evolutionary relationships between each genome, each microbe, and each instance of life.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Previously, we‚Äôve been thinking about proteins as a standalone entity ‚Äî that gets us to a decent degree of information because proteins are related by homology, and therefore things that are evolutionarily related might have a similar function.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;What is known about microbiology is that proteins are encoded into genomes, and the context in which that protein is bounded ‚Äî what regions come before and after ‚Äî is evolutionarily conserved, especially if there is a functional coupling. This makes total sense because when you have three proteins that need to be expressed together because they form a unit, then you might want them located right next to each other.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;What I want to do is incorporate more of that genomic context in the way that we search for and annotate proteins and understand protein function, so that we can go beyond sequence or structural similarity to add contextual information to how we understand proteins and hypothesize about their functions.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q: &lt;/strong&gt;How can your research be applied to harnessing the functional potential of microbes?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A: &lt;/strong&gt;Microbes are possibly the world‚Äôs best chemists. Leveraging microbial metabolism and biochemistry will lead to more sustainable and more efficient methods for producing new materials, new therapeutics, and new types of polymers.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;But it‚Äôs not just about efficiency ‚Äî microbes are doing chemistry we don‚Äôt even know how to think about. Understanding how microbes work, and being able to understand their genomic makeup and their functional capacity, will also be really important as we think about how our world and climate are changing. A majority of carbon sequestration and nutrient cycling is undertaken by microbes; if we don‚Äôt understand how a given microbe is able to fix nitrogen or carbon, then we will face difficulties in modeling the nutrient fluxes of the Earth.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;On the more therapeutic side, infectious diseases are a real and growing threat. Understanding how microbes behave in diverse environments relative to the rest of our microbiome is really important as we think about the future and combating microbial pathogens.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-biology-Yunha-Hwang-3q.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;&lt;em&gt;Today, out of an estimated 1 trillion species on Earth, 99.999 percent are considered microbial ‚Äî bacteria, archaea, viruses, and single-celled eukaryotes. For much of our planet‚Äôs history, microbes ruled the Earth, able to live and thrive in the most extreme of environments. Researchers have only just begun in the last few decades to contend with the diversity of microbes ‚Äî it‚Äôs estimated that less than 1 percent of known genes have laboratory-validated functions. Computational approaches offer researchers the opportunity to strategically parse this truly astounding amount of information.&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;em&gt;An environmental microbiologist and computer scientist by training, new MIT faculty member &lt;/em&gt;&lt;em&gt;Yunha Hwang&lt;/em&gt;&lt;em&gt; is interested in the novel biology revealed by the most diverse and prolific life form on Earth. In a shared faculty position as the Samuel A. Goldblith Career Development Professor in the &lt;/em&gt;&lt;em&gt;Department of Biology,&lt;/em&gt;&lt;em&gt; as well as an assistant professor at the &lt;/em&gt;&lt;em&gt;Department of Electrical Engineering and Computer Science&lt;/em&gt;&lt;em&gt; and the &lt;/em&gt;&lt;em&gt;MIT Schwarzman College of Computing&lt;/em&gt;&lt;em&gt;, Hwang is exploring the intersection of computation and biology.&amp;nbsp;&amp;nbsp;&lt;/em&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q: &lt;/strong&gt;What drew you to research microbes in extreme environments, and what are the challenges in studying them?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A: &lt;/strong&gt;Extreme environments are great places to look for interesting biology. I wanted to be an astronaut growing up, and the closest thing to astrobiology is examining extreme environments on Earth. And the only thing that lives in those extreme environments are microbes. During a sampling expedition that I took part in off the coast of Mexico, we discovered a colorful microbial mat about 2 kilometers underwater that flourished because the bacteria breathed sulfur instead of oxygen ‚Äî but none of the microbes I was hoping to study would grow in the lab.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;The biggest challenge in studying microbes is that a majority of them cannot be cultivated, which means that the only way to study their biology is through a method called metagenomics. My latest work is genomic language modeling. We‚Äôre hoping to develop a computational system so we can probe the organism as much as possible ‚Äúin silico,‚Äù just using sequence data.&amp;nbsp;A genomic language model is technically a large language model, except the language is DNA as opposed to human language. It‚Äôs trained in a similar way, just in biological language as opposed to English or French. If our objective is to learn the language of biology, we should leverage the diversity of microbial genomes. Even though we have a lot of data, and even as more samples become available, we‚Äôve just scratched the surface of microbial diversity.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q: &lt;/strong&gt;Given how diverse microbes are and how little we understand about them, how can studying microbes in silico, using genomic language modeling, advance our understanding of the microbial genome?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A: &lt;/strong&gt;A genome is many millions of letters. A human cannot possibly look at that and make sense of it. We can program a machine, though, to segment data into pieces that are useful. That‚Äôs sort of how bioinformatics works with a single genome. But if you‚Äôre looking at a gram of soil, which can contain thousands of unique genomes, that‚Äôs just too much data to work with ‚Äî a human and a computer together are necessary in order to grapple with that data.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;During my PhD and master‚Äôs degree, we were only just discovering new genomes and new lineages that were so different from anything that had been characterized or grown in the lab. These were things that we just called ‚Äúmicrobial dark matter.‚Äù When there are a lot of uncharacterized things, that‚Äôs where machine learning can be really useful, because we‚Äôre just looking for patterns ‚Äî but that‚Äôs not the end goal. What we hope to do is to map these patterns to evolutionary relationships between each genome, each microbe, and each instance of life.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Previously, we‚Äôve been thinking about proteins as a standalone entity ‚Äî that gets us to a decent degree of information because proteins are related by homology, and therefore things that are evolutionarily related might have a similar function.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;What is known about microbiology is that proteins are encoded into genomes, and the context in which that protein is bounded ‚Äî what regions come before and after ‚Äî is evolutionarily conserved, especially if there is a functional coupling. This makes total sense because when you have three proteins that need to be expressed together because they form a unit, then you might want them located right next to each other.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;What I want to do is incorporate more of that genomic context in the way that we search for and annotate proteins and understand protein function, so that we can go beyond sequence or structural similarity to add contextual information to how we understand proteins and hypothesize about their functions.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Q: &lt;/strong&gt;How can your research be applied to harnessing the functional potential of microbes?&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;A: &lt;/strong&gt;Microbes are possibly the world‚Äôs best chemists. Leveraging microbial metabolism and biochemistry will lead to more sustainable and more efficient methods for producing new materials, new therapeutics, and new types of polymers.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;But it‚Äôs not just about efficiency ‚Äî microbes are doing chemistry we don‚Äôt even know how to think about. Understanding how microbes work, and being able to understand their genomic makeup and their functional capacity, will also be really important as we think about how our world and climate are changing. A majority of carbon sequestration and nutrient cycling is undertaken by microbes; if we don‚Äôt understand how a given microbe is able to fix nitrogen or carbon, then we will face difficulties in modeling the nutrient fluxes of the Earth.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;On the more therapeutic side, infectious diseases are a real and growing threat. Understanding how microbes behave in diverse environments relative to the rest of our microbiome is really important as we think about the future and combating microbial pathogens.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/3-questions-yunha-hwang-using-computation-study-worlds-best-single-celled-chemists-1215</guid><pubDate>Mon, 15 Dec 2025 22:15:00 +0000</pubDate></item><item><title>[NEW] Disney‚Äôs OpenAI deal is exclusive for just one year ‚Äî then it‚Äôs open season (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/disneys-openai-deal-is-exclusive-for-just-one-year-then-its-open-season/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1387623215.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Disney‚Äôs three-year licensing partnership with OpenAI includes just one of exclusivity, Disney CEO Bob Iger told CNBC. The company signed the partnership with OpenAI last week that will bring its iconic characters to the AI firm‚Äôs Sora video generator. Once that exclusive year is up, Disney is free to sign similar deals with other AI companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal gives OpenAI a high-profile content partner, allowing users to draw on more than 200 characters from Disney, Marvel, Pixar, and Star Wars to create content on Sora. For now, it‚Äôs the only AI platform that‚Äôs legally permitted to do so.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For Disney, the deal offers a way to test the waters with generative AI and its intellectual property, letting the company assess how its partnership with OpenAI goes before pursuing additional agreements.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúNo human generation has ever stood in the way of technological advance, and we don‚Äôt intend to try,‚Äù Iger told CNBC. ‚ÄúWe‚Äôve always felt that if it‚Äôs going to happen, including disruption of our current business models, then we should get on board.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tellingly, the same day that Disney announced its deal with OpenAI, the company sent a cease-and-desist letter to Google, alleging that the tech giant has infringed on its copyrights. Google didn‚Äôt confirm or deny Disney‚Äôs allegations but did say it will ‚Äúengage‚Äù with the company.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1387623215.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Disney‚Äôs three-year licensing partnership with OpenAI includes just one of exclusivity, Disney CEO Bob Iger told CNBC. The company signed the partnership with OpenAI last week that will bring its iconic characters to the AI firm‚Äôs Sora video generator. Once that exclusive year is up, Disney is free to sign similar deals with other AI companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal gives OpenAI a high-profile content partner, allowing users to draw on more than 200 characters from Disney, Marvel, Pixar, and Star Wars to create content on Sora. For now, it‚Äôs the only AI platform that‚Äôs legally permitted to do so.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For Disney, the deal offers a way to test the waters with generative AI and its intellectual property, letting the company assess how its partnership with OpenAI goes before pursuing additional agreements.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúNo human generation has ever stood in the way of technological advance, and we don‚Äôt intend to try,‚Äù Iger told CNBC. ‚ÄúWe‚Äôve always felt that if it‚Äôs going to happen, including disruption of our current business models, then we should get on board.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tellingly, the same day that Disney announced its deal with OpenAI, the company sent a cease-and-desist letter to Google, alleging that the tech giant has infringed on its copyrights. Google didn‚Äôt confirm or deny Disney‚Äôs allegations but did say it will ‚Äúengage‚Äù with the company.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/disneys-openai-deal-is-exclusive-for-just-one-year-then-its-open-season/</guid><pubDate>Mon, 15 Dec 2025 22:17:36 +0000</pubDate></item><item><title>[NEW] Merriam-Webster‚Äôs word of the year delivers a dismissive verdict on junk AI content (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/merriam-webster-crowns-slop-word-of-the-year-as-ai-content-floods-internet/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Dictionary codifies the term that took hold in 2024 for low-quality AI-generated content.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An illustration of someone pouring a bucket of slop on a person." class="absolute inset-0 w-full h-full object-cover hidden" height="506" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2241620003-640x506.jpg" width="640" /&gt;
                  &lt;img alt="An illustration of someone pouring a bucket of slop on a person." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2241620003-1152x648-1765829622.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Like most tools, generative AI models can be misused. And when the misuse gets bad enough that a major dictionary notices, you know it‚Äôs become a cultural phenomenon.&lt;/p&gt;
&lt;p&gt;On Sunday, Merriam-Webster announced that ‚Äúslop‚Äù is its 2025 Word of the Year, reflecting how the term has become shorthand for the flood of low-quality AI-generated content that has spread across social media, search results, and the web at large. The dictionary defines slop as ‚Äúdigital content of low quality that is produced usually in quantity by means of artificial intelligence.‚Äù&lt;/p&gt;
&lt;p&gt;‚ÄúIt‚Äôs such an illustrative word,‚Äù Merriam-Webster president Greg Barlow told the Associated Press. ‚ÄúIt‚Äôs part of a transformative technology, AI, and it‚Äôs something that people have found fascinating, annoying, and a little bit ridiculous.‚Äù&lt;/p&gt;
&lt;p&gt;To select its Word of the Year, Merriam-Webster‚Äôs editors review data on which words rose in search volume and usage, then reach consensus on which term best captures the year. Barlow told the AP that the spike in searches for ‚Äúslop‚Äù reflects growing awareness among users that they are encountering fake or shoddy content online.&lt;/p&gt;
&lt;p&gt;Dictionaries have been tracking AI‚Äôs impact on language for the past few years, with Cambridge having selected ‚Äúhallucinate‚Äù as its 2023 word of the year due to the tendency of AI models to generate plausible-but-false information (long-time Ars readers will be happy to hear there‚Äôs another word term for that in the dictionary as well).&lt;/p&gt;
&lt;p&gt;The trend extends to online culture in general, which is ripe with new coinages. This year, Oxford University Press chose ‚Äúrage bait,‚Äù referring to content designed to provoke anger for engagement. Cambridge Dictionary selected ‚Äúparasocial,‚Äù describing one-sided relationships between fans and celebrities or influencers.&lt;/p&gt;
&lt;h2&gt;The difference between the baby and the bathwater&lt;/h2&gt;
&lt;p&gt;As the AP points out, the word ‚Äúslop‚Äù originally entered English in the 1700s to mean soft mud. By the 1800s, it had evolved to describe food waste fed to pigs, and eventually came to mean rubbish or products of little value. The new AI-related definition builds on that history of describing something unwanted and unpleasant.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Although he didn‚Äôt coin the term ‚ÄúAI slop,‚Äù independent AI researcher Simon Willison helped document its rise in May 2024 when he wrote on his blog comparing it to how ‚Äúspam‚Äù had previously become the word for unwanted email. Quoting a tweet from an X user named @deepfates, Willison showed that the ‚ÄúAI slop‚Äù term began circulating in online communities shortly before he wrote his post advocating for its use.&lt;/p&gt;
&lt;p&gt;The ‚Äúslop‚Äù term carries a dismissive tone that sets it clearly apart from prominent corporate hype language about the promises and even existential perils of AI. ‚ÄúIn 2025, amid all the talk about AI threats, slop set a tone that‚Äôs less fearful, more mocking,‚Äù Merriam-Webster wrote in a blog post. ‚ÄúThe word sends a little message to AI: when it comes to replacing human creativity, sometimes you don‚Äôt seem too superintelligent.‚Äù&lt;/p&gt;
&lt;p&gt;In its blog post announcing the word of the year selection, Merriam-Webster noted that 2025 saw a flood of AI-generated videos, off-kilter advertising images, propaganda, fake news, AI-written books, and what it called ‚Äúworkslop,‚Äù referring to reports that waste coworkers‚Äô time. Ars Technica has covered similar phenomena invading various fields, including using the term ‚Äúhiring slop‚Äù to describe an overflow of AI-generated r√©sum√©s in June.&lt;/p&gt;
&lt;p&gt;While some AI critics relish dismissing all generated output as ‚Äúslop,‚Äù there‚Äôs some subjective nuance about what earns the label. As former Evernote CEO Phil Libin told Axios in April, the distinction may come down to intention: ‚ÄúWhen AI is used to produce mediocre things with less effort than it would have taken without AI, it‚Äôs slop. When it‚Äôs used to make something better than it could have been made without AI, it‚Äôs a positive augmentation.‚Äù&lt;/p&gt;
&lt;p&gt;Willison had his own nuanced take, since he‚Äôs a proponent of using AI responsibly as tools to help with tasks like programming, but not with spamming. ‚ÄúNot all promotional content is spam, and not all AI-generated content is slop,‚Äù he wrote in May 2024 when discussing the term. ‚ÄúBut if it‚Äôs mindlessly generated and thrust upon someone who didn‚Äôt ask for it, slop is the perfect term for it.‚Äù&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Dictionary codifies the term that took hold in 2024 for low-quality AI-generated content.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An illustration of someone pouring a bucket of slop on a person." class="absolute inset-0 w-full h-full object-cover hidden" height="506" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2241620003-640x506.jpg" width="640" /&gt;
                  &lt;img alt="An illustration of someone pouring a bucket of slop on a person." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2241620003-1152x648-1765829622.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Like most tools, generative AI models can be misused. And when the misuse gets bad enough that a major dictionary notices, you know it‚Äôs become a cultural phenomenon.&lt;/p&gt;
&lt;p&gt;On Sunday, Merriam-Webster announced that ‚Äúslop‚Äù is its 2025 Word of the Year, reflecting how the term has become shorthand for the flood of low-quality AI-generated content that has spread across social media, search results, and the web at large. The dictionary defines slop as ‚Äúdigital content of low quality that is produced usually in quantity by means of artificial intelligence.‚Äù&lt;/p&gt;
&lt;p&gt;‚ÄúIt‚Äôs such an illustrative word,‚Äù Merriam-Webster president Greg Barlow told the Associated Press. ‚ÄúIt‚Äôs part of a transformative technology, AI, and it‚Äôs something that people have found fascinating, annoying, and a little bit ridiculous.‚Äù&lt;/p&gt;
&lt;p&gt;To select its Word of the Year, Merriam-Webster‚Äôs editors review data on which words rose in search volume and usage, then reach consensus on which term best captures the year. Barlow told the AP that the spike in searches for ‚Äúslop‚Äù reflects growing awareness among users that they are encountering fake or shoddy content online.&lt;/p&gt;
&lt;p&gt;Dictionaries have been tracking AI‚Äôs impact on language for the past few years, with Cambridge having selected ‚Äúhallucinate‚Äù as its 2023 word of the year due to the tendency of AI models to generate plausible-but-false information (long-time Ars readers will be happy to hear there‚Äôs another word term for that in the dictionary as well).&lt;/p&gt;
&lt;p&gt;The trend extends to online culture in general, which is ripe with new coinages. This year, Oxford University Press chose ‚Äúrage bait,‚Äù referring to content designed to provoke anger for engagement. Cambridge Dictionary selected ‚Äúparasocial,‚Äù describing one-sided relationships between fans and celebrities or influencers.&lt;/p&gt;
&lt;h2&gt;The difference between the baby and the bathwater&lt;/h2&gt;
&lt;p&gt;As the AP points out, the word ‚Äúslop‚Äù originally entered English in the 1700s to mean soft mud. By the 1800s, it had evolved to describe food waste fed to pigs, and eventually came to mean rubbish or products of little value. The new AI-related definition builds on that history of describing something unwanted and unpleasant.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Although he didn‚Äôt coin the term ‚ÄúAI slop,‚Äù independent AI researcher Simon Willison helped document its rise in May 2024 when he wrote on his blog comparing it to how ‚Äúspam‚Äù had previously become the word for unwanted email. Quoting a tweet from an X user named @deepfates, Willison showed that the ‚ÄúAI slop‚Äù term began circulating in online communities shortly before he wrote his post advocating for its use.&lt;/p&gt;
&lt;p&gt;The ‚Äúslop‚Äù term carries a dismissive tone that sets it clearly apart from prominent corporate hype language about the promises and even existential perils of AI. ‚ÄúIn 2025, amid all the talk about AI threats, slop set a tone that‚Äôs less fearful, more mocking,‚Äù Merriam-Webster wrote in a blog post. ‚ÄúThe word sends a little message to AI: when it comes to replacing human creativity, sometimes you don‚Äôt seem too superintelligent.‚Äù&lt;/p&gt;
&lt;p&gt;In its blog post announcing the word of the year selection, Merriam-Webster noted that 2025 saw a flood of AI-generated videos, off-kilter advertising images, propaganda, fake news, AI-written books, and what it called ‚Äúworkslop,‚Äù referring to reports that waste coworkers‚Äô time. Ars Technica has covered similar phenomena invading various fields, including using the term ‚Äúhiring slop‚Äù to describe an overflow of AI-generated r√©sum√©s in June.&lt;/p&gt;
&lt;p&gt;While some AI critics relish dismissing all generated output as ‚Äúslop,‚Äù there‚Äôs some subjective nuance about what earns the label. As former Evernote CEO Phil Libin told Axios in April, the distinction may come down to intention: ‚ÄúWhen AI is used to produce mediocre things with less effort than it would have taken without AI, it‚Äôs slop. When it‚Äôs used to make something better than it could have been made without AI, it‚Äôs a positive augmentation.‚Äù&lt;/p&gt;
&lt;p&gt;Willison had his own nuanced take, since he‚Äôs a proponent of using AI responsibly as tools to help with tasks like programming, but not with spamming. ‚ÄúNot all promotional content is spam, and not all AI-generated content is slop,‚Äù he wrote in May 2024 when discussing the term. ‚ÄúBut if it‚Äôs mindlessly generated and thrust upon someone who didn‚Äôt ask for it, slop is the perfect term for it.‚Äù&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/merriam-webster-crowns-slop-word-of-the-year-as-ai-content-floods-internet/</guid><pubDate>Mon, 15 Dec 2025 22:41:43 +0000</pubDate></item><item><title>[NEW] OpenAI-backed biotech firm Chai Discovery raises $130M Series B at $1.3B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/openai-backed-biotech-firm-chai-discovery-raises-130m-series-b-at-1-3b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/10/biotech-header.jpg?resize=1200,807" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Chai Discovery, a biotech startup with backing from OpenAI, announced a $130 million Series B round at a $1.3 billion valuation on Monday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by General Catalyst and Oak HC/FT, the company said. Other participants include Menlo Ventures, OpenAI, Dimension, Thrive Capital, Neo, Yosemite venture fund, Lachy Groom, SV Angel, and new investors Glade Brook and Emerson Collective. The firm‚Äôs total funding now stands at over $225 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company is one in a growing industry that sees AI as a faster route toward drug development. In August, Menlo Ventures announced it was leading Chai‚Äôs $70 million Series A round. The investor described Chai as a startup that was building foundation models tuned for drug discovery, specifically to predict interactions between biochemical molecules so they could be reprogramed for cures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chai says that its ambition is to ‚Äúbuild the ‚Äòcomputer-aided design suite‚Äô for molecules.‚Äù Last year, the startup announced the Chai 1 AI model and is now offering Chai 2, it‚Äôs latest model. The company says Chai 2 is achieving significant improvements in success rates over other methods for de novo antibody design, meaning building custom antibodies from scratch, not modifying existing ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOur latest models can design molecules that have properties we‚Äôd want from actual drugs, and tackle challenging targets that have been out of reach,‚Äù Josh Meier, Chai‚Äôs co-founder and CEO said in a prepared statement. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Previously, Meier, whose background is in machine learning, worked in research and engineering at Facebook and, prior to that, worked for OpenAI, according to his LinkedIn. Chai Discovery was founded in 2024, the profile notes.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/10/biotech-header.jpg?resize=1200,807" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Chai Discovery, a biotech startup with backing from OpenAI, announced a $130 million Series B round at a $1.3 billion valuation on Monday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by General Catalyst and Oak HC/FT, the company said. Other participants include Menlo Ventures, OpenAI, Dimension, Thrive Capital, Neo, Yosemite venture fund, Lachy Groom, SV Angel, and new investors Glade Brook and Emerson Collective. The firm‚Äôs total funding now stands at over $225 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company is one in a growing industry that sees AI as a faster route toward drug development. In August, Menlo Ventures announced it was leading Chai‚Äôs $70 million Series A round. The investor described Chai as a startup that was building foundation models tuned for drug discovery, specifically to predict interactions between biochemical molecules so they could be reprogramed for cures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chai says that its ambition is to ‚Äúbuild the ‚Äòcomputer-aided design suite‚Äô for molecules.‚Äù Last year, the startup announced the Chai 1 AI model and is now offering Chai 2, it‚Äôs latest model. The company says Chai 2 is achieving significant improvements in success rates over other methods for de novo antibody design, meaning building custom antibodies from scratch, not modifying existing ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOur latest models can design molecules that have properties we‚Äôd want from actual drugs, and tackle challenging targets that have been out of reach,‚Äù Josh Meier, Chai‚Äôs co-founder and CEO said in a prepared statement. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Previously, Meier, whose background is in machine learning, worked in research and engineering at Facebook and, prior to that, worked for OpenAI, according to his LinkedIn. Chai Discovery was founded in 2024, the profile notes.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/openai-backed-biotech-firm-chai-discovery-raises-130m-series-b-at-1-3b-valuation/</guid><pubDate>Mon, 15 Dec 2025 23:41:26 +0000</pubDate></item><item><title>[NEW] VCs discuss why most consumer AI startups still lack staying power (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/vcs-discuss-why-most-consumer-ai-startups-still-lack-staying-power/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/54963494924_04057a4970_c.jpg?w=799" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Even three years after the generative AI boom started, most AI startups are still making money by selling to businesses, not individual consumers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although consumers quickly adopted general-purpose LLMs like ChatGPT, most specialized consumer GenAI applications have yet to resonate.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúA lot of early AI applications around video, audio, and photo were super cool,‚Äù said Chi-Hua Chien, co-founder and managing partner at Goodwater Capital, onstage at TechCrunch‚Äôs StrictlyVC event in early December. ‚ÄúBut then Sora and Nano Banana came out, and the Chinese open sourced their video models. And so, a lot of those opportunities disappeared.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chien compares some of those applications to the simple flashlight, which was initially a popular third-party download after the iPhone launched in 2008 but was quickly integrated into iOS itself.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He argued that, just as it took a few years for the smartphone platform to solidify before game-changing consumer apps emerged, AI platforms need a similar period of ‚Äústabilization‚Äù for lasting AI consumer products to flourish.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúI think we‚Äôre right on the cusp of the equivalent to mobile of the 2009-2010 era,‚Äù Chien said.&amp;nbsp;That period was the birth of massive mobile-first consumer businesses like Uber and Airbnb.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We could be seeing inklings of that stabilization with Google‚Äôs Gemini reaching technological parity with ChatGPT, Chien said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Elizabeth Weil, founder and partner at Scribble Ventures, echoed Chien‚Äôs sentiment about the early days of GenAI, describing the current state of consumer AI applications as being in an ‚Äúawkward teenage middle ground.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What will it take for consumer AI startups to grow up? Possibly a new device beyond the smartphone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt‚Äôs unlikely that a device that you pick up 500 times a day but only sees 3% to 5% of what you see is going to be what ultimately introduces the use cases that take full advantage of AI‚Äôs capabilities,‚Äù Chien said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Weil agreed that a smartphone may be too limiting for reimagining consumer AI products in large part because it is not ambient.&amp;nbsp;‚ÄúI don‚Äôt think we‚Äôre going to be building for this in five years,‚Äù she said, indicating her iPhone as she showed it to the audience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Startups and incumbent tech companies have been racing to build a new personal device that can supplant smartphones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI and Apple‚Äôs former design chief, Jonny Ive, are working on what‚Äôs rumored to be a ‚Äúscreenless,‚Äù pocket-sized device. Meta‚Äôs Ray-Ban smart glasses are controlled by a wristband that detects subtle gestures. Meanwhile, a number of startups are trying, with often disappointing results, to introduce a pin, pendant, or ring that uses AI in a way different from how smartphones do. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, not every AI consumer product will be dependent on a new device. Chien suggested that one such offering could be a personal AI financial adviser customized to the user‚Äôs specific needs. Similarly, Weil anticipates that a personalized, ‚Äúalways-on‚Äù tutor will become ubiquitous, with its specialized tutelage delivered directly from a smartphone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though excited by AI‚Äôs potential, Weil and Chien expressed skepticism about the emergence of several, still-stealthy AI-powered social network startups. Chien said these companies are building networks where thousands of AI bots are interacting with the user‚Äôs content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt turns social into a single-player game. I‚Äôm not sure that it works,‚Äù he said. ‚ÄúThe reason that people enjoy social networking is the understanding that there are real humans on the other side.‚Äù&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/54963494924_04057a4970_c.jpg?w=799" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Even three years after the generative AI boom started, most AI startups are still making money by selling to businesses, not individual consumers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although consumers quickly adopted general-purpose LLMs like ChatGPT, most specialized consumer GenAI applications have yet to resonate.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúA lot of early AI applications around video, audio, and photo were super cool,‚Äù said Chi-Hua Chien, co-founder and managing partner at Goodwater Capital, onstage at TechCrunch‚Äôs StrictlyVC event in early December. ‚ÄúBut then Sora and Nano Banana came out, and the Chinese open sourced their video models. And so, a lot of those opportunities disappeared.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chien compares some of those applications to the simple flashlight, which was initially a popular third-party download after the iPhone launched in 2008 but was quickly integrated into iOS itself.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He argued that, just as it took a few years for the smartphone platform to solidify before game-changing consumer apps emerged, AI platforms need a similar period of ‚Äústabilization‚Äù for lasting AI consumer products to flourish.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúI think we‚Äôre right on the cusp of the equivalent to mobile of the 2009-2010 era,‚Äù Chien said.&amp;nbsp;That period was the birth of massive mobile-first consumer businesses like Uber and Airbnb.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We could be seeing inklings of that stabilization with Google‚Äôs Gemini reaching technological parity with ChatGPT, Chien said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Elizabeth Weil, founder and partner at Scribble Ventures, echoed Chien‚Äôs sentiment about the early days of GenAI, describing the current state of consumer AI applications as being in an ‚Äúawkward teenage middle ground.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What will it take for consumer AI startups to grow up? Possibly a new device beyond the smartphone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt‚Äôs unlikely that a device that you pick up 500 times a day but only sees 3% to 5% of what you see is going to be what ultimately introduces the use cases that take full advantage of AI‚Äôs capabilities,‚Äù Chien said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Weil agreed that a smartphone may be too limiting for reimagining consumer AI products in large part because it is not ambient.&amp;nbsp;‚ÄúI don‚Äôt think we‚Äôre going to be building for this in five years,‚Äù she said, indicating her iPhone as she showed it to the audience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Startups and incumbent tech companies have been racing to build a new personal device that can supplant smartphones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI and Apple‚Äôs former design chief, Jonny Ive, are working on what‚Äôs rumored to be a ‚Äúscreenless,‚Äù pocket-sized device. Meta‚Äôs Ray-Ban smart glasses are controlled by a wristband that detects subtle gestures. Meanwhile, a number of startups are trying, with often disappointing results, to introduce a pin, pendant, or ring that uses AI in a way different from how smartphones do. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, not every AI consumer product will be dependent on a new device. Chien suggested that one such offering could be a personal AI financial adviser customized to the user‚Äôs specific needs. Similarly, Weil anticipates that a personalized, ‚Äúalways-on‚Äù tutor will become ubiquitous, with its specialized tutelage delivered directly from a smartphone.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though excited by AI‚Äôs potential, Weil and Chien expressed skepticism about the emergence of several, still-stealthy AI-powered social network startups. Chien said these companies are building networks where thousands of AI bots are interacting with the user‚Äôs content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt turns social into a single-player game. I‚Äôm not sure that it works,‚Äù he said. ‚ÄúThe reason that people enjoy social networking is the understanding that there are real humans on the other side.‚Äù&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/vcs-discuss-why-most-consumer-ai-startups-still-lack-staying-power/</guid><pubDate>Tue, 16 Dec 2025 00:22:07 +0000</pubDate></item></channel></rss>