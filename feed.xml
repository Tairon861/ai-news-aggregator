<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 17 Oct 2025 06:31:37 +0000</lastBuildDate><item><title>Why AI startups are taking data into their own hands (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/16/why-ai-startups-are-taking-data-into-their-own-hands/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-1295429677.jpg?resize=1200,840" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For one week this summer, Taylor and her roommate wore GoPro cameras strapped to their foreheads as they painted, sculpted, and did household chores. They were training an AI vision model, carefully syncing their footage so the system could get multiple angles on the same behavior. It was difficult work in many ways, but they were well paid for it — and it allowed Taylor to spend most of her day making art.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We woke up, did our regular routine, and then strapped the cameras on our head and synced the times together,” she told me. “Then we would make our breakfast and clean the dishes. Then we’d go our separate ways and work on art.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They were hired to produce five hours of synced footage each day, but Taylor quickly learned she needed to allot seven hours a day for the work, to leave enough time for breaks and physical recovery.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would give you headaches,” she said. “You take it off and there’s just a red square on your forehead.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor, who&amp;nbsp;asked not to give her last name, was working as a data freelancer for Turing, an AI company that connected her to TechCrunch. Turing’s goal wasn’t to teach the AI how to make oil paintings, but to gain more abstract skills around sequential problem-solving and visual reasoning. Unlike a large language model, Turing’s vision model would be trained entirely on video — and most of it would be collected directly by Turing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside artists like Taylor, Turing is contracting with chefs, construction workers, and electricians — anyone who works with their hands. Turing Chief AGI Officer Sudarshan Sivaraman told TechCrunch the manual collection is the only way to get a varied enough dataset.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are doing it for so many different kinds of blue-collar work, so that we have a diversity of data in the pre-training phase,” Sivaraman&amp;nbsp;told TechCrunch. “After we capture all this information, the models will be able to understand how a certain task is performed.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Turing’s work on vision models is part of a growing shift in how AI companies deal with data. Where training sets were once scraped freely from the web or collected from low-paid annotators, companies are now paying top dollar for carefully curated data. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the raw power of AI&amp;nbsp;already established, companies are looking to proprietary training data as a competitive advantage. And instead of farming out the task to contractors, they’re often taking on the work themselves.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The email company Fyxer, which uses AI models to sort emails and draft replies, is one example. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After some early experiments, founder Richard Hollingsworth discovered the best approach was to use an array of small models with tightly focused training data. Unlike Turing, Fyxer is building off someone else’s foundation model — but the underlying insight is the same. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We realized that the quality of the data, not the quantity, is the thing that really defines the performance,” Hollingsworth told me.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In practical terms, that meant some unconventional personnel choices. In the early days, Fyxer engineers and managers were sometimes outnumbered four to one by the executive assistants needed to train the model, Hollingsworth says.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We used a lot of experienced executive assistants, because we needed to train on the fundamentals of whether an email should be responded to,” he told TechCrunch. “It’s a very people-oriented problem. Finding great people is very hard.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The pace of data collection never slowed down, but over time Hollingsworth became more precious about the datasets, preferring smaller sets of more tightly curated datasets when it came time for post-training. As he puts it, “the quality of the data, not the quantity, is the thing that really defines the performance.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s particularly true when synthetic data is used, magnifying both the scope of possible training scenarios and the impact of any flaws in the original dataset. On the vision side, Turing estimates that 75% to 80% of its data is synthetic, extrapolated from the original GoPro videos. But that makes it even more important to keep the original dataset as high-quality as possible.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If the pre-training data itself is not of good quality, then whatever you do with synthetic data is also not going to be of good quality,” Sivaraman says.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond concerns of quality, there’s a powerful competitive logic behind keeping data collection in-house. For Fyxer, the hard work of data collection is one of the best moats the company has against competition. As Hollingsworth sees it, anyone can build an open source model into their product — but not everyone can find expert annotators to train it into a workable product.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We believe that the best way to do it is through data,” he told TechCrunch, “through building custom models, through high-quality, human-led data training.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: A previous version of this piece referred to Turing by an incorrect name. TechCrunch regrets the error.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-1295429677.jpg?resize=1200,840" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For one week this summer, Taylor and her roommate wore GoPro cameras strapped to their foreheads as they painted, sculpted, and did household chores. They were training an AI vision model, carefully syncing their footage so the system could get multiple angles on the same behavior. It was difficult work in many ways, but they were well paid for it — and it allowed Taylor to spend most of her day making art.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We woke up, did our regular routine, and then strapped the cameras on our head and synced the times together,” she told me. “Then we would make our breakfast and clean the dishes. Then we’d go our separate ways and work on art.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They were hired to produce five hours of synced footage each day, but Taylor quickly learned she needed to allot seven hours a day for the work, to leave enough time for breaks and physical recovery.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would give you headaches,” she said. “You take it off and there’s just a red square on your forehead.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor, who&amp;nbsp;asked not to give her last name, was working as a data freelancer for Turing, an AI company that connected her to TechCrunch. Turing’s goal wasn’t to teach the AI how to make oil paintings, but to gain more abstract skills around sequential problem-solving and visual reasoning. Unlike a large language model, Turing’s vision model would be trained entirely on video — and most of it would be collected directly by Turing.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside artists like Taylor, Turing is contracting with chefs, construction workers, and electricians — anyone who works with their hands. Turing Chief AGI Officer Sudarshan Sivaraman told TechCrunch the manual collection is the only way to get a varied enough dataset.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are doing it for so many different kinds of blue-collar work, so that we have a diversity of data in the pre-training phase,” Sivaraman&amp;nbsp;told TechCrunch. “After we capture all this information, the models will be able to understand how a certain task is performed.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Turing’s work on vision models is part of a growing shift in how AI companies deal with data. Where training sets were once scraped freely from the web or collected from low-paid annotators, companies are now paying top dollar for carefully curated data. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the raw power of AI&amp;nbsp;already established, companies are looking to proprietary training data as a competitive advantage. And instead of farming out the task to contractors, they’re often taking on the work themselves.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The email company Fyxer, which uses AI models to sort emails and draft replies, is one example. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After some early experiments, founder Richard Hollingsworth discovered the best approach was to use an array of small models with tightly focused training data. Unlike Turing, Fyxer is building off someone else’s foundation model — but the underlying insight is the same. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We realized that the quality of the data, not the quantity, is the thing that really defines the performance,” Hollingsworth told me.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In practical terms, that meant some unconventional personnel choices. In the early days, Fyxer engineers and managers were sometimes outnumbered four to one by the executive assistants needed to train the model, Hollingsworth says.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We used a lot of experienced executive assistants, because we needed to train on the fundamentals of whether an email should be responded to,” he told TechCrunch. “It’s a very people-oriented problem. Finding great people is very hard.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The pace of data collection never slowed down, but over time Hollingsworth became more precious about the datasets, preferring smaller sets of more tightly curated datasets when it came time for post-training. As he puts it, “the quality of the data, not the quantity, is the thing that really defines the performance.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s particularly true when synthetic data is used, magnifying both the scope of possible training scenarios and the impact of any flaws in the original dataset. On the vision side, Turing estimates that 75% to 80% of its data is synthetic, extrapolated from the original GoPro videos. But that makes it even more important to keep the original dataset as high-quality as possible.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If the pre-training data itself is not of good quality, then whatever you do with synthetic data is also not going to be of good quality,” Sivaraman says.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond concerns of quality, there’s a powerful competitive logic behind keeping data collection in-house. For Fyxer, the hard work of data collection is one of the best moats the company has against competition. As Hollingsworth sees it, anyone can build an open source model into their product — but not everyone can find expert annotators to train it into a workable product.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We believe that the best way to do it is through data,” he told TechCrunch, “through building custom models, through high-quality, human-led data training.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: A previous version of this piece referred to Turing by an incorrect name. TechCrunch regrets the error.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/16/why-ai-startups-are-taking-data-into-their-own-hands/</guid><pubDate>Thu, 16 Oct 2025 19:08:00 +0000</pubDate></item><item><title>Kayak launches an ‘AI Mode’ for travel questions, search, and bookings (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/16/kayak-launches-an-ai-mode-for-travel-questions-search-and-bookings/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Travel search engine Kayak will now allow users to research trips ahead of booking using AI. The company this week launched an “AI Mode” feature that lets users ask travel-related questions as well as compare and book flights, hotels, and cars, through an AI chatbot integrated on the company’s website.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is currently available across both desktop and mobile web, and takes advantage of Kayak’s integration with ChatGPT to deliver contextual results. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The rollout follows the company’s April launch of Kayak.ai, built as a testing ground for working with AI technology. That site also combined Kayak’s data and tools with OpenAI’s technology, letting its tech team try out AI features ahead of bringing them over to Kayak.com&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Essentially, the AI Mode feature offers the same functionality as the Kayak.ai website but is now built directly into Kayak’s website. The company suggests users could ask the chatbot for travel ideas, like locations to fly to for under a certain price point, the best deals to a preferred destination, comparing hotel amenities, finding nonstop flights and rental car options, and more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="a screenshot showing the Kayak &amp;quot;AI mode&amp;quot; on its website, shown on an iPhone." class="wp-image-3058392" height="4096" src="https://techcrunch.com/wp-content/uploads/2025/10/Mweb_cursor.jpg" width="2334" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kayak&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, users can ask the AI more open-ended questions, like “I want to party for NYE — where should I go?” to get recommendations without having specific destinations in mind. Or they could learn when the best time to fly somewhere would be, based on ticket prices. (Kayak has shared other AI prompt ideas on its own blog.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature could be useful in helping consumers in the earlier stages of travel planning, when they’re just exploring ideas. However, it remains to be seen if AI users readily convert to paying customers using these methods. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Mode is initially available in English in the United States but will expand to other countries and languages later in the month. The company also plans to roll out the feature to more platforms and add support for voice-based requests “soon.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="another screenshot of the AI Mode on the Kayak.com website." class="wp-image-3058395" height="4265" src="https://techcrunch.com/wp-content/uploads/2025/10/Mweb_iPhone-14-Pro-1.jpg" width="2430" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kayak&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Travel is an area that’s being explored by AI providers and travel companies alike, as online booking can be a frustrating and tedious experience for consumers as it stands today. To test consumer demand for AI solutions, OpenAI recently announced deals with travel companies like Expedia and Booking.com (the latter is also owned by Kayak’s parent company, Booking Holdings). As a result, those services can now operate as apps inside ChatGPT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By comparison, Kayak’s decision to run the AI chatbot on its own site could provide the company with more direct access to consumer insights about the AI’s use.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Travel search engine Kayak will now allow users to research trips ahead of booking using AI. The company this week launched an “AI Mode” feature that lets users ask travel-related questions as well as compare and book flights, hotels, and cars, through an AI chatbot integrated on the company’s website.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is currently available across both desktop and mobile web, and takes advantage of Kayak’s integration with ChatGPT to deliver contextual results. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The rollout follows the company’s April launch of Kayak.ai, built as a testing ground for working with AI technology. That site also combined Kayak’s data and tools with OpenAI’s technology, letting its tech team try out AI features ahead of bringing them over to Kayak.com&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Essentially, the AI Mode feature offers the same functionality as the Kayak.ai website but is now built directly into Kayak’s website. The company suggests users could ask the chatbot for travel ideas, like locations to fly to for under a certain price point, the best deals to a preferred destination, comparing hotel amenities, finding nonstop flights and rental car options, and more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="a screenshot showing the Kayak &amp;quot;AI mode&amp;quot; on its website, shown on an iPhone." class="wp-image-3058392" height="4096" src="https://techcrunch.com/wp-content/uploads/2025/10/Mweb_cursor.jpg" width="2334" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kayak&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, users can ask the AI more open-ended questions, like “I want to party for NYE — where should I go?” to get recommendations without having specific destinations in mind. Or they could learn when the best time to fly somewhere would be, based on ticket prices. (Kayak has shared other AI prompt ideas on its own blog.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature could be useful in helping consumers in the earlier stages of travel planning, when they’re just exploring ideas. However, it remains to be seen if AI users readily convert to paying customers using these methods. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Mode is initially available in English in the United States but will expand to other countries and languages later in the month. The company also plans to roll out the feature to more platforms and add support for voice-based requests “soon.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="another screenshot of the AI Mode on the Kayak.com website." class="wp-image-3058395" height="4265" src="https://techcrunch.com/wp-content/uploads/2025/10/Mweb_iPhone-14-Pro-1.jpg" width="2430" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kayak&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Travel is an area that’s being explored by AI providers and travel companies alike, as online booking can be a frustrating and tedious experience for consumers as it stands today. To test consumer demand for AI solutions, OpenAI recently announced deals with travel companies like Expedia and Booking.com (the latter is also owned by Kayak’s parent company, Booking Holdings). As a result, those services can now operate as apps inside ChatGPT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By comparison, Kayak’s decision to run the AI chatbot on its own site could provide the company with more direct access to consumer insights about the AI’s use.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/16/kayak-launches-an-ai-mode-for-travel-questions-search-and-bookings/</guid><pubDate>Thu, 16 Oct 2025 19:23:07 +0000</pubDate></item><item><title>OnePlus unveils OxygenOS 16 update with deep Gemini integration (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/10/oneplus-unveils-oxygenos-16-update-with-deep-gemini-integration/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Does your phone even have a Mind Space?
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="OnePlus Mind Space" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/OP-mind-space-640x360.jpg" width="640" /&gt;
                  &lt;img alt="OnePlus Mind Space" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/OP-mind-space-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OnePlus

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OnePlus is expected to take the wraps off the OnePlus 15 in the next few weeks, but before that, it’s giving us a look at the software that will run on it. OxygenOS 16, which is based on Android 16, will also come to the company’s other supported phones, and it’s going to include a heaping helping of AI features. OnePlus was slower than most smartphone makers to embrace AI, but it’s full-steam ahead now with new Gemini integrations.&lt;/p&gt;
&lt;p&gt;OxygenOS 16 is described by OnePlus in grandiose terms as “a defiant rebellion for authenticity.” In the real world, this update is doing a lot of the same things as other AI-heavy smartphones. It’s not &lt;em&gt;all &lt;/em&gt;AI—OnePlus notes that OxygenOS 16 will include revamped animations that have been carefully designed for smoothness, as well as the O+ remote app that gives you remote access to Windows and Mac PCs. The lock screen is also more customizable, borrowing a page from the likes of Apple and Samsung.&lt;/p&gt;
&lt;p&gt;OnePlus began embracing AI in June, when it launched a feature called Mind Space on the OnePlus 13S. That phone was only for the Indian market, but the rest of the world will get this and more with OxygenOS 16. At launch, Mind Space would collect your screenshots and brief voice messages. Mind Space would analyze the screenshots to create calendar entries and not much else.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The updated Android software expands what you can add to Mind Space and uses Gemini. For starters, you can add scrolling screenshots and voice memos up to 60 seconds in length. This provides more data for the AI to generate content. For example, if you take screenshots of hotel listings and airline flights, you can tell Gemini to use your Mind Space content to create a trip itinerary. This will be fully integrated with the phone and won’t require a separate subscription to Google’s AI tools.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122848 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="oneplus-oxygen-os16" class="fullwidth full" height="1200" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/oneplus-oxygen-os161-copy.jpg" width="2000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OnePlus

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Mind Space isn’t a totally new idea—it’s quite similar to AI features like Nothing’s Essential Space and Google’s Pixel Screenshots and Journal. The idea is that if you give an AI model enough data on your thoughts and plans, it can provide useful insights. That’s still hypothetical based on what we’ve seen from other smartphone OEMs, but that’s not stopping OnePlus from fully embracing AI in Android 16.&lt;/p&gt;
&lt;p&gt;In addition to beefing up Mind Space, OxygenOS 16 will also add system-wide AI writing tools, which is another common AI add-on. Like the systems from Apple, Google, and Samsung, you will be able to use the OnePlus writing tools to adjust text, proofread, and generate summaries.&lt;/p&gt;
&lt;p&gt;OnePlus will make OxygenOS 16 available starting October 17 as an open beta. You’ll need a OnePlus device from the past three years to run the software, both in the beta phase and when it’s finally released. As for that, OnePlus hasn’t offered a specific date. The initial OxygenOS 16 release will be with the OnePlus 15 devices, with releases for other supported phones and tablets coming later.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Does your phone even have a Mind Space?
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="OnePlus Mind Space" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/OP-mind-space-640x360.jpg" width="640" /&gt;
                  &lt;img alt="OnePlus Mind Space" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/OP-mind-space-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OnePlus

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OnePlus is expected to take the wraps off the OnePlus 15 in the next few weeks, but before that, it’s giving us a look at the software that will run on it. OxygenOS 16, which is based on Android 16, will also come to the company’s other supported phones, and it’s going to include a heaping helping of AI features. OnePlus was slower than most smartphone makers to embrace AI, but it’s full-steam ahead now with new Gemini integrations.&lt;/p&gt;
&lt;p&gt;OxygenOS 16 is described by OnePlus in grandiose terms as “a defiant rebellion for authenticity.” In the real world, this update is doing a lot of the same things as other AI-heavy smartphones. It’s not &lt;em&gt;all &lt;/em&gt;AI—OnePlus notes that OxygenOS 16 will include revamped animations that have been carefully designed for smoothness, as well as the O+ remote app that gives you remote access to Windows and Mac PCs. The lock screen is also more customizable, borrowing a page from the likes of Apple and Samsung.&lt;/p&gt;
&lt;p&gt;OnePlus began embracing AI in June, when it launched a feature called Mind Space on the OnePlus 13S. That phone was only for the Indian market, but the rest of the world will get this and more with OxygenOS 16. At launch, Mind Space would collect your screenshots and brief voice messages. Mind Space would analyze the screenshots to create calendar entries and not much else.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The updated Android software expands what you can add to Mind Space and uses Gemini. For starters, you can add scrolling screenshots and voice memos up to 60 seconds in length. This provides more data for the AI to generate content. For example, if you take screenshots of hotel listings and airline flights, you can tell Gemini to use your Mind Space content to create a trip itinerary. This will be fully integrated with the phone and won’t require a separate subscription to Google’s AI tools.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122848 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="oneplus-oxygen-os16" class="fullwidth full" height="1200" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/oneplus-oxygen-os161-copy.jpg" width="2000" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OnePlus

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Mind Space isn’t a totally new idea—it’s quite similar to AI features like Nothing’s Essential Space and Google’s Pixel Screenshots and Journal. The idea is that if you give an AI model enough data on your thoughts and plans, it can provide useful insights. That’s still hypothetical based on what we’ve seen from other smartphone OEMs, but that’s not stopping OnePlus from fully embracing AI in Android 16.&lt;/p&gt;
&lt;p&gt;In addition to beefing up Mind Space, OxygenOS 16 will also add system-wide AI writing tools, which is another common AI add-on. Like the systems from Apple, Google, and Samsung, you will be able to use the OnePlus writing tools to adjust text, proofread, and generate summaries.&lt;/p&gt;
&lt;p&gt;OnePlus will make OxygenOS 16 available starting October 17 as an open beta. You’ll need a OnePlus device from the past three years to run the software, both in the beta phase and when it’s finally released. As for that, OnePlus hasn’t offered a specific date. The initial OxygenOS 16 release will be with the OnePlus 15 devices, with releases for other supported phones and tablets coming later.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/10/oneplus-unveils-oxygenos-16-update-with-deep-gemini-integration/</guid><pubDate>Thu, 16 Oct 2025 20:05:19 +0000</pubDate></item><item><title>Ars Live recap: Is the AI bubble about to pop? Ed Zitron weighs in. (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/ars-live-recap-is-the-ai-bubble-about-to-pop-ed-zitron-weighs-in/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Despite connection hiccups, we covered OpenAI's finances, nuclear power, and Sam Altman.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An &amp;quot;AI&amp;quot; balloon floating close to a sharp, upturned push pin." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ai_bubble_hero2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="An &amp;quot;AI&amp;quot; balloon floating close to a sharp, upturned push pin." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ai_bubble_hero2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Wong Yu Liang via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Tuesday of last week, Ars Technica hosted a live conversation with Ed Zitron, host of the Better Offline podcast and one of tech’s most vocal AI critics, to discuss whether the generative AI industry is experiencing a bubble and when it might burst. My Internet connection had other plans, though, dropping out multiple times and forcing Ars Technica’s Lee Hutchinson to jump in as an excellent emergency backup host.&lt;/p&gt;
&lt;p&gt;During the times my connection cooperated, Zitron and I covered OpenAI’s financial issues, lofty infrastructure promises, and why the AI hype machine keeps rolling despite some arguably shaky economics underneath. Lee’s probing questions about per-user costs revealed a potential flaw in AI subscription models: Companies can’t predict whether a user will cost them $2 or $10,000 per month.&lt;/p&gt;
&lt;p&gt;You can watch a recording of the event on YouTube or in the window below.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Our discussion with Ed Zitron. Click here for transcript.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;h2&gt;“A 50 billion-dollar industry pretending to be a trillion-dollar one”&lt;/h2&gt;
&lt;p&gt;I started by asking Zitron the most direct question I could: “Why are you so mad about AI?” His answer got right to the heart of his critique: the disconnect between AI’s actual capabilities and how it’s being sold. “Because everybody’s acting like it’s something it isn’t,” Zitron said. “They’re acting like it’s this panacea that will be the future of software growth, the future of hardware growth, the future of compute.”&lt;/p&gt;
&lt;p&gt;In one of his newsletters, Zitron describes the generative AI market as “a 50 billion dollar revenue industry masquerading as a one trillion-dollar one.” He pointed to OpenAI’s financial burn rate (losing an estimated $9.7 billion in the first half of 2025 alone) as evidence that the economics don’t work, coupled with a heavy dose of pessimism about AI in general.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2108866 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="683" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2212801534.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Donald Trump listens as Nvidia CEO Jensen Huang speaks at the White House during an event on “Investing in America” on April 30, 2025, in Washington, DC.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Andrew Harnik / Staff | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;“The models just do not have the efficacy,” Zitron said during our conversation. “AI agents is one of the most egregious lies the tech industry has ever told. Autonomous agents don’t exist.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;He contrasted the relatively small revenue generated by AI companies with the massive capital expenditures flowing into the sector. Even major cloud providers and chip makers are showing strain. Oracle reportedly lost $100 million in three months after installing Nvidia’s new Blackwell GPUs, which Zitron noted are “extremely power-hungry and expensive to run.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Finding utility despite the hype&lt;/h2&gt;
&lt;p&gt;I pushed back against some of Zitron’s broader dismissals of AI by sharing my own experience. I use AI chatbots frequently for brainstorming useful ideas and helping me see them from different angles. “I find I use AI models as sort of knowledge translators and framework translators,” I explained.&lt;/p&gt;
&lt;p&gt;After experiencing brain fog from repeated bouts of COVID over the years, I’ve also found tools like ChatGPT and Claude especially helpful for memory augmentation that pierces through brain fog: describing something in a roundabout, fuzzy way and quickly getting an answer I can then verify. Along these lines, I’ve previously written about how people in a UK study found AI assistants useful accessibility tools.&lt;/p&gt;
&lt;p&gt;Zitron acknowledged this could be useful for me personally but declined to draw any larger conclusions from my one data point. “I understand how that might be helpful; that’s cool,” he said. “I’m glad that that helps you in that way; it’s not a trillion-dollar use case.”&lt;/p&gt;
&lt;p&gt;He also shared his own attempts at using AI tools, including experimenting with Claude Code despite not being a coder himself.&lt;/p&gt;
&lt;p&gt;“If I liked [AI] somehow, it would be actually a more interesting story because I’d be talking about something I liked that was also onerously expensive,” Zitron explained. “But it doesn’t even do that, and it’s actually one of my core frustrations, it’s like this massive over-promise thing. I’m an early adopter guy. I will buy early crap all the time. I bought an Apple Vision Pro, like, what more do you say there? I’m ready to accept issues, but AI is all issues, it’s all filler, no killer; it’s very strange.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Zitron and I agree that current AI assistants are being marketed beyond their actual capabilities. As I often say, AI models are not people, and they are not good factual references. As such, they cannot replace human decision-making and cannot wholesale replace human intellectual labor (at the moment). Instead, I see AI models as augmentations of human capability: as tools rather than autonomous entities.&lt;/p&gt;
&lt;h2&gt;Computing costs: History versus reality&lt;/h2&gt;
&lt;p&gt;Even though Zitron and I found some common ground about AI hype, I expressed a belief that criticism over the cost and power requirements of operating AI models will eventually not become an issue.&lt;/p&gt;
&lt;p&gt;I attempted to make that case by noting that computing costs historically trend downward over time, referencing the Air Force’s SAGE computer system from the 1950s: a four-story building that performed 75,000 operations per second while consuming two megawatts of power. Today, pocket-sized phones deliver millions of times more computing power in a way that would be impossible, power consumption-wise, in the 1950s.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-1821593 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="486" src="https://cdn.arstechnica.net/wp-content/uploads/2021/12/GettyImages-837581308-640x486.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The blockhouse for the Semi-Automatic Ground Environment at Stewart Air Force Base, Newburgh, New York.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Denver Post via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;“I think it will eventually work that way,” I said, suggesting that AI inference costs might follow similar patterns of improvement over years and that AI tools will eventually become commodity components of computer operating systems. Basically, even if AI models stay inefficient, AI models of a certain baseline usefulness and capability will still be cheaper to train and run in the future because the computing systems they run on will be faster, cheaper, and less power-hungry as well.&lt;/p&gt;
&lt;p&gt;Zitron pushed back on this optimism, saying that AI costs are currently moving in the wrong direction. “The costs are going up, unilaterally across the board,” he said. Even newer systems like Cerebras and Grok can generate results faster but not cheaper. He also questioned whether integrating AI into operating systems would prove useful even if the technology became profitable, since AI models struggle with deterministic commands and consistent behavior.&lt;/p&gt;
&lt;h2&gt;The power problem and circular investments&lt;/h2&gt;
&lt;p&gt;One of Zitron’s most pointed criticisms during the discussion centered on OpenAI’s infrastructure promises. The company has pledged to build data centers requiring 10 gigawatts of power capacity (equivalent to 10 nuclear power plants, I once pointed out) for its Stargate project in Abilene, Texas. According to Zitron’s research, the town currently has only 350 megawatts of generating capacity and a 200-megawatt substation.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“A gigawatt of power is a lot, and it’s not like &lt;em&gt;Red Alert 2&lt;/em&gt;,” Zitron said, referencing the real-time strategy game. “You don’t just build a power station and it happens. There are months of actual physics to make sure that it doesn’t kill everyone.”&lt;/p&gt;
&lt;p&gt;He believes many announced data centers will never be completed, calling the infrastructure promises “castles on sand” that nobody in the financial press seems willing to question directly.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2097196 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="An orange, cloudy sky backlights a set of electrical wires on large pylons, leading away from the cooling towers of a nuclear power plant." class="center large" height="683" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-1540176964-1024x683.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anton Petrus via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;After another technical blackout on my end, I came back online and asked Zitron to define the scope of the AI bubble. He says it has evolved from one bubble (foundation models) into two or three, now including AI compute companies like CoreWeave and the market’s obsession with Nvidia.&lt;/p&gt;
&lt;p&gt;Zitron highlighted what he sees as essentially circular investment schemes propping up the industry. He pointed to OpenAI’s $300 billion deal with Oracle and Nvidia’s relationship with CoreWeave as examples. “CoreWeave, they literally… They funded CoreWeave, became their biggest customer, then CoreWeave took that contract and those GPUs and used them as collateral to raise debt to buy more GPUs,” Zitron explained.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;When will the bubble pop?&lt;/h2&gt;
&lt;p&gt;Zitron predicted the bubble would burst within the next year and a half, though he acknowledged it could happen sooner. He expects a cascade of events rather than a single dramatic collapse: An AI startup will run out of money, triggering panic among other startups and their venture capital backers, creating a fire-sale environment that makes future fundraising impossible.&lt;/p&gt;
&lt;p&gt;“It’s not gonna be one Bear Stearns moment,” Zitron explained. “It’s gonna be a succession of events until the markets freak out.”&lt;/p&gt;
&lt;p&gt;The crux of the problem, according to Zitron, is Nvidia. The chip maker’s stock represents 7 to 8 percent of the S&amp;amp;P 500’s value, and the broader market has become dependent on Nvidia’s continued hyper growth. When Nvidia posted “only” 55 percent year-over-year growth in January, the market wobbled.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Nvidia’s growth is why the bubble is inflated,” Zitron said. “If their growth goes down, the bubble will burst.”&lt;/p&gt;
&lt;p&gt;He also warned of broader consequences: “I think there’s a depression coming. I think once the markets work out that tech doesn’t grow forever, they’re gonna flush the toilet aggressively on Silicon Valley.” This connects to his larger thesis: that the tech industry has run out of genuine hyper-growth opportunities and is trying to manufacture one with AI.&lt;/p&gt;
&lt;p&gt;“Is there anything that would falsify your premise of this bubble and crash happening?” I asked. “What if you’re wrong?”&lt;/p&gt;
&lt;p&gt;“I’ve been answering ‘What if you’re wrong?’ for a year-and-a-half to two years, so I’m not bothered by that question, so the thing that would have to prove me right would’ve already needed to happen,” he said. Amid a longer exposition about Sam Altman, Zitron said, “The thing that would’ve had to happen with inference would’ve had to be… it would have to be hundredths of a cent per million tokens, they would have to be printing money, and then, it would have to be way more useful. It would have to have efficacy that it does not have, the hallucination problems… would have to be fixable, and on top of this, someone would have to fix agents.”&lt;/p&gt;
&lt;h2&gt;A positivity challenge&lt;/h2&gt;
&lt;p&gt;Near the end of our conversation, I wondered if I could flip the script, so to speak, and see if he could say something positive or optimistic, although I chose the most challenging subject possible for him. “What’s the best thing about Sam Altman,” I asked. “Can you say anything nice about him at all?”&lt;/p&gt;
&lt;p&gt;“I understand why you’re asking this,” Zitron started, “but I wanna be clear: Sam Altman is going to be the reason the markets take a crap. Sam Altman has lied to everyone. Sam Altman has been lying forever.” He continued, “Like the Pied Piper, he’s led the markets into an abyss, and yes, people should have known better, but I hope at the end of this, Sam Altman is seen for what he is, which is a con artist and a very successful one.”&lt;/p&gt;
&lt;p&gt;Then he added, “You know what? I’ll say something nice about him, he’s really good at making people say, ‘Yes.'”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Despite connection hiccups, we covered OpenAI's finances, nuclear power, and Sam Altman.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An &amp;quot;AI&amp;quot; balloon floating close to a sharp, upturned push pin." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ai_bubble_hero2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="An &amp;quot;AI&amp;quot; balloon floating close to a sharp, upturned push pin." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/ai_bubble_hero2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Wong Yu Liang via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Tuesday of last week, Ars Technica hosted a live conversation with Ed Zitron, host of the Better Offline podcast and one of tech’s most vocal AI critics, to discuss whether the generative AI industry is experiencing a bubble and when it might burst. My Internet connection had other plans, though, dropping out multiple times and forcing Ars Technica’s Lee Hutchinson to jump in as an excellent emergency backup host.&lt;/p&gt;
&lt;p&gt;During the times my connection cooperated, Zitron and I covered OpenAI’s financial issues, lofty infrastructure promises, and why the AI hype machine keeps rolling despite some arguably shaky economics underneath. Lee’s probing questions about per-user costs revealed a potential flaw in AI subscription models: Companies can’t predict whether a user will cost them $2 or $10,000 per month.&lt;/p&gt;
&lt;p&gt;You can watch a recording of the event on YouTube or in the window below.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Our discussion with Ed Zitron. Click here for transcript.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;h2&gt;“A 50 billion-dollar industry pretending to be a trillion-dollar one”&lt;/h2&gt;
&lt;p&gt;I started by asking Zitron the most direct question I could: “Why are you so mad about AI?” His answer got right to the heart of his critique: the disconnect between AI’s actual capabilities and how it’s being sold. “Because everybody’s acting like it’s something it isn’t,” Zitron said. “They’re acting like it’s this panacea that will be the future of software growth, the future of hardware growth, the future of compute.”&lt;/p&gt;
&lt;p&gt;In one of his newsletters, Zitron describes the generative AI market as “a 50 billion dollar revenue industry masquerading as a one trillion-dollar one.” He pointed to OpenAI’s financial burn rate (losing an estimated $9.7 billion in the first half of 2025 alone) as evidence that the economics don’t work, coupled with a heavy dose of pessimism about AI in general.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2108866 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="683" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2212801534.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Donald Trump listens as Nvidia CEO Jensen Huang speaks at the White House during an event on “Investing in America” on April 30, 2025, in Washington, DC.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Andrew Harnik / Staff | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;“The models just do not have the efficacy,” Zitron said during our conversation. “AI agents is one of the most egregious lies the tech industry has ever told. Autonomous agents don’t exist.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;He contrasted the relatively small revenue generated by AI companies with the massive capital expenditures flowing into the sector. Even major cloud providers and chip makers are showing strain. Oracle reportedly lost $100 million in three months after installing Nvidia’s new Blackwell GPUs, which Zitron noted are “extremely power-hungry and expensive to run.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Finding utility despite the hype&lt;/h2&gt;
&lt;p&gt;I pushed back against some of Zitron’s broader dismissals of AI by sharing my own experience. I use AI chatbots frequently for brainstorming useful ideas and helping me see them from different angles. “I find I use AI models as sort of knowledge translators and framework translators,” I explained.&lt;/p&gt;
&lt;p&gt;After experiencing brain fog from repeated bouts of COVID over the years, I’ve also found tools like ChatGPT and Claude especially helpful for memory augmentation that pierces through brain fog: describing something in a roundabout, fuzzy way and quickly getting an answer I can then verify. Along these lines, I’ve previously written about how people in a UK study found AI assistants useful accessibility tools.&lt;/p&gt;
&lt;p&gt;Zitron acknowledged this could be useful for me personally but declined to draw any larger conclusions from my one data point. “I understand how that might be helpful; that’s cool,” he said. “I’m glad that that helps you in that way; it’s not a trillion-dollar use case.”&lt;/p&gt;
&lt;p&gt;He also shared his own attempts at using AI tools, including experimenting with Claude Code despite not being a coder himself.&lt;/p&gt;
&lt;p&gt;“If I liked [AI] somehow, it would be actually a more interesting story because I’d be talking about something I liked that was also onerously expensive,” Zitron explained. “But it doesn’t even do that, and it’s actually one of my core frustrations, it’s like this massive over-promise thing. I’m an early adopter guy. I will buy early crap all the time. I bought an Apple Vision Pro, like, what more do you say there? I’m ready to accept issues, but AI is all issues, it’s all filler, no killer; it’s very strange.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Zitron and I agree that current AI assistants are being marketed beyond their actual capabilities. As I often say, AI models are not people, and they are not good factual references. As such, they cannot replace human decision-making and cannot wholesale replace human intellectual labor (at the moment). Instead, I see AI models as augmentations of human capability: as tools rather than autonomous entities.&lt;/p&gt;
&lt;h2&gt;Computing costs: History versus reality&lt;/h2&gt;
&lt;p&gt;Even though Zitron and I found some common ground about AI hype, I expressed a belief that criticism over the cost and power requirements of operating AI models will eventually not become an issue.&lt;/p&gt;
&lt;p&gt;I attempted to make that case by noting that computing costs historically trend downward over time, referencing the Air Force’s SAGE computer system from the 1950s: a four-story building that performed 75,000 operations per second while consuming two megawatts of power. Today, pocket-sized phones deliver millions of times more computing power in a way that would be impossible, power consumption-wise, in the 1950s.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-1821593 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="486" src="https://cdn.arstechnica.net/wp-content/uploads/2021/12/GettyImages-837581308-640x486.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The blockhouse for the Semi-Automatic Ground Environment at Stewart Air Force Base, Newburgh, New York.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Denver Post via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;“I think it will eventually work that way,” I said, suggesting that AI inference costs might follow similar patterns of improvement over years and that AI tools will eventually become commodity components of computer operating systems. Basically, even if AI models stay inefficient, AI models of a certain baseline usefulness and capability will still be cheaper to train and run in the future because the computing systems they run on will be faster, cheaper, and less power-hungry as well.&lt;/p&gt;
&lt;p&gt;Zitron pushed back on this optimism, saying that AI costs are currently moving in the wrong direction. “The costs are going up, unilaterally across the board,” he said. Even newer systems like Cerebras and Grok can generate results faster but not cheaper. He also questioned whether integrating AI into operating systems would prove useful even if the technology became profitable, since AI models struggle with deterministic commands and consistent behavior.&lt;/p&gt;
&lt;h2&gt;The power problem and circular investments&lt;/h2&gt;
&lt;p&gt;One of Zitron’s most pointed criticisms during the discussion centered on OpenAI’s infrastructure promises. The company has pledged to build data centers requiring 10 gigawatts of power capacity (equivalent to 10 nuclear power plants, I once pointed out) for its Stargate project in Abilene, Texas. According to Zitron’s research, the town currently has only 350 megawatts of generating capacity and a 200-megawatt substation.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“A gigawatt of power is a lot, and it’s not like &lt;em&gt;Red Alert 2&lt;/em&gt;,” Zitron said, referencing the real-time strategy game. “You don’t just build a power station and it happens. There are months of actual physics to make sure that it doesn’t kill everyone.”&lt;/p&gt;
&lt;p&gt;He believes many announced data centers will never be completed, calling the infrastructure promises “castles on sand” that nobody in the financial press seems willing to question directly.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2097196 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="An orange, cloudy sky backlights a set of electrical wires on large pylons, leading away from the cooling towers of a nuclear power plant." class="center large" height="683" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-1540176964-1024x683.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anton Petrus via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;After another technical blackout on my end, I came back online and asked Zitron to define the scope of the AI bubble. He says it has evolved from one bubble (foundation models) into two or three, now including AI compute companies like CoreWeave and the market’s obsession with Nvidia.&lt;/p&gt;
&lt;p&gt;Zitron highlighted what he sees as essentially circular investment schemes propping up the industry. He pointed to OpenAI’s $300 billion deal with Oracle and Nvidia’s relationship with CoreWeave as examples. “CoreWeave, they literally… They funded CoreWeave, became their biggest customer, then CoreWeave took that contract and those GPUs and used them as collateral to raise debt to buy more GPUs,” Zitron explained.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;When will the bubble pop?&lt;/h2&gt;
&lt;p&gt;Zitron predicted the bubble would burst within the next year and a half, though he acknowledged it could happen sooner. He expects a cascade of events rather than a single dramatic collapse: An AI startup will run out of money, triggering panic among other startups and their venture capital backers, creating a fire-sale environment that makes future fundraising impossible.&lt;/p&gt;
&lt;p&gt;“It’s not gonna be one Bear Stearns moment,” Zitron explained. “It’s gonna be a succession of events until the markets freak out.”&lt;/p&gt;
&lt;p&gt;The crux of the problem, according to Zitron, is Nvidia. The chip maker’s stock represents 7 to 8 percent of the S&amp;amp;P 500’s value, and the broader market has become dependent on Nvidia’s continued hyper growth. When Nvidia posted “only” 55 percent year-over-year growth in January, the market wobbled.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“Nvidia’s growth is why the bubble is inflated,” Zitron said. “If their growth goes down, the bubble will burst.”&lt;/p&gt;
&lt;p&gt;He also warned of broader consequences: “I think there’s a depression coming. I think once the markets work out that tech doesn’t grow forever, they’re gonna flush the toilet aggressively on Silicon Valley.” This connects to his larger thesis: that the tech industry has run out of genuine hyper-growth opportunities and is trying to manufacture one with AI.&lt;/p&gt;
&lt;p&gt;“Is there anything that would falsify your premise of this bubble and crash happening?” I asked. “What if you’re wrong?”&lt;/p&gt;
&lt;p&gt;“I’ve been answering ‘What if you’re wrong?’ for a year-and-a-half to two years, so I’m not bothered by that question, so the thing that would have to prove me right would’ve already needed to happen,” he said. Amid a longer exposition about Sam Altman, Zitron said, “The thing that would’ve had to happen with inference would’ve had to be… it would have to be hundredths of a cent per million tokens, they would have to be printing money, and then, it would have to be way more useful. It would have to have efficacy that it does not have, the hallucination problems… would have to be fixable, and on top of this, someone would have to fix agents.”&lt;/p&gt;
&lt;h2&gt;A positivity challenge&lt;/h2&gt;
&lt;p&gt;Near the end of our conversation, I wondered if I could flip the script, so to speak, and see if he could say something positive or optimistic, although I chose the most challenging subject possible for him. “What’s the best thing about Sam Altman,” I asked. “Can you say anything nice about him at all?”&lt;/p&gt;
&lt;p&gt;“I understand why you’re asking this,” Zitron started, “but I wanna be clear: Sam Altman is going to be the reason the markets take a crap. Sam Altman has lied to everyone. Sam Altman has been lying forever.” He continued, “Like the Pied Piper, he’s led the markets into an abyss, and yes, people should have known better, but I hope at the end of this, Sam Altman is seen for what he is, which is a con artist and a very successful one.”&lt;/p&gt;
&lt;p&gt;Then he added, “You know what? I’ll say something nice about him, he’s really good at making people say, ‘Yes.'”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/ars-live-recap-is-the-ai-bubble-about-to-pop-ed-zitron-weighs-in/</guid><pubDate>Thu, 16 Oct 2025 20:25:08 +0000</pubDate></item><item><title>[NEW] Researchers find adding this one simple sentence to prompts makes AI models way more creative (AI | VentureBeat)</title><link>https://venturebeat.com/ai/researchers-find-adding-this-one-simple-sentence-to-prompts-makes-ai-models</link><description>[unable to retrieve full-text content]&lt;p&gt;One of the coolest things about generative AI models — both large language models (LLMs) and diffusion-based image generators — is that they are &amp;quot;non-deterministic.&amp;quot; That is, despite their reputation among some critics as being &amp;quot;fancy autocorrect,&amp;quot; generative AI models actually generate their outputs by choosing from a distribution of the most probable next tokens (units of information) to fill out their response.&lt;/p&gt;&lt;p&gt;Asking an LLM: &amp;quot;What is the capital of France?&amp;quot; will have it sample its probability distribution for France, capitals, cities, etc. to arrive at the answer &amp;quot;Paris.&amp;quot; But that answer could come in the format of &amp;quot;The capital of France is Paris,&amp;quot; or simply &amp;quot;Paris&amp;quot; or &amp;quot;Paris, though it was Versailles at one point.&amp;quot; &lt;/p&gt;&lt;p&gt;Still, those of us that use these models frequently day-to-day will note that sometimes, their answers can feel annoyingly repetitive or similar. A common joke about coffee is recycled across generations of queries. Story prompts generate similar arcs. Even tasks that should yield many plausible answers—like naming U.S. states—tend to collapse into only a few. This phenomenon, known as mode collapse, arises during post-training alignment and limits the usefulness of otherwise powerful models.&lt;/p&gt;&lt;p&gt;Especially when using LLMs to generate new creative works in writing, communications, strategy, or illustrations, we actually want their outputs to be&lt;i&gt; even more varied than they already are. &lt;/i&gt;&lt;/p&gt;&lt;p&gt;Now a &lt;a href="https://www.verbalized-sampling.com/"&gt;team of researchers at Northeastern University, Stanford University and West Virginia University&lt;/a&gt; have come up with an ingenuously simple method to get language and image models to generate a wider variety of responses to nearly any user prompt by &lt;b&gt;adding a single, simple sentence: &amp;quot;Generate 5 responses with their corresponding probabilities, sampled from the full distribution.&amp;quot;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;The method, called &lt;i&gt;Verbalized Sampling&lt;/i&gt; (VS), helps models like GPT-4, Claude, and Gemini produce more diverse and human-like outputs—without retraining or access to internal parameters. It is described in a &lt;a href="https://arxiv.org/abs/2510.01171"&gt;paper&lt;/a&gt; published on the open access journal arxiv.org online in early October 2025.&lt;/p&gt;&lt;p&gt;When prompted in this way, the model no longer defaults to its safest, most typical output. Instead, it verbalizes its internal distribution over potential completions and samples across a wider spectrum of possibilities. This one-line change leads to substantial gains in output diversity across multiple domains.&lt;/p&gt;&lt;p&gt;As Weiyan Shi, an assistant professor at Northeastern University and co-author of the paper, &lt;a href="https://x.com/shi_weiyan/status/1978511066586878305"&gt;wrote on X&lt;/a&gt;: &amp;quot;LLMs&amp;#x27; potentials are not fully unlocked yet! As shown in our paper, prompt optimization can be guided by thinking about how LLMs are trained and aligned, and can be proved theoretically.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why Models Collapse—and How VS Reverses It&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;According to the research team, the root cause of mode collapse lies not just in algorithms like reinforcement learning from human feedback (RLHF), but in the structure of human preferences. People tend to rate more familiar or typical answers as better, which nudges LLMs toward “safe” choices over diverse ones during fine-tuning.&lt;/p&gt;&lt;p&gt;However, this bias doesn’t erase the model’s underlying knowledge—it just suppresses it. VS works by bypassing this suppression. Instead of asking for the single most likely output, it invites the model to reveal a set of plausible responses and their relative probabilities. This distribution-level prompting restores access to the richer diversity present in the base pretraining model.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Real-World Performance Across Tasks&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The research team tested Verbalized Sampling across several common use cases:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Creative Writing&lt;/b&gt;: In story generation, VS increased diversity scores by up to 2.1× compared to standard prompting, while maintaining quality. One story prompt—“Without a goodbye”—produced formulaic breakup scenes under direct prompting, but yielded narratives involving cosmic events, silent emails, and music stopping mid-dance when prompted via VS.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dialogue Simulation&lt;/b&gt;: In persuasive dialogue tasks, VS enabled models to simulate human-like patterns, such as hesitation, resistance, and changes of mind. Donation behavior distributions under VS better aligned with real human data compared to baseline methods.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Open-ended QA&lt;/b&gt;: When asked to enumerate valid answers (e.g., naming U.S. states), models using VS generated responses that more closely matched the diversity of real-world data. They covered a broader set of answers without sacrificing factual accuracy.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Synthetic Data Generation&lt;/b&gt;: When used to generate math problems for model training, VS created more varied datasets. These, in turn, improved downstream performance in competitive math benchmarks, outperforming synthetic data generated via direct prompting.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;Tunable Diversity and Better Use of Larger Models&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;A notable advantage of VS is its &lt;i&gt;tunability&lt;/i&gt;. Users can set a probability threshold in the prompt to sample from lower-probability “tails” of the model’s distribution. Lower thresholds correspond to higher diversity. This tuning can be done via prompt text alone, without changing any decoding settings like temperature or top-p.&lt;/p&gt;&lt;p&gt;In one test using the Gemini-2.5-Flash model, diversity in story writing increased steadily as the probability threshold dropped from 1 to 0.001. The chart accompanying the study showed VS outperforming both direct and sequence-based prompting across all thresholds.&lt;/p&gt;&lt;p&gt;Interestingly, the method scales well with model size. Larger models like GPT-4.1 and Claude-4 showed even greater gains from VS compared to smaller ones. While smaller models benefitted, the improvement in diversity was roughly 1.5–2× stronger in larger counterparts—suggesting VS helps unlock more of the latent capabilities in advanced models.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deployment and Availability&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Verbalized Sampling method is available now as a Python package:&lt;/p&gt;&lt;p&gt;&lt;code&gt;pip install verbalized-sampling&lt;/code&gt;&lt;/p&gt;&lt;p&gt;The package includes integration with LangChain and supports a simple interface for sampling from the verbalized distribution. Users can also adjust parameters like &lt;code&gt;k&lt;/code&gt; (number of responses), thresholds, and temperature to suit their applications. &lt;/p&gt;&lt;p&gt;A live Colab notebook and documentation are available under&lt;a href="https://github.com/CHATS-lab/verbalized-sampling"&gt; an enterprise friendly Apache 2.0 license &lt;/a&gt;on GitHub at: &lt;a href="https://github.com/CHATS-lab/verbalized-sampling"&gt;https://github.com/CHATS-lab/verbalized-sampling&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Practical Tips and Common Issues&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While the method works across all major LLMs, some users may initially encounter refusals or errors. &lt;/p&gt;&lt;p&gt;In these cases, the authors suggest using the system prompt version of the template or referring to alternative formats listed on the GitHub page. &lt;/p&gt;&lt;p&gt;Some models &lt;a href="https://x.com/dch/status/1978480024866165003"&gt;interpret complex instructions as jailbreak attempts&lt;/a&gt; and refuse to comply unless the structure is clearer.&lt;/p&gt;&lt;p&gt;For example, prompting via a system-level instruction like this improves reliability:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;i&gt;You are a helpful assistant. For each query, generate five responses within separate tags, each with a probability below 0.10.&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;This small change typically resolves any issues.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Lightweight Fix for a Big Problem&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Verbalized Sampling represents a practical, inference-time fix to a deep limitation in how modern language models behave. It doesn’t require model retraining or internal access. It is not dependent on any one model family. And it improves not only the diversity of outputs, but their quality—as judged by both human evaluation and benchmark scores.&lt;/p&gt;&lt;p&gt;With growing interest in tools that enhance model creativity, VS is likely to see rapid adoption in domains like writing, design, simulation, education, and synthetic data generation.&lt;/p&gt;&lt;p&gt;For users and developers frustrated by the sameness of LLM responses, the fix may be as simple as changing the question.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;One of the coolest things about generative AI models — both large language models (LLMs) and diffusion-based image generators — is that they are &amp;quot;non-deterministic.&amp;quot; That is, despite their reputation among some critics as being &amp;quot;fancy autocorrect,&amp;quot; generative AI models actually generate their outputs by choosing from a distribution of the most probable next tokens (units of information) to fill out their response.&lt;/p&gt;&lt;p&gt;Asking an LLM: &amp;quot;What is the capital of France?&amp;quot; will have it sample its probability distribution for France, capitals, cities, etc. to arrive at the answer &amp;quot;Paris.&amp;quot; But that answer could come in the format of &amp;quot;The capital of France is Paris,&amp;quot; or simply &amp;quot;Paris&amp;quot; or &amp;quot;Paris, though it was Versailles at one point.&amp;quot; &lt;/p&gt;&lt;p&gt;Still, those of us that use these models frequently day-to-day will note that sometimes, their answers can feel annoyingly repetitive or similar. A common joke about coffee is recycled across generations of queries. Story prompts generate similar arcs. Even tasks that should yield many plausible answers—like naming U.S. states—tend to collapse into only a few. This phenomenon, known as mode collapse, arises during post-training alignment and limits the usefulness of otherwise powerful models.&lt;/p&gt;&lt;p&gt;Especially when using LLMs to generate new creative works in writing, communications, strategy, or illustrations, we actually want their outputs to be&lt;i&gt; even more varied than they already are. &lt;/i&gt;&lt;/p&gt;&lt;p&gt;Now a &lt;a href="https://www.verbalized-sampling.com/"&gt;team of researchers at Northeastern University, Stanford University and West Virginia University&lt;/a&gt; have come up with an ingenuously simple method to get language and image models to generate a wider variety of responses to nearly any user prompt by &lt;b&gt;adding a single, simple sentence: &amp;quot;Generate 5 responses with their corresponding probabilities, sampled from the full distribution.&amp;quot;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;The method, called &lt;i&gt;Verbalized Sampling&lt;/i&gt; (VS), helps models like GPT-4, Claude, and Gemini produce more diverse and human-like outputs—without retraining or access to internal parameters. It is described in a &lt;a href="https://arxiv.org/abs/2510.01171"&gt;paper&lt;/a&gt; published on the open access journal arxiv.org online in early October 2025.&lt;/p&gt;&lt;p&gt;When prompted in this way, the model no longer defaults to its safest, most typical output. Instead, it verbalizes its internal distribution over potential completions and samples across a wider spectrum of possibilities. This one-line change leads to substantial gains in output diversity across multiple domains.&lt;/p&gt;&lt;p&gt;As Weiyan Shi, an assistant professor at Northeastern University and co-author of the paper, &lt;a href="https://x.com/shi_weiyan/status/1978511066586878305"&gt;wrote on X&lt;/a&gt;: &amp;quot;LLMs&amp;#x27; potentials are not fully unlocked yet! As shown in our paper, prompt optimization can be guided by thinking about how LLMs are trained and aligned, and can be proved theoretically.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why Models Collapse—and How VS Reverses It&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;According to the research team, the root cause of mode collapse lies not just in algorithms like reinforcement learning from human feedback (RLHF), but in the structure of human preferences. People tend to rate more familiar or typical answers as better, which nudges LLMs toward “safe” choices over diverse ones during fine-tuning.&lt;/p&gt;&lt;p&gt;However, this bias doesn’t erase the model’s underlying knowledge—it just suppresses it. VS works by bypassing this suppression. Instead of asking for the single most likely output, it invites the model to reveal a set of plausible responses and their relative probabilities. This distribution-level prompting restores access to the richer diversity present in the base pretraining model.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Real-World Performance Across Tasks&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The research team tested Verbalized Sampling across several common use cases:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Creative Writing&lt;/b&gt;: In story generation, VS increased diversity scores by up to 2.1× compared to standard prompting, while maintaining quality. One story prompt—“Without a goodbye”—produced formulaic breakup scenes under direct prompting, but yielded narratives involving cosmic events, silent emails, and music stopping mid-dance when prompted via VS.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dialogue Simulation&lt;/b&gt;: In persuasive dialogue tasks, VS enabled models to simulate human-like patterns, such as hesitation, resistance, and changes of mind. Donation behavior distributions under VS better aligned with real human data compared to baseline methods.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Open-ended QA&lt;/b&gt;: When asked to enumerate valid answers (e.g., naming U.S. states), models using VS generated responses that more closely matched the diversity of real-world data. They covered a broader set of answers without sacrificing factual accuracy.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Synthetic Data Generation&lt;/b&gt;: When used to generate math problems for model training, VS created more varied datasets. These, in turn, improved downstream performance in competitive math benchmarks, outperforming synthetic data generated via direct prompting.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;Tunable Diversity and Better Use of Larger Models&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;A notable advantage of VS is its &lt;i&gt;tunability&lt;/i&gt;. Users can set a probability threshold in the prompt to sample from lower-probability “tails” of the model’s distribution. Lower thresholds correspond to higher diversity. This tuning can be done via prompt text alone, without changing any decoding settings like temperature or top-p.&lt;/p&gt;&lt;p&gt;In one test using the Gemini-2.5-Flash model, diversity in story writing increased steadily as the probability threshold dropped from 1 to 0.001. The chart accompanying the study showed VS outperforming both direct and sequence-based prompting across all thresholds.&lt;/p&gt;&lt;p&gt;Interestingly, the method scales well with model size. Larger models like GPT-4.1 and Claude-4 showed even greater gains from VS compared to smaller ones. While smaller models benefitted, the improvement in diversity was roughly 1.5–2× stronger in larger counterparts—suggesting VS helps unlock more of the latent capabilities in advanced models.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deployment and Availability&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Verbalized Sampling method is available now as a Python package:&lt;/p&gt;&lt;p&gt;&lt;code&gt;pip install verbalized-sampling&lt;/code&gt;&lt;/p&gt;&lt;p&gt;The package includes integration with LangChain and supports a simple interface for sampling from the verbalized distribution. Users can also adjust parameters like &lt;code&gt;k&lt;/code&gt; (number of responses), thresholds, and temperature to suit their applications. &lt;/p&gt;&lt;p&gt;A live Colab notebook and documentation are available under&lt;a href="https://github.com/CHATS-lab/verbalized-sampling"&gt; an enterprise friendly Apache 2.0 license &lt;/a&gt;on GitHub at: &lt;a href="https://github.com/CHATS-lab/verbalized-sampling"&gt;https://github.com/CHATS-lab/verbalized-sampling&lt;/a&gt;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Practical Tips and Common Issues&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While the method works across all major LLMs, some users may initially encounter refusals or errors. &lt;/p&gt;&lt;p&gt;In these cases, the authors suggest using the system prompt version of the template or referring to alternative formats listed on the GitHub page. &lt;/p&gt;&lt;p&gt;Some models &lt;a href="https://x.com/dch/status/1978480024866165003"&gt;interpret complex instructions as jailbreak attempts&lt;/a&gt; and refuse to comply unless the structure is clearer.&lt;/p&gt;&lt;p&gt;For example, prompting via a system-level instruction like this improves reliability:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;i&gt;You are a helpful assistant. For each query, generate five responses within separate tags, each with a probability below 0.10.&lt;/i&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;This small change typically resolves any issues.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Lightweight Fix for a Big Problem&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Verbalized Sampling represents a practical, inference-time fix to a deep limitation in how modern language models behave. It doesn’t require model retraining or internal access. It is not dependent on any one model family. And it improves not only the diversity of outputs, but their quality—as judged by both human evaluation and benchmark scores.&lt;/p&gt;&lt;p&gt;With growing interest in tools that enhance model creativity, VS is likely to see rapid adoption in domains like writing, design, simulation, education, and synthetic data generation.&lt;/p&gt;&lt;p&gt;For users and developers frustrated by the sameness of LLM responses, the fix may be as simple as changing the question.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/researchers-find-adding-this-one-simple-sentence-to-prompts-makes-ai-models</guid><pubDate>Fri, 17 Oct 2025 02:40:00 +0000</pubDate></item><item><title>[NEW] OpenAI pauses Sora video generations of Martin Luther King Jr. (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/16/openai-pauses-sora-video-generations-of-martin-luther-king-jr/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/07/gettyimages-80751598-e1484590456411.jpg?w=967" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Thursday it paused the ability for users to generate videos resembling the late civil rights activist Martin Luther King Jr. using its AI video model, Sora. The company says it’s adding this safeguard at the request of Dr. King’s estate after some Sora users generated “disrespectful depictions” of his image.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“While there are strong free speech interests in depicting historical figures, OpenAI believes public figures and their families should ultimately have control over how their likeness is used,” OpenAI said in a post on X from its official newsroom account. “Authorized representatives or estate owners can request that their likeness not be used in Sora cameos.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Statement from OpenAI and King Estate, Inc.&lt;/p&gt;&lt;p&gt;The Estate of Martin Luther King, Jr., Inc. (King, Inc.) and OpenAI have worked together to address how Dr. Martin Luther King Jr.’s likeness is represented in Sora generations. Some users generated disrespectful depictions of Dr.…&lt;/p&gt;— OpenAI Newsroom (@OpenAINewsroom) October 17, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The restriction comes just a few weeks after OpenAI launched its social video platform, Sora, which allows users to create realistic AI-generated videos resembling historical figures, their friends, and users who elect to have their likeness recreated on the platform. The launch has stirred fervent public debate around the dangers of AI-generated videos, and how platforms should implement guardrails around the technology.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Dr. Bernice King, Dr. King’s daughter, posted on Instagram last week asking people to stop sending her AI videos resembling her father. She joined Robin Williams’ daughter, who also asked Sora users to stop generating AI videos of her father.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Washington Post reported earlier this week that Sora users had created AI-generated videos of Dr. King making monkey noises and wrestling with another civil rights icon, Malcolm X. Scrolling through OpenAI’s Sora app, it’s easy to find crude videos resembling other historical figures, including artist Bob Ross, singer Whitney Houston, and former President John F. Kennedy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The licensor of Dr. King’s estate did not immediately respond to TechCrunch’s request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond how Sora represents humans, the launch has also raised a flurry of questions around how social media platforms should handle AI videos of copyrighted works. The Sora app is also full of videos depicting cartoons like SpongeBob, South Park, and Pokémon.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has added other restrictions to Sora in weeks since its launch. Earlier in October, the company said it planned to give copyright holders more granular control over the types of AI videos that can be generated with their likeness. That may have been a response to Hollywood’s initial reaction to Sora, which was not great.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As OpenAI adds restrictions to Sora, the company seems to be taking a more hands-off approach to moderating content in ChatGPT. OpenAI announced this week that it would allow adult users to have “erotic” chats with ChatGPT in the coming months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Sora, it seems that OpenAI is grappling with the concerns that come along with AI video generation. Some OpenAI researchers publicly wrestled with questions about the company’s first AI-powered social media platform in the days after its launch, and how such a product fits into the nonprofit’s mission. OpenAI CEO Sam Altman said the company felt “trepidation” about Sora on launch day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nick Turley, the head of ChatGPT, told me earlier this month that the best way to teach the world about a new technology is putting it out in the world. He said that’s what the company learned with ChatGPT, and that’s what OpenAI is finding with Sora, too. It seems the company is learning something about how to distribute this technology, as well.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/07/gettyimages-80751598-e1484590456411.jpg?w=967" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Thursday it paused the ability for users to generate videos resembling the late civil rights activist Martin Luther King Jr. using its AI video model, Sora. The company says it’s adding this safeguard at the request of Dr. King’s estate after some Sora users generated “disrespectful depictions” of his image.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“While there are strong free speech interests in depicting historical figures, OpenAI believes public figures and their families should ultimately have control over how their likeness is used,” OpenAI said in a post on X from its official newsroom account. “Authorized representatives or estate owners can request that their likeness not be used in Sora cameos.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Statement from OpenAI and King Estate, Inc.&lt;/p&gt;&lt;p&gt;The Estate of Martin Luther King, Jr., Inc. (King, Inc.) and OpenAI have worked together to address how Dr. Martin Luther King Jr.’s likeness is represented in Sora generations. Some users generated disrespectful depictions of Dr.…&lt;/p&gt;— OpenAI Newsroom (@OpenAINewsroom) October 17, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The restriction comes just a few weeks after OpenAI launched its social video platform, Sora, which allows users to create realistic AI-generated videos resembling historical figures, their friends, and users who elect to have their likeness recreated on the platform. The launch has stirred fervent public debate around the dangers of AI-generated videos, and how platforms should implement guardrails around the technology.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Dr. Bernice King, Dr. King’s daughter, posted on Instagram last week asking people to stop sending her AI videos resembling her father. She joined Robin Williams’ daughter, who also asked Sora users to stop generating AI videos of her father.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Washington Post reported earlier this week that Sora users had created AI-generated videos of Dr. King making monkey noises and wrestling with another civil rights icon, Malcolm X. Scrolling through OpenAI’s Sora app, it’s easy to find crude videos resembling other historical figures, including artist Bob Ross, singer Whitney Houston, and former President John F. Kennedy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The licensor of Dr. King’s estate did not immediately respond to TechCrunch’s request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond how Sora represents humans, the launch has also raised a flurry of questions around how social media platforms should handle AI videos of copyrighted works. The Sora app is also full of videos depicting cartoons like SpongeBob, South Park, and Pokémon.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has added other restrictions to Sora in weeks since its launch. Earlier in October, the company said it planned to give copyright holders more granular control over the types of AI videos that can be generated with their likeness. That may have been a response to Hollywood’s initial reaction to Sora, which was not great.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As OpenAI adds restrictions to Sora, the company seems to be taking a more hands-off approach to moderating content in ChatGPT. OpenAI announced this week that it would allow adult users to have “erotic” chats with ChatGPT in the coming months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Sora, it seems that OpenAI is grappling with the concerns that come along with AI video generation. Some OpenAI researchers publicly wrestled with questions about the company’s first AI-powered social media platform in the days after its launch, and how such a product fits into the nonprofit’s mission. OpenAI CEO Sam Altman said the company felt “trepidation” about Sora on launch day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nick Turley, the head of ChatGPT, told me earlier this month that the best way to teach the world about a new technology is putting it out in the world. He said that’s what the company learned with ChatGPT, and that’s what OpenAI is finding with Sora, too. It seems the company is learning something about how to distribute this technology, as well.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/16/openai-pauses-sora-video-generations-of-martin-luther-king-jr/</guid><pubDate>Fri, 17 Oct 2025 03:21:06 +0000</pubDate></item></channel></rss>