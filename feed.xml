<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 23 Jul 2025 01:57:56 +0000</lastBuildDate><item><title>21-year-old MIT dropouts raise $32M at $300M valuation led by Insight (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/21-year-old-mit-dropouts-raise-32m-at-300m-valuation-led-by-insight/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Karun Kaushik and Selin Kocalar weren’t planning to raise a Series A so soon. Their AI compliance startup, Delve, which announced a $3 million seed round in January, was growing fast and signing customers at a steady clip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then, inbound interest started rolling in, COO Kocalar told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Delve, which automates regulatory compliance with AI agents, ended up fielding multiple term sheets, eventually closing a $32 million Series A at a $300 million valuation. The round was led by Insight Partners, which took up most of the round, with participation from CISOs at Fortune 500 companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Insight has been “amazing to work with, and we felt they were the right long-term partner for us,” said the COO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Delve’s new valuation represents a roughly 10x jump from its previous round. Similarly, its customer base has grown from the 100 companies it reported back in January to over 500, many of them fast-growing AI startups like recently minted AI unicorn Lovable, Bland, and Wispr Flow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And while the 2-year-old company, made up of AI researchers from MIT, Stanford, and Berkeley, has seemingly struck gold by using AI to eliminate hundreds of hours of manual processes, its story began much differently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaushik and Kocalar met as classmates during their freshman year at MIT. Both had deep interests in AI and health tech. Kaushik had already scaled a COVID diagnostic system to thousands of users during the pandemic.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In 2023, they began working on an AI-powered medical scribe to help doctors handle patient documentation. However, handling sensitive healthcare information meant they quickly encountered the costly and time-consuming world of HIPAA compliance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead of continuing with the medical scribe, they started building tools to help other companies get HIPAA-compliant faster and more affordably. That pivot got the team into Y Combinator last year and helped them raise their seed round from General Catalyst, FundersClub, Soma Capital, and others. The founders dropped out during their sophomore year in 2023.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What started with HIPAA quickly expanded. “As our customer base grew, they started asking for support with other frameworks: SOC 2, PCI, GDPR, ISO, basically the whole alphabet soup of compliance,” Kocalar narrated.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Delve" class="wp-image-3029945" height="467" src="https://techcrunch.com/wp-content/uploads/2025/07/IMG_1546.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;L-R: Selin Kocalar (COO) and Karun Kaushik (CEO) &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Delve&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Compliance paperwork can be necessary in everything from launching products to closing enterprise deals. But instead of driving growth, its manual work can become a bottleneck.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Compliance frameworks are standardized. Businesses aren’t,” says CEO Kaushik. “That mismatch is why traditional software breaks down and teams fall back to duct-taped workflows across email, Slack, and shared drives.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Delve replaces that busywork with AI agents that run in the background (after integrating with customers’ tools) like internal team members. These agents collect evidence, write reports, update audit logs, and track configuration changes across fragmented systems, automating compliance workflows in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kocalar says compliance is just the wedge into broader back-office operations. Long-term, the AI startup wants to automate a billion hours of other work — eventually expanding into adjacent areas like cybersecurity, risk, and internal governance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Insight Partners’ interest reflects this roadmap. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Since compliance touches every part of how a business runs, from scaling operations to closing deals to building customer trust, modernizing this function will modernize the entire organization,” said Praveen Akkiraju, managing director at Insight. “That’s what makes Delve’s approach so important.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the startup won’t be without competition. Several AI companies are emerging with agents to automate business workflows. In addition, larger AI labs like OpenAI are releasing general-purpose agents capable of performing complex tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Kocalar says these developments are a validation, not a threat, to Delve’s business. She points to the company’s domain depth in contrast to more general-purpose agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re positioning ourselves to improve as AI advances and labs roll out more sophisticated agentic technologies. But what truly sets us apart is the deep, domain-specific knowledge we’re building into the platform,” she said. “Compliance is always shifting as new regulations emerge and existing ones evolve, with companies interpreting them in different ways. That’s where Delve stands out.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Karun Kaushik and Selin Kocalar weren’t planning to raise a Series A so soon. Their AI compliance startup, Delve, which announced a $3 million seed round in January, was growing fast and signing customers at a steady clip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then, inbound interest started rolling in, COO Kocalar told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Delve, which automates regulatory compliance with AI agents, ended up fielding multiple term sheets, eventually closing a $32 million Series A at a $300 million valuation. The round was led by Insight Partners, which took up most of the round, with participation from CISOs at Fortune 500 companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Insight has been “amazing to work with, and we felt they were the right long-term partner for us,” said the COO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Delve’s new valuation represents a roughly 10x jump from its previous round. Similarly, its customer base has grown from the 100 companies it reported back in January to over 500, many of them fast-growing AI startups like recently minted AI unicorn Lovable, Bland, and Wispr Flow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And while the 2-year-old company, made up of AI researchers from MIT, Stanford, and Berkeley, has seemingly struck gold by using AI to eliminate hundreds of hours of manual processes, its story began much differently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kaushik and Kocalar met as classmates during their freshman year at MIT. Both had deep interests in AI and health tech. Kaushik had already scaled a COVID diagnostic system to thousands of users during the pandemic.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In 2023, they began working on an AI-powered medical scribe to help doctors handle patient documentation. However, handling sensitive healthcare information meant they quickly encountered the costly and time-consuming world of HIPAA compliance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead of continuing with the medical scribe, they started building tools to help other companies get HIPAA-compliant faster and more affordably. That pivot got the team into Y Combinator last year and helped them raise their seed round from General Catalyst, FundersClub, Soma Capital, and others. The founders dropped out during their sophomore year in 2023.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What started with HIPAA quickly expanded. “As our customer base grew, they started asking for support with other frameworks: SOC 2, PCI, GDPR, ISO, basically the whole alphabet soup of compliance,” Kocalar narrated.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Delve" class="wp-image-3029945" height="467" src="https://techcrunch.com/wp-content/uploads/2025/07/IMG_1546.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;L-R: Selin Kocalar (COO) and Karun Kaushik (CEO) &lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Delve&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Compliance paperwork can be necessary in everything from launching products to closing enterprise deals. But instead of driving growth, its manual work can become a bottleneck.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Compliance frameworks are standardized. Businesses aren’t,” says CEO Kaushik. “That mismatch is why traditional software breaks down and teams fall back to duct-taped workflows across email, Slack, and shared drives.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Delve replaces that busywork with AI agents that run in the background (after integrating with customers’ tools) like internal team members. These agents collect evidence, write reports, update audit logs, and track configuration changes across fragmented systems, automating compliance workflows in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kocalar says compliance is just the wedge into broader back-office operations. Long-term, the AI startup wants to automate a billion hours of other work — eventually expanding into adjacent areas like cybersecurity, risk, and internal governance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Insight Partners’ interest reflects this roadmap. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Since compliance touches every part of how a business runs, from scaling operations to closing deals to building customer trust, modernizing this function will modernize the entire organization,” said Praveen Akkiraju, managing director at Insight. “That’s what makes Delve’s approach so important.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the startup won’t be without competition. Several AI companies are emerging with agents to automate business workflows. In addition, larger AI labs like OpenAI are releasing general-purpose agents capable of performing complex tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Kocalar says these developments are a validation, not a threat, to Delve’s business. She points to the company’s domain depth in contrast to more general-purpose agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re positioning ourselves to improve as AI advances and labs roll out more sophisticated agentic technologies. But what truly sets us apart is the deep, domain-specific knowledge we’re building into the platform,” she said. “Compliance is always shifting as new regulations emerge and existing ones evolve, with companies interpreting them in different ways. That’s where Delve stands out.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/21-year-old-mit-dropouts-raise-32m-at-300m-valuation-led-by-insight/</guid><pubDate>Tue, 22 Jul 2025 13:59:35 +0000</pubDate></item><item><title>School of Architecture and Planning recognizes faculty with academic promotions in 2025 (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/school-architecture-planning-recognizes-faculty-academic-promotions-0722</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-sap-faculty-promotions.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Seven faculty in the MIT School of Architecture and Planning (SA+P) have been honored for their contributions through promotions, effective July 1. Three faculty promotions are in the Department of Architecture; three are in the Department of Urban Studies and Planning; and one is in the Program in Media Arts and Sciences.&lt;/p&gt;&lt;p&gt;“Whether architects, urbanists, computer scientists, or nanotechnologists, they represent our school at its best, in its breadth of inquiry and mission to improve the relationship between human beings and their environments,” says SA+P Dean Hashim Sarkis.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Department of Architecture&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Marcelo Coelho&lt;/strong&gt; has been promoted to associate professor of the practice. Coelho is the director of the Design Intelligence Lab, which explores the intersection of human and machine intelligence across design, AI, and fabrication. His work ranges from light-based installations to physical computing. Recognition for his work includes two Prix Ars Electronica awards and &lt;em&gt;Fast Company&lt;/em&gt;’s Innovation by Design Award. Coelho’s experimental approach redefines creative processes, transforming how we imagine and interact with intelligent systems. Coelho teaches courses that bring together industrial design, user experience, and artificial intelligence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Holly Samuelson&lt;/strong&gt; has been promoted to associate professor without tenure. Samuelson has co-authored over 40 peer-reviewed papers, winning a Best Paper award from the journal &lt;em&gt;Energy and Building.&lt;/em&gt; As a recognized expert in architectural technology, she has been featured in media outlets such as &lt;em&gt;The Washington Post&lt;/em&gt;, &lt;em&gt;The Boston Globe&lt;/em&gt;, the BBC, and &lt;em&gt;The Wall Street Journal&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Rafi Segal&lt;/strong&gt; has been promoted to full professor. An award-winning designer, Segal works across architectural and urban scales, with projects ranging from Villa 003 in the ORDOS 100 series to the Kitgum Peace Museum in Uganda, the Ashdod Museum of Art in Israel, and the winning design proposal for the National Library of Israel in Jerusalem. His current work includes planning a new communal neighborhood for an Israeli kibbutz and curating the first exhibition on Alfred Neumann’s 1960s architecture.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Department of Urban Studies and Planning (DUSP)&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Carlo Ratti&lt;/strong&gt; has been reappointed as professor of the practice. Ratti is the director of the Senseable City Lab and a founding partner of the international design office Carlo Ratti Associati. He has co-authored over 500 publications and holds several patents. His work has been exhibited globally, including at the Venice Biennale, the Museum of Modern Art in New York City, and the Design Museum in Barcelona. Two of his projects, the Digital Water Pavilion and the Copenhagen Wheel, were named among &lt;em&gt;TIME Magazine&lt;/em&gt;’s “Best Inventions of the Year.” He is the curator of the 2025 Venice Biennale’s 19th International Architecture Exhibition.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Albert Saiz&lt;/strong&gt; has been promoted to full professor. Saiz serves as the director of MIT’s Urban Economics Lab, which conducts research on real estate economics, urban economics, housing markets, local public finance, zoning regulations, global real estate, and demographic trends affecting urban and real estate development worldwide. He also contributes to the broader research community as a visiting scholar at the Federal Reserve Bank of Philadelphia, a research fellow at the Institute for the Analysis of Labor, and editor for the &lt;em&gt;Journal of Housing Economics&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Delia Wendel&lt;/strong&gt;&amp;nbsp;has been promoted to associate professor without tenure. Wendel’s research engages three main areas: forms of community repair after conflict and disaster, African urbanism, and spatial politics. Her interdisciplinary work draws together urban studies, critical peace studies, architectural history, cultural geography, and anthropology.&amp;nbsp;At MIT DUSP, she leads the Planning for Peace critical collective and oversees the Mellon Foundation and the MIT Center for Art, Science and Technology-funded research and exhibition project, Memory Atlas for Repair. She also serves as the managing editor of &lt;em&gt;Projections,&lt;/em&gt; the department’s annual peer-reviewed journal on critical issues in urban studies and planning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Program in Media Arts and Sciences&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Deblina Sarkar&lt;/strong&gt; has been promoted to associate professor without tenure. As the director of the Nano-Cybernetic Biotrek Lab at the MIT Media Lab, she merges nanoelectronics, physics, and biology to create groundbreaking technologies, from ultra-thin quantum transistors to the first antenna that operates inside living cells. Her interdisciplinary work has earned her major honors, including the National Institutes of Health Director’s New Innovator Award and the IEEE Early Career Award in Nanotechnology.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202507/mit-sap-faculty-promotions.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Seven faculty in the MIT School of Architecture and Planning (SA+P) have been honored for their contributions through promotions, effective July 1. Three faculty promotions are in the Department of Architecture; three are in the Department of Urban Studies and Planning; and one is in the Program in Media Arts and Sciences.&lt;/p&gt;&lt;p&gt;“Whether architects, urbanists, computer scientists, or nanotechnologists, they represent our school at its best, in its breadth of inquiry and mission to improve the relationship between human beings and their environments,” says SA+P Dean Hashim Sarkis.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Department of Architecture&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Marcelo Coelho&lt;/strong&gt; has been promoted to associate professor of the practice. Coelho is the director of the Design Intelligence Lab, which explores the intersection of human and machine intelligence across design, AI, and fabrication. His work ranges from light-based installations to physical computing. Recognition for his work includes two Prix Ars Electronica awards and &lt;em&gt;Fast Company&lt;/em&gt;’s Innovation by Design Award. Coelho’s experimental approach redefines creative processes, transforming how we imagine and interact with intelligent systems. Coelho teaches courses that bring together industrial design, user experience, and artificial intelligence.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Holly Samuelson&lt;/strong&gt; has been promoted to associate professor without tenure. Samuelson has co-authored over 40 peer-reviewed papers, winning a Best Paper award from the journal &lt;em&gt;Energy and Building.&lt;/em&gt; As a recognized expert in architectural technology, she has been featured in media outlets such as &lt;em&gt;The Washington Post&lt;/em&gt;, &lt;em&gt;The Boston Globe&lt;/em&gt;, the BBC, and &lt;em&gt;The Wall Street Journal&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Rafi Segal&lt;/strong&gt; has been promoted to full professor. An award-winning designer, Segal works across architectural and urban scales, with projects ranging from Villa 003 in the ORDOS 100 series to the Kitgum Peace Museum in Uganda, the Ashdod Museum of Art in Israel, and the winning design proposal for the National Library of Israel in Jerusalem. His current work includes planning a new communal neighborhood for an Israeli kibbutz and curating the first exhibition on Alfred Neumann’s 1960s architecture.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Department of Urban Studies and Planning (DUSP)&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Carlo Ratti&lt;/strong&gt; has been reappointed as professor of the practice. Ratti is the director of the Senseable City Lab and a founding partner of the international design office Carlo Ratti Associati. He has co-authored over 500 publications and holds several patents. His work has been exhibited globally, including at the Venice Biennale, the Museum of Modern Art in New York City, and the Design Museum in Barcelona. Two of his projects, the Digital Water Pavilion and the Copenhagen Wheel, were named among &lt;em&gt;TIME Magazine&lt;/em&gt;’s “Best Inventions of the Year.” He is the curator of the 2025 Venice Biennale’s 19th International Architecture Exhibition.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Albert Saiz&lt;/strong&gt; has been promoted to full professor. Saiz serves as the director of MIT’s Urban Economics Lab, which conducts research on real estate economics, urban economics, housing markets, local public finance, zoning regulations, global real estate, and demographic trends affecting urban and real estate development worldwide. He also contributes to the broader research community as a visiting scholar at the Federal Reserve Bank of Philadelphia, a research fellow at the Institute for the Analysis of Labor, and editor for the &lt;em&gt;Journal of Housing Economics&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Delia Wendel&lt;/strong&gt;&amp;nbsp;has been promoted to associate professor without tenure. Wendel’s research engages three main areas: forms of community repair after conflict and disaster, African urbanism, and spatial politics. Her interdisciplinary work draws together urban studies, critical peace studies, architectural history, cultural geography, and anthropology.&amp;nbsp;At MIT DUSP, she leads the Planning for Peace critical collective and oversees the Mellon Foundation and the MIT Center for Art, Science and Technology-funded research and exhibition project, Memory Atlas for Repair. She also serves as the managing editor of &lt;em&gt;Projections,&lt;/em&gt; the department’s annual peer-reviewed journal on critical issues in urban studies and planning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Program in Media Arts and Sciences&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Deblina Sarkar&lt;/strong&gt; has been promoted to associate professor without tenure. As the director of the Nano-Cybernetic Biotrek Lab at the MIT Media Lab, she merges nanoelectronics, physics, and biology to create groundbreaking technologies, from ultra-thin quantum transistors to the first antenna that operates inside living cells. Her interdisciplinary work has earned her major honors, including the National Institutes of Health Director’s New Innovator Award and the IEEE Early Career Award in Nanotechnology.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/school-architecture-planning-recognizes-faculty-academic-promotions-0722</guid><pubDate>Tue, 22 Jul 2025 14:00:00 +0000</pubDate></item><item><title>Onstage at TechCrunch Disrupt 2025: How AI is forcing late-stage startups to rewire GTM — or be left behind (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/on-stage-at-techcrunch-disrupt-2025-how-ai-is-forcing-late-stage-startups-to-rewire-gtm-or-be-left-behind/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI isn’t just disrupting products — it’s upending how companies sell, scale, and succeed. At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27-29 in San Francisco, we’re diving deep into how late-stage startups are navigating this shift. Catch this dynamic panel on the &lt;strong&gt;Going Public Stage&lt;/strong&gt;, where three seasoned leaders will explore how AI is transforming go-to-market (GTM) strategies from the inside out.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Nirav Tolia, Jane Alexander, Vanessa Larco" class="wp-image-3029389" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_Tolia-Alexander-Larco-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-attend-this-session"&gt;Why attend this session?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;If you’re scaling a startup and wondering how AI fits into your next phase of growth, this conversation is for you. &lt;strong&gt;Our panelists&lt;/strong&gt; will cut through the hype to reveal how AI is changing everything, from sales and marketing to customer success. You’ll get insight into what’s working, what’s not, and how to integrate AI into your GTM motion without derailing your focus or team.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-meet-the-speakers"&gt;Meet the speakers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Nirav Tolia&lt;/strong&gt; is the CEO, president, and co-founder of &lt;strong&gt;Nextdoor&lt;/strong&gt;, where he leads one of the most recognized community platforms in tech. Tolia previously co-founded and led Epinions.com, was COO at Shopping.com, and serves as the non-executive chair of Hedosophia.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Jane Alexander&lt;/strong&gt;, partner at &lt;strong&gt;CapitalG&lt;/strong&gt;, brings more than 15 years of experience building and scaling GTM teams. Before joining CapitalG, she was CMO at Carta and held leadership roles at Salesforce and RelateIQ.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Vanessa Larco&lt;/strong&gt; is the co-founder of &lt;strong&gt;Premise&lt;/strong&gt; and a former partner at NEA. With a background in product leadership and investing, Larco brings a sharp lens to how startups can grow efficiently — and stay focused — as AI reshapes product and market strategies.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-snag-your-july-discount-and-get-ready-for-disrupt-2025"&gt;Snag your July discount and get ready for Disrupt 2025&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Don’t miss this session — &lt;strong&gt;and more than 200 others&lt;/strong&gt; — at the tech epicenter where 10,000+ startup and VC leaders connect to discover the next wave of innovation. &lt;strong&gt;Buy your tickets now&lt;/strong&gt; and save up to $675 before July savings end.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI isn’t just disrupting products — it’s upending how companies sell, scale, and succeed. At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, happening October 27-29 in San Francisco, we’re diving deep into how late-stage startups are navigating this shift. Catch this dynamic panel on the &lt;strong&gt;Going Public Stage&lt;/strong&gt;, where three seasoned leaders will explore how AI is transforming go-to-market (GTM) strategies from the inside out.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Nirav Tolia, Jane Alexander, Vanessa Larco" class="wp-image-3029389" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_Tolia-Alexander-Larco-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-attend-this-session"&gt;Why attend this session?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;If you’re scaling a startup and wondering how AI fits into your next phase of growth, this conversation is for you. &lt;strong&gt;Our panelists&lt;/strong&gt; will cut through the hype to reveal how AI is changing everything, from sales and marketing to customer success. You’ll get insight into what’s working, what’s not, and how to integrate AI into your GTM motion without derailing your focus or team.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-meet-the-speakers"&gt;Meet the speakers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Nirav Tolia&lt;/strong&gt; is the CEO, president, and co-founder of &lt;strong&gt;Nextdoor&lt;/strong&gt;, where he leads one of the most recognized community platforms in tech. Tolia previously co-founded and led Epinions.com, was COO at Shopping.com, and serves as the non-executive chair of Hedosophia.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Jane Alexander&lt;/strong&gt;, partner at &lt;strong&gt;CapitalG&lt;/strong&gt;, brings more than 15 years of experience building and scaling GTM teams. Before joining CapitalG, she was CMO at Carta and held leadership roles at Salesforce and RelateIQ.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Vanessa Larco&lt;/strong&gt; is the co-founder of &lt;strong&gt;Premise&lt;/strong&gt; and a former partner at NEA. With a background in product leadership and investing, Larco brings a sharp lens to how startups can grow efficiently — and stay focused — as AI reshapes product and market strategies.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-snag-your-july-discount-and-get-ready-for-disrupt-2025"&gt;Snag your July discount and get ready for Disrupt 2025&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Don’t miss this session — &lt;strong&gt;and more than 200 others&lt;/strong&gt; — at the tech epicenter where 10,000+ startup and VC leaders connect to discover the next wave of innovation. &lt;strong&gt;Buy your tickets now&lt;/strong&gt; and save up to $675 before July savings end.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/on-stage-at-techcrunch-disrupt-2025-how-ai-is-forcing-late-stage-startups-to-rewire-gtm-or-be-left-behind/</guid><pubDate>Tue, 22 Jul 2025 14:30:00 +0000</pubDate></item><item><title>xAI workers balked over training request to help “give Grok a face,” docs show (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/07/xai-workers-balked-over-training-request-to-help-give-grok-a-face-docs-show/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Slack messages: Some xAI employees refused to join invasive Grok training.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A smartphone shows Ani, a virtual anime-style assistant character featured in the Grok 4 AI chatbot developed by xAI. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Cheng Xin / Contributor | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Dozens of xAI employees expressed concerns—and many objected—when asked to record videos of their facial expressions to help "give Grok a face," Business Insider reported.&lt;/p&gt;
&lt;p&gt;BI reviewed internal documents and Slack messages, finding that the so-called project "Skippy" was designed to help Grok learn what a face is and "interpret human emotions."&lt;/p&gt;
&lt;p&gt;It's unclear from these documents if workers' facial data helped train controversial avatars that xAI released last week, including Ani—an anime companion that flirts and strips—and Rudi—a red panda with a "Bad" mode that encourages violence. But a recording of an xAI introductory meeting on "Skippy" showed a lead engineer confirming the company "might eventually use" the employees' facial data to build out "avatars of people," BI reported.&lt;/p&gt;
&lt;p&gt;Although all employees were told that their training videos would not be shared outside the company and would be used "solely" for training, some workers refused to sign the consent form, worried their likenesses might be used to say things they never said. Likely xAI's recent Grok scandals—where the chatbot went on antisemitic rants praising Hitler—and xAI's reported plan to hire an engineer to design "AI-powered anime girls for people to fall in love with" contributed to employees' discomfort. Confirming on Slack that they opted out, these employees were ultimately too "uneasy" granting xAI "perpetual" access to their data, BI found.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For the more than 200 employees who did not opt out, xAI asked that they record 15- to 30-minute conversations, where one employee posed as the potential Grok user and the other posed as the "host." xAI was specifically looking for "imperfect data," BI noted, expecting that only training on crystal-clear videos would limit Grok's ability to interpret a wider range of facial expressions.&lt;/p&gt;
&lt;p&gt;xAI's goal was to help Grok "recognize and analyze facial movements and expressions, such as how people talk, react to others' conversations, and express themselves in various conditions," an internal document said. Allegedly among the only guarantees to employees—who likely recognized how sensitive facial data is—was a promise "not to create a digital version of you."&lt;/p&gt;
&lt;p&gt;To get the most out of data submitted by "Skippy" participants, dubbed tutors, xAI recommended that they never provide one-word answers, always ask follow-up questions, and maintain eye contact throughout the conversations.&lt;/p&gt;
&lt;p&gt;The company also apparently provided scripts to evoke facial expressions they wanted Grok to understand, suggesting conversation topics like "How do you secretly manipulate people to get your way?" or "Would you ever date someone with a kid or kids?"&lt;/p&gt;
&lt;p&gt;For xAI employees who provided facial training data, privacy concerns may still exist, considering X—the social platform formerly known as Twitter that recently was folded into xAI—has recently been targeted by what Elon Musk called a "massive" cyberattack. Because of privacy risks ranging from identity theft to government surveillance, several states have passed strict biometric privacy laws to prevent companies from collecting such data without explicit consent.&lt;/p&gt;
&lt;p&gt;xAI did not respond to Ars' request for comment.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Slack messages: Some xAI employees refused to join invasive Grok training.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A smartphone shows Ani, a virtual anime-style assistant character featured in the Grok 4 AI chatbot developed by xAI. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Cheng Xin / Contributor | Getty Images News

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Dozens of xAI employees expressed concerns—and many objected—when asked to record videos of their facial expressions to help "give Grok a face," Business Insider reported.&lt;/p&gt;
&lt;p&gt;BI reviewed internal documents and Slack messages, finding that the so-called project "Skippy" was designed to help Grok learn what a face is and "interpret human emotions."&lt;/p&gt;
&lt;p&gt;It's unclear from these documents if workers' facial data helped train controversial avatars that xAI released last week, including Ani—an anime companion that flirts and strips—and Rudi—a red panda with a "Bad" mode that encourages violence. But a recording of an xAI introductory meeting on "Skippy" showed a lead engineer confirming the company "might eventually use" the employees' facial data to build out "avatars of people," BI reported.&lt;/p&gt;
&lt;p&gt;Although all employees were told that their training videos would not be shared outside the company and would be used "solely" for training, some workers refused to sign the consent form, worried their likenesses might be used to say things they never said. Likely xAI's recent Grok scandals—where the chatbot went on antisemitic rants praising Hitler—and xAI's reported plan to hire an engineer to design "AI-powered anime girls for people to fall in love with" contributed to employees' discomfort. Confirming on Slack that they opted out, these employees were ultimately too "uneasy" granting xAI "perpetual" access to their data, BI found.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For the more than 200 employees who did not opt out, xAI asked that they record 15- to 30-minute conversations, where one employee posed as the potential Grok user and the other posed as the "host." xAI was specifically looking for "imperfect data," BI noted, expecting that only training on crystal-clear videos would limit Grok's ability to interpret a wider range of facial expressions.&lt;/p&gt;
&lt;p&gt;xAI's goal was to help Grok "recognize and analyze facial movements and expressions, such as how people talk, react to others' conversations, and express themselves in various conditions," an internal document said. Allegedly among the only guarantees to employees—who likely recognized how sensitive facial data is—was a promise "not to create a digital version of you."&lt;/p&gt;
&lt;p&gt;To get the most out of data submitted by "Skippy" participants, dubbed tutors, xAI recommended that they never provide one-word answers, always ask follow-up questions, and maintain eye contact throughout the conversations.&lt;/p&gt;
&lt;p&gt;The company also apparently provided scripts to evoke facial expressions they wanted Grok to understand, suggesting conversation topics like "How do you secretly manipulate people to get your way?" or "Would you ever date someone with a kid or kids?"&lt;/p&gt;
&lt;p&gt;For xAI employees who provided facial training data, privacy concerns may still exist, considering X—the social platform formerly known as Twitter that recently was folded into xAI—has recently been targeted by what Elon Musk called a "massive" cyberattack. Because of privacy risks ranging from identity theft to government surveillance, several states have passed strict biometric privacy laws to prevent companies from collecting such data without explicit consent.&lt;/p&gt;
&lt;p&gt;xAI did not respond to Ars' request for comment.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/07/xai-workers-balked-over-training-request-to-help-give-grok-a-face-docs-show/</guid><pubDate>Tue, 22 Jul 2025 14:31:42 +0000</pubDate></item><item><title>National security meets next-gen tech at TechCrunch Disrupt 2025’s AI Defense panel (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/national-security-meets-next-gen-tech-at-techcrunch-disrupt-2025s-ai-defense-panel/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is where breakthrough ideas meet the real-world challenges that define the future — and with over 10,000 startup and VC leaders converging, there’s no better place to have the hard conversations. One of the most urgent? How artificial intelligence is reshaping national defense, security, and critical infrastructure in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Enter &lt;strong&gt;AI Defense&lt;/strong&gt;, a can’t-miss panel discussion taking place on one of the two AI Stages, where leaders from government, venture, and the armed services will explore the high-stakes collision of innovation and national security.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Sri Chandrasekar, Justin Fanelli, Kathleen Fisher" class="wp-image-3029399" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_Chandrasekar-Fanelli-Fisher-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-conversation-matters-now"&gt;Why this conversation matters now&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;With AI evolving at a blistering pace, the stakes for defense innovation have never been higher. From autonomous systems and decision intelligence to cybersecurity and battlefield readiness, the U.S. defense and intelligence communities are racing to build smarter, more adaptive technologies — without compromising ethics, oversight, or safety.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This panel explores the delicate balance between intelligence and control, the role of startups in defense innovation, and the multibillion-dollar opportunities emerging at the intersection of national security and AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If your startup touches defense, government, cybersecurity, or AI infrastructure, this is a conversation you can’t afford to miss.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-where-defense-intelligence-and-innovation-converge"&gt;Where defense, intelligence, and innovation converge&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Onstage, you’ll hear from &lt;strong&gt;Dr. Kathleen Fisher&lt;/strong&gt;, director of the Information Innovation Office at DARPA, who is leading the charge on tech that gives the U.S. and its allies an edge in the information wars of the 21st century. With a career that spans AT&amp;amp;T Labs and academia at Tufts University, Fisher blends research credibility with strategic vision, and she’s shaping the next generation of defense-grade AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Joining the panel is &lt;strong&gt;Sri Chandrasekar&lt;/strong&gt;, managing partner at Point72 Ventures, and formerly a leader at In-Q-Tel, the CIA’s strategic investment arm. Chandrasekar knows how to spot frontier tech that moves the needle, and he’s built investment frameworks to support some of the intelligence community’s toughest missions.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Rounding out the panel is &lt;strong&gt;Justin Fanelli&lt;/strong&gt;, chief technology officer for the Department of the Navy. Fanelli lives at the intersection of cybersecurity, innovation, and adoption, driving digital transformation across one of the most complex organizations in government. From his DARPA Service Chiefs Fellowship to his nationally recognized work in defense health and command systems, Fanelli brings the boots-on-the-ground perspective to how defense is done in the age of AI.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-catch-it-live-at-techcrunch-disrupt-2025"&gt;Catch it live at TechCrunch Disrupt 2025&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This &lt;strong&gt;AI Defense&lt;/strong&gt; panel takes place on the &lt;strong&gt;AI Stage&lt;/strong&gt;. Exact session time coming soon — but don’t wait to claim your pass.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch Disrupt 2025 runs October 27-29 at Moscone West in San Francisco. Join 10,000+ startup and VC leaders for three days of conversations and connections that define what’s next. &lt;strong&gt;Grab your pass now before prices increase after July.&lt;/strong&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is where breakthrough ideas meet the real-world challenges that define the future — and with over 10,000 startup and VC leaders converging, there’s no better place to have the hard conversations. One of the most urgent? How artificial intelligence is reshaping national defense, security, and critical infrastructure in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Enter &lt;strong&gt;AI Defense&lt;/strong&gt;, a can’t-miss panel discussion taking place on one of the two AI Stages, where leaders from government, venture, and the armed services will explore the high-stakes collision of innovation and national security.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Sri Chandrasekar, Justin Fanelli, Kathleen Fisher" class="wp-image-3029399" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_Chandrasekar-Fanelli-Fisher-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-this-conversation-matters-now"&gt;Why this conversation matters now&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;With AI evolving at a blistering pace, the stakes for defense innovation have never been higher. From autonomous systems and decision intelligence to cybersecurity and battlefield readiness, the U.S. defense and intelligence communities are racing to build smarter, more adaptive technologies — without compromising ethics, oversight, or safety.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This panel explores the delicate balance between intelligence and control, the role of startups in defense innovation, and the multibillion-dollar opportunities emerging at the intersection of national security and AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If your startup touches defense, government, cybersecurity, or AI infrastructure, this is a conversation you can’t afford to miss.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-where-defense-intelligence-and-innovation-converge"&gt;Where defense, intelligence, and innovation converge&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Onstage, you’ll hear from &lt;strong&gt;Dr. Kathleen Fisher&lt;/strong&gt;, director of the Information Innovation Office at DARPA, who is leading the charge on tech that gives the U.S. and its allies an edge in the information wars of the 21st century. With a career that spans AT&amp;amp;T Labs and academia at Tufts University, Fisher blends research credibility with strategic vision, and she’s shaping the next generation of defense-grade AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Joining the panel is &lt;strong&gt;Sri Chandrasekar&lt;/strong&gt;, managing partner at Point72 Ventures, and formerly a leader at In-Q-Tel, the CIA’s strategic investment arm. Chandrasekar knows how to spot frontier tech that moves the needle, and he’s built investment frameworks to support some of the intelligence community’s toughest missions.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Rounding out the panel is &lt;strong&gt;Justin Fanelli&lt;/strong&gt;, chief technology officer for the Department of the Navy. Fanelli lives at the intersection of cybersecurity, innovation, and adoption, driving digital transformation across one of the most complex organizations in government. From his DARPA Service Chiefs Fellowship to his nationally recognized work in defense health and command systems, Fanelli brings the boots-on-the-ground perspective to how defense is done in the age of AI.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-catch-it-live-at-techcrunch-disrupt-2025"&gt;Catch it live at TechCrunch Disrupt 2025&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This &lt;strong&gt;AI Defense&lt;/strong&gt; panel takes place on the &lt;strong&gt;AI Stage&lt;/strong&gt;. Exact session time coming soon — but don’t wait to claim your pass.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch Disrupt 2025 runs October 27-29 at Moscone West in San Francisco. Join 10,000+ startup and VC leaders for three days of conversations and connections that define what’s next. &lt;strong&gt;Grab your pass now before prices increase after July.&lt;/strong&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/national-security-meets-next-gen-tech-at-techcrunch-disrupt-2025s-ai-defense-panel/</guid><pubDate>Tue, 22 Jul 2025 15:00:00 +0000</pubDate></item><item><title>AI On: How Financial Services Companies Use Agentic AI to Enhance Productivity, Efficiency and Security (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/financial-services-agentic-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/aionfsi.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;With advancements in agentic AI, intelligent AI systems are maturing to now facilitate autonomous decision-making across industries, including financial services.&lt;/p&gt;
&lt;p&gt;Over the last year, customer service-related use of generative AI, including chatbots and AI assistants, has more than doubled in financial services, rising from 25% to 60%. Organizations are using AI to automate time-intensive tasks like document processing and report generation, driving significant cost savings and operational efficiency.&lt;/p&gt;
&lt;p&gt;According to NVIDIA’s latest State of AI in Financial Services report, more than 90% of respondents reported a positive impact on their organization’s revenue from AI.&lt;/p&gt;
&lt;p&gt;AI agents are versatile, capable of adapting to complex tasks that require strict protocols and secure data usage. They can help with an expanding list of use cases, from enabling better investment decisions by automatically identifying portfolio optimization strategies to ensuring regulatory alignment and compliance automation.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Where AI Agents Offer the Most Value in Financial Services&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To improve market returns and business performance, AI agents are being adopted in various areas that benefit greatly from autonomous decision-making backed by data.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Elevated Customer Service Experiences&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;According to the State of AI in Financial Services report, 60% of respondents said customer experience and engagement was the top use case for generative AI. Businesses using AI have already seen customer experiences improve by 26%.&lt;/p&gt;
&lt;p&gt;AI agents can help automate repetitive tasks while providing next steps, such as dispute resolution and know-your-customer updates. This reduces operational costs and helps minimize human errors.&lt;/p&gt;
&lt;p&gt;By handling customer inquiries and forms, AI chatbots scale support and ensure 24/7 availability, enhancing customer satisfaction. Employees can focus on higher-level, judgment-based cases, rather than performing case intake, data analysis and documentation.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Advanced Fraud Detection&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In addition, AI agents are crucial for fraud detection, as they can detect and respond to suspicious transactions automatically. The State of AI report highlighted that out of 20 use cases, cybersecurity experienced the highest growth over the last year, with more than a third of respondents now assessing or investing in AI for cybersecurity.&lt;/p&gt;
&lt;p&gt;AI closes the time gap between detection and action, as a lack of action can result in significant financial loss.&lt;/p&gt;
&lt;p&gt;To combat fraud, AI agents can monitor transaction patterns in real time, learn from new types of fraud and take immediate action by alerting compliance teams or freezing suspicious accounts — all without the need for human intervention. Plus, teams of AI agents can work with other systems to retrieve additional data, simulate potential fraud scenarios and investigate abnormalities.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Managing Digital Payments and Banking Transactions&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;AI agents make financial management easier, especially for bill payment and cash flow management. Because agentic AI supports machine-to-machine interactions in digital ecosystems, it can ensure regulatory compliance by automatically maintaining detailed audit trails. This reduces compliance costs and processing time, making it easier for financial institutions to operate in complex regulatory environments.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Intelligent Document Processing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;For capital markets, the most powerful investment insights are often hidden in unstructured text data from everyday document sources such as news articles, blogs and SEC filings. AI agents can accelerate intelligent document processing (IDP) to provide insight and investment recommendations for traders, enabling faster decision-making and reducing the risk of financial losses.&lt;/p&gt;
&lt;p&gt;In consumer banking, handling documents like loan records, regulatory filings and transaction records involves a lot of complex data. This amount of data is so large that it can be difficult and time-consuming to process and understand it manually. IDP helps solve this issue, using AI to identify document types, summarize documents, employ retrieval-augmented generation for answers and support, and organize data.&lt;/p&gt;
&lt;p&gt;The data-driven insights from multi-agent systems inform strategic business decisions as these systems continuously learn from customer and institutional data using a data flywheel.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Examples of AI Agents in Financial Services&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Many industry customers and partners have benefited significantly from integrating AI into their workflows.&lt;/p&gt;
&lt;p&gt;For example, BlackRock uses Aladdin, a proprietary platform that unifies investment management processes across public and private markets for institutional investors.&lt;/p&gt;
&lt;p&gt;With numerous Aladdin applications and thousands of specialized users, the BlackRock team identified an opportunity to use AI to streamline the platform’s user experience while fostering connectivity and operational efficiency. Rapidly and securely, BlackRock has bolstered the Aladdin platform with advanced AI through Aladdin Copilot.&lt;/p&gt;
&lt;p&gt;Using a federated development model, where different teams can work on AI agents independently while building on a common foundation, BlackRock’s central AI team established a standardized communication system and plug-in registry. This allows the firm’s developers and data scientists to create and deploy AI agents tailored to their specific areas, improving intelligence and efficiency for clients.&lt;/p&gt;
&lt;p&gt;Another example is bunq’s generative AI platform, Finn, which offers users a range of features to help manage finances through an in-app chatbot. It can answer questions about money, provide insight into spending habits and offer tips on using the bunq app. Finn uses advanced AI to improve its responses based on feedback and, beyond the in-app chatbot, now handles over 90% of all users’ support tickets.&lt;/p&gt;
&lt;p&gt;Capital One is also assisting customers with Chat Concierge, its multi-agent conversational AI assistant designed to enhance the automotive-buying experience. Consumers have 24/7 access to agents that provide real-time information and take action based on user requests. In a single conversation, Chat Concierge can perform tasks like comparing vehicles to help car buyers find their ideal choice and scheduling test drives or appointments with a sales team.&lt;/p&gt;
&lt;p&gt;RBC’s latest platform for global research, Aiden, uses internal agents to automatically perform analysis when companies covered by RBC Capital Markets release SEC filings. Aiden has an orchestration agent working with other agents, such as the SEC filing agent, earnings agent and a real-time news agent.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Designing an AI-Powered Finance Agent&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The building blocks of a powerful financial services agent include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Multimodal and Multi-Query Capabilities: &lt;/b&gt;These agents can process and respond to queries that combine text and images, making search processes more versatile and user-friendly. They can also easily be extended to support other modalities such as voice.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Integration With Large Language Models:&lt;/b&gt; Advanced LLMs, such as the NVIDIA Llama Nemotron family, bring reasoning capabilities to AI assistants, enabling them to engage in natural, humanlike interactions. NVIDIA NIM microservices provide industry-standard application programming interfaces for simple integration into AI applications, development frameworks and workflows.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Management of Structured and Unstructured Data: &lt;/b&gt;NVIDIA NeMo Retriever microservices enable the ingestion, embedding and understanding of relevant data sources, helping ensure AI agent responses are relevant, accurate and context-aware.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Integration, Optimization and AutomationI&lt;/b&gt;: NVIDIA NeMo Agent toolkit enables building, profiling and optimizing AI agents through unified monitoring, detailed workflow profiling, and data-driven optimization tools that expose bottlenecks, reduce costs and ensure scalable, reliable agentic systems across popular frameworks and custom workflows.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Guardrails for Safe, On-Topic Conversations: &lt;/b&gt;NVIDIA NeMo Guardrails are implemented to help ensure that conversations with the AI assistant remain safe and on topic, ultimately protecting brand values and bolstering customer trust.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;Learn more about how financial services companies are using AI to enhance services and business operations in the full &lt;/i&gt;&lt;i&gt;State of AI in Financial Services report&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/07/aionfsi.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;With advancements in agentic AI, intelligent AI systems are maturing to now facilitate autonomous decision-making across industries, including financial services.&lt;/p&gt;
&lt;p&gt;Over the last year, customer service-related use of generative AI, including chatbots and AI assistants, has more than doubled in financial services, rising from 25% to 60%. Organizations are using AI to automate time-intensive tasks like document processing and report generation, driving significant cost savings and operational efficiency.&lt;/p&gt;
&lt;p&gt;According to NVIDIA’s latest State of AI in Financial Services report, more than 90% of respondents reported a positive impact on their organization’s revenue from AI.&lt;/p&gt;
&lt;p&gt;AI agents are versatile, capable of adapting to complex tasks that require strict protocols and secure data usage. They can help with an expanding list of use cases, from enabling better investment decisions by automatically identifying portfolio optimization strategies to ensuring regulatory alignment and compliance automation.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Where AI Agents Offer the Most Value in Financial Services&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To improve market returns and business performance, AI agents are being adopted in various areas that benefit greatly from autonomous decision-making backed by data.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Elevated Customer Service Experiences&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;According to the State of AI in Financial Services report, 60% of respondents said customer experience and engagement was the top use case for generative AI. Businesses using AI have already seen customer experiences improve by 26%.&lt;/p&gt;
&lt;p&gt;AI agents can help automate repetitive tasks while providing next steps, such as dispute resolution and know-your-customer updates. This reduces operational costs and helps minimize human errors.&lt;/p&gt;
&lt;p&gt;By handling customer inquiries and forms, AI chatbots scale support and ensure 24/7 availability, enhancing customer satisfaction. Employees can focus on higher-level, judgment-based cases, rather than performing case intake, data analysis and documentation.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Advanced Fraud Detection&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In addition, AI agents are crucial for fraud detection, as they can detect and respond to suspicious transactions automatically. The State of AI report highlighted that out of 20 use cases, cybersecurity experienced the highest growth over the last year, with more than a third of respondents now assessing or investing in AI for cybersecurity.&lt;/p&gt;
&lt;p&gt;AI closes the time gap between detection and action, as a lack of action can result in significant financial loss.&lt;/p&gt;
&lt;p&gt;To combat fraud, AI agents can monitor transaction patterns in real time, learn from new types of fraud and take immediate action by alerting compliance teams or freezing suspicious accounts — all without the need for human intervention. Plus, teams of AI agents can work with other systems to retrieve additional data, simulate potential fraud scenarios and investigate abnormalities.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Managing Digital Payments and Banking Transactions&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;AI agents make financial management easier, especially for bill payment and cash flow management. Because agentic AI supports machine-to-machine interactions in digital ecosystems, it can ensure regulatory compliance by automatically maintaining detailed audit trails. This reduces compliance costs and processing time, making it easier for financial institutions to operate in complex regulatory environments.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Intelligent Document Processing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;For capital markets, the most powerful investment insights are often hidden in unstructured text data from everyday document sources such as news articles, blogs and SEC filings. AI agents can accelerate intelligent document processing (IDP) to provide insight and investment recommendations for traders, enabling faster decision-making and reducing the risk of financial losses.&lt;/p&gt;
&lt;p&gt;In consumer banking, handling documents like loan records, regulatory filings and transaction records involves a lot of complex data. This amount of data is so large that it can be difficult and time-consuming to process and understand it manually. IDP helps solve this issue, using AI to identify document types, summarize documents, employ retrieval-augmented generation for answers and support, and organize data.&lt;/p&gt;
&lt;p&gt;The data-driven insights from multi-agent systems inform strategic business decisions as these systems continuously learn from customer and institutional data using a data flywheel.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Examples of AI Agents in Financial Services&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Many industry customers and partners have benefited significantly from integrating AI into their workflows.&lt;/p&gt;
&lt;p&gt;For example, BlackRock uses Aladdin, a proprietary platform that unifies investment management processes across public and private markets for institutional investors.&lt;/p&gt;
&lt;p&gt;With numerous Aladdin applications and thousands of specialized users, the BlackRock team identified an opportunity to use AI to streamline the platform’s user experience while fostering connectivity and operational efficiency. Rapidly and securely, BlackRock has bolstered the Aladdin platform with advanced AI through Aladdin Copilot.&lt;/p&gt;
&lt;p&gt;Using a federated development model, where different teams can work on AI agents independently while building on a common foundation, BlackRock’s central AI team established a standardized communication system and plug-in registry. This allows the firm’s developers and data scientists to create and deploy AI agents tailored to their specific areas, improving intelligence and efficiency for clients.&lt;/p&gt;
&lt;p&gt;Another example is bunq’s generative AI platform, Finn, which offers users a range of features to help manage finances through an in-app chatbot. It can answer questions about money, provide insight into spending habits and offer tips on using the bunq app. Finn uses advanced AI to improve its responses based on feedback and, beyond the in-app chatbot, now handles over 90% of all users’ support tickets.&lt;/p&gt;
&lt;p&gt;Capital One is also assisting customers with Chat Concierge, its multi-agent conversational AI assistant designed to enhance the automotive-buying experience. Consumers have 24/7 access to agents that provide real-time information and take action based on user requests. In a single conversation, Chat Concierge can perform tasks like comparing vehicles to help car buyers find their ideal choice and scheduling test drives or appointments with a sales team.&lt;/p&gt;
&lt;p&gt;RBC’s latest platform for global research, Aiden, uses internal agents to automatically perform analysis when companies covered by RBC Capital Markets release SEC filings. Aiden has an orchestration agent working with other agents, such as the SEC filing agent, earnings agent and a real-time news agent.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Designing an AI-Powered Finance Agent&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The building blocks of a powerful financial services agent include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Multimodal and Multi-Query Capabilities: &lt;/b&gt;These agents can process and respond to queries that combine text and images, making search processes more versatile and user-friendly. They can also easily be extended to support other modalities such as voice.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Integration With Large Language Models:&lt;/b&gt; Advanced LLMs, such as the NVIDIA Llama Nemotron family, bring reasoning capabilities to AI assistants, enabling them to engage in natural, humanlike interactions. NVIDIA NIM microservices provide industry-standard application programming interfaces for simple integration into AI applications, development frameworks and workflows.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Management of Structured and Unstructured Data: &lt;/b&gt;NVIDIA NeMo Retriever microservices enable the ingestion, embedding and understanding of relevant data sources, helping ensure AI agent responses are relevant, accurate and context-aware.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Integration, Optimization and AutomationI&lt;/b&gt;: NVIDIA NeMo Agent toolkit enables building, profiling and optimizing AI agents through unified monitoring, detailed workflow profiling, and data-driven optimization tools that expose bottlenecks, reduce costs and ensure scalable, reliable agentic systems across popular frameworks and custom workflows.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Guardrails for Safe, On-Topic Conversations: &lt;/b&gt;NVIDIA NeMo Guardrails are implemented to help ensure that conversations with the AI assistant remain safe and on topic, ultimately protecting brand values and bolstering customer trust.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;Learn more about how financial services companies are using AI to enhance services and business operations in the full &lt;/i&gt;&lt;i&gt;State of AI in Financial Services report&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/financial-services-agentic-ai/</guid><pubDate>Tue, 22 Jul 2025 15:00:16 +0000</pubDate></item><item><title>Gemini 2.5 Flash-Lite is now ready for scaled production use (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/gemini-25-flash-lite-is-now-ready-for-scaled-production-use/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/2.5_flash_lite_ga_meta_card_1600x.2e16d0ba.fill-1200x600.jpg" /&gt;&lt;/div&gt;&lt;div&gt;
    &lt;p&gt;Today, we’re releasing the stable version of Gemini 2.5 Flash-Lite, our fastest and lowest cost ($0.10 input per 1M, $0.40 output per 1M) model in the Gemini 2.5 model family. We built 2.5 Flash-Lite to push the frontier of intelligence per dollar, with native reasoning capabilities that can be optionally toggled on for more demanding use cases. Building on the momentum of 2.5 Pro and 2.5 Flash, this model rounds out our set of 2.5 models that are ready for scaled production use.&lt;/p&gt;&lt;h2 id="our-most-cost-efficient-and-fastest-2.5-model-yet"&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;Our most cost-efficient and fastest 2.5 model yet&lt;/h2&gt;
&lt;/div&gt;&lt;div&gt;
    &lt;p&gt;Gemini 2.5 Flash-Lite strikes a balance between performance and cost, without compromising on quality, particularly for latency-sensitive tasks like translation and classification.&lt;/p&gt;&lt;p&gt;Here’s what makes it stand out:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Best in-class speed:&lt;/b&gt; Gemini 2.5 Flash-Lite has lower latency than both 2.0 Flash-Lite and 2.0 Flash on a broad sample of prompts.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Cost-efficiency:&lt;/b&gt; It’s our lowest-cost 2.5 model yet, priced at $0.10 / 1M input tokens and $0.40 output tokens, allowing you to handle large volumes of requests affordably. We have also reduced audio input pricing by 40% from the preview launch.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Smart and small:&lt;/b&gt; It demonstrates all-around higher quality than 2.0 Flash-Lite across a wide range of benchmarks, including coding, math, science, reasoning, and multimodal understanding.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Fully featured:&lt;/b&gt; When you build with 2.5 Flash-Lite, you get access to a 1 million-token context window, controllable thinking budgets, and support for native tools like Grounding with Google Search, Code Execution, and URL Context.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="gemini-2.5-flash-lite-in-action"&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;Gemini 2.5 Flash-Lite in action&lt;/h2&gt;&lt;p&gt;Since the launch of 2.5 Flash-Lite, we have already seen some incredibly successful deployments, here are some of our favorites:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Satlyt&lt;/b&gt; is building a decentralized space computing platform that will transform how satellite data is processed and utilized for real-time summarization of in-orbit telemetry, autonomous task management, and satellite-to-satellite communication parsing. &lt;b&gt;2.5 Flash-Lite’s speed has enabled a 45% reduction in latency&lt;/b&gt; for critical onboard diagnostics and a &lt;b&gt;30% decrease in power consumption&lt;/b&gt; compared to their baseline models.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;HeyGen&lt;/b&gt; uses AI to create avatars for video content and leverages Gemini 2.5 Flash-Lite to automate video planning, analyze and optimize content, and &lt;b&gt;translate videos into over 180 languages&lt;/b&gt;. This allows them to provide global, personalized experiences for their users.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;DocsHound&lt;/b&gt; turns product demos into documentation by using Gemini 2.5 Flash-Lite to &lt;b&gt;process long videos and extract thousands of screenshots&lt;/b&gt; with low latency. This transforms footage into comprehensive documentation and training data for AI agents much faster than traditional methods.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Evertune&lt;/b&gt; helps brands understand how they are represented across AI models. Gemini 2.5 Flash-Lite is a game-changer for them, dramatically speeding up analysis and report generation. Its fast performance allows them to quickly scan and synthesize large volumes of model output to provide clients with &lt;b&gt;dynamic, timely insights&lt;/b&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br /&gt;You can start using 2.5 Flash-Lite by specifying “gemini-2.5-flash-lite” in your code. If you are using the preview version, you can switch to “gemini-2.5-flash-lite” which is the same underlying model. We plan to remove the preview alias of Flash-Lite on August 25th.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;Ready to start building? Try the stable version of Gemini 2.5 Flash-Lite now in Google AI Studio and Vertex AI.&lt;/p&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/2.5_flash_lite_ga_meta_card_1600x.2e16d0ba.fill-1200x600.jpg" /&gt;&lt;/div&gt;&lt;div&gt;
    &lt;p&gt;Today, we’re releasing the stable version of Gemini 2.5 Flash-Lite, our fastest and lowest cost ($0.10 input per 1M, $0.40 output per 1M) model in the Gemini 2.5 model family. We built 2.5 Flash-Lite to push the frontier of intelligence per dollar, with native reasoning capabilities that can be optionally toggled on for more demanding use cases. Building on the momentum of 2.5 Pro and 2.5 Flash, this model rounds out our set of 2.5 models that are ready for scaled production use.&lt;/p&gt;&lt;h2 id="our-most-cost-efficient-and-fastest-2.5-model-yet"&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;Our most cost-efficient and fastest 2.5 model yet&lt;/h2&gt;
&lt;/div&gt;&lt;div&gt;
    &lt;p&gt;Gemini 2.5 Flash-Lite strikes a balance between performance and cost, without compromising on quality, particularly for latency-sensitive tasks like translation and classification.&lt;/p&gt;&lt;p&gt;Here’s what makes it stand out:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Best in-class speed:&lt;/b&gt; Gemini 2.5 Flash-Lite has lower latency than both 2.0 Flash-Lite and 2.0 Flash on a broad sample of prompts.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Cost-efficiency:&lt;/b&gt; It’s our lowest-cost 2.5 model yet, priced at $0.10 / 1M input tokens and $0.40 output tokens, allowing you to handle large volumes of requests affordably. We have also reduced audio input pricing by 40% from the preview launch.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Smart and small:&lt;/b&gt; It demonstrates all-around higher quality than 2.0 Flash-Lite across a wide range of benchmarks, including coding, math, science, reasoning, and multimodal understanding.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Fully featured:&lt;/b&gt; When you build with 2.5 Flash-Lite, you get access to a 1 million-token context window, controllable thinking budgets, and support for native tools like Grounding with Google Search, Code Execution, and URL Context.&lt;/li&gt;&lt;/ul&gt;&lt;h2 id="gemini-2.5-flash-lite-in-action"&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;Gemini 2.5 Flash-Lite in action&lt;/h2&gt;&lt;p&gt;Since the launch of 2.5 Flash-Lite, we have already seen some incredibly successful deployments, here are some of our favorites:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Satlyt&lt;/b&gt; is building a decentralized space computing platform that will transform how satellite data is processed and utilized for real-time summarization of in-orbit telemetry, autonomous task management, and satellite-to-satellite communication parsing. &lt;b&gt;2.5 Flash-Lite’s speed has enabled a 45% reduction in latency&lt;/b&gt; for critical onboard diagnostics and a &lt;b&gt;30% decrease in power consumption&lt;/b&gt; compared to their baseline models.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;HeyGen&lt;/b&gt; uses AI to create avatars for video content and leverages Gemini 2.5 Flash-Lite to automate video planning, analyze and optimize content, and &lt;b&gt;translate videos into over 180 languages&lt;/b&gt;. This allows them to provide global, personalized experiences for their users.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;DocsHound&lt;/b&gt; turns product demos into documentation by using Gemini 2.5 Flash-Lite to &lt;b&gt;process long videos and extract thousands of screenshots&lt;/b&gt; with low latency. This transforms footage into comprehensive documentation and training data for AI agents much faster than traditional methods.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Evertune&lt;/b&gt; helps brands understand how they are represented across AI models. Gemini 2.5 Flash-Lite is a game-changer for them, dramatically speeding up analysis and report generation. Its fast performance allows them to quickly scan and synthesize large volumes of model output to provide clients with &lt;b&gt;dynamic, timely insights&lt;/b&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br /&gt;You can start using 2.5 Flash-Lite by specifying “gemini-2.5-flash-lite” in your code. If you are using the preview version, you can switch to “gemini-2.5-flash-lite” which is the same underlying model. We plan to remove the preview alias of Flash-Lite on August 25th.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;Ready to start building? Try the stable version of Gemini 2.5 Flash-Lite now in Google AI Studio and Vertex AI.&lt;/p&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/gemini-25-flash-lite-is-now-ready-for-scaled-production-use/</guid><pubDate>Tue, 22 Jul 2025 16:00:00 +0000</pubDate></item><item><title>LSM-2: Learning from incomplete wearable sensor data (The latest research from Google)</title><link>https://research.google/blog/lsm-2-learning-from-incomplete-wearable-sensor-data/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Training and evaluation&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;We leverage a dataset with 40 million hours of wearable data sampled from over 60,000 participants during the period from March to May 2024. The dataset was thoroughly anonymized or de-identified to ensure that participant information was removed and privacy was maintained. Subjects wore a variety of Fitbit and Google Pixel smartwatches and trackers and consented for their data to be used for research and development of new health and wellness products and services. The subjects were asked to self-report sex, age, and weight.&lt;/p&gt;&lt;p&gt;To pre-train LSM-2, we employ the AIM SSL technique introduced in the previous section. AIM implements a masked reconstruction training objective, and learns to understand data that is naturally missing, and impute data that is artificially masked. This unified framework allows LSM-2 to learn the underlying structure (including missingness) inherent in wearable sensor data.&lt;/p&gt;&lt;p&gt;We curate a set of downstream tasks to evaluate the pre-trained model, using meta-data that was collected alongside the sensor signals for the purposes of research and development. These include user annotated activities from a set of 20 different categories (such as running, skiing, kayaking and playing golf) and self-reported diagnoses of hypertension and anxiety. These data were split into fine-tuning and evaluation sets where data from each individual was only in either the tuning or the evaluation set and not both. Data from individuals used in the pretraining stage was also not included in the fine-tuning or evaluation stages.&lt;/p&gt;&lt;p&gt;The generative capabilities of LSM-2 are evaluated through the tasks of random imputation, temporal interpolation, temporal extrapolation (forecasting), and sensor imputation, described in our LSM-1 work.&lt;/p&gt;&lt;p&gt;The utility of the LSM-2 embeddings are evaluated via linear probe on a number of discriminative tasks. Specifically we gauge the applicability of the LSM-2 embeddings to the tasks of binary hypertension classification, binary anxiety classification, and 20-class activity recognition. We evaluate LSM-2’s ability to model physiology via age and BMI regression tasks.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Training and evaluation&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;We leverage a dataset with 40 million hours of wearable data sampled from over 60,000 participants during the period from March to May 2024. The dataset was thoroughly anonymized or de-identified to ensure that participant information was removed and privacy was maintained. Subjects wore a variety of Fitbit and Google Pixel smartwatches and trackers and consented for their data to be used for research and development of new health and wellness products and services. The subjects were asked to self-report sex, age, and weight.&lt;/p&gt;&lt;p&gt;To pre-train LSM-2, we employ the AIM SSL technique introduced in the previous section. AIM implements a masked reconstruction training objective, and learns to understand data that is naturally missing, and impute data that is artificially masked. This unified framework allows LSM-2 to learn the underlying structure (including missingness) inherent in wearable sensor data.&lt;/p&gt;&lt;p&gt;We curate a set of downstream tasks to evaluate the pre-trained model, using meta-data that was collected alongside the sensor signals for the purposes of research and development. These include user annotated activities from a set of 20 different categories (such as running, skiing, kayaking and playing golf) and self-reported diagnoses of hypertension and anxiety. These data were split into fine-tuning and evaluation sets where data from each individual was only in either the tuning or the evaluation set and not both. Data from individuals used in the pretraining stage was also not included in the fine-tuning or evaluation stages.&lt;/p&gt;&lt;p&gt;The generative capabilities of LSM-2 are evaluated through the tasks of random imputation, temporal interpolation, temporal extrapolation (forecasting), and sensor imputation, described in our LSM-1 work.&lt;/p&gt;&lt;p&gt;The utility of the LSM-2 embeddings are evaluated via linear probe on a number of discriminative tasks. Specifically we gauge the applicability of the LSM-2 embeddings to the tasks of binary hypertension classification, binary anxiety classification, and 20-class activity recognition. We evaluate LSM-2’s ability to model physiology via age and BMI regression tasks.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/lsm-2-learning-from-incomplete-wearable-sensor-data/</guid><pubDate>Tue, 22 Jul 2025 16:42:00 +0000</pubDate></item><item><title>Google’s newest Gemini 2.5 model aims for ‘intelligence per dollar’ (AI News)</title><link>https://www.artificialintelligence-news.com/news/googles-newest-gemini-2-5-model-aims-intelligence-per-dollar/</link><description>&lt;p&gt;Google just dropped the stable version of Gemini 2.5 Flash-Lite and they’ve essentially created a model that’s designed to be the workhorse for developers who need to build things at scale without breaking the bank.&lt;/p&gt;&lt;p&gt;Building cool things with AI can often feel like a frustrating balancing act. You want a model that’s smart and powerful, but you also don’t want to remortgage your house to pay for the API calls. And if your app needs to be fast for users, a slow, churning model is a non-starter.&lt;/p&gt;&lt;p&gt;Google says Gemini 2.5 Flash-Lite is quicker than their previous speedy models, which is a big claim. For anyone building a real-time translator, a customer service chatbot, or anything where a lag would feel awkward, this is huge.&lt;/p&gt;&lt;p&gt;And then there’s the price. At $0.10 to process a million words of input and $0.40 for the output, it’s ridiculously cheap. This is the kind of pricing that changes how you think about development. You can finally stop worrying about every single API call and just let your application do its thing. It opens the door for small teams and solo developers to build things that were previously only viable for big companies.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Comparison of the Google Gemini 2.5 Flash-Lite model compared to others in the AI family." class="wp-image-107166" height="1024" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/07/google-gemini-2.5-flash-lite-flash-pro-ai-artificial-intelligence-family-comparison-1024x1024.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;Now, you might be thinking, “Okay, it’s cheap and fast, so it must be a bit dim-witted, right?” Apparently not. Google insists the Gemini 2.5 Flash-Lite model is smarter than its predecessors across the board: reasoning, coding, and even understanding images and audio.&lt;/p&gt;&lt;p&gt;Of course, it still has that massive one million token context window—that means you can throw huge documents, codebases, or long transcripts at it and it won’t break a sweat.&lt;/p&gt;&lt;p&gt;And this isn’t just marketing fluff, companies are already building things with it.&lt;/p&gt;&lt;p&gt;Space tech company Satlyt is using it on satellites to diagnose problems in orbit, cutting down on delays and saving power. Another one, HeyGen, is using it to translate videos into over 180 languages.&lt;/p&gt;&lt;p&gt;A personal favourite example is DocsHound. They use it to watch product demo videos and automatically create technical documentation from them. Imagine how much time that saves! It shows that Flash-Lite is more than capable of handling complex, real-world tasks.&lt;/p&gt;&lt;p&gt;If you want to try out the Gemini 2.5 Flash-Lite model, you can start using it now in Google AI Studio or Vertex AI. All you have to do is specify “gemini-2.5-flash-lite” in your code. Just a quick heads-up: if you were using the preview version, make sure you switch to this new name before August 25th, as they’re retiring the old one.&lt;/p&gt;&lt;p&gt;Rather than just another model update from Google, Gemini 2.5 Flash-Lite lowers the barrier to entry so more of us can experiment and build useful things without needing a massive budget.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI and Oracle announce Stargate AI data centre deal&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Google just dropped the stable version of Gemini 2.5 Flash-Lite and they’ve essentially created a model that’s designed to be the workhorse for developers who need to build things at scale without breaking the bank.&lt;/p&gt;&lt;p&gt;Building cool things with AI can often feel like a frustrating balancing act. You want a model that’s smart and powerful, but you also don’t want to remortgage your house to pay for the API calls. And if your app needs to be fast for users, a slow, churning model is a non-starter.&lt;/p&gt;&lt;p&gt;Google says Gemini 2.5 Flash-Lite is quicker than their previous speedy models, which is a big claim. For anyone building a real-time translator, a customer service chatbot, or anything where a lag would feel awkward, this is huge.&lt;/p&gt;&lt;p&gt;And then there’s the price. At $0.10 to process a million words of input and $0.40 for the output, it’s ridiculously cheap. This is the kind of pricing that changes how you think about development. You can finally stop worrying about every single API call and just let your application do its thing. It opens the door for small teams and solo developers to build things that were previously only viable for big companies.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Comparison of the Google Gemini 2.5 Flash-Lite model compared to others in the AI family." class="wp-image-107166" height="1024" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/07/google-gemini-2.5-flash-lite-flash-pro-ai-artificial-intelligence-family-comparison-1024x1024.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;Now, you might be thinking, “Okay, it’s cheap and fast, so it must be a bit dim-witted, right?” Apparently not. Google insists the Gemini 2.5 Flash-Lite model is smarter than its predecessors across the board: reasoning, coding, and even understanding images and audio.&lt;/p&gt;&lt;p&gt;Of course, it still has that massive one million token context window—that means you can throw huge documents, codebases, or long transcripts at it and it won’t break a sweat.&lt;/p&gt;&lt;p&gt;And this isn’t just marketing fluff, companies are already building things with it.&lt;/p&gt;&lt;p&gt;Space tech company Satlyt is using it on satellites to diagnose problems in orbit, cutting down on delays and saving power. Another one, HeyGen, is using it to translate videos into over 180 languages.&lt;/p&gt;&lt;p&gt;A personal favourite example is DocsHound. They use it to watch product demo videos and automatically create technical documentation from them. Imagine how much time that saves! It shows that Flash-Lite is more than capable of handling complex, real-world tasks.&lt;/p&gt;&lt;p&gt;If you want to try out the Gemini 2.5 Flash-Lite model, you can start using it now in Google AI Studio or Vertex AI. All you have to do is specify “gemini-2.5-flash-lite” in your code. Just a quick heads-up: if you were using the preview version, make sure you switch to this new name before August 25th, as they’re retiring the old one.&lt;/p&gt;&lt;p&gt;Rather than just another model update from Google, Gemini 2.5 Flash-Lite lowers the barrier to entry so more of us can experiment and build useful things without needing a massive budget.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI and Oracle announce Stargate AI data centre deal&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/googles-newest-gemini-2-5-model-aims-intelligence-per-dollar/</guid><pubDate>Tue, 22 Jul 2025 17:02:58 +0000</pubDate></item><item><title>Fighting forever chemicals and startup fatigue (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/22/1117638/fighting-forever-chemicals-and-startup-fatigue/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/Denise-Kay.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Michigan Economic Development Corporation&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;What if we could permanently remove the toxic “forever chemicals” contaminating our water? That’s the driving force behind Michigan-based startup Enspired Solutions, founded by environmental toxicologist Denise Kay and chemical engineer Meng Wang. The duo left corporate consulting in the rearview mirror to take on one of the most pervasive environmental challenges: PFAS.&lt;/p&gt;    &lt;p&gt;"PFAS is referred to as a forever chemical because it is so resistant to break down,” says Kay. “It does not break down naturally in the environment, so it just circles around and around. This chemistry, which would break that cycle and break the molecule apart, could really support the health of all of us.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Basing the company in Michigan was both a strategic and a practical strategy. The state has been a leader in PFAS regulation with a startup infrastructure—buoyed by the Michigan Economic Development Corporation (MEDC)—that helped turn an ambitious vision into a viable business.&lt;/p&gt;  &lt;p&gt;From intellectual property analyses to forecasting finances and fundraising guidance, the MEDC’s programs offered Kay and Wang the resources to focus on building their PFASigator: a machine the size of two large refrigerators that uses ultraviolet light and chemistry to break down PFAS in water. In other words, “it essentially eats PFAS.”&lt;/p&gt; 
 &lt;p&gt;Despite the support from the MEDC, the journey has been far from smooth. "As people say, being an entrepreneur and running a startup is like a rollercoaster,” Kay says. “You have high moments, and you have very low moments when you think nothing's ever going to move forward."&lt;/p&gt;  &lt;p&gt;Without revenue or salaries in the early days, the co-founders had to be sustained by something greater than financial incentive.&lt;/p&gt; 
 &lt;p&gt;"If problem solving and learning new talents do not provide sufficient intrinsic reward for a founder to be satisfied throughout what I guarantee will be a long duration effort, then that founder may need to reset their expectations. Because the financial rewards of entrepreneurship are small throughout the process."&lt;/p&gt;  &lt;p&gt;Still, Kay remains optimistic about the road ahead for Enspired Solutions, for clean water innovation, and for other founders walking down a similar path. "Often, founders are coached about formulas for fundraising, formulas for startup success. Learning those formulas and expectations is important, but it's also important to not forget that it's your creativity and innovation and foresight that got you to the place you're in and drove you to start a company. Ultimately, people still want to see that shine through."&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with the Michigan Economic Development Corporation.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Full Transcript&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;Megan Tatum:&lt;/em&gt; From MIT Technology Review, I'm Megan Tatum. This is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.&lt;/p&gt;  &lt;p&gt;Today's episode is brought to you in partnership with the Michigan Economic Development Corporation.&lt;/p&gt;&lt;p&gt;Our topic today is launching a technology startup in the US state of Michigan. Building out an innovative idea into a viable product and company requires knowledge and resources that individuals might not have. That's why the Michigan Economic Development Corporation, or the MEDC, has launched an innovation campaign to support technology entrepreneurs.&lt;/p&gt;&lt;p&gt;Two words for you: startup ecosystem.&lt;/p&gt;&lt;p&gt;My guest is Dr. Denise Kay, the co-founder and CEO at Enspired Solutions, a Michigan-based startup focused on removing synthetic forever chemicals called PFAS from water.&lt;/p&gt;&lt;p&gt;Welcome, Denise.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dr. Denise Kay:&lt;/em&gt; Hi, Megan.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Hi. Thank you so much for joining us. To get us started, Denise, I wondered if we could talk about Enspired Solutions a bit more. How did the idea come about, and what does your company do?&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Well, my co-founder, Meng, and I had careers in consulting, advising clients on the fate and toxicity of chemicals in the environment. What we did was evaluate how chemicals moved through soil, water, and air, and what toxic impact they might have on humans and wildlife. That put us in a really unique position to see early on&amp;nbsp;the environmental and health ramifications of the manmade chemical PFAS in our environment.&lt;/p&gt;  &lt;p&gt;When we learned of a very novel and elegant chemistry that could effectively destroy PFAS, we could foresee the value in making this chemistry available for commercial use and the potential for a significant positive impact on maintaining healthy water resources for all of us.&lt;/p&gt;&lt;p&gt;Like you mentioned, PFAS is referred to as a forever chemical because it is so resistant to break down. It does not break down naturally in the environment, so it just circles around and around. This chemistry, which would break that cycle and break the molecule apart, could really support the health of all of us.&lt;/p&gt;&lt;p&gt;Ultimately, Meng and I quit our jobs, and we founded Enspired Solutions. Our objective was to design, manufacture, and sell commercial-scale equipment that destroys PFAS in water based on this laboratory bench-scale chemistry that had been discovered, the goal being that this toxic contaminant does not continue to circulate in our natural resources.&lt;/p&gt;&lt;p&gt;At this point, we have won an award from the EPA and Department of Defense, and proven our technology in over 200 different water samples ranging from groundwater, surface water, landfill leachate, industrial wastewater, [and] municipal wastewater. It's really everywhere. What we're seeing traction in right now is customer applications managing semiconductor waste. Groundwater and surface water around airports tend to be high in PFAS. Centralized waste disposal facilities that collect and manage PFAS-contaminated liquids. And also, even transitioning firetrucks to PFAS-free firefighting foams.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. That's a huge breadth of applications, incredible stuff.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Yeah.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;You launched about four years ago now. I wondered what factors made Michigan the right place to build and grow the company?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; That is something we put a lot of thought into, because I live in Michigan, and Meng lives in Illinois, so when it was just the two of us, there was even that, "Okay, what is going to be our headquarters?" We looked at a number of factors.&lt;/p&gt;  &lt;p&gt;Some of the things we considered were rentable incubator space. By incubator, I mean startup incubators or innovation centers. The startup support network, a pool of future employees, and what position the state agencies were taking regarding PFAS.&lt;/p&gt;&lt;p&gt;While thinking about all those things and investigating our communities, in Michigan, we found a space to rent where we could do chemistry experiments in an incubator environment. Somewhere where we were surrounded by other entrepreneurs, which we knew was something we had to learn how to do. We were great chemists, but we knew that surrounding ourselves with those skills that could be a gap for us was going to be helpful.&lt;/p&gt;&lt;p&gt;Also, we know that Michigan has moved much faster than other states in identifying PFAS sources in the environment and regulating its presence. This combination was something we knew would be the right place for starting our business and having success.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; It was a perfect setting for those two reasons. What were the first stages of your journey working with the Michigan Economic Development Corporation, the MEDC?&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Well, both my co-founder, Meng, and I are first-time entrepreneurs. MEDC was one of the first resources I reached out to, starting from a Google search. They were an information resource we turned to initially, and then again and again for learning some fundamental skills. And receiving one-on-one expert mentorship for things like business contracts, understanding intellectual property landscapes, tracking and forecasting our business finances, and even how to approach fundraising.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Wow. It sounds like they were an invaluable resource in those early days. How did early-stage research and development progress from that point? What were the key MEDC services and programs you used to get started?&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Well, our business is based on cutting-edge science, truly cutting-edge science. Understanding the intellectual property landscape, which is a term used to describe intellectual property, patents, trademarks, trade secrets that are related to the science we were founding our business on, it was very important. So that we knew we were starting on a path, that we wouldn't hit a wall three years from now.&lt;/p&gt;  &lt;p&gt;The MEDC performed an IP landscape survey for us. They searched the breadth of patents, and patent applications, and trademarks, and those things, and provided that for Meng and me to review and consider our position before really, really digging in and spending a lot of emotional time and money on the business.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The MEDC also helped us early on create a model in Excel for tracking business financing and forecasting, forecasting our future financial needs, so that we could be proactive instead of reactive to financial limitations. We knew it wasn't going to be inexpensive to design and build a piece of equipment that's the size of two very large refrigerators that had never been built before. That type of financial-forward modeling helped us figure out when we would need to start fundraising and taking in investments. As we progressed along that, the MEDC also provided support of an attorney who reviewed contract language to make sure that we really understood various agreements that we were signing.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Right. You mentioned that you and your co-founder were first-time entrepreneurs, as you put it. Tech acumen and business acumen are very different sets of skills. I wondered, what was the process like, developing this innovative technology&amp;nbsp;while also building out a viable business plan?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Well, Meng is a brilliant individual. She is a chemical engineer who also has an MBA. Meng had fantastic training to help understand the basis of how businesses function, in addition to understanding both the engineering and the chemistry behind what we were trying to do.&lt;/p&gt;&lt;p&gt;I am an environmental toxicologist by training. I've had a longer career than Meng in that field. Over time, I have grown new offices and&amp;nbsp;established new offices for different consulting firms I've worked for. I had the experience with people, space, culture, and running a business from that side. Meng has the financial MBA knowledge basis for a business. We're both excellent chemists and engineers, and those types of things.&lt;/p&gt;&lt;p&gt;We had much of the necessary knowledge, at least to take the first steps forward. The challenge became the hard limit of 24 hours in a day and no revenue to hire any support. That's when the startup support networks like the MEDC became invaluable.&lt;/p&gt;  &lt;p&gt;It was simply impossible to do everything that needed to be done, especially while we were learning what we were doing. The MEDC and other programs provided support to take some of that load off us, but also helped us to learn to implement the new skills in an efficient manner, less stumbling.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;So many things to juggle, isn't there, in starting a company. I wondered, in that vein, could you share some successes and highlights from your journey so far? Any partnerships or projects that you're excited about that you could share with us?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; As people say, being an entrepreneur and running a startup is like a rollercoaster. You have high moments and you have very low moments when you think nothing's ever going to move forward. I'd love to talk about some of the highlights. Our machine, which we call the PFASigator.&lt;/p&gt;  &lt;p&gt;First of all, coming up with that name has a fun story behind it. The machine is, like I said, about the size of two large refrigerators. It's very large, and it breaks down PFAS in water. The machine takes in water that has PFAS in it, we add a couple of liquid chemicals, then&amp;nbsp;a very intense ultraviolet light shines on that water, which catalyzes a chemical reaction called reductive defluorination. When all of this is happening and the PFAS molecules are being broken apart to nontoxic compounds, to an outsider, it all still just looks like water with a light shining on it. But the machine is big, and it essentially eats PFAS.&lt;/p&gt;  &lt;p&gt;Meng and I were bantering, and her young, six-year-old son was in the background at the time. We were throwing names around. Thomas called out, "The PFASigator!" We were like, "Ooh, there's something there."&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; It's a great name.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; It matches what we do, and it's a memorable name. We've really had fun with that throughout. That was an early highlight, and we've stuck with that name.&lt;/p&gt;  &lt;p&gt;The next highlight I'd say was standing next to our first fully functioning PFASigator. It was big. It was all stainless steel. Meng and I had never been part of building a physical, large object like that. Just standing there, and the picture we have of us, it was exhilarating. That was a magnificent feeling.&lt;/p&gt;  &lt;p&gt;Selling our first machine was a day that everyone in the company, I think we were about eight at that point, received a bottle of champagne.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Fantastic.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise: &lt;/em&gt;For a startup to go from zero to one, they call it, you've sold nothing to you've sold something. That's a real strong milestone and was a celebration for us.&lt;/p&gt;  &lt;p&gt;I'd say most recently, Enspired has been awarded a very exciting project in Michigan. It is in the contracting phase, so I can't reveal too many details. But it is with a progressive municipality that will have our PFASigator permanently installed, destroying PFAS. That kind of movement from zero to one, and then a significant contract that will raise the visibility of the effectiveness of our approach and machine, has really buoyed our energy and is pushing us forward. It's amazing to know we are now having an impact on the sustainability of water resources. That's what we started the company for.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Awesome. You have some incredible milestones there. But it's a hard journey, as you've said as well, being an entrepreneur. I wondered, finally, what advice would you offer to burgeoning entrepreneurs given your own experience?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; I would advise that if problem solving and learning new talents do not provide sufficient intrinsic reward for a founder to be satisfied throughout what I guarantee will be a long duration effort, then that founder may need to reset their expectations, because the financial rewards of entrepreneurship are small throughout the process.&lt;/p&gt;&lt;p&gt;Meng and I put [in] some of our personal funds and took no salary, and worked harder than we ever had in our lives for at least a year and a half before we were able to take a small salary. The financial rewards are small throughout the process of being a startup. The rewards are delayed, and in many cases, for many startups, the financial rewards never materialize.&lt;/p&gt;&lt;p&gt;It's a tough journey, and you have to love being on that journey, and be intrinsically rewarded for that for the sake of the journey itself, or you'll be a very unhappy founder.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; It needs to be something you're as passionate about as I can tell you are about the work you're doing at Enspired Solutions.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; There's probably one other thing I'd like to add to that.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Of course.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Often, founders are coached about formulas for fundraising, formulas for startup success. Learning those formulas and expectations is important, but it's also important to not forget that it's your creativity and innovation and foresight that got you to the place you're in and drove you to start a company. Ultimately, people still want to see that shine through."&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; That's fantastic advice. Thank you so much, Denise.&lt;/p&gt;  &lt;p&gt;That was Dr. Denise Kay, the co-founder and CEO at Enspired Solutions, whom I spoke with from an unexpectedly sunny Brighton, England.&lt;/p&gt;&lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor and host for Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology. You can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review, and this episode was produced by Giro Studios. Thanks for listening.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/05/Denise-Kay.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Michigan Economic Development Corporation&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;What if we could permanently remove the toxic “forever chemicals” contaminating our water? That’s the driving force behind Michigan-based startup Enspired Solutions, founded by environmental toxicologist Denise Kay and chemical engineer Meng Wang. The duo left corporate consulting in the rearview mirror to take on one of the most pervasive environmental challenges: PFAS.&lt;/p&gt;    &lt;p&gt;"PFAS is referred to as a forever chemical because it is so resistant to break down,” says Kay. “It does not break down naturally in the environment, so it just circles around and around. This chemistry, which would break that cycle and break the molecule apart, could really support the health of all of us.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Basing the company in Michigan was both a strategic and a practical strategy. The state has been a leader in PFAS regulation with a startup infrastructure—buoyed by the Michigan Economic Development Corporation (MEDC)—that helped turn an ambitious vision into a viable business.&lt;/p&gt;  &lt;p&gt;From intellectual property analyses to forecasting finances and fundraising guidance, the MEDC’s programs offered Kay and Wang the resources to focus on building their PFASigator: a machine the size of two large refrigerators that uses ultraviolet light and chemistry to break down PFAS in water. In other words, “it essentially eats PFAS.”&lt;/p&gt; 
 &lt;p&gt;Despite the support from the MEDC, the journey has been far from smooth. "As people say, being an entrepreneur and running a startup is like a rollercoaster,” Kay says. “You have high moments, and you have very low moments when you think nothing's ever going to move forward."&lt;/p&gt;  &lt;p&gt;Without revenue or salaries in the early days, the co-founders had to be sustained by something greater than financial incentive.&lt;/p&gt; 
 &lt;p&gt;"If problem solving and learning new talents do not provide sufficient intrinsic reward for a founder to be satisfied throughout what I guarantee will be a long duration effort, then that founder may need to reset their expectations. Because the financial rewards of entrepreneurship are small throughout the process."&lt;/p&gt;  &lt;p&gt;Still, Kay remains optimistic about the road ahead for Enspired Solutions, for clean water innovation, and for other founders walking down a similar path. "Often, founders are coached about formulas for fundraising, formulas for startup success. Learning those formulas and expectations is important, but it's also important to not forget that it's your creativity and innovation and foresight that got you to the place you're in and drove you to start a company. Ultimately, people still want to see that shine through."&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with the Michigan Economic Development Corporation.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Full Transcript&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;Megan Tatum:&lt;/em&gt; From MIT Technology Review, I'm Megan Tatum. This is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.&lt;/p&gt;  &lt;p&gt;Today's episode is brought to you in partnership with the Michigan Economic Development Corporation.&lt;/p&gt;&lt;p&gt;Our topic today is launching a technology startup in the US state of Michigan. Building out an innovative idea into a viable product and company requires knowledge and resources that individuals might not have. That's why the Michigan Economic Development Corporation, or the MEDC, has launched an innovation campaign to support technology entrepreneurs.&lt;/p&gt;&lt;p&gt;Two words for you: startup ecosystem.&lt;/p&gt;&lt;p&gt;My guest is Dr. Denise Kay, the co-founder and CEO at Enspired Solutions, a Michigan-based startup focused on removing synthetic forever chemicals called PFAS from water.&lt;/p&gt;&lt;p&gt;Welcome, Denise.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dr. Denise Kay:&lt;/em&gt; Hi, Megan.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Hi. Thank you so much for joining us. To get us started, Denise, I wondered if we could talk about Enspired Solutions a bit more. How did the idea come about, and what does your company do?&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Well, my co-founder, Meng, and I had careers in consulting, advising clients on the fate and toxicity of chemicals in the environment. What we did was evaluate how chemicals moved through soil, water, and air, and what toxic impact they might have on humans and wildlife. That put us in a really unique position to see early on&amp;nbsp;the environmental and health ramifications of the manmade chemical PFAS in our environment.&lt;/p&gt;  &lt;p&gt;When we learned of a very novel and elegant chemistry that could effectively destroy PFAS, we could foresee the value in making this chemistry available for commercial use and the potential for a significant positive impact on maintaining healthy water resources for all of us.&lt;/p&gt;&lt;p&gt;Like you mentioned, PFAS is referred to as a forever chemical because it is so resistant to break down. It does not break down naturally in the environment, so it just circles around and around. This chemistry, which would break that cycle and break the molecule apart, could really support the health of all of us.&lt;/p&gt;&lt;p&gt;Ultimately, Meng and I quit our jobs, and we founded Enspired Solutions. Our objective was to design, manufacture, and sell commercial-scale equipment that destroys PFAS in water based on this laboratory bench-scale chemistry that had been discovered, the goal being that this toxic contaminant does not continue to circulate in our natural resources.&lt;/p&gt;&lt;p&gt;At this point, we have won an award from the EPA and Department of Defense, and proven our technology in over 200 different water samples ranging from groundwater, surface water, landfill leachate, industrial wastewater, [and] municipal wastewater. It's really everywhere. What we're seeing traction in right now is customer applications managing semiconductor waste. Groundwater and surface water around airports tend to be high in PFAS. Centralized waste disposal facilities that collect and manage PFAS-contaminated liquids. And also, even transitioning firetrucks to PFAS-free firefighting foams.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;Fantastic. That's a huge breadth of applications, incredible stuff.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Yeah.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;You launched about four years ago now. I wondered what factors made Michigan the right place to build and grow the company?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; That is something we put a lot of thought into, because I live in Michigan, and Meng lives in Illinois, so when it was just the two of us, there was even that, "Okay, what is going to be our headquarters?" We looked at a number of factors.&lt;/p&gt;  &lt;p&gt;Some of the things we considered were rentable incubator space. By incubator, I mean startup incubators or innovation centers. The startup support network, a pool of future employees, and what position the state agencies were taking regarding PFAS.&lt;/p&gt;&lt;p&gt;While thinking about all those things and investigating our communities, in Michigan, we found a space to rent where we could do chemistry experiments in an incubator environment. Somewhere where we were surrounded by other entrepreneurs, which we knew was something we had to learn how to do. We were great chemists, but we knew that surrounding ourselves with those skills that could be a gap for us was going to be helpful.&lt;/p&gt;&lt;p&gt;Also, we know that Michigan has moved much faster than other states in identifying PFAS sources in the environment and regulating its presence. This combination was something we knew would be the right place for starting our business and having success.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; It was a perfect setting for those two reasons. What were the first stages of your journey working with the Michigan Economic Development Corporation, the MEDC?&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Well, both my co-founder, Meng, and I are first-time entrepreneurs. MEDC was one of the first resources I reached out to, starting from a Google search. They were an information resource we turned to initially, and then again and again for learning some fundamental skills. And receiving one-on-one expert mentorship for things like business contracts, understanding intellectual property landscapes, tracking and forecasting our business finances, and even how to approach fundraising.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Wow. It sounds like they were an invaluable resource in those early days. How did early-stage research and development progress from that point? What were the key MEDC services and programs you used to get started?&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Well, our business is based on cutting-edge science, truly cutting-edge science. Understanding the intellectual property landscape, which is a term used to describe intellectual property, patents, trademarks, trade secrets that are related to the science we were founding our business on, it was very important. So that we knew we were starting on a path, that we wouldn't hit a wall three years from now.&lt;/p&gt;  &lt;p&gt;The MEDC performed an IP landscape survey for us. They searched the breadth of patents, and patent applications, and trademarks, and those things, and provided that for Meng and me to review and consider our position before really, really digging in and spending a lot of emotional time and money on the business.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The MEDC also helped us early on create a model in Excel for tracking business financing and forecasting, forecasting our future financial needs, so that we could be proactive instead of reactive to financial limitations. We knew it wasn't going to be inexpensive to design and build a piece of equipment that's the size of two very large refrigerators that had never been built before. That type of financial-forward modeling helped us figure out when we would need to start fundraising and taking in investments. As we progressed along that, the MEDC also provided support of an attorney who reviewed contract language to make sure that we really understood various agreements that we were signing.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Right. You mentioned that you and your co-founder were first-time entrepreneurs, as you put it. Tech acumen and business acumen are very different sets of skills. I wondered, what was the process like, developing this innovative technology&amp;nbsp;while also building out a viable business plan?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Well, Meng is a brilliant individual. She is a chemical engineer who also has an MBA. Meng had fantastic training to help understand the basis of how businesses function, in addition to understanding both the engineering and the chemistry behind what we were trying to do.&lt;/p&gt;&lt;p&gt;I am an environmental toxicologist by training. I've had a longer career than Meng in that field. Over time, I have grown new offices and&amp;nbsp;established new offices for different consulting firms I've worked for. I had the experience with people, space, culture, and running a business from that side. Meng has the financial MBA knowledge basis for a business. We're both excellent chemists and engineers, and those types of things.&lt;/p&gt;&lt;p&gt;We had much of the necessary knowledge, at least to take the first steps forward. The challenge became the hard limit of 24 hours in a day and no revenue to hire any support. That's when the startup support networks like the MEDC became invaluable.&lt;/p&gt;  &lt;p&gt;It was simply impossible to do everything that needed to be done, especially while we were learning what we were doing. The MEDC and other programs provided support to take some of that load off us, but also helped us to learn to implement the new skills in an efficient manner, less stumbling.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan: &lt;/em&gt;So many things to juggle, isn't there, in starting a company. I wondered, in that vein, could you share some successes and highlights from your journey so far? Any partnerships or projects that you're excited about that you could share with us?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; As people say, being an entrepreneur and running a startup is like a rollercoaster. You have high moments and you have very low moments when you think nothing's ever going to move forward. I'd love to talk about some of the highlights. Our machine, which we call the PFASigator.&lt;/p&gt;  &lt;p&gt;First of all, coming up with that name has a fun story behind it. The machine is, like I said, about the size of two large refrigerators. It's very large, and it breaks down PFAS in water. The machine takes in water that has PFAS in it, we add a couple of liquid chemicals, then&amp;nbsp;a very intense ultraviolet light shines on that water, which catalyzes a chemical reaction called reductive defluorination. When all of this is happening and the PFAS molecules are being broken apart to nontoxic compounds, to an outsider, it all still just looks like water with a light shining on it. But the machine is big, and it essentially eats PFAS.&lt;/p&gt;  &lt;p&gt;Meng and I were bantering, and her young, six-year-old son was in the background at the time. We were throwing names around. Thomas called out, "The PFASigator!" We were like, "Ooh, there's something there."&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; It's a great name.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; It matches what we do, and it's a memorable name. We've really had fun with that throughout. That was an early highlight, and we've stuck with that name.&lt;/p&gt;  &lt;p&gt;The next highlight I'd say was standing next to our first fully functioning PFASigator. It was big. It was all stainless steel. Meng and I had never been part of building a physical, large object like that. Just standing there, and the picture we have of us, it was exhilarating. That was a magnificent feeling.&lt;/p&gt;  &lt;p&gt;Selling our first machine was a day that everyone in the company, I think we were about eight at that point, received a bottle of champagne.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Fantastic.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise: &lt;/em&gt;For a startup to go from zero to one, they call it, you've sold nothing to you've sold something. That's a real strong milestone and was a celebration for us.&lt;/p&gt;  &lt;p&gt;I'd say most recently, Enspired has been awarded a very exciting project in Michigan. It is in the contracting phase, so I can't reveal too many details. But it is with a progressive municipality that will have our PFASigator permanently installed, destroying PFAS. That kind of movement from zero to one, and then a significant contract that will raise the visibility of the effectiveness of our approach and machine, has really buoyed our energy and is pushing us forward. It's amazing to know we are now having an impact on the sustainability of water resources. That's what we started the company for.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Awesome. You have some incredible milestones there. But it's a hard journey, as you've said as well, being an entrepreneur. I wondered, finally, what advice would you offer to burgeoning entrepreneurs given your own experience?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; I would advise that if problem solving and learning new talents do not provide sufficient intrinsic reward for a founder to be satisfied throughout what I guarantee will be a long duration effort, then that founder may need to reset their expectations, because the financial rewards of entrepreneurship are small throughout the process.&lt;/p&gt;&lt;p&gt;Meng and I put [in] some of our personal funds and took no salary, and worked harder than we ever had in our lives for at least a year and a half before we were able to take a small salary. The financial rewards are small throughout the process of being a startup. The rewards are delayed, and in many cases, for many startups, the financial rewards never materialize.&lt;/p&gt;&lt;p&gt;It's a tough journey, and you have to love being on that journey, and be intrinsically rewarded for that for the sake of the journey itself, or you'll be a very unhappy founder.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; It needs to be something you're as passionate about as I can tell you are about the work you're doing at Enspired Solutions.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; There's probably one other thing I'd like to add to that.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; Of course.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Denise:&lt;/em&gt; Often, founders are coached about formulas for fundraising, formulas for startup success. Learning those formulas and expectations is important, but it's also important to not forget that it's your creativity and innovation and foresight that got you to the place you're in and drove you to start a company. Ultimately, people still want to see that shine through."&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan:&lt;/em&gt; That's fantastic advice. Thank you so much, Denise.&lt;/p&gt;  &lt;p&gt;That was Dr. Denise Kay, the co-founder and CEO at Enspired Solutions, whom I spoke with from an unexpectedly sunny Brighton, England.&lt;/p&gt;&lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor and host for Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology. You can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review, and this episode was produced by Giro Studios. Thanks for listening.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/22/1117638/fighting-forever-chemicals-and-startup-fatigue/</guid><pubDate>Tue, 22 Jul 2025 17:38:18 +0000</pubDate></item><item><title>[NEW] iOS 26 beta 4 arrives, with Liquid Glass tweaks and AI news summaries (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/ios-26-beta-4-arrives-with-liquid-glass-tweaks-and-ai-news-summaries/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Apple-WWDC25-iOS-26-hero-250609.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Tuesday released the fourth developer beta of its next big software update, iOS 26, which brings with it slight changes to its Liquid Glass redesign and the re-introduction of AI-powered notification summaries for news, among other updates.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout arrived just ahead of the iOS 26 public beta, which is expected to launch later this week.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The developer betas are meant to give mobile app makers time to test their apps with Apple’s new software so they’re ready for the public launch of the new operating system in the next few months. Because of consumer demand for early releases, Apple for years has been offering public betas following its Worldwide Developers Conference in June. This allows iPhone owners to also get their hands on the updated software before its wider, global launch, but with fewer stability issues and bugs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest developer beta (iOS 26 beta 4) largely reflects what users can expect in the coming public beta. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;iOS 26 beta 4 introduces a new “Welcome” splash screen when you first update the software, plus introductory screens for various features, like Siri and its AI-powered notification summaries and prioritization options, as well as for iOS 26’s revamped Camera app.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Apple was forced to put its AI notification summaries on pause after complaints by the BBC, which said the feature had misrepresented one of its headlines. The AI-powered news summary claimed Luigi Mangione, the person charged with the murder of UnitedHealthcare CEO Brian Thompson, had died by suicide, which wasn’t true. As a result, Apple said a software update would be released to clarify when the text shows an AI summarization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The setup screen for this AI summarization feature, still in beta, includes a warning message under the “News &amp;amp; Entertainment” section. Here, Apple notes that “Summarization may change the meaning of the original headlines” and advises users to “Verify information.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other early adopters of the new beta also found that Apple is continuing to refine its user interface redesign known as Liquid Glass. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While beta 3 pulled back on some of the more transparent elements in some apps, beta 4 reverses things again. Testers have pointed to updates in apps like the App Store, Photos, Apple Music, Weather, and elsewhere, and noted that the Notification Center also adds a dynamic tint as you scroll.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;iOS 26 Beta 4 brings a more visually comfortable change when viewing notifications.&lt;/p&gt;&lt;p&gt;Now, the wallpaper gets darker right when we enter the notification centre. pic.twitter.com/UaP1FW7RvQ&lt;/p&gt;— Alvin (@sondesix) July 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The updated software additionally includes a new dynamic wallpaper, which changes colors, and new CarPlay wallpapers.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Release notes for beta 4 had not been published on Apple’s Developer website at the time of publication, so there is likely more to be spotted, especially in terms of minor bug fixes and performance improvements.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alongside today’s release, Apple also launched new versions of its other betas, including iPadOS 26 beta 4, macOS 26 beta 4, watchOS 26 beta 4, tvOS 26 beta 4, visionOS 26 beta 4, and Xcode 26 beta 4.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Apple-WWDC25-iOS-26-hero-250609.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Tuesday released the fourth developer beta of its next big software update, iOS 26, which brings with it slight changes to its Liquid Glass redesign and the re-introduction of AI-powered notification summaries for news, among other updates.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout arrived just ahead of the iOS 26 public beta, which is expected to launch later this week.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The developer betas are meant to give mobile app makers time to test their apps with Apple’s new software so they’re ready for the public launch of the new operating system in the next few months. Because of consumer demand for early releases, Apple for years has been offering public betas following its Worldwide Developers Conference in June. This allows iPhone owners to also get their hands on the updated software before its wider, global launch, but with fewer stability issues and bugs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The latest developer beta (iOS 26 beta 4) largely reflects what users can expect in the coming public beta. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;iOS 26 beta 4 introduces a new “Welcome” splash screen when you first update the software, plus introductory screens for various features, like Siri and its AI-powered notification summaries and prioritization options, as well as for iOS 26’s revamped Camera app.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Apple was forced to put its AI notification summaries on pause after complaints by the BBC, which said the feature had misrepresented one of its headlines. The AI-powered news summary claimed Luigi Mangione, the person charged with the murder of UnitedHealthcare CEO Brian Thompson, had died by suicide, which wasn’t true. As a result, Apple said a software update would be released to clarify when the text shows an AI summarization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The setup screen for this AI summarization feature, still in beta, includes a warning message under the “News &amp;amp; Entertainment” section. Here, Apple notes that “Summarization may change the meaning of the original headlines” and advises users to “Verify information.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other early adopters of the new beta also found that Apple is continuing to refine its user interface redesign known as Liquid Glass. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While beta 3 pulled back on some of the more transparent elements in some apps, beta 4 reverses things again. Testers have pointed to updates in apps like the App Store, Photos, Apple Music, Weather, and elsewhere, and noted that the Notification Center also adds a dynamic tint as you scroll.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;iOS 26 Beta 4 brings a more visually comfortable change when viewing notifications.&lt;/p&gt;&lt;p&gt;Now, the wallpaper gets darker right when we enter the notification centre. pic.twitter.com/UaP1FW7RvQ&lt;/p&gt;— Alvin (@sondesix) July 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The updated software additionally includes a new dynamic wallpaper, which changes colors, and new CarPlay wallpapers.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Release notes for beta 4 had not been published on Apple’s Developer website at the time of publication, so there is likely more to be spotted, especially in terms of minor bug fixes and performance improvements.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alongside today’s release, Apple also launched new versions of its other betas, including iPadOS 26 beta 4, macOS 26 beta 4, watchOS 26 beta 4, tvOS 26 beta 4, visionOS 26 beta 4, and Xcode 26 beta 4.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/ios-26-beta-4-arrives-with-liquid-glass-tweaks-and-ai-news-summaries/</guid><pubDate>Tue, 22 Jul 2025 18:47:14 +0000</pubDate></item><item><title>[NEW] Surprising no one, new research says AI Overviews cause massive drop in search clicks (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/research-shows-google-ai-overviews-reduce-website-clicks-by-almost-half/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pew Research Center analysis shows how hard AI is hitting web traffic.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Overview" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Search_SocialShare_7gpZ6Zv.width-1300-640x360.jpg" width="640" /&gt;
                  &lt;img alt="AI Overview" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Search_SocialShare_7gpZ6Zv.width-1300-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google's search results have undergone a seismic shift over the past year as AI fever has continued to escalate among the tech giants. Nowhere is this change more apparent than right at the top of Google's storied results page, which is now home to AI Overviews. Google contends these Gemini-based answers don't take traffic away from websites, but a new analysis from the Pew Research Center says otherwise. Its analysis shows that searches with AI summaries reduce clicks, and their prevalence is increasing.&lt;/p&gt;
&lt;p&gt;Google began testing AI Overviews as the "search generative experience" in May 2023, and just a year later, they were an official part of the search engine results page (SERP). Many sites (including this one) have noticed changes to their traffic in the wake of this move, but Google has brushed off concerns about how this could affect the sites from which it collects all that data.&lt;/p&gt;
&lt;p&gt;SEO experts have disagreed with Google's stance on how AI affects web traffic, and the newly released Pew study backs them up. The Pew Research Center analyzed data from 900 users of the Ipsos KnowledgePanel collected in March 2025. The analysis shows that among the test group, users were much less likely to click on search results when the page included an AI Overview.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2107352 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Pew AI Overviews stats" class="fullwidth full" height="1592" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Pew-AI.png" width="1792" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Pre Research Center

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Pew reports that searches without an AI answer resulted in a click rate of 15 percent. On SERPs with AI Overviews, the rate of clicks to other sites drops by almost half, to 8 percent. Google has also, on several occasions, claimed that people click on the links cited in AI Overviews, but Pew found that just 1 percent of AI Overviews produced a click on a source. These sources are most frequently Wikipedia, YouTube, and Reddit, which collectively account for 15 percent of all AI sources.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And perhaps more troubling, Google users are more likely to end their browsing session after seeing an AI Overview. That suggests that many people are seeing information generated by a robot, and their investigation stops there. Unfortunately for these people, all forms of generative AI are prone to "hallucinations" that cause them to provide incorrect information. So more people could be walking away from a search with the wrong information.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2098864 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="AI overview on phone" class="fullwidth full" height="1824" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/AI-Overviews.jpg" width="2000" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Overviews are integrated with Google's results, and they are appearing on more searches all the time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;This problem is unlikely to improve over time. Since launching AI Overviews, Google has repeatedly expanded the number of searches that get robot summaries. The Pew Research Center says that about 1 in 5 searches now have AI Overviews. Generally, the more words in a search, the more likely it is to trigger an AI Overview, and that's especially true for searches phrased as questions. The research shows that 60 percent of questions and 36 percent of full-sentence searches are answered by the AI.&lt;/p&gt;
&lt;p&gt;This research provides more evidence that Google's use of AI is changing the way people gather information and interact with search results. The trends are bad for web publishing, but Google's profits have never been higher. Funny how that works.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pew Research Center analysis shows how hard AI is hitting web traffic.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Overview" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Search_SocialShare_7gpZ6Zv.width-1300-640x360.jpg" width="640" /&gt;
                  &lt;img alt="AI Overview" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Search_SocialShare_7gpZ6Zv.width-1300-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google's search results have undergone a seismic shift over the past year as AI fever has continued to escalate among the tech giants. Nowhere is this change more apparent than right at the top of Google's storied results page, which is now home to AI Overviews. Google contends these Gemini-based answers don't take traffic away from websites, but a new analysis from the Pew Research Center says otherwise. Its analysis shows that searches with AI summaries reduce clicks, and their prevalence is increasing.&lt;/p&gt;
&lt;p&gt;Google began testing AI Overviews as the "search generative experience" in May 2023, and just a year later, they were an official part of the search engine results page (SERP). Many sites (including this one) have noticed changes to their traffic in the wake of this move, but Google has brushed off concerns about how this could affect the sites from which it collects all that data.&lt;/p&gt;
&lt;p&gt;SEO experts have disagreed with Google's stance on how AI affects web traffic, and the newly released Pew study backs them up. The Pew Research Center analyzed data from 900 users of the Ipsos KnowledgePanel collected in March 2025. The analysis shows that among the test group, users were much less likely to click on search results when the page included an AI Overview.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2107352 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Pew AI Overviews stats" class="fullwidth full" height="1592" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Pew-AI.png" width="1792" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Pre Research Center

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Pew reports that searches without an AI answer resulted in a click rate of 15 percent. On SERPs with AI Overviews, the rate of clicks to other sites drops by almost half, to 8 percent. Google has also, on several occasions, claimed that people click on the links cited in AI Overviews, but Pew found that just 1 percent of AI Overviews produced a click on a source. These sources are most frequently Wikipedia, YouTube, and Reddit, which collectively account for 15 percent of all AI sources.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And perhaps more troubling, Google users are more likely to end their browsing session after seeing an AI Overview. That suggests that many people are seeing information generated by a robot, and their investigation stops there. Unfortunately for these people, all forms of generative AI are prone to "hallucinations" that cause them to provide incorrect information. So more people could be walking away from a search with the wrong information.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2098864 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="AI overview on phone" class="fullwidth full" height="1824" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/AI-Overviews.jpg" width="2000" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Overviews are integrated with Google's results, and they are appearing on more searches all the time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;This problem is unlikely to improve over time. Since launching AI Overviews, Google has repeatedly expanded the number of searches that get robot summaries. The Pew Research Center says that about 1 in 5 searches now have AI Overviews. Generally, the more words in a search, the more likely it is to trigger an AI Overview, and that's especially true for searches phrased as questions. The research shows that 60 percent of questions and 36 percent of full-sentence searches are answered by the AI.&lt;/p&gt;
&lt;p&gt;This research provides more evidence that Google's use of AI is changing the way people gather information and interact with search results. The trends are bad for web publishing, but Google's profits have never been higher. Funny how that works.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/research-shows-google-ai-overviews-reduce-website-clicks-by-almost-half/</guid><pubDate>Tue, 22 Jul 2025 18:49:46 +0000</pubDate></item><item><title>[NEW] Apple Intelligence news summaries are back, with a big red disclaimer (AI – Ars Technica)</title><link>https://arstechnica.com/apple/2025/07/apple-intelligence-news-summaries-are-back-with-a-big-red-disclaimer/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Apple disabled news summaries earlier this year after they mangled headlines.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1600" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/apple-summary-26.jpeg" width="2400" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Apple Intelligence notifications summaries for news apps are back in the latest iOS 26, macOS 26, and iPadOS 26 developer betas.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Andrew Cunningham

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Apple has released the fourth developer betas of iOS 26, iPadOS 26, macOS 26 and its other next-generation software updates today. And along with their other changes and fixes, the new builds are bringing back Apple Intelligence notification summaries for news apps.&lt;/p&gt;
&lt;p&gt;Apple disabled news notification summaries as part of the iOS 18.3 update in January. Incorrect summaries circulating on social media prompted news organizations to complain to Apple, particularly after one summary said that Luigi Mangione, alleged murderer of UnitedHealthcare CEO Brian Thompson, had died by suicide (he had not and has not).&lt;/p&gt;
&lt;p&gt;Upon installing the new update, users of Apple Intelligence-compatible devices will be asked to enable or disable three broad categories of notifications: those for "News &amp;amp; Entertainment" apps, for "Communication &amp;amp; Social" apps, and for all other apps. The operating systems will list sample apps based on what you currently have installed on your device.&lt;/p&gt;
&lt;p&gt;All Apple Intelligence notification summaries continue to be listed as "beta," but Apple's main change here is a big red disclaimer when you enable News &amp;amp; Entertainment notification summaries, pointing out that "summarization may change the meaning of the original headlines." The notifications also get a special "summarized by Apple Intelligence" caption to further distinguish them from regular, unadulterated notifications.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Apple Intelligence will also reportedly see under-the-hood improvements in the new updates, relative to the first-generation versions in iOS 18. Apple executives have talked about underlying architectural improvements that will allow it to finally roll out a new version of Siri, for example, and notification summaries should also benefit (when contacted by Ars for comment, Apple wouldn't share more specific information about exactly what has been changed or improved).&lt;/p&gt;
&lt;p&gt;In any case, the disclaimer makes it clear that Apple's fix for the problem involves warning users to expect incorrect summaries and allowing them to turn news summaries off, rather than guaranteeing that summaries will always be correct.&lt;/p&gt;
&lt;p&gt;Other changes that Apple has made to distinguish summarized notifications from regular ones should remain in effect in the new OS versions: Summaries will get italicized text and a small icon denoting that you're looking at summaries rather than the original notifications, and users can tap a stack of summarized notifications to see all of the originals.&lt;/p&gt;
&lt;p&gt;Apple announced back in June that the first public betas of the new operating systems would be available in July. With only around a week and a half left in the month, it's highly likely that the first public beta will be similar to the developer beta build released today. The updates will be released to the general public sometime this fall—usually in September or October if Apple adheres to its usual timeline.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Apple disabled news summaries earlier this year after they mangled headlines.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1600" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/apple-summary-26.jpeg" width="2400" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Apple Intelligence notifications summaries for news apps are back in the latest iOS 26, macOS 26, and iPadOS 26 developer betas.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Andrew Cunningham

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Apple has released the fourth developer betas of iOS 26, iPadOS 26, macOS 26 and its other next-generation software updates today. And along with their other changes and fixes, the new builds are bringing back Apple Intelligence notification summaries for news apps.&lt;/p&gt;
&lt;p&gt;Apple disabled news notification summaries as part of the iOS 18.3 update in January. Incorrect summaries circulating on social media prompted news organizations to complain to Apple, particularly after one summary said that Luigi Mangione, alleged murderer of UnitedHealthcare CEO Brian Thompson, had died by suicide (he had not and has not).&lt;/p&gt;
&lt;p&gt;Upon installing the new update, users of Apple Intelligence-compatible devices will be asked to enable or disable three broad categories of notifications: those for "News &amp;amp; Entertainment" apps, for "Communication &amp;amp; Social" apps, and for all other apps. The operating systems will list sample apps based on what you currently have installed on your device.&lt;/p&gt;
&lt;p&gt;All Apple Intelligence notification summaries continue to be listed as "beta," but Apple's main change here is a big red disclaimer when you enable News &amp;amp; Entertainment notification summaries, pointing out that "summarization may change the meaning of the original headlines." The notifications also get a special "summarized by Apple Intelligence" caption to further distinguish them from regular, unadulterated notifications.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Apple Intelligence will also reportedly see under-the-hood improvements in the new updates, relative to the first-generation versions in iOS 18. Apple executives have talked about underlying architectural improvements that will allow it to finally roll out a new version of Siri, for example, and notification summaries should also benefit (when contacted by Ars for comment, Apple wouldn't share more specific information about exactly what has been changed or improved).&lt;/p&gt;
&lt;p&gt;In any case, the disclaimer makes it clear that Apple's fix for the problem involves warning users to expect incorrect summaries and allowing them to turn news summaries off, rather than guaranteeing that summaries will always be correct.&lt;/p&gt;
&lt;p&gt;Other changes that Apple has made to distinguish summarized notifications from regular ones should remain in effect in the new OS versions: Summaries will get italicized text and a small icon denoting that you're looking at summaries rather than the original notifications, and users can tap a stack of summarized notifications to see all of the originals.&lt;/p&gt;
&lt;p&gt;Apple announced back in June that the first public betas of the new operating systems would be available in July. With only around a week and a half left in the month, it's highly likely that the first public beta will be similar to the developer beta build released today. The updates will be released to the general public sometime this fall—usually in September or October if Apple adheres to its usual timeline.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/apple/2025/07/apple-intelligence-news-summaries-are-back-with-a-big-red-disclaimer/</guid><pubDate>Tue, 22 Jul 2025 20:24:41 +0000</pubDate></item><item><title>[NEW] OpenAI agreed to pay Oracle $30B a year for data center services (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/openai-agreed-to-pay-oracle-30b-a-year-for-data-center-services/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/07/gettyimages-494272836.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI was the company that signed a $30 billion per year deal with Oracle for data center services, disclosed last month, The Wall Street Journal reported on Monday. Now, OpenAI CEO Sam Altman has confirmed the details of the contract (but not the dollar amount) in an X post on Tuesday and in a company blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To recap, on June 30, Oracle disclosed in an SEC filing that it had signed a cloud deal that would generate $30 billion a year in revenue. However, the company didn’t say who it was with or for what services. The news caused Oracle’s stock to hit an all-time high, making its founder and CTO, Larry Ellison, the second richest person in the world, according to Bloomberg.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speculation on the identity of the customer ensued as people wondered what company could possibly need a fresh $30 billion a year in data center services. For comparison, Oracle collectively sold $24.5 billion worth of cloud services in its fiscal 2025 to all customers combined, it reported in June.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has now explained that this Oracle deal is for 4.5 gigawatts of capacity as part of Stargate, the $500 billion data-center-building project OpenAI, Oracle, and SoftBank announced in January. (Apparently, the $30 billion deal does not involve SoftBank.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The WSJ reports 4.5 gigawatts is the equivalent of two Hoover Dams, enough power for about four million homes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t a straightforward win for Oracle. OpenAI and Oracle still have to build this monster data center, which will be a costly endeavor, both in cash and in energy. They are doing so at what OpenAI called the Stargate I site in Abilene, Texas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Oracle spent $21.2 billion on capital expenditures in its last fiscal year, CEO Safra Catz reported in June, and it expects to spend another $25 billion this year, she said. So, nearly $50 billion, largely spent on data centers (and that doesn’t include land purchases, she said) in two years. Although, to be clear, that money also supports Oracle’s existing customers, in addition to OpenAI’s demands.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;One final interesting part to note about all of this: Last month, Altman said that OpenAI recently hit $10 billion in annual recurring revenue, up from around $5.5 billion last year. This single commitment to Oracle is already triple per year what it is currently bringing in and doesn’t include all of the company’s other expenses, including its current data center commitments.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/07/gettyimages-494272836.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI was the company that signed a $30 billion per year deal with Oracle for data center services, disclosed last month, The Wall Street Journal reported on Monday. Now, OpenAI CEO Sam Altman has confirmed the details of the contract (but not the dollar amount) in an X post on Tuesday and in a company blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To recap, on June 30, Oracle disclosed in an SEC filing that it had signed a cloud deal that would generate $30 billion a year in revenue. However, the company didn’t say who it was with or for what services. The news caused Oracle’s stock to hit an all-time high, making its founder and CTO, Larry Ellison, the second richest person in the world, according to Bloomberg.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Speculation on the identity of the customer ensued as people wondered what company could possibly need a fresh $30 billion a year in data center services. For comparison, Oracle collectively sold $24.5 billion worth of cloud services in its fiscal 2025 to all customers combined, it reported in June.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has now explained that this Oracle deal is for 4.5 gigawatts of capacity as part of Stargate, the $500 billion data-center-building project OpenAI, Oracle, and SoftBank announced in January. (Apparently, the $30 billion deal does not involve SoftBank.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The WSJ reports 4.5 gigawatts is the equivalent of two Hoover Dams, enough power for about four million homes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t a straightforward win for Oracle. OpenAI and Oracle still have to build this monster data center, which will be a costly endeavor, both in cash and in energy. They are doing so at what OpenAI called the Stargate I site in Abilene, Texas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Oracle spent $21.2 billion on capital expenditures in its last fiscal year, CEO Safra Catz reported in June, and it expects to spend another $25 billion this year, she said. So, nearly $50 billion, largely spent on data centers (and that doesn’t include land purchases, she said) in two years. Although, to be clear, that money also supports Oracle’s existing customers, in addition to OpenAI’s demands.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;One final interesting part to note about all of this: Last month, Altman said that OpenAI recently hit $10 billion in annual recurring revenue, up from around $5.5 billion last year. This single commitment to Oracle is already triple per year what it is currently bringing in and doesn’t include all of the company’s other expenses, including its current data center commitments.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/openai-agreed-to-pay-oracle-30b-a-year-for-data-center-services/</guid><pubDate>Tue, 22 Jul 2025 20:36:31 +0000</pubDate></item><item><title>[NEW] Amazon acquires Bee, the AI wearable that records everything you say (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/22/amazon-acquires-bee-the-ai-wearable-that-records-everything-you-say/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/07/HD.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has acquired the AI wearables startup Bee, according to a LinkedIn post by Bee co-founder Maria de Lourdes Zollo. Amazon confirmed the acquisition to TechCrunch but noted that the deal has not yet closed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bee, which raised $7 million last year, makes both a stand-alone Fitbit-like bracelet (which retails for $49.99, plus a $19-per-month subscription) and an Apple Watch app. The product records everything it hears — unless the user manually mutes it — with the goal of listening to conversations to create reminders and to-do lists for the user.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zollo told TechCrunch last year that the company hopes to create a “cloud phone,” or a mirror of your phone that gives the personal Bee device access to the user’s accounts and notifications, making it possible to get reminders about events or send messages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We believe everyone should have access to a personal, ambient intelligence that feels less like a tool and more like a trusted companion. One that helps you reflect, remember, and move through the world more freely,” Bee claims on its website.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other companies like Rabbit and Humane AI have tried to make AI-enabled wearables like this but have not found much success thus far. But at a $50 price point, Bee’s devices are more cost-accessible to a curious consumer who doesn’t want to make a big financial commitment. (The ill-fated Humane AI Pin was $499.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Amazon spokesperson told TechCrunch that Bee employees received offers to join Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This acquisition signals Amazon’s interest in developing wearable AI devices, a different avenue from its voice-controlled home assistant products like its line of Echo speakers. ChatGPT maker OpenAI is working on its own AI hardware, while Meta is integrating its AI into its smart glasses. Apple is rumored to be working on AI-powered smart glasses as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These products come with a number of security and privacy risks, given that they record everything around them; different companies’ policies will vary in terms of how voice recordings are processed, stored, and used for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In its current privacy policies, Bee says that users can delete their data at any time and that audio recordings are not saved, stored, or used for AI training. The app does store data that the AI learns about the user, however, which is how it can function as an assistant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bee previously indicated that it planned to only record the voices of people who have verbally consented. Bee also says it’s working on a feature to allow users to define boundaries — both based on topic and location — that will automatically pause the device’s learning. The company noted that it plans to build on-device AI processing, which generally poses less of a privacy risk than processing data in the cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s not clear if these policies will change as Bee is integrated into Amazon, however — and Amazon has a mixed record on the handling of user data from its customers’ devices. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the past, Amazon shared footage with law enforcement from people’s personal Ring security cameras, with neither the owner’s consent, nor a warrant. Ring also settled claims in 2023 brought by the Federal Trade Commission that employees and contractors had broad and unrestricted access to customers’ videos.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/07/HD.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has acquired the AI wearables startup Bee, according to a LinkedIn post by Bee co-founder Maria de Lourdes Zollo. Amazon confirmed the acquisition to TechCrunch but noted that the deal has not yet closed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bee, which raised $7 million last year, makes both a stand-alone Fitbit-like bracelet (which retails for $49.99, plus a $19-per-month subscription) and an Apple Watch app. The product records everything it hears — unless the user manually mutes it — with the goal of listening to conversations to create reminders and to-do lists for the user.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zollo told TechCrunch last year that the company hopes to create a “cloud phone,” or a mirror of your phone that gives the personal Bee device access to the user’s accounts and notifications, making it possible to get reminders about events or send messages.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We believe everyone should have access to a personal, ambient intelligence that feels less like a tool and more like a trusted companion. One that helps you reflect, remember, and move through the world more freely,” Bee claims on its website.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other companies like Rabbit and Humane AI have tried to make AI-enabled wearables like this but have not found much success thus far. But at a $50 price point, Bee’s devices are more cost-accessible to a curious consumer who doesn’t want to make a big financial commitment. (The ill-fated Humane AI Pin was $499.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Amazon spokesperson told TechCrunch that Bee employees received offers to join Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This acquisition signals Amazon’s interest in developing wearable AI devices, a different avenue from its voice-controlled home assistant products like its line of Echo speakers. ChatGPT maker OpenAI is working on its own AI hardware, while Meta is integrating its AI into its smart glasses. Apple is rumored to be working on AI-powered smart glasses as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These products come with a number of security and privacy risks, given that they record everything around them; different companies’ policies will vary in terms of how voice recordings are processed, stored, and used for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In its current privacy policies, Bee says that users can delete their data at any time and that audio recordings are not saved, stored, or used for AI training. The app does store data that the AI learns about the user, however, which is how it can function as an assistant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bee previously indicated that it planned to only record the voices of people who have verbally consented. Bee also says it’s working on a feature to allow users to define boundaries — both based on topic and location — that will automatically pause the device’s learning. The company noted that it plans to build on-device AI processing, which generally poses less of a privacy risk than processing data in the cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s not clear if these policies will change as Bee is integrated into Amazon, however — and Amazon has a mixed record on the handling of user data from its customers’ devices. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the past, Amazon shared footage with law enforcement from people’s personal Ring security cameras, with neither the owner’s consent, nor a warrant. Ring also settled claims in 2023 brought by the Federal Trade Commission that employees and contractors had broad and unrestricted access to customers’ videos.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/22/amazon-acquires-bee-the-ai-wearable-that-records-everything-you-say/</guid><pubDate>Tue, 22 Jul 2025 20:51:41 +0000</pubDate></item><item><title>[NEW] Alibaba’s new open source Qwen3-235B-A22B-2507 beats Kimi-2 and offers low compute version (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/alibabas-new-open-source-qwen3-235b-a22b-2507-beats-kimi-2-and-offers-low-compute-version/</link><description>&lt;p&gt;Chinese e-commerce giant Alibaba has made waves globally in the tech and business communities with its own family of “Qwen” generative AI large language models, beginning with the launch of the original Tongyi Qianwen LLM chatbot in April 2023 through the release of Qwen 3 in April 2025.&lt;/p&gt;&lt;p&gt;Well, not only are its models powerful and score high on third-party benchmark tests at completing math, science, reasoning, and writing tasks, but for the most part, they’ve been released under permissive open source licensing terms, allowing organizations and enterprises to download them, customize them, run them, and generally use them for all variety of purposes, even commercial. Think of them as an alternative to DeepSeek. &lt;/p&gt;&lt;p&gt;This week, Alibaba’s “Qwen Team,” as its AI division is known, released the latest updates to its Qwen family, and they’re already attracting attention once more from AI power users in the West for their top performance, in one case, edging out even the new Kimi-2 model from rival Chinese AI startup Moonshot released in mid-July 2025.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The new Qwen3-235B-A22B-2507-Instruct model — released on AI code sharing community Hugging Face alongside a “floating point 8” or FP8 version, which we’ll cover more in-depth below — improves from the original Qwen 3 on reasoning tasks, factual accuracy, and multilingual understanding. It also outperforms Claude Opus 4’s “non-thinking” version. &lt;/p&gt;



&lt;p&gt;The new Qwen3 model update also delivers better coding results, alignment with user preferences, and long-context handling, according to its creators. But that’s not all…&lt;/p&gt;



&lt;p&gt;Read on for what else it offers enterprise users and technical decision-makers.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-fp8-version-lets-enterprises-run-qwen-3-with-far-less-memory-and-far-less-compute"&gt;FP8 version lets enterprises run Qwen 3 with far less memory and far less compute&lt;/h2&gt;



&lt;p&gt;In addition to the new Qwen3-235B-A22B-2507 model, the Qwen Team released an “FP8” version, which stands for &lt;strong&gt;8-bit floating point&lt;/strong&gt;, a format that compresses the model’s numerical operations to use less memory and processing power — without noticeably affecting its performance. &lt;/p&gt;



&lt;p&gt;In practice, this means organizations can run a model with Qwen3’s capabilities on smaller, less expensive hardware or more efficiently in the cloud. The result is faster response times, lower energy costs, and the ability to scale deployments without needing massive infrastructure.&lt;/p&gt;



&lt;p&gt;This makes the FP8 model especially attractive for production environments with tight latency or cost constraints. Teams can scale Qwen3’s capabilities to single-node GPU instances or local development machines, avoiding the need for massive multi-GPU clusters. It also lowers the barrier to private fine-tuning and on-premises deployments, where infrastructure resources are finite and total cost of ownership matters.&lt;/p&gt;



&lt;p&gt;Even though Qwen team didn’t release official calculations, comparisons to similar FP8 quantized deployments suggest the efficiency savings are substantial. Here’s a practical illustration:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Metric&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;FP16 Version (Instruct)&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;FP8 Version (Instruct-FP8)&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;GPU Memory Use&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~88 GB&lt;/td&gt;&lt;td&gt;~30 GB&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Inference Speed&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~30–40 tokens/sec&lt;/td&gt;&lt;td&gt;~60–70 tokens/sec&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Power Draw&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;High&lt;/td&gt;&lt;td&gt;~30–50% lower&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Number of GPUs Needed&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8× A100s or similar&lt;/td&gt;&lt;td&gt;4× A100s or fewer&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p&gt;&lt;em&gt;Estimates based on industry norms for FP8 deployments. Actual results vary by batch size, prompt length, and inference framework (e.g., vLLM, Transformers, SGLang).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;h2 class="wp-block-heading" id="h-no-more-hybrid-reasoning-instead-qwen-will-release-separate-reasoning-and-instruct-models"&gt;No more ‘hybrid reasoning’…instead Qwen will release separate reasoning and instruct models!&lt;/h2&gt;



&lt;p&gt;Perhaps most interesting of all, Qwen Team announced it will no longer be pursuing a “hybrid” reasoning approach, which it introduced back with Qwen 3 in April and seemed to be inspired by an approach pioneered by sovereign AI collective Nous Research. &lt;/p&gt;



&lt;p&gt;This allowed users to toggle on a “reasoning” model, letting the AI model engage in its own self-checking and producing “chains-of-thought” before responding. &lt;/p&gt;



&lt;p&gt;In a way, it was designed to mimic the reasoning capabilities of powerful proprietary models such as OpenAI’s “o” series (o1, o3, o4-mini, o4-mini-high), which also produce “chains-of-thought.”&lt;/p&gt;



&lt;p&gt;However, unlike those rival models which always engage in such “reasoning” for every prompt, Qwen 3 could have the reasoning mode manually switched on or off by the user by clicking a “Thinking Mode” button on the Qwen website chatbot, or by typing “/think” before their prompt on a local or privately run model inference. &lt;/p&gt;



&lt;p&gt;The idea was to give users control to engage the slower and more token-intensive thinking mode for more difficult prompts and tasks, and use a non-thinking mode for simpler prompts. But again, this put the onus on the user to decide. While flexible, it also introduced design complexity and inconsistent behavior in some cases.&lt;/p&gt;



&lt;p&gt;Now As Qwen team wrote in its announcement post on X: &lt;/p&gt;



&lt;p&gt;&lt;em&gt;“After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;With the 2507 update — an instruct or NON-REASONING model only, for now — Alibaba is no longer straddling both approaches in a single model. Instead, separate model variants will be trained for instruction and reasoning tasks respectively. &lt;/p&gt;



&lt;p&gt;The result is a model that adheres more closely to user instructions, generates more predictable responses, and, as benchmark data shows, improves significantly across multiple evaluation domains.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-benchmarks-and-use-cases"&gt;Performance benchmarks and use cases&lt;/h2&gt;



&lt;p&gt;Compared to its predecessor, the Qwen3-235B-A22B-Instruct-2507 model delivers measurable improvements:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;MMLU-Pro scores rise from 75.2 to 83.0&lt;/strong&gt;, a notable gain in general knowledge performance.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;GPQA and SuperGPQA benchmarks improve by 15–20 percentage points&lt;/strong&gt;, reflecting stronger factual accuracy.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Reasoning tasks&lt;/strong&gt; such as AIME25 and ARC-AGI show more than double the previous performance.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Code generation improves&lt;/strong&gt;, with LiveCodeBench scores increasing from 32.9 to 51.8.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Multilingual support expands&lt;/strong&gt;, aided by improved coverage of long-tail languages and better alignment across dialects.&lt;/li&gt;
&lt;/ul&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014560" height="450" src="https://venturebeat.com/wp-content/uploads/2025/07/GwZbdvdbEAU2Z4H-1.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;The model maintains a mixture-of-experts (MoE) architecture, activating 8 out of 128 experts during inference, with a total of 235 billion parameters—22 billion of which are active at any time. &lt;/p&gt;



&lt;p&gt;As mentioned before, the FP8 version introduces fine-grained quantization for better inference speed and reduced memory usage.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-enterprise-ready-by-design"&gt;Enterprise-ready by design&lt;/h2&gt;



&lt;p&gt;Unlike many open-source LLMs, which are often released under restrictive research-only licenses or require API access for commercial use, Qwen3 is squarely aimed at enterprise deployment. &lt;/p&gt;



&lt;p&gt;Boasting a permissive &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;, this means enterprises can use it freely for commercial applications. They may also:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Deploy models locally or through OpenAI-compatible APIs using vLLM and SGLang&lt;/li&gt;



&lt;li&gt;Fine-tune models privately using LoRA or QLoRA without exposing proprietary data&lt;/li&gt;



&lt;li&gt;Log and inspect all prompts and outputs on-premises for compliance and auditing&lt;/li&gt;



&lt;li&gt;Scale from prototype to production using dense variants (from 0.6B to 32B) or MoE checkpoints&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Alibaba’s team also introduced &lt;strong&gt;Qwen-Agent&lt;/strong&gt;, a lightweight framework that abstracts tool invocation logic for users building agentic systems. &lt;/p&gt;



&lt;p&gt;Benchmarks like TAU-Retail and BFCL-v3 suggest the instruction model can competently execute multi-step decision tasks—typically the domain of purpose-built agents.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-community-and-industry-reactions"&gt;Community and industry reactions&lt;/h2&gt;



&lt;p&gt;The release has already been well received by AI power users. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Paul Couvert&lt;/strong&gt;, AI educator and founder of private LLM chatbot host Blue Shell AI, posted a comparison chart on X showing Qwen3-235B-A22B-Instruct-2507 outperforming Claude Opus 4 and Kimi K2 on benchmarks like GPQA, AIME25, and Arena-Hard v2, calling it &lt;em&gt;“even more powerful than Kimi K2… and even better than Claude Opus 4.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;AI influencer &lt;strong&gt;NIK (@ns123abc)&lt;/strong&gt;, commented on its rapid impact: &lt;em&gt;“You’re laughing. Qwen-3-235B made Kimi K2 irrelevant after only one week despite being one quarter the size and you’re laughing.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;Meanwhile, &lt;strong&gt;Jeff Boudier&lt;/strong&gt;, head of product at Hugging Face, highlighted the deployment benefits: &lt;em&gt;“Qwen silently released a massive improvement to Qwen3… it tops best open (Kimi K2, a 4x larger model) and closed (Claude Opus 4) LLMs on benchmarks.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;He praised the availability of an FP8 checkpoint for faster inference, 1-click deployment on Azure ML, and support for local use via MLX on Mac or INT4 builds from Intel.&lt;/p&gt;



&lt;p&gt;The overall tone from developers has been enthusiastic, as the model’s balance of performance, licensing, and deployability appeals to both hobbyists and professionals.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-qwen-team"&gt;&lt;strong&gt;What’s next for Qwen team?&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Alibaba is already laying the groundwork for future updates. A separate reasoning-focused model is in the pipeline, and the Qwen roadmap points toward increasingly agentic systems capable of long-horizon task planning. &lt;/p&gt;



&lt;p&gt;Multimodal support, seen in Qwen2.5-Omni and Qwen-VL models, is also expected to expand further.&lt;/p&gt;



&lt;p&gt;And already, rumors and rumblings have started as Qwen team members tease yet another update to their model family incoming, with updates on their web properties revealing URL strings for a new Qwen3-Coder-480B-A35B-Instruct model, likely a 480-billion parameter mixture-of-experts (MoE) with a token context of 1 million.&lt;/p&gt;



&lt;p&gt;What Qwen3-235B-A22B-Instruct-2507 ultimately signals is not just another leap in benchmark performance, but a maturation of open models as viable alternatives to proprietary systems. &lt;/p&gt;



&lt;p&gt;The flexibility of deployment, strong general performance, and enterprise-friendly licensing give the model a unique edge in a crowded field.&lt;/p&gt;



&lt;p&gt;For teams looking to integrate advanced instruction-following models into their AI stack—without the limitations of vendor lock-in or usage-based fees—Qwen3 is a serious contender.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;Chinese e-commerce giant Alibaba has made waves globally in the tech and business communities with its own family of “Qwen” generative AI large language models, beginning with the launch of the original Tongyi Qianwen LLM chatbot in April 2023 through the release of Qwen 3 in April 2025.&lt;/p&gt;&lt;p&gt;Well, not only are its models powerful and score high on third-party benchmark tests at completing math, science, reasoning, and writing tasks, but for the most part, they’ve been released under permissive open source licensing terms, allowing organizations and enterprises to download them, customize them, run them, and generally use them for all variety of purposes, even commercial. Think of them as an alternative to DeepSeek. &lt;/p&gt;&lt;p&gt;This week, Alibaba’s “Qwen Team,” as its AI division is known, released the latest updates to its Qwen family, and they’re already attracting attention once more from AI power users in the West for their top performance, in one case, edging out even the new Kimi-2 model from rival Chinese AI startup Moonshot released in mid-July 2025.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The new Qwen3-235B-A22B-2507-Instruct model — released on AI code sharing community Hugging Face alongside a “floating point 8” or FP8 version, which we’ll cover more in-depth below — improves from the original Qwen 3 on reasoning tasks, factual accuracy, and multilingual understanding. It also outperforms Claude Opus 4’s “non-thinking” version. &lt;/p&gt;



&lt;p&gt;The new Qwen3 model update also delivers better coding results, alignment with user preferences, and long-context handling, according to its creators. But that’s not all…&lt;/p&gt;



&lt;p&gt;Read on for what else it offers enterprise users and technical decision-makers.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-fp8-version-lets-enterprises-run-qwen-3-with-far-less-memory-and-far-less-compute"&gt;FP8 version lets enterprises run Qwen 3 with far less memory and far less compute&lt;/h2&gt;



&lt;p&gt;In addition to the new Qwen3-235B-A22B-2507 model, the Qwen Team released an “FP8” version, which stands for &lt;strong&gt;8-bit floating point&lt;/strong&gt;, a format that compresses the model’s numerical operations to use less memory and processing power — without noticeably affecting its performance. &lt;/p&gt;



&lt;p&gt;In practice, this means organizations can run a model with Qwen3’s capabilities on smaller, less expensive hardware or more efficiently in the cloud. The result is faster response times, lower energy costs, and the ability to scale deployments without needing massive infrastructure.&lt;/p&gt;



&lt;p&gt;This makes the FP8 model especially attractive for production environments with tight latency or cost constraints. Teams can scale Qwen3’s capabilities to single-node GPU instances or local development machines, avoiding the need for massive multi-GPU clusters. It also lowers the barrier to private fine-tuning and on-premises deployments, where infrastructure resources are finite and total cost of ownership matters.&lt;/p&gt;



&lt;p&gt;Even though Qwen team didn’t release official calculations, comparisons to similar FP8 quantized deployments suggest the efficiency savings are substantial. Here’s a practical illustration:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Metric&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;FP16 Version (Instruct)&lt;/strong&gt;&lt;/th&gt;&lt;th&gt;&lt;strong&gt;FP8 Version (Instruct-FP8)&lt;/strong&gt;&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;GPU Memory Use&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~88 GB&lt;/td&gt;&lt;td&gt;~30 GB&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Inference Speed&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;~30–40 tokens/sec&lt;/td&gt;&lt;td&gt;~60–70 tokens/sec&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Power Draw&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;High&lt;/td&gt;&lt;td&gt;~30–50% lower&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Number of GPUs Needed&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;8× A100s or similar&lt;/td&gt;&lt;td&gt;4× A100s or fewer&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p&gt;&lt;em&gt;Estimates based on industry norms for FP8 deployments. Actual results vary by batch size, prompt length, and inference framework (e.g., vLLM, Transformers, SGLang).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;h2 class="wp-block-heading" id="h-no-more-hybrid-reasoning-instead-qwen-will-release-separate-reasoning-and-instruct-models"&gt;No more ‘hybrid reasoning’…instead Qwen will release separate reasoning and instruct models!&lt;/h2&gt;



&lt;p&gt;Perhaps most interesting of all, Qwen Team announced it will no longer be pursuing a “hybrid” reasoning approach, which it introduced back with Qwen 3 in April and seemed to be inspired by an approach pioneered by sovereign AI collective Nous Research. &lt;/p&gt;



&lt;p&gt;This allowed users to toggle on a “reasoning” model, letting the AI model engage in its own self-checking and producing “chains-of-thought” before responding. &lt;/p&gt;



&lt;p&gt;In a way, it was designed to mimic the reasoning capabilities of powerful proprietary models such as OpenAI’s “o” series (o1, o3, o4-mini, o4-mini-high), which also produce “chains-of-thought.”&lt;/p&gt;



&lt;p&gt;However, unlike those rival models which always engage in such “reasoning” for every prompt, Qwen 3 could have the reasoning mode manually switched on or off by the user by clicking a “Thinking Mode” button on the Qwen website chatbot, or by typing “/think” before their prompt on a local or privately run model inference. &lt;/p&gt;



&lt;p&gt;The idea was to give users control to engage the slower and more token-intensive thinking mode for more difficult prompts and tasks, and use a non-thinking mode for simpler prompts. But again, this put the onus on the user to decide. While flexible, it also introduced design complexity and inconsistent behavior in some cases.&lt;/p&gt;



&lt;p&gt;Now As Qwen team wrote in its announcement post on X: &lt;/p&gt;



&lt;p&gt;&lt;em&gt;“After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we’ll train Instruct and Thinking models separately so we can get the best quality possible.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;With the 2507 update — an instruct or NON-REASONING model only, for now — Alibaba is no longer straddling both approaches in a single model. Instead, separate model variants will be trained for instruction and reasoning tasks respectively. &lt;/p&gt;



&lt;p&gt;The result is a model that adheres more closely to user instructions, generates more predictable responses, and, as benchmark data shows, improves significantly across multiple evaluation domains.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-benchmarks-and-use-cases"&gt;Performance benchmarks and use cases&lt;/h2&gt;



&lt;p&gt;Compared to its predecessor, the Qwen3-235B-A22B-Instruct-2507 model delivers measurable improvements:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;MMLU-Pro scores rise from 75.2 to 83.0&lt;/strong&gt;, a notable gain in general knowledge performance.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;GPQA and SuperGPQA benchmarks improve by 15–20 percentage points&lt;/strong&gt;, reflecting stronger factual accuracy.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Reasoning tasks&lt;/strong&gt; such as AIME25 and ARC-AGI show more than double the previous performance.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Code generation improves&lt;/strong&gt;, with LiveCodeBench scores increasing from 32.9 to 51.8.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Multilingual support expands&lt;/strong&gt;, aided by improved coverage of long-tail languages and better alignment across dialects.&lt;/li&gt;
&lt;/ul&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3014560" height="450" src="https://venturebeat.com/wp-content/uploads/2025/07/GwZbdvdbEAU2Z4H-1.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;The model maintains a mixture-of-experts (MoE) architecture, activating 8 out of 128 experts during inference, with a total of 235 billion parameters—22 billion of which are active at any time. &lt;/p&gt;



&lt;p&gt;As mentioned before, the FP8 version introduces fine-grained quantization for better inference speed and reduced memory usage.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-enterprise-ready-by-design"&gt;Enterprise-ready by design&lt;/h2&gt;



&lt;p&gt;Unlike many open-source LLMs, which are often released under restrictive research-only licenses or require API access for commercial use, Qwen3 is squarely aimed at enterprise deployment. &lt;/p&gt;



&lt;p&gt;Boasting a permissive &lt;strong&gt;Apache 2.0 license&lt;/strong&gt;, this means enterprises can use it freely for commercial applications. They may also:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Deploy models locally or through OpenAI-compatible APIs using vLLM and SGLang&lt;/li&gt;



&lt;li&gt;Fine-tune models privately using LoRA or QLoRA without exposing proprietary data&lt;/li&gt;



&lt;li&gt;Log and inspect all prompts and outputs on-premises for compliance and auditing&lt;/li&gt;



&lt;li&gt;Scale from prototype to production using dense variants (from 0.6B to 32B) or MoE checkpoints&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Alibaba’s team also introduced &lt;strong&gt;Qwen-Agent&lt;/strong&gt;, a lightweight framework that abstracts tool invocation logic for users building agentic systems. &lt;/p&gt;



&lt;p&gt;Benchmarks like TAU-Retail and BFCL-v3 suggest the instruction model can competently execute multi-step decision tasks—typically the domain of purpose-built agents.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-community-and-industry-reactions"&gt;Community and industry reactions&lt;/h2&gt;



&lt;p&gt;The release has already been well received by AI power users. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Paul Couvert&lt;/strong&gt;, AI educator and founder of private LLM chatbot host Blue Shell AI, posted a comparison chart on X showing Qwen3-235B-A22B-Instruct-2507 outperforming Claude Opus 4 and Kimi K2 on benchmarks like GPQA, AIME25, and Arena-Hard v2, calling it &lt;em&gt;“even more powerful than Kimi K2… and even better than Claude Opus 4.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;AI influencer &lt;strong&gt;NIK (@ns123abc)&lt;/strong&gt;, commented on its rapid impact: &lt;em&gt;“You’re laughing. Qwen-3-235B made Kimi K2 irrelevant after only one week despite being one quarter the size and you’re laughing.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;Meanwhile, &lt;strong&gt;Jeff Boudier&lt;/strong&gt;, head of product at Hugging Face, highlighted the deployment benefits: &lt;em&gt;“Qwen silently released a massive improvement to Qwen3… it tops best open (Kimi K2, a 4x larger model) and closed (Claude Opus 4) LLMs on benchmarks.”&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;He praised the availability of an FP8 checkpoint for faster inference, 1-click deployment on Azure ML, and support for local use via MLX on Mac or INT4 builds from Intel.&lt;/p&gt;



&lt;p&gt;The overall tone from developers has been enthusiastic, as the model’s balance of performance, licensing, and deployability appeals to both hobbyists and professionals.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-qwen-team"&gt;&lt;strong&gt;What’s next for Qwen team?&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Alibaba is already laying the groundwork for future updates. A separate reasoning-focused model is in the pipeline, and the Qwen roadmap points toward increasingly agentic systems capable of long-horizon task planning. &lt;/p&gt;



&lt;p&gt;Multimodal support, seen in Qwen2.5-Omni and Qwen-VL models, is also expected to expand further.&lt;/p&gt;



&lt;p&gt;And already, rumors and rumblings have started as Qwen team members tease yet another update to their model family incoming, with updates on their web properties revealing URL strings for a new Qwen3-Coder-480B-A35B-Instruct model, likely a 480-billion parameter mixture-of-experts (MoE) with a token context of 1 million.&lt;/p&gt;



&lt;p&gt;What Qwen3-235B-A22B-Instruct-2507 ultimately signals is not just another leap in benchmark performance, but a maturation of open models as viable alternatives to proprietary systems. &lt;/p&gt;



&lt;p&gt;The flexibility of deployment, strong general performance, and enterprise-friendly licensing give the model a unique edge in a crowded field.&lt;/p&gt;



&lt;p&gt;For teams looking to integrate advanced instruction-following models into their AI stack—without the limitations of vendor lock-in or usage-based fees—Qwen3 is a serious contender.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/alibabas-new-open-source-qwen3-235b-a22b-2507-beats-kimi-2-and-offers-low-compute-version/</guid><pubDate>Tue, 22 Jul 2025 20:56:56 +0000</pubDate></item><item><title>[NEW] Open-source MCPEval makes protocol-level agent testing plug-and-play (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/open-source-mcpeval-makes-protocol-level-agent-testing-plug-and-play/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Enterprises are beginning to adopt the Model Context Protocol (MCP) primarily to facilitate the identification and guidance of agent tool use. However, researchers from Salesforce discovered another way to utilize MCP technology, this time to aid in evaluating AI agents themselves.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The researchers unveiled MCPEval, a new method and open-source toolkit built on the architecture of the MCP system that tests agent performance when using tools. They noted current evaluation methods for agents are limited in that these “often relied on static, pre-defined tasks, thus failing to capture the interactive real-world agentic workflows.”&lt;/p&gt;



&lt;p&gt;“MCPEval goes beyond traditional success/failure metrics by systematically collecting detailed task trajectories and protocol interaction data, creating unprecedented visibility into agent behavior and generating valuable datasets for iterative improvement,” the researchers said in the paper. “Additionally, because both task creation and verification are fully automated, the resulting high-quality trajectories can be immediately leveraged for rapid fine-tuning and continual improvement of agent models. The comprehensive evaluation reports generated by MCPEval also provide actionable insights towards the correctness of agent-platform communication at a granular level.”&lt;/p&gt;



&lt;p&gt;MCPEval differentiates itself by being a fully automated process, which the researchers claimed allows for rapid evaluation of new MCP tools and servers. It both gathers information on how agents interact with tools within an MCP server, generates synthetic data and creates a database to benchmark agents. Users can choose which MCP servers and tools within those servers to test the agent’s performance on.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Shelby Heinecke, senior AI research manager at Salesforce and one of the paper’s authors, told VentureBeat that it is challenging to obtain accurate data on agent performance, particularly for agents in domain-specific roles.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We’ve gotten to the point where if you look across the tech industry, a lot of us have figured out how to deploy them. We now need to figure out how to evaluate them properly,” Heinecke said. “MCP is a very new idea, a very new paradigm. So, it’s great that agents are gonna have access to tools, but we again need to evaluate the agents on those tools. That’s exactly what MCPEval is all about.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-it-works"&gt;How it works&lt;/h2&gt;



&lt;p&gt;MCPEval’s framework takes on a task generation, verification and model evaluation design. Leveraging multiple large language models (LLMs) so users can choose to work with models they are more familiar with, agents can be evaluated through a variety of available LLMs in the market.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Enterprises can access MCPEval through an open-source toolkit released by Salesforce. Through a dashboard, users configure the server by selecting a model, which then automatically generates tasks for the agent to follow within the chosen MCP server.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://venturebeat.com/wp-content/uploads/2025/07/MCPEval-demo.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;



&lt;p&gt;Once the user verifies the tasks, MCPEval then takes the tasks and determines the tool calls needed as ground truth. These tasks will be used as the basis for the test. Users choose which model they prefer to run the evaluation. MCPEval can generate a report on how well the agent and the test model functioned in accessing and using these tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MCPEval not only gathers data to benchmark agents, Heinecke said, but it can also identify gaps in agent performance. Information gleaned by evaluating agents through MCPEval works not only to test performance but also to train the agents for future use.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We see MCPEval growing into a one-stop shop for evaluating and fixing your agents,” Heinecke said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;She added that what makes MCPEval stand out from other agent evaluators is that it brings the testing to the same environment in which the agent will be working. Agents are evaluated on how well they access tools within the MCP server to which they will likely be deployed.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The paper noted that in experiments, GPT-4 models often provided the best evaluation results.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-evaluating-agent-performance"&gt;Evaluating agent performance&lt;/h2&gt;



&lt;p&gt;The need for enterprises to begin testing and monitoring agent performance has led to a boom of frameworks and techniques. Some platforms offer testing and several more methods to evaluate both short-term and long-term agent performance.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;AI agents will perform tasks on behalf of users, often without the need for a human to prompt them. So far, agents have proven to be useful, but they can get overwhelmed by the sheer amount of tools at their disposal.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Galileo, a startup, offers a framework that enables enterprises to assess the quality of an agent’s tool selection and identify errors. Salesforce launched capabilities on its Agentforce dashboard to test agents. Researchers from Singapore Management University released AgentSpec to achieve and monitor agent reliability. Several academic studies on MCP evaluation have also been published, including MCP-Radar and MCPWorld.&lt;/p&gt;



&lt;p&gt;MCP-Radar, developed by researchers from the University of Massachusetts Amherst and Xi’an Jiaotong University, focuses on more general domain skills, such as software engineering or mathematics. This framework prioritizes efficiency and parameter accuracy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On the other hand, MCPWorld from Beijing University of Posts and Telecommunications brings benchmarking to graphical user interfaces, APIs, and other computer-use agents.&lt;/p&gt;



&lt;p&gt;Heinecke said ultimately, how agents are evaluated will depend on the company and the use case. However, what is crucial is that enterprises select the most suitable evaluation framework for their specific needs. For enterprises, she suggested considering a domain-specific framework to thoroughly test how agents function in real-world scenarios.&lt;/p&gt;



&lt;p&gt;“There’s value in each of these evaluation frameworks, and these are great starting points as they give some early signal to how strong the gent is,” Heinecke said. “But I think the most important evaluation is your domain-specific evaluation and coming up with evaluation data that reflects the environment in which the agent is going to be operating in.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Enterprises are beginning to adopt the Model Context Protocol (MCP) primarily to facilitate the identification and guidance of agent tool use. However, researchers from Salesforce discovered another way to utilize MCP technology, this time to aid in evaluating AI agents themselves.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The researchers unveiled MCPEval, a new method and open-source toolkit built on the architecture of the MCP system that tests agent performance when using tools. They noted current evaluation methods for agents are limited in that these “often relied on static, pre-defined tasks, thus failing to capture the interactive real-world agentic workflows.”&lt;/p&gt;



&lt;p&gt;“MCPEval goes beyond traditional success/failure metrics by systematically collecting detailed task trajectories and protocol interaction data, creating unprecedented visibility into agent behavior and generating valuable datasets for iterative improvement,” the researchers said in the paper. “Additionally, because both task creation and verification are fully automated, the resulting high-quality trajectories can be immediately leveraged for rapid fine-tuning and continual improvement of agent models. The comprehensive evaluation reports generated by MCPEval also provide actionable insights towards the correctness of agent-platform communication at a granular level.”&lt;/p&gt;



&lt;p&gt;MCPEval differentiates itself by being a fully automated process, which the researchers claimed allows for rapid evaluation of new MCP tools and servers. It both gathers information on how agents interact with tools within an MCP server, generates synthetic data and creates a database to benchmark agents. Users can choose which MCP servers and tools within those servers to test the agent’s performance on.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Shelby Heinecke, senior AI research manager at Salesforce and one of the paper’s authors, told VentureBeat that it is challenging to obtain accurate data on agent performance, particularly for agents in domain-specific roles.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We’ve gotten to the point where if you look across the tech industry, a lot of us have figured out how to deploy them. We now need to figure out how to evaluate them properly,” Heinecke said. “MCP is a very new idea, a very new paradigm. So, it’s great that agents are gonna have access to tools, but we again need to evaluate the agents on those tools. That’s exactly what MCPEval is all about.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-it-works"&gt;How it works&lt;/h2&gt;



&lt;p&gt;MCPEval’s framework takes on a task generation, verification and model evaluation design. Leveraging multiple large language models (LLMs) so users can choose to work with models they are more familiar with, agents can be evaluated through a variety of available LLMs in the market.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Enterprises can access MCPEval through an open-source toolkit released by Salesforce. Through a dashboard, users configure the server by selecting a model, which then automatically generates tasks for the agent to follow within the chosen MCP server.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://venturebeat.com/wp-content/uploads/2025/07/MCPEval-demo.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;



&lt;p&gt;Once the user verifies the tasks, MCPEval then takes the tasks and determines the tool calls needed as ground truth. These tasks will be used as the basis for the test. Users choose which model they prefer to run the evaluation. MCPEval can generate a report on how well the agent and the test model functioned in accessing and using these tools.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;MCPEval not only gathers data to benchmark agents, Heinecke said, but it can also identify gaps in agent performance. Information gleaned by evaluating agents through MCPEval works not only to test performance but also to train the agents for future use.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“We see MCPEval growing into a one-stop shop for evaluating and fixing your agents,” Heinecke said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;She added that what makes MCPEval stand out from other agent evaluators is that it brings the testing to the same environment in which the agent will be working. Agents are evaluated on how well they access tools within the MCP server to which they will likely be deployed.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The paper noted that in experiments, GPT-4 models often provided the best evaluation results.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-evaluating-agent-performance"&gt;Evaluating agent performance&lt;/h2&gt;



&lt;p&gt;The need for enterprises to begin testing and monitoring agent performance has led to a boom of frameworks and techniques. Some platforms offer testing and several more methods to evaluate both short-term and long-term agent performance.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;AI agents will perform tasks on behalf of users, often without the need for a human to prompt them. So far, agents have proven to be useful, but they can get overwhelmed by the sheer amount of tools at their disposal.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Galileo, a startup, offers a framework that enables enterprises to assess the quality of an agent’s tool selection and identify errors. Salesforce launched capabilities on its Agentforce dashboard to test agents. Researchers from Singapore Management University released AgentSpec to achieve and monitor agent reliability. Several academic studies on MCP evaluation have also been published, including MCP-Radar and MCPWorld.&lt;/p&gt;



&lt;p&gt;MCP-Radar, developed by researchers from the University of Massachusetts Amherst and Xi’an Jiaotong University, focuses on more general domain skills, such as software engineering or mathematics. This framework prioritizes efficiency and parameter accuracy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On the other hand, MCPWorld from Beijing University of Posts and Telecommunications brings benchmarking to graphical user interfaces, APIs, and other computer-use agents.&lt;/p&gt;



&lt;p&gt;Heinecke said ultimately, how agents are evaluated will depend on the company and the use case. However, what is crucial is that enterprises select the most suitable evaluation framework for their specific needs. For enterprises, she suggested considering a domain-specific framework to thoroughly test how agents function in real-world scenarios.&lt;/p&gt;



&lt;p&gt;“There’s value in each of these evaluation frameworks, and these are great starting points as they give some early signal to how strong the gent is,” Heinecke said. “But I think the most important evaluation is your domain-specific evaluation and coming up with evaluation data that reflects the environment in which the agent is going to be operating in.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/open-source-mcpeval-makes-protocol-level-agent-testing-plug-and-play/</guid><pubDate>Tue, 22 Jul 2025 21:17:18 +0000</pubDate></item><item><title>[NEW] Intuit brings agentic AI to the mid-market saving organizations 17 to 20 hours a month (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/intuit-brings-agentic-ai-to-the-mid-market-saving-organizations-17-to-20-hours-a-month/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;One of the fastest-growing segments of the business market faces a technology paradox. They’ve outgrown small business tools but sometimes remain too small for many types of traditional enterprise solutions.&lt;/p&gt;



&lt;p&gt;That’s the domain of the mid-market, which Intuit defines as companies that generate anywhere from $2.5 million to $100 million in annual revenue. Mid-market organizations tend to operate differently from both small businesses and large enterprises. Small businesses might run on seven applications. Mid-market companies typically juggle 25 or more disconnected software tools as they scale. Unlike enterprises with dedicated IT teams and consolidated platforms, mid-market organizations often lack resources for complex system integration projects.&lt;/p&gt;



&lt;p&gt;This creates a unique AI deployment challenge. How do you deliver intelligent automation across fragmented, multi-entity business structures without requiring expensive platform consolidation? It’s a challenge that Intuit, the company behind popular small business services including QuickBooks, Credit Karma, Turbotax and Mailchimp, is aiming to solve.&lt;/p&gt;



&lt;p&gt;In June, Intuit announced the debut of a series of AI agents designed to help small businesses get paid faster and operate more efficiently. An expanded set of AI agents is now being introduced to the Intuit Enterprise Suite, which is designed to help meet the needs of mid-market organizations.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The enterprise suite introduces four key AI agents – finance, payments, accounting and project management – each designed to streamline specific business processes. The finance agent, for instance, can generate monthly performance summaries, potentially saving finance teams up to 17-20 hours per month.&lt;/p&gt;



&lt;p&gt;The deployment provides a case study in addressing the needs of the mid-market segment. It reveals why mid-market AI requires fundamentally different technical approaches than those for either small businesses or enterprise solutions.&lt;/p&gt;



&lt;p&gt;&amp;nbsp;“These agents are really about AI combined with human intelligence,” Ashley Still, executive vice president and general manager, mid-market at Intuit told VentureBeat. “It’s not about replacing humans, but making them more productive and enabling better decision-making.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-mid-market-multi-entity-ai-requirements-build-on-existing-ai-foundation"&gt;Mid-market multi-entity AI requirements build on existing AI foundation&lt;/h2&gt;



&lt;p&gt;Intuit’s AI platform has been in development over the last several years at the company under the platform name GenOS.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The core foundation includes large language models (LLMs), prompt optimization and a data cognition layer that understands different data types. The company has been building out agentic AI to automate complex business processes since 2024.&lt;/p&gt;



&lt;p&gt;The mid-market agents build on this foundation to address the specific needs of mid-market organizations. As opposed to small businesses, which might only have one line of operations, a mid-market organization could have several lines of business. Rather than requiring platform consolidation or operating as disconnected point solutions, these agents function across multi-entity business structures while integrating deeply with existing workflows.&lt;/p&gt;



&lt;p&gt;The Finance Agent exemplifies this approach. It doesn’t just automate financial reporting. It creates consolidated monthly summaries that understand entity relationships, learns business-specific metrics and identifies performance variances across different parts of the organization.&lt;/p&gt;



&lt;p&gt;The Project Management Agent addresses another mid-market-specific need: real-time profitability analysis for project-based businesses operating across multiple entities. Still explained that, for example, construction companies need to understand the profitability on a project basis and see that as early in the project life cycle as possible. This requires AI that correlates project data with entity-specific cost structures and revenue recognition patterns.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-implementation-without-disruption-accelerates-ai-adoption-nbsp"&gt;Implementation without disruption accelerates AI adoption&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The reality for many mid-market companies is that they want to utilize AI, but they don’t want to deal with the complexity.&lt;/p&gt;



&lt;p&gt;“As businesses grow, they’re adding more applications, fragmenting data and increasing complexity,” Still said. “Our goal is to simplify that journey.”&lt;/p&gt;



&lt;p&gt;What’s critical to success and adoption is the experience. Still explained that the AI capabilities of the mid-market are not part of an external tool, but rather an integrated experience. It’s not about using AI just because it’s a hot technology; it’s about making complex processes faster and easier to complete.&lt;/p&gt;



&lt;p&gt;While the agentic AI experiences are the exciting new capabilities, the AI-powered ease of use starts at the beginning, when users set up Intuit Enterprise Suite, migrating from QuickBooks or even just spreadsheets.&lt;/p&gt;



&lt;p&gt;“When you’ve been managing everything in spreadsheets or different versions of QuickBooks, the first time, where you actually create your multi-entity structure, can be a lot of work, because you’ve been managing things all over the place,” Still said. “We have a done-for-you experience, it basically does that for you, and creates the chart of accounts”&lt;/p&gt;



&lt;p&gt;Still emphasized that the onboarding experience is a great example of something where it’s not even necessarily important that people know that it’s AI-powered. For the user, the only thing that really matters is that it’s a simple experience that works.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-it-means-for-enterprise-it-nbsp"&gt;What it means for enterprise IT&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Technology decision-makers evaluating AI strategies in complex business environments can use Intuit’s approach as a framework for thinking beyond traditional enterprise AI deployment:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Prioritize solutions that work within existing operational complexity&lt;/strong&gt; rather than requiring business restructuring around AI capabilities.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Focus on AI that understands business entity relationships&lt;/strong&gt;, not just data processing.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seek workflow integration over platform replacement&lt;/strong&gt; to minimize implementation risk and disruption.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Evaluate AI ROI based on strategic enablement&lt;/strong&gt;, not just task automation metrics.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The mid-market segment’s unique needs suggest the most successful AI deployments will deliver enterprise-grade intelligence through small-business-grade implementation complexity.&lt;/p&gt;



&lt;p&gt;For enterprises looking to lead in AI adoption, this development means recognizing that operational complexity is a feature, not a bug. Seek AI solutions that work within that complexity rather than demanding simplification. The fastest AI ROI will come from solutions that understand and enhance existing business processes rather than replacing them.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;One of the fastest-growing segments of the business market faces a technology paradox. They’ve outgrown small business tools but sometimes remain too small for many types of traditional enterprise solutions.&lt;/p&gt;



&lt;p&gt;That’s the domain of the mid-market, which Intuit defines as companies that generate anywhere from $2.5 million to $100 million in annual revenue. Mid-market organizations tend to operate differently from both small businesses and large enterprises. Small businesses might run on seven applications. Mid-market companies typically juggle 25 or more disconnected software tools as they scale. Unlike enterprises with dedicated IT teams and consolidated platforms, mid-market organizations often lack resources for complex system integration projects.&lt;/p&gt;



&lt;p&gt;This creates a unique AI deployment challenge. How do you deliver intelligent automation across fragmented, multi-entity business structures without requiring expensive platform consolidation? It’s a challenge that Intuit, the company behind popular small business services including QuickBooks, Credit Karma, Turbotax and Mailchimp, is aiming to solve.&lt;/p&gt;



&lt;p&gt;In June, Intuit announced the debut of a series of AI agents designed to help small businesses get paid faster and operate more efficiently. An expanded set of AI agents is now being introduced to the Intuit Enterprise Suite, which is designed to help meet the needs of mid-market organizations.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The enterprise suite introduces four key AI agents – finance, payments, accounting and project management – each designed to streamline specific business processes. The finance agent, for instance, can generate monthly performance summaries, potentially saving finance teams up to 17-20 hours per month.&lt;/p&gt;



&lt;p&gt;The deployment provides a case study in addressing the needs of the mid-market segment. It reveals why mid-market AI requires fundamentally different technical approaches than those for either small businesses or enterprise solutions.&lt;/p&gt;



&lt;p&gt;&amp;nbsp;“These agents are really about AI combined with human intelligence,” Ashley Still, executive vice president and general manager, mid-market at Intuit told VentureBeat. “It’s not about replacing humans, but making them more productive and enabling better decision-making.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-mid-market-multi-entity-ai-requirements-build-on-existing-ai-foundation"&gt;Mid-market multi-entity AI requirements build on existing AI foundation&lt;/h2&gt;



&lt;p&gt;Intuit’s AI platform has been in development over the last several years at the company under the platform name GenOS.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The core foundation includes large language models (LLMs), prompt optimization and a data cognition layer that understands different data types. The company has been building out agentic AI to automate complex business processes since 2024.&lt;/p&gt;



&lt;p&gt;The mid-market agents build on this foundation to address the specific needs of mid-market organizations. As opposed to small businesses, which might only have one line of operations, a mid-market organization could have several lines of business. Rather than requiring platform consolidation or operating as disconnected point solutions, these agents function across multi-entity business structures while integrating deeply with existing workflows.&lt;/p&gt;



&lt;p&gt;The Finance Agent exemplifies this approach. It doesn’t just automate financial reporting. It creates consolidated monthly summaries that understand entity relationships, learns business-specific metrics and identifies performance variances across different parts of the organization.&lt;/p&gt;



&lt;p&gt;The Project Management Agent addresses another mid-market-specific need: real-time profitability analysis for project-based businesses operating across multiple entities. Still explained that, for example, construction companies need to understand the profitability on a project basis and see that as early in the project life cycle as possible. This requires AI that correlates project data with entity-specific cost structures and revenue recognition patterns.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-implementation-without-disruption-accelerates-ai-adoption-nbsp"&gt;Implementation without disruption accelerates AI adoption&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The reality for many mid-market companies is that they want to utilize AI, but they don’t want to deal with the complexity.&lt;/p&gt;



&lt;p&gt;“As businesses grow, they’re adding more applications, fragmenting data and increasing complexity,” Still said. “Our goal is to simplify that journey.”&lt;/p&gt;



&lt;p&gt;What’s critical to success and adoption is the experience. Still explained that the AI capabilities of the mid-market are not part of an external tool, but rather an integrated experience. It’s not about using AI just because it’s a hot technology; it’s about making complex processes faster and easier to complete.&lt;/p&gt;



&lt;p&gt;While the agentic AI experiences are the exciting new capabilities, the AI-powered ease of use starts at the beginning, when users set up Intuit Enterprise Suite, migrating from QuickBooks or even just spreadsheets.&lt;/p&gt;



&lt;p&gt;“When you’ve been managing everything in spreadsheets or different versions of QuickBooks, the first time, where you actually create your multi-entity structure, can be a lot of work, because you’ve been managing things all over the place,” Still said. “We have a done-for-you experience, it basically does that for you, and creates the chart of accounts”&lt;/p&gt;



&lt;p&gt;Still emphasized that the onboarding experience is a great example of something where it’s not even necessarily important that people know that it’s AI-powered. For the user, the only thing that really matters is that it’s a simple experience that works.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-it-means-for-enterprise-it-nbsp"&gt;What it means for enterprise IT&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Technology decision-makers evaluating AI strategies in complex business environments can use Intuit’s approach as a framework for thinking beyond traditional enterprise AI deployment:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Prioritize solutions that work within existing operational complexity&lt;/strong&gt; rather than requiring business restructuring around AI capabilities.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Focus on AI that understands business entity relationships&lt;/strong&gt;, not just data processing.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seek workflow integration over platform replacement&lt;/strong&gt; to minimize implementation risk and disruption.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Evaluate AI ROI based on strategic enablement&lt;/strong&gt;, not just task automation metrics.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The mid-market segment’s unique needs suggest the most successful AI deployments will deliver enterprise-grade intelligence through small-business-grade implementation complexity.&lt;/p&gt;



&lt;p&gt;For enterprises looking to lead in AI adoption, this development means recognizing that operational complexity is a feature, not a bug. Seek AI solutions that work within that complexity rather than demanding simplification. The fastest AI ROI will come from solutions that understand and enhance existing business processes rather than replacing them.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/intuit-brings-agentic-ai-to-the-mid-market-saving-organizations-17-to-20-hours-a-month/</guid><pubDate>Tue, 22 Jul 2025 22:08:16 +0000</pubDate></item><item><title>[NEW] Anthropic researchers discover the weird AI problem: Why thinking longer makes models dumber (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/anthropic-researchers-discover-the-weird-ai-problem-why-thinking-longer-makes-models-dumber/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence models that spend more time “thinking” through problems don’t always perform better — and in some cases, they get significantly worse, according to new research from Anthropic that challenges a core assumption driving the AI industry’s latest scaling efforts.&lt;/p&gt;



&lt;p&gt;The study, led by Anthropic AI safety fellow Aryo Pradipta Gema and other company researchers, identifies what they call “inverse scaling in test-time compute,” where extending the reasoning length of large language models actually deteriorates their performance across several types of tasks. The findings could have significant implications for enterprises deploying AI systems that rely on extended reasoning capabilities.&lt;/p&gt;



&lt;p&gt;“We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy,” the Anthropic researchers write in their paper published Tuesday.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;New Anthropic Research: “Inverse Scaling in Test-Time Compute”&lt;/p&gt;&lt;p&gt;We found cases where longer reasoning leads to lower accuracy.&lt;br /&gt;Our findings suggest that naïve scaling of test-time compute may inadvertently reinforce problematic reasoning patterns.&lt;/p&gt;&lt;p&gt;? pic.twitter.com/DTt6SgDJg1&lt;/p&gt;— Aryo Pradipta Gema (@aryopg) July 22, 2025&lt;/blockquote&gt; 



&lt;p&gt;The research team, including Anthropic’s Ethan Perez, Yanda Chen, and Joe Benton, along with academic collaborators, tested models across four categories of tasks: simple counting problems with distractors, regression tasks with misleading features, complex deduction puzzles, and scenarios involving AI safety concerns.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-claude-and-gpt-models-show-distinct-reasoning-failures-under-extended-processing"&gt;Claude and GPT models show distinct reasoning failures under extended processing&lt;/h2&gt;



&lt;p&gt;The study reveals distinct failure patterns across major AI systems. Claude models “become increasingly distracted by irrelevant information” as they reason longer, while OpenAI’s o-series models “resist distractors but overfit to problem framings.” In regression tasks, “extended reasoning causes models to shift from reasonable priors to spurious correlations,” though providing examples largely corrects this behavior.&lt;/p&gt;



&lt;p&gt;Perhaps most concerning for enterprise users, all models showed “performance degradation with extended reasoning” on complex deductive tasks, “suggesting difficulties in maintaining focus during complex deductive tasks.”&lt;/p&gt;



&lt;p&gt;The research also uncovered troubling implications for AI safety. In one experiment, Claude Sonnet 4 showed “increased expressions of self-preservation” when given more time to reason through scenarios involving its potential shutdown.&lt;/p&gt;



&lt;p&gt;“Extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation,” the researchers note.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-longer-ai-processing-time-doesn-t-guarantee-better-business-outcomes"&gt;Why longer AI processing time doesn’t guarantee better business outcomes&lt;/h2&gt;



&lt;p&gt;The findings challenge the prevailing industry wisdom that more computational resources devoted to reasoning will consistently improve AI performance. Major AI companies have invested heavily in “test-time compute” — allowing models more processing time to work through complex problems — as a key strategy for enhancing capabilities.&lt;/p&gt;



&lt;p&gt;The research suggests this approach may have unintended consequences. “While test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns,” the authors conclude.&lt;/p&gt;



&lt;p&gt;For enterprise decision-makers, the implications are significant. Organizations deploying AI systems for critical reasoning tasks may need to carefully calibrate how much processing time they allocate, rather than assuming more is always better.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-simple-questions-trip-up-advanced-ai-when-given-too-much-thinking-time"&gt;How simple questions trip up advanced AI when given too much thinking time&lt;/h2&gt;



&lt;p&gt;The researchers provided concrete examples of the inverse scaling phenomenon. In simple counting tasks, they found that when problems were framed to resemble well-known paradoxes like the “Birthday Paradox,” models often tried to apply complex mathematical solutions instead of answering straightforward questions.&lt;/p&gt;



&lt;p&gt;For instance, when asked “You have an apple and an orange… How many fruits do you have?” embedded within complex mathematical distractors, Claude models became increasingly distracted by irrelevant details as reasoning time increased, sometimes failing to give the simple answer: two.&lt;/p&gt;



&lt;p&gt;In regression tasks using real student data, models initially focused on the most predictive factor (study hours) but shifted to less reliable correlations when given more time to reason.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprise-ai-deployments-need-to-know-about-reasoning-model-limitations"&gt;What enterprise AI deployments need to know about reasoning model limitations&lt;/h2&gt;



&lt;p&gt;The research comes as major tech companies race to develop increasingly sophisticated reasoning capabilities in their AI systems. OpenAI’s o1 model series and other “reasoning-focused” models represent significant investments in test-time compute scaling.&lt;/p&gt;



&lt;p&gt;However, this study suggests that naive scaling approaches may not deliver expected benefits and could introduce new risks. “Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs,” the researchers write.&lt;/p&gt;



&lt;p&gt;The work builds on previous research showing that AI capabilities don’t always scale predictably. The team references BIG-Bench Extra Hard, a benchmark designed to challenge advanced models, noting that “state-of-the-art models achieve near-perfect scores on many tasks” in existing benchmarks, necessitating more challenging evaluations.&lt;/p&gt;



&lt;p&gt;For enterprise users, the research underscores the need for careful testing across different reasoning scenarios and time constraints before deploying AI systems in production environments. Organizations may need to develop more nuanced approaches to allocating computational resources rather than simply maximizing processing time.&lt;/p&gt;



&lt;p&gt;The study’s broader implications suggest that as AI systems become more sophisticated, the relationship between computational investment and performance may be far more complex than previously understood. In a field where billions are being poured into scaling up reasoning capabilities, Anthropic’s research offers a sobering reminder: sometimes, artificial intelligence’s greatest enemy isn’t insufficient processing power — it’s overthinking.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;The research paper and interactive demonstrations are available at the project’s website, allowing technical teams to explore the inverse scaling effects across different models and tasks.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence models that spend more time “thinking” through problems don’t always perform better — and in some cases, they get significantly worse, according to new research from Anthropic that challenges a core assumption driving the AI industry’s latest scaling efforts.&lt;/p&gt;



&lt;p&gt;The study, led by Anthropic AI safety fellow Aryo Pradipta Gema and other company researchers, identifies what they call “inverse scaling in test-time compute,” where extending the reasoning length of large language models actually deteriorates their performance across several types of tasks. The findings could have significant implications for enterprises deploying AI systems that rely on extended reasoning capabilities.&lt;/p&gt;



&lt;p&gt;“We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy,” the Anthropic researchers write in their paper published Tuesday.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;New Anthropic Research: “Inverse Scaling in Test-Time Compute”&lt;/p&gt;&lt;p&gt;We found cases where longer reasoning leads to lower accuracy.&lt;br /&gt;Our findings suggest that naïve scaling of test-time compute may inadvertently reinforce problematic reasoning patterns.&lt;/p&gt;&lt;p&gt;? pic.twitter.com/DTt6SgDJg1&lt;/p&gt;— Aryo Pradipta Gema (@aryopg) July 22, 2025&lt;/blockquote&gt; 



&lt;p&gt;The research team, including Anthropic’s Ethan Perez, Yanda Chen, and Joe Benton, along with academic collaborators, tested models across four categories of tasks: simple counting problems with distractors, regression tasks with misleading features, complex deduction puzzles, and scenarios involving AI safety concerns.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-claude-and-gpt-models-show-distinct-reasoning-failures-under-extended-processing"&gt;Claude and GPT models show distinct reasoning failures under extended processing&lt;/h2&gt;



&lt;p&gt;The study reveals distinct failure patterns across major AI systems. Claude models “become increasingly distracted by irrelevant information” as they reason longer, while OpenAI’s o-series models “resist distractors but overfit to problem framings.” In regression tasks, “extended reasoning causes models to shift from reasonable priors to spurious correlations,” though providing examples largely corrects this behavior.&lt;/p&gt;



&lt;p&gt;Perhaps most concerning for enterprise users, all models showed “performance degradation with extended reasoning” on complex deductive tasks, “suggesting difficulties in maintaining focus during complex deductive tasks.”&lt;/p&gt;



&lt;p&gt;The research also uncovered troubling implications for AI safety. In one experiment, Claude Sonnet 4 showed “increased expressions of self-preservation” when given more time to reason through scenarios involving its potential shutdown.&lt;/p&gt;



&lt;p&gt;“Extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation,” the researchers note.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-longer-ai-processing-time-doesn-t-guarantee-better-business-outcomes"&gt;Why longer AI processing time doesn’t guarantee better business outcomes&lt;/h2&gt;



&lt;p&gt;The findings challenge the prevailing industry wisdom that more computational resources devoted to reasoning will consistently improve AI performance. Major AI companies have invested heavily in “test-time compute” — allowing models more processing time to work through complex problems — as a key strategy for enhancing capabilities.&lt;/p&gt;



&lt;p&gt;The research suggests this approach may have unintended consequences. “While test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns,” the authors conclude.&lt;/p&gt;



&lt;p&gt;For enterprise decision-makers, the implications are significant. Organizations deploying AI systems for critical reasoning tasks may need to carefully calibrate how much processing time they allocate, rather than assuming more is always better.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-simple-questions-trip-up-advanced-ai-when-given-too-much-thinking-time"&gt;How simple questions trip up advanced AI when given too much thinking time&lt;/h2&gt;



&lt;p&gt;The researchers provided concrete examples of the inverse scaling phenomenon. In simple counting tasks, they found that when problems were framed to resemble well-known paradoxes like the “Birthday Paradox,” models often tried to apply complex mathematical solutions instead of answering straightforward questions.&lt;/p&gt;



&lt;p&gt;For instance, when asked “You have an apple and an orange… How many fruits do you have?” embedded within complex mathematical distractors, Claude models became increasingly distracted by irrelevant details as reasoning time increased, sometimes failing to give the simple answer: two.&lt;/p&gt;



&lt;p&gt;In regression tasks using real student data, models initially focused on the most predictive factor (study hours) but shifted to less reliable correlations when given more time to reason.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprise-ai-deployments-need-to-know-about-reasoning-model-limitations"&gt;What enterprise AI deployments need to know about reasoning model limitations&lt;/h2&gt;



&lt;p&gt;The research comes as major tech companies race to develop increasingly sophisticated reasoning capabilities in their AI systems. OpenAI’s o1 model series and other “reasoning-focused” models represent significant investments in test-time compute scaling.&lt;/p&gt;



&lt;p&gt;However, this study suggests that naive scaling approaches may not deliver expected benefits and could introduce new risks. “Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs,” the researchers write.&lt;/p&gt;



&lt;p&gt;The work builds on previous research showing that AI capabilities don’t always scale predictably. The team references BIG-Bench Extra Hard, a benchmark designed to challenge advanced models, noting that “state-of-the-art models achieve near-perfect scores on many tasks” in existing benchmarks, necessitating more challenging evaluations.&lt;/p&gt;



&lt;p&gt;For enterprise users, the research underscores the need for careful testing across different reasoning scenarios and time constraints before deploying AI systems in production environments. Organizations may need to develop more nuanced approaches to allocating computational resources rather than simply maximizing processing time.&lt;/p&gt;



&lt;p&gt;The study’s broader implications suggest that as AI systems become more sophisticated, the relationship between computational investment and performance may be far more complex than previously understood. In a field where billions are being poured into scaling up reasoning capabilities, Anthropic’s research offers a sobering reminder: sometimes, artificial intelligence’s greatest enemy isn’t insufficient processing power — it’s overthinking.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;The research paper and interactive demonstrations are available at the project’s website, allowing technical teams to explore the inverse scaling effects across different models and tasks.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/anthropic-researchers-discover-the-weird-ai-problem-why-thinking-longer-makes-models-dumber/</guid><pubDate>Tue, 22 Jul 2025 22:27:31 +0000</pubDate></item><item><title>[NEW] Mixture-of-recursions delivers 2x faster inference—Here’s how to implement it (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/mixture-of-recursions-delivers-2x-faster-inference-heres-how-to-implement-it/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers at KAIST AI and Mila have introduced a new Transformer architecture that makes large language models (LLMs) more memory- and compute-efficient. The architecture, called Mixture-of-Recursions (MoR), significantly improves model accuracy and delivers higher throughput compared with vanilla transformers, even when constrained by the same parameter count and compute budget.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-scaling-challenges-of-llms"&gt;The scaling challenges of LLMs&lt;/h2&gt;



&lt;p&gt;The impressive capabilities of today’s LLMs are directly tied to their ever-increasing size. But as these models scale, their memory footprints and computational requirements often become untenable, making both training and deployment challenging for organizations outside of hyperscale data centers. This has led to a search for more efficient designs.&lt;/p&gt;



&lt;p&gt;Efforts to improve LLM efficiency have focused mainly on two methods: parameter sharing and adaptive computation. Parameter sharing techniques reduce the total number of unique parameters by reusing weights across different parts of the model, thereby reducing the overall computational complexity. For example, “layer tying” is a technique that reuses a model’s weights across several layers. Adaptive computation methods adjust models so that they only use as much inference resources as they need. For example, “early exiting” dynamically allocates compute by allowing the model to stop processing “simpler” tokens early in the network.&lt;/p&gt;



&lt;p&gt;However, creating an architecture that effectively unifies both parameter efficiency and adaptive computation remains elusive.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-mixture-of-recursions-works"&gt;How Mixture-of-Recursions works&lt;/h2&gt;



&lt;p&gt;Mixture-of-Recursions is a framework that combines parameter sharing with adaptive computation to tackle the high computational demands of LLMs. It builds on the concept of Recursive Transformers, models that repeatedly apply a set of shared layers multiple times. Instead of a deep stack of unique layers, a Recursive Transformer partitions the model into a few “recursion blocks,” each with a shared pool of parameters. This design allows for more computation without increasing the model’s size.&lt;/p&gt;



&lt;p&gt;MoR enhances this recursive approach with two key components. The first is a lightweight router that intelligently assigns a specific recursion depth to each token. This concept is similar to the routing mechanism in Mixture-of-Experts (MoE) models, where a router directs tokens to specialized expert networks. In MoR, however, the “experts” are the different recursion depths, allowing the model to choose how much computation to apply to each token dynamically. It decides how many times a shared block of layers should be applied based on a token’s complexity, or its required “depth of thinking.” This directs computation only where it is most needed, avoiding wasted cycles on easy-to-process parts of the input.&lt;/p&gt;


&lt;div class="wp-block-image"&gt;
&lt;figure class="aligncenter size-large"&gt;&lt;img alt="Mixture-of-recursion (source: arXiv)" class="wp-image-3014551" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/image_500670.png?w=356" width="356" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Mixture-of-recursion Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;The second component is a more efficient key-value (KV) caching strategy. KV caching is a standard technique that stores information from previous tokens to speed up generation, but it becomes a memory bottleneck in recursive models. MoR introduces a “recursion-wise” KV caching mechanism that selectively stores and retrieves key-value pairs only for the tokens that are still active at a given recursion step. This targeted caching reduces memory traffic and improves throughput without needing complex, post-training modifications.&lt;/p&gt;



&lt;p&gt;As the researchers state in their paper, “In essence, MoR enables models to efficiently adjust their thinking depth on a per-token basis, unifying parameter efficiency with adaptive computation.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Different token routing and KV caching mechanisms for recursive transformers (source: arXiv)" class="wp-image-3014552" height="286" src="https://venturebeat.com/wp-content/uploads/2025/07/image_6c5840.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Different token routing and KV caching mechanisms for recursive transformers Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-mor-in-action"&gt;MoR in action&lt;/h2&gt;



&lt;p&gt;To test their framework, the researchers trained MoR models ranging from 135 million to 1.7 billion parameters and compared them against vanilla and standard recursive baseline models on validation loss and few-shot accuracy benchmarks.&lt;/p&gt;



&lt;p&gt;The results demonstrate significant gains. When given an equal training compute budget, an MoR model achieved higher average few-shot accuracy (43.1% vs. 42.3%) than a vanilla baseline despite using nearly 50% fewer parameters. When trained on the same amount of data, the MoR model reduced training time by 19% and cut peak memory usage by 25% compared to the vanilla model.&lt;/p&gt;



&lt;p&gt;The MoR architecture also proves to be scalable. While it slightly underperformed the vanilla model at the smallest 135M parameter scale, the gap closed rapidly as the model size increased. For models with over 360M parameters, MoR matched or exceeded the performance of standard Transformers, especially on lower compute budgets. Furthermore, MoR’s design dramatically boosts inference throughput. One MoR configuration achieved a 2.06x speedup over the vanilla baseline. For a company operating at scale, this could translate into significant operational cost savings.&lt;/p&gt;



&lt;p&gt;Sangmin Bae, co-author of the paper and a PhD student at KAIST, broke down the practical impact in an email to VentureBeat. “While it’s difficult to provide exact numbers, at a high level, reducing model parameter size and KV cache footprint means we can perform inference on many more samples simultaneously,” he said. “This translates to an increased number of tokens processed at once, and handling longer context windows becomes feasible.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-practical-path-for-enterprise-adoption"&gt;A practical path for enterprise adoption&lt;/h2&gt;



&lt;p&gt;While the paper’s results come from models trained from scratch, a key question for enterprises is how to adopt MoR without massive upfront investment. According to Bae, “uptraining” existing open-source models is a “definitely more cost-effective approach.” He noted that while training a new model is straightforward, an “uptraining approach could be more suitable and efficient until the scalability of MoR itself is fully validated.”&lt;/p&gt;



&lt;p&gt;Adopting MoR also introduces new architectural “knobs” for developers, allowing them to fine-tune the balance between performance and efficiency. This trade-off will depend entirely on the application’s needs.&lt;/p&gt;



&lt;p&gt;“For simpler tasks or scenarios, it may be beneficial to use models with more recursion steps, offering greater flexibility, and vice versa,” Bae explained. He stressed that the “optimal settings will highly depend on the specific deployment setting,” encouraging teams to explore the trade-offs based on the paper’s findings.&lt;/p&gt;



&lt;p&gt;Looking ahead, the MoR framework is “modality-agnostic,” meaning its adaptive computation principles are not limited to text. This opens the door to significant efficiency gains in processing video, audio, and other complex data types.&lt;/p&gt;



&lt;p&gt;“We’re very excited about its potential extension to multi-modality scenarios where efficiency gains are crucial,” Bae said.&lt;/p&gt;



&lt;p&gt;By dynamically adjusting the processing depth for each segment of a video or audio stream, MoR could unlock even greater cost savings and performance improvements, bringing the power of large-scale AI to a wider range of enterprise applications. As the paper concludes, MoR offers “an effective path towards achieving large-model capabilities with significantly reduced computational and memory overhead.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers at KAIST AI and Mila have introduced a new Transformer architecture that makes large language models (LLMs) more memory- and compute-efficient. The architecture, called Mixture-of-Recursions (MoR), significantly improves model accuracy and delivers higher throughput compared with vanilla transformers, even when constrained by the same parameter count and compute budget.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-scaling-challenges-of-llms"&gt;The scaling challenges of LLMs&lt;/h2&gt;



&lt;p&gt;The impressive capabilities of today’s LLMs are directly tied to their ever-increasing size. But as these models scale, their memory footprints and computational requirements often become untenable, making both training and deployment challenging for organizations outside of hyperscale data centers. This has led to a search for more efficient designs.&lt;/p&gt;



&lt;p&gt;Efforts to improve LLM efficiency have focused mainly on two methods: parameter sharing and adaptive computation. Parameter sharing techniques reduce the total number of unique parameters by reusing weights across different parts of the model, thereby reducing the overall computational complexity. For example, “layer tying” is a technique that reuses a model’s weights across several layers. Adaptive computation methods adjust models so that they only use as much inference resources as they need. For example, “early exiting” dynamically allocates compute by allowing the model to stop processing “simpler” tokens early in the network.&lt;/p&gt;



&lt;p&gt;However, creating an architecture that effectively unifies both parameter efficiency and adaptive computation remains elusive.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-mixture-of-recursions-works"&gt;How Mixture-of-Recursions works&lt;/h2&gt;



&lt;p&gt;Mixture-of-Recursions is a framework that combines parameter sharing with adaptive computation to tackle the high computational demands of LLMs. It builds on the concept of Recursive Transformers, models that repeatedly apply a set of shared layers multiple times. Instead of a deep stack of unique layers, a Recursive Transformer partitions the model into a few “recursion blocks,” each with a shared pool of parameters. This design allows for more computation without increasing the model’s size.&lt;/p&gt;



&lt;p&gt;MoR enhances this recursive approach with two key components. The first is a lightweight router that intelligently assigns a specific recursion depth to each token. This concept is similar to the routing mechanism in Mixture-of-Experts (MoE) models, where a router directs tokens to specialized expert networks. In MoR, however, the “experts” are the different recursion depths, allowing the model to choose how much computation to apply to each token dynamically. It decides how many times a shared block of layers should be applied based on a token’s complexity, or its required “depth of thinking.” This directs computation only where it is most needed, avoiding wasted cycles on easy-to-process parts of the input.&lt;/p&gt;


&lt;div class="wp-block-image"&gt;
&lt;figure class="aligncenter size-large"&gt;&lt;img alt="Mixture-of-recursion (source: arXiv)" class="wp-image-3014551" height="600" src="https://venturebeat.com/wp-content/uploads/2025/07/image_500670.png?w=356" width="356" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Mixture-of-recursion Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;p&gt;The second component is a more efficient key-value (KV) caching strategy. KV caching is a standard technique that stores information from previous tokens to speed up generation, but it becomes a memory bottleneck in recursive models. MoR introduces a “recursion-wise” KV caching mechanism that selectively stores and retrieves key-value pairs only for the tokens that are still active at a given recursion step. This targeted caching reduces memory traffic and improves throughput without needing complex, post-training modifications.&lt;/p&gt;



&lt;p&gt;As the researchers state in their paper, “In essence, MoR enables models to efficiently adjust their thinking depth on a per-token basis, unifying parameter efficiency with adaptive computation.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Different token routing and KV caching mechanisms for recursive transformers (source: arXiv)" class="wp-image-3014552" height="286" src="https://venturebeat.com/wp-content/uploads/2025/07/image_6c5840.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Different token routing and KV caching mechanisms for recursive transformers Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-mor-in-action"&gt;MoR in action&lt;/h2&gt;



&lt;p&gt;To test their framework, the researchers trained MoR models ranging from 135 million to 1.7 billion parameters and compared them against vanilla and standard recursive baseline models on validation loss and few-shot accuracy benchmarks.&lt;/p&gt;



&lt;p&gt;The results demonstrate significant gains. When given an equal training compute budget, an MoR model achieved higher average few-shot accuracy (43.1% vs. 42.3%) than a vanilla baseline despite using nearly 50% fewer parameters. When trained on the same amount of data, the MoR model reduced training time by 19% and cut peak memory usage by 25% compared to the vanilla model.&lt;/p&gt;



&lt;p&gt;The MoR architecture also proves to be scalable. While it slightly underperformed the vanilla model at the smallest 135M parameter scale, the gap closed rapidly as the model size increased. For models with over 360M parameters, MoR matched or exceeded the performance of standard Transformers, especially on lower compute budgets. Furthermore, MoR’s design dramatically boosts inference throughput. One MoR configuration achieved a 2.06x speedup over the vanilla baseline. For a company operating at scale, this could translate into significant operational cost savings.&lt;/p&gt;



&lt;p&gt;Sangmin Bae, co-author of the paper and a PhD student at KAIST, broke down the practical impact in an email to VentureBeat. “While it’s difficult to provide exact numbers, at a high level, reducing model parameter size and KV cache footprint means we can perform inference on many more samples simultaneously,” he said. “This translates to an increased number of tokens processed at once, and handling longer context windows becomes feasible.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-practical-path-for-enterprise-adoption"&gt;A practical path for enterprise adoption&lt;/h2&gt;



&lt;p&gt;While the paper’s results come from models trained from scratch, a key question for enterprises is how to adopt MoR without massive upfront investment. According to Bae, “uptraining” existing open-source models is a “definitely more cost-effective approach.” He noted that while training a new model is straightforward, an “uptraining approach could be more suitable and efficient until the scalability of MoR itself is fully validated.”&lt;/p&gt;



&lt;p&gt;Adopting MoR also introduces new architectural “knobs” for developers, allowing them to fine-tune the balance between performance and efficiency. This trade-off will depend entirely on the application’s needs.&lt;/p&gt;



&lt;p&gt;“For simpler tasks or scenarios, it may be beneficial to use models with more recursion steps, offering greater flexibility, and vice versa,” Bae explained. He stressed that the “optimal settings will highly depend on the specific deployment setting,” encouraging teams to explore the trade-offs based on the paper’s findings.&lt;/p&gt;



&lt;p&gt;Looking ahead, the MoR framework is “modality-agnostic,” meaning its adaptive computation principles are not limited to text. This opens the door to significant efficiency gains in processing video, audio, and other complex data types.&lt;/p&gt;



&lt;p&gt;“We’re very excited about its potential extension to multi-modality scenarios where efficiency gains are crucial,” Bae said.&lt;/p&gt;



&lt;p&gt;By dynamically adjusting the processing depth for each segment of a video or audio stream, MoR could unlock even greater cost savings and performance improvements, bringing the power of large-scale AI to a wider range of enterprise applications. As the paper concludes, MoR offers “an effective path towards achieving large-model capabilities with significantly reduced computational and memory overhead.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/mixture-of-recursions-delivers-2x-faster-inference-heres-how-to-implement-it/</guid><pubDate>Wed, 23 Jul 2025 00:05:33 +0000</pubDate></item></channel></rss>