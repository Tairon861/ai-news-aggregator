<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 13 Nov 2025 01:47:10 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>How Deductive AI saved DoorDash 1,000 engineering hours by automating software debugging (AI | VentureBeat)</title><link>https://venturebeat.com/ai/how-deductive-ai-saved-doordash-1-000-engineering-hours-by-automating</link><description>[unable to retrieve full-text content]&lt;p&gt;As software systems grow more complex and AI tools generate code faster than ever, a fundamental problem is getting worse: &lt;a href="https://algocademy.com/blog/why-debugging-takes-longer-than-writing-the-actual-code/"&gt;&lt;u&gt;Engineers are drowning in debugging work&lt;/u&gt;&lt;/a&gt;, spending up to half their time hunting down the causes of software failures instead of building new products. The challenge has become so acute that it&amp;#x27;s creating a new category of tooling — AI agents that can diagnose production failures in minutes instead of hours.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deductive.ai/"&gt;&lt;u&gt;Deductive AI&lt;/u&gt;&lt;/a&gt;, a startup emerging from stealth mode Wednesday, believes it has found a solution by applying reinforcement learning — the same technology that powers game-playing AI systems — to the messy, high-stakes world of production software incidents. The company announced it has raised $7.5 million in seed funding led by &lt;a href="https://www.crv.com/"&gt;&lt;u&gt;CRV&lt;/u&gt;&lt;/a&gt;, with participation from &lt;a href="https://www.databricks.com/databricks-ventures"&gt;&lt;u&gt;Databricks Ventures&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.thomvest.com/"&gt;&lt;u&gt;Thomvest Ventures&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.primeset.com/"&gt;&lt;u&gt;PrimeSet&lt;/u&gt;&lt;/a&gt;, to commercialize what it calls &amp;quot;&lt;a href="https://www.deductive.ai/product"&gt;&lt;u&gt;AI SRE agents&lt;/u&gt;&lt;/a&gt;&amp;quot; that can diagnose and help fix software failures at machine speed.&lt;/p&gt;&lt;p&gt;The pitch resonates with a growing frustration inside engineering organizations: Modern observability tools can show that something broke, but they rarely explain why. When a production system fails at 3 a.m., engineers still face hours of manual detective work, cross-referencing logs, metrics, deployment histories, and code changes across dozens of interconnected services to identify the root cause.&lt;/p&gt;&lt;p&gt;&amp;quot;The complexities and inter-dependencies of modern infrastructure means that investigating the root cause of an outage or incident can feel like searching for a needle in a haystack, except the haystack is the size of a football field, it&amp;#x27;s made of a million other needles, it&amp;#x27;s constantly reshuffling itself, and is on fire — and every second you don&amp;#x27;t find it equals lost revenue,&amp;quot; said Sameer Agarwal, Deductive&amp;#x27;s co-founder and chief technology officer, in an exclusive interview with VentureBeat.&lt;/p&gt;&lt;p&gt;Deductive&amp;#x27;s system builds what the company calls a &amp;quot;knowledge graph&amp;quot; that maps relationships across codebases, telemetry data, engineering discussions, and internal documentation. When an incident occurs, multiple AI agents work together to form hypotheses, test them against live system evidence, and converge on a root cause — mimicking the investigative workflow of experienced site reliability engineers, but completing the process in minutes rather than hours.&lt;/p&gt;&lt;p&gt;The technology has already shown measurable impact at some of the world&amp;#x27;s most demanding production environments. &lt;a href="https://www.deductive.ai/blogs/how-doordash-powers-a-reliable-high-performance-ad-platform-with-deductive-ai"&gt;&lt;u&gt;DoorDash&amp;#x27;s advertising platform&lt;/u&gt;&lt;/a&gt;, which runs real-time auctions that must complete in under 100 milliseconds, has integrated Deductive into its incident response workflow. The company has set an ambitious 2026 goal of resolving production incidents within 10 minutes.&lt;/p&gt;&lt;p&gt;&amp;quot;Our Ads Platform operates at a pace where manual, slow-moving investigations are no longer viable. Every minute of downtime directly affects company revenue,&amp;quot; said Shahrooz Ansari, Senior Director of Engineering at DoorDash, in an interview with VentureBeat. &amp;quot;Deductive has become a critical extension of our team, rapidly synthesizing signals across dozens of services and surfacing the insights that matter—within minutes.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt; estimates that Deductive has root-caused approximately 100 production incidents over the past few months, translating to more than 1,000 hours of annual engineering productivity and a revenue impact &amp;quot;in millions of dollars,&amp;quot; according to Ansari. At location intelligence company &lt;a href="https://foursquare.com/"&gt;&lt;u&gt;Foursquare&lt;/u&gt;&lt;/a&gt;, Deductive reduced the time to diagnose Apache Spark job failures by 90% —t urning a process that previously took hours or days into one that completes in under 10 minutes — while generating over $275,000 in annual savings.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI-generated code is creating a debugging crisis&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The timing of Deductive&amp;#x27;s launch reflects a brewing tension in software development: AI coding assistants are enabling engineers to generate code faster than ever, but the resulting software is often harder to understand and maintain.&lt;/p&gt;&lt;p&gt;&amp;quot;&lt;a href="https://x.com/karpathy/status/1886192184808149383?lang=en"&gt;&lt;u&gt;Vibe coding&lt;/u&gt;&lt;/a&gt;,&amp;quot; a term popularized by AI researcher &lt;a href="https://karpathy.ai/"&gt;&lt;u&gt;Andrej Karpathy&lt;/u&gt;&lt;/a&gt;, refers to using natural-language prompts to generate code through AI assistants. While these tools accelerate development, they can introduce what Agarwal describes as &amp;quot;redundancies, breaks in architectural boundaries, assumptions, or ignored design patterns&amp;quot; that accumulate over time.&lt;/p&gt;&lt;p&gt;&amp;quot;Most AI-generated code still introduces redundancies, breaks architectural boundaries, makes assumptions, or ignores established design patterns,&amp;quot; Agarwal told Venturebeat. &amp;quot;In many ways, we now need AI to help clean up the mess that AI itself is creating.&amp;quot;&lt;/p&gt;&lt;p&gt;The claim that engineers spend roughly half their time on debugging isn&amp;#x27;t hyperbole. The Association for Computing Machinery reports that developers spend &lt;a href="https://queue.acm.org/detail.cfm?id=3404974"&gt;&lt;u&gt;35% to 50% of their time validating and debugging software&lt;/u&gt;&lt;/a&gt;. More recently, &lt;a href="https://www.harness.io/blog/announcing-harness-ai"&gt;&lt;u&gt;Harness&amp;#x27;s State of Software Delivery 2025&lt;/u&gt;&lt;/a&gt; report found that 67% of developers are spending more time debugging AI-generated code.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;ve seen world-class engineers spending half of their time debugging instead of building,&amp;quot; said Rakesh Kothari, Deductive&amp;#x27;s co-founder and CEO. &amp;quot;And as vibe coding generates new code at a rate we&amp;#x27;ve never seen, this problem is only going to get worse.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Deductive&amp;#x27;s AI agents actually investigate production failures&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Deductive&amp;#x27;s technical approach differs substantially from the AI features being added to existing observability platforms like &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://newrelic.com/"&gt;&lt;u&gt;New Relic&lt;/u&gt;&lt;/a&gt;. Most of those systems use large language models to summarize data or identify correlations, but they lack what Agarwal calls &amp;quot;code-aware reasoning&amp;quot;—the ability to understand not just that something broke, but why the code behaves the way it does.&lt;/p&gt;&lt;p&gt;&amp;quot;Most enterprises use multiple observability tools across different teams and services, so no vendor has a single holistic view of how their systems behave, fail, and recover—nor are they able to pair that with an understanding of the code that defines system behavior,&amp;quot; Agarwal explained. &amp;quot;These are key ingredients to resolving software incidents and it is exactly the gap Deductive fills.&amp;quot;&lt;/p&gt;&lt;p&gt;The system connects to existing infrastructure using read-only API access to observability platforms, code repositories, incident management tools, and chat systems. It then continuously builds and updates its knowledge graph, mapping dependencies between services and tracking deployment histories.&lt;/p&gt;&lt;p&gt;When an alert fires, Deductive launches what the company describes as a multi-agent investigation. Different agents specialize in different aspects of the problem: one might analyze recent code changes, another examines trace data, while a third correlates the timing of the incident with recent deployments. The agents share findings and iteratively refine their hypotheses.&lt;/p&gt;&lt;p&gt;The critical difference from rule-based automation is Deductive&amp;#x27;s use of reinforcement learning. The system learns from every incident which investigative steps led to correct diagnoses and which were dead ends. When engineers provide feedback, the system incorporates that signal into its learning model.&lt;/p&gt;&lt;p&gt;&amp;quot;Each time it observes an investigation, it learns which steps, data sources, and decisions led to the right outcome,&amp;quot; Agarwal said. &amp;quot;It learns how to think through problems, not just point them out.&amp;quot;&lt;/p&gt;&lt;p&gt;At DoorDash, a recent latency spike in an API initially appeared to be an isolated service issue. Deductive&amp;#x27;s investigation revealed that the root cause was actually timeout errors from a downstream machine learning platform undergoing a deployment. The system connected these dots by analyzing log volumes, traces, and deployment metadata across multiple services.&lt;/p&gt;&lt;p&gt;&amp;quot;Without Deductive, our team would have had to manually correlate the latency spike across all logs, traces, and deployment histories,&amp;quot; Ansari said. &amp;quot;Deductive was able to explain not just what changed, but how and why it impacted production behavior.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The company keeps humans in the loop—for now&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While Deductive&amp;#x27;s technology could theoretically push fixes directly to production systems, the company has deliberately chosen to keep humans in the loop—at least for now.&lt;/p&gt;&lt;p&gt;&amp;quot;While our system is capable of deeper automation and could push fixes to production, currently, we recommend precise fixes and mitigations that engineers can review, validate, and apply,&amp;quot; Agarwal said. &amp;quot;We believe maintaining a human in the loop is essential for trust, transparency and operational safety.&amp;quot;&lt;/p&gt;&lt;p&gt;However, he acknowledged that &amp;quot;over time, we do think that deeper automation will come and how humans operate in the loop will evolve.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Databricks and ThoughtSpot veterans bet on reasoning over observability&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The founding team brings deep expertise from building some of Silicon Valley&amp;#x27;s most successful data infrastructure platforms. Agarwal earned his Ph.D. at UC Berkeley, where he created &lt;a href="https://arxiv.org/abs/1203.5485"&gt;&lt;u&gt;BlinkDB&lt;/u&gt;&lt;/a&gt;, an influential system for approximate query processing. He was among the first engineers at &lt;a href="https://www.databricks.com/"&gt;&lt;u&gt;Databricks&lt;/u&gt;&lt;/a&gt;, where he helped build &lt;a href="https://docs.databricks.com/gcp/en/spark/?scid=701Vp000004h4b1IAA&amp;amp;utm_medium=paid+search&amp;amp;utm_source=google&amp;amp;utm_campaign=23156677199&amp;amp;utm_adgroup=189768475320&amp;amp;utm_content=aimax&amp;amp;utm_offer=aimax&amp;amp;utm_ad=779965794184&amp;amp;utm_term=apache%20iceberg%20spark&amp;amp;gad_source=1&amp;amp;gad_campaignid=23156677199&amp;amp;gbraid=0AAAAABYBeAhSzbWjXCf1Ok8HU2XzyuNAb&amp;amp;gclid=CjwKCAiA_dDIBhB6EiwAvzc1cN1MnT40-rmesA_-YwBm870Sksy-DQYqWaR9mqQLAIQjzo7yRJpIfBoC7GsQAvD_BwE"&gt;&lt;u&gt;Apache Spark&lt;/u&gt;&lt;/a&gt;. Kothari was an early engineer at &lt;a href="https://www.thoughtspot.com/"&gt;&lt;u&gt;ThoughtSpot&lt;/u&gt;&lt;/a&gt;, where he led teams focused on distributed query processing and large-scale system optimization.&lt;/p&gt;&lt;p&gt;The investor syndicate reflects both the technical credibility and market opportunity. Beyond CRV&amp;#x27;s &lt;a href="https://www.crv.com/team/max-gazor"&gt;&lt;u&gt;Max Gazor&lt;/u&gt;&lt;/a&gt;, the round included participation from &lt;a href="https://sequoiacap.com/podcast/training-data-ion-stoica/"&gt;&lt;u&gt;Ion Stoica&lt;/u&gt;&lt;/a&gt;, founder of Databricks and Anyscale; &lt;a href="https://www.thoughtspot.com/author/ajeet-singh"&gt;&lt;u&gt;Ajeet Singh&lt;/u&gt;&lt;/a&gt;, founder of Nutanix and ThoughtSpot; and &lt;a href="http://bensigelman.org/"&gt;&lt;u&gt;Ben Sigelman&lt;/u&gt;&lt;/a&gt;, founder of Lightstep.&lt;/p&gt;&lt;p&gt;Rather than competing with platforms like &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://www.pagerduty.com/"&gt;&lt;u&gt;PagerDuty&lt;/u&gt;&lt;/a&gt;, Deductive positions itself as a complementary layer that sits on top of existing tools. The pricing model reflects this: Instead of charging based on data volume, Deductive charges based on the number of incidents investigated, plus a base platform fee.&lt;/p&gt;&lt;p&gt;The company offers both cloud-hosted and self-hosted deployment options and emphasizes that it doesn&amp;#x27;t store customer data on its servers or use it to train models for other customers — a critical assurance given the proprietary nature of both code and production system behavior.&lt;/p&gt;&lt;p&gt;With fresh capital and early customer traction at companies like &lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://foursquare.com/"&gt;&lt;u&gt;Foursquare&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://kumo.ai/"&gt;&lt;u&gt;Kumo AI&lt;/u&gt;&lt;/a&gt;, Deductive plans to expand its team and deepen the system&amp;#x27;s reasoning capabilities from reactive incident analysis to proactive prevention. The near-term vision: helping teams predict problems before they occur.&lt;/p&gt;&lt;p&gt;DoorDash&amp;#x27;s Ansari offers a pragmatic endorsement of where the technology stands today: &amp;quot;Investigations that were previously manual and time-consuming are now automated, allowing engineers to shift their energy toward prevention, business impact, and innovation.&amp;quot;&lt;/p&gt;&lt;p&gt;In an industry where every second of downtime translates to lost revenue, that shift from firefighting to building increasingly looks less like a luxury and more like table stakes.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;As software systems grow more complex and AI tools generate code faster than ever, a fundamental problem is getting worse: &lt;a href="https://algocademy.com/blog/why-debugging-takes-longer-than-writing-the-actual-code/"&gt;&lt;u&gt;Engineers are drowning in debugging work&lt;/u&gt;&lt;/a&gt;, spending up to half their time hunting down the causes of software failures instead of building new products. The challenge has become so acute that it&amp;#x27;s creating a new category of tooling — AI agents that can diagnose production failures in minutes instead of hours.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deductive.ai/"&gt;&lt;u&gt;Deductive AI&lt;/u&gt;&lt;/a&gt;, a startup emerging from stealth mode Wednesday, believes it has found a solution by applying reinforcement learning — the same technology that powers game-playing AI systems — to the messy, high-stakes world of production software incidents. The company announced it has raised $7.5 million in seed funding led by &lt;a href="https://www.crv.com/"&gt;&lt;u&gt;CRV&lt;/u&gt;&lt;/a&gt;, with participation from &lt;a href="https://www.databricks.com/databricks-ventures"&gt;&lt;u&gt;Databricks Ventures&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.thomvest.com/"&gt;&lt;u&gt;Thomvest Ventures&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.primeset.com/"&gt;&lt;u&gt;PrimeSet&lt;/u&gt;&lt;/a&gt;, to commercialize what it calls &amp;quot;&lt;a href="https://www.deductive.ai/product"&gt;&lt;u&gt;AI SRE agents&lt;/u&gt;&lt;/a&gt;&amp;quot; that can diagnose and help fix software failures at machine speed.&lt;/p&gt;&lt;p&gt;The pitch resonates with a growing frustration inside engineering organizations: Modern observability tools can show that something broke, but they rarely explain why. When a production system fails at 3 a.m., engineers still face hours of manual detective work, cross-referencing logs, metrics, deployment histories, and code changes across dozens of interconnected services to identify the root cause.&lt;/p&gt;&lt;p&gt;&amp;quot;The complexities and inter-dependencies of modern infrastructure means that investigating the root cause of an outage or incident can feel like searching for a needle in a haystack, except the haystack is the size of a football field, it&amp;#x27;s made of a million other needles, it&amp;#x27;s constantly reshuffling itself, and is on fire — and every second you don&amp;#x27;t find it equals lost revenue,&amp;quot; said Sameer Agarwal, Deductive&amp;#x27;s co-founder and chief technology officer, in an exclusive interview with VentureBeat.&lt;/p&gt;&lt;p&gt;Deductive&amp;#x27;s system builds what the company calls a &amp;quot;knowledge graph&amp;quot; that maps relationships across codebases, telemetry data, engineering discussions, and internal documentation. When an incident occurs, multiple AI agents work together to form hypotheses, test them against live system evidence, and converge on a root cause — mimicking the investigative workflow of experienced site reliability engineers, but completing the process in minutes rather than hours.&lt;/p&gt;&lt;p&gt;The technology has already shown measurable impact at some of the world&amp;#x27;s most demanding production environments. &lt;a href="https://www.deductive.ai/blogs/how-doordash-powers-a-reliable-high-performance-ad-platform-with-deductive-ai"&gt;&lt;u&gt;DoorDash&amp;#x27;s advertising platform&lt;/u&gt;&lt;/a&gt;, which runs real-time auctions that must complete in under 100 milliseconds, has integrated Deductive into its incident response workflow. The company has set an ambitious 2026 goal of resolving production incidents within 10 minutes.&lt;/p&gt;&lt;p&gt;&amp;quot;Our Ads Platform operates at a pace where manual, slow-moving investigations are no longer viable. Every minute of downtime directly affects company revenue,&amp;quot; said Shahrooz Ansari, Senior Director of Engineering at DoorDash, in an interview with VentureBeat. &amp;quot;Deductive has become a critical extension of our team, rapidly synthesizing signals across dozens of services and surfacing the insights that matter—within minutes.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt; estimates that Deductive has root-caused approximately 100 production incidents over the past few months, translating to more than 1,000 hours of annual engineering productivity and a revenue impact &amp;quot;in millions of dollars,&amp;quot; according to Ansari. At location intelligence company &lt;a href="https://foursquare.com/"&gt;&lt;u&gt;Foursquare&lt;/u&gt;&lt;/a&gt;, Deductive reduced the time to diagnose Apache Spark job failures by 90% —t urning a process that previously took hours or days into one that completes in under 10 minutes — while generating over $275,000 in annual savings.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI-generated code is creating a debugging crisis&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The timing of Deductive&amp;#x27;s launch reflects a brewing tension in software development: AI coding assistants are enabling engineers to generate code faster than ever, but the resulting software is often harder to understand and maintain.&lt;/p&gt;&lt;p&gt;&amp;quot;&lt;a href="https://x.com/karpathy/status/1886192184808149383?lang=en"&gt;&lt;u&gt;Vibe coding&lt;/u&gt;&lt;/a&gt;,&amp;quot; a term popularized by AI researcher &lt;a href="https://karpathy.ai/"&gt;&lt;u&gt;Andrej Karpathy&lt;/u&gt;&lt;/a&gt;, refers to using natural-language prompts to generate code through AI assistants. While these tools accelerate development, they can introduce what Agarwal describes as &amp;quot;redundancies, breaks in architectural boundaries, assumptions, or ignored design patterns&amp;quot; that accumulate over time.&lt;/p&gt;&lt;p&gt;&amp;quot;Most AI-generated code still introduces redundancies, breaks architectural boundaries, makes assumptions, or ignores established design patterns,&amp;quot; Agarwal told Venturebeat. &amp;quot;In many ways, we now need AI to help clean up the mess that AI itself is creating.&amp;quot;&lt;/p&gt;&lt;p&gt;The claim that engineers spend roughly half their time on debugging isn&amp;#x27;t hyperbole. The Association for Computing Machinery reports that developers spend &lt;a href="https://queue.acm.org/detail.cfm?id=3404974"&gt;&lt;u&gt;35% to 50% of their time validating and debugging software&lt;/u&gt;&lt;/a&gt;. More recently, &lt;a href="https://www.harness.io/blog/announcing-harness-ai"&gt;&lt;u&gt;Harness&amp;#x27;s State of Software Delivery 2025&lt;/u&gt;&lt;/a&gt; report found that 67% of developers are spending more time debugging AI-generated code.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;ve seen world-class engineers spending half of their time debugging instead of building,&amp;quot; said Rakesh Kothari, Deductive&amp;#x27;s co-founder and CEO. &amp;quot;And as vibe coding generates new code at a rate we&amp;#x27;ve never seen, this problem is only going to get worse.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Deductive&amp;#x27;s AI agents actually investigate production failures&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Deductive&amp;#x27;s technical approach differs substantially from the AI features being added to existing observability platforms like &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://newrelic.com/"&gt;&lt;u&gt;New Relic&lt;/u&gt;&lt;/a&gt;. Most of those systems use large language models to summarize data or identify correlations, but they lack what Agarwal calls &amp;quot;code-aware reasoning&amp;quot;—the ability to understand not just that something broke, but why the code behaves the way it does.&lt;/p&gt;&lt;p&gt;&amp;quot;Most enterprises use multiple observability tools across different teams and services, so no vendor has a single holistic view of how their systems behave, fail, and recover—nor are they able to pair that with an understanding of the code that defines system behavior,&amp;quot; Agarwal explained. &amp;quot;These are key ingredients to resolving software incidents and it is exactly the gap Deductive fills.&amp;quot;&lt;/p&gt;&lt;p&gt;The system connects to existing infrastructure using read-only API access to observability platforms, code repositories, incident management tools, and chat systems. It then continuously builds and updates its knowledge graph, mapping dependencies between services and tracking deployment histories.&lt;/p&gt;&lt;p&gt;When an alert fires, Deductive launches what the company describes as a multi-agent investigation. Different agents specialize in different aspects of the problem: one might analyze recent code changes, another examines trace data, while a third correlates the timing of the incident with recent deployments. The agents share findings and iteratively refine their hypotheses.&lt;/p&gt;&lt;p&gt;The critical difference from rule-based automation is Deductive&amp;#x27;s use of reinforcement learning. The system learns from every incident which investigative steps led to correct diagnoses and which were dead ends. When engineers provide feedback, the system incorporates that signal into its learning model.&lt;/p&gt;&lt;p&gt;&amp;quot;Each time it observes an investigation, it learns which steps, data sources, and decisions led to the right outcome,&amp;quot; Agarwal said. &amp;quot;It learns how to think through problems, not just point them out.&amp;quot;&lt;/p&gt;&lt;p&gt;At DoorDash, a recent latency spike in an API initially appeared to be an isolated service issue. Deductive&amp;#x27;s investigation revealed that the root cause was actually timeout errors from a downstream machine learning platform undergoing a deployment. The system connected these dots by analyzing log volumes, traces, and deployment metadata across multiple services.&lt;/p&gt;&lt;p&gt;&amp;quot;Without Deductive, our team would have had to manually correlate the latency spike across all logs, traces, and deployment histories,&amp;quot; Ansari said. &amp;quot;Deductive was able to explain not just what changed, but how and why it impacted production behavior.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The company keeps humans in the loop—for now&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While Deductive&amp;#x27;s technology could theoretically push fixes directly to production systems, the company has deliberately chosen to keep humans in the loop—at least for now.&lt;/p&gt;&lt;p&gt;&amp;quot;While our system is capable of deeper automation and could push fixes to production, currently, we recommend precise fixes and mitigations that engineers can review, validate, and apply,&amp;quot; Agarwal said. &amp;quot;We believe maintaining a human in the loop is essential for trust, transparency and operational safety.&amp;quot;&lt;/p&gt;&lt;p&gt;However, he acknowledged that &amp;quot;over time, we do think that deeper automation will come and how humans operate in the loop will evolve.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Databricks and ThoughtSpot veterans bet on reasoning over observability&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The founding team brings deep expertise from building some of Silicon Valley&amp;#x27;s most successful data infrastructure platforms. Agarwal earned his Ph.D. at UC Berkeley, where he created &lt;a href="https://arxiv.org/abs/1203.5485"&gt;&lt;u&gt;BlinkDB&lt;/u&gt;&lt;/a&gt;, an influential system for approximate query processing. He was among the first engineers at &lt;a href="https://www.databricks.com/"&gt;&lt;u&gt;Databricks&lt;/u&gt;&lt;/a&gt;, where he helped build &lt;a href="https://docs.databricks.com/gcp/en/spark/?scid=701Vp000004h4b1IAA&amp;amp;utm_medium=paid+search&amp;amp;utm_source=google&amp;amp;utm_campaign=23156677199&amp;amp;utm_adgroup=189768475320&amp;amp;utm_content=aimax&amp;amp;utm_offer=aimax&amp;amp;utm_ad=779965794184&amp;amp;utm_term=apache%20iceberg%20spark&amp;amp;gad_source=1&amp;amp;gad_campaignid=23156677199&amp;amp;gbraid=0AAAAABYBeAhSzbWjXCf1Ok8HU2XzyuNAb&amp;amp;gclid=CjwKCAiA_dDIBhB6EiwAvzc1cN1MnT40-rmesA_-YwBm870Sksy-DQYqWaR9mqQLAIQjzo7yRJpIfBoC7GsQAvD_BwE"&gt;&lt;u&gt;Apache Spark&lt;/u&gt;&lt;/a&gt;. Kothari was an early engineer at &lt;a href="https://www.thoughtspot.com/"&gt;&lt;u&gt;ThoughtSpot&lt;/u&gt;&lt;/a&gt;, where he led teams focused on distributed query processing and large-scale system optimization.&lt;/p&gt;&lt;p&gt;The investor syndicate reflects both the technical credibility and market opportunity. Beyond CRV&amp;#x27;s &lt;a href="https://www.crv.com/team/max-gazor"&gt;&lt;u&gt;Max Gazor&lt;/u&gt;&lt;/a&gt;, the round included participation from &lt;a href="https://sequoiacap.com/podcast/training-data-ion-stoica/"&gt;&lt;u&gt;Ion Stoica&lt;/u&gt;&lt;/a&gt;, founder of Databricks and Anyscale; &lt;a href="https://www.thoughtspot.com/author/ajeet-singh"&gt;&lt;u&gt;Ajeet Singh&lt;/u&gt;&lt;/a&gt;, founder of Nutanix and ThoughtSpot; and &lt;a href="http://bensigelman.org/"&gt;&lt;u&gt;Ben Sigelman&lt;/u&gt;&lt;/a&gt;, founder of Lightstep.&lt;/p&gt;&lt;p&gt;Rather than competing with platforms like &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://www.pagerduty.com/"&gt;&lt;u&gt;PagerDuty&lt;/u&gt;&lt;/a&gt;, Deductive positions itself as a complementary layer that sits on top of existing tools. The pricing model reflects this: Instead of charging based on data volume, Deductive charges based on the number of incidents investigated, plus a base platform fee.&lt;/p&gt;&lt;p&gt;The company offers both cloud-hosted and self-hosted deployment options and emphasizes that it doesn&amp;#x27;t store customer data on its servers or use it to train models for other customers — a critical assurance given the proprietary nature of both code and production system behavior.&lt;/p&gt;&lt;p&gt;With fresh capital and early customer traction at companies like &lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://foursquare.com/"&gt;&lt;u&gt;Foursquare&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://kumo.ai/"&gt;&lt;u&gt;Kumo AI&lt;/u&gt;&lt;/a&gt;, Deductive plans to expand its team and deepen the system&amp;#x27;s reasoning capabilities from reactive incident analysis to proactive prevention. The near-term vision: helping teams predict problems before they occur.&lt;/p&gt;&lt;p&gt;DoorDash&amp;#x27;s Ansari offers a pragmatic endorsement of where the technology stands today: &amp;quot;Investigations that were previously manual and time-consuming are now automated, allowing engineers to shift their energy toward prevention, business impact, and innovation.&amp;quot;&lt;/p&gt;&lt;p&gt;In an industry where every second of downtime translates to lost revenue, that shift from firefighting to building increasingly looks less like a luxury and more like table stakes.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/how-deductive-ai-saved-doordash-1-000-engineering-hours-by-automating</guid><pubDate>Wed, 12 Nov 2025 14:00:00 +0000</pubDate></item><item><title>Faster Than a Click: Hyperlink Agent Search Now Available on NVIDIA RTX PCs (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-nexa-hyperlink-local-agent/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Large language model (LLM)-based AI assistants are powerful productivity tools, but without the right context and information, they can struggle to provide nuanced, relevant answers. While most LLM-based chat apps allow users to supply a few files for context, they often don’t have access to all the information buried across slides, notes, PDFs and images in a user’s PC.&lt;/p&gt;
&lt;p&gt;Nexa.ai’s Hyperlink is a local AI agent that addresses this challenge. It can quickly index thousands of files, understand the intent of a user’s question and provide contextual, tailored insights.&lt;/p&gt;
&lt;p&gt;A new version of the app, available today, includes accelerations for NVIDIA RTX AI PCs, tripling retrieval-augmented generation indexing speed. For example, a dense 1GB folder that would previously take almost 15 minutes to index can now be ready for search in just four to five minutes. In addition, LLM inference is accelerated by 2x for faster responses to user queries.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87266"&gt;&lt;img alt="alt" class="size-large wp-image-87266" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/Nexa-Benchmarks-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87266"&gt;Hyperlink on NVIDIA RTX AI PCs delivers up to 3x faster indexing and 2x faster LLM inference. Benchmarked on an RTX 5090 using a test dataset; indexing measured as total index time, inference measured in tokens per second.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Turn Local Data Into Instant Intelligence&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Hyperlink uses generative AI to search thousands of files for the right information, understanding the intent and context of a user’s query, rather than merely matching keywords.&lt;/p&gt;
&lt;p&gt;To do this, it creates a searchable index of all local files a user indicates — whether a small folder or every single file on a computer. Users can describe what they’re looking for in natural language and find relevant content across documents, slides, PDFs and images.&lt;/p&gt;
&lt;p&gt;For example, if a user asks for help with a “Sci-Fi book report comparing themes between two novels,” Hyperlink can find the relevant information — even if it’s saved in a file named “Lit_Homework_Final.docx.”&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Combining search with the reasoning capabilities of RTX-accelerated LLMs, Hyperlink then answers questions based on insights from a user’s files. It connects ideas across sources, identifies relationships between documents and generates well-reasoned answers with clear citations.&lt;/p&gt;
&lt;p&gt;All user data stays on the device and is kept private. This means personal files never leave the computer, so users don’t have to worry about sensitive information being sent to the cloud. They get the benefits of powerful AI without sacrificing control or peace of mind.&lt;/p&gt;
&lt;p&gt;Hyperlink is already being adopted by professionals, students and creators to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Prepare for meetings&lt;/b&gt;: Summarize key discussion points across notes and transcripts.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Analyze reports&lt;/b&gt;: Get well-researched answers, citing key data points from across industry reports.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Create content: &lt;/b&gt;Compile writing or video ideas from years of saved notes and drafts.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Study smarter: &lt;/b&gt;Review a key concept while cramming for a test, searching through lecture notes, slides and tutorials — all at once.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Organize receipts: &lt;/b&gt;Sort scanned documents and automatically complete expense reports.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Debug faster:&lt;/b&gt; Search across documentation and comments in code to resolve errors or version conflicts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download the Hyperlink app to start experimenting with AI search on RTX PCs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Large language model (LLM)-based AI assistants are powerful productivity tools, but without the right context and information, they can struggle to provide nuanced, relevant answers. While most LLM-based chat apps allow users to supply a few files for context, they often don’t have access to all the information buried across slides, notes, PDFs and images in a user’s PC.&lt;/p&gt;
&lt;p&gt;Nexa.ai’s Hyperlink is a local AI agent that addresses this challenge. It can quickly index thousands of files, understand the intent of a user’s question and provide contextual, tailored insights.&lt;/p&gt;
&lt;p&gt;A new version of the app, available today, includes accelerations for NVIDIA RTX AI PCs, tripling retrieval-augmented generation indexing speed. For example, a dense 1GB folder that would previously take almost 15 minutes to index can now be ready for search in just four to five minutes. In addition, LLM inference is accelerated by 2x for faster responses to user queries.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87266"&gt;&lt;img alt="alt" class="size-large wp-image-87266" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/Nexa-Benchmarks-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87266"&gt;Hyperlink on NVIDIA RTX AI PCs delivers up to 3x faster indexing and 2x faster LLM inference. Benchmarked on an RTX 5090 using a test dataset; indexing measured as total index time, inference measured in tokens per second.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Turn Local Data Into Instant Intelligence&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Hyperlink uses generative AI to search thousands of files for the right information, understanding the intent and context of a user’s query, rather than merely matching keywords.&lt;/p&gt;
&lt;p&gt;To do this, it creates a searchable index of all local files a user indicates — whether a small folder or every single file on a computer. Users can describe what they’re looking for in natural language and find relevant content across documents, slides, PDFs and images.&lt;/p&gt;
&lt;p&gt;For example, if a user asks for help with a “Sci-Fi book report comparing themes between two novels,” Hyperlink can find the relevant information — even if it’s saved in a file named “Lit_Homework_Final.docx.”&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Combining search with the reasoning capabilities of RTX-accelerated LLMs, Hyperlink then answers questions based on insights from a user’s files. It connects ideas across sources, identifies relationships between documents and generates well-reasoned answers with clear citations.&lt;/p&gt;
&lt;p&gt;All user data stays on the device and is kept private. This means personal files never leave the computer, so users don’t have to worry about sensitive information being sent to the cloud. They get the benefits of powerful AI without sacrificing control or peace of mind.&lt;/p&gt;
&lt;p&gt;Hyperlink is already being adopted by professionals, students and creators to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Prepare for meetings&lt;/b&gt;: Summarize key discussion points across notes and transcripts.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Analyze reports&lt;/b&gt;: Get well-researched answers, citing key data points from across industry reports.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Create content: &lt;/b&gt;Compile writing or video ideas from years of saved notes and drafts.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Study smarter: &lt;/b&gt;Review a key concept while cramming for a test, searching through lecture notes, slides and tutorials — all at once.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Organize receipts: &lt;/b&gt;Sort scanned documents and automatically complete expense reports.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Debug faster:&lt;/b&gt; Search across documentation and comments in code to resolve errors or version conflicts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download the Hyperlink app to start experimenting with AI search on RTX PCs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-nexa-hyperlink-local-agent/</guid><pubDate>Wed, 12 Nov 2025 14:00:21 +0000</pubDate></item><item><title>Differentially private machine learning at scale with JAX-Privacy (The latest research from Google)</title><link>https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.&lt;/p&gt;&lt;p&gt;That’s where JAX and JAX-Privacy come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including automatic differentiation, just-in-time compilation, and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX has become a cornerstone for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including Flax, which simplifies the implementation of neural network architectures, and Optax, which implements state-of-the-art optimizers.&lt;/p&gt;&lt;p&gt;Built on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement differentially private (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our advances on private training. It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.&lt;/p&gt;&lt;p&gt;Today, we are proud to announce the release of JAX-Privacy 1.0. Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.&lt;/p&gt;&lt;p&gt;That’s where JAX and JAX-Privacy come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including automatic differentiation, just-in-time compilation, and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX has become a cornerstone for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including Flax, which simplifies the implementation of neural network architectures, and Optax, which implements state-of-the-art optimizers.&lt;/p&gt;&lt;p&gt;Built on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement differentially private (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our advances on private training. It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.&lt;/p&gt;&lt;p&gt;Today, we are proud to announce the release of JAX-Privacy 1.0. Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/</guid><pubDate>Wed, 12 Nov 2025 15:32:00 +0000</pubDate></item><item><title>Anthropic announces $50 billion data center plan (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/anthropic-announces-50-billion-data-center-plan/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-2154160973-e1723115200227.jpg?resize=1200,673" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic on Wednesday said it had signed an ambitious new data center partnership with U.K.-based neocloud provider Fluidstack, committing $50 billion to building facilities across the U.S. to meet its growing compute needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The data centers will be located in Texas and New York, and come online throughout 2026. The company described the sites as “custom built for Anthropic with a focus on maximizing efficiency for our workloads.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re getting closer to AI that can accelerate scientific discovery and help solve complex problems in ways that weren’t possible before,” Anthropic’s CEO and co-founder Dario Amodei (pictured above) said in a statement. “Realizing that potential requires infrastructure that can support continued development at the frontier.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of the intense compute demands of Anthropic’s Claude family of models, the company is already engaged in significant cloud partnerships with both Google and Amazon (which is also an investor). But this is the company’s first major effort to build custom infrastructure. The $50 billion outlay, while large, is in line with the company’s internal revenue projections, which reportedly see Anthropic reaching $70 billion in revenue and $17 billion in positive cash flow by 2028.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While $50 billion represents a massive commitment in both cash and compute power, it is nonetheless dwarfed by similar projects from Anthropic’s competitors. Meta has committed to building $600 billion worth of data centers over the next three years, while the Stargate partnership between SoftBank, OpenAI and Oracle has already planned $500 billion in infrastructure spending. The spending has fueled concerns about an AI bubble due to flagging demand or even misallocated spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project marks a major success for Fluidstack, a relatively young neocloud company that has become a vendor of choice in the AI building boom. Founded in 2017, the company was named in February as the primary partner for a 1 gigawatt AI project backed by the French government, which represented more than $11 billion in spending. According to Forbes, the company already has partnerships in place with Meta, Black Forest Labs, and France’s Mistral.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fluidstack was also one of the first third-party vendors to receive Google’s custom-built TPUs, a major vote of confidence for the company.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-2154160973-e1723115200227.jpg?resize=1200,673" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic on Wednesday said it had signed an ambitious new data center partnership with U.K.-based neocloud provider Fluidstack, committing $50 billion to building facilities across the U.S. to meet its growing compute needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The data centers will be located in Texas and New York, and come online throughout 2026. The company described the sites as “custom built for Anthropic with a focus on maximizing efficiency for our workloads.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re getting closer to AI that can accelerate scientific discovery and help solve complex problems in ways that weren’t possible before,” Anthropic’s CEO and co-founder Dario Amodei (pictured above) said in a statement. “Realizing that potential requires infrastructure that can support continued development at the frontier.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of the intense compute demands of Anthropic’s Claude family of models, the company is already engaged in significant cloud partnerships with both Google and Amazon (which is also an investor). But this is the company’s first major effort to build custom infrastructure. The $50 billion outlay, while large, is in line with the company’s internal revenue projections, which reportedly see Anthropic reaching $70 billion in revenue and $17 billion in positive cash flow by 2028.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While $50 billion represents a massive commitment in both cash and compute power, it is nonetheless dwarfed by similar projects from Anthropic’s competitors. Meta has committed to building $600 billion worth of data centers over the next three years, while the Stargate partnership between SoftBank, OpenAI and Oracle has already planned $500 billion in infrastructure spending. The spending has fueled concerns about an AI bubble due to flagging demand or even misallocated spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project marks a major success for Fluidstack, a relatively young neocloud company that has become a vendor of choice in the AI building boom. Founded in 2017, the company was named in February as the primary partner for a 1 gigawatt AI project backed by the French government, which represented more than $11 billion in spending. According to Forbes, the company already has partnerships in place with Meta, Black Forest Labs, and France’s Mistral.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fluidstack was also one of the first third-party vendors to receive Google’s custom-built TPUs, a major vote of confidence for the company.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/anthropic-announces-50-billion-data-center-plan/</guid><pubDate>Wed, 12 Nov 2025 15:52:47 +0000</pubDate></item><item><title>AI data startup WisdomAI has raised another $50M, led by Kleiner, Nvidia (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/ai-data-startup-wisdomai-has-raised-another-50m-led-by-kleiner-nvidia/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/WisdomAI-cofounders.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;WisdomAI, the new AI data analytics startup from Rubrik co-founder Soham Mazumdar, has landed a fresh $50 million Series A led by Kleiner Perkins with participation from new investor NVentures (Nvidia’s venture capital arm).&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This round comes roughly six months after the startup announced a seed round of $23 million led by Coatue.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;WisdomAI offers AI-driven data analytics that can answer business questions from structured, unstructured, and even “dirty” data, meaning data that hasn’t been cleaned of typos or errors.&amp;nbsp;A business user simply asks questions in natural language, such as, “How many customers do I have in my pipeline and what’s preventing them from closing this quarter?”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the company is using a clever method to eliminate the LLM hallucination problem. WisdomAI is not using LLMs to write answers to questions. Instead, LLMs are only used to write the query —  the part that will go out to a data warehouse to retrieve data. So if the LLM hallucinates, it will simply write an ineffective query rather than inventing false answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;WisdomAI has written its own logic that it calls the “enterprise context layer,” which studies the customer data to understand it. All of the startup’s co-founders worked with Mazumdar at data security company Rubrik, giving them deep experience with enterprise storage warehouses. (Mazumdar left Rubrik in 2023.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since formally launching in late 2024, WisdomAI has grown from two enterprise customers to around 40 enterprise customers, CEO Mazumdar says. WisdomAI counts such companies like Descope, ConocoPhillips, Cisco, and Patreon as customers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mazumdar said the young company is also growing usage within its enterprise customers. Some customers have doubled usage within two months. Another customer started with 10 seats and expanded to 450, which is almost everyone in the company, he said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the last six months, WisdomAI has also added an agentic feature that will alert users in real time to important changes in situations they are monitoring.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I have created an agent which is watching our product usage metrics, our ticket information,” Mazumdar says, adding that it took him about five minutes to create it. But rather than sending him a daily or, even, hourly report on usage and helpdesk tickets, it alerts him “when something interesting happens,” he describes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think that’s the magic with analytics. It’s always been a static report, but we are making it dynamic. We are making it proactive,” he said.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/WisdomAI-cofounders.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;WisdomAI, the new AI data analytics startup from Rubrik co-founder Soham Mazumdar, has landed a fresh $50 million Series A led by Kleiner Perkins with participation from new investor NVentures (Nvidia’s venture capital arm).&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This round comes roughly six months after the startup announced a seed round of $23 million led by Coatue.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;WisdomAI offers AI-driven data analytics that can answer business questions from structured, unstructured, and even “dirty” data, meaning data that hasn’t been cleaned of typos or errors.&amp;nbsp;A business user simply asks questions in natural language, such as, “How many customers do I have in my pipeline and what’s preventing them from closing this quarter?”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the company is using a clever method to eliminate the LLM hallucination problem. WisdomAI is not using LLMs to write answers to questions. Instead, LLMs are only used to write the query —  the part that will go out to a data warehouse to retrieve data. So if the LLM hallucinates, it will simply write an ineffective query rather than inventing false answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;WisdomAI has written its own logic that it calls the “enterprise context layer,” which studies the customer data to understand it. All of the startup’s co-founders worked with Mazumdar at data security company Rubrik, giving them deep experience with enterprise storage warehouses. (Mazumdar left Rubrik in 2023.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since formally launching in late 2024, WisdomAI has grown from two enterprise customers to around 40 enterprise customers, CEO Mazumdar says. WisdomAI counts such companies like Descope, ConocoPhillips, Cisco, and Patreon as customers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mazumdar said the young company is also growing usage within its enterprise customers. Some customers have doubled usage within two months. Another customer started with 10 seats and expanded to 450, which is almost everyone in the company, he said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the last six months, WisdomAI has also added an agentic feature that will alert users in real time to important changes in situations they are monitoring.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I have created an agent which is watching our product usage metrics, our ticket information,” Mazumdar says, adding that it took him about five minutes to create it. But rather than sending him a daily or, even, hourly report on usage and helpdesk tickets, it alerts him “when something interesting happens,” he describes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think that’s the magic with analytics. It’s always been a static report, but we are making it dynamic. We are making it proactive,” he said.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/ai-data-startup-wisdomai-has-raised-another-50m-led-by-kleiner-nvidia/</guid><pubDate>Wed, 12 Nov 2025 16:00:00 +0000</pubDate></item><item><title>NVIDIA Wins Every MLPerf Training v5.1 Benchmark (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/mlperf-training-benchmark-blackwell-ultra/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;In the age of AI reasoning, training smarter, more capable models is critical to scaling intelligence. Delivering the massive performance to meet this new age requires breakthroughs across GPUs, CPUs, NICs, scale-up and scale-out networking, system architectures, and mountains of software and algorithms.&lt;/p&gt;
&lt;p&gt;In MLPerf Training v5.1 — the latest round in a long-running series of industry-standard tests of AI training performance — NVIDIA swept all seven tests, delivering the fastest time to train across large language models (LLMs), image generation, recommender systems, computer vision and graph neural networks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87247 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/nvidia-wins-every-mlperf-training-benchmark-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA was also the only platform to submit results on every test, underscoring the rich programmability of NVIDIA GPUs, and the maturity and versatility of its CUDA software stack.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Blackwell Ultra Doubles Down&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The GB300 NVL72 rack-scale system, powered by the NVIDIA Blackwell Ultra GPU architecture, made its debut in MLPerf Training this round, following a record-setting showing in the most recent MLPerf Inference round.&lt;/p&gt;
&lt;p&gt;Compared with the prior-generation Hopper architecture, the Blackwell Ultra-based GB300 NVL72 delivered more than 4x the Llama 3.1 405B pretraining and nearly 5x the Llama 2 70B LoRA fine-tuning performance using the same number of GPUs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87319 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/large-llm-training-leap-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;These gains were fueled by Blackwell Ultra’s architectural improvements — including new Tensor Cores that offer 15 petaflops of NVFP4 AI compute, twice the attention-layer compute and 279GB of HBM3e memory — as well as new training methods that tapped into the architecture’s enormous NVFP4 compute performance.&lt;/p&gt;
&lt;p&gt;Connecting multiple GB300 NVL72 systems, the NVIDIA Quantum-X800 InfiniBand platform — the industry’s first end-to-end 800 Gb/s scale-up networking platform — also made its MLPerf debut, doubling scale-out networking bandwidth compared with the prior generation.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Performance Unlocked: NVFP4 Accelerates LLM Training&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Key to the outstanding results this round was performing calculations using NVFP4 precision — a first in the history of MLPerf Training.&lt;/p&gt;
&lt;p&gt;One way to increase compute performance is to build an architecture capable of performing computations on data represented with fewer bits, and then to perform those calculations at a faster rate. However, lower precision means less information is available in each calculation. This means using low-precision calculations in the training process calls for careful design decisions to keep results accurate.&lt;/p&gt;
&lt;p&gt;NVIDIA teams innovated at every layer of the stack to adopt FP4 precision for LLM training. The NVIDIA Blackwell GPU can perform FP4 calculations — including the NVIDIA-designed NVFP4 format as well as other FP4 variants — at double the rate of FP8. Blackwell Ultra boosts that to 3x, enabling the GPUs to deliver substantially greater AI compute performance.&lt;/p&gt;
&lt;p&gt;NVIDIA is the only platform to date that has submitted MLPerf Training results with calculations performed using FP4 precision while meeting the benchmark’s strict accuracy requirements.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Blackwell Scales to New Heights&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA set a new Llama 3.1 405B time-to-train record of just 10 minutes, powered by more than 5,000 Blackwell GPUs working together efficiently. This entry was 2.7x faster than the best Blackwell-based result submitted in the prior round, resulting from efficient scaling to more than twice the number of GPUs, as well as the use of NVFP4 precision to dramatically increase the effective performance of each Blackwell GPU.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87316 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/gb200-nvl4-new-record-at-scale-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;To illustrate the performance increase per GPU, NVIDIA submitted results this round using 2,560 Blackwell GPUs, achieving a time to train of 18.79 minutes — 45% faster than the submission last round using 2,496 GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;New Benchmarks, New Records&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA also set performance records on the two new benchmarks added this round: Llama 3.1 8B and FLUX.1.&lt;/p&gt;
&lt;p&gt;Llama 3.1 8B — a compact yet highly capable LLM — replaced the long-running BERT-large model, adding a modern, smaller LLM to the benchmark suite. NVIDIA submitted results with up to 512 Blackwell Ultra GPUs, setting the bar at 5.2 minutes to train.&lt;/p&gt;
&lt;p&gt;In addition, FLUX.1 — a state-of-the-art image generation model — replaced Stable Diffusion v2, with only the NVIDIA platform submitting results on the benchmark. NVIDIA submitted results using 1,152 Blackwell GPUs, setting a record time to train of 12.5 minutes.&lt;/p&gt;
&lt;p&gt;NVIDIA continued to hold records on the existing graph neural network, object detection and recommender system tests.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;A Broad and Deep Partner Ecosystem&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA ecosystem participated extensively this round, with compelling submissions from 15 organizations including ASUSTeK, Dell Technologies, Giga Computing, Hewlett Packard Enterprise, Krai, Lambda, Lenovo, Nebius, Quanta Cloud Technology, Supermicro, University of Florida, Verda (formerly DataCrunch) and Wiwynn.&lt;/p&gt;
&lt;p&gt;NVIDIA is innovating at a one-year rhythm, driving significant and rapid performance increases across pretraining, post-training and inference — paving the way to new levels of intelligence and accelerating AI adoption.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See more NVIDIA performance data on the &lt;/i&gt;&lt;i&gt;Data Center Deep Learning Product Performance Hub&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Performance Explorer&lt;/i&gt;&lt;i&gt; pages.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;In the age of AI reasoning, training smarter, more capable models is critical to scaling intelligence. Delivering the massive performance to meet this new age requires breakthroughs across GPUs, CPUs, NICs, scale-up and scale-out networking, system architectures, and mountains of software and algorithms.&lt;/p&gt;
&lt;p&gt;In MLPerf Training v5.1 — the latest round in a long-running series of industry-standard tests of AI training performance — NVIDIA swept all seven tests, delivering the fastest time to train across large language models (LLMs), image generation, recommender systems, computer vision and graph neural networks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87247 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/nvidia-wins-every-mlperf-training-benchmark-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA was also the only platform to submit results on every test, underscoring the rich programmability of NVIDIA GPUs, and the maturity and versatility of its CUDA software stack.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Blackwell Ultra Doubles Down&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The GB300 NVL72 rack-scale system, powered by the NVIDIA Blackwell Ultra GPU architecture, made its debut in MLPerf Training this round, following a record-setting showing in the most recent MLPerf Inference round.&lt;/p&gt;
&lt;p&gt;Compared with the prior-generation Hopper architecture, the Blackwell Ultra-based GB300 NVL72 delivered more than 4x the Llama 3.1 405B pretraining and nearly 5x the Llama 2 70B LoRA fine-tuning performance using the same number of GPUs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87319 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/large-llm-training-leap-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;These gains were fueled by Blackwell Ultra’s architectural improvements — including new Tensor Cores that offer 15 petaflops of NVFP4 AI compute, twice the attention-layer compute and 279GB of HBM3e memory — as well as new training methods that tapped into the architecture’s enormous NVFP4 compute performance.&lt;/p&gt;
&lt;p&gt;Connecting multiple GB300 NVL72 systems, the NVIDIA Quantum-X800 InfiniBand platform — the industry’s first end-to-end 800 Gb/s scale-up networking platform — also made its MLPerf debut, doubling scale-out networking bandwidth compared with the prior generation.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Performance Unlocked: NVFP4 Accelerates LLM Training&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Key to the outstanding results this round was performing calculations using NVFP4 precision — a first in the history of MLPerf Training.&lt;/p&gt;
&lt;p&gt;One way to increase compute performance is to build an architecture capable of performing computations on data represented with fewer bits, and then to perform those calculations at a faster rate. However, lower precision means less information is available in each calculation. This means using low-precision calculations in the training process calls for careful design decisions to keep results accurate.&lt;/p&gt;
&lt;p&gt;NVIDIA teams innovated at every layer of the stack to adopt FP4 precision for LLM training. The NVIDIA Blackwell GPU can perform FP4 calculations — including the NVIDIA-designed NVFP4 format as well as other FP4 variants — at double the rate of FP8. Blackwell Ultra boosts that to 3x, enabling the GPUs to deliver substantially greater AI compute performance.&lt;/p&gt;
&lt;p&gt;NVIDIA is the only platform to date that has submitted MLPerf Training results with calculations performed using FP4 precision while meeting the benchmark’s strict accuracy requirements.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Blackwell Scales to New Heights&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA set a new Llama 3.1 405B time-to-train record of just 10 minutes, powered by more than 5,000 Blackwell GPUs working together efficiently. This entry was 2.7x faster than the best Blackwell-based result submitted in the prior round, resulting from efficient scaling to more than twice the number of GPUs, as well as the use of NVFP4 precision to dramatically increase the effective performance of each Blackwell GPU.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87316 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/gb200-nvl4-new-record-at-scale-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;To illustrate the performance increase per GPU, NVIDIA submitted results this round using 2,560 Blackwell GPUs, achieving a time to train of 18.79 minutes — 45% faster than the submission last round using 2,496 GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;New Benchmarks, New Records&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA also set performance records on the two new benchmarks added this round: Llama 3.1 8B and FLUX.1.&lt;/p&gt;
&lt;p&gt;Llama 3.1 8B — a compact yet highly capable LLM — replaced the long-running BERT-large model, adding a modern, smaller LLM to the benchmark suite. NVIDIA submitted results with up to 512 Blackwell Ultra GPUs, setting the bar at 5.2 minutes to train.&lt;/p&gt;
&lt;p&gt;In addition, FLUX.1 — a state-of-the-art image generation model — replaced Stable Diffusion v2, with only the NVIDIA platform submitting results on the benchmark. NVIDIA submitted results using 1,152 Blackwell GPUs, setting a record time to train of 12.5 minutes.&lt;/p&gt;
&lt;p&gt;NVIDIA continued to hold records on the existing graph neural network, object detection and recommender system tests.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;A Broad and Deep Partner Ecosystem&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA ecosystem participated extensively this round, with compelling submissions from 15 organizations including ASUSTeK, Dell Technologies, Giga Computing, Hewlett Packard Enterprise, Krai, Lambda, Lenovo, Nebius, Quanta Cloud Technology, Supermicro, University of Florida, Verda (formerly DataCrunch) and Wiwynn.&lt;/p&gt;
&lt;p&gt;NVIDIA is innovating at a one-year rhythm, driving significant and rapid performance increases across pretraining, post-training and inference — paving the way to new levels of intelligence and accelerating AI adoption.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See more NVIDIA performance data on the &lt;/i&gt;&lt;i&gt;Data Center Deep Learning Product Performance Hub&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Performance Explorer&lt;/i&gt;&lt;i&gt; pages.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/mlperf-training-benchmark-blackwell-ultra/</guid><pubDate>Wed, 12 Nov 2025 16:00:35 +0000</pubDate></item><item><title>Baidu ERNIE multimodal AI beats GPT and Gemini in benchmarks (AI News)</title><link>https://www.artificialintelligence-news.com/news/baidu-ernie-multimodal-ai-gpt-and-gemini-benchmarks/</link><description>&lt;p&gt;Baidu’s latest ERNIE model, a super-efficient multimodal AI, is beating GPT and Gemini on key benchmarks and targets enterprise data often ignored by text-focused models.&lt;/p&gt;&lt;p&gt;For many businesses, valuable insights are locked in engineering schematics, factory-floor video feeds, medical scans, and logistics dashboards. Baidu’s new model, ERNIE-4.5-VL-28B-A3B-Thinking, is designed to fill this gap.&lt;/p&gt;&lt;p&gt;What’s interesting to enterprise architects is not just its multimodal capability, but its architecture. It’s described as a “lightweight” model, activating only three billion parameters during operation. This approach targets the high inference costs that often stall AI-scaling projects. Baidu is betting on efficiency as a path to adoption, training the system as a foundation for “multimodal agents” that can reason and act, not just perceive.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-complex-visual-data-analysis-capabilities-supported-by-ai-benchmarks"&gt;Complex visual data analysis capabilities supported by AI benchmarks&lt;/h3&gt;&lt;p&gt;Baidu’s multimodal ERNIE AI model excels at handling dense, non-text data. For example, it can interpret a “Peak Time Reminder” chart to find optimal visiting hours, a task that reflects the resource-scheduling challenges in logistics or retail.&lt;/p&gt;&lt;p&gt;ERNIE 4.5 also shows capability in technical domains, like solving a bridge circuit diagram by applying Ohm’s and Kirchhoff’s laws. For R&amp;amp;D and engineering arms, a future assistant could validate designs or explain complex schematics to new hires.&lt;/p&gt;&lt;p&gt;This capability is supported by Baidu’s benchmarks, which show ERNIE-4.5-VL-28B-A3B-Thinking outperforming competitors like GPT-5-High and Gemini 2.5 Pro on some key tests:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;MathVista: ERNIE (82.5) vs Gemini (82.3) and GPT (81.3)&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;ChartQA: ERNIE (87.1) vs Gemini (76.3) and GPT (78.2)&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;VLMs Are Blind: ERNIE (77.3) vs Gemini (76.5) and GPT (69.6)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It’s worth noting, of course, that AI benchmarks provide a guide but can be flawed. Always perform internal tests for your needs before deploying any AI model for mission-critical applications.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-baidu-shifts-from-perception-to-automation-with-its-latest-ernie-ai-model"&gt;Baidu shifts from perception to automation with its latest ERNIE AI model&lt;/h3&gt;&lt;p&gt;The primary hurdle for enterprise AI is moving from perception (“what is this?”) to automation (“what now?”). ERNIE 4.5 claims to address this by integrating visual grounding with tool use.&lt;/p&gt;&lt;p&gt;Asking the multimodal AI to find all people wearing suits in an image and return their coordinates in JSON format works. The model generates the structured data, a function easily transferable to a production line for visual inspection or to a system auditing site images for safety compliance.&lt;/p&gt;&lt;p&gt;The model also manages external tools and can autonomously zoom in on a photograph to read small text. If it faces an unknown object, it can trigger an image search to identify it. This represents a less passive form of AI that could power an agent to not only flag a data centre error, but also zoom in on the code, search the internal knowledge base, and suggest the fix.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-unlocking-business-intelligence-with-multimodal-ai"&gt;Unlocking business intelligence with multimodal AI&lt;/h3&gt;&lt;p&gt;Baidu’s latest ERNIE AI model also targets corporate video archives from training sessions and meetings to security footage. It can extract all on-screen subtitles and map them to their precise timestamps.&lt;/p&gt;&lt;p&gt;It also demonstrates temporal awareness, finding specific scenes (like those “filmed on a bridge”) by analysing visual cues. The clear end-goal is making vast video libraries searchable, allowing an employee to find the exact moment a specific topic was discussed in a two-hour webinar they may have dozed off a couple of times during.&lt;/p&gt;&lt;p&gt;Baidu provides deployment guidance for several paths, including transformers, vLLM, and FastDeploy. However, the hardware requirements are a major barrier. A single-card deployment needs 80GB of GPU memory. This is not a tool for casual experimentation, but for organisations with existing and high-performance AI infrastructure.&lt;/p&gt;&lt;p&gt;For those with the hardware, Baidu’s ERNIEKit toolkit allows fine-tuning on proprietary data; a necessity for most high-value use cases. Baidu is providing its latest ERNIE AI model with an Apache 2.0 licence that permits commercial use, which is essential for adoption.&lt;/p&gt;&lt;p&gt;The market is finally moving toward multimodal AI that can see, read, and act within a specific business context, and the benchmarks suggest it’s doing so with impressive capability. The immediate task is to identify high-value visual reasoning jobs within your own operation and weigh them against the substantial hardware and governance costs.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Wiz: Security lapses emerge amid the global AI race&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Baidu’s latest ERNIE model, a super-efficient multimodal AI, is beating GPT and Gemini on key benchmarks and targets enterprise data often ignored by text-focused models.&lt;/p&gt;&lt;p&gt;For many businesses, valuable insights are locked in engineering schematics, factory-floor video feeds, medical scans, and logistics dashboards. Baidu’s new model, ERNIE-4.5-VL-28B-A3B-Thinking, is designed to fill this gap.&lt;/p&gt;&lt;p&gt;What’s interesting to enterprise architects is not just its multimodal capability, but its architecture. It’s described as a “lightweight” model, activating only three billion parameters during operation. This approach targets the high inference costs that often stall AI-scaling projects. Baidu is betting on efficiency as a path to adoption, training the system as a foundation for “multimodal agents” that can reason and act, not just perceive.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-complex-visual-data-analysis-capabilities-supported-by-ai-benchmarks"&gt;Complex visual data analysis capabilities supported by AI benchmarks&lt;/h3&gt;&lt;p&gt;Baidu’s multimodal ERNIE AI model excels at handling dense, non-text data. For example, it can interpret a “Peak Time Reminder” chart to find optimal visiting hours, a task that reflects the resource-scheduling challenges in logistics or retail.&lt;/p&gt;&lt;p&gt;ERNIE 4.5 also shows capability in technical domains, like solving a bridge circuit diagram by applying Ohm’s and Kirchhoff’s laws. For R&amp;amp;D and engineering arms, a future assistant could validate designs or explain complex schematics to new hires.&lt;/p&gt;&lt;p&gt;This capability is supported by Baidu’s benchmarks, which show ERNIE-4.5-VL-28B-A3B-Thinking outperforming competitors like GPT-5-High and Gemini 2.5 Pro on some key tests:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;MathVista: ERNIE (82.5) vs Gemini (82.3) and GPT (81.3)&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;ChartQA: ERNIE (87.1) vs Gemini (76.3) and GPT (78.2)&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;VLMs Are Blind: ERNIE (77.3) vs Gemini (76.5) and GPT (69.6)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It’s worth noting, of course, that AI benchmarks provide a guide but can be flawed. Always perform internal tests for your needs before deploying any AI model for mission-critical applications.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-baidu-shifts-from-perception-to-automation-with-its-latest-ernie-ai-model"&gt;Baidu shifts from perception to automation with its latest ERNIE AI model&lt;/h3&gt;&lt;p&gt;The primary hurdle for enterprise AI is moving from perception (“what is this?”) to automation (“what now?”). ERNIE 4.5 claims to address this by integrating visual grounding with tool use.&lt;/p&gt;&lt;p&gt;Asking the multimodal AI to find all people wearing suits in an image and return their coordinates in JSON format works. The model generates the structured data, a function easily transferable to a production line for visual inspection or to a system auditing site images for safety compliance.&lt;/p&gt;&lt;p&gt;The model also manages external tools and can autonomously zoom in on a photograph to read small text. If it faces an unknown object, it can trigger an image search to identify it. This represents a less passive form of AI that could power an agent to not only flag a data centre error, but also zoom in on the code, search the internal knowledge base, and suggest the fix.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-unlocking-business-intelligence-with-multimodal-ai"&gt;Unlocking business intelligence with multimodal AI&lt;/h3&gt;&lt;p&gt;Baidu’s latest ERNIE AI model also targets corporate video archives from training sessions and meetings to security footage. It can extract all on-screen subtitles and map them to their precise timestamps.&lt;/p&gt;&lt;p&gt;It also demonstrates temporal awareness, finding specific scenes (like those “filmed on a bridge”) by analysing visual cues. The clear end-goal is making vast video libraries searchable, allowing an employee to find the exact moment a specific topic was discussed in a two-hour webinar they may have dozed off a couple of times during.&lt;/p&gt;&lt;p&gt;Baidu provides deployment guidance for several paths, including transformers, vLLM, and FastDeploy. However, the hardware requirements are a major barrier. A single-card deployment needs 80GB of GPU memory. This is not a tool for casual experimentation, but for organisations with existing and high-performance AI infrastructure.&lt;/p&gt;&lt;p&gt;For those with the hardware, Baidu’s ERNIEKit toolkit allows fine-tuning on proprietary data; a necessity for most high-value use cases. Baidu is providing its latest ERNIE AI model with an Apache 2.0 licence that permits commercial use, which is essential for adoption.&lt;/p&gt;&lt;p&gt;The market is finally moving toward multimodal AI that can see, read, and act within a specific business context, and the benchmarks suggest it’s doing so with impressive capability. The immediate task is to identify high-value visual reasoning jobs within your own operation and weigh them against the substantial hardware and governance costs.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Wiz: Security lapses emerge amid the global AI race&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/baidu-ernie-multimodal-ai-gpt-and-gemini-benchmarks/</guid><pubDate>Wed, 12 Nov 2025 16:09:44 +0000</pubDate></item><item><title>Meta’s star AI scientist Yann LeCun plans to leave for own startup (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/metas-star-ai-scientist-yann-lecun-plans-to-leave-for-own-startup/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI pioneer reportedly frustrated with Meta’s shift from research to rapid product releases.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="205" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-300x205.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Yann LeCun, Vice President and Chief AI Scientist for Meta Platforms, as seen in September 2023.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Photo by Kevin Dietsch/Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Meta’s chief AI scientist and Turing Award winner Yann LeCun plans to leave the company to launch his own startup focused on a different type of AI called “world models,” the Financial Times reported. The French-US scientist has reportedly told associates he will depart in the coming months and is already in early talks to raise funds for the new venture. The departure comes as CEO Mark Zuckerberg radically overhauled Meta’s AI operations after deciding the company had fallen behind rivals such as OpenAI and Google.&lt;/p&gt;
&lt;p&gt;World models are hypothetical AI systems that some AI engineers expect to develop an internal “understanding” of the physical world by learning from video and spatial data rather than text alone. Unlike current large language models (such as the kind that power ChatGPT) that predict the next segment of data in a sequence, world models would ideally simulate cause-and-effect scenarios, understand physics, and enable machines to reason and plan more like animals do. LeCun has said this architecture could take a decade to fully develop.&lt;/p&gt;
&lt;p&gt;While some AI experts believe that Transformer-based AI models—such as large language models, video synthesis models, and interactive world synthesis models—have emergently modeled physics or absorbed the structural rules of the physical world from training data examples, the evidence so far generally points to sophisticated pattern-matching rather than a base understanding of how the physical world actually works.&lt;/p&gt;
&lt;p&gt;LeCun’s planned exit is the latest in a string of leadership reshuffles at Meta during what has been a tumultuous year for the company. A key turning point was the disappointing launch and benchmark-gaming controversy of the AI language model Llama 4 in April, which many in the industry saw as a flop when it performed worse than the most advanced offerings from Google, OpenAI, and Anthropic. Meanwhile, the Meta AI chatbot has failed to gain traction with consumers and suffered controversies and setbacks over its interactions with children.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A different approach to AI&lt;/h2&gt;
&lt;p&gt;LeCun founded Meta’s Fundamental AI Research lab, known as FAIR, in 2013 and has served as the company’s chief AI scientist ever since. He is one of three researchers who won the 2018 Turing Award for pioneering work on deep learning and convolutional neural networks. After leaving Meta, LeCun will remain a professor at New York University, where he has taught since 2003.&lt;/p&gt;
&lt;p&gt;LeCun has previously argued that large language models like Llama that Zuckerberg has put at the center of his strategy are useful, but they will never be able to reason and plan like humans, increasingly appearing to contradict his boss’s grandiose AI vision for developing “superintelligence.”&lt;/p&gt;
&lt;p&gt;For example, in May 2024, when an OpenAI researcher discussed the need to control ultra-intelligent AI, LeCun responded on X by writing that before urgently figuring out how to control AI systems much smarter than humans, researchers need to have the beginning of a hint of a design for a system smarter than a house cat.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2104309 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="569" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/zuckmetaverse-1024x569.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Mark Zuckerberg once believed the “metaverse” was the future and renamed his company because of it.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Facebook

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Within FAIR, LeCun has instead focused on developing world models that can truly plan and reason. Over the past year, though, Meta’s AI research groups have seen growing tension and mass layoffs as Zuckerberg has shifted the company’s AI strategy away from long-term research and toward the rapid deployment of commercial products.&lt;/p&gt;
&lt;p&gt;Over the summer, Zuckerberg hired Alexandr Wang to lead a new superintelligence team at Meta, paying $14.3 billion to hire the 28-year-old founder of data-labeling startup Scale AI and acquire a 49 percent interest in his company. LeCun, who had previously reported to Chief Product Officer Chris Cox, now reports to Wang, which seems like a sharp rebuke of LeCun’s approach to AI.&lt;/p&gt;
&lt;p&gt;Zuckerberg also personally handpicked an exclusive team called TBD Lab to accelerate the development of the next iteration of large language models, luring staff from rivals such as OpenAI and Google with astonishingly large $100 to $250 million pay packages. As a result, Zuckerberg has come under growing pressure from Wall Street to show that his multibillion-dollar investment in becoming an AI leader will pay off and boost revenue. But if it turns out like his previous pivot to the metaverse, Zuckerberg’s latest bet could prove equally expensive and unfruitful.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI pioneer reportedly frustrated with Meta’s shift from research to rapid product releases.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="205" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-300x205.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Yann LeCun, Vice President and Chief AI Scientist for Meta Platforms, as seen in September 2023.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Photo by Kevin Dietsch/Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Meta’s chief AI scientist and Turing Award winner Yann LeCun plans to leave the company to launch his own startup focused on a different type of AI called “world models,” the Financial Times reported. The French-US scientist has reportedly told associates he will depart in the coming months and is already in early talks to raise funds for the new venture. The departure comes as CEO Mark Zuckerberg radically overhauled Meta’s AI operations after deciding the company had fallen behind rivals such as OpenAI and Google.&lt;/p&gt;
&lt;p&gt;World models are hypothetical AI systems that some AI engineers expect to develop an internal “understanding” of the physical world by learning from video and spatial data rather than text alone. Unlike current large language models (such as the kind that power ChatGPT) that predict the next segment of data in a sequence, world models would ideally simulate cause-and-effect scenarios, understand physics, and enable machines to reason and plan more like animals do. LeCun has said this architecture could take a decade to fully develop.&lt;/p&gt;
&lt;p&gt;While some AI experts believe that Transformer-based AI models—such as large language models, video synthesis models, and interactive world synthesis models—have emergently modeled physics or absorbed the structural rules of the physical world from training data examples, the evidence so far generally points to sophisticated pattern-matching rather than a base understanding of how the physical world actually works.&lt;/p&gt;
&lt;p&gt;LeCun’s planned exit is the latest in a string of leadership reshuffles at Meta during what has been a tumultuous year for the company. A key turning point was the disappointing launch and benchmark-gaming controversy of the AI language model Llama 4 in April, which many in the industry saw as a flop when it performed worse than the most advanced offerings from Google, OpenAI, and Anthropic. Meanwhile, the Meta AI chatbot has failed to gain traction with consumers and suffered controversies and setbacks over its interactions with children.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A different approach to AI&lt;/h2&gt;
&lt;p&gt;LeCun founded Meta’s Fundamental AI Research lab, known as FAIR, in 2013 and has served as the company’s chief AI scientist ever since. He is one of three researchers who won the 2018 Turing Award for pioneering work on deep learning and convolutional neural networks. After leaving Meta, LeCun will remain a professor at New York University, where he has taught since 2003.&lt;/p&gt;
&lt;p&gt;LeCun has previously argued that large language models like Llama that Zuckerberg has put at the center of his strategy are useful, but they will never be able to reason and plan like humans, increasingly appearing to contradict his boss’s grandiose AI vision for developing “superintelligence.”&lt;/p&gt;
&lt;p&gt;For example, in May 2024, when an OpenAI researcher discussed the need to control ultra-intelligent AI, LeCun responded on X by writing that before urgently figuring out how to control AI systems much smarter than humans, researchers need to have the beginning of a hint of a design for a system smarter than a house cat.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2104309 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="569" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/zuckmetaverse-1024x569.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Mark Zuckerberg once believed the “metaverse” was the future and renamed his company because of it.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Facebook

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Within FAIR, LeCun has instead focused on developing world models that can truly plan and reason. Over the past year, though, Meta’s AI research groups have seen growing tension and mass layoffs as Zuckerberg has shifted the company’s AI strategy away from long-term research and toward the rapid deployment of commercial products.&lt;/p&gt;
&lt;p&gt;Over the summer, Zuckerberg hired Alexandr Wang to lead a new superintelligence team at Meta, paying $14.3 billion to hire the 28-year-old founder of data-labeling startup Scale AI and acquire a 49 percent interest in his company. LeCun, who had previously reported to Chief Product Officer Chris Cox, now reports to Wang, which seems like a sharp rebuke of LeCun’s approach to AI.&lt;/p&gt;
&lt;p&gt;Zuckerberg also personally handpicked an exclusive team called TBD Lab to accelerate the development of the next iteration of large language models, luring staff from rivals such as OpenAI and Google with astonishingly large $100 to $250 million pay packages. As a result, Zuckerberg has come under growing pressure from Wall Street to show that his multibillion-dollar investment in becoming an AI leader will pay off and boost revenue. But if it turns out like his previous pivot to the metaverse, Zuckerberg’s latest bet could prove equally expensive and unfruitful.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/metas-star-ai-scientist-yann-lecun-plans-to-leave-for-own-startup/</guid><pubDate>Wed, 12 Nov 2025 17:14:16 +0000</pubDate></item><item><title>What startups want from OpenAI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/what-startups-want-from-openai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54885185707_36953b477e_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Marc Manara, OpenAI’s head of startups, says the reality of AI has advanced far beyond ideas and experiments. AI-native companies are hitting $200 million in annual recurring revenue, and product cycles have shrunk from two-week sprints to single days. And OpenAI is helping. &lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s Equity podcast, Russell Brandom sits down with Manara at TechCrunch Disrupt 2025 to explore how OpenAI is serving the startups building on its platform.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The shift from two-week sprints to one-day development cycles, and what that means for how startups should structure their engineering teams&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why some startups are customizing models for specific tasks in healthcare, finance, and other verticals that seemed out of reach&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Where AI still hasn’t fully integrated with companies, and why longer-horizon autonomous tasks remain the next frontier for both models and startups&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54885185707_36953b477e_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Marc Manara, OpenAI’s head of startups, says the reality of AI has advanced far beyond ideas and experiments. AI-native companies are hitting $200 million in annual recurring revenue, and product cycles have shrunk from two-week sprints to single days. And OpenAI is helping. &lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s Equity podcast, Russell Brandom sits down with Manara at TechCrunch Disrupt 2025 to explore how OpenAI is serving the startups building on its platform.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The shift from two-week sprints to one-day development cycles, and what that means for how startups should structure their engineering teams&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why some startups are customizing models for specific tasks in healthcare, finance, and other verticals that seemed out of reach&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Where AI still hasn’t fully integrated with companies, and why longer-horizon autonomous tasks remain the next frontier for both models and startups&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/what-startups-want-from-openai/</guid><pubDate>Wed, 12 Nov 2025 17:18:48 +0000</pubDate></item><item><title>OpenAI slams court order that lets NYT read 20 million complete user chats (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/11/openai-fights-order-to-hand-over-20-million-private-chatgpt-conversations/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI: NYT wants evidence of ChatGPT users trying to get around news paywall.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A person's finger tapping a ChatGPT app icon on an iPhone screen that also shows icons for DeepSeek, Gemini, Copilot, Grok, and Claude." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-640x427.jpg" width="640" /&gt;
                  &lt;img alt="A person's finger tapping a ChatGPT app icon on an iPhone screen that also shows icons for DeepSeek, Gemini, Copilot, Grok, and Claude." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-1152x648-1762971088.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | alexsl

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI wants a court to reverse a ruling forcing the ChatGPT maker to give 20 million user chats to The New York Times and other news plaintiffs that sued it over alleged copyright infringement. Although OpenAI previously offered 20 million user chats as a counter to the NYT’s demand for 120 million, the AI company says a court order requiring production of the chats is too broad.&lt;/p&gt;
&lt;p&gt;“The logs at issue here are &lt;em&gt;complete conversations&lt;/em&gt;: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT,” OpenAI said today in a filing in US District Court for the Southern District of New York. “Disclosure of those logs is thus much more likely to expose private information [than individual prompt-output pairs], in the same way that eavesdropping on an entire conversation reveals more private information than a 5-second conversation fragment.”&lt;/p&gt;
&lt;p&gt;OpenAI’s filing said that “more than 99.99%” of the chats “have &lt;em&gt;nothing to do&lt;/em&gt; with this case.” It asked the district court to “vacate the order and order News Plaintiffs to respond to OpenAI’s proposal for identifying relevant logs.” OpenAI could also seek review in a federal court of appeals.&lt;/p&gt;
&lt;p&gt;OpenAI posted a message on its website to users today saying that “The New York Times is demanding that we turn over 20 million of your private ChatGPT conversations” in order to “find examples of you using ChatGPT to try to get around their paywall.”&lt;/p&gt;
&lt;p&gt;ChatGPT users concerned about privacy have more to worry about than the NYT case. For example, ChatGPT conversations have been found in Google search results and the Google Search Console tool that developers can use to monitor search traffic. OpenAI today said it plans to develop “advanced security features designed to keep your data private, including client-side encryption for your messages with ChatGPT. ”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI: AI chats should be treated like private emails&lt;/h2&gt;
&lt;p&gt;OpenAI’s court filing argues that the chat log production should be narrowed based on the relevance of chats to the case.&lt;/p&gt;
&lt;p&gt;“OpenAI is unaware of any court ordering wholesale production of personal information at this scale,” the filing said. “This sets a dangerous precedent: it suggests that anyone who files a lawsuit against an AI company can demand production of tens of millions of conversations without first narrowing for relevance. This is not how discovery works in other cases: courts do not allow plaintiffs suing Google to dig through the private emails of tens of millions of Gmail users irrespective of their relevance. And it is not how discovery should work for generative AI tools either.”&lt;/p&gt;
&lt;p&gt;A November 7 order by US Magistrate Judge Ona Wang sided with the NYT, saying that OpenAI must “produce the 20 million de-identified Consumer ChatGPT Logs to News Plaintiffs by November 14, 2025, or within 7 days of completing the de-identification process.” Wang ruled that the production must go forward even though the parties don’t agree on whether the logs must be produced in full:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Whether or not the parties had reached agreement to produce the 20 million Consumer ChatGPT Logs &lt;u&gt;in whole&lt;/u&gt;—which the parties vehemently dispute—such production here is appropriate. OpenAI has failed to explain how its consumers’ privacy rights are not adequately protected by: (1) the existing protective order in this multidistrict litigation or (2) OpenAI’s exhaustive de-identification of all of the 20 million Consumer ChatGPT Logs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OpenAI’s filing today said the court order “did not acknowledge OpenAI’s sworn witness declaration explaining that the de-identification process is not intended to remove information that is non-identifying but may nonetheless be private, like a Washington Post reporter’s hypothetical use of ChatGPT to assist in the preparation of a news article.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The New York Times provided a statement today after being contacted by Ars. “The New York Times’s case against OpenAI and Microsoft is about holding these companies accountable for stealing millions of copyrighted works to create products that directly compete with The Times,” the company said. “In another attempt to cover up its illegal conduct, OpenAI’s blog post purposely misleads its users and omits the facts. No ChatGPT user’s privacy is at risk. The court ordered OpenAI to provide a sample of chats, anonymized by OpenAI itself, under a legal protective order. This fear-mongering is all the more dishonest given that OpenAI’s own terms of service permit the company to train its models on users’ chats and turn over chats for litigation.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Chats stored under legal hold&lt;/h2&gt;
&lt;p&gt;The 20 million chats consist of a random sampling of ChatGPT conversations from December 2022 to November 2024 and do not include chats of business customers, OpenAI said in the message on its website.&lt;/p&gt;
&lt;p&gt;“We presented several privacy-preserving options to The Times, including targeted searches over the sample (&lt;em&gt;e.g.&lt;/em&gt;, to search for chats that might include text from a New York Times article so they only receive the conversations relevant to their claims), as well as high-level data classifying how ChatGPT was used in the sample. These were rejected by The Times,” OpenAI said.&lt;/p&gt;
&lt;p&gt;The chats are stored in a secure system that is “protected under legal hold, meaning it can’t be accessed or used for purposes other than meeting legal obligations,” OpenAI said. The NYT “would be legally obligated at this time to not make any data public outside the court process,” and OpenAI said it will fight any attempts to make the user conversations public.&lt;/p&gt;
&lt;p&gt;A NYT filing on October 30 accused OpenAI of defying prior agreements “by refusing to produce even a small sample of the billions of model outputs that its conduct has put in issue in this case.” The filing continued:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;blockquote&gt;&lt;p&gt;Immediate production of the output log sample is essential to stay on track for the February 26, 2026, discovery deadline. OpenAI’s proposal to run searches on this small subset of its model outputs on Plaintiffs’ behalf is as inefficient as it is inadequate to allow Plaintiffs to fairly analyze how “real world” users interact with a core product at the center of this litigation. Plaintiffs cannot reasonably conduct expert analyses about how OpenAI’s models function in its core consumer-facing product, how retrieval augmented generation (“RAG”) functions to deliver news content, how consumers interact with that product, and the frequency of hallucinations without access to the model outputs themselves.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OpenAI said the NYT’s discovery requests were initially limited to logs “related to Times content” and that it has “been working to satisfy those requests by sampling conversation logs. Towards the end of that process, News Plaintiffs filed a motion with a new demand: that instead of finding and producing logs that are ‘related to Times content,’ OpenAI should hand over the entire 20 million-log sample ‘via hard drive.'”&lt;/p&gt;
&lt;h2&gt;OpenAI disputes judge’s reasoning&lt;/h2&gt;
&lt;p&gt;The November 7 order cited a California case, &lt;em&gt;Concord Music Group, Inc. v. Anthropic PBC&lt;/em&gt;, in which US District Magistrate Judge Susan van Keulen ordered the production of 5 million records. OpenAI consistently relied on van Keulen’s use of a sample-size formula “in support of its previous proposed methodology for conversation data sampling, but fails to explain why Judge [van] Keulen’s subsequent order directing production of the entire 5 million-record sample to the plaintiff in that case is not similarly instructive here,” Wang wrote.&lt;/p&gt;
&lt;p&gt;OpenAI’s filing today said the company was never given an opportunity to explain why &lt;em&gt;Concord&lt;/em&gt; shouldn’t apply in this case because the news plaintiffs did not reference it in their motion.&lt;/p&gt;
&lt;p&gt;“The cited &lt;em&gt;Concord&lt;/em&gt; order was not about whether wholesale production of the sample was appropriate; it was about the mechanism through which Anthropic would effectuate an &lt;em&gt;already agreed-upon&lt;/em&gt; production,” OpenAI wrote. “Nothing about that order suggests that Judge van Keulen would have ordered wholesale production had Anthropic raised the privacy concerns that OpenAI has raised throughout this case.”&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Concord&lt;/em&gt; logs were just prompt-output pairs, “&lt;em&gt;i.e.&lt;/em&gt;, a single user prompt followed by a single model output,” OpenAI wrote. “The logs at issue here are &lt;em&gt;complete conversations&lt;/em&gt;: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT.” That could result in “up to 80 million prompt-output pairs,” OpenAI said.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI: NYT wants evidence of ChatGPT users trying to get around news paywall.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A person's finger tapping a ChatGPT app icon on an iPhone screen that also shows icons for DeepSeek, Gemini, Copilot, Grok, and Claude." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-640x427.jpg" width="640" /&gt;
                  &lt;img alt="A person's finger tapping a ChatGPT app icon on an iPhone screen that also shows icons for DeepSeek, Gemini, Copilot, Grok, and Claude." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-1152x648-1762971088.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | alexsl

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI wants a court to reverse a ruling forcing the ChatGPT maker to give 20 million user chats to The New York Times and other news plaintiffs that sued it over alleged copyright infringement. Although OpenAI previously offered 20 million user chats as a counter to the NYT’s demand for 120 million, the AI company says a court order requiring production of the chats is too broad.&lt;/p&gt;
&lt;p&gt;“The logs at issue here are &lt;em&gt;complete conversations&lt;/em&gt;: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT,” OpenAI said today in a filing in US District Court for the Southern District of New York. “Disclosure of those logs is thus much more likely to expose private information [than individual prompt-output pairs], in the same way that eavesdropping on an entire conversation reveals more private information than a 5-second conversation fragment.”&lt;/p&gt;
&lt;p&gt;OpenAI’s filing said that “more than 99.99%” of the chats “have &lt;em&gt;nothing to do&lt;/em&gt; with this case.” It asked the district court to “vacate the order and order News Plaintiffs to respond to OpenAI’s proposal for identifying relevant logs.” OpenAI could also seek review in a federal court of appeals.&lt;/p&gt;
&lt;p&gt;OpenAI posted a message on its website to users today saying that “The New York Times is demanding that we turn over 20 million of your private ChatGPT conversations” in order to “find examples of you using ChatGPT to try to get around their paywall.”&lt;/p&gt;
&lt;p&gt;ChatGPT users concerned about privacy have more to worry about than the NYT case. For example, ChatGPT conversations have been found in Google search results and the Google Search Console tool that developers can use to monitor search traffic. OpenAI today said it plans to develop “advanced security features designed to keep your data private, including client-side encryption for your messages with ChatGPT. ”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI: AI chats should be treated like private emails&lt;/h2&gt;
&lt;p&gt;OpenAI’s court filing argues that the chat log production should be narrowed based on the relevance of chats to the case.&lt;/p&gt;
&lt;p&gt;“OpenAI is unaware of any court ordering wholesale production of personal information at this scale,” the filing said. “This sets a dangerous precedent: it suggests that anyone who files a lawsuit against an AI company can demand production of tens of millions of conversations without first narrowing for relevance. This is not how discovery works in other cases: courts do not allow plaintiffs suing Google to dig through the private emails of tens of millions of Gmail users irrespective of their relevance. And it is not how discovery should work for generative AI tools either.”&lt;/p&gt;
&lt;p&gt;A November 7 order by US Magistrate Judge Ona Wang sided with the NYT, saying that OpenAI must “produce the 20 million de-identified Consumer ChatGPT Logs to News Plaintiffs by November 14, 2025, or within 7 days of completing the de-identification process.” Wang ruled that the production must go forward even though the parties don’t agree on whether the logs must be produced in full:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Whether or not the parties had reached agreement to produce the 20 million Consumer ChatGPT Logs &lt;u&gt;in whole&lt;/u&gt;—which the parties vehemently dispute—such production here is appropriate. OpenAI has failed to explain how its consumers’ privacy rights are not adequately protected by: (1) the existing protective order in this multidistrict litigation or (2) OpenAI’s exhaustive de-identification of all of the 20 million Consumer ChatGPT Logs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OpenAI’s filing today said the court order “did not acknowledge OpenAI’s sworn witness declaration explaining that the de-identification process is not intended to remove information that is non-identifying but may nonetheless be private, like a Washington Post reporter’s hypothetical use of ChatGPT to assist in the preparation of a news article.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The New York Times provided a statement today after being contacted by Ars. “The New York Times’s case against OpenAI and Microsoft is about holding these companies accountable for stealing millions of copyrighted works to create products that directly compete with The Times,” the company said. “In another attempt to cover up its illegal conduct, OpenAI’s blog post purposely misleads its users and omits the facts. No ChatGPT user’s privacy is at risk. The court ordered OpenAI to provide a sample of chats, anonymized by OpenAI itself, under a legal protective order. This fear-mongering is all the more dishonest given that OpenAI’s own terms of service permit the company to train its models on users’ chats and turn over chats for litigation.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Chats stored under legal hold&lt;/h2&gt;
&lt;p&gt;The 20 million chats consist of a random sampling of ChatGPT conversations from December 2022 to November 2024 and do not include chats of business customers, OpenAI said in the message on its website.&lt;/p&gt;
&lt;p&gt;“We presented several privacy-preserving options to The Times, including targeted searches over the sample (&lt;em&gt;e.g.&lt;/em&gt;, to search for chats that might include text from a New York Times article so they only receive the conversations relevant to their claims), as well as high-level data classifying how ChatGPT was used in the sample. These were rejected by The Times,” OpenAI said.&lt;/p&gt;
&lt;p&gt;The chats are stored in a secure system that is “protected under legal hold, meaning it can’t be accessed or used for purposes other than meeting legal obligations,” OpenAI said. The NYT “would be legally obligated at this time to not make any data public outside the court process,” and OpenAI said it will fight any attempts to make the user conversations public.&lt;/p&gt;
&lt;p&gt;A NYT filing on October 30 accused OpenAI of defying prior agreements “by refusing to produce even a small sample of the billions of model outputs that its conduct has put in issue in this case.” The filing continued:&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;blockquote&gt;&lt;p&gt;Immediate production of the output log sample is essential to stay on track for the February 26, 2026, discovery deadline. OpenAI’s proposal to run searches on this small subset of its model outputs on Plaintiffs’ behalf is as inefficient as it is inadequate to allow Plaintiffs to fairly analyze how “real world” users interact with a core product at the center of this litigation. Plaintiffs cannot reasonably conduct expert analyses about how OpenAI’s models function in its core consumer-facing product, how retrieval augmented generation (“RAG”) functions to deliver news content, how consumers interact with that product, and the frequency of hallucinations without access to the model outputs themselves.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OpenAI said the NYT’s discovery requests were initially limited to logs “related to Times content” and that it has “been working to satisfy those requests by sampling conversation logs. Towards the end of that process, News Plaintiffs filed a motion with a new demand: that instead of finding and producing logs that are ‘related to Times content,’ OpenAI should hand over the entire 20 million-log sample ‘via hard drive.'”&lt;/p&gt;
&lt;h2&gt;OpenAI disputes judge’s reasoning&lt;/h2&gt;
&lt;p&gt;The November 7 order cited a California case, &lt;em&gt;Concord Music Group, Inc. v. Anthropic PBC&lt;/em&gt;, in which US District Magistrate Judge Susan van Keulen ordered the production of 5 million records. OpenAI consistently relied on van Keulen’s use of a sample-size formula “in support of its previous proposed methodology for conversation data sampling, but fails to explain why Judge [van] Keulen’s subsequent order directing production of the entire 5 million-record sample to the plaintiff in that case is not similarly instructive here,” Wang wrote.&lt;/p&gt;
&lt;p&gt;OpenAI’s filing today said the company was never given an opportunity to explain why &lt;em&gt;Concord&lt;/em&gt; shouldn’t apply in this case because the news plaintiffs did not reference it in their motion.&lt;/p&gt;
&lt;p&gt;“The cited &lt;em&gt;Concord&lt;/em&gt; order was not about whether wholesale production of the sample was appropriate; it was about the mechanism through which Anthropic would effectuate an &lt;em&gt;already agreed-upon&lt;/em&gt; production,” OpenAI wrote. “Nothing about that order suggests that Judge van Keulen would have ordered wholesale production had Anthropic raised the privacy concerns that OpenAI has raised throughout this case.”&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Concord&lt;/em&gt; logs were just prompt-output pairs, “&lt;em&gt;i.e.&lt;/em&gt;, a single user prompt followed by a single model output,” OpenAI wrote. “The logs at issue here are &lt;em&gt;complete conversations&lt;/em&gt;: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT.” That could result in “up to 80 million prompt-output pairs,” OpenAI said.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/11/openai-fights-order-to-hand-over-20-million-private-chatgpt-conversations/</guid><pubDate>Wed, 12 Nov 2025 18:27:27 +0000</pubDate></item><item><title>[NEW] Court rules that OpenAI violated German copyright law; ordered it to pay damages (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/court-rules-that-openai-violated-german-copyright-law-ordered-it-to-pay-damages/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2213399157.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A German court ruled that&amp;nbsp;OpenAI’s&amp;nbsp;ChatGPT violated&amp;nbsp;the nation’s&amp;nbsp;copyright laws by training its language models on licensed musical work&amp;nbsp;without&amp;nbsp;permission, multiple news outlets, including&amp;nbsp;The Guardian, reported.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decision&amp;nbsp;came from a lawsuit that GEMA, the society that handles music rights in Germany, filed last November against OpenAI.&amp;nbsp;The company&amp;nbsp;was ordered to pay an undisclosed amount of damages to GEMA, but&amp;nbsp;said it disagreed with the ruling and is “considering next steps.”&amp;nbsp;GEMA, meanwhile, regarded this as the “first landmark AI ruling in Europe.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Today, we have set a precedent that protects and clarifies the rights of authors: even operators of AI tools such as ChatGPT must comply with copyright law,” GEMA chief executive Tobias Holzmüller said, as The Guardian reported. “Today, we have successfully defended the livelihoods of music creators.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is being sued by other creatives and media groups over the same issue.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2213399157.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A German court ruled that&amp;nbsp;OpenAI’s&amp;nbsp;ChatGPT violated&amp;nbsp;the nation’s&amp;nbsp;copyright laws by training its language models on licensed musical work&amp;nbsp;without&amp;nbsp;permission, multiple news outlets, including&amp;nbsp;The Guardian, reported.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decision&amp;nbsp;came from a lawsuit that GEMA, the society that handles music rights in Germany, filed last November against OpenAI.&amp;nbsp;The company&amp;nbsp;was ordered to pay an undisclosed amount of damages to GEMA, but&amp;nbsp;said it disagreed with the ruling and is “considering next steps.”&amp;nbsp;GEMA, meanwhile, regarded this as the “first landmark AI ruling in Europe.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Today, we have set a precedent that protects and clarifies the rights of authors: even operators of AI tools such as ChatGPT must comply with copyright law,” GEMA chief executive Tobias Holzmüller said, as The Guardian reported. “Today, we have successfully defended the livelihoods of music creators.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is being sued by other creatives and media groups over the same issue.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/court-rules-that-openai-violated-german-copyright-law-ordered-it-to-pay-damages/</guid><pubDate>Wed, 12 Nov 2025 19:18:59 +0000</pubDate></item><item><title>[NEW] Weibo's new open source AI model VibeThinker-1.5B outperforms DeepSeek-R1 on $7,800 post-training budget (AI | VentureBeat)</title><link>https://venturebeat.com/ai/weibos-new-open-source-ai-model-vibethinker-1-5b-outperforms-deepseek-r1-on</link><description>[unable to retrieve full-text content]&lt;p&gt;Another day in late 2025, another impressive result from a Chinese company in open source artificial intelligence.&lt;/p&gt;&lt;p&gt;Chinese social networking company &lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;Weibo&amp;#x27;s AI division recently released its open source VibeThinker-1.5B&lt;/a&gt;—a 1.5 billion parameter large language model (LLM) that is a fine-tuned variant of rival Chinese tech firm &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Math-1.5B"&gt;Alibaba&amp;#x27;s Qwen2.5-Math-1.5B. &lt;/a&gt;&lt;/p&gt;&lt;p&gt;It&amp;#x27;s available now for free download and usage by researchers and enterprise developers—even for commercial purposes—under a permissive MIT License on &lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;Hugging Face&lt;/a&gt;, &lt;a href="https://github.com/WeiboAI/VibeThinker"&gt;GitHub&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/WeiboAI/VibeThinker-1.5B"&gt;ModelScope&lt;/a&gt;, with a &lt;a href="https://arxiv.org/pdf/2511.06221"&gt;technical report&lt;/a&gt; on open access science publishing site arxiv.org.&lt;/p&gt;&lt;p&gt;And yet, despite its compact size, VibeThinker-1.5B achieves benchmark-topping reasoning performance on math and code tasks, rivaling or surpassing models hundreds of times its size, even outperforming Chinese rival DeepSeek&amp;#x27;s famed R1 that went viral at the start of this year—a 671-billion parameter model—on formal reasoning benchmark.&lt;/p&gt;&lt;p&gt;It further eclipses Mistral AI&amp;#x27;s Magistral Medium and holds its own against Anthropic&amp;#x27;s Claude Opus 4 and OpenAI&amp;#x27;s gpt-oss-20B Medium, all while requiring a fraction of the infrastructure and investment.&lt;/p&gt;&lt;p&gt;It also does so having been post-trained on a budget of merely $7800 USD for compute resources (3900 GPU hours on Nvidia H800s) — far less than the tens, or even hundreds, of thousands of dollars typically required to fine-tune models of similar or larger scale.&lt;/p&gt;&lt;p&gt;Recall this is not the total cost of the model&amp;#x27;s development, however: LLMs are trained in stages. First comes pre-training, when the model learns basic language structure and general knowledge by predicting the next word across enormous amounts of text from the internet, books, and articles. This gives it fluency but not much sense of how to follow instructions or hold a conversation&lt;/p&gt;&lt;p&gt;Post-training comes next, using much smaller, higher-quality datasets—typically collections of example questions, prompts, and expert-written answers—to teach the model how to respond helpfully, reason through problems, and align with human expectations. Still, Weibo&amp;#x27;s post-training cost effectiveness on VibeThinker-1.5B is noteworthy and should be commended.&lt;/p&gt;&lt;p&gt;The open-source release upends assumptions about parameter scale, compute intensity, and the minimum viable size for high-performance LLMs.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Different Training Approach: Spectrum-to-Signal&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;VibeThinker-1.5B owes its performance not to scale, but to the training framework behind it: the Spectrum-to-Signal Principle (SSP).&lt;/p&gt;&lt;p&gt;Instead of optimizing a model purely for single-answer correctness (Pass@1), the SSP framework decouples supervised fine-tuning (SFT) and reinforcement learning (RL) into two distinct phases with different goals:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;SFT (“Spectrum Phase”)&lt;/b&gt;: The model is trained to maximize diversity across potential correct answers, improving its Pass@K score. This builds a wide range of plausible solution paths.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;RL (“Signal Phase”)&lt;/b&gt;: A second-stage reinforcement learning system (called MaxEnt-Guided Policy Optimization, or MGPO) is used to identify and amplify the most correct paths from this diverse solution pool. MGPO prioritizes problems where the model is most uncertain, using entropy-based weighting to focus learning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The authors argue this separation allows small models to explore reasoning space more effectively—achieving signal amplification without relying on massive parameter counts.&lt;/p&gt;&lt;p&gt;VibeThinker-1.5B makes a compelling case that the industry’s reliance on parameter scaling as the only route to better reasoning performance may be outdated. &lt;/p&gt;&lt;p&gt;By adopting a diversity-first training pipeline, WeiboAI has shown that smaller, more accessible models can match and even outperform billion-dollar systems in logic-heavy tasks.&lt;/p&gt;&lt;p&gt;The low resource footprint is among the most significant aspects of VibeThinker-1.5B. At under $8,000, the post-training cost is 30–60x lower than models like DeepSeek R1 and MiniMax-M1, which cost between $294K and $535K to train.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance Across Domains&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite its small size, VibeThinker-1.5B delivers cross-domain reasoning that outpaces many larger open-source and commercial models:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;AIME25&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;LiveCodeBench v6&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPQA-Diamond&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;i&gt;VibeThinker-1.5B&lt;/i&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;74.4&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;51.1&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;46.7&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-OSS-20B-Medium&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;72.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;54.9&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;66.0&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;69.2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;56.6&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;79.6&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MiniMax M1 (456B)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;74.6&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;62.3&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;69.2&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;DeepSeek R1 (671B)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;70.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;65.9&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;71.5&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Kimi K2 (1.09T)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;49.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;53.7&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;75.1&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;VibeThinker was benchmarked against both reasoning-centric models (Magistral, Claude, OpenAI o3-mini) and non-reasoning LLMs (GPT-4.1, Kimi K2, DeepSeek V3). Across structured reasoning benchmarks, the model consistently outperformed non-reasoning models, regardless of size:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;On AIME24 (math), it beat Kimi K2 (1.09T) by over 10 points (80.3 vs. 69.6).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;On LiveCodeBench v6, it surpassed Claude Opus 4 (51.1 vs. 47.4).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;On GPQA, it scored below GPT-4.1 and Claude, but still doubled its base model (from 16.4 to 46.7).&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This supports the authors’ claim that size is not the only path to reasoning capability—with proper training design, smaller models can reach or even exceed the performance of far larger systems in targeted tasks.&lt;/p&gt;&lt;p&gt;Notably, it achieves parity with models hundreds of times larger on math and code, though it lags behind in general knowledge reasoning (GPQA), where larger models maintain an edge.&lt;/p&gt;&lt;p&gt;This suggests a potential specialization trade-off: while VibeThinker excels at structured logical tasks, it has less capacity for wide-ranging encyclopedic recall, a known limitation of smaller architectures.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Guidance for Enterprise Adoption&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release includes recommended inference settings (temperature = 0.6, top_p = 0.95, max tokens = 40960).&lt;/p&gt;&lt;p&gt;The model is small enough to be deployed on edge devices, including mobile phones and vehicle-embedded systems, while inference costs are estimated to be 20–70x cheaper than with large models.&lt;/p&gt;&lt;p&gt;This positions VibeThinker-1.5B not just as a research achievement, but as a potential foundation for cost-efficient, locally deployable reasoning systems.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Weibo’s Strategy and Market Position&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Weibo, launched by Sina Corporation in 2009, remains a cornerstone of China’s social media ecosystem. Often described as China’s version of X (formerly Twitter), the platform blends microblogging, multimedia content, and trending-topic features with a regulatory environment shaped by tight government oversight. &lt;/p&gt;&lt;p&gt;Despite counting 600 million monthly active users (more than twice that of X), &lt;a href="https://m.aastocks.com/en/stocks/analysis/stock-aafn-con/9898/HK6/NOW.1483101/hk-stock-news"&gt;investors are not optimistic about its advertising revenue growth potential&lt;/a&gt; in the near term, and Weibo is navigating intensifying competition from video-first platforms like Douyin, which are drawing younger users and increasing time-spent elsewhere. &lt;/p&gt;&lt;p&gt;In response, Weibo has leaned into creator-economy monetization, live-streaming, and vertical video—adding tools for influencer engagement, e-commerce integration, and richer analytics for brands.&lt;/p&gt;&lt;p&gt;The platform’s role as a digital public square also makes it a focus of regulatory scrutiny. Chinese authorities continue to apply pressure on issues ranging from content governance to data security. In September 2025, &lt;a href="https://www.reuters.com/technology/chinas-internet-regulator-issues-warnings-kuaishou-weibo-over-content-violations-2025-09-20/"&gt;Weibo was among the platforms cited in official warnings&lt;/a&gt;, highlighting its ongoing exposure to policy risks.&lt;/p&gt;&lt;p&gt;Weibo’s push into AI R&amp;amp;D—exemplified by the release of VibeThinker-1.5B—signals a shift in ambition. Beyond being a media platform, Weibo is positioning itself as a player in the next phase of Chinese AI development, using its capital reserves, user behavior data, and in-house research capacity to pursue adjacent technical domains.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;What It Means for Enterprise Technical Decision Makers&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For engineering leaders and enterprise AI teams, VibeThinker’s release has practical implications for everything from orchestration pipelines to cost modeling. &lt;/p&gt;&lt;p&gt;A 1.5B-parameter model that outperforms 100x larger models on math and programming tasks doesn’t just save compute—it shifts the architectural balance. It enables LLM inference on constrained infrastructure, reduces latency at the edge, and lowers the barrier to entry for applications that otherwise would have required API access to closed, frontier-scale models.&lt;/p&gt;&lt;p&gt;That matters for enterprise ML leads trying to deploy reasoning-capable agents within existing systems, or for platform owners tasked with integrating LLMs into automated workflows. &lt;/p&gt;&lt;p&gt;It also speaks to those running reinforcement learning from human feedback (RLHF) pipelines or managing inference optimization across hybrid cloud environments. &lt;/p&gt;&lt;p&gt;The model’s post-training methodology—particularly its entropy-targeted reinforcement learning approach—offers a roadmap for teams looking to refine smaller checkpoints instead of relying on large-scale pretraining.&lt;/p&gt;&lt;p&gt;VibeThinker’s benchmark transparency and data decontamination steps also address another emerging priority in enterprise AI: auditability. While its performance on general-knowledge tests still trails large frontier models, its task-specific reliability makes it an attractive candidate for controlled environments where correctness matters more than coverage.&lt;/p&gt;&lt;p&gt;In short, VibeThinker-1.5B isn’t just a research milestone—it’s a strong candidate for practical enterprise use, deployment and learnings. It suggests that a new class of compact, reasoning-optimized models is viable for enterprise use cases that were previously the domain of far larger systems. For organizations trying to balance cost, latency, interpretability, and control, it’s a good new option to the long, growing list of Chinese open source offerings.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Another day in late 2025, another impressive result from a Chinese company in open source artificial intelligence.&lt;/p&gt;&lt;p&gt;Chinese social networking company &lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;Weibo&amp;#x27;s AI division recently released its open source VibeThinker-1.5B&lt;/a&gt;—a 1.5 billion parameter large language model (LLM) that is a fine-tuned variant of rival Chinese tech firm &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Math-1.5B"&gt;Alibaba&amp;#x27;s Qwen2.5-Math-1.5B. &lt;/a&gt;&lt;/p&gt;&lt;p&gt;It&amp;#x27;s available now for free download and usage by researchers and enterprise developers—even for commercial purposes—under a permissive MIT License on &lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;Hugging Face&lt;/a&gt;, &lt;a href="https://github.com/WeiboAI/VibeThinker"&gt;GitHub&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/WeiboAI/VibeThinker-1.5B"&gt;ModelScope&lt;/a&gt;, with a &lt;a href="https://arxiv.org/pdf/2511.06221"&gt;technical report&lt;/a&gt; on open access science publishing site arxiv.org.&lt;/p&gt;&lt;p&gt;And yet, despite its compact size, VibeThinker-1.5B achieves benchmark-topping reasoning performance on math and code tasks, rivaling or surpassing models hundreds of times its size, even outperforming Chinese rival DeepSeek&amp;#x27;s famed R1 that went viral at the start of this year—a 671-billion parameter model—on formal reasoning benchmark.&lt;/p&gt;&lt;p&gt;It further eclipses Mistral AI&amp;#x27;s Magistral Medium and holds its own against Anthropic&amp;#x27;s Claude Opus 4 and OpenAI&amp;#x27;s gpt-oss-20B Medium, all while requiring a fraction of the infrastructure and investment.&lt;/p&gt;&lt;p&gt;It also does so having been post-trained on a budget of merely $7800 USD for compute resources (3900 GPU hours on Nvidia H800s) — far less than the tens, or even hundreds, of thousands of dollars typically required to fine-tune models of similar or larger scale.&lt;/p&gt;&lt;p&gt;Recall this is not the total cost of the model&amp;#x27;s development, however: LLMs are trained in stages. First comes pre-training, when the model learns basic language structure and general knowledge by predicting the next word across enormous amounts of text from the internet, books, and articles. This gives it fluency but not much sense of how to follow instructions or hold a conversation&lt;/p&gt;&lt;p&gt;Post-training comes next, using much smaller, higher-quality datasets—typically collections of example questions, prompts, and expert-written answers—to teach the model how to respond helpfully, reason through problems, and align with human expectations. Still, Weibo&amp;#x27;s post-training cost effectiveness on VibeThinker-1.5B is noteworthy and should be commended.&lt;/p&gt;&lt;p&gt;The open-source release upends assumptions about parameter scale, compute intensity, and the minimum viable size for high-performance LLMs.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Different Training Approach: Spectrum-to-Signal&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;VibeThinker-1.5B owes its performance not to scale, but to the training framework behind it: the Spectrum-to-Signal Principle (SSP).&lt;/p&gt;&lt;p&gt;Instead of optimizing a model purely for single-answer correctness (Pass@1), the SSP framework decouples supervised fine-tuning (SFT) and reinforcement learning (RL) into two distinct phases with different goals:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;SFT (“Spectrum Phase”)&lt;/b&gt;: The model is trained to maximize diversity across potential correct answers, improving its Pass@K score. This builds a wide range of plausible solution paths.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;RL (“Signal Phase”)&lt;/b&gt;: A second-stage reinforcement learning system (called MaxEnt-Guided Policy Optimization, or MGPO) is used to identify and amplify the most correct paths from this diverse solution pool. MGPO prioritizes problems where the model is most uncertain, using entropy-based weighting to focus learning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The authors argue this separation allows small models to explore reasoning space more effectively—achieving signal amplification without relying on massive parameter counts.&lt;/p&gt;&lt;p&gt;VibeThinker-1.5B makes a compelling case that the industry’s reliance on parameter scaling as the only route to better reasoning performance may be outdated. &lt;/p&gt;&lt;p&gt;By adopting a diversity-first training pipeline, WeiboAI has shown that smaller, more accessible models can match and even outperform billion-dollar systems in logic-heavy tasks.&lt;/p&gt;&lt;p&gt;The low resource footprint is among the most significant aspects of VibeThinker-1.5B. At under $8,000, the post-training cost is 30–60x lower than models like DeepSeek R1 and MiniMax-M1, which cost between $294K and $535K to train.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance Across Domains&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite its small size, VibeThinker-1.5B delivers cross-domain reasoning that outpaces many larger open-source and commercial models:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;AIME25&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;LiveCodeBench v6&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPQA-Diamond&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;i&gt;VibeThinker-1.5B&lt;/i&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;74.4&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;51.1&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;46.7&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-OSS-20B-Medium&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;72.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;54.9&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;66.0&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;69.2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;56.6&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;79.6&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;MiniMax M1 (456B)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;74.6&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;62.3&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;69.2&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;DeepSeek R1 (671B)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;70.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;65.9&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;71.5&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Kimi K2 (1.09T)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;49.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;53.7&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;75.1&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;VibeThinker was benchmarked against both reasoning-centric models (Magistral, Claude, OpenAI o3-mini) and non-reasoning LLMs (GPT-4.1, Kimi K2, DeepSeek V3). Across structured reasoning benchmarks, the model consistently outperformed non-reasoning models, regardless of size:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;On AIME24 (math), it beat Kimi K2 (1.09T) by over 10 points (80.3 vs. 69.6).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;On LiveCodeBench v6, it surpassed Claude Opus 4 (51.1 vs. 47.4).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;On GPQA, it scored below GPT-4.1 and Claude, but still doubled its base model (from 16.4 to 46.7).&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This supports the authors’ claim that size is not the only path to reasoning capability—with proper training design, smaller models can reach or even exceed the performance of far larger systems in targeted tasks.&lt;/p&gt;&lt;p&gt;Notably, it achieves parity with models hundreds of times larger on math and code, though it lags behind in general knowledge reasoning (GPQA), where larger models maintain an edge.&lt;/p&gt;&lt;p&gt;This suggests a potential specialization trade-off: while VibeThinker excels at structured logical tasks, it has less capacity for wide-ranging encyclopedic recall, a known limitation of smaller architectures.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Guidance for Enterprise Adoption&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release includes recommended inference settings (temperature = 0.6, top_p = 0.95, max tokens = 40960).&lt;/p&gt;&lt;p&gt;The model is small enough to be deployed on edge devices, including mobile phones and vehicle-embedded systems, while inference costs are estimated to be 20–70x cheaper than with large models.&lt;/p&gt;&lt;p&gt;This positions VibeThinker-1.5B not just as a research achievement, but as a potential foundation for cost-efficient, locally deployable reasoning systems.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Weibo’s Strategy and Market Position&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Weibo, launched by Sina Corporation in 2009, remains a cornerstone of China’s social media ecosystem. Often described as China’s version of X (formerly Twitter), the platform blends microblogging, multimedia content, and trending-topic features with a regulatory environment shaped by tight government oversight. &lt;/p&gt;&lt;p&gt;Despite counting 600 million monthly active users (more than twice that of X), &lt;a href="https://m.aastocks.com/en/stocks/analysis/stock-aafn-con/9898/HK6/NOW.1483101/hk-stock-news"&gt;investors are not optimistic about its advertising revenue growth potential&lt;/a&gt; in the near term, and Weibo is navigating intensifying competition from video-first platforms like Douyin, which are drawing younger users and increasing time-spent elsewhere. &lt;/p&gt;&lt;p&gt;In response, Weibo has leaned into creator-economy monetization, live-streaming, and vertical video—adding tools for influencer engagement, e-commerce integration, and richer analytics for brands.&lt;/p&gt;&lt;p&gt;The platform’s role as a digital public square also makes it a focus of regulatory scrutiny. Chinese authorities continue to apply pressure on issues ranging from content governance to data security. In September 2025, &lt;a href="https://www.reuters.com/technology/chinas-internet-regulator-issues-warnings-kuaishou-weibo-over-content-violations-2025-09-20/"&gt;Weibo was among the platforms cited in official warnings&lt;/a&gt;, highlighting its ongoing exposure to policy risks.&lt;/p&gt;&lt;p&gt;Weibo’s push into AI R&amp;amp;D—exemplified by the release of VibeThinker-1.5B—signals a shift in ambition. Beyond being a media platform, Weibo is positioning itself as a player in the next phase of Chinese AI development, using its capital reserves, user behavior data, and in-house research capacity to pursue adjacent technical domains.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;What It Means for Enterprise Technical Decision Makers&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;For engineering leaders and enterprise AI teams, VibeThinker’s release has practical implications for everything from orchestration pipelines to cost modeling. &lt;/p&gt;&lt;p&gt;A 1.5B-parameter model that outperforms 100x larger models on math and programming tasks doesn’t just save compute—it shifts the architectural balance. It enables LLM inference on constrained infrastructure, reduces latency at the edge, and lowers the barrier to entry for applications that otherwise would have required API access to closed, frontier-scale models.&lt;/p&gt;&lt;p&gt;That matters for enterprise ML leads trying to deploy reasoning-capable agents within existing systems, or for platform owners tasked with integrating LLMs into automated workflows. &lt;/p&gt;&lt;p&gt;It also speaks to those running reinforcement learning from human feedback (RLHF) pipelines or managing inference optimization across hybrid cloud environments. &lt;/p&gt;&lt;p&gt;The model’s post-training methodology—particularly its entropy-targeted reinforcement learning approach—offers a roadmap for teams looking to refine smaller checkpoints instead of relying on large-scale pretraining.&lt;/p&gt;&lt;p&gt;VibeThinker’s benchmark transparency and data decontamination steps also address another emerging priority in enterprise AI: auditability. While its performance on general-knowledge tests still trails large frontier models, its task-specific reliability makes it an attractive candidate for controlled environments where correctness matters more than coverage.&lt;/p&gt;&lt;p&gt;In short, VibeThinker-1.5B isn’t just a research milestone—it’s a strong candidate for practical enterprise use, deployment and learnings. It suggests that a new class of compact, reasoning-optimized models is viable for enterprise use cases that were previously the domain of far larger systems. For organizations trying to balance cost, latency, interpretability, and control, it’s a good new option to the long, growing list of Chinese open source offerings.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/weibos-new-open-source-ai-model-vibethinker-1-5b-outperforms-deepseek-r1-on</guid><pubDate>Wed, 12 Nov 2025 19:31:00 +0000</pubDate></item><item><title>[NEW] ElevenLabs strike deals with celebs to create AI audio (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/elevenlabs-strike-deals-with-celebs-to-create-ai-audio/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/ElevenLabs-co-founders.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ElevenLabs struck a deal with actors Michael Caine and Matthew McConaughey to AI-generate their voices, the company announced this week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hollywood and AI have an up-and-down relationship, with its guardrails — or lack thereof — serving among the main concerns that led to the Hollywood strikes a few years ago. Since then, some artists have started to warm up to the idea of AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, Meta announced that its Meta AI would offer voice assistants that sound like actresses Kristen Bell and Judi Dench. With McConaughey, an investor in ElevenLabs, the company will translate his newsletter into Spanish audio with the use of his AI voice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs also announced this week that it was launching a marketplace to let brands use authorized AI-generated voices of celebrities, which will include Caine and other names like Liza Minnelli and Dr. Maya Angelou. ElevenLabs is one of the more popular AI unicorn companies and has backers including a16z and ICONIQ.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/ElevenLabs-co-founders.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ElevenLabs struck a deal with actors Michael Caine and Matthew McConaughey to AI-generate their voices, the company announced this week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hollywood and AI have an up-and-down relationship, with its guardrails — or lack thereof — serving among the main concerns that led to the Hollywood strikes a few years ago. Since then, some artists have started to warm up to the idea of AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, Meta announced that its Meta AI would offer voice assistants that sound like actresses Kristen Bell and Judi Dench. With McConaughey, an investor in ElevenLabs, the company will translate his newsletter into Spanish audio with the use of his AI voice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs also announced this week that it was launching a marketplace to let brands use authorized AI-generated voices of celebrities, which will include Caine and other names like Liza Minnelli and Dr. Maya Angelou. ElevenLabs is one of the more popular AI unicorn companies and has backers including a16z and ICONIQ.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/elevenlabs-strike-deals-with-celebs-to-create-ai-audio/</guid><pubDate>Wed, 12 Nov 2025 20:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI walks a tricky tightrope with GPT-5.1’s eight new personalities (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/openai-walks-a-tricky-tightrope-with-gpt-5-1s-eight-new-personalities/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New controls attempt to please critics on both sides with a balance between bland and habit-forming.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Group of People with differing personalities" class="absolute inset-0 w-full h-full object-cover hidden" height="409" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-640x409.jpg" width="640" /&gt;
                  &lt;img alt="Group of People with differing personalities" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Chris Madden via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Wednesday, OpenAI released GPT-5.1 Instant and GPT-5.1 Thinking, two updated versions of its flagship AI models now available in ChatGPT. The company is wrapping the models in the language of anthropomorphism, claiming that they’re warmer, more conversational, and better at following instructions.&lt;/p&gt;
&lt;p&gt;The release follows complaints earlier this year that its previous models were excessively cheerful and sycophantic, along with an opposing controversy among users over how OpenAI modified the default GPT-5 output style after several suicide lawsuits.&lt;/p&gt;
&lt;p&gt;The company now faces intense scrutiny from lawyers and regulators that could threaten its future operations. In that kind of environment, it’s difficult to just release a new AI model, throw out a few stats, and move on like the company could even a year ago. But here are the basics: The new GPT-5.1 Instant model will serve as ChatGPT’s faster default option for most tasks, while GPT-5.1 Thinking is a simulated reasoning model that attempts to handle more complex problem-solving tasks.&lt;/p&gt;
&lt;p&gt;OpenAI claims that both models perform better on technical benchmarks such as math and coding evaluations (including AIME 2025 and Codeforces) than GPT-5, which was released in August.&lt;/p&gt;
&lt;p&gt;Improved benchmarks may win over some users, but the biggest change with GPT-5.1 is in its presentation. OpenAI says it heard from users that they wanted AI models to simulate different communication styles depending on the task, so the company is offering eight preset options, including Professional, Friendly, Candid, Quirky, Efficient, Cynical, and Nerdy, alongside a Default setting.&lt;/p&gt;
&lt;p&gt;These presets alter the instructions fed into each prompt to simulate different personality styles, but the underlying model capabilities remain the same across all settings.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2127176 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="An illustration showing GPT-5.1's eight personality styles in ChatGPT." class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/5-1_X_Thread_Card_05_V2-1024x576.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An illustration showing GPT-5.1’s eight personality styles in ChatGPT.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In addition, the company trained GPT-5.1 Instant to use “adaptive reasoning,” meaning that the model decides when to spend more computational time processing a prompt before generating output.&lt;/p&gt;
&lt;p&gt;The company plans to roll out the models gradually over the next few days, starting with paid subscribers before expanding to free users. OpenAI plans to bring both GPT-5.1 Instant and GPT-5.1 Thinking to its API later this week. GPT-5.1 Instant will appear as gpt-5.1-chat-latest, and GPT-5.1 Thinking will be released as GPT-5.1 in the API, both with adaptive reasoning enabled. The older GPT-5 models will remain available in ChatGPT under the legacy models dropdown for paid subscribers for three months.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The company says it wants to give people time to compare model outputs and adapt at their own pace and that going forward, it will communicate deprecation periods clearly with advance notice. OpenAI also published a system card with information on its safety approach for GPT-5.1.&lt;/p&gt;
&lt;h2&gt;Seeking balance&lt;/h2&gt;
&lt;p&gt;In a blog post published Wednesday, OpenAI CEO of Applications Fidji Simo wrote that the company wants ChatGPT to “feel like yours and work with you in the way that suits you best.” Simo wrote that with more than 800 million people using ChatGPT, the company has moved past one-size-fits-all approaches. She wrote that people experience ChatGPT in individual ways, with some wanting direct and neutral responses while others prefer different output patterns.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The preset “personality” options work by injecting different instructions into the system prompt that the model processes before generating each response. OpenAI says the original Cynical and Nerdy options from earlier this year will remain available in the personalization settings dropdown.&lt;/p&gt;
&lt;p&gt;For users who want more control over outputs, OpenAI is experimenting with options to adjust specific characteristics from personalization settings, including how concise responses are and how frequently the model generates emojis. ChatGPT can also offer to update these settings during conversations when it detects users requesting certain output patterns. The company says updates to personalization settings now take effect across all chats immediately, including ongoing conversations.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2127177 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A benchmark chart from OpenAI. &amp;quot;GPT‑5.1 Thinking varies its thinking time more dynamically than GPT‑5 Thinking,&amp;quot; the company writes." class="center large" height="702" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GPT-5.1-spends-less-time-on-easy-tasks-and-more-time-on-hard-tasks-1024x702.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A benchmark chart from OpenAI. “GPT‑5.1 Thinking varies its thinking time more dynamically than GPT‑5 Thinking,” the company writes.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Simo addressed the balance between customization and accuracy in her blog post. “Personalization taken to an extreme wouldn’t be helpful if it only reinforces your worldview or tells you what you want to hear,” she wrote. She compared excessive customization to editing a spouse’s traits to always agree, noting that “the best people in our lives are the ones who listen and adapt, but also challenge us and help us grow.”&lt;/p&gt;
&lt;p&gt;That concern about excessive personalization is not theoretical. Amid a year full of accusations of AI chatbots inspiring suicides and people descending into obsessive fantasy-rabbit-hole scenarios, OpenAI recently released safety research that details its plan to deal with people who develop unhealthy attachments to its AI chatbots. The company says these situations are rare, but it is working with an expert council and mental health clinicians to understand what healthy interactions with AI models should look like.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Even so, the root problem is arguably that ChatGPT still pretends to be a person—a consistent entity that knows you and learns your preferences over time. It assumes the mantle of human emotion and acts like it understands you and sympathizes with what you’re going through, which could potentially lead users into the same kind of thorny situations we’ve seen repeatedly in the past.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2127175 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A screenshot example of new &amp;quot;warmer&amp;quot; GPT-5.1 outputs presented on the OpenAI website." class="center large" height="668" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/screenshot_from_openai-1024x668.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot example of new “warmer” GPT-5.1 outputs presented on the OpenAI website.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;It’s a tricky position for OpenAI to be in. When the company changes ChatGPT’s output style to be too reserved and robotic, it gets complaints from one set of users. When the models are too warm, the company receives criticism from experts who worry about how the models might affect vulnerable users. The new personality choices are OpenAI’s attempt to balance the needs of a broad spectrum of users who approach its chatbot with vastly different use cases, from programming assistance to being a virtual best friend.&lt;/p&gt;
&lt;p&gt;Meanwhile, the company faces a fundamental business tension between making AI models engaging enough for widespread adoption while attempting to avoid inspiring user behavior that could become harmful. Simo addressed some of these concerns in her blog post. “We also have to be vigilant about the potential for some people to develop attachment to our models at the expense of their real world relationships, well being, or obligations,” she wrote. “There will be many new challenges as this technology evolves and people use it in new ways. Building at this scale means never assuming we have all the answers.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New controls attempt to please critics on both sides with a balance between bland and habit-forming.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Group of People with differing personalities" class="absolute inset-0 w-full h-full object-cover hidden" height="409" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-640x409.jpg" width="640" /&gt;
                  &lt;img alt="Group of People with differing personalities" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Chris Madden via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Wednesday, OpenAI released GPT-5.1 Instant and GPT-5.1 Thinking, two updated versions of its flagship AI models now available in ChatGPT. The company is wrapping the models in the language of anthropomorphism, claiming that they’re warmer, more conversational, and better at following instructions.&lt;/p&gt;
&lt;p&gt;The release follows complaints earlier this year that its previous models were excessively cheerful and sycophantic, along with an opposing controversy among users over how OpenAI modified the default GPT-5 output style after several suicide lawsuits.&lt;/p&gt;
&lt;p&gt;The company now faces intense scrutiny from lawyers and regulators that could threaten its future operations. In that kind of environment, it’s difficult to just release a new AI model, throw out a few stats, and move on like the company could even a year ago. But here are the basics: The new GPT-5.1 Instant model will serve as ChatGPT’s faster default option for most tasks, while GPT-5.1 Thinking is a simulated reasoning model that attempts to handle more complex problem-solving tasks.&lt;/p&gt;
&lt;p&gt;OpenAI claims that both models perform better on technical benchmarks such as math and coding evaluations (including AIME 2025 and Codeforces) than GPT-5, which was released in August.&lt;/p&gt;
&lt;p&gt;Improved benchmarks may win over some users, but the biggest change with GPT-5.1 is in its presentation. OpenAI says it heard from users that they wanted AI models to simulate different communication styles depending on the task, so the company is offering eight preset options, including Professional, Friendly, Candid, Quirky, Efficient, Cynical, and Nerdy, alongside a Default setting.&lt;/p&gt;
&lt;p&gt;These presets alter the instructions fed into each prompt to simulate different personality styles, but the underlying model capabilities remain the same across all settings.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2127176 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="An illustration showing GPT-5.1's eight personality styles in ChatGPT." class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/5-1_X_Thread_Card_05_V2-1024x576.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An illustration showing GPT-5.1’s eight personality styles in ChatGPT.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In addition, the company trained GPT-5.1 Instant to use “adaptive reasoning,” meaning that the model decides when to spend more computational time processing a prompt before generating output.&lt;/p&gt;
&lt;p&gt;The company plans to roll out the models gradually over the next few days, starting with paid subscribers before expanding to free users. OpenAI plans to bring both GPT-5.1 Instant and GPT-5.1 Thinking to its API later this week. GPT-5.1 Instant will appear as gpt-5.1-chat-latest, and GPT-5.1 Thinking will be released as GPT-5.1 in the API, both with adaptive reasoning enabled. The older GPT-5 models will remain available in ChatGPT under the legacy models dropdown for paid subscribers for three months.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The company says it wants to give people time to compare model outputs and adapt at their own pace and that going forward, it will communicate deprecation periods clearly with advance notice. OpenAI also published a system card with information on its safety approach for GPT-5.1.&lt;/p&gt;
&lt;h2&gt;Seeking balance&lt;/h2&gt;
&lt;p&gt;In a blog post published Wednesday, OpenAI CEO of Applications Fidji Simo wrote that the company wants ChatGPT to “feel like yours and work with you in the way that suits you best.” Simo wrote that with more than 800 million people using ChatGPT, the company has moved past one-size-fits-all approaches. She wrote that people experience ChatGPT in individual ways, with some wanting direct and neutral responses while others prefer different output patterns.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The preset “personality” options work by injecting different instructions into the system prompt that the model processes before generating each response. OpenAI says the original Cynical and Nerdy options from earlier this year will remain available in the personalization settings dropdown.&lt;/p&gt;
&lt;p&gt;For users who want more control over outputs, OpenAI is experimenting with options to adjust specific characteristics from personalization settings, including how concise responses are and how frequently the model generates emojis. ChatGPT can also offer to update these settings during conversations when it detects users requesting certain output patterns. The company says updates to personalization settings now take effect across all chats immediately, including ongoing conversations.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2127177 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A benchmark chart from OpenAI. &amp;quot;GPT‑5.1 Thinking varies its thinking time more dynamically than GPT‑5 Thinking,&amp;quot; the company writes." class="center large" height="702" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GPT-5.1-spends-less-time-on-easy-tasks-and-more-time-on-hard-tasks-1024x702.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A benchmark chart from OpenAI. “GPT‑5.1 Thinking varies its thinking time more dynamically than GPT‑5 Thinking,” the company writes.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Simo addressed the balance between customization and accuracy in her blog post. “Personalization taken to an extreme wouldn’t be helpful if it only reinforces your worldview or tells you what you want to hear,” she wrote. She compared excessive customization to editing a spouse’s traits to always agree, noting that “the best people in our lives are the ones who listen and adapt, but also challenge us and help us grow.”&lt;/p&gt;
&lt;p&gt;That concern about excessive personalization is not theoretical. Amid a year full of accusations of AI chatbots inspiring suicides and people descending into obsessive fantasy-rabbit-hole scenarios, OpenAI recently released safety research that details its plan to deal with people who develop unhealthy attachments to its AI chatbots. The company says these situations are rare, but it is working with an expert council and mental health clinicians to understand what healthy interactions with AI models should look like.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Even so, the root problem is arguably that ChatGPT still pretends to be a person—a consistent entity that knows you and learns your preferences over time. It assumes the mantle of human emotion and acts like it understands you and sympathizes with what you’re going through, which could potentially lead users into the same kind of thorny situations we’ve seen repeatedly in the past.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2127175 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A screenshot example of new &amp;quot;warmer&amp;quot; GPT-5.1 outputs presented on the OpenAI website." class="center large" height="668" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/screenshot_from_openai-1024x668.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot example of new “warmer” GPT-5.1 outputs presented on the OpenAI website.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;It’s a tricky position for OpenAI to be in. When the company changes ChatGPT’s output style to be too reserved and robotic, it gets complaints from one set of users. When the models are too warm, the company receives criticism from experts who worry about how the models might affect vulnerable users. The new personality choices are OpenAI’s attempt to balance the needs of a broad spectrum of users who approach its chatbot with vastly different use cases, from programming assistance to being a virtual best friend.&lt;/p&gt;
&lt;p&gt;Meanwhile, the company faces a fundamental business tension between making AI models engaging enough for widespread adoption while attempting to avoid inspiring user behavior that could become harmful. Simo addressed some of these concerns in her blog post. “We also have to be vigilant about the potential for some people to develop attachment to our models at the expense of their real world relationships, well being, or obligations,” she wrote. “There will be many new challenges as this technology evolves and people use it in new ways. Building at this scale means never assuming we have all the answers.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/openai-walks-a-tricky-tightrope-with-gpt-5-1s-eight-new-personalities/</guid><pubDate>Wed, 12 Nov 2025 22:54:47 +0000</pubDate></item><item><title>[NEW] ‘Chad: The Brainrot IDE’ is a new Y Combinator-backed product so wild, people thought it was fake (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/chad-the-brainrot-ide-is-a-new-y-combinator-backed-product-so-wild-people-thought-it-was-fake/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-924636730.jpg?resize=1200,1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When former Twitter CEO Dick Costolo spoke at TechCrunch Disrupt, someone from the audience asked him if HBO’s hit satire “Silicon Valley”&lt;em&gt; &lt;/em&gt;would be revived. Costolo, who was a writer for the show, essentially answered no (at timestamp 38:17).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the writers talk about that regularly, he said, they don’t pursue it because today’s actual Silicon Valley is so bizarre, it can’t be parodied.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The latest case in point is a new company called Clad Labs that launched out of Y Combinator this week. Clad’s product is so outside-the-box that people thought it was an April Fools’ joke in November.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s a real product, founder Richard Wang told TechCrunch. The product is called “Chad: The Brainrot IDE.” It is yet another vibe coding integrated development environment — an IDE is the software developers use to code — but with a twist. While waiting for the AI coding tool to finish its task, the developer can mess around with their favorite brainrot activities within a window of the IDE.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Or, as the company’s website advertises: “Gamble while you code. Watch TikToks. Swipe on Tinder. Play minigames. This isn’t a joke — it’s Chad IDE, and it’s solving the biggest productivity problem in AI-powered development that nobody’s talking about.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The founders say their IDE increases productivity by helping with “context switching.” Their argument is, by doing your brainrot activities within the IDE itself, as soon as the AI is done with the task, you’ll get right back to work rather than be focused on your phone or browser.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reaction on X was mixed. While some people thought it was a fake satire, others thought it was a good — or a terrible — idea.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Like it or hate it, everyone had an opinion, even Jordi Hays, co-host of the enthusiastically pro-tech podcast TBPN. Hays penned a post on the product called, “Rage Baiting is for Losers.” In it he said of Chad IDE: “On one hand it’s funny. On the other hand, what are we doing here and why does this belong on the official YC account?”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He argued that products like Chad IDE and Cluely have moved rage bait from a marketing gimmick to a “product strategy” and “it really should not be.” He urged YC to start teaching founders that “rage baiting is for losers.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is particularly interesting advice from someone who, as a founder, had mastered viral marketing without rage. Hays and his wife Sarah founded Party Round, a funding startup that went viral for their friendly marketing gimmicks like launching NFT versions of top “helpful” VCs. (Party Round rebranded to Capital and sold to Rho in 2024.)&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wang tells TechCrunch what the haters don’t get about his brainrot IDE is that it wasn’t intended to be rage bait. The founders hope it becomes a genuinely beloved AI vibe coder for consumer-app type developers. They want to give these folks a consumer app-like experience in an IDE.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the product is real, it’s not available to the public yet. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;”We’re currently in a closed beta,” Wang said. Right now, Chad is attempting to build a “community” of users who like the idea. Clad Labs hopes to open the product to the public soon, but for now, users must get an invite from someone already in the beta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;No doubt there’s a certain type of developer who would love Chad. But whatever the future holds for this product, one thing is true: It is nearly impossible to parody Silicon Valley these days.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-924636730.jpg?resize=1200,1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When former Twitter CEO Dick Costolo spoke at TechCrunch Disrupt, someone from the audience asked him if HBO’s hit satire “Silicon Valley”&lt;em&gt; &lt;/em&gt;would be revived. Costolo, who was a writer for the show, essentially answered no (at timestamp 38:17).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the writers talk about that regularly, he said, they don’t pursue it because today’s actual Silicon Valley is so bizarre, it can’t be parodied.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The latest case in point is a new company called Clad Labs that launched out of Y Combinator this week. Clad’s product is so outside-the-box that people thought it was an April Fools’ joke in November.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s a real product, founder Richard Wang told TechCrunch. The product is called “Chad: The Brainrot IDE.” It is yet another vibe coding integrated development environment — an IDE is the software developers use to code — but with a twist. While waiting for the AI coding tool to finish its task, the developer can mess around with their favorite brainrot activities within a window of the IDE.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Or, as the company’s website advertises: “Gamble while you code. Watch TikToks. Swipe on Tinder. Play minigames. This isn’t a joke — it’s Chad IDE, and it’s solving the biggest productivity problem in AI-powered development that nobody’s talking about.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The founders say their IDE increases productivity by helping with “context switching.” Their argument is, by doing your brainrot activities within the IDE itself, as soon as the AI is done with the task, you’ll get right back to work rather than be focused on your phone or browser.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reaction on X was mixed. While some people thought it was a fake satire, others thought it was a good — or a terrible — idea.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Like it or hate it, everyone had an opinion, even Jordi Hays, co-host of the enthusiastically pro-tech podcast TBPN. Hays penned a post on the product called, “Rage Baiting is for Losers.” In it he said of Chad IDE: “On one hand it’s funny. On the other hand, what are we doing here and why does this belong on the official YC account?”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He argued that products like Chad IDE and Cluely have moved rage bait from a marketing gimmick to a “product strategy” and “it really should not be.” He urged YC to start teaching founders that “rage baiting is for losers.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is particularly interesting advice from someone who, as a founder, had mastered viral marketing without rage. Hays and his wife Sarah founded Party Round, a funding startup that went viral for their friendly marketing gimmicks like launching NFT versions of top “helpful” VCs. (Party Round rebranded to Capital and sold to Rho in 2024.)&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wang tells TechCrunch what the haters don’t get about his brainrot IDE is that it wasn’t intended to be rage bait. The founders hope it becomes a genuinely beloved AI vibe coder for consumer-app type developers. They want to give these folks a consumer app-like experience in an IDE.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the product is real, it’s not available to the public yet. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;”We’re currently in a closed beta,” Wang said. Right now, Chad is attempting to build a “community” of users who like the idea. Clad Labs hopes to open the product to the public soon, but for now, users must get an invite from someone already in the beta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;No doubt there’s a certain type of developer who would love Chad. But whatever the future holds for this product, one thing is true: It is nearly impossible to parody Silicon Valley these days.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/chad-the-brainrot-ide-is-a-new-y-combinator-backed-product-so-wild-people-thought-it-was-fake/</guid><pubDate>Thu, 13 Nov 2025 00:05:30 +0000</pubDate></item></channel></rss>