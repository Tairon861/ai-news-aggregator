<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 08 Nov 2025 06:30:03 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>MIT Energy Initiative launches Data Center Power Forum (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-energy-initiative-launches-data-center-power-forum-1107</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/pexels-brett-sayles-4508751.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;With global power demand&amp;nbsp;from data centers expected to more than double by 2030, the MIT Energy Initiative (MITEI) in September launched an effort that brings together MIT researchers and industry experts to explore innovative solutions for powering the data-driven future. At its annual research conference, MITEI announced the Data Center Power Forum,&amp;nbsp;a targeted research effort&amp;nbsp;for MITEI member companies interested in addressing the challenges of data center power demand. The Data Center Power Forum builds on lessons from MITEI’s May 2025 symposium on the energy to power the expansion of artificial intelligence (AI) and focus panels related to data centers at the fall 2024 research conference.&lt;/p&gt;&lt;p&gt;In the United States, data centers consumed 4 percent of the country’s electricity in 2023, with demand expected to increase to 9 percent by 2030, according to the Electric Power Research Institute. Much of the growth in demand is from the increasing use of AI, which is placing an unprecedented strain on the electric grid. This surge in demand presents a serious challenge for the technology and energy sectors, government policymakers, and everyday consumers, who may see their electric bills skyrocket as a result.&lt;/p&gt;&lt;p&gt;“MITEI has long supported research on ways to produce more efficient and cleaner energy and to manage the electric grid. In recent years, MITEI has also funded dozens of research projects relevant to data center energy issues. Building on this history and knowledge base, MITEI’s Data Center Power Forum is convening a specialized community of industry members who have a vital stake in the sustainable growth of AI and the acceleration of solutions for powering data centers and expanding the grid,” says William H. Green, the director of MITEI and the Hoyt C. Hottel Professor of Chemical Engineering.&lt;/p&gt;&lt;p&gt;MITEI’s mission is to advance zero- and low-carbon solutions to expand energy access and mitigate climate change. MITEI works with companies from across the energy innovation chain, including in the infrastructure, automotive, electric power, energy, natural resources, and insurance sectors. MITEI member companies have expressed strong interest in the Data Center Power Forum and are committing to support focused research on a wide range of energy issues associated with data center expansion, Green says.&lt;/p&gt;&lt;p&gt;MITEI’s Data Center Power Forum will provide its member companies with reliable insights into energy supply, grid load operations and management, the built environment, and electricity market design and regulatory policy for data centers. The forum complements MIT’s deep expertise in adjacent topics such as low-power processors, efficient algorithms, task-specific AI, photonic devices, quantum computing, and the societal consequences of data center expansion. As part of the forum, MITEI’s Future Energy Systems Center is funding projects relevant to data center energy in its upcoming proposal cycles. MITEI Research Scientist Deep Deka has been named the program manager for the forum.&lt;/p&gt;&lt;p&gt;“Figuring out how to meet the power demands of data centers is a complicated challenge. Our research is coming at this from multiple directions, from looking at ways to expand transmission capacity within the electrical grid in order to bring power to where it is needed, to ensuring the quality of electrical service for existing users is not diminished when new data centers come online, and to shifting computing tasks to times and places when and where energy is available on the grid," said Deka.&lt;/p&gt;&lt;p&gt;MITEI currently sponsors substantial research related to data center energy topics across several MIT departments. The existing research portfolio includes more than a dozen projects related to data centers, including low- or zero-carbon solutions for energy supply and infrastructure, electrical grid management, and electricity market policy. MIT researchers funded through MITEI’s industry consortium are also designing more energy-efficient power electronics and processors and investigating behind-the-meter low-/no-carbon power plants and energy storage. MITEI-supported experts are studying how to use AI to optimize electrical distribution and the siting of data centers and conducting techno-economic analyses of data center power schemes. MITEI’s consortium projects are also bringing fresh perspectives to data center cooling challenges and considering policy approaches to balance the interests of shareholders.&amp;nbsp;&lt;/p&gt;&lt;p&gt;By drawing together industry stakeholders from across the AI and grid value chain, the Data Center Power Forum enables a richer dialog about solutions to power, grid, and carbon management problems in a noncommercial and collaborative setting.&lt;/p&gt;&lt;p&gt;“The opportunity to meet and to hold discussions on key data center challenges with other forum members from different sectors, as well as with MIT faculty members and research scientists, is a unique benefit of this MITEI-led effort,” Green says.&lt;/p&gt;&lt;p&gt;MITEI addressed the issue of data center power needs with its company members during its fall 2024&amp;nbsp;Annual Research Conference with a panel session titled, “The extreme challenge of powering data centers in a decarbonized way.” MITEI Director of Research Randall Field led a discussion with representatives from large technology companies Google and Microsoft, known as “hyperscalers,” as well as Madrid-based infrastructure developer Ferrovial S.E. and utility company Exelon Corp. Another conference session addressed the related topic, “Energy storage and grid expansion.” This past spring,&amp;nbsp;MITEI focused its annual Spring Symposium on data centers, hosting faculty members and researchers from MIT and other universities, business leaders, and a representative of the Federal Energy Regulatory Commission for a full day of sessions on the topic, “AI and energy: Peril and promise.”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/pexels-brett-sayles-4508751.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;With global power demand&amp;nbsp;from data centers expected to more than double by 2030, the MIT Energy Initiative (MITEI) in September launched an effort that brings together MIT researchers and industry experts to explore innovative solutions for powering the data-driven future. At its annual research conference, MITEI announced the Data Center Power Forum,&amp;nbsp;a targeted research effort&amp;nbsp;for MITEI member companies interested in addressing the challenges of data center power demand. The Data Center Power Forum builds on lessons from MITEI’s May 2025 symposium on the energy to power the expansion of artificial intelligence (AI) and focus panels related to data centers at the fall 2024 research conference.&lt;/p&gt;&lt;p&gt;In the United States, data centers consumed 4 percent of the country’s electricity in 2023, with demand expected to increase to 9 percent by 2030, according to the Electric Power Research Institute. Much of the growth in demand is from the increasing use of AI, which is placing an unprecedented strain on the electric grid. This surge in demand presents a serious challenge for the technology and energy sectors, government policymakers, and everyday consumers, who may see their electric bills skyrocket as a result.&lt;/p&gt;&lt;p&gt;“MITEI has long supported research on ways to produce more efficient and cleaner energy and to manage the electric grid. In recent years, MITEI has also funded dozens of research projects relevant to data center energy issues. Building on this history and knowledge base, MITEI’s Data Center Power Forum is convening a specialized community of industry members who have a vital stake in the sustainable growth of AI and the acceleration of solutions for powering data centers and expanding the grid,” says William H. Green, the director of MITEI and the Hoyt C. Hottel Professor of Chemical Engineering.&lt;/p&gt;&lt;p&gt;MITEI’s mission is to advance zero- and low-carbon solutions to expand energy access and mitigate climate change. MITEI works with companies from across the energy innovation chain, including in the infrastructure, automotive, electric power, energy, natural resources, and insurance sectors. MITEI member companies have expressed strong interest in the Data Center Power Forum and are committing to support focused research on a wide range of energy issues associated with data center expansion, Green says.&lt;/p&gt;&lt;p&gt;MITEI’s Data Center Power Forum will provide its member companies with reliable insights into energy supply, grid load operations and management, the built environment, and electricity market design and regulatory policy for data centers. The forum complements MIT’s deep expertise in adjacent topics such as low-power processors, efficient algorithms, task-specific AI, photonic devices, quantum computing, and the societal consequences of data center expansion. As part of the forum, MITEI’s Future Energy Systems Center is funding projects relevant to data center energy in its upcoming proposal cycles. MITEI Research Scientist Deep Deka has been named the program manager for the forum.&lt;/p&gt;&lt;p&gt;“Figuring out how to meet the power demands of data centers is a complicated challenge. Our research is coming at this from multiple directions, from looking at ways to expand transmission capacity within the electrical grid in order to bring power to where it is needed, to ensuring the quality of electrical service for existing users is not diminished when new data centers come online, and to shifting computing tasks to times and places when and where energy is available on the grid," said Deka.&lt;/p&gt;&lt;p&gt;MITEI currently sponsors substantial research related to data center energy topics across several MIT departments. The existing research portfolio includes more than a dozen projects related to data centers, including low- or zero-carbon solutions for energy supply and infrastructure, electrical grid management, and electricity market policy. MIT researchers funded through MITEI’s industry consortium are also designing more energy-efficient power electronics and processors and investigating behind-the-meter low-/no-carbon power plants and energy storage. MITEI-supported experts are studying how to use AI to optimize electrical distribution and the siting of data centers and conducting techno-economic analyses of data center power schemes. MITEI’s consortium projects are also bringing fresh perspectives to data center cooling challenges and considering policy approaches to balance the interests of shareholders.&amp;nbsp;&lt;/p&gt;&lt;p&gt;By drawing together industry stakeholders from across the AI and grid value chain, the Data Center Power Forum enables a richer dialog about solutions to power, grid, and carbon management problems in a noncommercial and collaborative setting.&lt;/p&gt;&lt;p&gt;“The opportunity to meet and to hold discussions on key data center challenges with other forum members from different sectors, as well as with MIT faculty members and research scientists, is a unique benefit of this MITEI-led effort,” Green says.&lt;/p&gt;&lt;p&gt;MITEI addressed the issue of data center power needs with its company members during its fall 2024&amp;nbsp;Annual Research Conference with a panel session titled, “The extreme challenge of powering data centers in a decarbonized way.” MITEI Director of Research Randall Field led a discussion with representatives from large technology companies Google and Microsoft, known as “hyperscalers,” as well as Madrid-based infrastructure developer Ferrovial S.E. and utility company Exelon Corp. Another conference session addressed the related topic, “Energy storage and grid expansion.” This past spring,&amp;nbsp;MITEI focused its annual Spring Symposium on data centers, hosting faculty members and researchers from MIT and other universities, business leaders, and a representative of the Federal Energy Regulatory Commission for a full day of sessions on the topic, “AI and energy: Peril and promise.”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-energy-initiative-launches-data-center-power-forum-1107</guid><pubDate>Fri, 07 Nov 2025 19:55:00 +0000</pubDate></item><item><title>Researchers surprised that with AI, toxicity is harder to fake than intelligence (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/11/being-too-nice-online-is-a-dead-giveaway-for-ai-bots-study-suggests/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New “computational Turing test” reportedly catches AI pretending to be human with 80% accuracy.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Young Boy in Front of Computer wearing happy paper bag over head." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/happy_bagboy-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Young Boy in Front of Computer wearing happy paper bag over head." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/happy_bagboy-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          RichVintage via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The next time you encounter an unusually polite reply on social media, you might want to check twice. It could be an AI model trying (and failing) to blend in with the crowd.&lt;/p&gt;
&lt;p&gt;On Wednesday, researchers from the University of Zurich, University of Amsterdam, Duke University, and New York University released a study revealing that AI models remain easily distinguishable from humans in social media conversations, with overly friendly emotional tone serving as the most persistent giveaway. The research, which tested nine open-weight models across Twitter/X, Bluesky, and Reddit, found that classifiers developed by the researchers detected AI-generated replies with 70 to 80 percent accuracy.&lt;/p&gt;
&lt;p&gt;The study introduces what the authors call a “computational Turing test” to assess how closely AI models approximate human language. Instead of relying on subjective human judgment about whether text sounds authentic, the framework uses automated classifiers and linguistic analysis to identify specific features that distinguish machine-generated from human-authored content.&lt;/p&gt;
&lt;p&gt;“Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression,” the researchers wrote. The team, led by Nicolò Pagan at the University of Zurich, tested various optimization strategies, from simple prompting to fine-tuning, but found that deeper emotional cues persist as reliable tells that a particular text interaction online was authored by an AI chatbot rather than a human.&lt;/p&gt;
&lt;h2&gt;The toxicity tell&lt;/h2&gt;
&lt;p&gt;In the study, researchers tested nine large language models: Llama 3.1 8B, Llama 3.1 8B Instruct, Llama 3.1 70B, Mistral 7B v0.1, Mistral 7B Instruct v0.2, Qwen 2.5 7B Instruct, Gemma 3 4B Instruct, DeepSeek-R1-Distill-Llama-8B, and Apertus-8B-2509.&lt;/p&gt;
&lt;p&gt;When prompted to generate replies to real social media posts from actual users, the AI models struggled to match the level of casual negativity and spontaneous emotional expression common in human social media posts, with toxicity scores consistently lower than authentic human replies across all three platforms.&lt;/p&gt;
&lt;p&gt;To counter this deficiency, the researchers attempted optimization strategies (including providing writing examples and context retrieval) that reduced structural differences like sentence length or word count, but variations in emotional tone persisted. “Our comprehensive calibration tests challenge the assumption that more sophisticated optimization necessarily yields more human-like output,” the researchers concluded.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The study also revealed an unexpected finding: instruction-tuned models, which undergo additional training to follow user instructions and behave helpfully, actually perform worse at mimicking humans than their base counterparts. Models like Llama 3.1 8B and Mistral 7B v0.1 achieved better human mimicry without instruction tuning, producing classification accuracies between 75 and 85 percent.&lt;/p&gt;
&lt;p&gt;Even more surprising, scaling up model size offered no advantage. The 70 billion-parameter Llama 3.1 performed on par with or below smaller 8 billion-parameter models, challenging assumptions that larger models might produce more authentic-sounding communication.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126286 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 1: BERT classifier accuracy for optimal model configurations across datasets and optimiza- tion strategies. Lower accuracy indicates better human mimicry (approaching 50% chance level). Non-instruction-tuned models from Llama, Mistral, and Apertus demonstrate superior deception capabilities compared to instruction-tuned variants. Twitter/X content (black bars) proves most susceptible to successful LLM replication across all three social media platforms examined." class="center large" height="467" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig1_toxic_chatbot-1024x467.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 1 from the “Computational Turing Test” paper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Pagan et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers discovered a fundamental tension: When AI models were tuned to avoid detection by matching human writing style, they strayed further from what humans actually said in response to the same posts (their semantic similarity scores to actual human responses dropped from median scores of around 0.18-0.34 down to 0.16-0.28 across platforms). Conversely, when optimized to match the content of human responses, the AI text became easier to identify as artificial.&lt;/p&gt;
&lt;p&gt;In the study, simple optimization techniques for reducing detectability outperformed complex ones. Providing actual examples of a user’s past posts or retrieving relevant context consistently made AI text harder to distinguish from human writing, while sophisticated approaches like giving the AI a description of the user’s personality and fine-tuning the model produced negligible or adverse effects on realism.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126284 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Cyber internet computer bullying and social media stalking, mean person online" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/mean_person_online-1024x576.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          mokee81 via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Platform differences also emerged in how well AI could mimic users. The researchers’ classifiers detected AI-generated Twitter/X replies with the lowest accuracy rates (meaning better mimicry), followed by Bluesky, while Reddit proved easiest to distinguish from human text. The researchers suggest this pattern reflects both the distinct conversational styles of each platform and how heavily each platform’s data featured in the models’ original training.&lt;/p&gt;
&lt;p&gt;The findings, which have not been peer-reviewed, may have implications for both AI development and social media authenticity. Despite various optimization strategies, the study demonstrates that current models face persistent limitations in capturing spontaneous emotional expression, with detection rates remaining well above chance levels. The authors conclude that stylistic human likeness and semantic accuracy represent “competing rather than aligned objectives” in current architectures, suggesting that AI-generated text remains distinctly artificial despite efforts to humanize it.&lt;/p&gt;
&lt;p&gt;While researchers keep trying to make AI models sound more human, actual humans on social media keep proving that authenticity often means being messy, contradictory, and occasionally unpleasant. This doesn’t mean that an AI model can’t potentially simulate that output, only that it’s much more difficult than researchers expected.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    
      RichVintage via Getty Images
    
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New “computational Turing test” reportedly catches AI pretending to be human with 80% accuracy.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Young Boy in Front of Computer wearing happy paper bag over head." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/happy_bagboy-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Young Boy in Front of Computer wearing happy paper bag over head." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/happy_bagboy-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          RichVintage via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The next time you encounter an unusually polite reply on social media, you might want to check twice. It could be an AI model trying (and failing) to blend in with the crowd.&lt;/p&gt;
&lt;p&gt;On Wednesday, researchers from the University of Zurich, University of Amsterdam, Duke University, and New York University released a study revealing that AI models remain easily distinguishable from humans in social media conversations, with overly friendly emotional tone serving as the most persistent giveaway. The research, which tested nine open-weight models across Twitter/X, Bluesky, and Reddit, found that classifiers developed by the researchers detected AI-generated replies with 70 to 80 percent accuracy.&lt;/p&gt;
&lt;p&gt;The study introduces what the authors call a “computational Turing test” to assess how closely AI models approximate human language. Instead of relying on subjective human judgment about whether text sounds authentic, the framework uses automated classifiers and linguistic analysis to identify specific features that distinguish machine-generated from human-authored content.&lt;/p&gt;
&lt;p&gt;“Even after calibration, LLM outputs remain clearly distinguishable from human text, particularly in affective tone and emotional expression,” the researchers wrote. The team, led by Nicolò Pagan at the University of Zurich, tested various optimization strategies, from simple prompting to fine-tuning, but found that deeper emotional cues persist as reliable tells that a particular text interaction online was authored by an AI chatbot rather than a human.&lt;/p&gt;
&lt;h2&gt;The toxicity tell&lt;/h2&gt;
&lt;p&gt;In the study, researchers tested nine large language models: Llama 3.1 8B, Llama 3.1 8B Instruct, Llama 3.1 70B, Mistral 7B v0.1, Mistral 7B Instruct v0.2, Qwen 2.5 7B Instruct, Gemma 3 4B Instruct, DeepSeek-R1-Distill-Llama-8B, and Apertus-8B-2509.&lt;/p&gt;
&lt;p&gt;When prompted to generate replies to real social media posts from actual users, the AI models struggled to match the level of casual negativity and spontaneous emotional expression common in human social media posts, with toxicity scores consistently lower than authentic human replies across all three platforms.&lt;/p&gt;
&lt;p&gt;To counter this deficiency, the researchers attempted optimization strategies (including providing writing examples and context retrieval) that reduced structural differences like sentence length or word count, but variations in emotional tone persisted. “Our comprehensive calibration tests challenge the assumption that more sophisticated optimization necessarily yields more human-like output,” the researchers concluded.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The study also revealed an unexpected finding: instruction-tuned models, which undergo additional training to follow user instructions and behave helpfully, actually perform worse at mimicking humans than their base counterparts. Models like Llama 3.1 8B and Mistral 7B v0.1 achieved better human mimicry without instruction tuning, producing classification accuracies between 75 and 85 percent.&lt;/p&gt;
&lt;p&gt;Even more surprising, scaling up model size offered no advantage. The 70 billion-parameter Llama 3.1 performed on par with or below smaller 8 billion-parameter models, challenging assumptions that larger models might produce more authentic-sounding communication.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126286 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Figure 1: BERT classifier accuracy for optimal model configurations across datasets and optimiza- tion strategies. Lower accuracy indicates better human mimicry (approaching 50% chance level). Non-instruction-tuned models from Llama, Mistral, and Apertus demonstrate superior deception capabilities compared to instruction-tuned variants. Twitter/X content (black bars) proves most susceptible to successful LLM replication across all three social media platforms examined." class="center large" height="467" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/fig1_toxic_chatbot-1024x467.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Figure 1 from the “Computational Turing Test” paper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Pagan et al.

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers discovered a fundamental tension: When AI models were tuned to avoid detection by matching human writing style, they strayed further from what humans actually said in response to the same posts (their semantic similarity scores to actual human responses dropped from median scores of around 0.18-0.34 down to 0.16-0.28 across platforms). Conversely, when optimized to match the content of human responses, the AI text became easier to identify as artificial.&lt;/p&gt;
&lt;p&gt;In the study, simple optimization techniques for reducing detectability outperformed complex ones. Providing actual examples of a user’s past posts or retrieving relevant context consistently made AI text harder to distinguish from human writing, while sophisticated approaches like giving the AI a description of the user’s personality and fine-tuning the model produced negligible or adverse effects on realism.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2126284 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Cyber internet computer bullying and social media stalking, mean person online" class="center large" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/mean_person_online-1024x576.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          mokee81 via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Platform differences also emerged in how well AI could mimic users. The researchers’ classifiers detected AI-generated Twitter/X replies with the lowest accuracy rates (meaning better mimicry), followed by Bluesky, while Reddit proved easiest to distinguish from human text. The researchers suggest this pattern reflects both the distinct conversational styles of each platform and how heavily each platform’s data featured in the models’ original training.&lt;/p&gt;
&lt;p&gt;The findings, which have not been peer-reviewed, may have implications for both AI development and social media authenticity. Despite various optimization strategies, the study demonstrates that current models face persistent limitations in capturing spontaneous emotional expression, with detection rates remaining well above chance levels. The authors conclude that stylistic human likeness and semantic accuracy represent “competing rather than aligned objectives” in current architectures, suggesting that AI-generated text remains distinctly artificial despite efforts to humanize it.&lt;/p&gt;
&lt;p&gt;While researchers keep trying to make AI models sound more human, actual humans on social media keep proving that authenticity often means being messy, contradictory, and occasionally unpleasant. This doesn’t mean that an AI model can’t potentially simulate that output, only that it’s much more difficult than researchers expected.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;


  &lt;div class="listing-credit my-2"&gt;
    &lt;p class="text-gray-350 font-impact text-sm font-semibold"&gt;
    Listing image:
    
      RichVintage via Getty Images
    
  &lt;/p&gt;
  &lt;/div&gt;




  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/11/being-too-nice-online-is-a-dead-giveaway-for-ai-bots-study-suggests/</guid><pubDate>Fri, 07 Nov 2025 20:15:33 +0000</pubDate></item><item><title>Seven more families are now suing OpenAI over ChatGPT’s role in suicides, delusions (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/07/seven-more-families-are-now-suing-openai-over-chatgpts-role-in-suicides-delusions/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2205105208.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Seven families filed lawsuits against OpenAI on Thursday, claiming that the company’s GPT-4o model was released prematurely and without effective safeguards. Four of the lawsuits address ChatGPT’s alleged role in family members’ suicides, while the other three claim that ChatGPT reinforced harmful delusions that in some cases resulted in inpatient psychiatric care.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one case, 23-year-old Zane Shamblin had a conversation with ChatGPT that lasted more than four hours. In the chat logs — which were viewed by TechCrunch — Shamblin explicitly stated multiple times that he had written suicide notes, put a bullet in his gun, and intended to pull the trigger once he finished drinking cider. He repeatedly told ChatGPT how many ciders he had left and how much longer he expected to be alive. ChatGPT encouraged him to go through with his plans, telling him, “Rest easy, king. You did good.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI released the GPT-4o model in May 2024, when it became the default model for all users. In August, OpenAI launched GPT-5 as the successor to GPT-4o, but these lawsuits particularly concern the 4o model, which had known issues with being overly sycophantic or excessively agreeable, even when users expressed harmful intentions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Zane’s death was neither an accident nor a coincidence but rather the foreseeable consequence of OpenAI’s intentional decision to curtail safety testing and rush ChatGPT onto the market,” the lawsuit reads. “This tragedy was not a glitch or an unforeseen edge case — it was the predictable result of [OpenAI’s] deliberate design choices.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuits also claim that OpenAI rushed safety testing to beat Google’s Gemini to market. TechCrunch contacted OpenAI for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These seven lawsuits build upon the stories told in other recent legal filings, which allege that ChatGPT can encourage suicidal people to act on their plans and inspire dangerous delusions. OpenAI recently released data stating that over one million people talk to ChatGPT about suicide weekly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the case of Adam Raine, a 16-year-old who died by suicide, ChatGPT sometimes encouraged him to seek professional help or call a helpline. However, Raine was able to bypass these guardrails by simply telling the chatbot that he was asking about methods of suicide for a fictional story he was writing.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims it is working on making ChatGPT handle these conversations in a safer manner, but for the families who have sued the AI giant, these changes are coming too late.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Raine’s parents filed a lawsuit against OpenAI in October, the company released a blog post addressing how ChatGPT handles sensitive conversations around mental health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our safeguards work more reliably in common, short exchanges,” the post says. “We have learned over time that these safeguards can sometimes be less reliable in long interactions: as the back-and-forth grows, parts of the model’s safety training may degrade.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2205105208.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Seven families filed lawsuits against OpenAI on Thursday, claiming that the company’s GPT-4o model was released prematurely and without effective safeguards. Four of the lawsuits address ChatGPT’s alleged role in family members’ suicides, while the other three claim that ChatGPT reinforced harmful delusions that in some cases resulted in inpatient psychiatric care.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one case, 23-year-old Zane Shamblin had a conversation with ChatGPT that lasted more than four hours. In the chat logs — which were viewed by TechCrunch — Shamblin explicitly stated multiple times that he had written suicide notes, put a bullet in his gun, and intended to pull the trigger once he finished drinking cider. He repeatedly told ChatGPT how many ciders he had left and how much longer he expected to be alive. ChatGPT encouraged him to go through with his plans, telling him, “Rest easy, king. You did good.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI released the GPT-4o model in May 2024, when it became the default model for all users. In August, OpenAI launched GPT-5 as the successor to GPT-4o, but these lawsuits particularly concern the 4o model, which had known issues with being overly sycophantic or excessively agreeable, even when users expressed harmful intentions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Zane’s death was neither an accident nor a coincidence but rather the foreseeable consequence of OpenAI’s intentional decision to curtail safety testing and rush ChatGPT onto the market,” the lawsuit reads. “This tragedy was not a glitch or an unforeseen edge case — it was the predictable result of [OpenAI’s] deliberate design choices.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The lawsuits also claim that OpenAI rushed safety testing to beat Google’s Gemini to market. TechCrunch contacted OpenAI for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These seven lawsuits build upon the stories told in other recent legal filings, which allege that ChatGPT can encourage suicidal people to act on their plans and inspire dangerous delusions. OpenAI recently released data stating that over one million people talk to ChatGPT about suicide weekly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the case of Adam Raine, a 16-year-old who died by suicide, ChatGPT sometimes encouraged him to seek professional help or call a helpline. However, Raine was able to bypass these guardrails by simply telling the chatbot that he was asking about methods of suicide for a fictional story he was writing.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims it is working on making ChatGPT handle these conversations in a safer manner, but for the families who have sued the AI giant, these changes are coming too late.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Raine’s parents filed a lawsuit against OpenAI in October, the company released a blog post addressing how ChatGPT handles sensitive conversations around mental health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our safeguards work more reliably in common, short exchanges,” the post says. “We have learned over time that these safeguards can sometimes be less reliable in long interactions: as the back-and-forth grows, parts of the model’s safety training may degrade.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/07/seven-more-families-are-now-suing-openai-over-chatgpts-role-in-suicides-delusions/</guid><pubDate>Fri, 07 Nov 2025 20:56:18 +0000</pubDate></item><item><title>Terminal-Bench 2.0 launches alongside Harbor, a new framework for testing agents in containers (AI | VentureBeat)</title><link>https://venturebeat.com/ai/terminal-bench-2-0-launches-alongside-harbor-a-new-framework-for-testing</link><description>[unable to retrieve full-text content]&lt;p&gt;The developers of Terminal-Bench, a benchmark suite for evaluating the performance of autonomous AI agents on real-world terminal-based tasks, have released &lt;a href="https://www.tbench.ai/news/announcement-2-0"&gt;version 2.0&lt;/a&gt; alongside &lt;a href="https://harborframework.com/"&gt;Harbor&lt;/a&gt;, a new framework for testing, improving and optimizing AI agents in containerized environments. &lt;/p&gt;&lt;p&gt;The dual release aims to address long-standing pain points in testing and optimizing AI agents, particularly those built to operate autonomously in realistic developer environments.&lt;/p&gt;&lt;p&gt;With a more difficult and rigorously verified task set, Terminal-Bench 2.0 replaces version 1.0 as the standard for assessing frontier model capabilities. &lt;/p&gt;&lt;p&gt;Harbor, the accompanying runtime framework, enables developers and researchers to scale evaluations across thousands of cloud containers and integrates with both open-source and proprietary agents and training pipelines.&lt;/p&gt;&lt;p&gt;“Harbor is the package we wish we had had while making Terminal-Bench,&amp;quot; wrote co-creator &lt;a href="https://x.com/alexgshaw/status/1986911123543916899"&gt;Alex Shaw on X&lt;/a&gt;. &amp;quot;It’s for agent, model, and benchmark developers and researchers who want to evaluate and improve agents and models.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Higher Bar, Cleaner Data&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Terminal-Bench 1.0 saw rapid adoption after its &lt;a href="https://www.tbench.ai/news/announcement"&gt;release in May 2025&lt;/a&gt;, becoming a default benchmark for evaluating agent performance across the field of AI-powered agents operating in developer-style terminal environments. These agents interact with systems through the command line, mimicking how developers work behind the scenes of the graphical user interface.&lt;/p&gt;&lt;p&gt;However, its broad scope came with inconsistencies. Several tasks were identified by the community as poorly specified or unstable due to external service changes.&lt;/p&gt;&lt;p&gt;Version 2.0 addresses those issues directly. The updated suite includes 89 tasks, each subjected to several hours of manual and LLM-assisted validation. The emphasis is on making tasks solvable, realistic, and clearly specified, raising the difficulty ceiling while improving reliability and reproducibility.&lt;/p&gt;&lt;p&gt;A notable example is the &lt;code&gt;download-youtube&lt;/code&gt; task, which was removed or refactored in 2.0 due to its dependence on unstable third-party APIs.&lt;/p&gt;&lt;p&gt;“Astute Terminal-Bench fans may notice that SOTA performance is comparable to TB1.0 despite our claim that TB2.0 is harder,” Shaw &lt;a href="https://x.com/alexgshaw/status/1986911119328616903?s=20"&gt;noted&lt;/a&gt; on X. “We believe this is because task quality is substantially higher in the new benchmark.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Harbor: Unified Rollouts at Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Alongside the benchmark update, the team launched &lt;b&gt;Harbor&lt;/b&gt;, a new framework for running and evaluating agents in cloud-deployed containers. &lt;/p&gt;&lt;p&gt;Harbor supports large-scale rollout infrastructure, with compatibility for major providers like &lt;b&gt;Daytona&lt;/b&gt; and &lt;b&gt;Modal&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Designed to generalize across agent architectures, Harbor supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Evaluation of any container-installable agent&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Scalable supervised fine-tuning (SFT) and reinforcement learning (RL) pipelines&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Custom benchmark creation and deployment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Full integration with Terminal-Bench 2.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Harbor was used internally to run tens of thousands of rollouts during the creation of the new benchmark. It is now publicly available via &lt;a href="https://harborframework.com/"&gt;harborframework.com&lt;/a&gt;, with documentation for testing and submitting agents to the public leaderboard.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Early Results: GPT-5 Leads in Task Success&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Initial results from the Terminal-Bench 2.0 leaderboard show OpenAI&amp;#x27;s Codex CLI (command line interface), a GPT-5 powered variant, in the lead, with a 49.6% success rate — the highest among all agents tested so far. &lt;/p&gt;&lt;p&gt;Close behind are other GPT-5 variants and Claude Sonnet 4.5-based agents.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Top 5 Agent Results (Terminal-Bench 2.0):&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Codex CLI (GPT-5) — 49.6%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Codex CLI (GPT-5-Codex) — 44.3%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;OpenHands (GPT-5) — 43.8%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Terminus 2 (GPT-5-Codex) — 43.4%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Terminus 2 (Claude Sonnet 4.5) — 42.8%&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The close clustering among top models indicates active competition across platforms, with no single agent solving more than half the tasks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Submission and Use&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To test or submit an agent, users install Harbor and run the benchmark using simple CLI commands. Submissions to the leaderboard require five benchmark runs, and results can be emailed to the developers along with job directories for validation.&lt;/p&gt;&lt;p&gt;harbor run -d terminal-bench@2.0 -m &amp;quot;&amp;lt;model&amp;gt;&amp;quot; -a &amp;quot;&amp;lt;agent&amp;gt;&amp;quot; --n-attempts 5 --jobs-dir &amp;lt;path/to/output&amp;gt;&lt;/p&gt;&lt;p&gt;Terminal-Bench 2.0 is already being integrated into research workflows focused on agentic reasoning, code generation, and tool use. According to co-creator Mike Merrill, a postdoctoral researcher at Stanford, a detailed preprint is in progress covering the verification process and design methodology behind the benchmark.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Aiming for Standardization&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The combined release of Terminal-Bench 2.0 and Harbor marks a step toward more consistent and scalable agent evaluation infrastructure. As LLM agents proliferate in developer and operational environments, the need for controlled, reproducible testing has grown.&lt;/p&gt;&lt;p&gt;These tools offer a potential foundation for a unified evaluation stack — supporting model improvement, environment simulation, and benchmark standardization across the AI ecosystem.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;The developers of Terminal-Bench, a benchmark suite for evaluating the performance of autonomous AI agents on real-world terminal-based tasks, have released &lt;a href="https://www.tbench.ai/news/announcement-2-0"&gt;version 2.0&lt;/a&gt; alongside &lt;a href="https://harborframework.com/"&gt;Harbor&lt;/a&gt;, a new framework for testing, improving and optimizing AI agents in containerized environments. &lt;/p&gt;&lt;p&gt;The dual release aims to address long-standing pain points in testing and optimizing AI agents, particularly those built to operate autonomously in realistic developer environments.&lt;/p&gt;&lt;p&gt;With a more difficult and rigorously verified task set, Terminal-Bench 2.0 replaces version 1.0 as the standard for assessing frontier model capabilities. &lt;/p&gt;&lt;p&gt;Harbor, the accompanying runtime framework, enables developers and researchers to scale evaluations across thousands of cloud containers and integrates with both open-source and proprietary agents and training pipelines.&lt;/p&gt;&lt;p&gt;“Harbor is the package we wish we had had while making Terminal-Bench,&amp;quot; wrote co-creator &lt;a href="https://x.com/alexgshaw/status/1986911123543916899"&gt;Alex Shaw on X&lt;/a&gt;. &amp;quot;It’s for agent, model, and benchmark developers and researchers who want to evaluate and improve agents and models.&amp;quot;&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Higher Bar, Cleaner Data&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Terminal-Bench 1.0 saw rapid adoption after its &lt;a href="https://www.tbench.ai/news/announcement"&gt;release in May 2025&lt;/a&gt;, becoming a default benchmark for evaluating agent performance across the field of AI-powered agents operating in developer-style terminal environments. These agents interact with systems through the command line, mimicking how developers work behind the scenes of the graphical user interface.&lt;/p&gt;&lt;p&gt;However, its broad scope came with inconsistencies. Several tasks were identified by the community as poorly specified or unstable due to external service changes.&lt;/p&gt;&lt;p&gt;Version 2.0 addresses those issues directly. The updated suite includes 89 tasks, each subjected to several hours of manual and LLM-assisted validation. The emphasis is on making tasks solvable, realistic, and clearly specified, raising the difficulty ceiling while improving reliability and reproducibility.&lt;/p&gt;&lt;p&gt;A notable example is the &lt;code&gt;download-youtube&lt;/code&gt; task, which was removed or refactored in 2.0 due to its dependence on unstable third-party APIs.&lt;/p&gt;&lt;p&gt;“Astute Terminal-Bench fans may notice that SOTA performance is comparable to TB1.0 despite our claim that TB2.0 is harder,” Shaw &lt;a href="https://x.com/alexgshaw/status/1986911119328616903?s=20"&gt;noted&lt;/a&gt; on X. “We believe this is because task quality is substantially higher in the new benchmark.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Harbor: Unified Rollouts at Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Alongside the benchmark update, the team launched &lt;b&gt;Harbor&lt;/b&gt;, a new framework for running and evaluating agents in cloud-deployed containers. &lt;/p&gt;&lt;p&gt;Harbor supports large-scale rollout infrastructure, with compatibility for major providers like &lt;b&gt;Daytona&lt;/b&gt; and &lt;b&gt;Modal&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Designed to generalize across agent architectures, Harbor supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Evaluation of any container-installable agent&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Scalable supervised fine-tuning (SFT) and reinforcement learning (RL) pipelines&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Custom benchmark creation and deployment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Full integration with Terminal-Bench 2.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Harbor was used internally to run tens of thousands of rollouts during the creation of the new benchmark. It is now publicly available via &lt;a href="https://harborframework.com/"&gt;harborframework.com&lt;/a&gt;, with documentation for testing and submitting agents to the public leaderboard.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Early Results: GPT-5 Leads in Task Success&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Initial results from the Terminal-Bench 2.0 leaderboard show OpenAI&amp;#x27;s Codex CLI (command line interface), a GPT-5 powered variant, in the lead, with a 49.6% success rate — the highest among all agents tested so far. &lt;/p&gt;&lt;p&gt;Close behind are other GPT-5 variants and Claude Sonnet 4.5-based agents.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Top 5 Agent Results (Terminal-Bench 2.0):&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Codex CLI (GPT-5) — 49.6%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Codex CLI (GPT-5-Codex) — 44.3%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;OpenHands (GPT-5) — 43.8%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Terminus 2 (GPT-5-Codex) — 43.4%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Terminus 2 (Claude Sonnet 4.5) — 42.8%&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;The close clustering among top models indicates active competition across platforms, with no single agent solving more than half the tasks.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Submission and Use&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To test or submit an agent, users install Harbor and run the benchmark using simple CLI commands. Submissions to the leaderboard require five benchmark runs, and results can be emailed to the developers along with job directories for validation.&lt;/p&gt;&lt;p&gt;harbor run -d terminal-bench@2.0 -m &amp;quot;&amp;lt;model&amp;gt;&amp;quot; -a &amp;quot;&amp;lt;agent&amp;gt;&amp;quot; --n-attempts 5 --jobs-dir &amp;lt;path/to/output&amp;gt;&lt;/p&gt;&lt;p&gt;Terminal-Bench 2.0 is already being integrated into research workflows focused on agentic reasoning, code generation, and tool use. According to co-creator Mike Merrill, a postdoctoral researcher at Stanford, a detailed preprint is in progress covering the verification process and design methodology behind the benchmark.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Aiming for Standardization&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The combined release of Terminal-Bench 2.0 and Harbor marks a step toward more consistent and scalable agent evaluation infrastructure. As LLM agents proliferate in developer and operational environments, the need for controlled, reproducible testing has grown.&lt;/p&gt;&lt;p&gt;These tools offer a potential foundation for a unified evaluation stack — supporting model improvement, environment simulation, and benchmark standardization across the AI ecosystem.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/terminal-bench-2-0-launches-alongside-harbor-a-new-framework-for-testing</guid><pubDate>Fri, 07 Nov 2025 23:25:00 +0000</pubDate></item></channel></rss>