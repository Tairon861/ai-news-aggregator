<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 10 Dec 2025 01:51:51 +0000</lastBuildDate><item><title>OpenAI targets AI skills gap with new certification standards (AI News)</title><link>https://www.artificialintelligence-news.com/news/openai-targets-ai-skills-gap-with-new-certification-standards/</link><description>&lt;p&gt;Adoption of generative AI has outpaced workforce capability, prompting OpenAI to target the skills gap with new certification standards.&lt;/p&gt;&lt;p&gt;While it’s safe to say OpenAI’s tools have reached mass adoption, organisations struggle to convert this usage into reliable output. To address this, OpenAI has announced ‘AI Foundations,’ a structured initiative designed to standardise how employees learn and apply the technology.&lt;/p&gt;&lt;p&gt;OpenAI’s initiative marks a necessary evolution in the vendor ecosystem; indicating a departure from the “move fast” phase of experimental deployment toward a focus on verifiable competence. OpenAI explicitly states its intention to certify 10 million Americans by 2030.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-workers-and-employers-have-an-incentive-to-close-the-ai-skills-gap"&gt;Workers and employers have an incentive to close the AI skills gap&lt;/h3&gt;&lt;p&gt;The economic case for AI training and certification is rooted in wage and productivity data. Workers possessing AI skills earn approximately 50 percent more than those without them. However, CIOs often find that productivity gains on paper fail to materialise in practice. OpenAI notes that gains “only materialise when people have the skills to use the technology.”&lt;/p&gt;&lt;p&gt;Without guidance, widespread access can create operational risk. OpenAI admits the technology is “disruptive, leaving many people unsure which skills matter most.” By defining a standard curriculum, OpenAI aims to help organisations capture the efficiency gains promised by their software investments.&lt;/p&gt;&lt;p&gt;The delivery method for AI Foundations differs from traditional corporate LMS (Learning Management System) modules. The course sits directly inside ChatGPT, allowing the platform to act as “tutor, the practice space, and the feedback loop” simultaneously. This integration allows learners to execute real tasks and receive context-aware corrections to help close the AI skills gap, rather than just watching passive video content.&lt;/p&gt;&lt;p&gt;Completing the programme yields a badge verifying “job-ready AI skills”. This credential serves as a stepping stone toward a full OpenAI Certification. To ensure these badges carry weight in the labour market, OpenAI has engaged Coursera, ETS, and Credly by Pearson to validate the psychometric rigour and design of the assessments.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-operational-pilots-for-the-ai-certification-and-improving-the-hiring-pipeline"&gt;Operational pilots for the AI certification and improving the hiring pipeline&lt;/h3&gt;&lt;p&gt;A consortium of large-scale employers and public-sector bodies will test the curriculum before a wider rollout. Pilot partners include Walmart, John Deere, Lowe’s, Boston Consulting Group, Russell Reynolds Associates, Upwork, Elevance Health, and Accenture. The Office of the Governor of Delaware is also participating, which shows interest from state-level administration.&lt;/p&gt;&lt;p&gt;These partners span industries with heavy operational footprints (including retail, agriculture, and healthcare) suggesting the training targets core business functions rather than just technical roles. OpenAI plans to use the next few months to refine the course based on data from these pilots to ensure that it can effectively close the AI skills gap.&lt;/p&gt;&lt;p&gt;OpenAI’s initiative extends into recruitment. The company is developing an ‘OpenAI Jobs Platform’ to connect certified workers with employers. Partnerships with Indeed and Upwork support this mechanism, aiming to make it easier for businesses to identify candidates with verified technical expertise.&lt;/p&gt;&lt;p&gt;For hiring managers, this offers a potential solution to the difficulty of vetting AI literacy. A standardised AI certification could reduce the reliance on self-reported skills, providing “portable evidence” of a candidate’s development.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-academic-alignment-to-seed-future-ai-talent"&gt;Academic alignment to seed future AI talent&lt;/h3&gt;&lt;p&gt;While the enterprise focus is immediate, OpenAI is also seeding the future talent pipeline. A ‘ChatGPT Foundations for Teachers’ course has launched on Coursera. With three in five teachers already using AI tools to save time and personalise materials, this stream aims to formalise existing habits.&lt;/p&gt;&lt;p&gt;Simultaneously, pilots with Arizona State University and the California State University system are creating pathways for students to certify their skills before entering the job market. This ensures that the next wave of graduates arrives with the “job-ready” verification that enterprise employers are beginning to demand.&lt;/p&gt;&lt;p&gt;Organisations must now decide whether to rely on vendor-supplied certification or continue developing proprietary training. The involvement of firms like Boston Consulting Group and Accenture implies that major players see value in a standardised external benchmark.&lt;/p&gt;&lt;p&gt;As OpenAI moves to certify millions of people and close the AI skills gap, the certification badge may become a baseline expectation for knowledge workers much like office suite proficiency in previous decades.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Instacart pilots agentic commerce by embedding in ChatGPT&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Adoption of generative AI has outpaced workforce capability, prompting OpenAI to target the skills gap with new certification standards.&lt;/p&gt;&lt;p&gt;While it’s safe to say OpenAI’s tools have reached mass adoption, organisations struggle to convert this usage into reliable output. To address this, OpenAI has announced ‘AI Foundations,’ a structured initiative designed to standardise how employees learn and apply the technology.&lt;/p&gt;&lt;p&gt;OpenAI’s initiative marks a necessary evolution in the vendor ecosystem; indicating a departure from the “move fast” phase of experimental deployment toward a focus on verifiable competence. OpenAI explicitly states its intention to certify 10 million Americans by 2030.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-workers-and-employers-have-an-incentive-to-close-the-ai-skills-gap"&gt;Workers and employers have an incentive to close the AI skills gap&lt;/h3&gt;&lt;p&gt;The economic case for AI training and certification is rooted in wage and productivity data. Workers possessing AI skills earn approximately 50 percent more than those without them. However, CIOs often find that productivity gains on paper fail to materialise in practice. OpenAI notes that gains “only materialise when people have the skills to use the technology.”&lt;/p&gt;&lt;p&gt;Without guidance, widespread access can create operational risk. OpenAI admits the technology is “disruptive, leaving many people unsure which skills matter most.” By defining a standard curriculum, OpenAI aims to help organisations capture the efficiency gains promised by their software investments.&lt;/p&gt;&lt;p&gt;The delivery method for AI Foundations differs from traditional corporate LMS (Learning Management System) modules. The course sits directly inside ChatGPT, allowing the platform to act as “tutor, the practice space, and the feedback loop” simultaneously. This integration allows learners to execute real tasks and receive context-aware corrections to help close the AI skills gap, rather than just watching passive video content.&lt;/p&gt;&lt;p&gt;Completing the programme yields a badge verifying “job-ready AI skills”. This credential serves as a stepping stone toward a full OpenAI Certification. To ensure these badges carry weight in the labour market, OpenAI has engaged Coursera, ETS, and Credly by Pearson to validate the psychometric rigour and design of the assessments.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-operational-pilots-for-the-ai-certification-and-improving-the-hiring-pipeline"&gt;Operational pilots for the AI certification and improving the hiring pipeline&lt;/h3&gt;&lt;p&gt;A consortium of large-scale employers and public-sector bodies will test the curriculum before a wider rollout. Pilot partners include Walmart, John Deere, Lowe’s, Boston Consulting Group, Russell Reynolds Associates, Upwork, Elevance Health, and Accenture. The Office of the Governor of Delaware is also participating, which shows interest from state-level administration.&lt;/p&gt;&lt;p&gt;These partners span industries with heavy operational footprints (including retail, agriculture, and healthcare) suggesting the training targets core business functions rather than just technical roles. OpenAI plans to use the next few months to refine the course based on data from these pilots to ensure that it can effectively close the AI skills gap.&lt;/p&gt;&lt;p&gt;OpenAI’s initiative extends into recruitment. The company is developing an ‘OpenAI Jobs Platform’ to connect certified workers with employers. Partnerships with Indeed and Upwork support this mechanism, aiming to make it easier for businesses to identify candidates with verified technical expertise.&lt;/p&gt;&lt;p&gt;For hiring managers, this offers a potential solution to the difficulty of vetting AI literacy. A standardised AI certification could reduce the reliance on self-reported skills, providing “portable evidence” of a candidate’s development.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-academic-alignment-to-seed-future-ai-talent"&gt;Academic alignment to seed future AI talent&lt;/h3&gt;&lt;p&gt;While the enterprise focus is immediate, OpenAI is also seeding the future talent pipeline. A ‘ChatGPT Foundations for Teachers’ course has launched on Coursera. With three in five teachers already using AI tools to save time and personalise materials, this stream aims to formalise existing habits.&lt;/p&gt;&lt;p&gt;Simultaneously, pilots with Arizona State University and the California State University system are creating pathways for students to certify their skills before entering the job market. This ensures that the next wave of graduates arrives with the “job-ready” verification that enterprise employers are beginning to demand.&lt;/p&gt;&lt;p&gt;Organisations must now decide whether to rely on vendor-supplied certification or continue developing proprietary training. The involvement of firms like Boston Consulting Group and Accenture implies that major players see value in a standardised external benchmark.&lt;/p&gt;&lt;p&gt;As OpenAI moves to certify millions of people and close the AI skills gap, the certification badge may become a baseline expectation for knowledge workers much like office suite proficiency in previous decades.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Instacart pilots agentic commerce by embedding in ChatGPT&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/openai-targets-ai-skills-gap-with-new-certification-standards/</guid><pubDate>Tue, 09 Dec 2025 14:45:06 +0000</pubDate></item><item><title>Pebble maker announces Index 01, a smart-ish ring for under $100 (AI – Ars Technica)</title><link>https://arstechnica.com/gadgets/2025/12/resurrected-pebble-maker-announces-a-kind-of-smart-ring-for-capturing-audio-notes/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pebble Index 01 isn’t quite a smart ring, but it can do some smart things.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Index 01 rock background" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Index 01 rock background" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Nearly a decade after Pebble’s nascent smartwatch empire crumbled, the brand is staging a comeback with new wearables. The Pebble Core Duo 2 and Core Time 2 are a natural evolution of the company’s low-power smartwatch designs, but its next wearable is something different. The Index 01 is a ring, but you probably shouldn’t call it a smart ring. The Index does just one thing—capture voice notes—but the firm says it does that one thing extremely well.&lt;/p&gt;
&lt;p&gt;Most of today’s smart rings offer users the ability to track health stats, along with various minor smartphone integrations. With all the sensors and data collection, these devices can cost as much as a smartwatch and require frequent charging. The Index 01 doesn’t do any of that. It contains a Bluetooth radio, a microphone, a hearing aid battery, and a physical button. You press the button, record your note, and that’s it. The company says the Index 01 will run for years on a charge and will cost just $75 during the preorder period. After that, it will go up to $99.&lt;/p&gt;
&lt;p&gt;Core Devices, the new home of Pebble, says the Index is designed to be worn on your index finger (get it?), where you can easily mash the device’s button with your thumb. Unlike recording notes with a phone or smartwatch, you don’t need both hands to create voice notes with the Index.&lt;/p&gt;
&lt;p&gt;The ring’s lone physical control is tactile, ensuring you’ll know when it’s activated and recording. When you’re done talking, just release the button. If that button is not depressed, the ring won’t record audio for any reason. The company apparently worked to ensure this process is 100 percent reliable—it only does one thing, so it really has to do it well.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130868 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Index 01 holding bag" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-lifestyle.png" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The ring is designed to be worn on the index finger so the button can be pressed with your thumb.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A smart ring usually needs to be recharged every few days, but you will never recharge the Index. The idea is that since you never have to take it off to charge, using the Index 01 “becomes muscle memory.” The integrated battery will power the device for 12–14 total hours of recording. The designers estimate that to be roughly two years of usage if you record 10 to 20 short voice notes per day. And what happens when the battery runs out? You just send the ring back to be recycled.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A different approach to AI&lt;/h2&gt;
&lt;p&gt;There is a little more to the Index than meets the eye. The ring makes use of generative AI in a way that might have tempted most companies in 2025 to shout about it from the rooftops. However, Pebble isn’t looking to sell you an AI subscription or feed on your personal data.&lt;/p&gt;
&lt;p&gt;After you record a voice note, it’s beamed over Bluetooth to your phone (Android or iOS), and it stays there. The recording is converted to text and fed into a large language model (LLM) that runs locally on your device to take actions. The speech-to-text process and LLM operate in the open source Pebble app, and no data from your notes is sent to the Internet. However, there is an optional online backup service for your recordings.&lt;/p&gt;
&lt;p&gt;While the company is anxious to talk about the ironclad reliability of voice notes on the Index 01, there’s no such guarantee with an LLM. A model small enough to run on your phone has to focus on specific functionality rather than doing everything like a big cloud-based AI. So the Index will only support a few actions out of the box. Here’s the full list.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create or add to notes&lt;/li&gt;
&lt;li&gt;Set reminder&lt;/li&gt;
&lt;li&gt;Create alarm&lt;/li&gt;
&lt;li&gt;Create timer&lt;/li&gt;
&lt;li&gt;Play/pause/skip music track (via button press)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If that’s not enough, you’re in luck. The Index 01 is also designed to be hacking-friendly. The audio and transcribed text is yours to do with as you please. You can route it to a different app via a webhook, and the LLM supports model context protocol (MCP), so you can add new functionality that also runs locally. The AI model will also be released as an open source project.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130869 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-trio-together.jpg" width="1920" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The Index 01 comes in polished silver, polished gold, and matte black colorways and US sizes 6 through 13. Preorders start today at the $75 price. Worldwide shipping will begin in March 2026, at which time the price will go up to $99.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Pebble Index 01 isn’t quite a smart ring, but it can do some smart things.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Index 01 rock background" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Index 01 rock background" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Nearly a decade after Pebble’s nascent smartwatch empire crumbled, the brand is staging a comeback with new wearables. The Pebble Core Duo 2 and Core Time 2 are a natural evolution of the company’s low-power smartwatch designs, but its next wearable is something different. The Index 01 is a ring, but you probably shouldn’t call it a smart ring. The Index does just one thing—capture voice notes—but the firm says it does that one thing extremely well.&lt;/p&gt;
&lt;p&gt;Most of today’s smart rings offer users the ability to track health stats, along with various minor smartphone integrations. With all the sensors and data collection, these devices can cost as much as a smartwatch and require frequent charging. The Index 01 doesn’t do any of that. It contains a Bluetooth radio, a microphone, a hearing aid battery, and a physical button. You press the button, record your note, and that’s it. The company says the Index 01 will run for years on a charge and will cost just $75 during the preorder period. After that, it will go up to $99.&lt;/p&gt;
&lt;p&gt;Core Devices, the new home of Pebble, says the Index is designed to be worn on your index finger (get it?), where you can easily mash the device’s button with your thumb. Unlike recording notes with a phone or smartwatch, you don’t need both hands to create voice notes with the Index.&lt;/p&gt;
&lt;p&gt;The ring’s lone physical control is tactile, ensuring you’ll know when it’s activated and recording. When you’re done talking, just release the button. If that button is not depressed, the ring won’t record audio for any reason. The company apparently worked to ensure this process is 100 percent reliable—it only does one thing, so it really has to do it well.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130868 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Index 01 holding bag" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-lifestyle.png" width="1920" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The ring is designed to be worn on the index finger so the button can be pressed with your thumb.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;A smart ring usually needs to be recharged every few days, but you will never recharge the Index. The idea is that since you never have to take it off to charge, using the Index 01 “becomes muscle memory.” The integrated battery will power the device for 12–14 total hours of recording. The designers estimate that to be roughly two years of usage if you record 10 to 20 short voice notes per day. And what happens when the battery runs out? You just send the ring back to be recycled.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A different approach to AI&lt;/h2&gt;
&lt;p&gt;There is a little more to the Index than meets the eye. The ring makes use of generative AI in a way that might have tempted most companies in 2025 to shout about it from the rooftops. However, Pebble isn’t looking to sell you an AI subscription or feed on your personal data.&lt;/p&gt;
&lt;p&gt;After you record a voice note, it’s beamed over Bluetooth to your phone (Android or iOS), and it stays there. The recording is converted to text and fed into a large language model (LLM) that runs locally on your device to take actions. The speech-to-text process and LLM operate in the open source Pebble app, and no data from your notes is sent to the Internet. However, there is an optional online backup service for your recordings.&lt;/p&gt;
&lt;p&gt;While the company is anxious to talk about the ironclad reliability of voice notes on the Index 01, there’s no such guarantee with an LLM. A model small enough to run on your phone has to focus on specific functionality rather than doing everything like a big cloud-based AI. So the Index will only support a few actions out of the box. Here’s the full list.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create or add to notes&lt;/li&gt;
&lt;li&gt;Set reminder&lt;/li&gt;
&lt;li&gt;Create alarm&lt;/li&gt;
&lt;li&gt;Create timer&lt;/li&gt;
&lt;li&gt;Play/pause/skip music track (via button press)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If that’s not enough, you’re in luck. The Index 01 is also designed to be hacking-friendly. The audio and transcribed text is yours to do with as you please. You can route it to a different app via a webhook, and the LLM supports model context protocol (MCP), so you can add new functionality that also runs locally. The AI model will also be released as an open source project.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2130869 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1080" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-trio-together.jpg" width="1920" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Core Devices

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The Index 01 comes in polished silver, polished gold, and matte black colorways and US sizes 6 through 13. Preorders start today at the $75 price. Worldwide shipping will begin in March 2026, at which time the price will go up to $99.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gadgets/2025/12/resurrected-pebble-maker-announces-a-kind-of-smart-ring-for-capturing-audio-notes/</guid><pubDate>Tue, 09 Dec 2025 15:00:56 +0000</pubDate></item><item><title>GigaTIME: Scaling tumor microenvironment modeling using virtual population generated by multimodal AI (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/gigatime-scaling-tumor-microenvironment-modeling-using-virtual-population-generated-by-multimodal-ai/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustration of a person kneeling and looking through a large telescope on a textured pink surface. The telescope is aimed at a dark sky filled with colorful circular icons, each containing abstract shapes. The telescope itself has circuit-like patterns inside." class="wp-image-1156682" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/LANDSCAPE-INMUNE-MAPS_holasoyka.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;The convergence of digital transformation and the GenAI revolution creates an unprecedented opportunity for accelerating progress in precision health. Precision immunotherapy is a poster child for this transformation. Emerging technologies such as multiplex immunofluorescence (mIF) can assess internal states of individual cells along with their spatial locations, which is critical for deciphering how tumors interact with the immune system. The resulting insights, often referred to as the “grammar” of the tumor microenvironment, can help predict whether a tumor will respond to immunotherapy. If it is unlikely to respond, these insights can also inform strategies to reprogram the tumor from “cold” to “hot,” increasing its susceptibility to treatment.&lt;/p&gt;



&lt;p&gt;This is exciting, but progress is hindered by the high cost and limited scalability of current technology. For example, obtaining mIF data of a couple dozen protein channels for a tissue sample can cost thousands of dollars, and even the most advanced labs can barely scale it to a tiny fraction of their available tissue samples.&lt;/p&gt;



&lt;p&gt;In our paper published in Cell on&amp;nbsp;December&amp;nbsp;9, “Multimodal AI generates virtual population for tumor microenvironment modeling&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,” we present&amp;nbsp;GigaTIME&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a&amp;nbsp;multimodal AI&amp;nbsp;model&amp;nbsp;for translating routinely available hematoxylin and eosin (H&amp;amp;E)&amp;nbsp;pathology slides&amp;nbsp;to virtual&amp;nbsp;mIF&amp;nbsp;images. Developed in collaboration with Providence and the University of Washington, GigaTIME was trained on a Providence dataset of 40 million cells with paired H&amp;amp;E and mIF images across 21 protein channels. We applied&amp;nbsp;GigaTIME&amp;nbsp;to&amp;nbsp;14,256&amp;nbsp;cancer&amp;nbsp;patients from 51 hospitals and over a thousand clinics within the Providence system. This effort generated a virtual population of around 300,000 mIF images spanning 24 cancer types and 306 cancer subtypes. This virtual population uncovered&amp;nbsp;1,234 statistically significant&amp;nbsp;associations linking&amp;nbsp;mIF&amp;nbsp;protein activations with key clinical attributes such as biomarkers, staging,&amp;nbsp;and patient&amp;nbsp;survival.&amp;nbsp;Independent external validation&amp;nbsp;on&amp;nbsp;10,200&amp;nbsp;Cancer Genome Atlas (TCGA) patients further corroborated our findings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To our knowledge, this is the first population-scale study of tumor immune microenvironment (TIME) based on spatial proteomics. Such studies were previously infeasible due to the scarcity of mIF data. By translating readily available H&amp;amp;E pathology slides into high-resolution virtual mIF data, GigaTIME provides a novel research framework for exploring precision immuno-oncology through population-scale TIME analysis and discovery. We have made our GigaTIME model publicly available at Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and on Hugging Face&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to help accelerate clinical research in precision oncology.&lt;/p&gt;



&lt;p&gt;“GigaTIME&amp;nbsp;is about unlocking insights that were previously out of reach,” explained Carlo Bifulco, MD, chief medical officer of Providence Genomics and medical director of cancer genomics and precision oncology at the Providence Cancer Institute. “By analyzing the tumor microenvironment of thousands of patients,&amp;nbsp;GigaTIME&amp;nbsp;has the potential to accelerate discoveries that will shape the future of precision oncology and improve patient outcomes.”&amp;nbsp;&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Microsoft research newsletter&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Newsletter&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-newsletter"&gt;Stay connected to the research community at Microsoft.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="gigatime-generates-a-virtual-population-for-tumor-microenvironment-modeling"&gt;GigaTIME generates a virtual population for tumor microenvironment modeling&lt;/h2&gt;



&lt;p&gt;Digital pathology transforms a microscopy slide of stained tumor tissue into a high-resolution digital image, revealing details of cell morphology such as nucleus and cytoplasm. Such a slide only costs $5 to $10 per image and has become routinely available in cancer care. It is well known that H&amp;amp;E-based cell morphology contains information about the cellular states. Last year, we released GigaPath, the first digital pathology foundation model for scaling transformer architectures to gigapixel H&amp;amp;E slides. Afterward, researchers at Mount Sinai Hospital and Memorial Sloan Kettering Cancer Center showed in a global prospective trial that it can reliably predict a key biomarker from H&amp;amp;E slides for precision oncology triaging. However, such prior works are generally limited to average biomarker status across the entire tissue. GigaTIME thus represents a major step forward by learning to predict spatially resolved, single-cell states essential for tumor microenvironment modeling. In turn, this enables us to generate a virtual population of mIF images for large-scale TIME analysis (Figure 1).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. GigaTIME enables population-scale tumor immune microenvironment (TIME) analysis. A, GigaTIME inputs a hematoxylin and eosin (H&amp;amp;E) whole-slide image and outputs multiplex immunofluorescence (mIF) across 21 protein channels. By applying GigaTIME to 14,256 patients, we generated a virtual population with mIF information, leading to population-scale discovery on clinical biomarkers and patient stratification, with independent validation on TCGA. B, Circular plot visualizing a TIME spectrum encompassing the GigaTIME-translated virtual mIF activation scores across different protein channels at the population scale, where each channel is represented as an individual circular bar chart segment. The inner circle encodes OncoTree, which classifies 14,256 patients into 306 subtypes across 24 cancer types. The outer circle groups these activations by cancer types, allowing visual comparison across major categories. C, Scatter plot comparing the subtype-level GigaTIME-translated virtual mIF activations between TCGA and Providence virtual populations. Each dot denotes the average activation score of a protein channel among all tumors of a cancer subtype." class="wp-image-1153563" height="1518" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure1.png" width="1265" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. GigaTIME enables population-scale tumor immune microenvironment (TIME) analysis. A, GigaTIME inputs a hematoxylin and eosin (H&amp;amp;E) whole-slide image and outputs multiplex immunofluorescence (mIF) across 21 protein channels. By applying GigaTIME to 14,256 patients, we generated a virtual population with mIF information, leading to population-scale discovery on clinical biomarkers and patient stratification, with independent validation on TCGA. B, Circular plot visualizing a TIME spectrum encompassing the GigaTIME-translated virtual mIF activation scores across different protein channels at the population scale, where each channel is represented as an individual circular bar chart segment. The inner circle encodes OncoTree, which classifies 14,256 patients into 306 subtypes across 24 cancer types. The outer circle groups these activations by cancer types, allowing visual comparison across major categories. C, Scatter plot comparing the subtype-level GigaTIME-translated virtual mIF activations between TCGA and Providence virtual populations. Each dot denotes the average activation score of a protein channel among all tumors of a cancer subtype.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="gigatime-learns-a-multimodal-ai-model-to-translate-pathology-slides-into-spatial-proteomics-images-bridging-cell-morphology-and-cell-states"&gt;GigaTIME learns a multimodal AI model to translate pathology slides into spatial proteomics images, bridging cell morphology and cell states&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. GigaTIME enables translation from hematoxylin and eosin (H&amp;amp;E) to multiplex immunofluorescence (mIF) images. A,B, Bar plot comparing GigaTIME and CycleGAN on the translation performance in terms of Dice score (A) and Pearson correlation (B). C, Scatter plots comparing the activation density of the translated mIF and the ground truth mIF across four channels. D, Qualitative results for a sample H&amp;amp;E whole-slide image from our held-out test set with zoomed-in visualizations of the measured mIF and GigaTIME-translated mIF for DAPI, PD-L1, and CD68 channels." class="wp-image-1153564" height="1418" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure2.png" width="1241" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. GigaTIME enables translation from hematoxylin and eosin (H&amp;amp;E) to multiplex immunofluorescence (mIF) images. A,B, Bar plot comparing GigaTIME and CycleGAN on the translation performance in terms of Dice score (A) and Pearson correlation (B). C, Scatter plots comparing the activation density of the translated mIF and the ground truth mIF across four channels. D, Qualitative results for a sample H&amp;amp;E whole-slide image from our held-out test set with zoomed-in visualizations of the measured mIF and GigaTIME-translated mIF for DAPI, PD-L1, and CD68 channels.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;GigaTIME learned a cross-modal AI translator from digital pathology to spatial multiplex proteomics by training on 40 million cells with paired H&amp;amp;E slides and mIF images from Providence. To our knowledge, this is the first large-scale study exploring multimodal AI for scaling virtual mIF generation. The high-quality paired data enabled much more accurate cross-modal translation compared to prior state-of-the-art methods (Figure 2).&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-enables-population-scale-discovery-of-associations-between-cell-states-and-key-biomarkers"&gt;Virtual population enables population-scale discovery of associations between cell states and key biomarkers&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3. GigaTIME identifies novel TIME protein vs biomarker associations at pan-cancer, cancer type, cancer subtype levels. A, GigaTIME generates a virtual population of 14,256 with virtual mIF by translating available H&amp;amp;E images to mIF images, enabling pan-cancer, cancer type, and cancer subtype levels of biomedical discovery. B-G, Correlation analysis between protein channels in virtual mIF and patient biomarkers reveal TIME protein-biomarker associations at pan-cancer level (B), cancer-type level (C-E), and cancer-subtype level (F,G). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation occurs. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. H, A case study showcasing the activation maps across different virtual mIF channels for a H&amp;amp;E slide in our virtual population, and virtual mIF of sample patches from this slide." class="wp-image-1153565" height="1747" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure3.png" width="1257" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. GigaTIME identifies novel TIME protein vs biomarker associations at pan-cancer, cancer type, cancer subtype levels. A, GigaTIME generates a virtual population of 14,256 with virtual mIF by translating available H&amp;amp;E images to mIF images, enabling pan-cancer, cancer type, and cancer subtype levels of biomedical discovery. B-G, Correlation analysis between protein channels in virtual mIF and patient biomarkers reveal TIME protein-biomarker associations at pan-cancer level (B), cancer-type level (C-E), and cancer-subtype level (F,G). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation occurs. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. H, A case study showcasing the activation maps across different virtual mIF channels for a H&amp;amp;E slide in our virtual population, and virtual mIF of sample patches from this slide.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;By applying GigaTIME to Providence real-world data, we generated a virtual population of 14,256 patients with virtual mIF and key clinical attributes. After correcting for multiple hypothesis testing, we have identified 1,234 statistically significant associations between tumor immune cell states (CD138, CD20, CD4) and clinical biomarkers (tumor mutation burden, KRAS, KMT2D), from pan-cancer to cancer subtypes (Figure 3). Many of these findings are supported by existing literature. For example, MSI high and TMB high associated with increased activations of TIME-related channels such as CD138. Additionally, the virtual population also uncovered previously unknown associations, such as pan-cancer associations between immune activations and key tumor biomarkers, such as the tumor suppressor KMT2D and the oncogene KRAS).&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-enables-population-scale-discovery-of-tumor-immune-signatures-for-patient-stratification"&gt;Virtual population enables population-scale discovery of tumor immune signatures for patient stratification&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4. GigaTIME enables effective patient stratification across pathological stages and survival groups. A-C, Correlation analysis between virtual mIF and pathological stages at pan-cancer level (A), cancer-type level (B), and cancer-subtype level (C). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation happens. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D-F, Survival analysis on lung cancer by using virtual CD3, virtual CD8, and virtual GigaTIME signature (all 21 GigaTIME protein channels) to stratify patients at pan-cancer level (D) and cancer-type level: lung (E), brain (F). G, Bar plot comparing pan-cancer patient stratification performance in terms of survival log rank p-values among virtual GigaTIME signature and individual virtual protein channels." class="wp-image-1153566" height="1733" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure4.png" width="1654" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. GigaTIME enables effective patient stratification across pathological stages and survival groups. A-C, Correlation analysis between virtual mIF and pathological stages at pan-cancer level (A), cancer-type level (B), and cancer-subtype level (C). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation happens. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D-F, Survival analysis on lung cancer by using virtual CD3, virtual CD8, and virtual GigaTIME signature (all 21 GigaTIME protein channels) to stratify patients at pan-cancer level (D) and cancer-type level: lung (E), brain (F). G, Bar plot comparing pan-cancer patient stratification performance in terms of survival log rank p-values among virtual GigaTIME signature and individual virtual protein channels.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The virtual population also uncovered&amp;nbsp;GigaTIME&amp;nbsp;signatures for&amp;nbsp;effective&amp;nbsp;patient stratification&amp;nbsp;across staging and survival profiles&amp;nbsp;(Figure 4), from pan-cancer to cancer subtypes.&amp;nbsp;Prior studies have&amp;nbsp;explored patient stratification based on&amp;nbsp;individual&amp;nbsp;immune&amp;nbsp;proteins&amp;nbsp;such as CD3 and CD8. We found that GigaTIME-simulated CD3 and CD8 are similarly effective. Moreover, the combined GigaTIME signature across all 21 protein channels attained even better patient stratification compared to individual channels.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-uncovers-interesting-spatial-and-combinatorial-interactions"&gt;Virtual population uncovers interesting spatial and combinatorial interactions&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5. GigaTIME uncovers interesting spatial and combinatorial virtual mIF patterns. A,B,C Bar plots comparing virtual mIF activation density with spatial metrics on identifying TIME protein-biomarker correlations. We investigated three spatial metrics based on entropy (A), signal-to-noise ratio (SNR) (B), and sharpness (C). D,E, Bar plots comparing single-channel and combinatorial-channel (using the OR logical operation) in biomarker associations for two GigaTIME virtual protein pairs: CD138/CD68 (D) and PD-L1/Caspase 3 (E), demonstrating substantially improved associations for the combination. F, Case studies visualizing the virtual mIF activation maps of individual channels (CD138, CD68; PD-L1, Caspase 3) and their combinations." class="wp-image-1153567" height="1606" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure5.png" width="1249" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. GigaTIME uncovers interesting spatial and combinatorial virtual mIF patterns. A,B,C Bar plots comparing virtual mIF activation density with spatial metrics on identifying TIME protein-biomarker correlations. We investigated three spatial metrics based on entropy (A), signal-to-noise ratio (SNR) (B), and sharpness (C). D,E, Bar plots comparing single-channel and combinatorial-channel (using the OR logical operation) in biomarker associations for two GigaTIME virtual protein pairs: CD138/CD68 (D) and PD-L1/Caspase 3 (E), demonstrating substantially improved associations for the combination. F, Case studies visualizing the virtual mIF activation maps of individual channels (CD138, CD68; PD-L1, Caspase 3) and their combinations.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The virtual population uncovered interesting non-linear interactions across the GigaTIME virtual protein channels, revealing associations with spatial features such as sharpness and entropy, as well as with key clinical biomarkers like APC and KMT2D (Figure 6). Such combinatorial studies were previously out of reach given the scarcity of mIF data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="independent-external-validation-on-tcga"&gt;Independent external validation on TCGA&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6. Independent validation on a virtual population from TCGA. A, Grid charts showing significantly correlated pan-cancer GigaTIME protein-biomarker pairs in Providence (left), TCGA (middle), and both (right). B, Grid charts showing significantly correlated GigaTIME protein-biomarker pairs for lung cancer in Providence and TCGA. C, Grid chart showing significantly correlated GigaTIME protein-biomarker pairs for LUAD in Providence. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D, Case studies with visualizations of H&amp;amp;E slides and the corresponding virtual mIF activations for the pair of a GigaTIME protein channel and a biomarker (mutated/non-mutated), where the patient with the given mutation demonstrates much higher activation scores for that GigaTIME protein channel." class="wp-image-1153568" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure6-1-scaled.png" width="1977" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Independent validation on a virtual population from TCGA. A, Grid charts showing significantly correlated pan-cancer GigaTIME protein-biomarker pairs in Providence (left), TCGA (middle), and both (right). B, Grid charts showing significantly correlated GigaTIME protein-biomarker pairs for lung cancer in Providence and TCGA. C, Grid chart showing significantly correlated GigaTIME protein-biomarker pairs for LUAD in Providence. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D, Case studies with visualizations of H&amp;amp;E slides and the corresponding virtual mIF activations for the pair of a GigaTIME protein channel and a biomarker (mutated/non-mutated), where the patient with the given mutation demonstrates much higher activation scores for that GigaTIME protein channel.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;We conducted an independent external validation by applying GigaTIME to 10,200 patients in The Cancer Genome Atlas (TCGA) dataset and studied associations between GigaTIME-simulated virtual mIF and clinical biomarkers available in TCGA. We observed significant concordance across the virtual populations from Providence and TCGA, with a Spearman correlation of 0.88 for virtual protein activations across cancer subtypes. The two populations also uncovered a significant overlap of associations between GigaTIME-simulated protein activations and clinical biomarkers (Fisher’s exact test p  2 × 10−9). On the other hand, the Providence virtual population yielded 33% more significant associations than TCGA, highlighting the value of large and diverse real-world data for clinical discovery.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="gigatime-is-a-promising-step-toward-the-moonshot-of-virtual-patient"&gt;GigaTIME is a promising step toward the moonshot of “virtual patient”&lt;/h2&gt;



&lt;p&gt;By learning to translate across modalities, GigaTIME is a promising step toward “learning the language of patients” for the ultimate goal of developing a “virtual patient”, a high-fidelity digital twin that could one day accurately forecast disease progression and counterfactual treatment response. By converting routinely available cell morphology data into otherwise scarce high-resolution cell states signals, GigaTIME demonstrated the potential in harnessing multimodal AI to scale real-world evidence (RWE) generation.&lt;/p&gt;



&lt;p&gt;Going forward, growth opportunities abound. GigaTIME can be extended to handle more spatial modalities and cell-state channels. It can be integrated into advanced multimodal frameworks such as LLaVA-Med to facilitate conversational image analysis by “talking to the data.” To facilitate research in tumor microenvironment modeling, we have made GigaTIME open-source&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; on Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Hugging Face&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;GigaTIME is a joint work with Providence and the University of Washington’s Paul G. Allen School of Computer Science &amp;amp; Engineering. It reflects Microsoft’s larger commitment to advancing multimodal generative AI for precision health&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, with other exciting progress such as GigaPath, BiomedCLIP, LLaVA-Rad&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, BiomedJourney, BiomedParse, TrialScope, Curiosity.&lt;/p&gt;







&lt;p&gt;Paper co-authors: &lt;em&gt;Jeya Maria Jose Valanarasu, Hanwen Xu, Naoto Usuyama, Chanwoo Kim, Cliff Wong, Peniel Argaw, Racheli Ben Shimol, Angela Crabtree, Kevin Matlock, Alexandra Q. Bartlett, Jaspreet Bagga, Yu Gu, Sheng Zhang, Tristan Naumann, Bernard A. Fox, Bill Wright, Ari Robicsek, Brian Piening, Carlo Bifulco, Sheng Wang, Hoifung Poon&lt;/em&gt;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustration of a person kneeling and looking through a large telescope on a textured pink surface. The telescope is aimed at a dark sky filled with colorful circular icons, each containing abstract shapes. The telescope itself has circuit-like patterns inside." class="wp-image-1156682" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/LANDSCAPE-INMUNE-MAPS_holasoyka.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;The convergence of digital transformation and the GenAI revolution creates an unprecedented opportunity for accelerating progress in precision health. Precision immunotherapy is a poster child for this transformation. Emerging technologies such as multiplex immunofluorescence (mIF) can assess internal states of individual cells along with their spatial locations, which is critical for deciphering how tumors interact with the immune system. The resulting insights, often referred to as the “grammar” of the tumor microenvironment, can help predict whether a tumor will respond to immunotherapy. If it is unlikely to respond, these insights can also inform strategies to reprogram the tumor from “cold” to “hot,” increasing its susceptibility to treatment.&lt;/p&gt;



&lt;p&gt;This is exciting, but progress is hindered by the high cost and limited scalability of current technology. For example, obtaining mIF data of a couple dozen protein channels for a tissue sample can cost thousands of dollars, and even the most advanced labs can barely scale it to a tiny fraction of their available tissue samples.&lt;/p&gt;



&lt;p&gt;In our paper published in Cell on&amp;nbsp;December&amp;nbsp;9, “Multimodal AI generates virtual population for tumor microenvironment modeling&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,” we present&amp;nbsp;GigaTIME&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a&amp;nbsp;multimodal AI&amp;nbsp;model&amp;nbsp;for translating routinely available hematoxylin and eosin (H&amp;amp;E)&amp;nbsp;pathology slides&amp;nbsp;to virtual&amp;nbsp;mIF&amp;nbsp;images. Developed in collaboration with Providence and the University of Washington, GigaTIME was trained on a Providence dataset of 40 million cells with paired H&amp;amp;E and mIF images across 21 protein channels. We applied&amp;nbsp;GigaTIME&amp;nbsp;to&amp;nbsp;14,256&amp;nbsp;cancer&amp;nbsp;patients from 51 hospitals and over a thousand clinics within the Providence system. This effort generated a virtual population of around 300,000 mIF images spanning 24 cancer types and 306 cancer subtypes. This virtual population uncovered&amp;nbsp;1,234 statistically significant&amp;nbsp;associations linking&amp;nbsp;mIF&amp;nbsp;protein activations with key clinical attributes such as biomarkers, staging,&amp;nbsp;and patient&amp;nbsp;survival.&amp;nbsp;Independent external validation&amp;nbsp;on&amp;nbsp;10,200&amp;nbsp;Cancer Genome Atlas (TCGA) patients further corroborated our findings.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To our knowledge, this is the first population-scale study of tumor immune microenvironment (TIME) based on spatial proteomics. Such studies were previously infeasible due to the scarcity of mIF data. By translating readily available H&amp;amp;E pathology slides into high-resolution virtual mIF data, GigaTIME provides a novel research framework for exploring precision immuno-oncology through population-scale TIME analysis and discovery. We have made our GigaTIME model publicly available at Microsoft Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and on Hugging Face&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to help accelerate clinical research in precision oncology.&lt;/p&gt;



&lt;p&gt;“GigaTIME&amp;nbsp;is about unlocking insights that were previously out of reach,” explained Carlo Bifulco, MD, chief medical officer of Providence Genomics and medical director of cancer genomics and precision oncology at the Providence Cancer Institute. “By analyzing the tumor microenvironment of thousands of patients,&amp;nbsp;GigaTIME&amp;nbsp;has the potential to accelerate discoveries that will shape the future of precision oncology and improve patient outcomes.”&amp;nbsp;&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Microsoft research newsletter&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Newsletter&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-newsletter"&gt;Stay connected to the research community at Microsoft.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="gigatime-generates-a-virtual-population-for-tumor-microenvironment-modeling"&gt;GigaTIME generates a virtual population for tumor microenvironment modeling&lt;/h2&gt;



&lt;p&gt;Digital pathology transforms a microscopy slide of stained tumor tissue into a high-resolution digital image, revealing details of cell morphology such as nucleus and cytoplasm. Such a slide only costs $5 to $10 per image and has become routinely available in cancer care. It is well known that H&amp;amp;E-based cell morphology contains information about the cellular states. Last year, we released GigaPath, the first digital pathology foundation model for scaling transformer architectures to gigapixel H&amp;amp;E slides. Afterward, researchers at Mount Sinai Hospital and Memorial Sloan Kettering Cancer Center showed in a global prospective trial that it can reliably predict a key biomarker from H&amp;amp;E slides for precision oncology triaging. However, such prior works are generally limited to average biomarker status across the entire tissue. GigaTIME thus represents a major step forward by learning to predict spatially resolved, single-cell states essential for tumor microenvironment modeling. In turn, this enables us to generate a virtual population of mIF images for large-scale TIME analysis (Figure 1).&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. GigaTIME enables population-scale tumor immune microenvironment (TIME) analysis. A, GigaTIME inputs a hematoxylin and eosin (H&amp;amp;E) whole-slide image and outputs multiplex immunofluorescence (mIF) across 21 protein channels. By applying GigaTIME to 14,256 patients, we generated a virtual population with mIF information, leading to population-scale discovery on clinical biomarkers and patient stratification, with independent validation on TCGA. B, Circular plot visualizing a TIME spectrum encompassing the GigaTIME-translated virtual mIF activation scores across different protein channels at the population scale, where each channel is represented as an individual circular bar chart segment. The inner circle encodes OncoTree, which classifies 14,256 patients into 306 subtypes across 24 cancer types. The outer circle groups these activations by cancer types, allowing visual comparison across major categories. C, Scatter plot comparing the subtype-level GigaTIME-translated virtual mIF activations between TCGA and Providence virtual populations. Each dot denotes the average activation score of a protein channel among all tumors of a cancer subtype." class="wp-image-1153563" height="1518" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure1.png" width="1265" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. GigaTIME enables population-scale tumor immune microenvironment (TIME) analysis. A, GigaTIME inputs a hematoxylin and eosin (H&amp;amp;E) whole-slide image and outputs multiplex immunofluorescence (mIF) across 21 protein channels. By applying GigaTIME to 14,256 patients, we generated a virtual population with mIF information, leading to population-scale discovery on clinical biomarkers and patient stratification, with independent validation on TCGA. B, Circular plot visualizing a TIME spectrum encompassing the GigaTIME-translated virtual mIF activation scores across different protein channels at the population scale, where each channel is represented as an individual circular bar chart segment. The inner circle encodes OncoTree, which classifies 14,256 patients into 306 subtypes across 24 cancer types. The outer circle groups these activations by cancer types, allowing visual comparison across major categories. C, Scatter plot comparing the subtype-level GigaTIME-translated virtual mIF activations between TCGA and Providence virtual populations. Each dot denotes the average activation score of a protein channel among all tumors of a cancer subtype.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="gigatime-learns-a-multimodal-ai-model-to-translate-pathology-slides-into-spatial-proteomics-images-bridging-cell-morphology-and-cell-states"&gt;GigaTIME learns a multimodal AI model to translate pathology slides into spatial proteomics images, bridging cell morphology and cell states&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. GigaTIME enables translation from hematoxylin and eosin (H&amp;amp;E) to multiplex immunofluorescence (mIF) images. A,B, Bar plot comparing GigaTIME and CycleGAN on the translation performance in terms of Dice score (A) and Pearson correlation (B). C, Scatter plots comparing the activation density of the translated mIF and the ground truth mIF across four channels. D, Qualitative results for a sample H&amp;amp;E whole-slide image from our held-out test set with zoomed-in visualizations of the measured mIF and GigaTIME-translated mIF for DAPI, PD-L1, and CD68 channels." class="wp-image-1153564" height="1418" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure2.png" width="1241" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. GigaTIME enables translation from hematoxylin and eosin (H&amp;amp;E) to multiplex immunofluorescence (mIF) images. A,B, Bar plot comparing GigaTIME and CycleGAN on the translation performance in terms of Dice score (A) and Pearson correlation (B). C, Scatter plots comparing the activation density of the translated mIF and the ground truth mIF across four channels. D, Qualitative results for a sample H&amp;amp;E whole-slide image from our held-out test set with zoomed-in visualizations of the measured mIF and GigaTIME-translated mIF for DAPI, PD-L1, and CD68 channels.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;GigaTIME learned a cross-modal AI translator from digital pathology to spatial multiplex proteomics by training on 40 million cells with paired H&amp;amp;E slides and mIF images from Providence. To our knowledge, this is the first large-scale study exploring multimodal AI for scaling virtual mIF generation. The high-quality paired data enabled much more accurate cross-modal translation compared to prior state-of-the-art methods (Figure 2).&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-enables-population-scale-discovery-of-associations-between-cell-states-and-key-biomarkers"&gt;Virtual population enables population-scale discovery of associations between cell states and key biomarkers&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3. GigaTIME identifies novel TIME protein vs biomarker associations at pan-cancer, cancer type, cancer subtype levels. A, GigaTIME generates a virtual population of 14,256 with virtual mIF by translating available H&amp;amp;E images to mIF images, enabling pan-cancer, cancer type, and cancer subtype levels of biomedical discovery. B-G, Correlation analysis between protein channels in virtual mIF and patient biomarkers reveal TIME protein-biomarker associations at pan-cancer level (B), cancer-type level (C-E), and cancer-subtype level (F,G). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation occurs. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. H, A case study showcasing the activation maps across different virtual mIF channels for a H&amp;amp;E slide in our virtual population, and virtual mIF of sample patches from this slide." class="wp-image-1153565" height="1747" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure3.png" width="1257" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. GigaTIME identifies novel TIME protein vs biomarker associations at pan-cancer, cancer type, cancer subtype levels. A, GigaTIME generates a virtual population of 14,256 with virtual mIF by translating available H&amp;amp;E images to mIF images, enabling pan-cancer, cancer type, and cancer subtype levels of biomedical discovery. B-G, Correlation analysis between protein channels in virtual mIF and patient biomarkers reveal TIME protein-biomarker associations at pan-cancer level (B), cancer-type level (C-E), and cancer-subtype level (F,G). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation occurs. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. H, A case study showcasing the activation maps across different virtual mIF channels for a H&amp;amp;E slide in our virtual population, and virtual mIF of sample patches from this slide.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;By applying GigaTIME to Providence real-world data, we generated a virtual population of 14,256 patients with virtual mIF and key clinical attributes. After correcting for multiple hypothesis testing, we have identified 1,234 statistically significant associations between tumor immune cell states (CD138, CD20, CD4) and clinical biomarkers (tumor mutation burden, KRAS, KMT2D), from pan-cancer to cancer subtypes (Figure 3). Many of these findings are supported by existing literature. For example, MSI high and TMB high associated with increased activations of TIME-related channels such as CD138. Additionally, the virtual population also uncovered previously unknown associations, such as pan-cancer associations between immune activations and key tumor biomarkers, such as the tumor suppressor KMT2D and the oncogene KRAS).&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-enables-population-scale-discovery-of-tumor-immune-signatures-for-patient-stratification"&gt;Virtual population enables population-scale discovery of tumor immune signatures for patient stratification&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4. GigaTIME enables effective patient stratification across pathological stages and survival groups. A-C, Correlation analysis between virtual mIF and pathological stages at pan-cancer level (A), cancer-type level (B), and cancer-subtype level (C). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation happens. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D-F, Survival analysis on lung cancer by using virtual CD3, virtual CD8, and virtual GigaTIME signature (all 21 GigaTIME protein channels) to stratify patients at pan-cancer level (D) and cancer-type level: lung (E), brain (F). G, Bar plot comparing pan-cancer patient stratification performance in terms of survival log rank p-values among virtual GigaTIME signature and individual virtual protein channels." class="wp-image-1153566" height="1733" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure4.png" width="1654" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. GigaTIME enables effective patient stratification across pathological stages and survival groups. A-C, Correlation analysis between virtual mIF and pathological stages at pan-cancer level (A), cancer-type level (B), and cancer-subtype level (C). Circle size denotes significance strength. Circle color denotes the directionality in which the correlation happens. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D-F, Survival analysis on lung cancer by using virtual CD3, virtual CD8, and virtual GigaTIME signature (all 21 GigaTIME protein channels) to stratify patients at pan-cancer level (D) and cancer-type level: lung (E), brain (F). G, Bar plot comparing pan-cancer patient stratification performance in terms of survival log rank p-values among virtual GigaTIME signature and individual virtual protein channels.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The virtual population also uncovered&amp;nbsp;GigaTIME&amp;nbsp;signatures for&amp;nbsp;effective&amp;nbsp;patient stratification&amp;nbsp;across staging and survival profiles&amp;nbsp;(Figure 4), from pan-cancer to cancer subtypes.&amp;nbsp;Prior studies have&amp;nbsp;explored patient stratification based on&amp;nbsp;individual&amp;nbsp;immune&amp;nbsp;proteins&amp;nbsp;such as CD3 and CD8. We found that GigaTIME-simulated CD3 and CD8 are similarly effective. Moreover, the combined GigaTIME signature across all 21 protein channels attained even better patient stratification compared to individual channels.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="virtual-population-uncovers-interesting-spatial-and-combinatorial-interactions"&gt;Virtual population uncovers interesting spatial and combinatorial interactions&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5. GigaTIME uncovers interesting spatial and combinatorial virtual mIF patterns. A,B,C Bar plots comparing virtual mIF activation density with spatial metrics on identifying TIME protein-biomarker correlations. We investigated three spatial metrics based on entropy (A), signal-to-noise ratio (SNR) (B), and sharpness (C). D,E, Bar plots comparing single-channel and combinatorial-channel (using the OR logical operation) in biomarker associations for two GigaTIME virtual protein pairs: CD138/CD68 (D) and PD-L1/Caspase 3 (E), demonstrating substantially improved associations for the combination. F, Case studies visualizing the virtual mIF activation maps of individual channels (CD138, CD68; PD-L1, Caspase 3) and their combinations." class="wp-image-1153567" height="1606" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure5.png" width="1249" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. GigaTIME uncovers interesting spatial and combinatorial virtual mIF patterns. A,B,C Bar plots comparing virtual mIF activation density with spatial metrics on identifying TIME protein-biomarker correlations. We investigated three spatial metrics based on entropy (A), signal-to-noise ratio (SNR) (B), and sharpness (C). D,E, Bar plots comparing single-channel and combinatorial-channel (using the OR logical operation) in biomarker associations for two GigaTIME virtual protein pairs: CD138/CD68 (D) and PD-L1/Caspase 3 (E), demonstrating substantially improved associations for the combination. F, Case studies visualizing the virtual mIF activation maps of individual channels (CD138, CD68; PD-L1, Caspase 3) and their combinations.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The virtual population uncovered interesting non-linear interactions across the GigaTIME virtual protein channels, revealing associations with spatial features such as sharpness and entropy, as well as with key clinical biomarkers like APC and KMT2D (Figure 6). Such combinatorial studies were previously out of reach given the scarcity of mIF data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="independent-external-validation-on-tcga"&gt;Independent external validation on TCGA&lt;/h2&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6. Independent validation on a virtual population from TCGA. A, Grid charts showing significantly correlated pan-cancer GigaTIME protein-biomarker pairs in Providence (left), TCGA (middle), and both (right). B, Grid charts showing significantly correlated GigaTIME protein-biomarker pairs for lung cancer in Providence and TCGA. C, Grid chart showing significantly correlated GigaTIME protein-biomarker pairs for LUAD in Providence. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D, Case studies with visualizations of H&amp;amp;E slides and the corresponding virtual mIF activations for the pair of a GigaTIME protein channel and a biomarker (mutated/non-mutated), where the patient with the given mutation demonstrates much higher activation scores for that GigaTIME protein channel." class="wp-image-1153568" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/GigaTIME_Figure6-1-scaled.png" width="1977" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Independent validation on a virtual population from TCGA. A, Grid charts showing significantly correlated pan-cancer GigaTIME protein-biomarker pairs in Providence (left), TCGA (middle), and both (right). B, Grid charts showing significantly correlated GigaTIME protein-biomarker pairs for lung cancer in Providence and TCGA. C, Grid chart showing significantly correlated GigaTIME protein-biomarker pairs for LUAD in Providence. Channel color denotes high, medium, and low confidence based on pearson correlations evaluated using test set. D, Case studies with visualizations of H&amp;amp;E slides and the corresponding virtual mIF activations for the pair of a GigaTIME protein channel and a biomarker (mutated/non-mutated), where the patient with the given mutation demonstrates much higher activation scores for that GigaTIME protein channel.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;We conducted an independent external validation by applying GigaTIME to 10,200 patients in The Cancer Genome Atlas (TCGA) dataset and studied associations between GigaTIME-simulated virtual mIF and clinical biomarkers available in TCGA. We observed significant concordance across the virtual populations from Providence and TCGA, with a Spearman correlation of 0.88 for virtual protein activations across cancer subtypes. The two populations also uncovered a significant overlap of associations between GigaTIME-simulated protein activations and clinical biomarkers (Fisher’s exact test p  2 × 10−9). On the other hand, the Providence virtual population yielded 33% more significant associations than TCGA, highlighting the value of large and diverse real-world data for clinical discovery.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="gigatime-is-a-promising-step-toward-the-moonshot-of-virtual-patient"&gt;GigaTIME is a promising step toward the moonshot of “virtual patient”&lt;/h2&gt;



&lt;p&gt;By learning to translate across modalities, GigaTIME is a promising step toward “learning the language of patients” for the ultimate goal of developing a “virtual patient”, a high-fidelity digital twin that could one day accurately forecast disease progression and counterfactual treatment response. By converting routinely available cell morphology data into otherwise scarce high-resolution cell states signals, GigaTIME demonstrated the potential in harnessing multimodal AI to scale real-world evidence (RWE) generation.&lt;/p&gt;



&lt;p&gt;Going forward, growth opportunities abound. GigaTIME can be extended to handle more spatial modalities and cell-state channels. It can be integrated into advanced multimodal frameworks such as LLaVA-Med to facilitate conversational image analysis by “talking to the data.” To facilitate research in tumor microenvironment modeling, we have made GigaTIME open-source&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; on Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Hugging Face&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;GigaTIME is a joint work with Providence and the University of Washington’s Paul G. Allen School of Computer Science &amp;amp; Engineering. It reflects Microsoft’s larger commitment to advancing multimodal generative AI for precision health&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, with other exciting progress such as GigaPath, BiomedCLIP, LLaVA-Rad&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, BiomedJourney, BiomedParse, TrialScope, Curiosity.&lt;/p&gt;







&lt;p&gt;Paper co-authors: &lt;em&gt;Jeya Maria Jose Valanarasu, Hanwen Xu, Naoto Usuyama, Chanwoo Kim, Cliff Wong, Peniel Argaw, Racheli Ben Shimol, Angela Crabtree, Kevin Matlock, Alexandra Q. Bartlett, Jaspreet Bagga, Yu Gu, Sheng Zhang, Tristan Naumann, Bernard A. Fox, Bill Wright, Ari Robicsek, Brian Piening, Carlo Bifulco, Sheng Wang, Hoifung Poon&lt;/em&gt;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/gigatime-scaling-tumor-microenvironment-modeling-using-virtual-population-generated-by-multimodal-ai/</guid><pubDate>Tue, 09 Dec 2025 16:00:00 +0000</pubDate></item><item><title>Accenture and Anthropic partner to boost enterprise AI integration (AI News)</title><link>https://www.artificialintelligence-news.com/news/accenture-anthropic-partner-boost-enterprise-ai-integration/</link><description>&lt;p&gt;Accenture and Anthropic are setting out to boost enterprise AI integration with a newly-expanded partnership.&lt;/p&gt;&lt;p&gt;While 2024 was defined by corporate curiosity regarding Large Language Models (LLMs), the current mandate for business leaders is operationalising these tools to achieve a return on investment.&lt;/p&gt;&lt;p&gt;The new Accenture Anthropic Business Group combines Anthropic’s model capabilities with Accenture’s implementation machinery to industrialise the deployment of generative AI across regulated sectors.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-industrialising-the-developer-workflow"&gt;Industrialising the developer workflow&lt;/h3&gt;&lt;p&gt;A primary component of this collaboration focuses on software engineering. Coding assistance is often seen as the path of least resistance for AI adoption, yet integrating these tools into existing CI/CD pipelines remains complex.&lt;/p&gt;&lt;p&gt;Accenture is positioning itself as a primary partner for Claude Code, Anthropic’s coding tool, which the company claims now holds over half of the AI coding market. The consultancy plans to train approximately 30,000 of its own professionals on Claude, creating one of the largest global ecosystems of practitioners familiar with the tool.&lt;/p&gt;&lt;p&gt;The promise of deeper enterprise integration of AI coding tools is a complete restructuring of the development hierarchy. The joint offering suggests that junior developers can utilise these tools to produce senior-level code and complete integration tasks more quickly to reduce onboarding times from months to weeks. Senior developers can then concentrate on high-value architecture, validation, and oversight.&lt;/p&gt;&lt;p&gt;Dario Amodei, CEO and Co-Founder of Anthropic, said: “AI is changing how almost everyone works, and enterprises need both cutting-edge AI and trusted expertise to deploy it at scale. Accenture brings deep enterprise transformation experience, and Anthropic brings the most capable models.&lt;/p&gt;&lt;p&gt;“Our new partnership means that tens of thousands of Accenture developers will be using Claude Code, making this our largest ever deployment—and the new Accenture Anthropic Business Group will help enterprise clients use our smartest AI models to make major productivity gains.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-justifying-ai-inference-costs-and-removing-deployment-barriers"&gt;Justifying AI inference costs and removing deployment barriers&lt;/h3&gt;&lt;p&gt;A persistent friction point for enterprise leaders seeking deeper AI integration is justifying the ongoing cost of inference against actual business value. To counter this, the partnership is launching a specific product designed to help CIOs measure value and drive adoption across engineering organisations.&lt;/p&gt;&lt;p&gt;This offering attempts to provide a structured path for software design and maintenance, moving beyond the ad-hoc usage of coding assistants. It combines Claude Code with a framework for quantifying productivity gains and workflow redesigns tailored for AI-first development teams.&lt;/p&gt;&lt;p&gt;For the enterprise, the goal is to translate individual developer efficiency into broader company impact; such as shorter development cycles and faster time-to-market for new products.&lt;/p&gt;&lt;p&gt;However, the most substantial barrier to AI adoption in the Global 2000 remains compliance. Sectors such as financial services, healthcare, and the public sector face strict governance requirements that often stall AI initiatives.&lt;/p&gt;&lt;p&gt;Accenture and Anthropic are developing industry-specific enterprise AI solutions to address these deployment challenges. In financial services, for instance, the focus is on automating compliance workflows and processing complex documents with the precision required for high-stakes decisions.&lt;/p&gt;&lt;p&gt;Health and life sciences firms face a parallel demand. Here, the partnership aims to leverage Claude’s analytical capabilities to query proprietary datasets and streamline clinical trial processing. For the public sector, the utility lies in AI agents that assist citizens in navigating government services while adhering to statutory data privacy requirements.&lt;/p&gt;&lt;p&gt;Julie Sweet, Chair and CEO of Accenture, commented: “With the powerful combination of Anthropic’s Claude capabilities and Accenture’s AI expertise and industry and function domain knowledge, organisations can embed AI everywhere responsibly and at speed – from software development to customer experience – to drive innovation, unlock new sources of growth, and build their confidence to lead in the age of AI.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-accenture-and-anthropic-are-mitigating-risks-to-support-enterprise-ai-integration"&gt;How Accenture and Anthropic are mitigating risks to support enterprise AI integration&lt;/h3&gt;&lt;p&gt;To mitigate the risks associated with deploying non-deterministic models, the partnership emphasises “responsible AI.” This involves combining Anthropic’s “constitutional AI” principles – which embed safety rules directly into the model – with Accenture’s governance expertise.&lt;/p&gt;&lt;p&gt;Practical implementation will occur through Accenture’s network of Innovation Hubs, which will serve as controlled environments or “sandboxes”. These hubs allow clients to prototype and validate solutions without exposing production systems or sensitive data to risk. The companies also plan to co-invest in a ‘Claude Center of Excellence’ to design bespoke AI offerings tailored to specific industry needs.&lt;/p&gt;&lt;p&gt;This expanded partnership with Accenture follows Anthropic reporting a growth in its enterprise AI market share from 24 percent to 40 percent. For Accenture, establishing a dedicated business group with specific go-to-market focus reflects a long-term commitment to the platform.&lt;/p&gt;&lt;p&gt;The era of standalone AI pilots is fading. The next phase for enterprise AI integration demands tight coupling between model capabilities, workforce training, and rigorous value measurement.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI targets AI skills gap with new certification standards&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111183" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Accenture and Anthropic are setting out to boost enterprise AI integration with a newly-expanded partnership.&lt;/p&gt;&lt;p&gt;While 2024 was defined by corporate curiosity regarding Large Language Models (LLMs), the current mandate for business leaders is operationalising these tools to achieve a return on investment.&lt;/p&gt;&lt;p&gt;The new Accenture Anthropic Business Group combines Anthropic’s model capabilities with Accenture’s implementation machinery to industrialise the deployment of generative AI across regulated sectors.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-industrialising-the-developer-workflow"&gt;Industrialising the developer workflow&lt;/h3&gt;&lt;p&gt;A primary component of this collaboration focuses on software engineering. Coding assistance is often seen as the path of least resistance for AI adoption, yet integrating these tools into existing CI/CD pipelines remains complex.&lt;/p&gt;&lt;p&gt;Accenture is positioning itself as a primary partner for Claude Code, Anthropic’s coding tool, which the company claims now holds over half of the AI coding market. The consultancy plans to train approximately 30,000 of its own professionals on Claude, creating one of the largest global ecosystems of practitioners familiar with the tool.&lt;/p&gt;&lt;p&gt;The promise of deeper enterprise integration of AI coding tools is a complete restructuring of the development hierarchy. The joint offering suggests that junior developers can utilise these tools to produce senior-level code and complete integration tasks more quickly to reduce onboarding times from months to weeks. Senior developers can then concentrate on high-value architecture, validation, and oversight.&lt;/p&gt;&lt;p&gt;Dario Amodei, CEO and Co-Founder of Anthropic, said: “AI is changing how almost everyone works, and enterprises need both cutting-edge AI and trusted expertise to deploy it at scale. Accenture brings deep enterprise transformation experience, and Anthropic brings the most capable models.&lt;/p&gt;&lt;p&gt;“Our new partnership means that tens of thousands of Accenture developers will be using Claude Code, making this our largest ever deployment—and the new Accenture Anthropic Business Group will help enterprise clients use our smartest AI models to make major productivity gains.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-justifying-ai-inference-costs-and-removing-deployment-barriers"&gt;Justifying AI inference costs and removing deployment barriers&lt;/h3&gt;&lt;p&gt;A persistent friction point for enterprise leaders seeking deeper AI integration is justifying the ongoing cost of inference against actual business value. To counter this, the partnership is launching a specific product designed to help CIOs measure value and drive adoption across engineering organisations.&lt;/p&gt;&lt;p&gt;This offering attempts to provide a structured path for software design and maintenance, moving beyond the ad-hoc usage of coding assistants. It combines Claude Code with a framework for quantifying productivity gains and workflow redesigns tailored for AI-first development teams.&lt;/p&gt;&lt;p&gt;For the enterprise, the goal is to translate individual developer efficiency into broader company impact; such as shorter development cycles and faster time-to-market for new products.&lt;/p&gt;&lt;p&gt;However, the most substantial barrier to AI adoption in the Global 2000 remains compliance. Sectors such as financial services, healthcare, and the public sector face strict governance requirements that often stall AI initiatives.&lt;/p&gt;&lt;p&gt;Accenture and Anthropic are developing industry-specific enterprise AI solutions to address these deployment challenges. In financial services, for instance, the focus is on automating compliance workflows and processing complex documents with the precision required for high-stakes decisions.&lt;/p&gt;&lt;p&gt;Health and life sciences firms face a parallel demand. Here, the partnership aims to leverage Claude’s analytical capabilities to query proprietary datasets and streamline clinical trial processing. For the public sector, the utility lies in AI agents that assist citizens in navigating government services while adhering to statutory data privacy requirements.&lt;/p&gt;&lt;p&gt;Julie Sweet, Chair and CEO of Accenture, commented: “With the powerful combination of Anthropic’s Claude capabilities and Accenture’s AI expertise and industry and function domain knowledge, organisations can embed AI everywhere responsibly and at speed – from software development to customer experience – to drive innovation, unlock new sources of growth, and build their confidence to lead in the age of AI.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-accenture-and-anthropic-are-mitigating-risks-to-support-enterprise-ai-integration"&gt;How Accenture and Anthropic are mitigating risks to support enterprise AI integration&lt;/h3&gt;&lt;p&gt;To mitigate the risks associated with deploying non-deterministic models, the partnership emphasises “responsible AI.” This involves combining Anthropic’s “constitutional AI” principles – which embed safety rules directly into the model – with Accenture’s governance expertise.&lt;/p&gt;&lt;p&gt;Practical implementation will occur through Accenture’s network of Innovation Hubs, which will serve as controlled environments or “sandboxes”. These hubs allow clients to prototype and validate solutions without exposing production systems or sensitive data to risk. The companies also plan to co-invest in a ‘Claude Center of Excellence’ to design bespoke AI offerings tailored to specific industry needs.&lt;/p&gt;&lt;p&gt;This expanded partnership with Accenture follows Anthropic reporting a growth in its enterprise AI market share from 24 percent to 40 percent. For Accenture, establishing a dedicated business group with specific go-to-market focus reflects a long-term commitment to the platform.&lt;/p&gt;&lt;p&gt;The era of standalone AI pilots is fading. The next phase for enterprise AI integration demands tight coupling between model capabilities, workforce training, and rigorous value measurement.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI targets AI skills gap with new certification standards&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111183" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/accenture-anthropic-partner-boost-enterprise-ai-integration/</guid><pubDate>Tue, 09 Dec 2025 16:20:18 +0000</pubDate></item><item><title>[NEW] Amazon’s Ring rolls out controversial, AI-powered facial-recognition feature to video doorbells (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/amazons-ring-rolls-out-controversial-ai-powered-facial-recognition-feature-to-video-doorbells/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Dystopian or useful? Amazon’s Ring doorbells will now be able to identify your visitors through a new AI-powered facial-recognition feature, the company said on Tuesday. The controversial feature, dubbed “Familiar Faces,” was announced earlier this September and is now rolling out to Ring device owners in the United States.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says the feature lets you identify the people who regularly come to your door by creating a catalog of up to 50 faces. These could include family members, friends and neighbors, delivery drivers, household staff, and others. After you label someone in the Ring app, the device will recognize them as they approach the Ring’s camera. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Then, instead of alerting you that “a person is at your door,” you’ll receive a personalized notification, like “Mom at Front Door,” the company explains in its launch announcement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature has already received pushback from consumer protection organizations, like the EFF, and a U.S. senator. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon Ring owners can use the feature to help them disable alerts they don’t want to see — like those notifications referencing their own comings and goings, for instance, the company says. And they can set these alerts on a per-face basis.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is not enabled by default. Instead, users will need to turn it on in their app’s settings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, faces can be named in the app directly from the Event History section or from the new Familiar Faces library. Once labeled, the face will be named in all notifications, in the app’s timeline, and in the Event History. These labels can be edited at any time, and there are tools to merge duplicates or delete faces.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon claims the face data is encrypted and never shared with others. Plus, it says unnamed faces are automatically removed after 30 days. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074302" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/FF-Mockups.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Ring&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-privacy-concerns-over-ai-facial-recognition"&gt;Privacy concerns over AI facial recognition&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Despite Amazon’s privacy assurances, the addition of the feature raises concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has a history of forging partnerships with law enforcement and even once gave police and fire departments the ability to request data from the Ring Neighbors app by asking Amazon directly for people’s doorbell footage. More recently, Amazon partnered with Flock, the maker of AI-powered surveillance cameras used by police, federal law enforcement, and ICE.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ring’s own security efforts have fallen short in the past.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ring had to pay a $5.8 million fine in 2023 after the U.S. Federal Trade Commission found that Ring employees and contractors had broad and unrestricted access to customers’ videos for years. Its Neighbors app also exposed users’ home addresses and precise locations, and users’ Ring passwords have been floating around the dark web for years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given Amazon’s willingness to work with law enforcement and digital surveillance providers, combined with its poor security track record, we’d suggest Ring owners, at the very least, be careful about identifying anyone using their proper name; better yet, keep the feature disabled and just look to see who it is. Not everything needs an AI upgrade.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of the privacy concerns, Amazon’s Ring has already faced calls from U.S. senator Ed Markey (D-Mass.) to abandon this feature, and is facing backlash from consumer protection organizations, like the EFF. Privacy laws are preventing Amazon from launching the feature in Illinois,&amp;nbsp;Texas, and Portland, Oregon, the EFF had also noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to questions posed by the organization, Amazon said the users’ biometric data will be processed in the cloud and claimed it doesn’t use the data to train AI models. It also claimed it wouldn’t be able to identify all the locations where a person had been detected, from a technical standpoint, even if law enforcement requested this data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, it’s unclear why that would not be the case, given the similarity to the “Search Party” feature that looks across a neighborhood’s network of Ring cameras to find lost dogs and cats.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reached for comment, EFF’s Staff Attorney, F. Mario Trujillo, said, “Knocking on a door, or even just walking in front of it, shouldn’t require abandoning your privacy. With this feature going live, it’s more important than ever that state privacy regulators step in to investigate, protect people’s privacy, and test the strength of their biometric privacy laws.” &lt;br /&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated after publication with EFF comment.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Dystopian or useful? Amazon’s Ring doorbells will now be able to identify your visitors through a new AI-powered facial-recognition feature, the company said on Tuesday. The controversial feature, dubbed “Familiar Faces,” was announced earlier this September and is now rolling out to Ring device owners in the United States.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says the feature lets you identify the people who regularly come to your door by creating a catalog of up to 50 faces. These could include family members, friends and neighbors, delivery drivers, household staff, and others. After you label someone in the Ring app, the device will recognize them as they approach the Ring’s camera. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Then, instead of alerting you that “a person is at your door,” you’ll receive a personalized notification, like “Mom at Front Door,” the company explains in its launch announcement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature has already received pushback from consumer protection organizations, like the EFF, and a U.S. senator. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon Ring owners can use the feature to help them disable alerts they don’t want to see — like those notifications referencing their own comings and goings, for instance, the company says. And they can set these alerts on a per-face basis.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is not enabled by default. Instead, users will need to turn it on in their app’s settings. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, faces can be named in the app directly from the Event History section or from the new Familiar Faces library. Once labeled, the face will be named in all notifications, in the app’s timeline, and in the Event History. These labels can be edited at any time, and there are tools to merge duplicates or delete faces.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon claims the face data is encrypted and never shared with others. Plus, it says unnamed faces are automatically removed after 30 days. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3074302" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/FF-Mockups.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Ring&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-privacy-concerns-over-ai-facial-recognition"&gt;Privacy concerns over AI facial recognition&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Despite Amazon’s privacy assurances, the addition of the feature raises concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has a history of forging partnerships with law enforcement and even once gave police and fire departments the ability to request data from the Ring Neighbors app by asking Amazon directly for people’s doorbell footage. More recently, Amazon partnered with Flock, the maker of AI-powered surveillance cameras used by police, federal law enforcement, and ICE.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ring’s own security efforts have fallen short in the past.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ring had to pay a $5.8 million fine in 2023 after the U.S. Federal Trade Commission found that Ring employees and contractors had broad and unrestricted access to customers’ videos for years. Its Neighbors app also exposed users’ home addresses and precise locations, and users’ Ring passwords have been floating around the dark web for years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given Amazon’s willingness to work with law enforcement and digital surveillance providers, combined with its poor security track record, we’d suggest Ring owners, at the very least, be careful about identifying anyone using their proper name; better yet, keep the feature disabled and just look to see who it is. Not everything needs an AI upgrade.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of the privacy concerns, Amazon’s Ring has already faced calls from U.S. senator Ed Markey (D-Mass.) to abandon this feature, and is facing backlash from consumer protection organizations, like the EFF. Privacy laws are preventing Amazon from launching the feature in Illinois,&amp;nbsp;Texas, and Portland, Oregon, the EFF had also noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to questions posed by the organization, Amazon said the users’ biometric data will be processed in the cloud and claimed it doesn’t use the data to train AI models. It also claimed it wouldn’t be able to identify all the locations where a person had been detected, from a technical standpoint, even if law enforcement requested this data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, it’s unclear why that would not be the case, given the similarity to the “Search Party” feature that looks across a neighborhood’s network of Ring cameras to find lost dogs and cats.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reached for comment, EFF’s Staff Attorney, F. Mario Trujillo, said, “Knocking on a door, or even just walking in front of it, shouldn’t require abandoning your privacy. With this feature going live, it’s more important than ever that state privacy regulators step in to investigate, protect people’s privacy, and test the strength of their biometric privacy laws.” &lt;br /&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated after publication with EFF comment.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/amazons-ring-rolls-out-controversial-ai-powered-facial-recognition-feature-to-video-doorbells/</guid><pubDate>Tue, 09 Dec 2025 19:04:08 +0000</pubDate></item><item><title>[NEW] Slack CEO Denise Dresser to join OpenAI as chief revenue officer (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/slack-ceo-denise-dresser-to-join-openai-as-chief-revenue-officer/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/54103320653_6eb7d509ce_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is hiring Slack CEO Denise Dresser as its new chief revenue officer. The news was first reported by Wired, then confirmed by OpenAI in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dresser’s new role comes after more than 14 years at Salesforce, Slack’s parent company. While at Slack, Dresser oversaw the introduction of several AI features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says that Dresser will be responsible for the company’s revenue strategy in enterprise and customer success. That’s a pivotal role, given that the company has a rocky road ahead if it ever wants to turn a profit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re on a path to put AI tools into the hands of millions of workers, across every industry,” Fidji Simo, OpenAI’s CEO of Applications, said in a statement. “Denise has led that kind of shift before, and her experience will help us make AI useful, reliable, and accessible for businesses everywhere.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like Dresser, Simo also joined OpenAI this year after a long track record of high-profile leadership, most recently as CEO of Instacart, which has become a close partner of OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Wired, Slack’s chief product officer, Rob Seaman, will become interim CEO of Slack.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/54103320653_6eb7d509ce_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is hiring Slack CEO Denise Dresser as its new chief revenue officer. The news was first reported by Wired, then confirmed by OpenAI in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dresser’s new role comes after more than 14 years at Salesforce, Slack’s parent company. While at Slack, Dresser oversaw the introduction of several AI features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says that Dresser will be responsible for the company’s revenue strategy in enterprise and customer success. That’s a pivotal role, given that the company has a rocky road ahead if it ever wants to turn a profit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re on a path to put AI tools into the hands of millions of workers, across every industry,” Fidji Simo, OpenAI’s CEO of Applications, said in a statement. “Denise has led that kind of shift before, and her experience will help us make AI useful, reliable, and accessible for businesses everywhere.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like Dresser, Simo also joined OpenAI this year after a long track record of high-profile leadership, most recently as CEO of Instacart, which has become a close partner of OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Wired, Slack’s chief product officer, Rob Seaman, will become interim CEO of Slack.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/slack-ceo-denise-dresser-to-join-openai-as-chief-revenue-officer/</guid><pubDate>Tue, 09 Dec 2025 19:41:40 +0000</pubDate></item><item><title>[NEW] Mistral launches powerful Devstral 2 coding model including open source, laptop-friendly version (AI | VentureBeat)</title><link>https://venturebeat.com/ai/mistral-launches-powerful-devstral-2-coding-model-including-open-source</link><description>[unable to retrieve full-text content]&lt;p&gt;French AI startup Mistral has weathered a rocky period of public questioning over the last year to emerge, now here in December 2025, with new, crowd-pleasing models for enterprise and indie developers.&lt;/p&gt;&lt;p&gt;Just days after releasing its &lt;a href="https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on"&gt;powerful open source, general purpose Mistral 3 LLM family&lt;/a&gt; for edge devices and local hardware, the &lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;&lt;b&gt;company returned today to debut Devstral 2&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The release includes a new pair of models optimized for software engineering tasks — again, with one small enough to run on a single laptop, offline and privately — alongside &lt;b&gt;Mistral Vibe,&lt;/b&gt; a command-line interface (CLI) agent designed to allow developers to call the models up directly within their terminal environments. &lt;/p&gt;&lt;p&gt;The models are fast, lean, and open—at least in theory. But the real story lies not just in the benchmarks, but in how Mistral is packaging this capability: one model fully free, another conditionally so, and a terminal interface built to scale with either.&lt;/p&gt;&lt;p&gt;It’s an attempt not just to match proprietary systems like Claude and GPT-4 in performance, but to compete with them on developer experience—and to do so while holding onto the flag of open-source.&lt;/p&gt;&lt;p&gt;Both models are available now for free for a limited time &lt;a href="https://docs.mistral.ai/models/devstral-2-25-12"&gt;via Mistral’s API&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/mistralai/devstral-2"&gt;Hugging Face&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The full Devstral 2 model is supported out-of-the-box in the community inference provider &lt;a href="https://x.com/vllm_project/status/1998428798891765926?s=20"&gt;vLLM&lt;/a&gt; and on the open source agentic coding platform &lt;a href="https://x.com/kilocode/status/1998412042357588461"&gt;Kilo Code&lt;/a&gt;.  &lt;/p&gt;&lt;h2&gt;&lt;b&gt;A Coding Model Meant to Drive&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At the top of the announcement is Devstral 2, a 123-billion parameter dense transformer with a 256K-token context window, engineered specifically for agentic software development. &lt;/p&gt;&lt;p&gt;Mistral says the model achieves 72.2% on SWE-bench Verified, a benchmark designed to evaluate long-context software engineering tasks in real-world repositories.&lt;/p&gt;&lt;p&gt;The smaller sibling, Devstral Small 2, weighs in at 24B parameters, with the same long context window and a performance of 68.0% on SWE-bench. &lt;/p&gt;&lt;p&gt;On paper, that makes it the strongest open-weight model of its size, even outscoring many 70B-class competitors.&lt;/p&gt;&lt;p&gt;But the performance story isn’t just about raw percentages. Mistral is betting that efficient intelligence beats scale, and has made much of the fact that Devstral 2 is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;5× smaller than DeepSeek V3.2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;8× smaller than Kimi K2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Yet still matches or surpasses them on key software reasoning benchmarks.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Human evaluations back this up. In side-by-side comparisons:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Devstral 2 beat DeepSeek V3.2 in 42.8% of tasks, losing only 28.6%.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Against Claude Sonnet 4.5, it lost more often (53.1%)—a reminder that while the gap is narrowing, closed models still lead in overall preference.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Still, for an open-weight model, these results place Devstral 2 at the frontier of what’s currently available to run and modify independently.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Vibe CLI: A Terminal-Native Agent&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Alongside the models, Mistral released Vibe CLI, a command-line assistant that integrates directly with Devstral models. It’s not an IDE plugin or a ChatGPT-style code explainer. It’s a native interface designed for project-wide code understanding and orchestration, built to live inside the developer’s actual workflow.&lt;/p&gt;&lt;p&gt;Vibe brings a surprising degree of intelligence to the terminal:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;It reads your file tree and Git status to understand project scope.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It lets you reference files with @, run shell commands with !, and toggle behavior with slash commands.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It orchestrates changes across multiple files, tracks dependencies, retries failed executions, and can even refactor at architectural scale.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Unlike most developer agents, which simulate a REPL from within a chat UI, Vibe starts with the shell and pulls intelligence in from there. It’s programmable, scriptable, and themeable. And it’s released under the Apache 2.0 license, meaning it’s truly free to use—in commercial settings, internal tools, or open-source extensions.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Licensing Structure: Open-ish — With Revenue Limitations&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At first glance, Mistral’s licensing approach appears straightforward: the models are open-weight and publicly available. But a closer look reveals a line drawn through the middle of the release, with different rules for different users.&lt;/p&gt;&lt;p&gt;Devstral Small 2, the 24-billion parameter variant, is covered under a standard, enterprise- and developer-friendly &lt;a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md"&gt;Apache 2.0 license&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;That’s a gold standard in open-source: no revenue restrictions, no fine print, no need to check with legal. Enterprises can use it in production, embed it into products, and redistribute fine-tuned versions without asking for permission.&lt;/p&gt;&lt;p&gt;Devstral 2, the flagship 123B model, is released under what Mistral calls a “&lt;a href="https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512/blob/main/LICENSE"&gt;modified MIT license&lt;/a&gt;.” That phrase sounds innocuous, but the modification introduces a critical limitation: any company making more than $20 million in monthly revenue cannot use the model at all—not even internally—without securing a separate commercial license from Mistral.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“You are not authorized to exercise any rights under this license if the global consolidated monthly revenue of your company […] exceeds $20 million,” the license reads.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The clause applies not only to the base model, but to derivatives, fine-tuned versions, and redistributed variants, regardless of who hosts them. In effect, it means that while the weights are “open,” their use is gated for large enterprises—unless they’re willing to engage with Mistral’s sales team or use the hosted API at metered pricing.&lt;/p&gt;&lt;p&gt;To draw an analogy: Apache 2.0 is like a public library—you walk in, borrow the book, and use it however you need. Mistral’s modified MIT license is more like a corporate co-working space that’s free for freelancers but charges rent once your company hits a certain size.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Weighing Devstral Small 2 for Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;This division raises an obvious question for larger companies: can Devstral Small 2 with its more permissive and unrestricted Apache 2.0 licensing serve as a viable alternative for medium-to-large enterprises?&lt;/p&gt;&lt;p&gt;The answer depends on context. Devstral Small 2 scores 68.0% on SWE-bench, significantly ahead of many larger open models, and remains deployable on single-GPU or CPU-only setups. For teams focused on:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;internal tooling,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;on-prem deployment,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;low-latency edge inference,&lt;/p&gt;&lt;p&gt;…it offers a rare combination of legality, performance, and convenience.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;But the performance gap from Devstral 2 is real. For multi-agent setups, deep monorepo refactoring, or long-context code analysis, that 4-point benchmark delta may understate the actual experience difference.&lt;/p&gt;&lt;p&gt;For most enterprises, Devstral Small 2 will serve either as a low-friction way to prototype—or as a pragmatic bridge until licensing for Devstral 2 becomes feasible. It is not a drop-in replacement for the flagship, but it may be “good enough” in specific production slices, particularly when paired with Vibe CLI.&lt;/p&gt;&lt;p&gt;But because Devstral Small 2 can be run entirely offline — including on a single GPU machine or a sufficiently specced laptop — it unlocks a critical use case for developers and teams operating in tightly controlled environments. &lt;/p&gt;&lt;p&gt;Whether you’re a solo indie building tools on the go, or part of a company with strict data governance or compliance mandates, the ability to run a performant, long-context coding model without ever hitting the internet is a powerful differentiator. No cloud calls, no third-party telemetry, no risk of data leakage — just local inference with full visibility and control.&lt;/p&gt;&lt;p&gt;This matters in industries like finance, healthcare, defense, and advanced manufacturing, where data often cannot leave the network perimeter. But it’s just as useful for developers who prefer autonomy over vendor lock-in — or who want their tools to work the same on a plane, in the field, or inside an air-gapped lab. In a market where most top-tier code models are delivered as API-only SaaS products, Devstral Small 2 offers a rare level of portability, privacy, and ownership.&lt;/p&gt;&lt;p&gt;In that sense, Mistral isn’t just offering open models—they’re offering multiple paths to adoption, depending on your scale, compliance posture, and willingness to engage.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Integration, Infrastructure, and Access&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;From a technical standpoint, Mistral’s models are built for deployment. Devstral 2 requires a minimum of 4× H100-class GPUs, and is already available on build.nvidia.com. &lt;/p&gt;&lt;p&gt;Devstral Small 2 can run on a single GPU or CPU such as those in a standard laptop, making it accessible to solo developers and embedded teams alike.&lt;/p&gt;&lt;p&gt;Both models support quantized FP4 and FP8 weights, and are compatible with vLLM for scalable inference. Fine-tuning is supported out of the box.&lt;/p&gt;&lt;p&gt;API pricing—after the free introductory window—follows a token-based structure:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Devstral 2:&lt;/b&gt; $0.40 per million input tokens / $2.00 for output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Devstral Small 2:&lt;/b&gt; $0.10 input / $0.30 output&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That pricing sits just below OpenAI’s GPT-4 Turbo, and well below Anthropic’s Claude Sonnet at comparable performance levels.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Developer Reception: Ground-Level Buzz&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;On X (formerly Twitter), developers reacted quickly with a wave of positive reception, with Hugging Face&amp;#x27;s Head of Product &lt;a href="https://x.com/victormustar/status/1998414127400923246"&gt;Victor Mustar asking&lt;/a&gt; if the small, Apache 2.0 licensed variant was the &amp;quot;new local coding king,&amp;quot; i.e., the one developers could use to run on their laptops directly and privately, without an internet connection:&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Another popular AI news and rumors account, TestingCatalogNews, posted that it was &amp;quot;SOTTA in coding,&amp;quot; or &amp;quot;State Of The Tiny Art&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Another user, &lt;a href="https://x.com/xlr8harder/status/1998458990565396505"&gt;@xlr8harder&lt;/a&gt;, took issue with the custom licensing terms for Devstral 2, writing &amp;quot;calling the Devstral 2 license &amp;#x27;modified MIT&amp;#x27; is misleading at best. It’s a proprietary license with MIT-like attribution requirements.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;While the tone was critical, it reflected some attention Mistral’s license structuring was receiving, particularly among developers familiar with open-use norms.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Strategic Context: From Codestral to Devstral and Mistral 3&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral’s steady push into software development tools didn’t start with Devstral 2—it began in May 2024 with &lt;a href="https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model"&gt;Codestral&lt;/a&gt;, the company’s first code-focused large language model. A 22-billion parameter system trained on more than 80 programming languages, Codestral was designed for use in developer environments ranging from basic autocompletions to full function generation. The model launched under a non-commercial license but still outperformed heavyweight competitors like CodeLlama 70B and Deepseek Coder 33B in early benchmarks such as HumanEval and RepoBench.&lt;/p&gt;&lt;p&gt;Codestral’s release marked Mistral’s first move into the competitive coding-model space, but it also established a now-familiar pattern: technically lean models with surprisingly strong results, a wide context window, and licensing choices that invited developer experimentation. Industry partners including JetBrains, LlamaIndex, and LangChain quickly began integrating the model into their workflows, citing its speed and tool compatibility as key differentiators.&lt;/p&gt;&lt;p&gt;One year later, the company followed up with &lt;a href="https://venturebeat.com/ai/mistral-ai-launches-devstral-powerful-new-open-source-swe-agent-model-that-runs-on-laptops"&gt;Devstral&lt;/a&gt;, a 24B model purpose-built for “agentic” behavior—handling long-range reasoning, file navigation, and autonomous code modification. Released in partnership with All Hands AI and licensed under Apache 2.0, Devstral was notable not just for its portability (it could run on a MacBook or RTX 4090), but for its performance: it beat out several closed models on SWE-Bench Verified, a benchmark of 500 real-world GitHub issues.&lt;/p&gt;&lt;p&gt;Then came Mistral 3, announced in December 2025 as &lt;a href="https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on"&gt;a portfolio of 10 open-weight models&lt;/a&gt; targeting everything from drones and smartphones to cloud infrastructure. This suite included both high-end models like Mistral Large 3 (a MoE system with 41 active parameters and 256K context) and lightweight “Ministral” variants that could run on 4GB of VRAM. All were licensed under Apache 2.0, reinforcing Mistral’s commitment to flexible, edge-friendly deployment.&lt;/p&gt;&lt;p&gt;Mistral 3 positioned the company not as a direct competitor to frontier models like GPT-5 or Gemini 3, but as a developer-first platform for customized, localized AI systems. Co-founder Guillaume Lample described the vision as “distributed intelligence”—many smaller systems tuned for specific tasks and running outside centralized infrastructure. “In more than 90% of cases, a small model can do the job,” he told VentureBeat. “It doesn’t have to be a model with hundreds of billions of parameters.”&lt;/p&gt;&lt;p&gt;That broader strategy helps explain the significance of Devstral 2. It’s not a one-off release but a continuation of Mistral’s long-running commitment to code agents, local-first deployment, and open-weight availability—an ecosystem that began with Codestral, matured through Devstral, and scaled up with Mistral 3. Devstral 2, in this framing, is not just a model. It’s the next version of a playbook that’s been unfolding in public for over a year.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Final Thoughts (For Now): A Fork in the Road&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With Devstral 2, Devstral Small 2, and Vibe CLI, Mistral AI has drawn a clear map for developers and companies alike. The tools are fast, capable, and thoughtfully integrated. But they also present &lt;b&gt;a choice&lt;/b&gt;—not just in architecture, but in how and where you’re allowed to use them.&lt;/p&gt;&lt;p&gt;If you’re an individual developer, small startup, or open-source maintainer, this is one of the most powerful AI systems you can freely run today. &lt;/p&gt;&lt;p&gt;If you’re a Fortune 500 engineering lead, you’ll need to either talk to Mistral—or settle for the smaller model and make it work.&lt;/p&gt;&lt;p&gt;In a market increasingly dominated by black-box models and SaaS lock-ins, Mistral’s offer is still a breath of fresh air. Just read the fine print before you start building.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;French AI startup Mistral has weathered a rocky period of public questioning over the last year to emerge, now here in December 2025, with new, crowd-pleasing models for enterprise and indie developers.&lt;/p&gt;&lt;p&gt;Just days after releasing its &lt;a href="https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on"&gt;powerful open source, general purpose Mistral 3 LLM family&lt;/a&gt; for edge devices and local hardware, the &lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;&lt;b&gt;company returned today to debut Devstral 2&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The release includes a new pair of models optimized for software engineering tasks — again, with one small enough to run on a single laptop, offline and privately — alongside &lt;b&gt;Mistral Vibe,&lt;/b&gt; a command-line interface (CLI) agent designed to allow developers to call the models up directly within their terminal environments. &lt;/p&gt;&lt;p&gt;The models are fast, lean, and open—at least in theory. But the real story lies not just in the benchmarks, but in how Mistral is packaging this capability: one model fully free, another conditionally so, and a terminal interface built to scale with either.&lt;/p&gt;&lt;p&gt;It’s an attempt not just to match proprietary systems like Claude and GPT-4 in performance, but to compete with them on developer experience—and to do so while holding onto the flag of open-source.&lt;/p&gt;&lt;p&gt;Both models are available now for free for a limited time &lt;a href="https://docs.mistral.ai/models/devstral-2-25-12"&gt;via Mistral’s API&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/mistralai/devstral-2"&gt;Hugging Face&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The full Devstral 2 model is supported out-of-the-box in the community inference provider &lt;a href="https://x.com/vllm_project/status/1998428798891765926?s=20"&gt;vLLM&lt;/a&gt; and on the open source agentic coding platform &lt;a href="https://x.com/kilocode/status/1998412042357588461"&gt;Kilo Code&lt;/a&gt;.  &lt;/p&gt;&lt;h2&gt;&lt;b&gt;A Coding Model Meant to Drive&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At the top of the announcement is Devstral 2, a 123-billion parameter dense transformer with a 256K-token context window, engineered specifically for agentic software development. &lt;/p&gt;&lt;p&gt;Mistral says the model achieves 72.2% on SWE-bench Verified, a benchmark designed to evaluate long-context software engineering tasks in real-world repositories.&lt;/p&gt;&lt;p&gt;The smaller sibling, Devstral Small 2, weighs in at 24B parameters, with the same long context window and a performance of 68.0% on SWE-bench. &lt;/p&gt;&lt;p&gt;On paper, that makes it the strongest open-weight model of its size, even outscoring many 70B-class competitors.&lt;/p&gt;&lt;p&gt;But the performance story isn’t just about raw percentages. Mistral is betting that efficient intelligence beats scale, and has made much of the fact that Devstral 2 is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;5× smaller than DeepSeek V3.2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;8× smaller than Kimi K2&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Yet still matches or surpasses them on key software reasoning benchmarks.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Human evaluations back this up. In side-by-side comparisons:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Devstral 2 beat DeepSeek V3.2 in 42.8% of tasks, losing only 28.6%.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Against Claude Sonnet 4.5, it lost more often (53.1%)—a reminder that while the gap is narrowing, closed models still lead in overall preference.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Still, for an open-weight model, these results place Devstral 2 at the frontier of what’s currently available to run and modify independently.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Vibe CLI: A Terminal-Native Agent&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Alongside the models, Mistral released Vibe CLI, a command-line assistant that integrates directly with Devstral models. It’s not an IDE plugin or a ChatGPT-style code explainer. It’s a native interface designed for project-wide code understanding and orchestration, built to live inside the developer’s actual workflow.&lt;/p&gt;&lt;p&gt;Vibe brings a surprising degree of intelligence to the terminal:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;It reads your file tree and Git status to understand project scope.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It lets you reference files with @, run shell commands with !, and toggle behavior with slash commands.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;It orchestrates changes across multiple files, tracks dependencies, retries failed executions, and can even refactor at architectural scale.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Unlike most developer agents, which simulate a REPL from within a chat UI, Vibe starts with the shell and pulls intelligence in from there. It’s programmable, scriptable, and themeable. And it’s released under the Apache 2.0 license, meaning it’s truly free to use—in commercial settings, internal tools, or open-source extensions.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Licensing Structure: Open-ish — With Revenue Limitations&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At first glance, Mistral’s licensing approach appears straightforward: the models are open-weight and publicly available. But a closer look reveals a line drawn through the middle of the release, with different rules for different users.&lt;/p&gt;&lt;p&gt;Devstral Small 2, the 24-billion parameter variant, is covered under a standard, enterprise- and developer-friendly &lt;a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md"&gt;Apache 2.0 license&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;That’s a gold standard in open-source: no revenue restrictions, no fine print, no need to check with legal. Enterprises can use it in production, embed it into products, and redistribute fine-tuned versions without asking for permission.&lt;/p&gt;&lt;p&gt;Devstral 2, the flagship 123B model, is released under what Mistral calls a “&lt;a href="https://huggingface.co/mistralai/Devstral-2-123B-Instruct-2512/blob/main/LICENSE"&gt;modified MIT license&lt;/a&gt;.” That phrase sounds innocuous, but the modification introduces a critical limitation: any company making more than $20 million in monthly revenue cannot use the model at all—not even internally—without securing a separate commercial license from Mistral.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“You are not authorized to exercise any rights under this license if the global consolidated monthly revenue of your company […] exceeds $20 million,” the license reads.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The clause applies not only to the base model, but to derivatives, fine-tuned versions, and redistributed variants, regardless of who hosts them. In effect, it means that while the weights are “open,” their use is gated for large enterprises—unless they’re willing to engage with Mistral’s sales team or use the hosted API at metered pricing.&lt;/p&gt;&lt;p&gt;To draw an analogy: Apache 2.0 is like a public library—you walk in, borrow the book, and use it however you need. Mistral’s modified MIT license is more like a corporate co-working space that’s free for freelancers but charges rent once your company hits a certain size.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Weighing Devstral Small 2 for Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;This division raises an obvious question for larger companies: can Devstral Small 2 with its more permissive and unrestricted Apache 2.0 licensing serve as a viable alternative for medium-to-large enterprises?&lt;/p&gt;&lt;p&gt;The answer depends on context. Devstral Small 2 scores 68.0% on SWE-bench, significantly ahead of many larger open models, and remains deployable on single-GPU or CPU-only setups. For teams focused on:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;internal tooling,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;on-prem deployment,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;low-latency edge inference,&lt;/p&gt;&lt;p&gt;…it offers a rare combination of legality, performance, and convenience.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;But the performance gap from Devstral 2 is real. For multi-agent setups, deep monorepo refactoring, or long-context code analysis, that 4-point benchmark delta may understate the actual experience difference.&lt;/p&gt;&lt;p&gt;For most enterprises, Devstral Small 2 will serve either as a low-friction way to prototype—or as a pragmatic bridge until licensing for Devstral 2 becomes feasible. It is not a drop-in replacement for the flagship, but it may be “good enough” in specific production slices, particularly when paired with Vibe CLI.&lt;/p&gt;&lt;p&gt;But because Devstral Small 2 can be run entirely offline — including on a single GPU machine or a sufficiently specced laptop — it unlocks a critical use case for developers and teams operating in tightly controlled environments. &lt;/p&gt;&lt;p&gt;Whether you’re a solo indie building tools on the go, or part of a company with strict data governance or compliance mandates, the ability to run a performant, long-context coding model without ever hitting the internet is a powerful differentiator. No cloud calls, no third-party telemetry, no risk of data leakage — just local inference with full visibility and control.&lt;/p&gt;&lt;p&gt;This matters in industries like finance, healthcare, defense, and advanced manufacturing, where data often cannot leave the network perimeter. But it’s just as useful for developers who prefer autonomy over vendor lock-in — or who want their tools to work the same on a plane, in the field, or inside an air-gapped lab. In a market where most top-tier code models are delivered as API-only SaaS products, Devstral Small 2 offers a rare level of portability, privacy, and ownership.&lt;/p&gt;&lt;p&gt;In that sense, Mistral isn’t just offering open models—they’re offering multiple paths to adoption, depending on your scale, compliance posture, and willingness to engage.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Integration, Infrastructure, and Access&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;From a technical standpoint, Mistral’s models are built for deployment. Devstral 2 requires a minimum of 4× H100-class GPUs, and is already available on build.nvidia.com. &lt;/p&gt;&lt;p&gt;Devstral Small 2 can run on a single GPU or CPU such as those in a standard laptop, making it accessible to solo developers and embedded teams alike.&lt;/p&gt;&lt;p&gt;Both models support quantized FP4 and FP8 weights, and are compatible with vLLM for scalable inference. Fine-tuning is supported out of the box.&lt;/p&gt;&lt;p&gt;API pricing—after the free introductory window—follows a token-based structure:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Devstral 2:&lt;/b&gt; $0.40 per million input tokens / $2.00 for output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Devstral Small 2:&lt;/b&gt; $0.10 input / $0.30 output&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;That pricing sits just below OpenAI’s GPT-4 Turbo, and well below Anthropic’s Claude Sonnet at comparable performance levels.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Developer Reception: Ground-Level Buzz&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;On X (formerly Twitter), developers reacted quickly with a wave of positive reception, with Hugging Face&amp;#x27;s Head of Product &lt;a href="https://x.com/victormustar/status/1998414127400923246"&gt;Victor Mustar asking&lt;/a&gt; if the small, Apache 2.0 licensed variant was the &amp;quot;new local coding king,&amp;quot; i.e., the one developers could use to run on their laptops directly and privately, without an internet connection:&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Another popular AI news and rumors account, TestingCatalogNews, posted that it was &amp;quot;SOTTA in coding,&amp;quot; or &amp;quot;State Of The Tiny Art&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Another user, &lt;a href="https://x.com/xlr8harder/status/1998458990565396505"&gt;@xlr8harder&lt;/a&gt;, took issue with the custom licensing terms for Devstral 2, writing &amp;quot;calling the Devstral 2 license &amp;#x27;modified MIT&amp;#x27; is misleading at best. It’s a proprietary license with MIT-like attribution requirements.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;While the tone was critical, it reflected some attention Mistral’s license structuring was receiving, particularly among developers familiar with open-use norms.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Strategic Context: From Codestral to Devstral and Mistral 3&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Mistral’s steady push into software development tools didn’t start with Devstral 2—it began in May 2024 with &lt;a href="https://venturebeat.com/ai/mistral-announces-codestral-its-first-programming-focused-ai-model"&gt;Codestral&lt;/a&gt;, the company’s first code-focused large language model. A 22-billion parameter system trained on more than 80 programming languages, Codestral was designed for use in developer environments ranging from basic autocompletions to full function generation. The model launched under a non-commercial license but still outperformed heavyweight competitors like CodeLlama 70B and Deepseek Coder 33B in early benchmarks such as HumanEval and RepoBench.&lt;/p&gt;&lt;p&gt;Codestral’s release marked Mistral’s first move into the competitive coding-model space, but it also established a now-familiar pattern: technically lean models with surprisingly strong results, a wide context window, and licensing choices that invited developer experimentation. Industry partners including JetBrains, LlamaIndex, and LangChain quickly began integrating the model into their workflows, citing its speed and tool compatibility as key differentiators.&lt;/p&gt;&lt;p&gt;One year later, the company followed up with &lt;a href="https://venturebeat.com/ai/mistral-ai-launches-devstral-powerful-new-open-source-swe-agent-model-that-runs-on-laptops"&gt;Devstral&lt;/a&gt;, a 24B model purpose-built for “agentic” behavior—handling long-range reasoning, file navigation, and autonomous code modification. Released in partnership with All Hands AI and licensed under Apache 2.0, Devstral was notable not just for its portability (it could run on a MacBook or RTX 4090), but for its performance: it beat out several closed models on SWE-Bench Verified, a benchmark of 500 real-world GitHub issues.&lt;/p&gt;&lt;p&gt;Then came Mistral 3, announced in December 2025 as &lt;a href="https://venturebeat.com/ai/mistral-launches-mistral-3-a-family-of-open-models-designed-to-run-on"&gt;a portfolio of 10 open-weight models&lt;/a&gt; targeting everything from drones and smartphones to cloud infrastructure. This suite included both high-end models like Mistral Large 3 (a MoE system with 41 active parameters and 256K context) and lightweight “Ministral” variants that could run on 4GB of VRAM. All were licensed under Apache 2.0, reinforcing Mistral’s commitment to flexible, edge-friendly deployment.&lt;/p&gt;&lt;p&gt;Mistral 3 positioned the company not as a direct competitor to frontier models like GPT-5 or Gemini 3, but as a developer-first platform for customized, localized AI systems. Co-founder Guillaume Lample described the vision as “distributed intelligence”—many smaller systems tuned for specific tasks and running outside centralized infrastructure. “In more than 90% of cases, a small model can do the job,” he told VentureBeat. “It doesn’t have to be a model with hundreds of billions of parameters.”&lt;/p&gt;&lt;p&gt;That broader strategy helps explain the significance of Devstral 2. It’s not a one-off release but a continuation of Mistral’s long-running commitment to code agents, local-first deployment, and open-weight availability—an ecosystem that began with Codestral, matured through Devstral, and scaled up with Mistral 3. Devstral 2, in this framing, is not just a model. It’s the next version of a playbook that’s been unfolding in public for over a year.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Final Thoughts (For Now): A Fork in the Road&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With Devstral 2, Devstral Small 2, and Vibe CLI, Mistral AI has drawn a clear map for developers and companies alike. The tools are fast, capable, and thoughtfully integrated. But they also present &lt;b&gt;a choice&lt;/b&gt;—not just in architecture, but in how and where you’re allowed to use them.&lt;/p&gt;&lt;p&gt;If you’re an individual developer, small startup, or open-source maintainer, this is one of the most powerful AI systems you can freely run today. &lt;/p&gt;&lt;p&gt;If you’re a Fortune 500 engineering lead, you’ll need to either talk to Mistral—or settle for the smaller model and make it work.&lt;/p&gt;&lt;p&gt;In a market increasingly dominated by black-box models and SaaS lock-ins, Mistral’s offer is still a breath of fresh air. Just read the fine print before you start building.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/mistral-launches-powerful-devstral-2-coding-model-including-open-source</guid><pubDate>Tue, 09 Dec 2025 19:44:00 +0000</pubDate></item><item><title>[NEW] Three in 10 US teens use AI chatbots every day, but safety concerns are growing (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/three-in-ten-u-s-teens-use-ai-chatbots-every-day-but-safety-concerns-are-growing/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Pew Research Center released a study on Tuesday that shows how young people are using both social media and AI chatbots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Teen internet safety has remained a global hot topic, with Australia planning to enforce a social media ban for under-16s starting on Wednesday. The impact of social media on teen mental health has been extensively debated — some studies show how online communities can improve mental health, while other research shows the adverse effects of doomscrolling or spending too much time online. The U.S. surgeon general even called for social media platforms to put warning labels on their products last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pew found that 97% of teens use the internet daily, with about 40% of respondents saying they are “almost constantly online.” While this marks a decrease from last year’s survey (46%), it’s significantly higher than the results from a decade ago, when 24% of teens said they were online almost constantly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as the prevalence of AI chatbots grows in the U.S., this technology has become yet another factor in the internet’s impact on American youth.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074371" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-2.47.26-PM.png?w=619" width="619" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;About three in 10 U.S. teens are using AI chatbots every day, the Pew study reveals, with 4% saying they use them almost constantly. Fifty-nine percent of teens say they use ChatGPT, which is more than twice as popular as the next two most-used chatbots, Google’s Gemini (23%) and Meta AI (20%). Forty-six percent of U.S. teens say that they use AI chatbots at least several times a week, while 36% report not using AI chatbots at all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pew’s research also details how race, age, and class impact teen chatbot use. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;About 68% of Black and Hispanic teens surveyed said they use chatbots, compared to 58% of white respondents. In particular, Black teens were about twice as likely to use Gemini and Meta AI as white teens.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The racial and ethnic differences in teen chatbot use were striking […] but it’s tough to speculate about the reasons behind those differences,” Pew Research Associate Michelle Faverio told TechCrunch. “This pattern is consistent with other racial and ethnic differences we’ve seen in teen technology use. Black and Hispanic teens are more likely than white teens to say they’re on certain social media sites — such as TikTok, YouTube, and Instagram.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074370" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-2.46.37-PM.png?w=652" width="652" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Across all internet use, Black (55%) and Hispanic teens (52%) were around twice as likely as white teens (27%) to say that they are online “almost constantly.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Older teens (ages 15 to 17) tend to use both social media and AI chatbots more often than younger teens (ages 13 to 14). When it comes to household income, about 62% of teens living in households making more than $75,000 per year said they use ChatGPT, compared to 52% of teens below that threshold. But Character.AI usage is twice as popular (14%) in homes with incomes below $75,000.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While teenagers may start out using these tools for basic questions or homework help, their relationship to AI chatbots can become addictive and potentially harmful. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The families of at least two teens, Adam Raine and Amaurie Lacey, have sued ChatGPT maker OpenAI for its alleged role in their children’s suicides — in both cases, ChatGPT gave the teenagers detailed instructions on how to hang themselves, which were tragically effective. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(OpenAI claims it should not be held liable for Raine’s death because the sixteen-year-old allegedly circumvented ChatGPT’s safety features and thus violated the chatbot’s terms of service; the company has yet to respond to the Lacey family’s complaint.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Character.AI, an AI role-playing platform, is also facing scrutiny for its impact on teen mental health; at least two teenagers&amp;nbsp;died by suicide after having prolonged conversations with AI chatbots. The startup ended up making the decision to stop offering its chatbots to minors, and instead launched a product called “Stories” for underage users that more closely resembles a choose-your-own-adventure game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The experiences reflected in the lawsuits against these companies make up a small percentage of all interactions that happen on ChatGPT or Character.AI. In many cases, conversations with chatbots can be incredibly benign. According to OpenAI’s data, only 0.15% of ChatGPT’s active users have conversations about suicide each week — but on a platform with 800 million weekly active users, that small percentage reflects over one million people who discuss suicide with the chatbot per week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even if [AI companies’] tools weren’t designed for emotional support, people are using them in that way, and that means companies do have a responsibility to adjust their models to be solving for user well-being,” Dr. Nina Vasan, a psychiatrist and director of Brainstorm: The Stanford Lab for Mental Health Innovation, told TechCrunch.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Pew Research Center released a study on Tuesday that shows how young people are using both social media and AI chatbots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Teen internet safety has remained a global hot topic, with Australia planning to enforce a social media ban for under-16s starting on Wednesday. The impact of social media on teen mental health has been extensively debated — some studies show how online communities can improve mental health, while other research shows the adverse effects of doomscrolling or spending too much time online. The U.S. surgeon general even called for social media platforms to put warning labels on their products last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pew found that 97% of teens use the internet daily, with about 40% of respondents saying they are “almost constantly online.” While this marks a decrease from last year’s survey (46%), it’s significantly higher than the results from a decade ago, when 24% of teens said they were online almost constantly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as the prevalence of AI chatbots grows in the U.S., this technology has become yet another factor in the internet’s impact on American youth.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074371" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-2.47.26-PM.png?w=619" width="619" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;About three in 10 U.S. teens are using AI chatbots every day, the Pew study reveals, with 4% saying they use them almost constantly. Fifty-nine percent of teens say they use ChatGPT, which is more than twice as popular as the next two most-used chatbots, Google’s Gemini (23%) and Meta AI (20%). Forty-six percent of U.S. teens say that they use AI chatbots at least several times a week, while 36% report not using AI chatbots at all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pew’s research also details how race, age, and class impact teen chatbot use. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;About 68% of Black and Hispanic teens surveyed said they use chatbots, compared to 58% of white respondents. In particular, Black teens were about twice as likely to use Gemini and Meta AI as white teens.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The racial and ethnic differences in teen chatbot use were striking […] but it’s tough to speculate about the reasons behind those differences,” Pew Research Associate Michelle Faverio told TechCrunch. “This pattern is consistent with other racial and ethnic differences we’ve seen in teen technology use. Black and Hispanic teens are more likely than white teens to say they’re on certain social media sites — such as TikTok, YouTube, and Instagram.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3074370" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-2.46.37-PM.png?w=652" width="652" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Pew Research Center&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Across all internet use, Black (55%) and Hispanic teens (52%) were around twice as likely as white teens (27%) to say that they are online “almost constantly.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Older teens (ages 15 to 17) tend to use both social media and AI chatbots more often than younger teens (ages 13 to 14). When it comes to household income, about 62% of teens living in households making more than $75,000 per year said they use ChatGPT, compared to 52% of teens below that threshold. But Character.AI usage is twice as popular (14%) in homes with incomes below $75,000.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While teenagers may start out using these tools for basic questions or homework help, their relationship to AI chatbots can become addictive and potentially harmful. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The families of at least two teens, Adam Raine and Amaurie Lacey, have sued ChatGPT maker OpenAI for its alleged role in their children’s suicides — in both cases, ChatGPT gave the teenagers detailed instructions on how to hang themselves, which were tragically effective. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(OpenAI claims it should not be held liable for Raine’s death because the sixteen-year-old allegedly circumvented ChatGPT’s safety features and thus violated the chatbot’s terms of service; the company has yet to respond to the Lacey family’s complaint.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Character.AI, an AI role-playing platform, is also facing scrutiny for its impact on teen mental health; at least two teenagers&amp;nbsp;died by suicide after having prolonged conversations with AI chatbots. The startup ended up making the decision to stop offering its chatbots to minors, and instead launched a product called “Stories” for underage users that more closely resembles a choose-your-own-adventure game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The experiences reflected in the lawsuits against these companies make up a small percentage of all interactions that happen on ChatGPT or Character.AI. In many cases, conversations with chatbots can be incredibly benign. According to OpenAI’s data, only 0.15% of ChatGPT’s active users have conversations about suicide each week — but on a platform with 800 million weekly active users, that small percentage reflects over one million people who discuss suicide with the chatbot per week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even if [AI companies’] tools weren’t designed for emotional support, people are using them in that way, and that means companies do have a responsibility to adjust their models to be solving for user well-being,” Dr. Nina Vasan, a psychiatrist and director of Brainstorm: The Stanford Lab for Mental Health Innovation, told TechCrunch.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/three-in-ten-u-s-teens-use-ai-chatbots-every-day-but-safety-concerns-are-growing/</guid><pubDate>Tue, 09 Dec 2025 20:00:00 +0000</pubDate></item><item><title>[NEW] Apriel-1.6-15b-Thinker: Cost-efficient Frontier Multimodal Performance (Hugging Face - Blog)</title><link>https://huggingface.co/blog/ServiceNow-AI/apriel-1p6-15b-thinker</link><description>&lt;!-- HTML_TAG_START --&gt;
We release Apriel-1.6-15b-Thinker, a 15-billion parameter multimodal reasoning model in ServiceNow’s Apriel SLM series which achieves SOTA performance against models 10 times it's size. Apriel-1.6 builds on top of Apriel-1.5-15b-Thinker with an extensive focus on improving text and vision reasoning, while improving token efficiency. This version was trained on NVIDIA DGX™ Cloud with GB200 Grace™ Blackwell Superchips.
&lt;p&gt;Apriel-1.6 scores 57 on the Artificial Analysis Index, outperforming models like Gemini 2.5 Flash, Claude Haiku 4.5 and GPT OSS 20b. It obtains a score on par with Qwen3 235B A22B, while being signficantly more efficient. This new release improves or maintains task performance in comparison with the previous Apriel-1.5-15B-Thinker [1], while reducing reasoning token usage by more than 30%.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Artificial Analysis Intelligence Index (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/liUuJ-2ZPz_xtHEgq4wN2.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Mid-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We follow the same overall training process used for Apriel-1.5-15B-Thinker, which includes a depth-upscaling phase followed by two Continual Pretraining (CPT) stages (detailed in [1]). The depth-upscaling corpus consists of 35% data from diverse sources, including high-quality web content, scientific and technical literature, mathematical problem sets, and programming code; 15% high-quality datasets from NVIDIA Nemotron™; and the remaining 50% pretraining-style data serving as replay.&lt;/p&gt;
&lt;p&gt;For Apriel-1.6-15B-Thinker, we expand the Stage-1 CPT mixture, which focuses on strengthening textual reasoning and image understanding, with additional text-only samples and image-text pairs. The new text data is fully synthetic, covering general reasoning, knowledge, coding, and creative writing, while the multimodal portion spans document and chart understanding, OCR, visual-reasoning tasks, and SVG/web-code synthesis.&lt;/p&gt;
&lt;p&gt;Following Stage-1, we perform a text-only CPT run at an extended 49K sequence length and then run Stage 2 to further refine the model’s visual-reasoning capabilities. This combination produced a strong base model that provided a solid foundation for subsequent post-training. Training for this mid-training pipeline required approximately 10,000 GPU hours on NVIDIA's GB200s, a small compute footprint enabled by their high throughput and aligned with our goal of building strong models with limited resources through careful data strategy and training methodology.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Using the midtrained model, we perform post-training following a pipeline that consists of large scale Supervised Finetuning (SFT) and Reinforcement Learning (RL) targeting both vision and text abilities. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Supervised Finetuning (SFT)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our Supervised Fine-Tuning (SFT) stage focuses on improving the reasoning quality of Apriel-1.6 by training on a meticulously curated dataset of 2.4 million high-signal text samples. Each example includes explicit, step-by-step reasoning traces, enabling the model to internalize transparent reasoning processes rather than merely reproducing final answers.&lt;/p&gt;
&lt;p&gt;To construct this dataset, we combined execution-verifiable synthetic samples for math, coding, and scientific problem-solving with a broad mix of instruction-following, conversational, API/function-calling, creative writing, safety, and other knowledge-intensive samples. Data quality was treated as a first-class priority: every sample passed through multi-stage de-duplication, content filtering, heuristic quality pruning, LLM-as-Judge validation, execution-based verification (where applicable), and strict decontamination against evaluation benchmarks.&lt;/p&gt;
&lt;p&gt;SFT was carried out in two phases, both trained at a 32K context length. In the first phase, we ran a large-scale text-only training run on the 2.4M samples for 4 epochs. Compared to Apriel-1.5-15b-Thinker, we simplified the chat template by removing redundant tags and introduced four special tokens to the tokenizer (&lt;code&gt;&amp;lt;tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;[BEGIN FINAL RESPONSE]&lt;/code&gt;, &lt;code&gt;&amp;lt;|end|&amp;gt;&lt;/code&gt;) for easier output parsing.&lt;/p&gt;
&lt;p&gt;The second phase was a lightweight, multimodal run trained for 3 epochs, using rejection-sampled data from Apriel-1.5-15b-Thinker to ensure the model maintained strong performance on image inputs after the introduction of these special tokens, while also preparing it for downstream RL stages.&lt;/p&gt;
&lt;p&gt;This approach provided us with a robust, high-quality SFT foundation on top of which our RL pipeline could operate effectively. The resulting model exhibits strong multimodal understanding, improved text reasoning capabilities, and enhanced agentic behavior.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reinforcement Learning (RL)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We adopt a multi-stage RL setup that focuses on simultaneously improving reasoning capability and efficiency.
We train the model on image domains such as visual reasoning, general visual question answering (VQA) and optical character recognition (OCR). Our training data also consists of data across different domains, such as simple questions (to encourage short, direct answers on easy queries), math (numerical reasoning), STEM (multiple-choice scientific questions), and function calling (structured tool use).&lt;/p&gt;
&lt;p&gt;Rewards are given for correctness of the response, along with penalties for undesirable behaviour, such as verbosity, incorrect formats, etc. Overall, our setup is designed to improve the model’s reasoning ability while using fewer reasoning tokens, encouraging it to avoid unnecessary intermediate steps, stop earlier when confident, and answer more directly for simpler queries.&lt;/p&gt;
&lt;p&gt;Training is done with the Group Sequence Policy Optimization loss (GSPO) [2] using the VeRL framework and rule-based verification.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Evaluation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Text Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We evaluate Apriel-1.6 on various domains such as tool use, math, coding, instruction following and long context.&lt;/p&gt;

&lt;p&gt;* This score is with DCA enabled. Without this, the model scores 36.&lt;/p&gt;
&lt;p&gt;** The average score is calculated using all benchmarks except BFCL v3 Only and DeepResearchBench, since some models do not have scores for these two benchmarks.&lt;/p&gt;
&lt;p&gt;*** AA LCR score for o3-mini-high is projected score based on its AA Index score.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Image Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We evaluate the Apriel-1.6 model on a representative set of evaluations with the prime focus on mathematical reasoning, visual question answering, logical reasoning, STEM related tasks and chart based reasoning. All evaluations are done using VLMEvalkit. Apriel-1.6 improves on its predecessor by &lt;strong&gt;4 points&lt;/strong&gt; on the average of 13 benchmarks of the &lt;strong&gt;Image Index&lt;/strong&gt; comprising of the following benchmarks: MathVision, MathVista, MMMU (validation), MMMU-Pro (10 choice COT), MMMU-Pro (Vision only COT), MathVerse (Vision Dominant), MathVerse (Text Dominant), MMStar, BLINK, LogicVista, CharXiV (descriptive), CharXiV (reasoning), AI2D (test).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Performance on the Image Index" src="https://cdn-uploads.huggingface.co/production/uploads/65036ffdb96045f918094fd6/B5PogpJIui7vGsZ7xx-jH.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Cost-Efficient Frontier Performance
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Intelligence vs Total Parameters (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/q2Y4Xjp9Sy5dma9wFt7Ny.png" /&gt;&lt;/p&gt;
&lt;p&gt;Apriel-1.6-15B-Thinker sits in the sweet spot of the cost-efficient frontier. It delivers intelligence scores that rival or surpass much larger models while using only 15B parameters. On the chart, it’s firmly inside the &lt;em&gt;&lt;strong&gt;most attractive&lt;/strong&gt;&lt;/em&gt; quadrant, balancing efficiency with top-tier reasoning. In practice, this means Apriel-1.6-15B-Thinker offers strong performance and deep reasoning at a fraction of the compute and deployment cost of heavyweight competitors, making it an exceptionally efficient choice for the real-world, especially in enterprise applications.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Intelligence vs Output Tokens Used in Artificial Analysis Intelligence Index (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/QoymhuGcJf6lFAIruUQ2p.png" /&gt;&lt;/p&gt;
&lt;p&gt;Our post-training focuses heavily on improving reasoning-token efficiency. The image above showing intelligence score against token usage highlights the effectiveness of our post-training. Apriel-1.6-15B-Thinker again lands in &lt;em&gt;&lt;strong&gt;most attractive&lt;/strong&gt;&lt;/em&gt; quadrant. The model reaches a high Artificial Analysis Intelligence Index score while using far fewer tokens than many similarly capable or larger models. In comparison to Apriel-1.5-15b-Thinker [1], we reduce token usage by over 30%. &lt;/p&gt;
&lt;p&gt;Overall, Apriel-1.6 is a highly-capable reasoner, that maintains memory and efficiency characteristics required for enterprise deployment.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Acknowledgements
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We gratefully acknowledge the following people for their contributions: Varun Pandey, Shashank Maiya, Dhruv Jhamb, Massimo Caccia, Dheeraj Vattikonda, Nicolas Gontier, Patrice Bechard, Tayfun Tuna, Kavya Sriram, Denis Akhiyarov, Hari Subramani, Tara Bogavelli.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Notes and Limitations
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We are a small lab with big goals. While we are not GPU poor, our lab, in comparison has a tiny fraction of the compute available to other Frontier labs. Our goal with this work is to show that a SOTA model can be built with limited resources if you have the right data, design and solid methodology.&lt;/p&gt;
&lt;p&gt;We set out to build a small but powerful model, aiming for capabilities on par with frontier models. Developing a 15B model with this level of performance requires tradeoffs, so we prioritized getting SOTA-level performance and improving reasoning token efficiency.&lt;/p&gt;
&lt;p&gt;This model is trained to perform extensive reasoning for difficult questions and less reasoning effort for simpler questions. We are always actively working to make our models more efficient and concise in future releases.&lt;/p&gt;
&lt;p&gt;The model has a few vision-related limitations to be aware of. Complex or low-quality images can reduce OCR accuracy, dense scenes (like crowds or many similar objects) can make subtle details and counting more challenging, and highly detailed or unusually formatted charts may occasionally lead to imperfect interpretations. It may also be less precise with fine-grained visual grounding, so bounding-box predictions can sometimes be approximate or inconsistent.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		References
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;[1] Radhakrishna, S., Tiwari, A., Shukla, A., Hashemi, M., Maheshwary, R., Malay, S.K.R., Mehta, J., Pattnaik, P., Mittal, S., Slimi, K., Ogueji, K., Oladipo, A., Parikh, S., Bamgbose, O., Liang, T., Masry, A., Mahajan, K., Mudumba, S.R., Yadav, V., Madhusudhan, S.T., Scholak, T., Davasam, S., Sunkara, S. and Chapados, N., 2025. Apriel-1.5-15b-Thinker. arXiv preprint arXiv:2510.01141.&lt;/p&gt;
&lt;p&gt;[2] Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J. and Lin, J., 2025. Group Sequence Policy Optimization. arXiv preprint arXiv:2507.18071.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
We release Apriel-1.6-15b-Thinker, a 15-billion parameter multimodal reasoning model in ServiceNow’s Apriel SLM series which achieves SOTA performance against models 10 times it's size. Apriel-1.6 builds on top of Apriel-1.5-15b-Thinker with an extensive focus on improving text and vision reasoning, while improving token efficiency. This version was trained on NVIDIA DGX™ Cloud with GB200 Grace™ Blackwell Superchips.
&lt;p&gt;Apriel-1.6 scores 57 on the Artificial Analysis Index, outperforming models like Gemini 2.5 Flash, Claude Haiku 4.5 and GPT OSS 20b. It obtains a score on par with Qwen3 235B A22B, while being signficantly more efficient. This new release improves or maintains task performance in comparison with the previous Apriel-1.5-15B-Thinker [1], while reducing reasoning token usage by more than 30%.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Artificial Analysis Intelligence Index (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/liUuJ-2ZPz_xtHEgq4wN2.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Mid-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We follow the same overall training process used for Apriel-1.5-15B-Thinker, which includes a depth-upscaling phase followed by two Continual Pretraining (CPT) stages (detailed in [1]). The depth-upscaling corpus consists of 35% data from diverse sources, including high-quality web content, scientific and technical literature, mathematical problem sets, and programming code; 15% high-quality datasets from NVIDIA Nemotron™; and the remaining 50% pretraining-style data serving as replay.&lt;/p&gt;
&lt;p&gt;For Apriel-1.6-15B-Thinker, we expand the Stage-1 CPT mixture, which focuses on strengthening textual reasoning and image understanding, with additional text-only samples and image-text pairs. The new text data is fully synthetic, covering general reasoning, knowledge, coding, and creative writing, while the multimodal portion spans document and chart understanding, OCR, visual-reasoning tasks, and SVG/web-code synthesis.&lt;/p&gt;
&lt;p&gt;Following Stage-1, we perform a text-only CPT run at an extended 49K sequence length and then run Stage 2 to further refine the model’s visual-reasoning capabilities. This combination produced a strong base model that provided a solid foundation for subsequent post-training. Training for this mid-training pipeline required approximately 10,000 GPU hours on NVIDIA's GB200s, a small compute footprint enabled by their high throughput and aligned with our goal of building strong models with limited resources through careful data strategy and training methodology.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Using the midtrained model, we perform post-training following a pipeline that consists of large scale Supervised Finetuning (SFT) and Reinforcement Learning (RL) targeting both vision and text abilities. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Supervised Finetuning (SFT)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our Supervised Fine-Tuning (SFT) stage focuses on improving the reasoning quality of Apriel-1.6 by training on a meticulously curated dataset of 2.4 million high-signal text samples. Each example includes explicit, step-by-step reasoning traces, enabling the model to internalize transparent reasoning processes rather than merely reproducing final answers.&lt;/p&gt;
&lt;p&gt;To construct this dataset, we combined execution-verifiable synthetic samples for math, coding, and scientific problem-solving with a broad mix of instruction-following, conversational, API/function-calling, creative writing, safety, and other knowledge-intensive samples. Data quality was treated as a first-class priority: every sample passed through multi-stage de-duplication, content filtering, heuristic quality pruning, LLM-as-Judge validation, execution-based verification (where applicable), and strict decontamination against evaluation benchmarks.&lt;/p&gt;
&lt;p&gt;SFT was carried out in two phases, both trained at a 32K context length. In the first phase, we ran a large-scale text-only training run on the 2.4M samples for 4 epochs. Compared to Apriel-1.5-15b-Thinker, we simplified the chat template by removing redundant tags and introduced four special tokens to the tokenizer (&lt;code&gt;&amp;lt;tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/tool_calls&amp;gt;&lt;/code&gt;, &lt;code&gt;[BEGIN FINAL RESPONSE]&lt;/code&gt;, &lt;code&gt;&amp;lt;|end|&amp;gt;&lt;/code&gt;) for easier output parsing.&lt;/p&gt;
&lt;p&gt;The second phase was a lightweight, multimodal run trained for 3 epochs, using rejection-sampled data from Apriel-1.5-15b-Thinker to ensure the model maintained strong performance on image inputs after the introduction of these special tokens, while also preparing it for downstream RL stages.&lt;/p&gt;
&lt;p&gt;This approach provided us with a robust, high-quality SFT foundation on top of which our RL pipeline could operate effectively. The resulting model exhibits strong multimodal understanding, improved text reasoning capabilities, and enhanced agentic behavior.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reinforcement Learning (RL)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We adopt a multi-stage RL setup that focuses on simultaneously improving reasoning capability and efficiency.
We train the model on image domains such as visual reasoning, general visual question answering (VQA) and optical character recognition (OCR). Our training data also consists of data across different domains, such as simple questions (to encourage short, direct answers on easy queries), math (numerical reasoning), STEM (multiple-choice scientific questions), and function calling (structured tool use).&lt;/p&gt;
&lt;p&gt;Rewards are given for correctness of the response, along with penalties for undesirable behaviour, such as verbosity, incorrect formats, etc. Overall, our setup is designed to improve the model’s reasoning ability while using fewer reasoning tokens, encouraging it to avoid unnecessary intermediate steps, stop earlier when confident, and answer more directly for simpler queries.&lt;/p&gt;
&lt;p&gt;Training is done with the Group Sequence Policy Optimization loss (GSPO) [2] using the VeRL framework and rule-based verification.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Evaluation
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Text Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We evaluate Apriel-1.6 on various domains such as tool use, math, coding, instruction following and long context.&lt;/p&gt;

&lt;p&gt;* This score is with DCA enabled. Without this, the model scores 36.&lt;/p&gt;
&lt;p&gt;** The average score is calculated using all benchmarks except BFCL v3 Only and DeepResearchBench, since some models do not have scores for these two benchmarks.&lt;/p&gt;
&lt;p&gt;*** AA LCR score for o3-mini-high is projected score based on its AA Index score.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Image Evaluation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We evaluate the Apriel-1.6 model on a representative set of evaluations with the prime focus on mathematical reasoning, visual question answering, logical reasoning, STEM related tasks and chart based reasoning. All evaluations are done using VLMEvalkit. Apriel-1.6 improves on its predecessor by &lt;strong&gt;4 points&lt;/strong&gt; on the average of 13 benchmarks of the &lt;strong&gt;Image Index&lt;/strong&gt; comprising of the following benchmarks: MathVision, MathVista, MMMU (validation), MMMU-Pro (10 choice COT), MMMU-Pro (Vision only COT), MathVerse (Vision Dominant), MathVerse (Text Dominant), MMStar, BLINK, LogicVista, CharXiV (descriptive), CharXiV (reasoning), AI2D (test).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Performance on the Image Index" src="https://cdn-uploads.huggingface.co/production/uploads/65036ffdb96045f918094fd6/B5PogpJIui7vGsZ7xx-jH.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Cost-Efficient Frontier Performance
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Intelligence vs Total Parameters (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/q2Y4Xjp9Sy5dma9wFt7Ny.png" /&gt;&lt;/p&gt;
&lt;p&gt;Apriel-1.6-15B-Thinker sits in the sweet spot of the cost-efficient frontier. It delivers intelligence scores that rival or surpass much larger models while using only 15B parameters. On the chart, it’s firmly inside the &lt;em&gt;&lt;strong&gt;most attractive&lt;/strong&gt;&lt;/em&gt; quadrant, balancing efficiency with top-tier reasoning. In practice, this means Apriel-1.6-15B-Thinker offers strong performance and deep reasoning at a fraction of the compute and deployment cost of heavyweight competitors, making it an exceptionally efficient choice for the real-world, especially in enterprise applications.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Intelligence vs Output Tokens Used in Artificial Analysis Intelligence Index (30 Nov '25)" src="https://cdn-uploads.huggingface.co/production/uploads/614cf0be2c0ca05fbc33a827/QoymhuGcJf6lFAIruUQ2p.png" /&gt;&lt;/p&gt;
&lt;p&gt;Our post-training focuses heavily on improving reasoning-token efficiency. The image above showing intelligence score against token usage highlights the effectiveness of our post-training. Apriel-1.6-15B-Thinker again lands in &lt;em&gt;&lt;strong&gt;most attractive&lt;/strong&gt;&lt;/em&gt; quadrant. The model reaches a high Artificial Analysis Intelligence Index score while using far fewer tokens than many similarly capable or larger models. In comparison to Apriel-1.5-15b-Thinker [1], we reduce token usage by over 30%. &lt;/p&gt;
&lt;p&gt;Overall, Apriel-1.6 is a highly-capable reasoner, that maintains memory and efficiency characteristics required for enterprise deployment.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Acknowledgements
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We gratefully acknowledge the following people for their contributions: Varun Pandey, Shashank Maiya, Dhruv Jhamb, Massimo Caccia, Dheeraj Vattikonda, Nicolas Gontier, Patrice Bechard, Tayfun Tuna, Kavya Sriram, Denis Akhiyarov, Hari Subramani, Tara Bogavelli.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Notes and Limitations
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We are a small lab with big goals. While we are not GPU poor, our lab, in comparison has a tiny fraction of the compute available to other Frontier labs. Our goal with this work is to show that a SOTA model can be built with limited resources if you have the right data, design and solid methodology.&lt;/p&gt;
&lt;p&gt;We set out to build a small but powerful model, aiming for capabilities on par with frontier models. Developing a 15B model with this level of performance requires tradeoffs, so we prioritized getting SOTA-level performance and improving reasoning token efficiency.&lt;/p&gt;
&lt;p&gt;This model is trained to perform extensive reasoning for difficult questions and less reasoning effort for simpler questions. We are always actively working to make our models more efficient and concise in future releases.&lt;/p&gt;
&lt;p&gt;The model has a few vision-related limitations to be aware of. Complex or low-quality images can reduce OCR accuracy, dense scenes (like crowds or many similar objects) can make subtle details and counting more challenging, and highly detailed or unusually formatted charts may occasionally lead to imperfect interpretations. It may also be less precise with fine-grained visual grounding, so bounding-box predictions can sometimes be approximate or inconsistent.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		References
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;[1] Radhakrishna, S., Tiwari, A., Shukla, A., Hashemi, M., Maheshwary, R., Malay, S.K.R., Mehta, J., Pattnaik, P., Mittal, S., Slimi, K., Ogueji, K., Oladipo, A., Parikh, S., Bamgbose, O., Liang, T., Masry, A., Mahajan, K., Mudumba, S.R., Yadav, V., Madhusudhan, S.T., Scholak, T., Davasam, S., Sunkara, S. and Chapados, N., 2025. Apriel-1.5-15b-Thinker. arXiv preprint arXiv:2510.01141.&lt;/p&gt;
&lt;p&gt;[2] Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J. and Lin, J., 2025. Group Sequence Policy Optimization. arXiv preprint arXiv:2507.18071.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/ServiceNow-AI/apriel-1p6-15b-thinker</guid><pubDate>Tue, 09 Dec 2025 20:06:56 +0000</pubDate></item><item><title>[NEW] Rivian is building its own AI assistant (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/rivian-is-building-its-own-ai-assistant/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/250522-ANASTASIA-BENSON-GOOGLE-MAPS-AEB08056-FINAL.jpg?resize=1200,790" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rivian has spent nearly two years building its own AI assistant, an effort that remains separate from its multibillion-dollar technology joint venture with Volkswagen, TechCrunch has learned.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rivian hasn’t revealed when it will put the AI assistant in consumers hands. However, in an interview earlier this year, Rivian’s software chief Wassym Bensaid told TechCrunch it was targeting the end of the year. The company will likely share more during its upcoming AI &amp;amp; Autonomy Day, which will be livestreamed starting at 9 a.m. PT December 11.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rivian’s plans are reflective of the moment as the pace of development from foundational AI companies — the tech giants and startups like Anthropic, Google, Microsoft, Meta, and OpenAI that are building the core models and infrastructure — accelerates and industries scramble to keep up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as Bensaid noted to TechCrunch earlier this year, this isn’t some slapdash effort to stay on trend. Nor is it simply a chatbot thrown into the infotainment system. The company has put considerable thought, resources, and time into the product, Bensaid said, noting that it’s designed to be integrated with all vehicle controls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company started with an underlying philosophy to build an overall architecture that is model and platform agnostic, according to Bensaid. The Rivian AI assistant team, which is based out of the company’s Palo Alto office, soon realized effort and attention should also be directed toward developing the software layers that help coordinate various workflows as well as the control logic that resolves conflicts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And that’s the in-vehicle platform we have built,” Bensaid said. “We use what the industry loves to now call an agentic framework; but we thought about that architecture since very early so that we can interface with different models.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The in-house AI assistant program is consistent with Rivian’s push to become more vertically integrated. In 2024, Rivian overhauled its flagship R1T truck and R1S SUV, changing everything from the battery pack and suspension system to the electrical architecture, sensor stack, and software user interface. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also put considerable resources toward developing and improving its own software stack, which includes everything related to real-time operating systems (RTOS) that manage the car, such as thermal dynamics, ADAS, and safety systems, as well as another layer related to the infotainment system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bensaid didn’t provide detailed information about the AI assistant, but he did say it includes a mix of models that handle specific tasks. The result is a hybrid software stack that combines edge AI, where tasks are handled on the device, and cloud AI, in which large models that require more compute are handled by remote servers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This should mean a flexible, customized AI assistant that splits the workload between the edge and cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rivian developed much of the AI software stack in-house, including its own custom models and the “orchestration layer,” the conductor or traffic cop of sorts that makes sure the various AI models work together. Rivian tapped other companies for specific agentic AI functions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The mission is to develop an AI assistant that increases customer trust and engagement, Bensaid said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, the AI assistant is staying within Rivian. The company’s joint venture with Volkswagen is focused on software, but not an AI assistant or anything to do with automated driving.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The technology joint venture with Volkswagen, which was announced in 2024 and is worth up to $5.8 billion, is centered on the underlying electrical architecture and zonal compute, and infotainment. The joint venture officially kicked off in November 2024 and is expected to supply the electrical architecture and software for Volkswagen Group as early as 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autonomy and AI are separate for now, but “it doesn’t mean that it may not be in the future,” Bensaid said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/250522-ANASTASIA-BENSON-GOOGLE-MAPS-AEB08056-FINAL.jpg?resize=1200,790" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Rivian has spent nearly two years building its own AI assistant, an effort that remains separate from its multibillion-dollar technology joint venture with Volkswagen, TechCrunch has learned.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rivian hasn’t revealed when it will put the AI assistant in consumers hands. However, in an interview earlier this year, Rivian’s software chief Wassym Bensaid told TechCrunch it was targeting the end of the year. The company will likely share more during its upcoming AI &amp;amp; Autonomy Day, which will be livestreamed starting at 9 a.m. PT December 11.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rivian’s plans are reflective of the moment as the pace of development from foundational AI companies — the tech giants and startups like Anthropic, Google, Microsoft, Meta, and OpenAI that are building the core models and infrastructure — accelerates and industries scramble to keep up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as Bensaid noted to TechCrunch earlier this year, this isn’t some slapdash effort to stay on trend. Nor is it simply a chatbot thrown into the infotainment system. The company has put considerable thought, resources, and time into the product, Bensaid said, noting that it’s designed to be integrated with all vehicle controls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company started with an underlying philosophy to build an overall architecture that is model and platform agnostic, according to Bensaid. The Rivian AI assistant team, which is based out of the company’s Palo Alto office, soon realized effort and attention should also be directed toward developing the software layers that help coordinate various workflows as well as the control logic that resolves conflicts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And that’s the in-vehicle platform we have built,” Bensaid said. “We use what the industry loves to now call an agentic framework; but we thought about that architecture since very early so that we can interface with different models.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The in-house AI assistant program is consistent with Rivian’s push to become more vertically integrated. In 2024, Rivian overhauled its flagship R1T truck and R1S SUV, changing everything from the battery pack and suspension system to the electrical architecture, sensor stack, and software user interface. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also put considerable resources toward developing and improving its own software stack, which includes everything related to real-time operating systems (RTOS) that manage the car, such as thermal dynamics, ADAS, and safety systems, as well as another layer related to the infotainment system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bensaid didn’t provide detailed information about the AI assistant, but he did say it includes a mix of models that handle specific tasks. The result is a hybrid software stack that combines edge AI, where tasks are handled on the device, and cloud AI, in which large models that require more compute are handled by remote servers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This should mean a flexible, customized AI assistant that splits the workload between the edge and cloud.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rivian developed much of the AI software stack in-house, including its own custom models and the “orchestration layer,” the conductor or traffic cop of sorts that makes sure the various AI models work together. Rivian tapped other companies for specific agentic AI functions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The mission is to develop an AI assistant that increases customer trust and engagement, Bensaid said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, the AI assistant is staying within Rivian. The company’s joint venture with Volkswagen is focused on software, but not an AI assistant or anything to do with automated driving.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The technology joint venture with Volkswagen, which was announced in 2024 and is worth up to $5.8 billion, is centered on the underlying electrical architecture and zonal compute, and infotainment. The joint venture officially kicked off in November 2024 and is expected to supply the electrical architecture and software for Volkswagen Group as early as 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autonomy and AI are separate for now, but “it doesn’t mean that it may not be in the future,” Bensaid said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/rivian-is-building-its-own-ai-assistant/</guid><pubDate>Tue, 09 Dec 2025 20:11:13 +0000</pubDate></item><item><title>[NEW] Why Cursor’s CEO believes OpenAI, Anthropic competition won’t crush his startup (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/why-cursors-ceo-believes-openai-anthropic-competition-wont-crush-his-startup/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Anysphere-Cursor-Michael-Truell-.png?resize=1200,745" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anysphere, the company that makes AI coding assistant darling Cursor, isn’t thinking about an IPO any time soon, its co-founder CEO Michael Truell said onstage Monday at Fortune’s AI Brainstorm conference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After reaching $1 billion in annualized revenue in November and raising $2.3 billion at a $29.3 billion valuation last month, Truell said his company is instead focused on building out more features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, he noted that Cursor’s homegrown LLMs were geared to support specific products. Cursor also confirmed the existence of those models in November when it said in a blog post, “Our in-house models now generate more code than almost any other LLMs in the world.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His comments about the models came up at Fortune’s event when the founder was asked how he plans to compete with the LLM makers that he relies on when the major ones — OpenAI, Anthropic — have their own AI coding offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Truell likened their coding products to “a concept car,” whereas his product is a production automobile.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would be like taking an engine and a concept car around it instead of a whole end-to-end car that was manufactured,” Truell said. “What we do is we take the best intelligence that the market has to offer from many different providers. And we also do our own product-specific models in places. We take that, we build it together and integrate it, then also build the best tool and end UX for working with AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor’s dependence on its competitors — and its need to build its own LLMs — has been a subject of speculation among VCs in Silicon Valley since earlier this year when OpenAI reportedly looked at Anysphere as an acquisition target. Anysphere turned the idea down. (This was around the same time that Windsurf’s OpenAI deal also didn’t materialize, with the founder eventually joining Google.)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The issue, investors told TechCrunch, was that AI coding editors were losing money thanks to high costs they paid to the model makers. In Cursor’s case, instead of selling, it adjusted pricing to a usage model in July, directly passing along the API fees that model makers charge to its users. This change from an all-inclusive subscription fee (and the surprise big bills some customers faced) caused an uproar among some of its users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, when asked about the pricing kerfuffle, Truell said, “When we started Cursor, you would turn to Cursor for a quick JavaScript question and now you’re turning to it to do hours of work for you. So the pricing model had to shift for us and others in the space. That means shifting more towards a consumption model,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Truell added that one of the tools the company is working on is cloud-computing-like cost-management tools, which lets enterprises monitor their total usage and keep tabs on the bills their engineers are running up.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We have a whole team internally dedicated to enterprise engineering and building things like spend controls and billing groups and visibility,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, he said Cursor is focused on two major areas for the next year. One is handling more complex agentic functions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want you to take end-to-end tasks, ones that are concise to specify but then are really hard to do, and have them entirely be done by Cursor. An example is a bug fix,” Truell explained. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He particularly wants Cursor to be able to fix the kinds of bugs that might be easy to describe but take “weeks of someone’s time, thousands of times running the code” to handle. “We want Cursor to do that, end-to-end,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The other area he named, but didn’t explain with much detail, was the idea of “thinking about teams as the atomic unit that we serve,” he said. This must be in contrast to serving individual coders, and a hint to how well its enterprise business is going. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to cost-monitoring features, Truell said he wants Cursor to handle more parts of the software development life cycle outside of writing code. He pointed to Cursor’s code review product as an example, which he said is being used by some customers to analyze every pull request, be it written by AI or human. (A pull request is when a programmer submits code for review before it is merged into the main project.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So you’ll see us start to help teams more as a whole,” with more features like that, he promised.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, big competitors are also all gearing up for the complex-task agentic world. Amazon just released a coding tool it promises can already run for days on end. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Just this week, the AI power players, including Anthropic, OpenAI, Microsoft, AWS, and many others, launched a new consortium under the Linux Foundation to develop open source agentic interoperability standards. They even contributed some of their key projects, like Anthropic’s wildly popular Model Context Protocol (MCP).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His plans for the year likely won’t put Anysphere firmly ahead of Cursor’s main model-maker competitors. They should, however, keep the company in the race.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Anysphere-Cursor-Michael-Truell-.png?resize=1200,745" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anysphere, the company that makes AI coding assistant darling Cursor, isn’t thinking about an IPO any time soon, its co-founder CEO Michael Truell said onstage Monday at Fortune’s AI Brainstorm conference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After reaching $1 billion in annualized revenue in November and raising $2.3 billion at a $29.3 billion valuation last month, Truell said his company is instead focused on building out more features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, he noted that Cursor’s homegrown LLMs were geared to support specific products. Cursor also confirmed the existence of those models in November when it said in a blog post, “Our in-house models now generate more code than almost any other LLMs in the world.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His comments about the models came up at Fortune’s event when the founder was asked how he plans to compete with the LLM makers that he relies on when the major ones — OpenAI, Anthropic — have their own AI coding offerings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Truell likened their coding products to “a concept car,” whereas his product is a production automobile.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would be like taking an engine and a concept car around it instead of a whole end-to-end car that was manufactured,” Truell said. “What we do is we take the best intelligence that the market has to offer from many different providers. And we also do our own product-specific models in places. We take that, we build it together and integrate it, then also build the best tool and end UX for working with AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor’s dependence on its competitors — and its need to build its own LLMs — has been a subject of speculation among VCs in Silicon Valley since earlier this year when OpenAI reportedly looked at Anysphere as an acquisition target. Anysphere turned the idea down. (This was around the same time that Windsurf’s OpenAI deal also didn’t materialize, with the founder eventually joining Google.)&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The issue, investors told TechCrunch, was that AI coding editors were losing money thanks to high costs they paid to the model makers. In Cursor’s case, instead of selling, it adjusted pricing to a usage model in July, directly passing along the API fees that model makers charge to its users. This change from an all-inclusive subscription fee (and the surprise big bills some customers faced) caused an uproar among some of its users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, when asked about the pricing kerfuffle, Truell said, “When we started Cursor, you would turn to Cursor for a quick JavaScript question and now you’re turning to it to do hours of work for you. So the pricing model had to shift for us and others in the space. That means shifting more towards a consumption model,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Truell added that one of the tools the company is working on is cloud-computing-like cost-management tools, which lets enterprises monitor their total usage and keep tabs on the bills their engineers are running up.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We have a whole team internally dedicated to enterprise engineering and building things like spend controls and billing groups and visibility,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, he said Cursor is focused on two major areas for the next year. One is handling more complex agentic functions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want you to take end-to-end tasks, ones that are concise to specify but then are really hard to do, and have them entirely be done by Cursor. An example is a bug fix,” Truell explained. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He particularly wants Cursor to be able to fix the kinds of bugs that might be easy to describe but take “weeks of someone’s time, thousands of times running the code” to handle. “We want Cursor to do that, end-to-end,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The other area he named, but didn’t explain with much detail, was the idea of “thinking about teams as the atomic unit that we serve,” he said. This must be in contrast to serving individual coders, and a hint to how well its enterprise business is going. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to cost-monitoring features, Truell said he wants Cursor to handle more parts of the software development life cycle outside of writing code. He pointed to Cursor’s code review product as an example, which he said is being used by some customers to analyze every pull request, be it written by AI or human. (A pull request is when a programmer submits code for review before it is merged into the main project.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So you’ll see us start to help teams more as a whole,” with more features like that, he promised.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, big competitors are also all gearing up for the complex-task agentic world. Amazon just released a coding tool it promises can already run for days on end. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Just this week, the AI power players, including Anthropic, OpenAI, Microsoft, AWS, and many others, launched a new consortium under the Linux Foundation to develop open source agentic interoperability standards. They even contributed some of their key projects, like Anthropic’s wildly popular Model Context Protocol (MCP).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His plans for the year likely won’t put Anysphere firmly ahead of Cursor’s main model-maker competitors. They should, however, keep the company in the race.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/why-cursors-ceo-believes-openai-anthropic-competition-wont-crush-his-startup/</guid><pubDate>Tue, 09 Dec 2025 20:59:21 +0000</pubDate></item><item><title>[NEW] Big Tech joins forces with Linux Foundation to standardize AI agents (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/big-tech-joins-forces-with-linux-foundation-to-standardize-ai-agents/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Agentic AI Foundation launches to support MCP, AGENTS.md, and goose.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      It's thinking, but not in words.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Big Tech has spent the past year telling us we’re living in the era of AI agents, but most of what we’ve been promised is still theoretical. As companies race to turn fantasy into reality, they’ve developed a collection of tools to guide the development of generative AI. A cadre of major players in the AI race, including Anthropic, Block, and OpenAI, has come together to promote interoperability with the newly formed Agentic AI Foundation (AAIF). This move elevates a handful of popular technologies and could make them a de facto standard for AI development going forward.&lt;/p&gt;
&lt;p&gt;The development path for agentic AI models is cloudy to say the least, but companies have invested so heavily in creating these systems that some tools have percolated to the surface. The AAIF, which is part of the nonprofit Linux Foundation, has been launched to govern the development of three key AI technologies: Model Context Protocol (MCP), goose, and AGENTS.md.&lt;/p&gt;
&lt;p&gt;MCP is probably the most well-known of the trio, having been open-sourced by Anthropic a year ago. The goal of MCP is to link AI agents to data sources in a standardized way—Anthropic (and now the AAIF) is fond of calling MCP a “USB-C port for AI.” Rather than creating custom integrations for every different database or cloud storage platform, MCP allows developers to quickly and easily connect to any MCP-compliant server.&lt;/p&gt;
&lt;p&gt;Since its release, MCP has been widely used across the AI industry. Google announced at I/O 2025 that it was adding support for MCP in its dev tools, and many of its products have since added MCP servers to make data more accessible to agents. OpenAI also adopted MCP just a few months after it was released.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131122 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="mcp simple diagram" class="none medium" height="250" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/mcp-simple-diagram-640x250.png" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Anthropic

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Expanding use of MCP might help users customize their AI experience. For instance, the new Pebble Index 01 ring uses a local LLM that can act on your voice notes, and it supports MCP for user customization.&lt;/p&gt;
&lt;p&gt;Local AI models have to make some sacrifices compared to bigger cloud-based models, but MCP can fill in the functionality gaps. “A lot of tasks on productivity and content are fully doable on the edge,” Qualcomm head of AI products, Vinesh Sukumar, tells Ars. “With MCP, you have a handshake with multiple cloud service providers for any kind of complex task to be completed.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The Model Context Protocol is the most well-established of the AAIF’s new charges. Goose, which was contributed to the project by Square owner Block, launched in early 2025. This is a customizable open source agent for coding. It’s designed to run locally or in the cloud and can use any LLM you choose. It also has built-in support for MCP.&lt;/p&gt;
&lt;p&gt;Meanwhile, AGENTS.md comes from OpenAI, and it’s also a very recent arrival in the AI sphere. OpenAI announced the tool this past August, and now it’s also part of the AAIF. AGENTS.md is essentially a markdown-based readme for AI coding agents to guide their behavior in more predictable ways.&lt;/p&gt;
&lt;h2&gt;Moving fast&lt;/h2&gt;
&lt;p&gt;Think about the timeline here. The world in which tech companies operate has changed considerably in a short time as everyone rushes to stuff gen AI into every product and process. And no one knows who is on the right track—maybe no one!&lt;/p&gt;
&lt;p&gt;Against that backdrop, big tech has seemingly decided to standardize. Even for MCP, the most widely supported of these tools, there’s still considerable flux in how basic technologies like OAuth will be handled.&lt;/p&gt;
&lt;p&gt;The Linux Foundation has spun up numerous projects to support neutral and interoperable development of key technologies. For example, it formed the Cloud Native Computing Foundation (CNCF) in 2015 to support Google’s open Kubernetes cluster manager, but the project has since integrated a few dozen cloud computing tools. Certification and training for these tools help keep the lights on at the foundation, but Kubernetes was already a proven technology when Google released it widely. All these AI technologies are popular right now, sure, but is MCP or AGENTS.md going to be important in the long term?&lt;/p&gt;
&lt;p&gt;Regardless, everyone in the AI industry seems to be on board. In addition to the companies adding their tools to the project, the AAIF has support from Amazon, Google, Cloudflare, Microsoft, and others. The Linux Foundation says it intends to shepherd these key technologies forward in the name of openness, but it may end up collecting a lot of nascent AI tools at this rate.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Agentic AI Foundation launches to support MCP, AGENTS.md, and goose.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      It's thinking, but not in words.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Big Tech has spent the past year telling us we’re living in the era of AI agents, but most of what we’ve been promised is still theoretical. As companies race to turn fantasy into reality, they’ve developed a collection of tools to guide the development of generative AI. A cadre of major players in the AI race, including Anthropic, Block, and OpenAI, has come together to promote interoperability with the newly formed Agentic AI Foundation (AAIF). This move elevates a handful of popular technologies and could make them a de facto standard for AI development going forward.&lt;/p&gt;
&lt;p&gt;The development path for agentic AI models is cloudy to say the least, but companies have invested so heavily in creating these systems that some tools have percolated to the surface. The AAIF, which is part of the nonprofit Linux Foundation, has been launched to govern the development of three key AI technologies: Model Context Protocol (MCP), goose, and AGENTS.md.&lt;/p&gt;
&lt;p&gt;MCP is probably the most well-known of the trio, having been open-sourced by Anthropic a year ago. The goal of MCP is to link AI agents to data sources in a standardized way—Anthropic (and now the AAIF) is fond of calling MCP a “USB-C port for AI.” Rather than creating custom integrations for every different database or cloud storage platform, MCP allows developers to quickly and easily connect to any MCP-compliant server.&lt;/p&gt;
&lt;p&gt;Since its release, MCP has been widely used across the AI industry. Google announced at I/O 2025 that it was adding support for MCP in its dev tools, and many of its products have since added MCP servers to make data more accessible to agents. OpenAI also adopted MCP just a few months after it was released.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2131122 align-none"&gt;
    &lt;div&gt;
                        &lt;img alt="mcp simple diagram" class="none medium" height="250" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/mcp-simple-diagram-640x250.png" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Anthropic

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Expanding use of MCP might help users customize their AI experience. For instance, the new Pebble Index 01 ring uses a local LLM that can act on your voice notes, and it supports MCP for user customization.&lt;/p&gt;
&lt;p&gt;Local AI models have to make some sacrifices compared to bigger cloud-based models, but MCP can fill in the functionality gaps. “A lot of tasks on productivity and content are fully doable on the edge,” Qualcomm head of AI products, Vinesh Sukumar, tells Ars. “With MCP, you have a handshake with multiple cloud service providers for any kind of complex task to be completed.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The Model Context Protocol is the most well-established of the AAIF’s new charges. Goose, which was contributed to the project by Square owner Block, launched in early 2025. This is a customizable open source agent for coding. It’s designed to run locally or in the cloud and can use any LLM you choose. It also has built-in support for MCP.&lt;/p&gt;
&lt;p&gt;Meanwhile, AGENTS.md comes from OpenAI, and it’s also a very recent arrival in the AI sphere. OpenAI announced the tool this past August, and now it’s also part of the AAIF. AGENTS.md is essentially a markdown-based readme for AI coding agents to guide their behavior in more predictable ways.&lt;/p&gt;
&lt;h2&gt;Moving fast&lt;/h2&gt;
&lt;p&gt;Think about the timeline here. The world in which tech companies operate has changed considerably in a short time as everyone rushes to stuff gen AI into every product and process. And no one knows who is on the right track—maybe no one!&lt;/p&gt;
&lt;p&gt;Against that backdrop, big tech has seemingly decided to standardize. Even for MCP, the most widely supported of these tools, there’s still considerable flux in how basic technologies like OAuth will be handled.&lt;/p&gt;
&lt;p&gt;The Linux Foundation has spun up numerous projects to support neutral and interoperable development of key technologies. For example, it formed the Cloud Native Computing Foundation (CNCF) in 2015 to support Google’s open Kubernetes cluster manager, but the project has since integrated a few dozen cloud computing tools. Certification and training for these tools help keep the lights on at the foundation, but Kubernetes was already a proven technology when Google released it widely. All these AI technologies are popular right now, sure, but is MCP or AGENTS.md going to be important in the long term?&lt;/p&gt;
&lt;p&gt;Regardless, everyone in the AI industry seems to be on board. In addition to the companies adding their tools to the project, the AAIF has support from Amazon, Google, Cloudflare, Microsoft, and others. The Linux Foundation says it intends to shepherd these key technologies forward in the name of openness, but it may end up collecting a lot of nascent AI tools at this rate.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/big-tech-joins-forces-with-linux-foundation-to-standardize-ai-agents/</guid><pubDate>Tue, 09 Dec 2025 21:08:11 +0000</pubDate></item><item><title>[NEW] B Capital founding partner Kabir Narang leaves to launch new investment platform (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/b-capital-founding-partner-kabir-narang-leaves-to-launch-new-investment-platform/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/kabir-narang-b-capital-GettyImages-1027568616.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kabir Narang, a founding general partner at B Capital and an early backer of several Indian startups, has left the global venture firm, TechCrunch has learned and confirmed with the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Narang is laying the groundwork for a new investment platform slated for 2026 that will focus on “compounding at the intersection of technology, AI, and global capital flows,” per a note shared with founders and reviewed by TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After joining B Capital in March 2017, Narang co-led the firm’s Asia strategy from Singapore and chaired its global investment committee. During his tenure, he backed Indian startups such as Meesho, Khatabook, CredAvenue, Bounce, and Bizongo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are living through one of the most profound technological revolutions in history, and one of the toughest tests of investor discipline,” Narang wrote. “AI scales thought itself, compressing the gap between idea and output. The founders who pair that speed with pricing power and improving unit economics will define the next generation of enduring value.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside developing the new investment platform, Narang told founders that he is taking 1% to 2% personal stakes in companies he believes can “compound intelligently.” This suggests he plans to stay active in early-stage investing while setting up a broader vehicle.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;B Capital confirmed Narang’s exit to TechCrunch and noted that Eduardo Saverin, Karan Mohla, and Howard Morgan would manage its Asia portfolio alongside the existing team in South and Southeast Asia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“After more than eight years with the firm, Kabir Narang, who focused on later stage growth investing efforts in Asia, has left his role to pursue other opportunities,” a B Capital spokesperson said. “We are grateful for his contributions, and we wish him continued success in the future.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2015 by Facebook co-founder Eduardo Saverin and former Bain Capital executive Raj Ganguly, B Capital is a multi-stage investor focused on technology, healthcare, and resilience tech. The San Francisco–based firm manages more than $9 billion across nine offices in the U.S. and Asia. Through a partnership with Boston Consulting Group, B Capital also provides portfolio companies with strategic and operational support.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before joining B Capital, Narang spent nearly nine years at Fidelity-backed Eight Roads Ventures India, where he was a managing director.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“B Capital remains deeply committed to our strategy in Asia and our broader global platform,” the B Capital spokesperson said. “With strong leadership and an experienced team across the region, we are well-positioned to capitalize on the next wave of innovation and continue backing category-defining companies across our core markets.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Narang did not respond to a request for comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/kabir-narang-b-capital-GettyImages-1027568616.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Kabir Narang, a founding general partner at B Capital and an early backer of several Indian startups, has left the global venture firm, TechCrunch has learned and confirmed with the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Narang is laying the groundwork for a new investment platform slated for 2026 that will focus on “compounding at the intersection of technology, AI, and global capital flows,” per a note shared with founders and reviewed by TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;After joining B Capital in March 2017, Narang co-led the firm’s Asia strategy from Singapore and chaired its global investment committee. During his tenure, he backed Indian startups such as Meesho, Khatabook, CredAvenue, Bounce, and Bizongo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are living through one of the most profound technological revolutions in history, and one of the toughest tests of investor discipline,” Narang wrote. “AI scales thought itself, compressing the gap between idea and output. The founders who pair that speed with pricing power and improving unit economics will define the next generation of enduring value.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside developing the new investment platform, Narang told founders that he is taking 1% to 2% personal stakes in companies he believes can “compound intelligently.” This suggests he plans to stay active in early-stage investing while setting up a broader vehicle.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;B Capital confirmed Narang’s exit to TechCrunch and noted that Eduardo Saverin, Karan Mohla, and Howard Morgan would manage its Asia portfolio alongside the existing team in South and Southeast Asia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“After more than eight years with the firm, Kabir Narang, who focused on later stage growth investing efforts in Asia, has left his role to pursue other opportunities,” a B Capital spokesperson said. “We are grateful for his contributions, and we wish him continued success in the future.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2015 by Facebook co-founder Eduardo Saverin and former Bain Capital executive Raj Ganguly, B Capital is a multi-stage investor focused on technology, healthcare, and resilience tech. The San Francisco–based firm manages more than $9 billion across nine offices in the U.S. and Asia. Through a partnership with Boston Consulting Group, B Capital also provides portfolio companies with strategic and operational support.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before joining B Capital, Narang spent nearly nine years at Fidelity-backed Eight Roads Ventures India, where he was a managing director.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“B Capital remains deeply committed to our strategy in Asia and our broader global platform,” the B Capital spokesperson said. “With strong leadership and an experienced team across the region, we are well-positioned to capitalize on the next wave of innovation and continue backing category-defining companies across our core markets.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Narang did not respond to a request for comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/b-capital-founding-partner-kabir-narang-leaves-to-launch-new-investment-platform/</guid><pubDate>Tue, 09 Dec 2025 21:13:19 +0000</pubDate></item><item><title>[NEW] Max Hodak is more worried about Twitter than brain-computer interface hacking (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/max-hodak-is-more-worried-about-twitter-than-brain-computer-interface-hacking/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-2.45.47-AM.png?resize=1200,852" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;This week on StrictlyVC Download, Connie Loizos speaks with Science Corp. founder Max Hodak to discuss how brain-computer interfaces are arriving faster than anyone realizes. The Neuralink co-founder and former president shares how his company recently achieved what may be the biggest breakthrough in vision restoration in decades, enabling 80% of blind patients to read again with a tiny retinal implant smaller than a grain of rice. In this conversation, the two also explore the near-term commercial path for BCIs through medical applications, the long-term potential for cognitive enhancement, and “binding” multiple brains together, and why Science, which has so far raised $260 million from investors, is keen to generate revenue while it invests in its future products. Not last, Hodak addresses the practical and ethical questions around hacking, enhancement, and why he thinks it may well be possible in the not-too-distant future to “move consciousness” outside of the body.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;StrictlyVC Download posts every Tuesday. Subscribe on &lt;/em&gt;&lt;strong&gt;&lt;em&gt;Apple&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;, &lt;/em&gt;&lt;strong&gt;&lt;em&gt;Spotify&lt;/em&gt;&lt;/strong&gt;, &lt;em&gt;or &lt;/em&gt;&lt;strong&gt;&lt;em&gt;wherever you listen to podcasts&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;to be alerted when new episodes drop.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-2.45.47-AM.png?resize=1200,852" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;This week on StrictlyVC Download, Connie Loizos speaks with Science Corp. founder Max Hodak to discuss how brain-computer interfaces are arriving faster than anyone realizes. The Neuralink co-founder and former president shares how his company recently achieved what may be the biggest breakthrough in vision restoration in decades, enabling 80% of blind patients to read again with a tiny retinal implant smaller than a grain of rice. In this conversation, the two also explore the near-term commercial path for BCIs through medical applications, the long-term potential for cognitive enhancement, and “binding” multiple brains together, and why Science, which has so far raised $260 million from investors, is keen to generate revenue while it invests in its future products. Not last, Hodak addresses the practical and ethical questions around hacking, enhancement, and why he thinks it may well be possible in the not-too-distant future to “move consciousness” outside of the body.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;StrictlyVC Download posts every Tuesday. Subscribe on &lt;/em&gt;&lt;strong&gt;&lt;em&gt;Apple&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;, &lt;/em&gt;&lt;strong&gt;&lt;em&gt;Spotify&lt;/em&gt;&lt;/strong&gt;, &lt;em&gt;or &lt;/em&gt;&lt;strong&gt;&lt;em&gt;wherever you listen to podcasts&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;to be alerted when new episodes drop.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/max-hodak-is-more-worried-about-twitter-than-brain-computer-interface-hacking/</guid><pubDate>Tue, 09 Dec 2025 21:56:34 +0000</pubDate></item><item><title>[NEW] Cashew Research is going after the $90B market research industry with AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/cashew-research-is-going-after-the-90b-market-research-industry-with-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Cashew-photo.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Market research is a $90 billion industry that helps brands figure out how to best present themselves to potential customers. But that market insight isn’t cheap, nor is it quick. Cashew Research wants to change that using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calgary, Alberta-based Cashew uses AI to develop market research plans and surveys for brands based on what information they are looking for — like what their brand recognition is for a specific population or how a marketing tagline resonates with customers. Cashew then sends the survey to real people and uses AI to summarize and digest the findings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cashew was one of the 200 startups chosen for TechCrunch’s Startup Battlefield competition in 2025 and won the Enterprise Stage pitch competition at TechCrunch Disrupt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can use an LLM to try to do deep research and get answers to your questions, or you could use a firm that’s going to be really expensive,” Addy Graves, co-founder and CEO of Cashew, told TechCrunch in describing the current market research industry. “Now there’s Cashew that exists in the middle. It creates custom, fresh data to answer your question instead of you just using an LLM that’s surfacing the same recycled pool of data that everybody’s finding on the internet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves has more than a decade of market research experience. The original idea for Cashew was sparked by an issue she ran into frequently: Clients wanted full research projects — with real-world data from humans — done within a few days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For years, shortening that timeline while still producing the same quality of research results wasn’t possible, Graves said, because the technology to speed up the process wasn’t ready yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That was definitely the aha moment,” Graves said. “And it wasn’t until the onset of AI that we were actually able to automate these processes that we use as researchers, best practices, these data science-backed methodologies, as well as the formatting of reports that we know that everybody wants.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Bringing automation to the process also brings the cost down, which makes Cashew an option for small and medium-size brands that wouldn’t have been able to afford to work with a traditional market research firm, Graves added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves founded Cashew in 2023 alongside Rose Wong, chief operating officer, with an initial focus on consumer packaged goods, specifically food and beverage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves said she thinks Cashew can stand out in the increasingly crowded AI marketing tools category because it isn’t fully automated. Each Cashew client gets fresh human data with each project, which requires market research expertise, Graves said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cashew’s competitive advantage may only grow as the company matures. The company takes all of the real-world data it collects from its clients’ projects, anonymizes it, and puts it in a database, which can help add additional proprietary data to future research projects, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised C$1.5 million in pre-seed funding and is gearing up to launch its seed round in early 2026 with the hope of raising up to $5 million, Graves said. That capital will be put toward continuing to develop the product’s tech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves said the company’s two main areas of focus heading into next year are increasing the company’s presence in the U.S. and also working to build up its B2B business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The people who are already buying research, that’s already a massive category, but that doesn’t even include all the people that could be buying research but just can’t afford it or can’t do it right now because they don’t have the timelines,” Graves said. “We’re actually creating this new category for marketers to gain access to answers to these questions that they’ve had.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Cashew-photo.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Market research is a $90 billion industry that helps brands figure out how to best present themselves to potential customers. But that market insight isn’t cheap, nor is it quick. Cashew Research wants to change that using AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calgary, Alberta-based Cashew uses AI to develop market research plans and surveys for brands based on what information they are looking for — like what their brand recognition is for a specific population or how a marketing tagline resonates with customers. Cashew then sends the survey to real people and uses AI to summarize and digest the findings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cashew was one of the 200 startups chosen for TechCrunch’s Startup Battlefield competition in 2025 and won the Enterprise Stage pitch competition at TechCrunch Disrupt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can use an LLM to try to do deep research and get answers to your questions, or you could use a firm that’s going to be really expensive,” Addy Graves, co-founder and CEO of Cashew, told TechCrunch in describing the current market research industry. “Now there’s Cashew that exists in the middle. It creates custom, fresh data to answer your question instead of you just using an LLM that’s surfacing the same recycled pool of data that everybody’s finding on the internet.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves has more than a decade of market research experience. The original idea for Cashew was sparked by an issue she ran into frequently: Clients wanted full research projects — with real-world data from humans — done within a few days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For years, shortening that timeline while still producing the same quality of research results wasn’t possible, Graves said, because the technology to speed up the process wasn’t ready yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That was definitely the aha moment,” Graves said. “And it wasn’t until the onset of AI that we were actually able to automate these processes that we use as researchers, best practices, these data science-backed methodologies, as well as the formatting of reports that we know that everybody wants.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Bringing automation to the process also brings the cost down, which makes Cashew an option for small and medium-size brands that wouldn’t have been able to afford to work with a traditional market research firm, Graves added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves founded Cashew in 2023 alongside Rose Wong, chief operating officer, with an initial focus on consumer packaged goods, specifically food and beverage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves said she thinks Cashew can stand out in the increasingly crowded AI marketing tools category because it isn’t fully automated. Each Cashew client gets fresh human data with each project, which requires market research expertise, Graves said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cashew’s competitive advantage may only grow as the company matures. The company takes all of the real-world data it collects from its clients’ projects, anonymizes it, and puts it in a database, which can help add additional proprietary data to future research projects, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised C$1.5 million in pre-seed funding and is gearing up to launch its seed round in early 2026 with the hope of raising up to $5 million, Graves said. That capital will be put toward continuing to develop the product’s tech.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Graves said the company’s two main areas of focus heading into next year are increasing the company’s presence in the U.S. and also working to build up its B2B business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The people who are already buying research, that’s already a massive category, but that doesn’t even include all the people that could be buying research but just can’t afford it or can’t do it right now because they don’t have the timelines,” Graves said. “We’re actually creating this new category for marketers to gain access to answers to these questions that they’ve had.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/cashew-research-is-going-after-the-90b-market-research-industry-with-ai/</guid><pubDate>Tue, 09 Dec 2025 22:20:39 +0000</pubDate></item><item><title>[NEW] Unconventional AI confirms its massive  $475M seed round (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/unconventional-ai-confirms-its-massive-475m-seed-round/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/04/Naveen-Rao-headshot.png?resize=1200,1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Naveen Rao, the former head of AI at Databricks, has raised $475 million in seed capital at a $4.5 billion valuation for his new startup, Unconventional AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Andreessen Horowitz and Lightspeed Ventures, with participation from Lux Capital and DCVC. The funding is a first installment toward the goal of up to $1 billion for the round, Rao told Bloomberg.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch was first to report, back in October, that Unconventional AI was seeking this mega funding for Rao’s new startup, although the final valuation is marginally lower than the $5 billion sources told us he was seeking. If he does eventually raise as much as $1 billion, we’ll see how that impacts his company’s value.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unconventional AI is set on building a new, energy-efficient computer for AI. Rao previously wrote on X that his goal is to create a computer that is “as efficient as biology.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Databricks acquired Rao’s previous startup,&amp;nbsp;MosaicML in 2023, for $1.3 billion. Prior to MosaicML, Rao co-founded the machine learning platform&amp;nbsp;Nervana Systems, which Intel Corp. acquired in 2016 for reportedly more than $400 million.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/04/Naveen-Rao-headshot.png?resize=1200,1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Naveen Rao, the former head of AI at Databricks, has raised $475 million in seed capital at a $4.5 billion valuation for his new startup, Unconventional AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Andreessen Horowitz and Lightspeed Ventures, with participation from Lux Capital and DCVC. The funding is a first installment toward the goal of up to $1 billion for the round, Rao told Bloomberg.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch was first to report, back in October, that Unconventional AI was seeking this mega funding for Rao’s new startup, although the final valuation is marginally lower than the $5 billion sources told us he was seeking. If he does eventually raise as much as $1 billion, we’ll see how that impacts his company’s value.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unconventional AI is set on building a new, energy-efficient computer for AI. Rao previously wrote on X that his goal is to create a computer that is “as efficient as biology.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Databricks acquired Rao’s previous startup,&amp;nbsp;MosaicML in 2023, for $1.3 billion. Prior to MosaicML, Rao co-founded the machine learning platform&amp;nbsp;Nervana Systems, which Intel Corp. acquired in 2016 for reportedly more than $400 million.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/unconventional-ai-confirms-its-massive-475m-seed-round/</guid><pubDate>Wed, 10 Dec 2025 00:24:16 +0000</pubDate></item><item><title>[NEW] Coreweave CEO defends AI circular deals as ‘working together’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/09/coreweave-ceo-defends-ai-circular-deals-as-working-together/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Mike-Intrator-Headshot-e1750958788389.jpg?w=898" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It’s been quite the year for Coreweave. In March, the AI cloud infrastructure provider went public in one of the biggest and most anticipated IPOs of the year that didn’t live up to its hype.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another setback took place in October, when a planned acquisition of the cloud provider’s business partner, Core Scientific, faltered due to skepticism from the acquisition target’s shareholders.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the meantime, the firm has acquired a number of different companies, its stock has gone up and down, and it’s been both criticized and lauded for its role in the booming AI data center market.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview at Fortune’s AI Brainstorm summit in San Francisco on Tuesday, Coreweave’s co-founder and CEO, Michael Intrator, defended his company’s performance from critics, noting that it was in the midst of creating a “new business model” for how cloud computing can be built and run. Their collection of Nvidia GPUs is so valuable, they borrow against it to help finance their business. The executive seemed to imply: If you’re charting a new path, you’re destined to encounter some road bumps along the way.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think people are myopic a lot of times,” Intrator said, when questioned about his company’s occasionally unstable stock price. “Yes, it is see-sawing,” he admitted, while noting that the Coreweave IPO took place not long before President Trump’s tariffs went into effect—a notably uncertain moment for the overall economy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We came out into one of the most challenging environments, right around Liberation Day and, in spite of the incredible headwinds, were able to launch a successful IPO,” the CEO told Brainstorm editorial director Andrew Nusca. “I couldn’t be prouder of what the company has accomplished,” he added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Coreweave’s stock may have debuted amidst the economic doldrums of March but its price has gone on quite the journey since then. It debuted at $40 and, over the past eight months, has climbed to well over $150, but currently rests at around $90. Its more wary critics have compared it to a meme stock due to its penchant for going up and down.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Some of the uncertainty around Coreweave’s stock has been credited to the company’s hefty level of debt. Not long after Coreweave announced a deal on Monday to issue even more debt to finance its data center buildout, its stock dropped some 8 percent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intrator seems to see his company as a disruptor, one whose unconventional tactics may take some getting used to. “When you introduce a new model, when you introduce a new way of doing business, when you disrupt what has been a static environment, it’s going to take some people some time,” he said, during his appearance Tuesday.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Coreweave actually started its corporate life as a crypto-miner but, in short order, built itself into a pivotal provider of “AI infrastructure” to some of the tech industry’s most major players. In that role, it provides GPUs to AI developers, and has made major partnerships with Microsoft, OpenAI, Nvidia, Meta, and other tech titans.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another topic broached Tuesday was the notion of “circularity” within the AI industry. “Circular” business deals, in which a small number of powerful AI companies invest in one another, have frequently been criticized, and have raised questions about the industry’s long term economic stability. Perhaps not surprisingly, since Nvidia is one of its investors as well as its supplier of GPUs, Intrator swatted away such concerns. “Companies are trying to address a violent change in supply and demand,” he said. “You do that by working together.”&lt;br /&gt;&amp;nbsp;&lt;br /&gt;Since the IPO, Coreweave has continued to make efforts to expand its business. After it acquired Weights and Balances, an AI developer platform, in March, it went on to acquire OpenPipe, a startup that helps companies create and deploy AI agents through reinforcement learning. In October, it also made deals to acquire Marimo (the creator of an open source notebook) and Monolith, another AI company. It also recently announced an expansion of its cloud partnership with OpenAI and said it has plans to move into the federal market, where it wants to provide cloud infrastructure to U.S. government agencies and the defense industrial base.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Mike-Intrator-Headshot-e1750958788389.jpg?w=898" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It’s been quite the year for Coreweave. In March, the AI cloud infrastructure provider went public in one of the biggest and most anticipated IPOs of the year that didn’t live up to its hype.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another setback took place in October, when a planned acquisition of the cloud provider’s business partner, Core Scientific, faltered due to skepticism from the acquisition target’s shareholders.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the meantime, the firm has acquired a number of different companies, its stock has gone up and down, and it’s been both criticized and lauded for its role in the booming AI data center market.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview at Fortune’s AI Brainstorm summit in San Francisco on Tuesday, Coreweave’s co-founder and CEO, Michael Intrator, defended his company’s performance from critics, noting that it was in the midst of creating a “new business model” for how cloud computing can be built and run. Their collection of Nvidia GPUs is so valuable, they borrow against it to help finance their business. The executive seemed to imply: If you’re charting a new path, you’re destined to encounter some road bumps along the way.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think people are myopic a lot of times,” Intrator said, when questioned about his company’s occasionally unstable stock price. “Yes, it is see-sawing,” he admitted, while noting that the Coreweave IPO took place not long before President Trump’s tariffs went into effect—a notably uncertain moment for the overall economy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We came out into one of the most challenging environments, right around Liberation Day and, in spite of the incredible headwinds, were able to launch a successful IPO,” the CEO told Brainstorm editorial director Andrew Nusca. “I couldn’t be prouder of what the company has accomplished,” he added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Coreweave’s stock may have debuted amidst the economic doldrums of March but its price has gone on quite the journey since then. It debuted at $40 and, over the past eight months, has climbed to well over $150, but currently rests at around $90. Its more wary critics have compared it to a meme stock due to its penchant for going up and down.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Some of the uncertainty around Coreweave’s stock has been credited to the company’s hefty level of debt. Not long after Coreweave announced a deal on Monday to issue even more debt to finance its data center buildout, its stock dropped some 8 percent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intrator seems to see his company as a disruptor, one whose unconventional tactics may take some getting used to. “When you introduce a new model, when you introduce a new way of doing business, when you disrupt what has been a static environment, it’s going to take some people some time,” he said, during his appearance Tuesday.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Coreweave actually started its corporate life as a crypto-miner but, in short order, built itself into a pivotal provider of “AI infrastructure” to some of the tech industry’s most major players. In that role, it provides GPUs to AI developers, and has made major partnerships with Microsoft, OpenAI, Nvidia, Meta, and other tech titans.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another topic broached Tuesday was the notion of “circularity” within the AI industry. “Circular” business deals, in which a small number of powerful AI companies invest in one another, have frequently been criticized, and have raised questions about the industry’s long term economic stability. Perhaps not surprisingly, since Nvidia is one of its investors as well as its supplier of GPUs, Intrator swatted away such concerns. “Companies are trying to address a violent change in supply and demand,” he said. “You do that by working together.”&lt;br /&gt;&amp;nbsp;&lt;br /&gt;Since the IPO, Coreweave has continued to make efforts to expand its business. After it acquired Weights and Balances, an AI developer platform, in March, it went on to acquire OpenPipe, a startup that helps companies create and deploy AI agents through reinforcement learning. In October, it also made deals to acquire Marimo (the creator of an open source notebook) and Monolith, another AI company. It also recently announced an expansion of its cloud partnership with OpenAI and said it has plans to move into the federal market, where it wants to provide cloud infrastructure to U.S. government agencies and the defense industrial base.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/09/coreweave-ceo-defends-ai-circular-deals-as-working-together/</guid><pubDate>Wed, 10 Dec 2025 00:49:25 +0000</pubDate></item><item><title>[NEW] The AI that scored 95% — until consultants learned it was AI (AI | VentureBeat)</title><link>https://venturebeat.com/ai/the-ai-that-scored-95-until-consultants-learned-it-was-ai</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by SAP&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;When SAP ran a quiet internal experiment to gauge consultant attitudes toward AI, the results were striking. Five teams were asked to validate answers to more than 1,000 business requirements completed by SAP’s AI co-pilot, Joule for Consultants — a workload that would normally take several weeks.&lt;/p&gt;&lt;p&gt;Four teams were told the analysis had been completed by junior interns fresh out of school. They reviewed the material, found it impressive, and rated the work about 95% accurate.&lt;/p&gt;&lt;p&gt;The fifth team was told the very same answers had come from AI.&lt;/p&gt;&lt;p&gt;They rejected almost everything.&lt;/p&gt;&lt;p&gt;Only when asked to validate each answer one by one did they discover that the AI was, in fact, highly accurate — surfacing detailed insights the consultants had initially dismissed. The overall accuracy? Again, about 95%.&lt;/p&gt;&lt;p&gt;“The lesson learned here is that we need to be very cautious as we introduce AI — especially in how we communicate with senior consultants about its possibilities and how to integrate it into their workflows,” says Guillermo B. Vazquez Mendez, chief architect, RI business transformation and architecture, SAP America Inc.&lt;/p&gt;&lt;p&gt;The experiment has since become a revealing starting point for SAP’s push toward the consultant of 2030: a practitioner who is deeply human, enabled by AI, and no longer weighed down by the technical grunt work of the past.&lt;/p&gt;&lt;h3&gt;Overcoming AI skepticism&lt;/h3&gt;&lt;p&gt;Resistance isn’t surprising, Vazquez notes. Consultants with two or three decades of experience carry enormous institutional knowledge — and an understandable degree of caution.&lt;/p&gt;&lt;p&gt;But AI copilots like Joule for Consultants are not replacing expertise. They’re amplifying it.&lt;/p&gt;&lt;p&gt;“What Joule really does is make their very expensive time far more effective,” Vazquez says. “It removes the clerical work, so they can focus on turning out high-quality answers in a fraction of the time.”&lt;/p&gt;&lt;p&gt;He emphasizes this message constantly: “AI is not replacing you. It’s a tool for you. Human oversight is always required. But now, instead of spending your time looking for documentation, you’re gaining significant time and boosting the effectiveness and detail of your answers.”&lt;/p&gt;&lt;h3&gt;The consultant time-shift: from tech execution to business insight&lt;/h3&gt;&lt;p&gt;Historically, consultants spent about 80% of their time understanding technical systems — how processes run, how data flows, how functions execute. Customers, by contrast, spend 80% of their time focused on their business.&lt;/p&gt;&lt;p&gt;That mismatch is exactly where Joule steps in.&lt;/p&gt;&lt;p&gt;“There’s a gap there — and the bridge is AI,” Vazquez says. “It flips the time equation, enabling consultants to invest more of their energy in understanding the customer’s industry and business goals. AI takes on the heavy technical lift, so consultants can focus on driving the right business outcomes.”&lt;/p&gt;&lt;h3&gt;Bringing new consultants up to speed&lt;/h3&gt;&lt;p&gt;AI is also transforming how new hires learn.&lt;/p&gt;&lt;p&gt;“We’re excited to see Joule acting as a bridge between senior consultants, who are adapting more slowly, and interns and new consultants who are already technically savvy,” Vazquez says.&lt;/p&gt;&lt;p&gt;Junior consultants ramp up faster because Joule helps them operate independently. Seniors, meanwhile, engage where their insight matters most.&lt;/p&gt;&lt;p&gt;This is also where many consultants learn the fundamentals of today’s AI copilots. Much of the work depends on prompt engineering — for instance, instructing Joule to act as a senior chief technology architect specializing in finance and SAP S/4HANA 2023, then asking it to analyze business requirements and deliver the output as tables or PowerPoint slides.&lt;/p&gt;&lt;p&gt;Once they grasp how to frame prompts, consultants consistently get higher-quality, more structured answers.&lt;/p&gt;&lt;p&gt;New architects are also able to communicate more clearly with their more experienced counterparts. They know what they don’t know and can ask targeted questions, which makes mentorship far smoother. It’s created a real synergy, Vazquez adds — senior consultants see how quickly new hires are adapting and learning with AI, and that momentum encourages them to keep pace and adopt the technology themselves.&lt;/p&gt;&lt;h3&gt;Looking ahead to the future of AI copilots&lt;/h3&gt;&lt;p&gt;“We’re still in the baby steps of AI — we’re toddlers,” Vazquez says. “Right now, copilots depend on prompt engineering to get good answers. The better you prompt, the better the answer you get.”&lt;/p&gt;&lt;p&gt;But that represents only the earliest phase of what these systems will eventually do. As copilots mature, they’ll move beyond responding to prompts and start interpreting entire business processes — understanding the sequence of steps, identifying where human intervention is needed, and spotting where an AI agent could take over. That shift is what leads directly into agentic AI.&lt;/p&gt;&lt;p&gt;SAP’s depth of process knowledge is what makes that evolution possible. The company has mapped more than 3,500 business processes across industries — a repository Vazquez calls “some of the most valuable, rigorously tested processes developed in the last 50 years.” Every day, SAP systems support roughly $7.3 trillion in global commerce, giving these emerging AI agents a rich foundation to navigate and reason over.&lt;/p&gt;&lt;p&gt;“With that level of process insight and data, we can take a real leap forward,” he says, “equipping our consultants with agentic AI that can solve complex challenges and push us toward increasingly autonomous systems.”&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;
&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by SAP&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;When SAP ran a quiet internal experiment to gauge consultant attitudes toward AI, the results were striking. Five teams were asked to validate answers to more than 1,000 business requirements completed by SAP’s AI co-pilot, Joule for Consultants — a workload that would normally take several weeks.&lt;/p&gt;&lt;p&gt;Four teams were told the analysis had been completed by junior interns fresh out of school. They reviewed the material, found it impressive, and rated the work about 95% accurate.&lt;/p&gt;&lt;p&gt;The fifth team was told the very same answers had come from AI.&lt;/p&gt;&lt;p&gt;They rejected almost everything.&lt;/p&gt;&lt;p&gt;Only when asked to validate each answer one by one did they discover that the AI was, in fact, highly accurate — surfacing detailed insights the consultants had initially dismissed. The overall accuracy? Again, about 95%.&lt;/p&gt;&lt;p&gt;“The lesson learned here is that we need to be very cautious as we introduce AI — especially in how we communicate with senior consultants about its possibilities and how to integrate it into their workflows,” says Guillermo B. Vazquez Mendez, chief architect, RI business transformation and architecture, SAP America Inc.&lt;/p&gt;&lt;p&gt;The experiment has since become a revealing starting point for SAP’s push toward the consultant of 2030: a practitioner who is deeply human, enabled by AI, and no longer weighed down by the technical grunt work of the past.&lt;/p&gt;&lt;h3&gt;Overcoming AI skepticism&lt;/h3&gt;&lt;p&gt;Resistance isn’t surprising, Vazquez notes. Consultants with two or three decades of experience carry enormous institutional knowledge — and an understandable degree of caution.&lt;/p&gt;&lt;p&gt;But AI copilots like Joule for Consultants are not replacing expertise. They’re amplifying it.&lt;/p&gt;&lt;p&gt;“What Joule really does is make their very expensive time far more effective,” Vazquez says. “It removes the clerical work, so they can focus on turning out high-quality answers in a fraction of the time.”&lt;/p&gt;&lt;p&gt;He emphasizes this message constantly: “AI is not replacing you. It’s a tool for you. Human oversight is always required. But now, instead of spending your time looking for documentation, you’re gaining significant time and boosting the effectiveness and detail of your answers.”&lt;/p&gt;&lt;h3&gt;The consultant time-shift: from tech execution to business insight&lt;/h3&gt;&lt;p&gt;Historically, consultants spent about 80% of their time understanding technical systems — how processes run, how data flows, how functions execute. Customers, by contrast, spend 80% of their time focused on their business.&lt;/p&gt;&lt;p&gt;That mismatch is exactly where Joule steps in.&lt;/p&gt;&lt;p&gt;“There’s a gap there — and the bridge is AI,” Vazquez says. “It flips the time equation, enabling consultants to invest more of their energy in understanding the customer’s industry and business goals. AI takes on the heavy technical lift, so consultants can focus on driving the right business outcomes.”&lt;/p&gt;&lt;h3&gt;Bringing new consultants up to speed&lt;/h3&gt;&lt;p&gt;AI is also transforming how new hires learn.&lt;/p&gt;&lt;p&gt;“We’re excited to see Joule acting as a bridge between senior consultants, who are adapting more slowly, and interns and new consultants who are already technically savvy,” Vazquez says.&lt;/p&gt;&lt;p&gt;Junior consultants ramp up faster because Joule helps them operate independently. Seniors, meanwhile, engage where their insight matters most.&lt;/p&gt;&lt;p&gt;This is also where many consultants learn the fundamentals of today’s AI copilots. Much of the work depends on prompt engineering — for instance, instructing Joule to act as a senior chief technology architect specializing in finance and SAP S/4HANA 2023, then asking it to analyze business requirements and deliver the output as tables or PowerPoint slides.&lt;/p&gt;&lt;p&gt;Once they grasp how to frame prompts, consultants consistently get higher-quality, more structured answers.&lt;/p&gt;&lt;p&gt;New architects are also able to communicate more clearly with their more experienced counterparts. They know what they don’t know and can ask targeted questions, which makes mentorship far smoother. It’s created a real synergy, Vazquez adds — senior consultants see how quickly new hires are adapting and learning with AI, and that momentum encourages them to keep pace and adopt the technology themselves.&lt;/p&gt;&lt;h3&gt;Looking ahead to the future of AI copilots&lt;/h3&gt;&lt;p&gt;“We’re still in the baby steps of AI — we’re toddlers,” Vazquez says. “Right now, copilots depend on prompt engineering to get good answers. The better you prompt, the better the answer you get.”&lt;/p&gt;&lt;p&gt;But that represents only the earliest phase of what these systems will eventually do. As copilots mature, they’ll move beyond responding to prompts and start interpreting entire business processes — understanding the sequence of steps, identifying where human intervention is needed, and spotting where an AI agent could take over. That shift is what leads directly into agentic AI.&lt;/p&gt;&lt;p&gt;SAP’s depth of process knowledge is what makes that evolution possible. The company has mapped more than 3,500 business processes across industries — a repository Vazquez calls “some of the most valuable, rigorously tested processes developed in the last 50 years.” Every day, SAP systems support roughly $7.3 trillion in global commerce, giving these emerging AI agents a rich foundation to navigate and reason over.&lt;/p&gt;&lt;p&gt;“With that level of process insight and data, we can take a real leap forward,” he says, “equipping our consultants with agentic AI that can solve complex challenges and push us toward increasingly autonomous systems.”&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;
&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-ai-that-scored-95-until-consultants-learned-it-was-ai</guid><pubDate>Wed, 10 Dec 2025 15:00:00 +0000</pubDate></item></channel></rss>