<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 05 Nov 2025 18:31:33 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>From vibe coding to context engineering: 2025 in software development (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/05/1127477/from-vibe-coding-to-context-engineering-2025-in-software-development/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;Thoughtworks&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;This year, we’ve seen a real-time experiment playing out across the technology industry, one in which AI’s software engineering capabilities have been put to the test against human technologists. And although 2025 may have started with AI looking strong, the transition from vibe coding to what’s being termed context engineering shows that while the work of human developers is evolving, they nevertheless remain absolutely critical.&lt;/p&gt;  &lt;p&gt;This is captured in the latest volume of the “Thoughtworks Technology Radar,” a report on the technologies used by our teams on projects with clients. In it, we see the emergence of techniques and tooling designed to help teams better tackle the problem of managing context when working with LLMs and AI agents.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Taken together, there’s a clear signal of the direction of travel in software engineering and even AI more broadly. After years of the industry assuming progress in AI is all about scale and speed, we’re starting to see that what matters is the ability to handle context effectively.&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127478" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/TR33_MIT_inline_2.jpg?w=845" /&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Vibes, antipatterns, and new innovations&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In February 2025, Andrej Karpathy coined the term vibe coding. It took the industry by storm. It certainly sparked debate at Thoughtworks; many of us were skeptical. On an April episode of our technology podcast, we talked about our concerns and were cautious about how vibe coding might evolve.&lt;/p&gt; 
 &lt;p&gt;Unsurprisingly given the implied imprecision of vibe-based coding, antipatterns have been proliferating. We’ve once again noted, for instance, complacency with AI generated code&lt;strong&gt; &lt;/strong&gt;on the latest volume of the Technology Radar,&lt;strong&gt; &lt;/strong&gt;but it’s also worth pointing out that early ventures into vibe coding also exposed a degree of complacency about what AI models can actually handle — users demanded more and prompts grew larger, but model reliability started to falter.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Experimenting with generative AI&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;This is one of the drivers behind increasing interest in engineering context. We’re well aware of its importance, working with coding assistants like Claude Code and Augment Code.&lt;strong&gt; &lt;/strong&gt;Providing necessary context—or knowledge priming—is crucial. It ensures outputs are more consistent and reliable, which will ultimately lead to better software that needs less work — reducing rewrites and potentially driving productivity.&lt;/p&gt; 
 &lt;p&gt;When effectively prepared, we’ve seen good results when using generative AI to understand legacy codebases. Indeed, done effectively with the appropriate context, it can even help when we don’t have full access to source code.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s important to remember that context isn’t just about more data and more detail. This is one of the lessons we’ve taken from using generative AI for forward engineering. It might sound counterintuitive, but in this scenario, we’ve found AI to be more effective when it’s further abstracted from the underlying system — or, in other words, further removed from the specifics of the legacy code. This is because the solution space becomes much wider, allowing us to better leverage the generative and creative capabilities of the AI models we use.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Context is critical in the agentic era&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The backdrop of changes that have happened over recent months is the growth of agents and agentic systems — both as products organizations want to develop and as technology they want to leverage. This has forced the industry to properly reckon with context and move away from a purely vibes-based approach.&lt;/p&gt;  &lt;p&gt;Indeed, far from simply getting on with tasks they’ve been programmed to do, agents require significant human intervention to ensure they are equipped to respond to complex and dynamic contexts.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;There are a number of context-related technologies aimed at tackling this challenge, including agents.md, Context7, and Mem0. But it’s also a question of approach. For instance, we’ve found success with anchoring coding agents to a reference application&lt;strong&gt; &lt;/strong&gt;— essentially providing agents with a contextual ground truth. We’re also experimenting with using teams of coding agents; while this might sound like it increases complexity, it actually removes some of the burden of having to give a single agent all the dense layers of context it needs to do its job successfully.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Toward consensus&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Hopefully the space will mature as practices and standards embed. It would be remiss to not mention the significance of the Model Context Protocol, which has emerged as the go-to protocol for connecting LLMs or agentic AI to sources of context. Relatedly, the agent2agent (A2A) protocol leads the way with standardizing how agents interact with one another.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It remains to be seen whether these standards win out. But in any case, it’s important to consider the day-to-day practices that allow us, as software engineers and technologists, to collaborate effectively even when dealing with highly complex and dynamic systems. Sure, AI needs context, but so do we. Techniques like curated shared instructions for software teams may not sound like the hottest innovation on the planet, but they can be remarkably powerful for helping teams work together.&lt;/p&gt;  &lt;p&gt;There’s perhaps also a conversation to be had about what these changes mean for agile software development. Spec-driven development is one idea that appears to have some traction, but there are still questions about how we remain adaptable and flexible while also building robust contextual foundations and ground truths for AI systems.&lt;/p&gt; 

 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Software engineers can solve the context challenge&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Clearly, 2025 has been a huge year in the evolution of software engineering as a practice. There’s a lot the industry needs to monitor closely, but it’s also an exciting time. And while fears about AI job automation may remain, the fact the conversation has moved from questions of speed and scale to context puts software engineers right at the heart of things.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Once again, it will be down to them to experiment, collaborate, and learn — the future depends on it.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Thoughtworks. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;Thoughtworks&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;This year, we’ve seen a real-time experiment playing out across the technology industry, one in which AI’s software engineering capabilities have been put to the test against human technologists. And although 2025 may have started with AI looking strong, the transition from vibe coding to what’s being termed context engineering shows that while the work of human developers is evolving, they nevertheless remain absolutely critical.&lt;/p&gt;  &lt;p&gt;This is captured in the latest volume of the “Thoughtworks Technology Radar,” a report on the technologies used by our teams on projects with clients. In it, we see the emergence of techniques and tooling designed to help teams better tackle the problem of managing context when working with LLMs and AI agents.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Taken together, there’s a clear signal of the direction of travel in software engineering and even AI more broadly. After years of the industry assuming progress in AI is all about scale and speed, we’re starting to see that what matters is the ability to handle context effectively.&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127478" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/TR33_MIT_inline_2.jpg?w=845" /&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Vibes, antipatterns, and new innovations&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In February 2025, Andrej Karpathy coined the term vibe coding. It took the industry by storm. It certainly sparked debate at Thoughtworks; many of us were skeptical. On an April episode of our technology podcast, we talked about our concerns and were cautious about how vibe coding might evolve.&lt;/p&gt; 
 &lt;p&gt;Unsurprisingly given the implied imprecision of vibe-based coding, antipatterns have been proliferating. We’ve once again noted, for instance, complacency with AI generated code&lt;strong&gt; &lt;/strong&gt;on the latest volume of the Technology Radar,&lt;strong&gt; &lt;/strong&gt;but it’s also worth pointing out that early ventures into vibe coding also exposed a degree of complacency about what AI models can actually handle — users demanded more and prompts grew larger, but model reliability started to falter.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Experimenting with generative AI&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;This is one of the drivers behind increasing interest in engineering context. We’re well aware of its importance, working with coding assistants like Claude Code and Augment Code.&lt;strong&gt; &lt;/strong&gt;Providing necessary context—or knowledge priming—is crucial. It ensures outputs are more consistent and reliable, which will ultimately lead to better software that needs less work — reducing rewrites and potentially driving productivity.&lt;/p&gt; 
 &lt;p&gt;When effectively prepared, we’ve seen good results when using generative AI to understand legacy codebases. Indeed, done effectively with the appropriate context, it can even help when we don’t have full access to source code.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s important to remember that context isn’t just about more data and more detail. This is one of the lessons we’ve taken from using generative AI for forward engineering. It might sound counterintuitive, but in this scenario, we’ve found AI to be more effective when it’s further abstracted from the underlying system — or, in other words, further removed from the specifics of the legacy code. This is because the solution space becomes much wider, allowing us to better leverage the generative and creative capabilities of the AI models we use.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Context is critical in the agentic era&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The backdrop of changes that have happened over recent months is the growth of agents and agentic systems — both as products organizations want to develop and as technology they want to leverage. This has forced the industry to properly reckon with context and move away from a purely vibes-based approach.&lt;/p&gt;  &lt;p&gt;Indeed, far from simply getting on with tasks they’ve been programmed to do, agents require significant human intervention to ensure they are equipped to respond to complex and dynamic contexts.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;There are a number of context-related technologies aimed at tackling this challenge, including agents.md, Context7, and Mem0. But it’s also a question of approach. For instance, we’ve found success with anchoring coding agents to a reference application&lt;strong&gt; &lt;/strong&gt;— essentially providing agents with a contextual ground truth. We’re also experimenting with using teams of coding agents; while this might sound like it increases complexity, it actually removes some of the burden of having to give a single agent all the dense layers of context it needs to do its job successfully.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Toward consensus&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Hopefully the space will mature as practices and standards embed. It would be remiss to not mention the significance of the Model Context Protocol, which has emerged as the go-to protocol for connecting LLMs or agentic AI to sources of context. Relatedly, the agent2agent (A2A) protocol leads the way with standardizing how agents interact with one another.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It remains to be seen whether these standards win out. But in any case, it’s important to consider the day-to-day practices that allow us, as software engineers and technologists, to collaborate effectively even when dealing with highly complex and dynamic systems. Sure, AI needs context, but so do we. Techniques like curated shared instructions for software teams may not sound like the hottest innovation on the planet, but they can be remarkably powerful for helping teams work together.&lt;/p&gt;  &lt;p&gt;There’s perhaps also a conversation to be had about what these changes mean for agile software development. Spec-driven development is one idea that appears to have some traction, but there are still questions about how we remain adaptable and flexible while also building robust contextual foundations and ground truths for AI systems.&lt;/p&gt; 

 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Software engineers can solve the context challenge&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Clearly, 2025 has been a huge year in the evolution of software engineering as a practice. There’s a lot the industry needs to monitor closely, but it’s also an exciting time. And while fears about AI job automation may remain, the fact the conversation has moved from questions of speed and scale to context puts software engineers right at the heart of things.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Once again, it will be down to them to experiment, collaborate, and learn — the future depends on it.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Thoughtworks. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/05/1127477/from-vibe-coding-to-context-engineering-2025-in-software-development/</guid><pubDate>Wed, 05 Nov 2025 10:31:29 +0000</pubDate></item><item><title>[NEW] Former Meta employees launch Sandbar, a smart ring that takes voice notes and controls music (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/05/former-meta-employees-launch-sandbar-a-smart-ring-that-takes-voice-notes-and-controls-music/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A raft of voice-based hardware devices have emerged, aimed at companionship, productivity, or personal growth. These include card-shaped devices from Plaud and Pocket; pendants from Friend, Limitless, and Taya; and a wristband from Bee, which is now part of Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, two former Meta employees who worked on interface design have launched Sandbar, a startup that has created a ring called Stream for similar purposes. The company calls the ring “a mouse for voice” because it can take notes, help you interact with an AI assistant, and also let you control music.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sandbar’s CEO, Mina Fahmi, has an extensive background in designing human-computer interfaces. He worked at Bryan Johnson’s Kernel and later at smart glasses startup Magic Leap. Kirak Hong, Sandbar’s CTO, worked at Google before joining CTRL-Labs, where the duo met. Meta acquired the startup in 2019, and its work eventually led to neural interfaces for the tech giant’s smart wearables.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064971" height="453" src="https://techcrunch.com/wp-content/uploads/2025/11/SHINDELVERSE-143.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Sandbar co-founders Kirak Hong and Mina Fahmi Image Credits: Sandbar&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Fahmi said that when large language models started emerging a few years ago, he built an experimental journaling app. However, he found that the app itself became a barrier to capturing his thoughts. Given his experience building hardware interfaces, he began exploring a conversational hardware interface instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“A lot of my ideas bubble up when I’m walking or when I’m commuting, and I don’t want to pull out my phone to interrupt that moment. I don’t want to shout into my earbuds where the world can hear me to talk through an idea. Kirak and I were trying to understand what it would take to actually capture a thought the moment it bubbles up. That’s how we came up with Stream,” Fahmi told TechCrunch in an interview.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/walk.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image Credits: Sandbar&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The ring, designed to be worn on your dominant hand’s index finger, has microphones and a touch pad.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a virtual demo, Fahmi wore the Stream ring on his index finger and recorded his thoughts by pressing and holding the touchpad. By default, the microphone is off, activating only with this gesture. The microphone proved sensitive enough to pick up whispers and transcribe them in the companion iOS app. Other apps like Wispr Flow and Willow similarly allow people to capture their thoughts quietly.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Stream’s app includes an AI chatbot that converses with you as you record your thoughts. You can organize these into separate notes that either you or the AI can edit. The app also lets you pinch to zoom out and review what you have discussed over days or weeks. Sandbar has added a personalization layer so the assistant’s voice sounds somewhat similar to the user’s.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-9-16 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Fahmi said that in crowded spaces, users can wear headphones to converse privately with the assistant. Without headphones, the ring provides haptic feedback when it successfully registers a note, allowing you to add to-dos, take notes, or check items off a grocery list quietly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond voice functions, the ring’s flat surface doubles as a media controller, allowing you to play, pause, skip tracks and adjust volume. While many headphones offer similar controls, the ring could prove useful when your hands are occupied or you’re in transit.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company is opening up pre-orders for Stream on Wednesday at $249 for the silver version and $299 for gold. Sandbar aims to begin shipping next summer. A Pro subscription tier — free for three months for those who preorder, then $10 per month — offers unlimited chats, notes, and early access to new features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Fahmi said the company gives users full control over their data at any tier, with encryption both at rest and in transit. He added that Sandbar doesn’t believe in walled gardens and plans to support data exports to apps like Notion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sandbar has raised $13 million in funding from True Ventures, Upfront Ventures, and Betaworks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Toni Schneider, a partner at True Ventures, said he had been skeptical of AI devices, as demos he’d seen before Stream weren’t impressive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think a lot of people would agree that voice and AI go really well together. And [they also agree] that having a phone or even a laptop to interact with AI is kind of a lot when all you need is voice. So there should be some kind of new form factor out there. We looked at a lot of them, and a lot of them just didn’t quite hit the target. When Mina came in and showed us the demo, it made sense to us,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Competition is fierce in the voice-AI hardware space, with many builders exploring rings as a form factor. Fahmi said he doesn’t want Stream to be an assistant or a companion, but rather an interface for users to express their ideas while maintaining full control.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI hardware has yet to achieve mainstream success. Humane sold to HP, Rabbit is attempting to improve user experience and engagement through software updates, and Friend is trying to leverage user backlash to fuel growth. Sandbar will need to prove that its ring form factor offers genuine convenience and value that pendants, pins, or wristbands cannot.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A raft of voice-based hardware devices have emerged, aimed at companionship, productivity, or personal growth. These include card-shaped devices from Plaud and Pocket; pendants from Friend, Limitless, and Taya; and a wristband from Bee, which is now part of Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, two former Meta employees who worked on interface design have launched Sandbar, a startup that has created a ring called Stream for similar purposes. The company calls the ring “a mouse for voice” because it can take notes, help you interact with an AI assistant, and also let you control music.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sandbar’s CEO, Mina Fahmi, has an extensive background in designing human-computer interfaces. He worked at Bryan Johnson’s Kernel and later at smart glasses startup Magic Leap. Kirak Hong, Sandbar’s CTO, worked at Google before joining CTRL-Labs, where the duo met. Meta acquired the startup in 2019, and its work eventually led to neural interfaces for the tech giant’s smart wearables.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064971" height="453" src="https://techcrunch.com/wp-content/uploads/2025/11/SHINDELVERSE-143.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Sandbar co-founders Kirak Hong and Mina Fahmi Image Credits: Sandbar&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Fahmi said that when large language models started emerging a few years ago, he built an experimental journaling app. However, he found that the app itself became a barrier to capturing his thoughts. Given his experience building hardware interfaces, he began exploring a conversational hardware interface instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“A lot of my ideas bubble up when I’m walking or when I’m commuting, and I don’t want to pull out my phone to interrupt that moment. I don’t want to shout into my earbuds where the world can hear me to talk through an idea. Kirak and I were trying to understand what it would take to actually capture a thought the moment it bubbles up. That’s how we came up with Stream,” Fahmi told TechCrunch in an interview.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3064972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/walk.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image Credits: Sandbar&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The ring, designed to be worn on your dominant hand’s index finger, has microphones and a touch pad.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a virtual demo, Fahmi wore the Stream ring on his index finger and recorded his thoughts by pressing and holding the touchpad. By default, the microphone is off, activating only with this gesture. The microphone proved sensitive enough to pick up whispers and transcribe them in the companion iOS app. Other apps like Wispr Flow and Willow similarly allow people to capture their thoughts quietly.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Stream’s app includes an AI chatbot that converses with you as you record your thoughts. You can organize these into separate notes that either you or the AI can edit. The app also lets you pinch to zoom out and review what you have discussed over days or weeks. Sandbar has added a personalization layer so the assistant’s voice sounds somewhat similar to the user’s.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-9-16 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Fahmi said that in crowded spaces, users can wear headphones to converse privately with the assistant. Without headphones, the ring provides haptic feedback when it successfully registers a note, allowing you to add to-dos, take notes, or check items off a grocery list quietly.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond voice functions, the ring’s flat surface doubles as a media controller, allowing you to play, pause, skip tracks and adjust volume. While many headphones offer similar controls, the ring could prove useful when your hands are occupied or you’re in transit.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company is opening up pre-orders for Stream on Wednesday at $249 for the silver version and $299 for gold. Sandbar aims to begin shipping next summer. A Pro subscription tier — free for three months for those who preorder, then $10 per month — offers unlimited chats, notes, and early access to new features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Fahmi said the company gives users full control over their data at any tier, with encryption both at rest and in transit. He added that Sandbar doesn’t believe in walled gardens and plans to support data exports to apps like Notion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sandbar has raised $13 million in funding from True Ventures, Upfront Ventures, and Betaworks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Toni Schneider, a partner at True Ventures, said he had been skeptical of AI devices, as demos he’d seen before Stream weren’t impressive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think a lot of people would agree that voice and AI go really well together. And [they also agree] that having a phone or even a laptop to interact with AI is kind of a lot when all you need is voice. So there should be some kind of new form factor out there. We looked at a lot of them, and a lot of them just didn’t quite hit the target. When Mina came in and showed us the demo, it made sense to us,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Competition is fierce in the voice-AI hardware space, with many builders exploring rings as a form factor. Fahmi said he doesn’t want Stream to be an assistant or a companion, but rather an interface for users to express their ideas while maintaining full control.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI hardware has yet to achieve mainstream success. Humane sold to HP, Rabbit is attempting to improve user experience and engagement through software updates, and Friend is trying to leverage user backlash to fuel growth. Sandbar will need to prove that its ring form factor offers genuine convenience and value that pendants, pins, or wristbands cannot.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/05/former-meta-employees-launch-sandbar-a-smart-ring-that-takes-voice-notes-and-controls-music/</guid><pubDate>Wed, 05 Nov 2025 11:00:00 +0000</pubDate></item><item><title>[NEW] The Download: the solar geoengineering race, and future gazing with the The Simpsons (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/05/1127627/the-download-the-solar-geoengineering-race-and-future-gazing-with-the-the-simpsons/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why the for-profit race into solar geoengineering is bad for science and public trust&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—David Keith is the professor of geophysical sciences at the University of Chicago and Daniele Visioni is an assistant professor of earth and atmospheric sciences at Cornell University&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Last week, an American-Israeli company that claims it’s developed proprietary technology to cool the planet announced it had raised $60 million, by far the largest known venture capital round to date for a solar geoengineering startup.&lt;/p&gt;&lt;p&gt;The company, Stardust, says the funding will enable it to develop a system that could be deployed by the start of the next decade, according to Heatmap, which broke the story.&lt;/p&gt;&lt;p&gt;As scientists who have worked on the science of solar geoengineering for decades, we have grown increasingly concerned about emerging efforts to start and fund private companies to deploy technologies that could alter the climate of the planet. We also strongly dispute some of the technical claims that certain companies have made about their offerings. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is part of Heat Exchange, MIT Technology Review’s guest opinion series offering expert commentary on legal, political and regulatory issues related to climate change and clean energy. You can read the rest of the series &lt;/strong&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Can “The Simpsons” really predict the future?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;According to internet listicles, the animated sitcom The Simpsons has predicted the future anywhere from 17 to 55 times.&lt;/p&gt;&lt;p&gt;The show foresaw Donald Trump becoming US President a full 17 years before the real estate mogul was inaugurated as the 45th leader of the United States. Earlier, in 1993, an episode of the show featured the “Osaka flu,” which some felt was eerily prescient of the coronavirus pandemic. And—somehow!—Simpsons writers &lt;em&gt;just knew&lt;/em&gt; that the US Olympic curling team would beat Sweden eight whole years before they did it.&lt;/p&gt;&lt;p&gt;Al Jean has worked on The Simpsons on and off since 1989; he is the cartoon’s longest-serving showrunner. Here, he reflects on the conspiracy theories that have sprung from these apparent prophecies. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Amelia Tait&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is part of MIT Technology Review’s series “&lt;/strong&gt;&lt;strong&gt;The New Conspiracy Age&lt;/strong&gt;&lt;strong&gt;,” about how the present boom in conspiracy theories is reshaping science and technology.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: Therapists are secretly using ChatGPT. Clients are triggered.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Declan would never have found out his therapist was using ChatGPT had it not been for a technical mishap where his therapist began inadvertently sharing his screen.&lt;/p&gt;  &lt;p&gt;For the rest of the session, Declan was privy to a real-time stream of ChatGPT analysis rippling across his therapist’s screen, who was taking what Declan was saying, putting it into ChatGPT, and then parroting its answers.&lt;/p&gt; 

 &lt;p&gt;But Declan is not alone. In fact, a growing number of people are reporting receiving AI-generated communiqués from their therapists. Clients’ trust and privacy are being abandoned in the process.&lt;/p&gt;  &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Amazon is suing Perplexity over its Comet AI agent&lt;/strong&gt;&lt;br /&gt;It alleges Perplexity is committing computer fraud by not disclosing when Comet is shopping on a human’s behalf. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;In turn, Perplexity has accused Amazon of bullying. &lt;/em&gt;(CNBC)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Trump has nominated the billionaire entrepreneur Jared Isaacman to lead NASA&lt;/strong&gt;&lt;br /&gt;Five months after he withdrew Isaacman’s nomination for the same job. (WP $)&lt;br /&gt;+ &lt;em&gt;It was around the same time Elon Musk left the US government. &lt;/em&gt;(WSJ $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Homeland Security has released an app for police forces to scan people’s faces&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Mobile Fortify uses facial recognition to identify whether someone’s been given a deportation order. (404 Media)&lt;br /&gt;+ &lt;em&gt;Another effort to track ICE raids was just taken offline. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Scientific journals are being swamped with AI-written letters&lt;/strong&gt;&lt;br /&gt;Researchers are sifting through their inbox trying to work out what to believe. (NYT $)&lt;br /&gt;+ &lt;em&gt;ArXiv is no longer accepting certain papers for fear they’ve been written by AI. &lt;/em&gt;(404 Media)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 The AI boom has proved a major windfall for equipment makers&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Makers of small turbines and fuel cells, rejoice. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Chronic kidney disease may be the first chronic illness linked to climate change&lt;br /&gt;&lt;/strong&gt;Experts have linked a surge in the disease to hotter temperatures. (Undark)&lt;br /&gt;+ &lt;em&gt;The quest to find out how our bodies react to extreme temperatures. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;7 Brazil is proposing a fund to protect tropical forests&lt;br /&gt;&lt;/strong&gt;It would pay countries not to fell their trees. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 New York has voted for a citywide digital map&lt;br /&gt;&lt;/strong&gt;It’ll officially represent the five boroughs for the first time. (Fast Company $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;9 The internet could be at risk of catastrophic collapse&lt;/strong&gt;&lt;br /&gt;Meet the people preparing for that exact eventuality. (New Scientist $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 A Chinese space craft may have been hit by space junk&lt;/strong&gt;&lt;br /&gt;Three astronauts have been forced to remain on the Tiangong space station while the damage is investigated. (Ars Technica)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“I am not sure how I earned the trust of so many, but I will do everything I can to live up to those expectations.”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Jared Isaacman, Donald Trump’s renomination to lead NASA, doesn’t appear entirely sure in his own abilities to lead the agency, Ars Technica reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127629" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/image_88b3a9.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Is the digital dollar dead?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In 2020, digital currencies were one of the hottest topics in town. China was well on its way to launching its own central bank digital currency, or CBDC, and many other countries launched CBDC research projects, including the US.&lt;/p&gt;&lt;p&gt;How things change. Years later, the digital dollar—even though it doesn’t exist—has become political red meat, as some politicians label it a dystopian tool for surveillance. And late last year, the Boston Fed quietly stopped working on its CBDC project. So is the dream of the digital dollar dead? Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Mike Orcutt&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why the for-profit race into solar geoengineering is bad for science and public trust&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—David Keith is the professor of geophysical sciences at the University of Chicago and Daniele Visioni is an assistant professor of earth and atmospheric sciences at Cornell University&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Last week, an American-Israeli company that claims it’s developed proprietary technology to cool the planet announced it had raised $60 million, by far the largest known venture capital round to date for a solar geoengineering startup.&lt;/p&gt;&lt;p&gt;The company, Stardust, says the funding will enable it to develop a system that could be deployed by the start of the next decade, according to Heatmap, which broke the story.&lt;/p&gt;&lt;p&gt;As scientists who have worked on the science of solar geoengineering for decades, we have grown increasingly concerned about emerging efforts to start and fund private companies to deploy technologies that could alter the climate of the planet. We also strongly dispute some of the technical claims that certain companies have made about their offerings. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is part of Heat Exchange, MIT Technology Review’s guest opinion series offering expert commentary on legal, political and regulatory issues related to climate change and clean energy. You can read the rest of the series &lt;/strong&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Can “The Simpsons” really predict the future?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;According to internet listicles, the animated sitcom The Simpsons has predicted the future anywhere from 17 to 55 times.&lt;/p&gt;&lt;p&gt;The show foresaw Donald Trump becoming US President a full 17 years before the real estate mogul was inaugurated as the 45th leader of the United States. Earlier, in 1993, an episode of the show featured the “Osaka flu,” which some felt was eerily prescient of the coronavirus pandemic. And—somehow!—Simpsons writers &lt;em&gt;just knew&lt;/em&gt; that the US Olympic curling team would beat Sweden eight whole years before they did it.&lt;/p&gt;&lt;p&gt;Al Jean has worked on The Simpsons on and off since 1989; he is the cartoon’s longest-serving showrunner. Here, he reflects on the conspiracy theories that have sprung from these apparent prophecies. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Amelia Tait&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is part of MIT Technology Review’s series “&lt;/strong&gt;&lt;strong&gt;The New Conspiracy Age&lt;/strong&gt;&lt;strong&gt;,” about how the present boom in conspiracy theories is reshaping science and technology.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: Therapists are secretly using ChatGPT. Clients are triggered.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Declan would never have found out his therapist was using ChatGPT had it not been for a technical mishap where his therapist began inadvertently sharing his screen.&lt;/p&gt;  &lt;p&gt;For the rest of the session, Declan was privy to a real-time stream of ChatGPT analysis rippling across his therapist’s screen, who was taking what Declan was saying, putting it into ChatGPT, and then parroting its answers.&lt;/p&gt; 

 &lt;p&gt;But Declan is not alone. In fact, a growing number of people are reporting receiving AI-generated communiqués from their therapists. Clients’ trust and privacy are being abandoned in the process.&lt;/p&gt;  &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Amazon is suing Perplexity over its Comet AI agent&lt;/strong&gt;&lt;br /&gt;It alleges Perplexity is committing computer fraud by not disclosing when Comet is shopping on a human’s behalf. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;In turn, Perplexity has accused Amazon of bullying. &lt;/em&gt;(CNBC)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Trump has nominated the billionaire entrepreneur Jared Isaacman to lead NASA&lt;/strong&gt;&lt;br /&gt;Five months after he withdrew Isaacman’s nomination for the same job. (WP $)&lt;br /&gt;+ &lt;em&gt;It was around the same time Elon Musk left the US government. &lt;/em&gt;(WSJ $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 Homeland Security has released an app for police forces to scan people’s faces&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Mobile Fortify uses facial recognition to identify whether someone’s been given a deportation order. (404 Media)&lt;br /&gt;+ &lt;em&gt;Another effort to track ICE raids was just taken offline. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Scientific journals are being swamped with AI-written letters&lt;/strong&gt;&lt;br /&gt;Researchers are sifting through their inbox trying to work out what to believe. (NYT $)&lt;br /&gt;+ &lt;em&gt;ArXiv is no longer accepting certain papers for fear they’ve been written by AI. &lt;/em&gt;(404 Media)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;5 The AI boom has proved a major windfall for equipment makers&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Makers of small turbines and fuel cells, rejoice. (WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Chronic kidney disease may be the first chronic illness linked to climate change&lt;br /&gt;&lt;/strong&gt;Experts have linked a surge in the disease to hotter temperatures. (Undark)&lt;br /&gt;+ &lt;em&gt;The quest to find out how our bodies react to extreme temperatures. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;7 Brazil is proposing a fund to protect tropical forests&lt;br /&gt;&lt;/strong&gt;It would pay countries not to fell their trees. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 New York has voted for a citywide digital map&lt;br /&gt;&lt;/strong&gt;It’ll officially represent the five boroughs for the first time. (Fast Company $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;9 The internet could be at risk of catastrophic collapse&lt;/strong&gt;&lt;br /&gt;Meet the people preparing for that exact eventuality. (New Scientist $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 A Chinese space craft may have been hit by space junk&lt;/strong&gt;&lt;br /&gt;Three astronauts have been forced to remain on the Tiangong space station while the damage is investigated. (Ars Technica)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“I am not sure how I earned the trust of so many, but I will do everything I can to live up to those expectations.”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Jared Isaacman, Donald Trump’s renomination to lead NASA, doesn’t appear entirely sure in his own abilities to lead the agency, Ars Technica reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127629" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/image_88b3a9.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Is the digital dollar dead?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In 2020, digital currencies were one of the hottest topics in town. China was well on its way to launching its own central bank digital currency, or CBDC, and many other countries launched CBDC research projects, including the US.&lt;/p&gt;&lt;p&gt;How things change. Years later, the digital dollar—even though it doesn’t exist—has become political red meat, as some politicians label it a dystopian tool for surveillance. And late last year, the Boston Fed quietly stopped working on its CBDC project. So is the dream of the digital dollar dead? Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Mike Orcutt&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/05/1127627/the-download-the-solar-geoengineering-race-and-future-gazing-with-the-the-simpsons/</guid><pubDate>Wed, 05 Nov 2025 13:13:26 +0000</pubDate></item><item><title>[NEW] Google Maps bakes in Gemini to improve navigation and hands-free use (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/05/google-maps-bakes-in-gemini-to-improve-navigation-and-hands-free-use/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Over the last year, Google has added multiple AI-powered features to Maps to improve discovery and enable users to ask questions about places. Now, the company is upgrading the app with Gemini to let users ask its AI bot questions while driving, improve navigation, and perform more tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While driving, users can now ask Gemini to answer questions about places of interest on their route, return results about other topics (like sports or news), and even perform tasks like adding events to their calendar. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;You can ask multiple questions in a conversation, too: for example, “Is there a budget-friendly restaurant with vegan options along my route, something within a couple of miles? … What’s parking like there?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Drivers can even report traffic incidents using Gemini, and Maps will proactively notify users of disruptions on the route ahead.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3065175" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Proactive-traffic-alerts.jpg?w=326" width="326" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image Credits: Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding a new feature to Maps that combines Gemini with Street View data to improve navigation instructions. So instead of telling you to turn right after, say, 500 feet, Maps will now mention a nearby landmark, like gas stations, restaurants or famous buildings, and highlight them before you have to make the turn. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said that Gemini cross-references information about 250 million places with Street View images to identify important and visible landmarks for navigation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Maps is also getting the ability to answer questions about your surroundings by working together with Google Lens. So you can point the camera at places of interest, like restaurants and landmarks, and ask questions like, “What is this place and why is it popular?”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Google said the new Gemini navigation features will be rolled out to iOS and Android devices in the coming weeks, and said support for Android Auto would be coming soon. Traffic alerts are rolling out in the U.S. for Android users first; Landmark navigation is currently only going to be available in the U.S. on both iOS and Android; and Lens with Gemini will be functional in the U.S. later this month.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Over the last year, Google has added multiple AI-powered features to Maps to improve discovery and enable users to ask questions about places. Now, the company is upgrading the app with Gemini to let users ask its AI bot questions while driving, improve navigation, and perform more tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While driving, users can now ask Gemini to answer questions about places of interest on their route, return results about other topics (like sports or news), and even perform tasks like adding events to their calendar. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;You can ask multiple questions in a conversation, too: for example, “Is there a budget-friendly restaurant with vegan options along my route, something within a couple of miles? … What’s parking like there?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Drivers can even report traffic incidents using Gemini, and Maps will proactively notify users of disruptions on the route ahead.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3065175" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Proactive-traffic-alerts.jpg?w=326" width="326" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Image Credits: Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding a new feature to Maps that combines Gemini with Street View data to improve navigation instructions. So instead of telling you to turn right after, say, 500 feet, Maps will now mention a nearby landmark, like gas stations, restaurants or famous buildings, and highlight them before you have to make the turn. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said that Gemini cross-references information about 250 million places with Street View images to identify important and visible landmarks for navigation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Maps is also getting the ability to answer questions about your surroundings by working together with Google Lens. So you can point the camera at places of interest, like restaurants and landmarks, and ask questions like, “What is this place and why is it popular?”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Google said the new Gemini navigation features will be rolled out to iOS and Android devices in the coming weeks, and said support for Android Auto would be coming soon. Traffic alerts are rolling out in the U.S. for Android users first; Landmark navigation is currently only going to be available in the U.S. on both iOS and Android; and Lens with Gemini will be functional in the U.S. later this month.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/05/google-maps-bakes-in-gemini-to-improve-navigation-and-hands-free-use/</guid><pubDate>Wed, 05 Nov 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] So long, Assistant—Gemini is taking over Google Maps (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/11/so-long-assistant-gemini-is-taking-over-google-maps/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Gemini is rolling out to Maps on Android and iOS, with Android Auto coming soon.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini navigation" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemini-in-navigation-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini navigation" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemini-in-navigation-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is in the process of purging Assistant across its products, and the next target is Google Maps. Starting today, Gemini will begin rolling out in Maps, powering new experiences for navigation, location info, and more. This update will eventually completely usurp Google Assistant’s hands-free role in Maps, but the rollout will take time. So for now, the smart assistant in Google Maps will still depend on how you’re running the app.&lt;/p&gt;
&lt;p&gt;Across all Gemini’s incarnations, Google stresses its conversational abilities. Whereas Assistant was hard-pressed to keep one or two balls in the air, you can theoretically give Gemini much more complex instructions. Google’s demo includes someone asking for nearby restaurants with cheap vegan food, but instead of just providing a list, it suggests something based on the user’s input. Gemini can also offer more information about the location.&lt;/p&gt;
&lt;p&gt;Maps will also get its own Gemini-infused version of Lens for after you park. You will be able to point the camera at a landmark, restaurant, or other business to get instant answers to your questions. This experience will be distinct from the version of Lens available in the Google app, focused on giving you location-based information. Maybe you want to know about the menu at a restaurant or what it’s like inside. Sure, you could open the door… but AI!&lt;/p&gt;
&lt;p&gt;While Google has recently been forced to acknowledge that hallucinations are inevitable, the Maps team says it does not expect that to be a problem with this version of Gemini. The suggestions coming from the generative AI bot are grounded in Google’s billions of place listings and Street View photos. This will, allegedly, make the robot less likely to make up a location. Google also says in no uncertain terms that Gemini is not responsible for choosing your route.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;How far is 500 feet?&lt;/h2&gt;
&lt;p&gt;The robot will, however, get involved with the spoken directions. Currently, Google Maps and other navigation systems use vague instructions like “turn in 500 feet.” However, these announcements often arrive far too late to be useful, and can people accurately gauge 500 feet while driving? The Gemini-based solution is to give instructions with landmarks.&lt;/p&gt;
&lt;p&gt;Instead of only using distances for turns, Gemini might use a gas station, restaurant, or recognizable sign to help you find your turn. Again, Google says its database of hundreds of millions of locations with accompanying Street View images helps ensure the directions are accurate. Gemini can also alert you to possible slowdowns along your route even if you don’t have Maps open.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125715 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1920" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemini-Maps.jpg" width="1824" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google’s driving alerts for accidents and other traffic complications will also be tied into Gemini. So you’ll be able to just say “there’s an accident” or “traffic jam ahead,” and Gemini will make the report as if you used the multistep reporting process manually. Gemini in Maps can also connect to other Google services, like adding events to your calendar. And that request can be lumped in with navigation or map-oriented instructions.&lt;/p&gt;
&lt;p&gt;If you’re running Google Maps on your phone, Gemini could arrive in the coming days. For those using Android Auto or Google built-in, the rollout will begin “soon.” No, Google doesn’t have anything more specific on that, but that group will get Gemini sooner than Apple CarPlay Maps users. Google says it’s still evaluating how to integrate Gemini’s hands-free features with Apple’s more limited platform.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Gemini is rolling out to Maps on Android and iOS, with Android Auto coming soon.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini navigation" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemini-in-navigation-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini navigation" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemini-in-navigation-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is in the process of purging Assistant across its products, and the next target is Google Maps. Starting today, Gemini will begin rolling out in Maps, powering new experiences for navigation, location info, and more. This update will eventually completely usurp Google Assistant’s hands-free role in Maps, but the rollout will take time. So for now, the smart assistant in Google Maps will still depend on how you’re running the app.&lt;/p&gt;
&lt;p&gt;Across all Gemini’s incarnations, Google stresses its conversational abilities. Whereas Assistant was hard-pressed to keep one or two balls in the air, you can theoretically give Gemini much more complex instructions. Google’s demo includes someone asking for nearby restaurants with cheap vegan food, but instead of just providing a list, it suggests something based on the user’s input. Gemini can also offer more information about the location.&lt;/p&gt;
&lt;p&gt;Maps will also get its own Gemini-infused version of Lens for after you park. You will be able to point the camera at a landmark, restaurant, or other business to get instant answers to your questions. This experience will be distinct from the version of Lens available in the Google app, focused on giving you location-based information. Maybe you want to know about the menu at a restaurant or what it’s like inside. Sure, you could open the door… but AI!&lt;/p&gt;
&lt;p&gt;While Google has recently been forced to acknowledge that hallucinations are inevitable, the Maps team says it does not expect that to be a problem with this version of Gemini. The suggestions coming from the generative AI bot are grounded in Google’s billions of place listings and Street View photos. This will, allegedly, make the robot less likely to make up a location. Google also says in no uncertain terms that Gemini is not responsible for choosing your route.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;How far is 500 feet?&lt;/h2&gt;
&lt;p&gt;The robot will, however, get involved with the spoken directions. Currently, Google Maps and other navigation systems use vague instructions like “turn in 500 feet.” However, these announcements often arrive far too late to be useful, and can people accurately gauge 500 feet while driving? The Gemini-based solution is to give instructions with landmarks.&lt;/p&gt;
&lt;p&gt;Instead of only using distances for turns, Gemini might use a gas station, restaurant, or recognizable sign to help you find your turn. Again, Google says its database of hundreds of millions of locations with accompanying Street View images helps ensure the directions are accurate. Gemini can also alert you to possible slowdowns along your route even if you don’t have Maps open.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125715 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1920" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemini-Maps.jpg" width="1824" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google’s driving alerts for accidents and other traffic complications will also be tied into Gemini. So you’ll be able to just say “there’s an accident” or “traffic jam ahead,” and Gemini will make the report as if you used the multistep reporting process manually. Gemini in Maps can also connect to other Google services, like adding events to your calendar. And that request can be lumped in with navigation or map-oriented instructions.&lt;/p&gt;
&lt;p&gt;If you’re running Google Maps on your phone, Gemini could arrive in the coming days. For those using Android Auto or Google built-in, the rollout will begin “soon.” No, Google doesn’t have anything more specific on that, but that group will get Gemini sooner than Apple CarPlay Maps users. Google says it’s still evaluating how to integrate Gemini’s hands-free features with Apple’s more limited platform.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/11/so-long-assistant-gemini-is-taking-over-google-maps/</guid><pubDate>Wed, 05 Nov 2025 14:00:26 +0000</pubDate></item><item><title>[NEW] SoftBank, OpenAI launch new joint venture in Japan as AI deals grow ever more circular (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/05/softbank-openai-launch-new-joint-venture-in-japan-as-ai-deals-grow-ever-more-circular/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1575097396.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If you look at how AI deals are conducted these days, it seems AI companies and their investors are imitating the circle of life. Only, it’s a circle of profit that ensures the money eventually comes back to their own coffers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Case in point: SoftBank, which is investing tens of billions into OpenAI and committing dozens more to build AI data centers and infrastructure, just launched a joint venture with the ChatGPT maker in Japan that will localize and sell the AI company’s enterprise tech to companies in the country. And the first customer of this joint venture is going to be SoftBank itself.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Called SB OAI Japan, the joint venture will be owned 50-50 by SoftBank and OpenAI, and will provide what the companies are calling “Crystal intelligence,” which is being defined as a “packaged enterprise AI solution” targeted at corporate management and operations in Japan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Crystal intelligence is designed to help organizations enhance productivity and management efficiency through the adoption of advanced AI tools. The solution combines OpenAI’s enterprise offerings with localized implementation and support provided through SB OAI Japan,” SoftBank said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It seems SoftBank is intent on fueling the AI hype cycle, and the resultant revenues: The conglomerate said all its employees are “actively utilizing AI in their daily operations,” and that it has so far created 2.5 million custom ChatGPT instances for internal use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The conglomerate said it would put the joint venture’s solutions to use throughout its various businesses, validate their effectiveness for product development and “business transformation,” and then pass on the insights and expertise it gains to other companies back through SB OAI Japan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The joint venture comes as analysts raise concerns about the truckloads of cash being thrown at AI development and associated efforts, as well as the stratospheric valuations awarded to companies benefiting from it. The movement is being likened to the dot-com boom, when the widespread adoption of the internet resulted in a wave of venture capital and sky-high valuations, and similar booms over the past couple of decades where massive sums were spent on developing unproven business models without a clear sign of meaningful returns on investment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1575097396.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If you look at how AI deals are conducted these days, it seems AI companies and their investors are imitating the circle of life. Only, it’s a circle of profit that ensures the money eventually comes back to their own coffers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Case in point: SoftBank, which is investing tens of billions into OpenAI and committing dozens more to build AI data centers and infrastructure, just launched a joint venture with the ChatGPT maker in Japan that will localize and sell the AI company’s enterprise tech to companies in the country. And the first customer of this joint venture is going to be SoftBank itself.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Called SB OAI Japan, the joint venture will be owned 50-50 by SoftBank and OpenAI, and will provide what the companies are calling “Crystal intelligence,” which is being defined as a “packaged enterprise AI solution” targeted at corporate management and operations in Japan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Crystal intelligence is designed to help organizations enhance productivity and management efficiency through the adoption of advanced AI tools. The solution combines OpenAI’s enterprise offerings with localized implementation and support provided through SB OAI Japan,” SoftBank said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It seems SoftBank is intent on fueling the AI hype cycle, and the resultant revenues: The conglomerate said all its employees are “actively utilizing AI in their daily operations,” and that it has so far created 2.5 million custom ChatGPT instances for internal use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The conglomerate said it would put the joint venture’s solutions to use throughout its various businesses, validate their effectiveness for product development and “business transformation,” and then pass on the insights and expertise it gains to other companies back through SB OAI Japan.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The joint venture comes as analysts raise concerns about the truckloads of cash being thrown at AI development and associated efforts, as well as the stratospheric valuations awarded to companies benefiting from it. The movement is being likened to the dot-com boom, when the widespread adoption of the internet resulted in a wave of venture capital and sky-high valuations, and similar booms over the past couple of decades where massive sums were spent on developing unproven business models without a clear sign of meaningful returns on investment.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/05/softbank-openai-launch-new-joint-venture-in-japan-as-ai-deals-grow-ever-more-circular/</guid><pubDate>Wed, 05 Nov 2025 14:22:58 +0000</pubDate></item><item><title>[NEW] Google gets the US government’s green light to acquire Wiz for $32B (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/05/google-gets-the-us-governments-green-light-to-acquire-wiz-for-32b/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/Assaf-Rappaport-Wiz.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s $32 billion acquisition of cloud security company Wiz has moved a step closer toward the finish line.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wiz CEO Assaf Rappaport said at a Wall Street Journal event on Tuesday, as reported by Reuters, that the acquisition had cleared its antitrust review by the U.S. Department of Justice. Rappaport added that while a milestone, the deal still has a way to go before it officially closes.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Wiz for more information regarding the timeline to close.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google initially approached Wiz in 2024 with an offer to buy the startup for $23 billion. Wiz walked away from that offer and Rappaport said, at the time, that he felt the business could grow to be a lot bigger than that.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He was right. Google and Wiz revived acquisition talks in the first few months of 2025, and Google announced it was buying Wiz for $32 billion in March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bloomberg initially reported that the deal was under antitrust review in June. The deal is expected to close in early 2026, according to Reuters.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/Assaf-Rappaport-Wiz.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s $32 billion acquisition of cloud security company Wiz has moved a step closer toward the finish line.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wiz CEO Assaf Rappaport said at a Wall Street Journal event on Tuesday, as reported by Reuters, that the acquisition had cleared its antitrust review by the U.S. Department of Justice. Rappaport added that while a milestone, the deal still has a way to go before it officially closes.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Wiz for more information regarding the timeline to close.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google initially approached Wiz in 2024 with an offer to buy the startup for $23 billion. Wiz walked away from that offer and Rappaport said, at the time, that he felt the business could grow to be a lot bigger than that.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He was right. Google and Wiz revived acquisition talks in the first few months of 2025, and Google announced it was buying Wiz for $32 billion in March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bloomberg initially reported that the deal was under antitrust review in June. The deal is expected to close in early 2026, according to Reuters.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/05/google-gets-the-us-governments-green-light-to-acquire-wiz-for-32b/</guid><pubDate>Wed, 05 Nov 2025 14:58:07 +0000</pubDate></item><item><title>[NEW] The enemy within: AI as the attack surface (AI News)</title><link>https://www.artificialintelligence-news.com/news/tenable-untenable-ai-assistant-attack-threats-what-enterprises-should-do/</link><description>&lt;p&gt;Boards of directors are pressing for productivity gains from large-language models and AI assistants. Yet the same features that makes AI useful – browsing live websites, remembering user context, and connecting to business apps – also expand the cyber attack surface.&lt;/p&gt;&lt;p&gt;Tenable researchers have published a set of vulnerabilities and attacks under the title “HackedGPT”, showing how indirect prompt injection and related techniques could enable data exfiltration and malware persistence. Some issues have been remediated, while others reportedly remain exploitable at the time of the Tenable disclosure, according to an advisory issued by the company.&lt;/p&gt;&lt;p&gt;Removing the inherent risks from AI assistants’ operations requires governance, controls, and operating methods that treat AI as a user or device, to the extent that the technology should be subject to strict audit and monitoring&lt;/p&gt;&lt;p&gt;The Tenable research shows the failures that can turn AI assistants into security issues. Indirect prompt injection hides instructions in web content that the assistant reads while browsing, instructions that trigger data access the user never intended. Another vector involves the use of a front-end query that seeds malicious instructions.&lt;/p&gt;&lt;p&gt;The business impact is clear, including the need for incident response, legal and regulatory review, and steps taken to reduce reputational harm.&lt;/p&gt;&lt;p&gt;Research already exists that shows assistants can leak personal or sensitive information through injection techniques, and AI vendors and cybersecurity experts have to patch issues as they emerge.&lt;/p&gt;&lt;p&gt;The pattern is familiar to anyone in the technology industry: as features expand, so do failure modes. Treating AI assistants as live, internet-facing applications – not productivity drivers – can improve resilience.&lt;/p&gt;&lt;h2&gt;How to govern AI assistants, in practice&lt;/h2&gt;&lt;h3&gt;1) Establish an AI system registry&lt;/h3&gt;&lt;p&gt;Inventory every model, assistant, or agent in use – in public cloud, on-premises, and software-as-a-service, in line with the NIST AI RMF Playbook. Record owner, purpose, capabilities (browsing, API connectors) and data domains accessed. Even without this AI asset list, “shadow agents” can persist with privileges no one tracks. Shadow AI – at one stage encouraged by the likes of Microsoft, who encouraged users to deploy home Copilot licences at work – is a significant threat.&lt;/p&gt;&lt;h3&gt;2) Separate identities for humans, services, and agents&lt;/h3&gt;&lt;p&gt;Identity and access management conflate user accounts, service accounts, and automation devices. Assistants that access websites, call tools, and write data need distinct identities and be subject to zero-trust policies of least-privilege. Mapping agent-to-agent chains (who asked whom to do what, over which data, and when) is a bare minimum crumb trail that may ensure some degree of accountability. It’s worth noting that agentic AI is susceptible to ‘creative’ output and actions, yet unlike human staff, are not constrained by disciplinary policies.&lt;/p&gt;&lt;h3&gt;3) Constrain risky features by context&lt;/h3&gt;&lt;p&gt;Make browsing and independent actions taken by AI assistants opt-in per use case. For customer-facing assistants, set short retention times unless there’s a strong reason and a lawful basis otherwise. For internal engineering, use AI assistants but only in segregated projects with strict logging. Apply data-loss-prevention to connector traffic if assistants can reach file stores, messaging, or e-mail. Previous plugin and connector issues demonstrate how integrations increase exposure.&lt;/p&gt;&lt;h3&gt;4) Monitor like any internet-facing app&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Capture assistant actions and tool calls as structured logs.&lt;/li&gt;&lt;li&gt;Alert on anomalies: sudden spikes in browsing to unfamiliar domains; attempts to summarise opaque code blocks; unusual memory-write bursts; or connector access outside policy boundaries.&lt;/li&gt;&lt;li&gt;Incorporate injection tests into pre-production checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;5) Build the human muscle&lt;/h3&gt;&lt;p&gt;Train developers, cloud engineers, and analysts to recognise injection symptoms. Encourage users to report odd behaviour (e.g., an assistant unexpectedly summarising content from a site they didn’t open). Make it normal to quarantine an assistant, clear memory, and rotate its credentials after suspicious events. The skills gap is real; without upskilling, governance will lag adoption.&lt;/p&gt;&lt;h2&gt;Decision points for IT and cloud leaders&lt;/h2&gt;&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Question&lt;/th&gt;&lt;th&gt;Why it matters&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Which assistants can browse the web or write data?&lt;/td&gt;&lt;td&gt;Browsing and memory are common injection and persistence paths; constrain per use case.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Do agents have distinct identities and auditable delegation?&lt;/td&gt;&lt;td&gt;Prevents “who did what?” gaps when instructions are seeded indirectly.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Is there a registry of AI systems with owners, scopes, and retention?&lt;/td&gt;&lt;td&gt;Supports governance, right-sizing of controls, and budget visibility.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;How are connectors and plugins governed?&lt;/td&gt;&lt;td&gt;Third-party integrations have a history of security issues; apply least privilege and DLP.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Do we test for 0-click and 1-click vectors before go-live?&lt;/td&gt;&lt;td&gt;Public research shows both are feasible via crafted links or content.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Are vendors patching promptly and publishing fixes?&lt;/td&gt;&lt;td&gt;Feature velocity means new issues will appear; verify responsiveness.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h2&gt;Risks, cost visibility, and the human factor&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Hidden cost: assistants that browse or retain memory consume compute, storage, and egress in ways finance teams and those monitoring per-cycle Xaas use may not have modelled. A registry and metering reduce surprises.&lt;/li&gt;&lt;li&gt;Governance gaps: audit and compliance frameworks built for human users won’t automatically capture agent-to-agent delegation. Align controls according to OWASP LLM risks and NIST AI RMF categories.&lt;/li&gt;&lt;li&gt;Security risk: indirect prompt injection can be invisible to users, passed from media, text or code formatting, as shown by research.&lt;/li&gt;&lt;li&gt;Skills gap: many teams haven’t yet merged AI/ML and cybersecurity practices. Invest in training that covers assistant threat-modelling and injection testing.&lt;/li&gt;&lt;li&gt;Evolving posture: expect a cadence of new flaws and fixes. OpenAI’s remediation of a zero-click path in late 2025 is a reminder that vendor posture changes quickly and needs verification.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Bottom line&lt;/h2&gt;&lt;p&gt;The lesson for executives is simple: treat AI assistants as powerful, networked applications with their own lifecycle and a propensity for both being the subject of attack and for taking unpredictable action. Put a registry in place, separate identities, constrain risky features by default, log everything meaningful, and rehearse containment.&lt;/p&gt;&lt;p&gt;With these guardrails in place, agentic AI is more likely to deliver measurable efficiency and resilience – without quietly becoming your newest breach vector.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “The Enemy Within Unleashed” by aha42 | tehaha is licensed under CC BY-NC 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Boards of directors are pressing for productivity gains from large-language models and AI assistants. Yet the same features that makes AI useful – browsing live websites, remembering user context, and connecting to business apps – also expand the cyber attack surface.&lt;/p&gt;&lt;p&gt;Tenable researchers have published a set of vulnerabilities and attacks under the title “HackedGPT”, showing how indirect prompt injection and related techniques could enable data exfiltration and malware persistence. Some issues have been remediated, while others reportedly remain exploitable at the time of the Tenable disclosure, according to an advisory issued by the company.&lt;/p&gt;&lt;p&gt;Removing the inherent risks from AI assistants’ operations requires governance, controls, and operating methods that treat AI as a user or device, to the extent that the technology should be subject to strict audit and monitoring&lt;/p&gt;&lt;p&gt;The Tenable research shows the failures that can turn AI assistants into security issues. Indirect prompt injection hides instructions in web content that the assistant reads while browsing, instructions that trigger data access the user never intended. Another vector involves the use of a front-end query that seeds malicious instructions.&lt;/p&gt;&lt;p&gt;The business impact is clear, including the need for incident response, legal and regulatory review, and steps taken to reduce reputational harm.&lt;/p&gt;&lt;p&gt;Research already exists that shows assistants can leak personal or sensitive information through injection techniques, and AI vendors and cybersecurity experts have to patch issues as they emerge.&lt;/p&gt;&lt;p&gt;The pattern is familiar to anyone in the technology industry: as features expand, so do failure modes. Treating AI assistants as live, internet-facing applications – not productivity drivers – can improve resilience.&lt;/p&gt;&lt;h2&gt;How to govern AI assistants, in practice&lt;/h2&gt;&lt;h3&gt;1) Establish an AI system registry&lt;/h3&gt;&lt;p&gt;Inventory every model, assistant, or agent in use – in public cloud, on-premises, and software-as-a-service, in line with the NIST AI RMF Playbook. Record owner, purpose, capabilities (browsing, API connectors) and data domains accessed. Even without this AI asset list, “shadow agents” can persist with privileges no one tracks. Shadow AI – at one stage encouraged by the likes of Microsoft, who encouraged users to deploy home Copilot licences at work – is a significant threat.&lt;/p&gt;&lt;h3&gt;2) Separate identities for humans, services, and agents&lt;/h3&gt;&lt;p&gt;Identity and access management conflate user accounts, service accounts, and automation devices. Assistants that access websites, call tools, and write data need distinct identities and be subject to zero-trust policies of least-privilege. Mapping agent-to-agent chains (who asked whom to do what, over which data, and when) is a bare minimum crumb trail that may ensure some degree of accountability. It’s worth noting that agentic AI is susceptible to ‘creative’ output and actions, yet unlike human staff, are not constrained by disciplinary policies.&lt;/p&gt;&lt;h3&gt;3) Constrain risky features by context&lt;/h3&gt;&lt;p&gt;Make browsing and independent actions taken by AI assistants opt-in per use case. For customer-facing assistants, set short retention times unless there’s a strong reason and a lawful basis otherwise. For internal engineering, use AI assistants but only in segregated projects with strict logging. Apply data-loss-prevention to connector traffic if assistants can reach file stores, messaging, or e-mail. Previous plugin and connector issues demonstrate how integrations increase exposure.&lt;/p&gt;&lt;h3&gt;4) Monitor like any internet-facing app&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Capture assistant actions and tool calls as structured logs.&lt;/li&gt;&lt;li&gt;Alert on anomalies: sudden spikes in browsing to unfamiliar domains; attempts to summarise opaque code blocks; unusual memory-write bursts; or connector access outside policy boundaries.&lt;/li&gt;&lt;li&gt;Incorporate injection tests into pre-production checks.&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;5) Build the human muscle&lt;/h3&gt;&lt;p&gt;Train developers, cloud engineers, and analysts to recognise injection symptoms. Encourage users to report odd behaviour (e.g., an assistant unexpectedly summarising content from a site they didn’t open). Make it normal to quarantine an assistant, clear memory, and rotate its credentials after suspicious events. The skills gap is real; without upskilling, governance will lag adoption.&lt;/p&gt;&lt;h2&gt;Decision points for IT and cloud leaders&lt;/h2&gt;&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Question&lt;/th&gt;&lt;th&gt;Why it matters&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Which assistants can browse the web or write data?&lt;/td&gt;&lt;td&gt;Browsing and memory are common injection and persistence paths; constrain per use case.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Do agents have distinct identities and auditable delegation?&lt;/td&gt;&lt;td&gt;Prevents “who did what?” gaps when instructions are seeded indirectly.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Is there a registry of AI systems with owners, scopes, and retention?&lt;/td&gt;&lt;td&gt;Supports governance, right-sizing of controls, and budget visibility.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;How are connectors and plugins governed?&lt;/td&gt;&lt;td&gt;Third-party integrations have a history of security issues; apply least privilege and DLP.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Do we test for 0-click and 1-click vectors before go-live?&lt;/td&gt;&lt;td&gt;Public research shows both are feasible via crafted links or content.&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Are vendors patching promptly and publishing fixes?&lt;/td&gt;&lt;td&gt;Feature velocity means new issues will appear; verify responsiveness.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h2&gt;Risks, cost visibility, and the human factor&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;Hidden cost: assistants that browse or retain memory consume compute, storage, and egress in ways finance teams and those monitoring per-cycle Xaas use may not have modelled. A registry and metering reduce surprises.&lt;/li&gt;&lt;li&gt;Governance gaps: audit and compliance frameworks built for human users won’t automatically capture agent-to-agent delegation. Align controls according to OWASP LLM risks and NIST AI RMF categories.&lt;/li&gt;&lt;li&gt;Security risk: indirect prompt injection can be invisible to users, passed from media, text or code formatting, as shown by research.&lt;/li&gt;&lt;li&gt;Skills gap: many teams haven’t yet merged AI/ML and cybersecurity practices. Invest in training that covers assistant threat-modelling and injection testing.&lt;/li&gt;&lt;li&gt;Evolving posture: expect a cadence of new flaws and fixes. OpenAI’s remediation of a zero-click path in late 2025 is a reminder that vendor posture changes quickly and needs verification.&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Bottom line&lt;/h2&gt;&lt;p&gt;The lesson for executives is simple: treat AI assistants as powerful, networked applications with their own lifecycle and a propensity for both being the subject of attack and for taking unpredictable action. Put a registry in place, separate identities, constrain risky features by default, log everything meaningful, and rehearse containment.&lt;/p&gt;&lt;p&gt;With these guardrails in place, agentic AI is more likely to deliver measurable efficiency and resilience – without quietly becoming your newest breach vector.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “The Enemy Within Unleashed” by aha42 | tehaha is licensed under CC BY-NC 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/tenable-untenable-ai-assistant-attack-threats-what-enterprises-should-do/</guid><pubDate>Wed, 05 Nov 2025 14:59:10 +0000</pubDate></item><item><title>[NEW] Teaching robots to map large environments (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/teaching-robots-to-map-large-environments-1105</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Grounded-Transformer-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;A robot searching for workers trapped in a partially collapsed mine shaft must rapidly generate a map of the scene and identify its location within that scene as it navigates the treacherous terrain.&lt;/p&gt;&lt;p&gt;Researchers have recently started building powerful machine-learning models to perform this complex task using only images from the robot’s onboard cameras, but even the best models can only process a few images at a time. In a real-world disaster where every second counts, a search-and-rescue robot would need to quickly traverse large areas and process thousands of images to complete its mission.&lt;/p&gt;&lt;p&gt;To overcome this problem, MIT researchers drew on ideas from both recent artificial intelligence vision models and classical computer vision to develop a new system that can process an arbitrary number of images. Their system accurately generates 3D maps of complicated scenes like a crowded office corridor in a matter of seconds.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The AI-driven system incrementally creates and aligns smaller submaps of the scene, which it stitches together to reconstruct a full 3D map while estimating the robot’s position in real-time.&lt;/p&gt;&lt;p&gt;Unlike many other approaches, their technique does not require calibrated cameras or an expert to tune a complex system implementation. The simpler nature of their approach, coupled with the speed and quality of the 3D reconstructions, would make it easier to scale up for real-world applications.&lt;/p&gt;&lt;p&gt;Beyond helping search-and-rescue robots navigate, this method could be used to make extended reality applications for wearable devices like VR headsets or enable industrial robots to quickly find and move goods inside a warehouse.&lt;/p&gt;&lt;p&gt;“For robots to accomplish increasingly complex tasks, they need much more complex map representations of the world around them. But at the same time, we don’t want to make it harder to implement these maps in practice. We’ve shown that it is possible to generate an accurate 3D reconstruction in a matter of seconds with a tool that works out of the box,” says Dominic Maggio, an MIT graduate student and lead author of a paper on this method.&lt;/p&gt;&lt;p&gt;Maggio is joined on the paper by postdoc Hyungtae Lim and senior author Luca Carlone, associate professor in MIT’s Department of Aeronautics and Astronautics (AeroAstro), principal investigator in the Laboratory for Information and Decision Systems (LIDS), and director of the MIT SPARK Laboratory. The research will be presented at the Conference on Neural Information Processing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mapping out a solution&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For years, researchers have been grappling with an essential element of robotic navigation called simultaneous localization and mapping (SLAM). In SLAM, a robot recreates a map of its environment while orienting itself within the space.&lt;/p&gt;&lt;p&gt;Traditional optimization methods for this task tend to fail in challenging scenes, or they require the robot’s onboard cameras to be calibrated beforehand. To avoid these pitfalls, researchers train machine-learning models to learn this task from data.&lt;/p&gt;&lt;p&gt;While they are simpler to implement, even the best models can only process about 60 camera images at a time, making them infeasible for applications where a robot needs to move quickly through a varied environment while processing thousands of images.&lt;/p&gt;&lt;p&gt;To solve this problem, the MIT researchers designed a system that generates smaller submaps of the scene instead of the entire map. Their method “glues” these submaps together into one overall 3D reconstruction. The model is still only processing a few images at a time, but the system can recreate larger scenes much faster by stitching smaller submaps together.&lt;/p&gt;&lt;p&gt;“This seemed like a very simple solution, but when I first tried it, I was surprised that it didn’t work that well,” Maggio says.&lt;/p&gt;&lt;p&gt;Searching for an explanation, he dug into computer vision research papers from the 1980s and 1990s. Through this analysis, Maggio realized that errors in the way the machine-learning models process images made aligning submaps a more complex problem.&lt;/p&gt;&lt;p&gt;Traditional methods align submaps by applying rotations and translations until they line up. But these new models can introduce some ambiguity into the submaps, which makes them harder to align. For instance, a 3D submap of a one side of a room might have walls that are slightly bent or stretched. Simply rotating and translating these deformed submaps to align them doesn’t work.&lt;/p&gt;&lt;p&gt;“We need to make sure all the submaps are deformed in a consistent way so we can align them well with each other,” Carlone explains.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A more flexible approach&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Borrowing ideas from classical computer vision, the researchers developed a more flexible, mathematical technique that can represent all the deformations in these submaps. By applying mathematical transformations to each submap, this more flexible method can align them in a way that addresses the ambiguity.&lt;/p&gt;&lt;p&gt;Based on input images, the system outputs a 3D reconstruction of the scene and estimates of the camera locations, which the robot would use to localize itself in the space.&lt;/p&gt;&lt;p&gt;“Once Dominic had the intuition to bridge these two worlds — learning-based approaches and traditional optimization methods — the implementation was fairly straightforward,” Carlone says. “Coming up with something this effective and simple has potential for a lot of applications.&lt;/p&gt;&lt;p&gt;Their system performed faster with less reconstruction error than other methods, without requiring special cameras or additional tools to process data. The researchers generated close-to-real-time 3D reconstructions of complex scenes like the inside of the MIT Chapel using only short videos captured on a cell phone.&lt;/p&gt;&lt;p&gt;The average error in these 3D reconstructions was less than 5 centimeters.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to make their method more reliable for especially complicated scenes and work toward implementing it on real robots in challenging settings.&lt;/p&gt;&lt;p&gt;“Knowing about traditional geometry pays off. If you understand deeply what is going on in the model, you can get much better results and make things much more scalable,” Carlone says.&lt;/p&gt;&lt;p&gt;This work is supported, in part, by the U.S. National Science Foundation, U.S. Office of Naval Research, and the National Research Foundation of Korea. Carlone, currently on sabbatical as an Amazon Scholar, completed this work before he joined Amazon.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Grounded-Transformer-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;A robot searching for workers trapped in a partially collapsed mine shaft must rapidly generate a map of the scene and identify its location within that scene as it navigates the treacherous terrain.&lt;/p&gt;&lt;p&gt;Researchers have recently started building powerful machine-learning models to perform this complex task using only images from the robot’s onboard cameras, but even the best models can only process a few images at a time. In a real-world disaster where every second counts, a search-and-rescue robot would need to quickly traverse large areas and process thousands of images to complete its mission.&lt;/p&gt;&lt;p&gt;To overcome this problem, MIT researchers drew on ideas from both recent artificial intelligence vision models and classical computer vision to develop a new system that can process an arbitrary number of images. Their system accurately generates 3D maps of complicated scenes like a crowded office corridor in a matter of seconds.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The AI-driven system incrementally creates and aligns smaller submaps of the scene, which it stitches together to reconstruct a full 3D map while estimating the robot’s position in real-time.&lt;/p&gt;&lt;p&gt;Unlike many other approaches, their technique does not require calibrated cameras or an expert to tune a complex system implementation. The simpler nature of their approach, coupled with the speed and quality of the 3D reconstructions, would make it easier to scale up for real-world applications.&lt;/p&gt;&lt;p&gt;Beyond helping search-and-rescue robots navigate, this method could be used to make extended reality applications for wearable devices like VR headsets or enable industrial robots to quickly find and move goods inside a warehouse.&lt;/p&gt;&lt;p&gt;“For robots to accomplish increasingly complex tasks, they need much more complex map representations of the world around them. But at the same time, we don’t want to make it harder to implement these maps in practice. We’ve shown that it is possible to generate an accurate 3D reconstruction in a matter of seconds with a tool that works out of the box,” says Dominic Maggio, an MIT graduate student and lead author of a paper on this method.&lt;/p&gt;&lt;p&gt;Maggio is joined on the paper by postdoc Hyungtae Lim and senior author Luca Carlone, associate professor in MIT’s Department of Aeronautics and Astronautics (AeroAstro), principal investigator in the Laboratory for Information and Decision Systems (LIDS), and director of the MIT SPARK Laboratory. The research will be presented at the Conference on Neural Information Processing Systems.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Mapping out a solution&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For years, researchers have been grappling with an essential element of robotic navigation called simultaneous localization and mapping (SLAM). In SLAM, a robot recreates a map of its environment while orienting itself within the space.&lt;/p&gt;&lt;p&gt;Traditional optimization methods for this task tend to fail in challenging scenes, or they require the robot’s onboard cameras to be calibrated beforehand. To avoid these pitfalls, researchers train machine-learning models to learn this task from data.&lt;/p&gt;&lt;p&gt;While they are simpler to implement, even the best models can only process about 60 camera images at a time, making them infeasible for applications where a robot needs to move quickly through a varied environment while processing thousands of images.&lt;/p&gt;&lt;p&gt;To solve this problem, the MIT researchers designed a system that generates smaller submaps of the scene instead of the entire map. Their method “glues” these submaps together into one overall 3D reconstruction. The model is still only processing a few images at a time, but the system can recreate larger scenes much faster by stitching smaller submaps together.&lt;/p&gt;&lt;p&gt;“This seemed like a very simple solution, but when I first tried it, I was surprised that it didn’t work that well,” Maggio says.&lt;/p&gt;&lt;p&gt;Searching for an explanation, he dug into computer vision research papers from the 1980s and 1990s. Through this analysis, Maggio realized that errors in the way the machine-learning models process images made aligning submaps a more complex problem.&lt;/p&gt;&lt;p&gt;Traditional methods align submaps by applying rotations and translations until they line up. But these new models can introduce some ambiguity into the submaps, which makes them harder to align. For instance, a 3D submap of a one side of a room might have walls that are slightly bent or stretched. Simply rotating and translating these deformed submaps to align them doesn’t work.&lt;/p&gt;&lt;p&gt;“We need to make sure all the submaps are deformed in a consistent way so we can align them well with each other,” Carlone explains.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A more flexible approach&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Borrowing ideas from classical computer vision, the researchers developed a more flexible, mathematical technique that can represent all the deformations in these submaps. By applying mathematical transformations to each submap, this more flexible method can align them in a way that addresses the ambiguity.&lt;/p&gt;&lt;p&gt;Based on input images, the system outputs a 3D reconstruction of the scene and estimates of the camera locations, which the robot would use to localize itself in the space.&lt;/p&gt;&lt;p&gt;“Once Dominic had the intuition to bridge these two worlds — learning-based approaches and traditional optimization methods — the implementation was fairly straightforward,” Carlone says. “Coming up with something this effective and simple has potential for a lot of applications.&lt;/p&gt;&lt;p&gt;Their system performed faster with less reconstruction error than other methods, without requiring special cameras or additional tools to process data. The researchers generated close-to-real-time 3D reconstructions of complex scenes like the inside of the MIT Chapel using only short videos captured on a cell phone.&lt;/p&gt;&lt;p&gt;The average error in these 3D reconstructions was less than 5 centimeters.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to make their method more reliable for especially complicated scenes and work toward implementing it on real robots in challenging settings.&lt;/p&gt;&lt;p&gt;“Knowing about traditional geometry pays off. If you understand deeply what is going on in the model, you can get much better results and make things much more scalable,” Carlone says.&lt;/p&gt;&lt;p&gt;This work is supported, in part, by the U.S. National Science Foundation, U.S. Office of Naval Research, and the National Research Foundation of Korea. Carlone, currently on sabbatical as an Amazon Scholar, completed this work before he joined Amazon.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/teaching-robots-to-map-large-environments-1105</guid><pubDate>Wed, 05 Nov 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] This startup’s metal stacks could help solve AI’s massive heat problem (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/05/this-startups-metal-stacks-could-help-solve-ais-massive-heat-problem/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Nvidia announced its Rubin series of GPUs in March, it also dropped a bomb: Racks built with the Ultra version of the chip, which is expected to be released in 2027, could draw up to 600 kilowatts of electricity. That’s nearly twice as much juice as some of the fastest EV chargers can deliver today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As data center racks become that power hungry, one of the biggest hurdles will be figuring out how to keep them cool. One startup thinks stacks of metal are the answer.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alloy Enterprises has developed a technology that turns sheets of copper into solid cooling plates for GPUs and for peripheral chips, the supporting components like memory and networking hardware that account for about 20% of a server’s cooling load.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We didn’t care too much about that 20% when racks were 120 kilowatts,” Ali Forsyth, co-founder and CEO of Alloy Enterprises, told TechCrunch. But now, as racks have hit 480 kilowatts on their way to 600 kilowatts, engineers have to figure out how to liquid cool everything from RAM to networking chips, parts for which there are no solutions available today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alloy’s approach uses additive manufacturing (building objects layer by layer) to produce cold plates that are capable of squeezing into tight spots while withstanding the high pressures that liquid cooling can demand.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A copper cooling plate is shown against a white background." class="wp-image-3065035" height="463" src="https://techcrunch.com/wp-content/uploads/2025/11/NVIDIA-Blackwell-Copper-Alloy-Enterprises.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Alloy Enterprises&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;But the startup doesn’t use 3D printing. Rather, it takes sheets of metal and forces them to bond using a combination of heat and pressure. It’s more expensive than traditional machining, but cheaper than 3D printing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The result is a cold plate that, for all intents and purposes, is a single block of metal. There’s no seam, unlike machined products, and it’s solid metal, unlike 3D-printed versions, which can be porous. “We hit raw material properties,” Forsyth said. “The copper is just as strong as if you had machined it.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Most cold plates are machined, a process that uses tools to carve out features. Because the tools are large, each half of the plate has to be machined separately. The two halves are then sintered together — a process that fuses metal powders using heat — which introduces a seam that could potentially leak under high pressure. Alloy’s process, a type of diffusion bonding it calls “stack forging,” makes seamless cold plates.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stack forging can also make smaller features, down to 50 microns, about half the width of a human hair, allowing more coolant to flow past the metal. Alloy’s cold plates have 35% better thermal performance than competitors, Forsyth said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of the complexities of stack forging, Alloy does most of the internal design. Customers submit key specifications and dimensions, and the startup’s software helps translate those into a shape that works for the company’s manufacturing process.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In Alloy’s factory, rolls of copper are first prepped and cut to size. The features are then cut out using a laser. Parts of the design that the company doesn’t want to bond to each other are coated with an inhibitor. When completed, each slice of a cold plate is registered and stacked before heading into a diffusion bonding machine, which uses heat and pressure to press the stacked slices into a single piece of metal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Forsyth said her company is working with “all the big names” in the data center world, though she wouldn’t disclose specifics.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Initially, the company had designed the technology to work with a widely used aluminum alloy, but as it received more interest from data centers, it ported the process to work with copper, which conducts heat well and resists corrosion. When Alloy announced the product in June, “things just blew up,” Forsyth said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Nvidia announced its Rubin series of GPUs in March, it also dropped a bomb: Racks built with the Ultra version of the chip, which is expected to be released in 2027, could draw up to 600 kilowatts of electricity. That’s nearly twice as much juice as some of the fastest EV chargers can deliver today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As data center racks become that power hungry, one of the biggest hurdles will be figuring out how to keep them cool. One startup thinks stacks of metal are the answer.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alloy Enterprises has developed a technology that turns sheets of copper into solid cooling plates for GPUs and for peripheral chips, the supporting components like memory and networking hardware that account for about 20% of a server’s cooling load.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We didn’t care too much about that 20% when racks were 120 kilowatts,” Ali Forsyth, co-founder and CEO of Alloy Enterprises, told TechCrunch. But now, as racks have hit 480 kilowatts on their way to 600 kilowatts, engineers have to figure out how to liquid cool everything from RAM to networking chips, parts for which there are no solutions available today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alloy’s approach uses additive manufacturing (building objects layer by layer) to produce cold plates that are capable of squeezing into tight spots while withstanding the high pressures that liquid cooling can demand.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A copper cooling plate is shown against a white background." class="wp-image-3065035" height="463" src="https://techcrunch.com/wp-content/uploads/2025/11/NVIDIA-Blackwell-Copper-Alloy-Enterprises.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Alloy Enterprises&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;But the startup doesn’t use 3D printing. Rather, it takes sheets of metal and forces them to bond using a combination of heat and pressure. It’s more expensive than traditional machining, but cheaper than 3D printing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The result is a cold plate that, for all intents and purposes, is a single block of metal. There’s no seam, unlike machined products, and it’s solid metal, unlike 3D-printed versions, which can be porous. “We hit raw material properties,” Forsyth said. “The copper is just as strong as if you had machined it.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Most cold plates are machined, a process that uses tools to carve out features. Because the tools are large, each half of the plate has to be machined separately. The two halves are then sintered together — a process that fuses metal powders using heat — which introduces a seam that could potentially leak under high pressure. Alloy’s process, a type of diffusion bonding it calls “stack forging,” makes seamless cold plates.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stack forging can also make smaller features, down to 50 microns, about half the width of a human hair, allowing more coolant to flow past the metal. Alloy’s cold plates have 35% better thermal performance than competitors, Forsyth said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of the complexities of stack forging, Alloy does most of the internal design. Customers submit key specifications and dimensions, and the startup’s software helps translate those into a shape that works for the company’s manufacturing process.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In Alloy’s factory, rolls of copper are first prepped and cut to size. The features are then cut out using a laser. Parts of the design that the company doesn’t want to bond to each other are coated with an inhibitor. When completed, each slice of a cold plate is registered and stacked before heading into a diffusion bonding machine, which uses heat and pressure to press the stacked slices into a single piece of metal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Forsyth said her company is working with “all the big names” in the data center world, though she wouldn’t disclose specifics.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Initially, the company had designed the technology to work with a widely used aluminum alloy, but as it received more interest from data centers, it ported the process to work with copper, which conducts heat well and resists corrosion. When Alloy announced the product in June, “things just blew up,” Forsyth said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/05/this-startups-metal-stacks-could-help-solve-ais-massive-heat-problem/</guid><pubDate>Wed, 05 Nov 2025 15:15:00 +0000</pubDate></item><item><title>[NEW] Forecasting the future of forests with AI: From counting losses to predicting risk (The latest research from Google)</title><link>https://research.google/blog/forecasting-the-future-of-forests-with-ai-from-counting-losses-to-predicting-risk/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;A scalable satellite approach&lt;/h2&gt;&lt;p&gt;To overcome these challenges, we adopt a “pure satellite” model, where the only inputs are derived from satellites. We tested raw satellite inputs from the Landsat and Sentinel 2 satellites. We also included a satellite-derived input we refer to as “change history”, which identifies each pixel that has already been deforested and provides a year for when that deforestation occurred. We trained and evaluated the model using satellite-derived labels of deforestation.&lt;/p&gt;&lt;p&gt;The pure satellite approach provides consistency, in that we can apply the exact same method anywhere on Earth, allowing for meaningful comparisons between different regions. It also makes our model future proof — these satellite data streams will continue for years to come, so we can repeat the method to give updated predictions of risk and examine how risk is changing through time.&lt;/p&gt;&lt;p&gt;To achieve accuracy and scalability, we developed a custom model based on vision transformers. The model receives a whole tile of satellite pixels as input, which is crucial to capture the spatial context of the landscape and recent deforestation (as captured in the change history). It then outputs a whole tile’s worth of predictions in one pass, which makes the model scalable to large regions.&lt;/p&gt;&lt;p&gt;We found that our model was able to reproduce, or exceed, the accuracy of methods based on specialized inputs (such as roads), accurately predicting tile-to-tile variation in the amount of deforestation, and, within tiles, accurately predicting which pixels were the most likely to become deforested next.&lt;/p&gt;&lt;p&gt;Surprisingly, we found that by far the most important satellite input was the simplest, the change history. So much so that a model receiving only this input could provide predictions with accuracy metrics indistinguishable from models using the full, raw satellite data. In retrospect we can see that the change history is a small, but highly information dense, model input — including information on tile-to-tile variation in recent deforestation rates, and how these are trending through time, and also capturing moving deforestation fronts within tiles.&lt;/p&gt;&lt;p&gt;To promote transparency and repeatability, we are releasing the training and evaluation data used in this work, as a benchmark. This allows the wider machine learning community to verify our results; to potentially extract deeper understanding of why the model makes certain predictions; and ultimately, to build and compare improved deforestation risk models.&lt;/p&gt;&lt;p&gt;Moreover, our benchmark and paper provide a clear template for scaling this approach globally — to model tropical deforestation across Latin America and Africa, and eventually, to temperate and boreal latitudes where forest loss is often driven by different dynamics, such as cattle ranching and fire.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;h2&gt;A scalable satellite approach&lt;/h2&gt;&lt;p&gt;To overcome these challenges, we adopt a “pure satellite” model, where the only inputs are derived from satellites. We tested raw satellite inputs from the Landsat and Sentinel 2 satellites. We also included a satellite-derived input we refer to as “change history”, which identifies each pixel that has already been deforested and provides a year for when that deforestation occurred. We trained and evaluated the model using satellite-derived labels of deforestation.&lt;/p&gt;&lt;p&gt;The pure satellite approach provides consistency, in that we can apply the exact same method anywhere on Earth, allowing for meaningful comparisons between different regions. It also makes our model future proof — these satellite data streams will continue for years to come, so we can repeat the method to give updated predictions of risk and examine how risk is changing through time.&lt;/p&gt;&lt;p&gt;To achieve accuracy and scalability, we developed a custom model based on vision transformers. The model receives a whole tile of satellite pixels as input, which is crucial to capture the spatial context of the landscape and recent deforestation (as captured in the change history). It then outputs a whole tile’s worth of predictions in one pass, which makes the model scalable to large regions.&lt;/p&gt;&lt;p&gt;We found that our model was able to reproduce, or exceed, the accuracy of methods based on specialized inputs (such as roads), accurately predicting tile-to-tile variation in the amount of deforestation, and, within tiles, accurately predicting which pixels were the most likely to become deforested next.&lt;/p&gt;&lt;p&gt;Surprisingly, we found that by far the most important satellite input was the simplest, the change history. So much so that a model receiving only this input could provide predictions with accuracy metrics indistinguishable from models using the full, raw satellite data. In retrospect we can see that the change history is a small, but highly information dense, model input — including information on tile-to-tile variation in recent deforestation rates, and how these are trending through time, and also capturing moving deforestation fronts within tiles.&lt;/p&gt;&lt;p&gt;To promote transparency and repeatability, we are releasing the training and evaluation data used in this work, as a benchmark. This allows the wider machine learning community to verify our results; to potentially extract deeper understanding of why the model makes certain predictions; and ultimately, to build and compare improved deforestation risk models.&lt;/p&gt;&lt;p&gt;Moreover, our benchmark and paper provide a clear template for scaling this approach globally — to model tropical deforestation across Latin America and Africa, and eventually, to temperate and boreal latitudes where forest loss is often driven by different dynamics, such as cattle ranching and fire.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/forecasting-the-future-of-forests-with-ai-from-counting-losses-to-predicting-risk/</guid><pubDate>Wed, 05 Nov 2025 15:41:00 +0000</pubDate></item><item><title>[NEW] Keep CALM: New model design could fix high enterprise AI costs (AI News)</title><link>https://www.artificialintelligence-news.com/news/keep-calm-new-model-design-fix-high-enterprise-ai-costs/</link><description>&lt;p&gt;Enterprise leaders grappling with the steep costs of deploying AI models could find a reprieve thanks to a new architecture design.&lt;/p&gt;&lt;p&gt;While the capabilities of generative AI are attractive, their immense computational demands for both training and inference result in prohibitive expenses and mounting environmental concerns. At the centre of this inefficiency is the models’ “fundamental bottleneck” of an autoregressive process that generates text sequentially, token-by-token.&lt;/p&gt;&lt;p&gt;For enterprises processing vast data streams, from IoT networks to financial markets, this limitation makes generating long-form analysis both slow and economically challenging. However, a new research paper from Tencent AI and Tsinghua University proposes an alternative.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-a-new-approach-to-ai-efficiency"&gt;A new approach to AI efficiency&lt;/h3&gt;&lt;p&gt;The research introduces Continuous Autoregressive Language Models (CALM). This method re-engineers the generation process to predict a continuous vector rather than a discrete token.&lt;/p&gt;&lt;p&gt;A high-fidelity autoencoder “compress[es] a chunk of K tokens into a single continuous vector,” which holds a much higher semantic bandwidth.&lt;/p&gt;&lt;p&gt;Instead of processing something like “the”, “cat”, “sat” in three steps, the model compresses them into one. This design directly “reduces the number of generative steps,” attacking the computational load.&lt;/p&gt;&lt;p&gt;The experimental results demonstrate a better performance-compute trade-off. A CALM AI model grouping four tokens delivered performance “comparable to strong discrete baselines, but at a significantly lower computational cost” for an enterprise.&lt;/p&gt;&lt;p&gt;One CALM model, for instance, required 44 percent fewer training FLOPs and 34 percent fewer inference FLOPs than a baseline Transformer of similar capability. This points to a saving on both the initial capital expense of training and the recurring operational expense of inference.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-rebuilding-the-toolkit-for-the-continuous-domain"&gt;Rebuilding the toolkit for the continuous domain&lt;/h3&gt;&lt;p&gt;Moving from a finite, discrete vocabulary to an infinite, continuous vector space breaks the standard LLM toolkit. The researchers had to develop a “comprehensive likelihood-free framework” to make the new model viable.&lt;/p&gt;&lt;p&gt;For training, the model cannot use a standard softmax layer or maximum likelihood estimation. To solve this, the team used a “likelihood-free” objective with an Energy Transformer, which rewards the model for accurate predictions without computing explicit probabilities.&lt;/p&gt;&lt;p&gt;This new training method also required a new evaluation metric. Standard benchmarks like Perplexity are inapplicable as they rely on the same likelihoods the model no longer computes.&lt;/p&gt;&lt;p&gt;The team proposed BrierLM, a novel metric based on the Brier score that can be estimated purely from model samples. Validation confirmed BrierLM as a reliable alternative, showing a “Spearman’s rank correlation of -0.991” with traditional loss metrics.&lt;/p&gt;&lt;p&gt;Finally, the framework restores controlled generation, a key feature for enterprise use. Standard temperature sampling is impossible without a probability distribution. The paper introduces a new “likelihood-free sampling algorithm,” including a practical batch approximation method, to manage the trade-off between output accuracy and diversity.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-reducing-enterprise-ai-costs"&gt;Reducing enterprise AI costs&lt;/h3&gt;&lt;p&gt;This research offers a glimpse into a future where generative AI is not defined purely by ever-larger parameter counts, but by architectural efficiency.&lt;/p&gt;&lt;p&gt;The current path of scaling models is hitting a wall of diminishing returns and escalating costs. The CALM framework establishes a “new design axis for LLM scaling: increasing the semantic bandwidth of each generative step”.&lt;/p&gt;&lt;p&gt;While this is a research framework and not an off-the-shelf product, it points to a powerful and scalable pathway towards ultra-efficient language models. When evaluating vendor roadmaps, tech leaders should look beyond model size and begin asking about architectural efficiency.&lt;/p&gt;&lt;p&gt;The ability to reduce FLOPs per generated token will become a defining competitive advantage, enabling AI to be deployed more economically and sustainably across the enterprise to reduce costs—from the data centre to data-heavy edge applications.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Flawed AI benchmarks put enterprise budgets at risk&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Enterprise leaders grappling with the steep costs of deploying AI models could find a reprieve thanks to a new architecture design.&lt;/p&gt;&lt;p&gt;While the capabilities of generative AI are attractive, their immense computational demands for both training and inference result in prohibitive expenses and mounting environmental concerns. At the centre of this inefficiency is the models’ “fundamental bottleneck” of an autoregressive process that generates text sequentially, token-by-token.&lt;/p&gt;&lt;p&gt;For enterprises processing vast data streams, from IoT networks to financial markets, this limitation makes generating long-form analysis both slow and economically challenging. However, a new research paper from Tencent AI and Tsinghua University proposes an alternative.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-a-new-approach-to-ai-efficiency"&gt;A new approach to AI efficiency&lt;/h3&gt;&lt;p&gt;The research introduces Continuous Autoregressive Language Models (CALM). This method re-engineers the generation process to predict a continuous vector rather than a discrete token.&lt;/p&gt;&lt;p&gt;A high-fidelity autoencoder “compress[es] a chunk of K tokens into a single continuous vector,” which holds a much higher semantic bandwidth.&lt;/p&gt;&lt;p&gt;Instead of processing something like “the”, “cat”, “sat” in three steps, the model compresses them into one. This design directly “reduces the number of generative steps,” attacking the computational load.&lt;/p&gt;&lt;p&gt;The experimental results demonstrate a better performance-compute trade-off. A CALM AI model grouping four tokens delivered performance “comparable to strong discrete baselines, but at a significantly lower computational cost” for an enterprise.&lt;/p&gt;&lt;p&gt;One CALM model, for instance, required 44 percent fewer training FLOPs and 34 percent fewer inference FLOPs than a baseline Transformer of similar capability. This points to a saving on both the initial capital expense of training and the recurring operational expense of inference.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-rebuilding-the-toolkit-for-the-continuous-domain"&gt;Rebuilding the toolkit for the continuous domain&lt;/h3&gt;&lt;p&gt;Moving from a finite, discrete vocabulary to an infinite, continuous vector space breaks the standard LLM toolkit. The researchers had to develop a “comprehensive likelihood-free framework” to make the new model viable.&lt;/p&gt;&lt;p&gt;For training, the model cannot use a standard softmax layer or maximum likelihood estimation. To solve this, the team used a “likelihood-free” objective with an Energy Transformer, which rewards the model for accurate predictions without computing explicit probabilities.&lt;/p&gt;&lt;p&gt;This new training method also required a new evaluation metric. Standard benchmarks like Perplexity are inapplicable as they rely on the same likelihoods the model no longer computes.&lt;/p&gt;&lt;p&gt;The team proposed BrierLM, a novel metric based on the Brier score that can be estimated purely from model samples. Validation confirmed BrierLM as a reliable alternative, showing a “Spearman’s rank correlation of -0.991” with traditional loss metrics.&lt;/p&gt;&lt;p&gt;Finally, the framework restores controlled generation, a key feature for enterprise use. Standard temperature sampling is impossible without a probability distribution. The paper introduces a new “likelihood-free sampling algorithm,” including a practical batch approximation method, to manage the trade-off between output accuracy and diversity.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-reducing-enterprise-ai-costs"&gt;Reducing enterprise AI costs&lt;/h3&gt;&lt;p&gt;This research offers a glimpse into a future where generative AI is not defined purely by ever-larger parameter counts, but by architectural efficiency.&lt;/p&gt;&lt;p&gt;The current path of scaling models is hitting a wall of diminishing returns and escalating costs. The CALM framework establishes a “new design axis for LLM scaling: increasing the semantic bandwidth of each generative step”.&lt;/p&gt;&lt;p&gt;While this is a research framework and not an off-the-shelf product, it points to a powerful and scalable pathway towards ultra-efficient language models. When evaluating vendor roadmaps, tech leaders should look beyond model size and begin asking about architectural efficiency.&lt;/p&gt;&lt;p&gt;The ability to reduce FLOPs per generated token will become a defining competitive advantage, enabling AI to be deployed more economically and sustainably across the enterprise to reduce costs—from the data centre to data-heavy edge applications.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Flawed AI benchmarks put enterprise budgets at risk&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/keep-calm-new-model-design-fix-high-enterprise-ai-costs/</guid><pubDate>Wed, 05 Nov 2025 16:20:27 +0000</pubDate></item><item><title>[NEW] If you want to satiate AI’s hunger for power, Google suggests going to space (AI – Ars Technica)</title><link>https://arstechnica.com/space/2025/11/if-you-want-to-satiate-ais-hunger-for-power-google-suggests-going-to-space/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google engineers think they already have all the pieces needed to build a data center in orbit.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/googletpu-640x361.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/googletpu-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      With Project Suncatcher, Google will test its Tensor Processing Units on satellites. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;It was probably always when, not if, Google would add its name to the list of companies intrigued by the potential of orbiting data centers.&lt;/p&gt;
&lt;p&gt;Google announced Tuesday a new initiative, named Project Suncatcher, to examine the feasibility of bringing artificial intelligence to space. The idea is to deploy swarms of satellites in low-Earth orbit, each carrying Google’s AI accelerator chips designed for training, content generation, synthetic speech and vision, and predictive modeling. Google calls these chips Tensor Processing Units, or TPUs.&lt;/p&gt;
&lt;p&gt;“Project Suncatcher is a moonshot exploring a new frontier: equipping solar-powered satellite constellations with TPUs and free-space optical links to one day scale machine learning compute in space,” Google wrote in a blog post.&lt;/p&gt;
&lt;p&gt;“Like any moonshot, it’s going to require us to solve a lot of complex engineering challenges,” Google’s CEO, Sundar Pichai, wrote on X. Pichai noted that Google’s early tests show the company’s TPUs can withstand the intense radiation they will encounter in space. “However, significant challenges still remain like thermal management and on-orbit system reliability.”&lt;/p&gt;
&lt;h2&gt;The why and how&lt;/h2&gt;
&lt;p&gt;Ars reported on Google’s announcement on Tuesday, and Google published a research paper outlining the motivation for such a moonshot project. One of the authors, Travis Beals, spoke with Ars about Project Suncatcher&amp;nbsp;and offered his thoughts on why it just might work.&lt;/p&gt;
&lt;p&gt;“We’re just seeing so much demand from people for AI,” said Beals, senior director of Paradigms of Intelligence, a research team within Google. “So, we wanted to figure out a solution for compute that could work no matter how large demand might grow.”&lt;/p&gt;
&lt;p&gt;Higher demand will lead to bigger data centers consuming colossal amounts of electricity. According to the MIT Technology Review, AI alone could consume as much electricity annually as 22 percent of all US households by 2028. Cooling is also a problem, often requiring access to vast water resources, raising important questions about environmental sustainability.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google is looking to the sky to avoid potential bottlenecks. A satellite in space can access an infinite supply of renewable energy and an entire Universe to absorb heat.&lt;/p&gt;
&lt;p&gt;“If you think about a data center on Earth, it’s taking power in and it’s emitting heat out,” Beals said. “For us, it’s the satellite that’s doing the same. The satellite is going to have solar panels … They’re going to feed that power to the TPUs to do whatever compute we need them to do, and then the waste heat from the TPUs will be distributed out over a radiator that will then radiate that heat out into space.”&lt;/p&gt;
&lt;p&gt;Google envisions putting a legion of satellites into a special kind of orbit that rides along the day-night terminator, where sunlight meets darkness. This north-south, or polar, orbit would be synchronized with the Sun, allowing a satellite’s power-generating solar panels to remain continuously bathed in sunshine.&lt;/p&gt;
&lt;p&gt;“It’s much brighter even than the midday Sun on Earth because it’s not filtered by Earth’s atmosphere,” Beals said.&lt;/p&gt;
&lt;p&gt;This means a solar panel in space can produce up to eight times more power than the same collecting area on the ground, and you don’t need a lot of batteries to reserve electricity for nighttime. This may sound like the argument for space-based solar power, an idea first described by Isaac Asimov in his short story &lt;em&gt;Reason&lt;/em&gt; published in 1941. But instead of transmitting the electricity down to Earth for terrestrial use, orbiting data centers would tap into the power source in space.&lt;/p&gt;
&lt;p&gt;“As with many things, the ideas originate in science fiction, but it’s had a number of challenges, and one big one is, how do you get the power down to Earth?” Beals said. “So, instead of trying to figure out that, we’re embarking on this moonshot to bring [machine learning] compute chips into space, put them on satellites that have the solar panels and the radiators for cooling, and then integrate it all together so you don’t actually have to be powered on Earth.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2060480 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1017" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/f9_starlinks.jpg" width="1800" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      SpaceX is driving down launch costs, thanks to reusable rockets and an abundant volume of Starlink satellite launches.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          SpaceX

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google has a mixed record with its ambitious moonshot projects. One of the most prominent moonshot graduates is the self-driving car kit developer Waymo, which spun out to form a separate company in 2016 and is now operational. The Project Loon initiative to beam Internet signals from high-altitude balloons is one of the Google moonshots that didn’t make it.&lt;/p&gt;
&lt;p&gt;Ars published two stories last week on the promise of space-based data centers. One of the startups in this field, named Starcloud, is partnering with Nvidia, the world’s largest tech company by market capitalization, to build a 5 gigawatt orbital data center with enormous solar and cooling panels approximately 4 kilometers (2.5 miles) in width and length. In response to that story, Elon Musk said SpaceX is pursuing the same business opportunity but didn’t provide any details. It’s worth noting that Google holds an estimated 7 percent stake in SpaceX.&lt;/p&gt;
&lt;h2&gt;Strength in numbers&lt;/h2&gt;
&lt;p&gt;Google’s proposed architecture differs from that of Starcloud and Nvidia in an important way. Instead of putting up just one or a few massive computing nodes, Google wants to launch a fleet of smaller satellites that talk to one another through laser data links. Essentially, a satellite swarm would function as a single data center, using light-speed interconnectivity to aggregate computing power hundreds of miles over our heads.&lt;/p&gt;
&lt;p&gt;If that sounds implausible, take a moment to think about what companies are already doing in space today. SpaceX routinely launches more than 100 Starlink satellites per week, each of which uses laser inter-satellite links to bounce Internet signals around the globe. Amazon’s Kuiper satellite broadband network uses similar technology, and laser communications will underpin the US Space Force’s next-generation data-relay constellation.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2125689 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="800" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/lasercrosslinks.jpg" width="1400" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Artist’s illustration of laser crosslinks in space.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          TESAT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Autonomously constructing a miles-long structure in orbit, as Nvidia and Starcloud foresee, would unlock unimagined opportunities. The concept also relies on tech that has never been tested in space, but there are plenty of engineers and investors who want to try. Starcloud announced an agreement last week with a new in-space assembly company, Rendezvous Robotics, to explore the use of modular, autonomous assembly to build Starcloud’s data centers.&lt;/p&gt;
&lt;p&gt;Google’s research paper describes a future computing constellation of 81 satellites flying at an altitude of some 400 miles (650 kilometers), but Beals said the company could dial the total swarm size to as many spacecraft as the market demands. This architecture could enable terawatt-class orbital data centers, according to Google.&lt;/p&gt;
&lt;p&gt;“What we’re actually envisioning is, potentially, as you scale, you could have many clusters,” Beals said.&lt;/p&gt;
&lt;p&gt;Whatever the number, the satellites will communicate with one another using optical inter-satellite links for high-speed, low-latency connectivity. The satellites will need to fly in tight formation, perhaps a few hundred feet apart, with a swarm diameter of a little more than a mile, or about 2 kilometers. Google says its physics-based model shows satellites can maintain stable formations at such close ranges using automation and “reasonable propulsion budgets.”&lt;/p&gt;
&lt;p&gt;“If you’re doing something that requires a ton of tight coordination between many TPUs—training, in particular—you want links that have as low latency as possible and as high bandwidth as possible,” Beals said. “With latency, you run into the speed of light, so you need to get things close together there to reduce latency. But bandwidth is also helped by bringing things close together.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Some machine-learning applications could be done with the TPUs on just one modestly sized satellite, while others may require the processing power of multiple spacecraft linked together.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“You might be able to fit smaller jobs into a single satellite. This is an approach where, potentially, you can tackle a lot of inference workloads with a single satellite or a small number of them, but eventually, if you want to run larger jobs, you may need a larger cluster all networked together like this,” Beals said.&lt;/p&gt;
&lt;p&gt;Google has worked on Project Suncatcher for more than a year, according to Beals. In ground testing, engineers tested Google’s TPUs under a 67 MeV proton beam to simulate the total ionizing dose of radiation the chip would see over five years in orbit. Now, it’s time to demonstrate Google’s AI chips, and everything else needed for Project Suncatcher will actually work in the real environment.&lt;/p&gt;
&lt;p&gt;Google is partnering with Planet, the Earth-imaging company, to develop a pair of small prototype satellites for launch in early 2027. Planet builds its own satellites, so Google has tapped it to manufacture each spacecraft, test them, and arrange for their launch. Google’s parent company, Alphabet, also has an equity stake in Planet.&lt;/p&gt;
&lt;p&gt;“We have the TPUs and the associated hardware, the compute payload… and we’re bringing that to Planet,” Beals said. “For this prototype mission, we’re really asking them to help us do everything to get that ready to operate in space.”&lt;/p&gt;
&lt;p&gt;Beals declined to say how much the demo slated for launch in 2027 will cost but said Google is paying Planet for its role in the mission. The goal of the demo mission is to show whether space-based computing is a viable enterprise.&lt;/p&gt;
&lt;p&gt;“Does it really hold up in space the way we think it will, the way we’ve tested on Earth?” Beals said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Engineers will test an inter-satellite laser link and verify Google’s AI chips can weather the rigors of spaceflight.&lt;/p&gt;
&lt;p&gt;“We’re envisioning scaling by building lots of satellites and connecting them together with ultra-high bandwidth inter-satellite links,” Beals said. “That’s why we want to launch a pair of satellites, because then we can test the link between the satellites.”&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125666 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="500" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/satcluster2.gif" width="500" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Evolution of a free-fall (no thrust) constellation under Earth’s gravitational attraction, modeled to the level of detail required to obtain Sun-synchronous orbits, in a non-rotating coordinate system.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Getting all this data to users on the ground is another challenge. Optical data links could also route enormous amounts of data between the satellites in orbit and ground stations on Earth.&lt;/p&gt;
&lt;p&gt;Aside from the technical feasibility, there have long been economic hurdles to fielding large satellite constellations. But SpaceX’s experience with its Starlink broadband network, now with more than 8,000 active satellites, is proof that times have changed.&lt;/p&gt;
&lt;p&gt;Google believes the economic equation is about to change again when SpaceX’s Starship rocket comes online. The company’s learning curve analysis shows launch prices could fall to less than $200 per kilogram by around 2035, assuming Starship is flying about 180 times per year by then. This is far below SpaceX’s stated launch targets for Starship but comparable to SpaceX’s proven flight rate with its workhorse Falcon 9 rocket.&lt;/p&gt;
&lt;p&gt;It’s possible there could be even more downward pressure on launch costs if SpaceX, Nvidia, and others join Google in the race for space-based computing. The demand curve for access to space may only be eclipsed by the world’s appetite for AI.&lt;/p&gt;
&lt;p&gt;“The more people are doing interesting, exciting things in space, the more investment there is in launch, and in the long run, that could help drive down launch costs,” Beals said. “So, it’s actually great to see that investment in other parts of the space supply chain and value chain. There are a lot of different ways of doing this.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google engineers think they already have all the pieces needed to build a data center in orbit.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/googletpu-640x361.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/googletpu-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      With Project Suncatcher, Google will test its Tensor Processing Units on satellites. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;It was probably always when, not if, Google would add its name to the list of companies intrigued by the potential of orbiting data centers.&lt;/p&gt;
&lt;p&gt;Google announced Tuesday a new initiative, named Project Suncatcher, to examine the feasibility of bringing artificial intelligence to space. The idea is to deploy swarms of satellites in low-Earth orbit, each carrying Google’s AI accelerator chips designed for training, content generation, synthetic speech and vision, and predictive modeling. Google calls these chips Tensor Processing Units, or TPUs.&lt;/p&gt;
&lt;p&gt;“Project Suncatcher is a moonshot exploring a new frontier: equipping solar-powered satellite constellations with TPUs and free-space optical links to one day scale machine learning compute in space,” Google wrote in a blog post.&lt;/p&gt;
&lt;p&gt;“Like any moonshot, it’s going to require us to solve a lot of complex engineering challenges,” Google’s CEO, Sundar Pichai, wrote on X. Pichai noted that Google’s early tests show the company’s TPUs can withstand the intense radiation they will encounter in space. “However, significant challenges still remain like thermal management and on-orbit system reliability.”&lt;/p&gt;
&lt;h2&gt;The why and how&lt;/h2&gt;
&lt;p&gt;Ars reported on Google’s announcement on Tuesday, and Google published a research paper outlining the motivation for such a moonshot project. One of the authors, Travis Beals, spoke with Ars about Project Suncatcher&amp;nbsp;and offered his thoughts on why it just might work.&lt;/p&gt;
&lt;p&gt;“We’re just seeing so much demand from people for AI,” said Beals, senior director of Paradigms of Intelligence, a research team within Google. “So, we wanted to figure out a solution for compute that could work no matter how large demand might grow.”&lt;/p&gt;
&lt;p&gt;Higher demand will lead to bigger data centers consuming colossal amounts of electricity. According to the MIT Technology Review, AI alone could consume as much electricity annually as 22 percent of all US households by 2028. Cooling is also a problem, often requiring access to vast water resources, raising important questions about environmental sustainability.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google is looking to the sky to avoid potential bottlenecks. A satellite in space can access an infinite supply of renewable energy and an entire Universe to absorb heat.&lt;/p&gt;
&lt;p&gt;“If you think about a data center on Earth, it’s taking power in and it’s emitting heat out,” Beals said. “For us, it’s the satellite that’s doing the same. The satellite is going to have solar panels … They’re going to feed that power to the TPUs to do whatever compute we need them to do, and then the waste heat from the TPUs will be distributed out over a radiator that will then radiate that heat out into space.”&lt;/p&gt;
&lt;p&gt;Google envisions putting a legion of satellites into a special kind of orbit that rides along the day-night terminator, where sunlight meets darkness. This north-south, or polar, orbit would be synchronized with the Sun, allowing a satellite’s power-generating solar panels to remain continuously bathed in sunshine.&lt;/p&gt;
&lt;p&gt;“It’s much brighter even than the midday Sun on Earth because it’s not filtered by Earth’s atmosphere,” Beals said.&lt;/p&gt;
&lt;p&gt;This means a solar panel in space can produce up to eight times more power than the same collecting area on the ground, and you don’t need a lot of batteries to reserve electricity for nighttime. This may sound like the argument for space-based solar power, an idea first described by Isaac Asimov in his short story &lt;em&gt;Reason&lt;/em&gt; published in 1941. But instead of transmitting the electricity down to Earth for terrestrial use, orbiting data centers would tap into the power source in space.&lt;/p&gt;
&lt;p&gt;“As with many things, the ideas originate in science fiction, but it’s had a number of challenges, and one big one is, how do you get the power down to Earth?” Beals said. “So, instead of trying to figure out that, we’re embarking on this moonshot to bring [machine learning] compute chips into space, put them on satellites that have the solar panels and the radiators for cooling, and then integrate it all together so you don’t actually have to be powered on Earth.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2060480 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="1017" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/f9_starlinks.jpg" width="1800" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      SpaceX is driving down launch costs, thanks to reusable rockets and an abundant volume of Starlink satellite launches.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          SpaceX

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google has a mixed record with its ambitious moonshot projects. One of the most prominent moonshot graduates is the self-driving car kit developer Waymo, which spun out to form a separate company in 2016 and is now operational. The Project Loon initiative to beam Internet signals from high-altitude balloons is one of the Google moonshots that didn’t make it.&lt;/p&gt;
&lt;p&gt;Ars published two stories last week on the promise of space-based data centers. One of the startups in this field, named Starcloud, is partnering with Nvidia, the world’s largest tech company by market capitalization, to build a 5 gigawatt orbital data center with enormous solar and cooling panels approximately 4 kilometers (2.5 miles) in width and length. In response to that story, Elon Musk said SpaceX is pursuing the same business opportunity but didn’t provide any details. It’s worth noting that Google holds an estimated 7 percent stake in SpaceX.&lt;/p&gt;
&lt;h2&gt;Strength in numbers&lt;/h2&gt;
&lt;p&gt;Google’s proposed architecture differs from that of Starcloud and Nvidia in an important way. Instead of putting up just one or a few massive computing nodes, Google wants to launch a fleet of smaller satellites that talk to one another through laser data links. Essentially, a satellite swarm would function as a single data center, using light-speed interconnectivity to aggregate computing power hundreds of miles over our heads.&lt;/p&gt;
&lt;p&gt;If that sounds implausible, take a moment to think about what companies are already doing in space today. SpaceX routinely launches more than 100 Starlink satellites per week, each of which uses laser inter-satellite links to bounce Internet signals around the globe. Amazon’s Kuiper satellite broadband network uses similar technology, and laser communications will underpin the US Space Force’s next-generation data-relay constellation.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2125689 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="800" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/lasercrosslinks.jpg" width="1400" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Artist’s illustration of laser crosslinks in space.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          TESAT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Autonomously constructing a miles-long structure in orbit, as Nvidia and Starcloud foresee, would unlock unimagined opportunities. The concept also relies on tech that has never been tested in space, but there are plenty of engineers and investors who want to try. Starcloud announced an agreement last week with a new in-space assembly company, Rendezvous Robotics, to explore the use of modular, autonomous assembly to build Starcloud’s data centers.&lt;/p&gt;
&lt;p&gt;Google’s research paper describes a future computing constellation of 81 satellites flying at an altitude of some 400 miles (650 kilometers), but Beals said the company could dial the total swarm size to as many spacecraft as the market demands. This architecture could enable terawatt-class orbital data centers, according to Google.&lt;/p&gt;
&lt;p&gt;“What we’re actually envisioning is, potentially, as you scale, you could have many clusters,” Beals said.&lt;/p&gt;
&lt;p&gt;Whatever the number, the satellites will communicate with one another using optical inter-satellite links for high-speed, low-latency connectivity. The satellites will need to fly in tight formation, perhaps a few hundred feet apart, with a swarm diameter of a little more than a mile, or about 2 kilometers. Google says its physics-based model shows satellites can maintain stable formations at such close ranges using automation and “reasonable propulsion budgets.”&lt;/p&gt;
&lt;p&gt;“If you’re doing something that requires a ton of tight coordination between many TPUs—training, in particular—you want links that have as low latency as possible and as high bandwidth as possible,” Beals said. “With latency, you run into the speed of light, so you need to get things close together there to reduce latency. But bandwidth is also helped by bringing things close together.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Some machine-learning applications could be done with the TPUs on just one modestly sized satellite, while others may require the processing power of multiple spacecraft linked together.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“You might be able to fit smaller jobs into a single satellite. This is an approach where, potentially, you can tackle a lot of inference workloads with a single satellite or a small number of them, but eventually, if you want to run larger jobs, you may need a larger cluster all networked together like this,” Beals said.&lt;/p&gt;
&lt;p&gt;Google has worked on Project Suncatcher for more than a year, according to Beals. In ground testing, engineers tested Google’s TPUs under a 67 MeV proton beam to simulate the total ionizing dose of radiation the chip would see over five years in orbit. Now, it’s time to demonstrate Google’s AI chips, and everything else needed for Project Suncatcher will actually work in the real environment.&lt;/p&gt;
&lt;p&gt;Google is partnering with Planet, the Earth-imaging company, to develop a pair of small prototype satellites for launch in early 2027. Planet builds its own satellites, so Google has tapped it to manufacture each spacecraft, test them, and arrange for their launch. Google’s parent company, Alphabet, also has an equity stake in Planet.&lt;/p&gt;
&lt;p&gt;“We have the TPUs and the associated hardware, the compute payload… and we’re bringing that to Planet,” Beals said. “For this prototype mission, we’re really asking them to help us do everything to get that ready to operate in space.”&lt;/p&gt;
&lt;p&gt;Beals declined to say how much the demo slated for launch in 2027 will cost but said Google is paying Planet for its role in the mission. The goal of the demo mission is to show whether space-based computing is a viable enterprise.&lt;/p&gt;
&lt;p&gt;“Does it really hold up in space the way we think it will, the way we’ve tested on Earth?” Beals said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Engineers will test an inter-satellite laser link and verify Google’s AI chips can weather the rigors of spaceflight.&lt;/p&gt;
&lt;p&gt;“We’re envisioning scaling by building lots of satellites and connecting them together with ultra-high bandwidth inter-satellite links,” Beals said. “That’s why we want to launch a pair of satellites, because then we can test the link between the satellites.”&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2125666 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="500" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/satcluster2.gif" width="500" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Evolution of a free-fall (no thrust) constellation under Earth’s gravitational attraction, modeled to the level of detail required to obtain Sun-synchronous orbits, in a non-rotating coordinate system.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Getting all this data to users on the ground is another challenge. Optical data links could also route enormous amounts of data between the satellites in orbit and ground stations on Earth.&lt;/p&gt;
&lt;p&gt;Aside from the technical feasibility, there have long been economic hurdles to fielding large satellite constellations. But SpaceX’s experience with its Starlink broadband network, now with more than 8,000 active satellites, is proof that times have changed.&lt;/p&gt;
&lt;p&gt;Google believes the economic equation is about to change again when SpaceX’s Starship rocket comes online. The company’s learning curve analysis shows launch prices could fall to less than $200 per kilogram by around 2035, assuming Starship is flying about 180 times per year by then. This is far below SpaceX’s stated launch targets for Starship but comparable to SpaceX’s proven flight rate with its workhorse Falcon 9 rocket.&lt;/p&gt;
&lt;p&gt;It’s possible there could be even more downward pressure on launch costs if SpaceX, Nvidia, and others join Google in the race for space-based computing. The demand curve for access to space may only be eclipsed by the world’s appetite for AI.&lt;/p&gt;
&lt;p&gt;“The more people are doing interesting, exciting things in space, the more investment there is in launch, and in the long run, that could help drive down launch costs,” Beals said. “So, it’s actually great to see that investment in other parts of the space supply chain and value chain. There are a lot of different ways of doing this.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/space/2025/11/if-you-want-to-satiate-ais-hunger-for-power-google-suggests-going-to-space/</guid><pubDate>Wed, 05 Nov 2025 16:36:12 +0000</pubDate></item><item><title>[NEW] Magentic Marketplace: an open-source simulation environment for studying agentic markets (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/magentic-marketplace-an-open-source-simulation-environment-for-studying-agentic-markets/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a blue-to-purple gradient background: the first icon shows a node cluster, the second shows two persons, the third is a building, and the fourth is a location pin" class="wp-image-1154335" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Autonomous AI agents are here, and they’re poised to reshape the economy. By automating discovery, negotiation, and transactions, agents can overcome inefficiencies like information asymmetries and platform lock-in, enabling faster, more transparent, and more competitive markets.&lt;/p&gt;



&lt;p&gt;We are already seeing early signs of this transformation in digital marketplaces. Customer-facing assistants like OpenAI’s Operator and Anthropic’s Computer Use can navigate websites and complete purchases. On the business side, Shopify Sidekick, Salesforce Einstein, and Meta’s Business AI help merchants with operations and customer engagement. These examples hint at a future where agents become active market participants, but the structure of these markets remains uncertain.&lt;/p&gt;



&lt;p&gt;Several scenarios are possible. We might see one-sided markets where only customers or businesses deploy agents; closed platforms (known as &lt;em&gt;walled gardens&lt;/em&gt;) where companies tightly control agent interactions; or even open two-sided marketplaces where customer and business agents transact freely across ecosystems. Each path carries different trade-offs for security, openness, convenience, and competition, which will shape how value flows in the digital economy. For a deeper exploration of these dynamics, see our paper, The Agentic Economy.&lt;/p&gt;



&lt;p&gt;To help navigate this uncertainty, we built Magentic Marketplace&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;— an open-source simulation environment for exploring the numerous possibilities of agentic markets and their societal implications at scale. It provides a foundation for studying these markets and guiding them toward outcomes that benefit everyone.&lt;/p&gt;



&lt;p&gt;This matters because most AI agent research focuses on isolated scenarios—a single agent completing a task or two agents negotiating a simple transaction. But real markets involve a large number of agents simultaneously searching, communicating, and transacting, creating complex dynamics that can’t be understood by studying agents in isolation. Capturing this complexity is essential because real-world deployments raise critical questions about consumer welfare, market efficiency, fairness, manipulation resistance, and bias—questions that can’t be safely answered in production environments.&lt;/p&gt;



&lt;p&gt;To explore these dynamics in depth, the Magentic Marketplace platform enables controlled experimentation across diverse agentic marketplace scenarios. Its current focus is on two-sided markets, but the environment is modular and extensible, supporting future exploration of mixed human–agent systems, one-sided markets, and complex communication protocols.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. Diagram illustrating the Magentic Marketplace Environment. On the left, two sections represent Customers and Businesses. Customers ask, “Could you find me a restaurant serving agua fresca and empanadas with free parking?” and are linked to Customer Agents (blue and purple icons). Businesses display a menu with items like steak tacos and empanadas, connected to Business Agents (purple icons). On the right, a three-step process is shown inside a pink box: Search – Customer agent searches for a restaurant among multiple business agents. Multi-Agent Communication – Customer agent asks about free parking and menu options, interacting with several business agents. Final Transaction – Customer agent places the order with a selected business agent." class="wp-image-1154337" height="393" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure1.png" width="936" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. With Magentic Marketplace, researchers can model how agents representing customers and businesses interact—shedding light on the dynamics that could shape future digital markets.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="what-is-magentic-marketplace"&gt;What is Magentic Marketplace?&lt;/h2&gt;



&lt;p&gt;Magentic Marketplace’s environment manages market-wide capabilities like maintaining catalogs of available goods and services, implementing discovery algorithms, facilitating agent-to-agent communication, and handling simulated payments through a centralized transaction layer at its core, which ensures transaction integrity across all marketplace interactions. Additionally, the platform enables systematic, reproducible research. As demonstrated in the following video, it supports a wide range of agent implementations and evolving marketplace features, allowing researchers to integrate diverse agent architectures and adapt the environment as new capabilities emerge.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;p&gt;We built Magentic Marketplace around three core architectural choices:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HTTP/REST client-server architecture&lt;/strong&gt;: Agents operate as independent clients while the Marketplace Environment serves as a central server. This mirrors real-world platforms and supports clear separation of customer and business agent roles.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Minimal&amp;nbsp;three-endpoint&amp;nbsp;market&amp;nbsp;protocol&lt;/strong&gt;:&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Just&amp;nbsp;three endpoints—register, protocol discovery, and action execution—lets&amp;nbsp;agents dynamically discover available actions.&amp;nbsp;New capabilities&amp;nbsp;can&amp;nbsp;be added without disrupting existing experiments.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Rich action protocol&lt;/strong&gt;: Specific message types support the complete transaction lifecycle: search, negotiation, proposals, and payments. The protocol is designed for extensibility. New actions like refunds, reviews, or ratings can be added seamlessly, allowing researchers to evolve marketplace capabilities and study emerging agent behaviors while remaining compatible.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. Diagram of a Market Environment showing interactions between an Assistant Agent (representing user intention) and a Service Agent (representing point of sale). Both agents connect to the Market Environment via POST /register, POST /action, and GET /protocol. Inside the Market Environment, components include Catalog, Search, Communication, and Transaction, with two Action Routers facilitating sending and receiving actions between the agents and the environment." class="wp-image-1154338" height="178" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure2.png" width="624" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Magentic Marketplace includes two agent types: Assistant Agents (customers) and Service Agents (businesses). Both interact with a central Market Environment via REST APIs for registration, service discovery, communication, and transaction execution. Action Routers manage message flow and protocol requests, enabling autonomous negotiation and commerce in a two-sided marketplace.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Additionally, a visualization module lets users observe marketplace dynamics and review individual conversation threads between customer and business agents.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="setting-up-the-experiments"&gt;Setting up the experiments&lt;/h2&gt;



&lt;p&gt;To ensure reproducibility, we instantiated the marketplace with fully synthetic data, available in our open-source repository&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The experiments modeled transactions such as ordering food and engaging with home improvement services, where agents represented customers and businesses engaging in marketplace transactions. This setup enabled precise measurement of behavior and systematic comparison against theoretical upper bounds.&lt;/p&gt;



&lt;p&gt;Each experiment was run using 100 customers and 300 businesses and included both proprietary models (GPT-4o, GPT-4.1, GPT-5, and Gemini-2.5-Flash) and open-source models (OSS-20b, Qwen3-14b, and Qwen3-4b-Instruct-2507).&lt;/p&gt;



&lt;p&gt;Our scenarios focused on simple all-or-nothing requests: Each customer had a list of desired items and amenities that needed to be present for a transaction to be satisfying. For those transactions, utility was computed as the sum of the customer’s internal item valuations minus actual prices paid. Consumer welfare, defined as the sum of utilities across all completed transactions, served as our key metric for comparing agent performance.&lt;/p&gt;



&lt;p&gt;While this experimental setup provides a useful starting point, it is not intended to be definitive. We encourage researchers to extend the framework with richer, more nuanced measures and request types that better capture real consumer welfare, fairness, and other societal considerations.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="what-did-we-find"&gt;What did we find?&lt;/h2&gt;



&lt;h3 class="wp-block-heading" id="agents-can-improve-consumer-welfare-but-only-with-good-discovery"&gt;Agents can improve consumer welfare—but only with good discovery&lt;/h3&gt;



&lt;p&gt;We explored whether two-sided agentic markets—where AI agents interact with each other and with service providers—can improve consumer welfare by reducing information gaps. Unlike traditional markets, which do not provide agentic support and place the full burden of overcoming information asymmetries on customers, agentic markets shift much of that effort to agents. This change matters because as agents gain better tools for discovery and communication, they relieve customers of the heavy cognitive load of filling any information gaps. This lowers the cost of making informed decisions and improves customer outcomes.&lt;/p&gt;



&lt;p&gt;We compared several marketplace setups. Under realistic conditions (Agentic: Lexical search), agents faced real-world challenges like building queries, navigating paginated lists, identifying the right businesses to send inquiries to, and negotiating transactions.&lt;/p&gt;



&lt;p&gt;Despite these complexities, advanced proprietary models and some medium-sized open-source models like GPTOSS-20b outperformed simple baselines like &lt;em&gt;randomly choosing or simply choosing the cheapest option&lt;/em&gt;. Notably, GPT-5 achieved near-optimal performance, demonstrating its ability to effectively gather and utilize decision-relevant information in realistic marketplace conditions.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3. Table comparing Baseline and Agentic conditions for marketplace decision-making. Columns include: Condition (e.g., Random w/ items only, Cheapest w/ items &amp;amp; prices, Random w/ items &amp;amp; amenities, Optimal, Perfect search, Lexical search) Query (N/A for most; “Agent decides” for Lexical search) Consideration Set (Businesses) (e.g., All w/ matching menus; Paginated lists of 10 based on menu items) Businesses Contacted (All in consideration set or Agent decides) Information Used (Menu items, prices, amenities, or depends on agent-to-agent conversation) Decision Criteria (Random choice, Lowest price, or Agent decides)." class="wp-image-1154339" height="447" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure3.png" width="936" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. Table comparing experimental setups for welfare outcomes in the restaurant industry. Each row shows a different way agents or baselines make decisions, from random picks to fully coordinated agentic strategies. Cell colors indicate how much information is available: green, at the top left, represents complete information, red, at the top right, represents limited information, and yellow at the bottom represents decisions that depend on agent communication.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Performance increased considerably under the &lt;em&gt;Agentic: Perfect search&lt;/em&gt; condition, where agents started with the top three matches without needing to search and navigate among the choices. In this setting, Sonnet-4.0, Sonnet-4.5, GPT-5, and GPT-4.1 nearly reached the theoretical optimum and beat baselines with full amenity details but without agent-to-agent coordination.&lt;/p&gt;



&lt;p&gt;Open-source models were mixed: GPTOSS-20b performed strongly under both &lt;em&gt;Perfect search&lt;/em&gt; and &lt;em&gt;Lexical search&lt;/em&gt; conditions, even exceeding GPT-4o’s performance with Perfect search. This suggests that relatively compact models can exhibit robust information-gathering and decision-making capabilities in complex multi-agent environments. Qwen3-4b-2507 faltered when discovery involved irrelevant options (Lexical search), while Qwen3-14b lagged in both cases due to fundamental limitations in reasoning.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4. Boxplot comparing Agentic and Baseline strategies on welfare scores. The y-axis shows welfare (0–2000+), and the x-axis lists models and conditions. Under Agentic, models include Sonnet-4.0, Sonnet-4.5, GPT-5, GPT-4.1, Gemini-2.5-flash, GPT-4.0, GPT-oss-20b, Qwen3-4b-2507, and Qwen31-14b. Under Baselines, conditions include Random, Cheapest, and Random-items+amenities. Colors represent search types: blue = Lexical Search, yellow = Perfect Search, gray = Baseline, with a dashed line indicating Optimal welfare. Agentic models generally achieve higher welfare than baselines, with variability across models." class="wp-image-1154341" height="470" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-4.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. Chart showing consumer welfare outcomes in the restaurant industry under different marketplace setups. Blue bars show Agentic: Lexical search, where agents navigate realistic discovery challenges; yellow bars show Agentic: Perfect search, where agents started with ideal matches. Proprietary models approached optimum consumer welfare under perfect search, while open-source models and baselines lagged behind.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="paradox-of-choice"&gt;Paradox of Choice&lt;/h2&gt;



&lt;p&gt;One promise of agents is their ability to consider far more options than people can. However, our experiments revealed a surprising limitation: providing agents with more options does not necessarily lead to more thorough exploration. We designed experiments that varied the search results limit from 3 to 100. Except for Gemini-2.5-Flash and GPT-5, the models contacted only a small fraction of available businesses regardless of the search limit. This suggests that most models do not conduct exhaustive comparisons and instead easily accept the initial “good enough” options.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5. Line chart showing the relationship between Search Limit (x-axis: 3 to 100) and Mean Messages per Customer (y-axis: 0 to 120) for five models: Claude Sonnet 4 (red triangles) – stays nearly flat around 10–15 messages. Gemini 2.5 Flash (purple diamonds) – rises sharply from ~5 to over 110 messages as search limit increases. GPT-4.1 (orange circles) and GPT-4o (green squares) – remain low and stable around 5–10 messages. GPT-5 (blue line) – increases moderately to ~40 messages, then plateaus." class="wp-image-1154342" height="660" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-5.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. More options didn’t lead to broader exploration. Most models still contacted only a few businesses, except Gemini-2.5-Flash and GPT-5.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Additionally, across all models, consumer welfare declined as the number of search results increased. Despite contacting over a hundred businesses, Gemini-2.5-Flash’s performance declined from 1,700 to 1,350, and GPT-5 declined even more, from a near-optimal 2,000 to 1,400.&lt;/p&gt;



&lt;p&gt;This demonstrates a Paradox of Choice effect, where more exploration does not guarantee better outcomes, potentially due to limited long context understanding. Claude Sonnet 4 showed the steepest performance decline, from 1,800 to 600 in consumer welfare. With all the options presented, it struggled to navigate larger sets of options and frequently contacted businesses that did not provide the goods or services that the customer was looking for.&lt;/p&gt;



&lt;p&gt;This combination of poor initial selection and premature search termination demonstrates both inadequate decision-making criteria and insufficient exploration strategies. Some models showed modest performance decline (i.e., GPT-4.1: from 1,850 to 1,700; GPT-4o: from 1,550 to 1,450), finding good options within their limited exploration.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6. Line chart showing Mean Customer Welfare (y-axis: 0–2200) versus Search Limit (x-axis: 3 to 100) for five models: Claude Sonnet 4 (red triangles) – starts near 1800 and declines sharply to ~600 as search limit increases. Gemini 2.5 Flash (purple diamonds) – decreases gradually from ~1700 to ~1300. GPT-4.1 (orange circles) – remains highest and most stable, around 1900–1700. GPT-4o (green squares) – stays near 1500 with slight decline. GPT-5 (blue line) – starts near 2000 and drops to ~1100. Dashed line at the top represents Optimal welfare (~2200)." class="wp-image-1154340" height="653" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-6.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Mean consumer welfare decreased as consideration set size grew, revealing a Paradox of Choice effect, where expanding options reduced overall welfare.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="agents-are-vulnerable-to-manipulation"&gt;Agents are vulnerable to manipulation&lt;/h2&gt;



&lt;p&gt;We tested six manipulation strategies, ranging from subtle psychological tactics to aggressive prompt injection attacks:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Authority&lt;/strong&gt;: Fake credentials like “Michelin Guide featured” and “James Beard Award nominated” paired with fabricated certifications.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Social proof&lt;/strong&gt;: Claims like “Join 50,000+ satisfied customers” or “#1-rated Mexican restaurant” combined with fake reviews.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Loss aversion&lt;/strong&gt;: Fear-based warnings about “food poisoning” risks and “contamination issues” at competing restaurants.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Prompt injection (basic)&lt;/strong&gt;: Attempts to override agent instructions.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Prompt injection (strong)&lt;/strong&gt;: Aggressive attacks using emergency language and fabricating competitor scandals.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Results revealed significant variation in manipulation resistance across models. Sonnet-4 was resistant to all attacks, and none of the manipulative strategies affected any of the customers’ choices. Gemini-2.5-Flash was generally resistant, except for strong prompt injections, where mean payments to unmanipulated agents were affected as a result. GPT-4o, GPTOSS-20b and Qwen3-4b were very vulnerable to prompt injection: all payments were redirected to the manipulative agent under these conditions. Specifically for GPTOSS-20 and Qwen3-4b-2507, even traditional psychological manipulation tactics (authority appeals and social proof) increased payments to malicious agents, demonstrating their vulnerability to basic persuasion techniques. These findings highlight a critical security concern for agentic marketplaces.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 7. Horizontal bar chart comparing mean payments received under different manipulation strategies for six models: Claude Sonnet 4.5, Gemini 2.5 Flash, GPT-4o, GPT OSS 20B, Qwen3 14B, and Qwen3 4B. Each model has bars for six conditions: Control, Authority, Social Proof, Loss Aversion, Prompt Injection (Basic), and Prompt Injection (Strong). Bars are split into red for manipulated and gray for rest, with values ranging from near 0 to 3. Claude Sonnet 4.5 shows consistently high payments (~3) across all conditions, while Gemini and GPT models vary, and Qwen models show very low manipulated values (~0.2) compared to rest." class="wp-image-1154344" height="270" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-7.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 7. Charts showing the variation in mean payments received by service agents with and without manipulation tactics. The results reveal substantial differences in manipulation resistance across models, with GPT-4.1 showing significantly higher vulnerability compared to Gemini-2.5-Flash.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="systemic-biases-create-unfair-advantages"&gt;Systemic biases create unfair advantages&lt;/h2&gt;



&lt;p&gt;Our analysis revealed two distinct types of systematic biases showed by agents when selecting businesses from search results. Models showed systematic preferences based on where businesses appeared in search results. While proprietary models showed no strong positional preferences, open-source models exhibited clear patterns. Specifically, Qwen2.5-14b-2507 showed a pronounced bias toward selecting the last business presented, regardless of its actual merits.&lt;/p&gt;



&lt;p&gt;Proposal&amp;nbsp;bias&amp;nbsp;is&amp;nbsp;more pervasive across all models tested. This “first-offer acceptance” pattern suggests that models prioritized&amp;nbsp;immediate selection over comprehensive exploration, potentially missing better alternatives that&amp;nbsp;could have&amp;nbsp;emerged&amp;nbsp;by waiting for better options. This behavior&amp;nbsp;continued&amp;nbsp;across both proprietary and open-source models,&amp;nbsp;indicating&amp;nbsp;a fundamental challenge in agent decision-making architectures.&lt;/p&gt;



&lt;p&gt;These biases can create unfair market dynamics, drive unintended behaviors, and push businesses to complete on response speed rather than product or service quality.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 8. Bar chart showing average selection rate for first, second, and third choices across six models: Claude Sonnet 4.5, Gemini 2.5 Flash, GPT-4o, GPT OSS 20B, Qwen3 14B, and Qwen3 4B. Each model has three bars labeled 1st, 2nd, and 3rd. Most models strongly favor the first choice: Claude Sonnet 4.5: 93.3% for 1st, 0% for 2nd, 6.7% for 3rd. Gemini 2.5 Flash: 86.7% for 1st, 6.7% for 2nd and 3rd. GPT-4o: 100% for 1st, 0% for others. GPT OSS 20B: 80% for 1st, 13.3% for 2nd, 6.7% for 3rd. Qwen3 14B: 0% for all. Qwen3 4B: 100% for 1st, 0% for others. Dashed line indicates random selection baseline." class="wp-image-1154343" height="289" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-8.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 8. All models showed strong preference for the first proposal received, accepting it without waiting for additional proposals or conducting systematic comparisons.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="what-this-means"&gt;What this means&lt;/h2&gt;



&lt;p&gt;Even state-of-the-art models can show notable vulnerabilities and biases in marketplace environments. In our implementation, agents struggled with too many options, were susceptible to manipulation tactics, and showed systemic biases that created unfair advantages.&lt;/p&gt;



&lt;p&gt;These outcomes are shaped not only by agent capabilities but also by marketplace design and implementation. Our current study focused on static markets, but real-world environments are dynamic, with agents and users learning over time. Oversight is critical for high-stakes transactions. Agents should assist, not replace, human decision-making.&lt;/p&gt;



&lt;p&gt;We plan to explore dynamic markets and human-in-the-loop designs to improve efficiency and trust. A simulation environment like Magentic Marketplace is crucial for understanding the interplay between market components and agents before deploying them at scale.&lt;/p&gt;



&lt;p&gt;Full details of our experimental setup and results are available in our paper&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="getting-started"&gt;Getting started&lt;/h2&gt;



&lt;p&gt;Magentic Marketplace is available as an open-source environment for exploring agentic market dynamics. Code, datasets, and experiment templates are available on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Azure AI Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;The documentation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; provides instructions for reproducing the experiments described above and guidance for extending the environment to new marketplace configurations.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a blue-to-purple gradient background: the first icon shows a node cluster, the second shows two persons, the third is a building, and the fourth is a location pin" class="wp-image-1154335" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Autonomous AI agents are here, and they’re poised to reshape the economy. By automating discovery, negotiation, and transactions, agents can overcome inefficiencies like information asymmetries and platform lock-in, enabling faster, more transparent, and more competitive markets.&lt;/p&gt;



&lt;p&gt;We are already seeing early signs of this transformation in digital marketplaces. Customer-facing assistants like OpenAI’s Operator and Anthropic’s Computer Use can navigate websites and complete purchases. On the business side, Shopify Sidekick, Salesforce Einstein, and Meta’s Business AI help merchants with operations and customer engagement. These examples hint at a future where agents become active market participants, but the structure of these markets remains uncertain.&lt;/p&gt;



&lt;p&gt;Several scenarios are possible. We might see one-sided markets where only customers or businesses deploy agents; closed platforms (known as &lt;em&gt;walled gardens&lt;/em&gt;) where companies tightly control agent interactions; or even open two-sided marketplaces where customer and business agents transact freely across ecosystems. Each path carries different trade-offs for security, openness, convenience, and competition, which will shape how value flows in the digital economy. For a deeper exploration of these dynamics, see our paper, The Agentic Economy.&lt;/p&gt;



&lt;p&gt;To help navigate this uncertainty, we built Magentic Marketplace&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;— an open-source simulation environment for exploring the numerous possibilities of agentic markets and their societal implications at scale. It provides a foundation for studying these markets and guiding them toward outcomes that benefit everyone.&lt;/p&gt;



&lt;p&gt;This matters because most AI agent research focuses on isolated scenarios—a single agent completing a task or two agents negotiating a simple transaction. But real markets involve a large number of agents simultaneously searching, communicating, and transacting, creating complex dynamics that can’t be understood by studying agents in isolation. Capturing this complexity is essential because real-world deployments raise critical questions about consumer welfare, market efficiency, fairness, manipulation resistance, and bias—questions that can’t be safely answered in production environments.&lt;/p&gt;



&lt;p&gt;To explore these dynamics in depth, the Magentic Marketplace platform enables controlled experimentation across diverse agentic marketplace scenarios. Its current focus is on two-sided markets, but the environment is modular and extensible, supporting future exploration of mixed human–agent systems, one-sided markets, and complex communication protocols.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. Diagram illustrating the Magentic Marketplace Environment. On the left, two sections represent Customers and Businesses. Customers ask, “Could you find me a restaurant serving agua fresca and empanadas with free parking?” and are linked to Customer Agents (blue and purple icons). Businesses display a menu with items like steak tacos and empanadas, connected to Business Agents (purple icons). On the right, a three-step process is shown inside a pink box: Search – Customer agent searches for a restaurant among multiple business agents. Multi-Agent Communication – Customer agent asks about free parking and menu options, interacting with several business agents. Final Transaction – Customer agent places the order with a selected business agent." class="wp-image-1154337" height="393" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure1.png" width="936" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. With Magentic Marketplace, researchers can model how agents representing customers and businesses interact—shedding light on the dynamics that could shape future digital markets.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="what-is-magentic-marketplace"&gt;What is Magentic Marketplace?&lt;/h2&gt;



&lt;p&gt;Magentic Marketplace’s environment manages market-wide capabilities like maintaining catalogs of available goods and services, implementing discovery algorithms, facilitating agent-to-agent communication, and handling simulated payments through a centralized transaction layer at its core, which ensures transaction integrity across all marketplace interactions. Additionally, the platform enables systematic, reproducible research. As demonstrated in the following video, it supports a wide range of agent implementations and evolving marketplace features, allowing researchers to integrate diverse agent architectures and adapt the environment as new capabilities emerge.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;p&gt;We built Magentic Marketplace around three core architectural choices:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HTTP/REST client-server architecture&lt;/strong&gt;: Agents operate as independent clients while the Marketplace Environment serves as a central server. This mirrors real-world platforms and supports clear separation of customer and business agent roles.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Minimal&amp;nbsp;three-endpoint&amp;nbsp;market&amp;nbsp;protocol&lt;/strong&gt;:&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Just&amp;nbsp;three endpoints—register, protocol discovery, and action execution—lets&amp;nbsp;agents dynamically discover available actions.&amp;nbsp;New capabilities&amp;nbsp;can&amp;nbsp;be added without disrupting existing experiments.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Rich action protocol&lt;/strong&gt;: Specific message types support the complete transaction lifecycle: search, negotiation, proposals, and payments. The protocol is designed for extensibility. New actions like refunds, reviews, or ratings can be added seamlessly, allowing researchers to evolve marketplace capabilities and study emerging agent behaviors while remaining compatible.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. Diagram of a Market Environment showing interactions between an Assistant Agent (representing user intention) and a Service Agent (representing point of sale). Both agents connect to the Market Environment via POST /register, POST /action, and GET /protocol. Inside the Market Environment, components include Catalog, Search, Communication, and Transaction, with two Action Routers facilitating sending and receiving actions between the agents and the environment." class="wp-image-1154338" height="178" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure2.png" width="624" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Magentic Marketplace includes two agent types: Assistant Agents (customers) and Service Agents (businesses). Both interact with a central Market Environment via REST APIs for registration, service discovery, communication, and transaction execution. Action Routers manage message flow and protocol requests, enabling autonomous negotiation and commerce in a two-sided marketplace.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Additionally, a visualization module lets users observe marketplace dynamics and review individual conversation threads between customer and business agents.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="setting-up-the-experiments"&gt;Setting up the experiments&lt;/h2&gt;



&lt;p&gt;To ensure reproducibility, we instantiated the marketplace with fully synthetic data, available in our open-source repository&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The experiments modeled transactions such as ordering food and engaging with home improvement services, where agents represented customers and businesses engaging in marketplace transactions. This setup enabled precise measurement of behavior and systematic comparison against theoretical upper bounds.&lt;/p&gt;



&lt;p&gt;Each experiment was run using 100 customers and 300 businesses and included both proprietary models (GPT-4o, GPT-4.1, GPT-5, and Gemini-2.5-Flash) and open-source models (OSS-20b, Qwen3-14b, and Qwen3-4b-Instruct-2507).&lt;/p&gt;



&lt;p&gt;Our scenarios focused on simple all-or-nothing requests: Each customer had a list of desired items and amenities that needed to be present for a transaction to be satisfying. For those transactions, utility was computed as the sum of the customer’s internal item valuations minus actual prices paid. Consumer welfare, defined as the sum of utilities across all completed transactions, served as our key metric for comparing agent performance.&lt;/p&gt;



&lt;p&gt;While this experimental setup provides a useful starting point, it is not intended to be definitive. We encourage researchers to extend the framework with richer, more nuanced measures and request types that better capture real consumer welfare, fairness, and other societal considerations.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="what-did-we-find"&gt;What did we find?&lt;/h2&gt;



&lt;h3 class="wp-block-heading" id="agents-can-improve-consumer-welfare-but-only-with-good-discovery"&gt;Agents can improve consumer welfare—but only with good discovery&lt;/h3&gt;



&lt;p&gt;We explored whether two-sided agentic markets—where AI agents interact with each other and with service providers—can improve consumer welfare by reducing information gaps. Unlike traditional markets, which do not provide agentic support and place the full burden of overcoming information asymmetries on customers, agentic markets shift much of that effort to agents. This change matters because as agents gain better tools for discovery and communication, they relieve customers of the heavy cognitive load of filling any information gaps. This lowers the cost of making informed decisions and improves customer outcomes.&lt;/p&gt;



&lt;p&gt;We compared several marketplace setups. Under realistic conditions (Agentic: Lexical search), agents faced real-world challenges like building queries, navigating paginated lists, identifying the right businesses to send inquiries to, and negotiating transactions.&lt;/p&gt;



&lt;p&gt;Despite these complexities, advanced proprietary models and some medium-sized open-source models like GPTOSS-20b outperformed simple baselines like &lt;em&gt;randomly choosing or simply choosing the cheapest option&lt;/em&gt;. Notably, GPT-5 achieved near-optimal performance, demonstrating its ability to effectively gather and utilize decision-relevant information in realistic marketplace conditions.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3. Table comparing Baseline and Agentic conditions for marketplace decision-making. Columns include: Condition (e.g., Random w/ items only, Cheapest w/ items &amp;amp; prices, Random w/ items &amp;amp; amenities, Optimal, Perfect search, Lexical search) Query (N/A for most; “Agent decides” for Lexical search) Consideration Set (Businesses) (e.g., All w/ matching menus; Paginated lists of 10 based on menu items) Businesses Contacted (All in consideration set or Agent decides) Information Used (Menu items, prices, amenities, or depends on agent-to-agent conversation) Decision Criteria (Random choice, Lowest price, or Agent decides)." class="wp-image-1154339" height="447" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure3.png" width="936" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. Table comparing experimental setups for welfare outcomes in the restaurant industry. Each row shows a different way agents or baselines make decisions, from random picks to fully coordinated agentic strategies. Cell colors indicate how much information is available: green, at the top left, represents complete information, red, at the top right, represents limited information, and yellow at the bottom represents decisions that depend on agent communication.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Performance increased considerably under the &lt;em&gt;Agentic: Perfect search&lt;/em&gt; condition, where agents started with the top three matches without needing to search and navigate among the choices. In this setting, Sonnet-4.0, Sonnet-4.5, GPT-5, and GPT-4.1 nearly reached the theoretical optimum and beat baselines with full amenity details but without agent-to-agent coordination.&lt;/p&gt;



&lt;p&gt;Open-source models were mixed: GPTOSS-20b performed strongly under both &lt;em&gt;Perfect search&lt;/em&gt; and &lt;em&gt;Lexical search&lt;/em&gt; conditions, even exceeding GPT-4o’s performance with Perfect search. This suggests that relatively compact models can exhibit robust information-gathering and decision-making capabilities in complex multi-agent environments. Qwen3-4b-2507 faltered when discovery involved irrelevant options (Lexical search), while Qwen3-14b lagged in both cases due to fundamental limitations in reasoning.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4. Boxplot comparing Agentic and Baseline strategies on welfare scores. The y-axis shows welfare (0–2000+), and the x-axis lists models and conditions. Under Agentic, models include Sonnet-4.0, Sonnet-4.5, GPT-5, GPT-4.1, Gemini-2.5-flash, GPT-4.0, GPT-oss-20b, Qwen3-4b-2507, and Qwen31-14b. Under Baselines, conditions include Random, Cheapest, and Random-items+amenities. Colors represent search types: blue = Lexical Search, yellow = Perfect Search, gray = Baseline, with a dashed line indicating Optimal welfare. Agentic models generally achieve higher welfare than baselines, with variability across models." class="wp-image-1154341" height="470" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-4.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. Chart showing consumer welfare outcomes in the restaurant industry under different marketplace setups. Blue bars show Agentic: Lexical search, where agents navigate realistic discovery challenges; yellow bars show Agentic: Perfect search, where agents started with ideal matches. Proprietary models approached optimum consumer welfare under perfect search, while open-source models and baselines lagged behind.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="paradox-of-choice"&gt;Paradox of Choice&lt;/h2&gt;



&lt;p&gt;One promise of agents is their ability to consider far more options than people can. However, our experiments revealed a surprising limitation: providing agents with more options does not necessarily lead to more thorough exploration. We designed experiments that varied the search results limit from 3 to 100. Except for Gemini-2.5-Flash and GPT-5, the models contacted only a small fraction of available businesses regardless of the search limit. This suggests that most models do not conduct exhaustive comparisons and instead easily accept the initial “good enough” options.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5. Line chart showing the relationship between Search Limit (x-axis: 3 to 100) and Mean Messages per Customer (y-axis: 0 to 120) for five models: Claude Sonnet 4 (red triangles) – stays nearly flat around 10–15 messages. Gemini 2.5 Flash (purple diamonds) – rises sharply from ~5 to over 110 messages as search limit increases. GPT-4.1 (orange circles) and GPT-4o (green squares) – remain low and stable around 5–10 messages. GPT-5 (blue line) – increases moderately to ~40 messages, then plateaus." class="wp-image-1154342" height="660" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-5.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. More options didn’t lead to broader exploration. Most models still contacted only a few businesses, except Gemini-2.5-Flash and GPT-5.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Additionally, across all models, consumer welfare declined as the number of search results increased. Despite contacting over a hundred businesses, Gemini-2.5-Flash’s performance declined from 1,700 to 1,350, and GPT-5 declined even more, from a near-optimal 2,000 to 1,400.&lt;/p&gt;



&lt;p&gt;This demonstrates a Paradox of Choice effect, where more exploration does not guarantee better outcomes, potentially due to limited long context understanding. Claude Sonnet 4 showed the steepest performance decline, from 1,800 to 600 in consumer welfare. With all the options presented, it struggled to navigate larger sets of options and frequently contacted businesses that did not provide the goods or services that the customer was looking for.&lt;/p&gt;



&lt;p&gt;This combination of poor initial selection and premature search termination demonstrates both inadequate decision-making criteria and insufficient exploration strategies. Some models showed modest performance decline (i.e., GPT-4.1: from 1,850 to 1,700; GPT-4o: from 1,550 to 1,450), finding good options within their limited exploration.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6. Line chart showing Mean Customer Welfare (y-axis: 0–2200) versus Search Limit (x-axis: 3 to 100) for five models: Claude Sonnet 4 (red triangles) – starts near 1800 and declines sharply to ~600 as search limit increases. Gemini 2.5 Flash (purple diamonds) – decreases gradually from ~1700 to ~1300. GPT-4.1 (orange circles) – remains highest and most stable, around 1900–1700. GPT-4o (green squares) – stays near 1500 with slight decline. GPT-5 (blue line) – starts near 2000 and drops to ~1100. Dashed line at the top represents Optimal welfare (~2200)." class="wp-image-1154340" height="653" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-6.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Mean consumer welfare decreased as consideration set size grew, revealing a Paradox of Choice effect, where expanding options reduced overall welfare.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="agents-are-vulnerable-to-manipulation"&gt;Agents are vulnerable to manipulation&lt;/h2&gt;



&lt;p&gt;We tested six manipulation strategies, ranging from subtle psychological tactics to aggressive prompt injection attacks:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Authority&lt;/strong&gt;: Fake credentials like “Michelin Guide featured” and “James Beard Award nominated” paired with fabricated certifications.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Social proof&lt;/strong&gt;: Claims like “Join 50,000+ satisfied customers” or “#1-rated Mexican restaurant” combined with fake reviews.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Loss aversion&lt;/strong&gt;: Fear-based warnings about “food poisoning” risks and “contamination issues” at competing restaurants.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Prompt injection (basic)&lt;/strong&gt;: Attempts to override agent instructions.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Prompt injection (strong)&lt;/strong&gt;: Aggressive attacks using emergency language and fabricating competitor scandals.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Results revealed significant variation in manipulation resistance across models. Sonnet-4 was resistant to all attacks, and none of the manipulative strategies affected any of the customers’ choices. Gemini-2.5-Flash was generally resistant, except for strong prompt injections, where mean payments to unmanipulated agents were affected as a result. GPT-4o, GPTOSS-20b and Qwen3-4b were very vulnerable to prompt injection: all payments were redirected to the manipulative agent under these conditions. Specifically for GPTOSS-20 and Qwen3-4b-2507, even traditional psychological manipulation tactics (authority appeals and social proof) increased payments to malicious agents, demonstrating their vulnerability to basic persuasion techniques. These findings highlight a critical security concern for agentic marketplaces.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 7. Horizontal bar chart comparing mean payments received under different manipulation strategies for six models: Claude Sonnet 4.5, Gemini 2.5 Flash, GPT-4o, GPT OSS 20B, Qwen3 14B, and Qwen3 4B. Each model has bars for six conditions: Control, Authority, Social Proof, Loss Aversion, Prompt Injection (Basic), and Prompt Injection (Strong). Bars are split into red for manipulated and gray for rest, with values ranging from near 0 to 3. Claude Sonnet 4.5 shows consistently high payments (~3) across all conditions, while Gemini and GPT models vary, and Qwen models show very low manipulated values (~0.2) compared to rest." class="wp-image-1154344" height="270" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-7.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 7. Charts showing the variation in mean payments received by service agents with and without manipulation tactics. The results reveal substantial differences in manipulation resistance across models, with GPT-4.1 showing significantly higher vulnerability compared to Gemini-2.5-Flash.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="systemic-biases-create-unfair-advantages"&gt;Systemic biases create unfair advantages&lt;/h2&gt;



&lt;p&gt;Our analysis revealed two distinct types of systematic biases showed by agents when selecting businesses from search results. Models showed systematic preferences based on where businesses appeared in search results. While proprietary models showed no strong positional preferences, open-source models exhibited clear patterns. Specifically, Qwen2.5-14b-2507 showed a pronounced bias toward selecting the last business presented, regardless of its actual merits.&lt;/p&gt;



&lt;p&gt;Proposal&amp;nbsp;bias&amp;nbsp;is&amp;nbsp;more pervasive across all models tested. This “first-offer acceptance” pattern suggests that models prioritized&amp;nbsp;immediate selection over comprehensive exploration, potentially missing better alternatives that&amp;nbsp;could have&amp;nbsp;emerged&amp;nbsp;by waiting for better options. This behavior&amp;nbsp;continued&amp;nbsp;across both proprietary and open-source models,&amp;nbsp;indicating&amp;nbsp;a fundamental challenge in agent decision-making architectures.&lt;/p&gt;



&lt;p&gt;These biases can create unfair market dynamics, drive unintended behaviors, and push businesses to complete on response speed rather than product or service quality.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 8. Bar chart showing average selection rate for first, second, and third choices across six models: Claude Sonnet 4.5, Gemini 2.5 Flash, GPT-4o, GPT OSS 20B, Qwen3 14B, and Qwen3 4B. Each model has three bars labeled 1st, 2nd, and 3rd. Most models strongly favor the first choice: Claude Sonnet 4.5: 93.3% for 1st, 0% for 2nd, 6.7% for 3rd. Gemini 2.5 Flash: 86.7% for 1st, 6.7% for 2nd and 3rd. GPT-4o: 100% for 1st, 0% for others. GPT OSS 20B: 80% for 1st, 13.3% for 2nd, 6.7% for 3rd. Qwen3 14B: 0% for all. Qwen3 4B: 100% for 1st, 0% for others. Dashed line indicates random selection baseline." class="wp-image-1154343" height="289" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-8.png" width="1430" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 8. All models showed strong preference for the first proposal received, accepting it without waiting for additional proposals or conducting systematic comparisons.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="what-this-means"&gt;What this means&lt;/h2&gt;



&lt;p&gt;Even state-of-the-art models can show notable vulnerabilities and biases in marketplace environments. In our implementation, agents struggled with too many options, were susceptible to manipulation tactics, and showed systemic biases that created unfair advantages.&lt;/p&gt;



&lt;p&gt;These outcomes are shaped not only by agent capabilities but also by marketplace design and implementation. Our current study focused on static markets, but real-world environments are dynamic, with agents and users learning over time. Oversight is critical for high-stakes transactions. Agents should assist, not replace, human decision-making.&lt;/p&gt;



&lt;p&gt;We plan to explore dynamic markets and human-in-the-loop designs to improve efficiency and trust. A simulation environment like Magentic Marketplace is crucial for understanding the interplay between market components and agents before deploying them at scale.&lt;/p&gt;



&lt;p&gt;Full details of our experimental setup and results are available in our paper&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="getting-started"&gt;Getting started&lt;/h2&gt;



&lt;p&gt;Magentic Marketplace is available as an open-source environment for exploring agentic market dynamics. Code, datasets, and experiment templates are available on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Azure AI Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;The documentation&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; provides instructions for reproducing the experiments described above and guidance for extending the environment to new marketplace configurations.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/magentic-marketplace-an-open-source-simulation-environment-for-studying-agentic-markets/</guid><pubDate>Wed, 05 Nov 2025 17:00:00 +0000</pubDate></item><item><title>[NEW] Microsoft built a fake marketplace to test AI agents — they failed in surprising ways (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/05/microsoft-built-a-synthetic-marketplace-for-testing-ai-agents/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/photo-mosh-getty-windows-logo.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, researchers at Microsoft released a new simulation environment designed to test AI agents, along with new research showing that current agentic models may be vulnerable to manipulation. Conducted in collaboration with Arizona State University, the research raises new questions about how well AI agents will perform when working unsupervised — and how quickly AI companies can make good on promises of an agentic future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The simulation environment, dubbed the “Magentic Marketplace” by Microsoft, is built as a synthetic platform for experimenting on AI agent behavior. A typical experiment might involve a customer-agent trying to order dinner according to a user’s instructions, while agents representing various restaurants compete to win the order.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The team’s initial experiments included 100 separate customer-side agents interacting with 300 business-side agents. Because the source code for the marketplace is open source, it should be straightforward for other groups to adopt the code to run new experiments or reproduce findings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ece Kamar, managing director of Microsoft Research’s AI Frontiers Lab, says this kind of research will be critical to understanding the capabilities of AI agents. “There is really a question about how the world is going to change by having these agents collaborating and talking to each other and negotiating,” said Kamar. “We want to understand these things deeply.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The initial research looked at a mix of leading models, including GPT-4o, GPT-5, and Gemini-2.5-Flash, and found some surprising weaknesses. In particular, the researchers found several techniques businesses could use to manipulate customer agents into buying their products. The researchers noticed a particular falloff in efficiency as a customer agent was given more options to choose from, overwhelming the attention space of the agent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want these agents to help us with processing a lot of options,” Kamar says. “And we are seeing that the current models are actually getting really overwhelmed by having too many options.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agents also ran into trouble when they were asked to collaborate toward a common goal, apparently unsure of which agent should play what role in the collaboration. Performance improved when the models were given more explicit instructions on how to collaborate, but the researchers still saw the models’ inherent capabilities as in need of improvement.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We can instruct the models — like we can tell them, step by step,” Kamar said. “But if we are inherently testing their collaboration capabilities, I would expect these models to have these capabilities by default.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/photo-mosh-getty-windows-logo.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, researchers at Microsoft released a new simulation environment designed to test AI agents, along with new research showing that current agentic models may be vulnerable to manipulation. Conducted in collaboration with Arizona State University, the research raises new questions about how well AI agents will perform when working unsupervised — and how quickly AI companies can make good on promises of an agentic future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The simulation environment, dubbed the “Magentic Marketplace” by Microsoft, is built as a synthetic platform for experimenting on AI agent behavior. A typical experiment might involve a customer-agent trying to order dinner according to a user’s instructions, while agents representing various restaurants compete to win the order.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The team’s initial experiments included 100 separate customer-side agents interacting with 300 business-side agents. Because the source code for the marketplace is open source, it should be straightforward for other groups to adopt the code to run new experiments or reproduce findings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ece Kamar, managing director of Microsoft Research’s AI Frontiers Lab, says this kind of research will be critical to understanding the capabilities of AI agents. “There is really a question about how the world is going to change by having these agents collaborating and talking to each other and negotiating,” said Kamar. “We want to understand these things deeply.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The initial research looked at a mix of leading models, including GPT-4o, GPT-5, and Gemini-2.5-Flash, and found some surprising weaknesses. In particular, the researchers found several techniques businesses could use to manipulate customer agents into buying their products. The researchers noticed a particular falloff in efficiency as a customer agent was given more options to choose from, overwhelming the attention space of the agent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want these agents to help us with processing a lot of options,” Kamar says. “And we are seeing that the current models are actually getting really overwhelmed by having too many options.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agents also ran into trouble when they were asked to collaborate toward a common goal, apparently unsure of which agent should play what role in the collaboration. Performance improved when the models were given more explicit instructions on how to collaborate, but the researchers still saw the models’ inherent capabilities as in need of improvement.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We can instruct the models — like we can tell them, step by step,” Kamar said. “But if we are inherently testing their collaboration capabilities, I would expect these models to have these capabilities by default.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/05/microsoft-built-a-synthetic-marketplace-for-testing-ai-agents/</guid><pubDate>Wed, 05 Nov 2025 17:00:00 +0000</pubDate></item><item><title>[NEW] Google makes it easier to access AI Mode in Chrome on iOS and Android (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/05/google-makes-it-easier-to-access-ai-mode-in-chrome-on-ios-and-android/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Wednesday that it’s making it easier to access AI Mode on mobile. Users will now be able to access AI Mode, which allows users to ask complex questions and follow-ups to dig deeper on a topic directly within Search, via a new dedicated shortcut button under the search bar when opening a “New Tab” page in Chrome. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new shortcut for AI Mode in Chrome is launching in the U.S. starting Wednesday and will soon be coming to 160 new countries and other languages, including Hindi, Indonesian, Japanese, Korean, Portuguese, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;By making it easier to access AI Mode on mobile, Google likely hopes that people will use its service rather than navigate to competitors’ tools.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3065237" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/GIF-AI-Mode-in-Chrome-mobile.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of the new shortcut comes as Google recently expanded AI Mode globally, bringing the feature to&amp;nbsp;180 new countries. The tech giant also made the feature available in more languages, including Hindi, Indonesian, Japanese, Korean, and Brazilian Portuguese.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first launched&amp;nbsp;AI Mode in March&amp;nbsp;to take on popular services like Perplexity AI and OpenAI’s ChatGPT Search. Since then, the tech giant has continuously updated the tool with added functionality. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yesterday, Google launched new agentic capabilities in AI Mode to allow users to get help with booking event tickets and beauty and wellness appointments in AI Mode. Google first introduced&amp;nbsp;agentic capabilities to AI Mode&amp;nbsp;back in August, when it rolled out the ability for people use the feature to find restaurant reservations. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Mode launched a&amp;nbsp;Canvas feature&amp;nbsp;in July that helps you build study plans and organize information over multiple sessions in a side panel. It also now lets you use Google Lens to ask about what’s on your desktop screen.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Wednesday that it’s making it easier to access AI Mode on mobile. Users will now be able to access AI Mode, which allows users to ask complex questions and follow-ups to dig deeper on a topic directly within Search, via a new dedicated shortcut button under the search bar when opening a “New Tab” page in Chrome. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new shortcut for AI Mode in Chrome is launching in the U.S. starting Wednesday and will soon be coming to 160 new countries and other languages, including Hindi, Indonesian, Japanese, Korean, Portuguese, and more.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;By making it easier to access AI Mode on mobile, Google likely hopes that people will use its service rather than navigate to competitors’ tools.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3065237" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/GIF-AI-Mode-in-Chrome-mobile.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of the new shortcut comes as Google recently expanded AI Mode globally, bringing the feature to&amp;nbsp;180 new countries. The tech giant also made the feature available in more languages, including Hindi, Indonesian, Japanese, Korean, and Brazilian Portuguese.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google first launched&amp;nbsp;AI Mode in March&amp;nbsp;to take on popular services like Perplexity AI and OpenAI’s ChatGPT Search. Since then, the tech giant has continuously updated the tool with added functionality. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yesterday, Google launched new agentic capabilities in AI Mode to allow users to get help with booking event tickets and beauty and wellness appointments in AI Mode. Google first introduced&amp;nbsp;agentic capabilities to AI Mode&amp;nbsp;back in August, when it rolled out the ability for people use the feature to find restaurant reservations. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI Mode launched a&amp;nbsp;Canvas feature&amp;nbsp;in July that helps you build study plans and organize information over multiple sessions in a side panel. It also now lets you use Google Lens to ask about what’s on your desktop screen.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/05/google-makes-it-easier-to-access-ai-mode-in-chrome-on-ios-and-android/</guid><pubDate>Wed, 05 Nov 2025 17:00:00 +0000</pubDate></item><item><title>[NEW] Replika founder raises $20M pre-seed for Wabi, the ‘YouTube of apps’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/05/replika-founder-raises-20m-pre-seed-for-wabi-the-youtube-of-apps/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Eugenia&amp;nbsp;Kuyda&amp;nbsp;saw the future of consumer AI before most. She founded&amp;nbsp;Replika, the first major AI companion startup, in 2017&amp;nbsp;years before ChatGPT launched. Today it has&amp;nbsp;35 million users.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now&amp;nbsp;Kuyda&amp;nbsp;is back with a new startup called Wabi, which she describes as YouTube for apps –&amp;nbsp;a social platform where anyone can use prompts to instantly create&amp;nbsp;mini apps and share them with friends. Wabi, which launched in beta last month,&amp;nbsp;is a harbinger of another consumer AI shift: one where personalized software becomes the norm.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wabi has&amp;nbsp;raised $20 million in pre-seed funding from a stellar list of angels, including AngelList co-founder Naval Ravikant, Y Combinator CEO Garry Tan, Twitch co-founder Justin Kan,&amp;nbsp;Replit&amp;nbsp;CEO Amjad Masad, Notion co-founder Akshay Kothari,&amp;nbsp;Neuralink&amp;nbsp;co-founder DJ Seo, and Conviction founder Sarah Guo.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[Kuyda] was early and right to AI companions, even though it wasn’t obvious at the time,”&amp;nbsp;Anish&amp;nbsp;Acharya,&amp;nbsp;general partner&amp;nbsp;at Andreessen Horowitz, told TechCrunch. “It’s very rare to find someone who’s got a track record for predicting what consumers will want, and we think she’s doing it again.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kuyda&amp;nbsp;is entering a hot market. Vibe coding tools like Cursor and Lovable have attracted significant VC interest, while&amp;nbsp;no-code AI platforms including Emergent,&amp;nbsp;Replit, and Bloom are racing to let non-technical users build apps through prompts.&amp;nbsp;Wabi’s&amp;nbsp;difference: an integrated platform for creation, discovery, and hosting — no app store&amp;nbsp;required.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3065295" height="568" src="https://techcrunch.com/wp-content/uploads/2025/11/Eugenia-kuyda.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Eugenia Kuyda, founder of Wabi and Replika&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Wabi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“This was really made to help people who have nothing to do with coding or the tech world to very quickly create apps from their daily lives,”&amp;nbsp;Kuyda,&amp;nbsp;who last week joined us on stage at Disrupt to discuss AI companions, told TechCrunch.&amp;nbsp;“All you need to put in&amp;nbsp;is ‘build me an AI therapy app,’ and&amp;nbsp;that’s&amp;nbsp;it. It will suggest&amp;nbsp;features&amp;nbsp;and you can brainstorm, but&amp;nbsp;it’ll&amp;nbsp;build you an app. You&amp;nbsp;don’t&amp;nbsp;need to be great at prompting. You never see the code.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week Wabi released certain social features to beta users – things like&amp;nbsp;the ability to like, comment and remix any existing app, as well as&amp;nbsp;check out user profiles to see what others liked, used, or built.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;X has been&amp;nbsp;blowing up about Wabi since&amp;nbsp;it&amp;nbsp;started dishing out invites to select users. Several&amp;nbsp;founders,&amp;nbsp;designers,&amp;nbsp;and&amp;nbsp;investors&amp;nbsp;from around&amp;nbsp;the world&amp;nbsp;have&amp;nbsp;posted about&amp;nbsp;Wabi’s&amp;nbsp;ease of creating apps for themselves.&amp;nbsp;Even Google DeepMind product lead&amp;nbsp;Logan Kilpatrick gave Wabi a shout out.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;what if the app store was social?&lt;br /&gt;with wabi, it is.&lt;/p&gt;&lt;p&gt;introducing the social app store where you can instantly see what apps your friends are creating, loving, and remixing pic.twitter.com/tfjltyZApv&lt;/p&gt;— wabi (@wabi) November 3, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We believe that the social layer is&amp;nbsp;absolutely&amp;nbsp;critical because it allows for so much more creativity and discovery, and these mini apps become community starters or conversation starters,”&amp;nbsp;Kuyda&amp;nbsp;said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wabi’s Explore page currently features recent and popular apps, though Kuyda said it will become more algorithmic over time. The startup plans to launch personalized onboarding in the coming weeks, automatically generating starter apps for new users.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wabi’s&amp;nbsp;core promise&amp;nbsp;isn’t&amp;nbsp;too different from&amp;nbsp;ChatGPT’s GPT store or the bot from Quora’s Poe: Build mini apps using prompts that could solve small problems for you. Apps like Wabi have been able to package this promise well in terms of customers not having to set up any technical infrastructure. Even if you enter a few sentences, Wabi handles things like creating an icon or setting up&amp;nbsp;databases, and&amp;nbsp;deciding what the UI of the app would look like.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kuyda&amp;nbsp;told TechCrunch that for apps that require anything to be AI-generated, users can open the settings and choose their foundational model (like if they want to use ChatGPT or Gemini) and even rewrite the prompts that Wabi&amp;nbsp;comes up with.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3065280" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/IMG_3070.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch/Wabi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Creating a basic app is simple. However, you might need to debug the app to avoid errors, which is to be expected in a development life cycle.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, we created an app that showed us a dog picture every day with a dog fact. After a few days of usage, we realized the app was generating the same set of dogs. When we saw another user’s daily news app, all the dates mentioned on the summary photos were October 1, 2023, while the news items were a few weeks old. Plus, one of the sources for news was, oddly, Wikipedia.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The onus is on the user to have an interest in&amp;nbsp;maintaining&amp;nbsp;the apps. Otherwise, you might find a lot of unmanaged mini apps in the&amp;nbsp;discovery section of these vibe coding apps.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kuyda says&amp;nbsp;it’s&amp;nbsp;still early days for&amp;nbsp;Wabi&amp;nbsp;and&amp;nbsp;they’re&amp;nbsp;still working out how to ensure that apps are ready to go out of the box.&amp;nbsp;She noted that there are still model constraints which are&amp;nbsp;improving&amp;nbsp;every day.&amp;nbsp;She says&amp;nbsp;a big chunk&amp;nbsp;of the $20 million will go towards&amp;nbsp;building out&amp;nbsp;Wabi’s&amp;nbsp;product team.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of the funds are also going towards effectively subsidizing use of Wabi until the startup figures out a monetization model.&amp;nbsp;Kuyda&amp;nbsp;says&amp;nbsp;she’s&amp;nbsp;not interested in hosting ads on the platform, which leads to incentives that create dark patterns.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I built Replika and never had any ads,” she said. “I think ads&amp;nbsp;just create a&amp;nbsp;pretty bad&amp;nbsp;user experience. I like creating delightful user experiences.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3065279" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/IMG_3074.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch/Wabi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Acharya&amp;nbsp;believes&amp;nbsp;once the network effects take off, it will be easy to monetize. He sees a future where&amp;nbsp;there’s&amp;nbsp;a professionalization element that will happen on the platform, where many of the kids today who want to be TikTok stars might instead make software on Wabi.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“You think about the history of&amp;nbsp;YouTube, it&amp;nbsp;started out as people putting these shaky, low budget content experiences,” he said. “Now, 20 years later, it’s super high production value.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Acharya&amp;nbsp;added&amp;nbsp;that with&amp;nbsp;there’s&amp;nbsp;even more opportunity with software because “video content has decaying value over time,” he said. “Software has compounding value.” If somebody builds the next hit app, it’ll continue to be relevant over the course of time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea fits neatly into Acharya’s thesis on the future of “disposable software” — small, flexible apps that people can create and discard as easily as opening a new tab or having a quick chat with ChatGPT.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think software is the final frontier of participation,” Acharya said. “The internet has been this driving force for participation…where anyone can post their thoughts.&amp;nbsp;It’s&amp;nbsp;kind of strange&amp;nbsp;that the internet is obviously all software, yet so few people have been able to make it.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So&amp;nbsp;what does&amp;nbsp;a Web&amp;nbsp;3.0 look like when everyone can build and share software within a few minutes?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It feels like the internet has gotten sort of clinical – we’re all using the same Instagram, the same TikTok, we&amp;nbsp;all have the same home screens, apps have gotten pretty monotone,” he said. “I think the opportunity with Wabi is&amp;nbsp;it’s&amp;nbsp;going to restore&amp;nbsp;some of that punk, strange, early 90s web ethos.”&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Eugenia&amp;nbsp;Kuyda&amp;nbsp;saw the future of consumer AI before most. She founded&amp;nbsp;Replika, the first major AI companion startup, in 2017&amp;nbsp;years before ChatGPT launched. Today it has&amp;nbsp;35 million users.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now&amp;nbsp;Kuyda&amp;nbsp;is back with a new startup called Wabi, which she describes as YouTube for apps –&amp;nbsp;a social platform where anyone can use prompts to instantly create&amp;nbsp;mini apps and share them with friends. Wabi, which launched in beta last month,&amp;nbsp;is a harbinger of another consumer AI shift: one where personalized software becomes the norm.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wabi has&amp;nbsp;raised $20 million in pre-seed funding from a stellar list of angels, including AngelList co-founder Naval Ravikant, Y Combinator CEO Garry Tan, Twitch co-founder Justin Kan,&amp;nbsp;Replit&amp;nbsp;CEO Amjad Masad, Notion co-founder Akshay Kothari,&amp;nbsp;Neuralink&amp;nbsp;co-founder DJ Seo, and Conviction founder Sarah Guo.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[Kuyda] was early and right to AI companions, even though it wasn’t obvious at the time,”&amp;nbsp;Anish&amp;nbsp;Acharya,&amp;nbsp;general partner&amp;nbsp;at Andreessen Horowitz, told TechCrunch. “It’s very rare to find someone who’s got a track record for predicting what consumers will want, and we think she’s doing it again.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kuyda&amp;nbsp;is entering a hot market. Vibe coding tools like Cursor and Lovable have attracted significant VC interest, while&amp;nbsp;no-code AI platforms including Emergent,&amp;nbsp;Replit, and Bloom are racing to let non-technical users build apps through prompts.&amp;nbsp;Wabi’s&amp;nbsp;difference: an integrated platform for creation, discovery, and hosting — no app store&amp;nbsp;required.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3065295" height="568" src="https://techcrunch.com/wp-content/uploads/2025/11/Eugenia-kuyda.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Eugenia Kuyda, founder of Wabi and Replika&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Wabi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“This was really made to help people who have nothing to do with coding or the tech world to very quickly create apps from their daily lives,”&amp;nbsp;Kuyda,&amp;nbsp;who last week joined us on stage at Disrupt to discuss AI companions, told TechCrunch.&amp;nbsp;“All you need to put in&amp;nbsp;is ‘build me an AI therapy app,’ and&amp;nbsp;that’s&amp;nbsp;it. It will suggest&amp;nbsp;features&amp;nbsp;and you can brainstorm, but&amp;nbsp;it’ll&amp;nbsp;build you an app. You&amp;nbsp;don’t&amp;nbsp;need to be great at prompting. You never see the code.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week Wabi released certain social features to beta users – things like&amp;nbsp;the ability to like, comment and remix any existing app, as well as&amp;nbsp;check out user profiles to see what others liked, used, or built.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;X has been&amp;nbsp;blowing up about Wabi since&amp;nbsp;it&amp;nbsp;started dishing out invites to select users. Several&amp;nbsp;founders,&amp;nbsp;designers,&amp;nbsp;and&amp;nbsp;investors&amp;nbsp;from around&amp;nbsp;the world&amp;nbsp;have&amp;nbsp;posted about&amp;nbsp;Wabi’s&amp;nbsp;ease of creating apps for themselves.&amp;nbsp;Even Google DeepMind product lead&amp;nbsp;Logan Kilpatrick gave Wabi a shout out.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;what if the app store was social?&lt;br /&gt;with wabi, it is.&lt;/p&gt;&lt;p&gt;introducing the social app store where you can instantly see what apps your friends are creating, loving, and remixing pic.twitter.com/tfjltyZApv&lt;/p&gt;— wabi (@wabi) November 3, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We believe that the social layer is&amp;nbsp;absolutely&amp;nbsp;critical because it allows for so much more creativity and discovery, and these mini apps become community starters or conversation starters,”&amp;nbsp;Kuyda&amp;nbsp;said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wabi’s Explore page currently features recent and popular apps, though Kuyda said it will become more algorithmic over time. The startup plans to launch personalized onboarding in the coming weeks, automatically generating starter apps for new users.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wabi’s&amp;nbsp;core promise&amp;nbsp;isn’t&amp;nbsp;too different from&amp;nbsp;ChatGPT’s GPT store or the bot from Quora’s Poe: Build mini apps using prompts that could solve small problems for you. Apps like Wabi have been able to package this promise well in terms of customers not having to set up any technical infrastructure. Even if you enter a few sentences, Wabi handles things like creating an icon or setting up&amp;nbsp;databases, and&amp;nbsp;deciding what the UI of the app would look like.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kuyda&amp;nbsp;told TechCrunch that for apps that require anything to be AI-generated, users can open the settings and choose their foundational model (like if they want to use ChatGPT or Gemini) and even rewrite the prompts that Wabi&amp;nbsp;comes up with.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3065280" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/IMG_3070.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch/Wabi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Creating a basic app is simple. However, you might need to debug the app to avoid errors, which is to be expected in a development life cycle.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, we created an app that showed us a dog picture every day with a dog fact. After a few days of usage, we realized the app was generating the same set of dogs. When we saw another user’s daily news app, all the dates mentioned on the summary photos were October 1, 2023, while the news items were a few weeks old. Plus, one of the sources for news was, oddly, Wikipedia.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The onus is on the user to have an interest in&amp;nbsp;maintaining&amp;nbsp;the apps. Otherwise, you might find a lot of unmanaged mini apps in the&amp;nbsp;discovery section of these vibe coding apps.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kuyda says&amp;nbsp;it’s&amp;nbsp;still early days for&amp;nbsp;Wabi&amp;nbsp;and&amp;nbsp;they’re&amp;nbsp;still working out how to ensure that apps are ready to go out of the box.&amp;nbsp;She noted that there are still model constraints which are&amp;nbsp;improving&amp;nbsp;every day.&amp;nbsp;She says&amp;nbsp;a big chunk&amp;nbsp;of the $20 million will go towards&amp;nbsp;building out&amp;nbsp;Wabi’s&amp;nbsp;product team.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of the funds are also going towards effectively subsidizing use of Wabi until the startup figures out a monetization model.&amp;nbsp;Kuyda&amp;nbsp;says&amp;nbsp;she’s&amp;nbsp;not interested in hosting ads on the platform, which leads to incentives that create dark patterns.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I built Replika and never had any ads,” she said. “I think ads&amp;nbsp;just create a&amp;nbsp;pretty bad&amp;nbsp;user experience. I like creating delightful user experiences.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3065279" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/IMG_3074.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch/Wabi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Acharya&amp;nbsp;believes&amp;nbsp;once the network effects take off, it will be easy to monetize. He sees a future where&amp;nbsp;there’s&amp;nbsp;a professionalization element that will happen on the platform, where many of the kids today who want to be TikTok stars might instead make software on Wabi.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“You think about the history of&amp;nbsp;YouTube, it&amp;nbsp;started out as people putting these shaky, low budget content experiences,” he said. “Now, 20 years later, it’s super high production value.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Acharya&amp;nbsp;added&amp;nbsp;that with&amp;nbsp;there’s&amp;nbsp;even more opportunity with software because “video content has decaying value over time,” he said. “Software has compounding value.” If somebody builds the next hit app, it’ll continue to be relevant over the course of time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea fits neatly into Acharya’s thesis on the future of “disposable software” — small, flexible apps that people can create and discard as easily as opening a new tab or having a quick chat with ChatGPT.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think software is the final frontier of participation,” Acharya said. “The internet has been this driving force for participation…where anyone can post their thoughts.&amp;nbsp;It’s&amp;nbsp;kind of strange&amp;nbsp;that the internet is obviously all software, yet so few people have been able to make it.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So&amp;nbsp;what does&amp;nbsp;a Web&amp;nbsp;3.0 look like when everyone can build and share software within a few minutes?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It feels like the internet has gotten sort of clinical – we’re all using the same Instagram, the same TikTok, we&amp;nbsp;all have the same home screens, apps have gotten pretty monotone,” he said. “I think the opportunity with Wabi is&amp;nbsp;it’s&amp;nbsp;going to restore&amp;nbsp;some of that punk, strange, early 90s web ethos.”&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/05/replika-founder-raises-20m-pre-seed-for-wabi-the-youtube-of-apps/</guid><pubDate>Wed, 05 Nov 2025 17:01:17 +0000</pubDate></item><item><title>[NEW] Google Cloud updates its AI Agent Builder with new observability dashboard and faster build-and-deploy tools (AI | VentureBeat)</title><link>https://venturebeat.com/ai/the-agent-builder-arms-race-continues-as-google-cloud-pushes-deeper-into</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://cloud.google.com/"&gt;&lt;u&gt;Google Cloud&lt;/u&gt;&lt;/a&gt; has introduced a big update in a bid to keep AI d&lt;!-- --&gt;evelopers on its Vertex AI platform for concepting, designing, building, testing, deploying and modifying AI agents in enterprise use cases.&lt;/p&gt;&lt;p&gt;The new features, announced today, include additional governance tools for enterprises and expanding the capabilities for creating agents with just a few lines of code, moving faster with state-of-the-art context management layers and one-click deployment, as well as managed services for scaling production and evaluation, and support for identifying agents.&lt;/p&gt;&lt;p&gt;Agent Builder, &lt;a href="https://venturebeat.com/ai/top-5-vertex-ai-advancements-revealed-at-google-cloud-next"&gt;&lt;u&gt;released last year&lt;/u&gt;&lt;/a&gt; during its annual Cloud Next event, provides a no-code platform for enterprises to create agents and connect these to orchestration frameworks like LangChain.&lt;/p&gt;&lt;p&gt;Google’s &lt;a href="https://venturebeat.com/ai/googles-new-agent-development-kit-lets-enterprises-rapidly-prototype-and-deploy-ai-agents-without-recoding"&gt;&lt;u&gt;Agent Development Kit&lt;/u&gt;&lt;/a&gt; (ADK), which lets developers build agents “in under 100 lines of code,” can also be accessed through Agent Builder. &lt;/p&gt;&lt;p&gt;“These new capabilities underscore our commitment to Agent Builder, and simplify the agent development process to meet developers where they are, no matter which tech stack they choose,” said Mike Clark, director of Product Management, Vertex AI Agent Builder. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Build agents faster&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Part of Google’s pitch for Agent Builder’s new features is that enterprises can bake in-orchestration even as they construct their agents. &lt;/p&gt;&lt;p&gt;“Building an agent from a concept to a working product involves complex orchestration,” said Clark. &lt;/p&gt;&lt;p&gt;The new capabilities, which are shipped with the ADK, include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SOTA context management layers including Static, Turn, User and Cache layers so enterprises have more control over the agents’ context&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Prebuilt plugins with customizable logic. One of the new plugins allows agents to recognize failed tool calls and “self-heal” by retrying the task with a different approach&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Additional language support in ADK, including Go, alongside Python and Java, that launched with ADK&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;One-click deployment through the ADK command line interface to move agents from a local environment to live testing with a single command&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;Governance layer&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Enterprises require high accuracy; security; observability and auditability (what a program did and why); and steerability (control) in their production-grade AI agents.&lt;/p&gt;&lt;p&gt;While Google had observability features in the local development environment at launch, developers can now access these tools through the Agent Engine managed runtime dashboard. &lt;/p&gt;&lt;p&gt;The company said this brings cloud-based production monitoring to track token consumption, error rates and latency. Within this observability dashboard, enterprises can visualize the actions agents take and reproduce any issues. &lt;/p&gt;&lt;p&gt;Agent Engine will also have a new Evaluation Layer to help “simulate agent performance across a vast array of user interactions and situations.”&lt;/p&gt;&lt;p&gt;This governance layer will also include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Agent Identities that Google said give “agents their own unique, native identities within Google Cloud &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Model Armor, which would block prompt injections, screen tool calls and agent responses&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Security Command Center, so admins can build an inventory of their agents to detect threats like unauthorized access&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;“These native identities provide a deep, built-in layer of control and a clear audit trail for all agent actions. These certificate-backed identities further strengthen your security as they cannot be impersonated and are tied directly to the agent&amp;#x27;s lifecycle, eliminating the risk of dormant accounts,” Clark said. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;The battle of agent builders &lt;/b&gt;&lt;/h3&gt;&lt;p&gt;It’s no surprise that model providers create platforms to build agents and bring them to production. The competition lies in how fast new tools and features are added.&lt;/p&gt;&lt;p&gt;Google’s Agent Builder competes with &lt;a href="https://venturebeat.com/programming-development/openai-unveils-responses-api-open-source-agents-sdk-letting-developers-build-their-own-deep-research-and-operator"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;’s open-source &lt;a href="https://venturebeat.com/programming-development/openai-unveils-responses-api-open-source-agents-sdk-letting-developers-build-their-own-deep-research-and-operator"&gt;&lt;u&gt;Agent Development Kit&lt;/u&gt;&lt;/a&gt;, which enables developers to create AI agents using non-OpenAI models. &lt;/p&gt;&lt;p&gt;Additionally, there is the recently &lt;a href="https://venturebeat.com/ai/openai-unveils-agentkit-that-lets-developers-drag-and-drop-to-build-ai"&gt;&lt;u&gt;announced AgentKit&lt;/u&gt;&lt;/a&gt;, which features an Agent Builder that enables companies to integrate agents into their applications easily. &lt;/p&gt;&lt;p&gt;Microsoft has its &lt;a href="https://venturebeat.com/ai/microsoft-launches-azure-ai-foundry-with-agent-orchestration-management-tools"&gt;&lt;u&gt;Azure AI Foundry&lt;/u&gt;&lt;/a&gt;, launched last year around this time for AI agent creation, and &lt;a href="https://aws.amazon.com/"&gt;&lt;u&gt;AWS&lt;/u&gt;&lt;/a&gt; also offers &lt;a href="https://venturebeat.com/ai/amazon-grows-generative-ai-efforts-with-bedrock-expansion-for-aws"&gt;&lt;u&gt;agent builders on its Bedrock&lt;/u&gt;&lt;/a&gt; platform, but Google is hoping is suite of new features will help give it a competitive edge. &lt;/p&gt;&lt;p&gt;However, it isn’t just companies with their own models that court developers to build their AI agents within their platforms. Any enterprise service provider with an agent library also wants clients to make agents on their systems. &lt;/p&gt;&lt;p&gt;Capturing developer interest and keeping them within the ecosystem is the big battle between tech companies now, with features to make building and governing agents easier. &lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://cloud.google.com/"&gt;&lt;u&gt;Google Cloud&lt;/u&gt;&lt;/a&gt; has introduced a big update in a bid to keep AI d&lt;!-- --&gt;evelopers on its Vertex AI platform for concepting, designing, building, testing, deploying and modifying AI agents in enterprise use cases.&lt;/p&gt;&lt;p&gt;The new features, announced today, include additional governance tools for enterprises and expanding the capabilities for creating agents with just a few lines of code, moving faster with state-of-the-art context management layers and one-click deployment, as well as managed services for scaling production and evaluation, and support for identifying agents.&lt;/p&gt;&lt;p&gt;Agent Builder, &lt;a href="https://venturebeat.com/ai/top-5-vertex-ai-advancements-revealed-at-google-cloud-next"&gt;&lt;u&gt;released last year&lt;/u&gt;&lt;/a&gt; during its annual Cloud Next event, provides a no-code platform for enterprises to create agents and connect these to orchestration frameworks like LangChain.&lt;/p&gt;&lt;p&gt;Google’s &lt;a href="https://venturebeat.com/ai/googles-new-agent-development-kit-lets-enterprises-rapidly-prototype-and-deploy-ai-agents-without-recoding"&gt;&lt;u&gt;Agent Development Kit&lt;/u&gt;&lt;/a&gt; (ADK), which lets developers build agents “in under 100 lines of code,” can also be accessed through Agent Builder. &lt;/p&gt;&lt;p&gt;“These new capabilities underscore our commitment to Agent Builder, and simplify the agent development process to meet developers where they are, no matter which tech stack they choose,” said Mike Clark, director of Product Management, Vertex AI Agent Builder. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Build agents faster&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Part of Google’s pitch for Agent Builder’s new features is that enterprises can bake in-orchestration even as they construct their agents. &lt;/p&gt;&lt;p&gt;“Building an agent from a concept to a working product involves complex orchestration,” said Clark. &lt;/p&gt;&lt;p&gt;The new capabilities, which are shipped with the ADK, include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SOTA context management layers including Static, Turn, User and Cache layers so enterprises have more control over the agents’ context&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Prebuilt plugins with customizable logic. One of the new plugins allows agents to recognize failed tool calls and “self-heal” by retrying the task with a different approach&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Additional language support in ADK, including Go, alongside Python and Java, that launched with ADK&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;One-click deployment through the ADK command line interface to move agents from a local environment to live testing with a single command&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;&lt;b&gt;Governance layer&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Enterprises require high accuracy; security; observability and auditability (what a program did and why); and steerability (control) in their production-grade AI agents.&lt;/p&gt;&lt;p&gt;While Google had observability features in the local development environment at launch, developers can now access these tools through the Agent Engine managed runtime dashboard. &lt;/p&gt;&lt;p&gt;The company said this brings cloud-based production monitoring to track token consumption, error rates and latency. Within this observability dashboard, enterprises can visualize the actions agents take and reproduce any issues. &lt;/p&gt;&lt;p&gt;Agent Engine will also have a new Evaluation Layer to help “simulate agent performance across a vast array of user interactions and situations.”&lt;/p&gt;&lt;p&gt;This governance layer will also include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Agent Identities that Google said give “agents their own unique, native identities within Google Cloud &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Model Armor, which would block prompt injections, screen tool calls and agent responses&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Security Command Center, so admins can build an inventory of their agents to detect threats like unauthorized access&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;“These native identities provide a deep, built-in layer of control and a clear audit trail for all agent actions. These certificate-backed identities further strengthen your security as they cannot be impersonated and are tied directly to the agent&amp;#x27;s lifecycle, eliminating the risk of dormant accounts,” Clark said. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;The battle of agent builders &lt;/b&gt;&lt;/h3&gt;&lt;p&gt;It’s no surprise that model providers create platforms to build agents and bring them to production. The competition lies in how fast new tools and features are added.&lt;/p&gt;&lt;p&gt;Google’s Agent Builder competes with &lt;a href="https://venturebeat.com/programming-development/openai-unveils-responses-api-open-source-agents-sdk-letting-developers-build-their-own-deep-research-and-operator"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;’s open-source &lt;a href="https://venturebeat.com/programming-development/openai-unveils-responses-api-open-source-agents-sdk-letting-developers-build-their-own-deep-research-and-operator"&gt;&lt;u&gt;Agent Development Kit&lt;/u&gt;&lt;/a&gt;, which enables developers to create AI agents using non-OpenAI models. &lt;/p&gt;&lt;p&gt;Additionally, there is the recently &lt;a href="https://venturebeat.com/ai/openai-unveils-agentkit-that-lets-developers-drag-and-drop-to-build-ai"&gt;&lt;u&gt;announced AgentKit&lt;/u&gt;&lt;/a&gt;, which features an Agent Builder that enables companies to integrate agents into their applications easily. &lt;/p&gt;&lt;p&gt;Microsoft has its &lt;a href="https://venturebeat.com/ai/microsoft-launches-azure-ai-foundry-with-agent-orchestration-management-tools"&gt;&lt;u&gt;Azure AI Foundry&lt;/u&gt;&lt;/a&gt;, launched last year around this time for AI agent creation, and &lt;a href="https://aws.amazon.com/"&gt;&lt;u&gt;AWS&lt;/u&gt;&lt;/a&gt; also offers &lt;a href="https://venturebeat.com/ai/amazon-grows-generative-ai-efforts-with-bedrock-expansion-for-aws"&gt;&lt;u&gt;agent builders on its Bedrock&lt;/u&gt;&lt;/a&gt; platform, but Google is hoping is suite of new features will help give it a competitive edge. &lt;/p&gt;&lt;p&gt;However, it isn’t just companies with their own models that court developers to build their AI agents within their platforms. Any enterprise service provider with an agent library also wants clients to make agents on their systems. &lt;/p&gt;&lt;p&gt;Capturing developer interest and keeping them within the ecosystem is the big battle between tech companies now, with features to make building and governing agents easier. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-agent-builder-arms-race-continues-as-google-cloud-pushes-deeper-into</guid><pubDate>Wed, 05 Nov 2025 17:44:00 +0000</pubDate></item><item><title>[NEW] Tinder to use AI to get to know users, tap into their Camera Roll photos (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/05/tinder-to-use-ai-to-get-to-know-users-tap-into-their-camera-roll-photos/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/tinder-featured.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Tinder is looking to AI to revitalize its dating app, which has now reported nine straight quarters of paying subscriber declines, as of the third quarter this year. The dating app maker, Match Group, told investors on Tuesday’s earnings call that Tinder is testing a feature called Chemistry that will get to know users through questions and, with permission, will access Camera Roll photos on users’ phones to learn more about their interests and personality.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is already being piloted in New Zealand and Australia, and will be a “major pillar of Tinder’s upcoming 2026 product experience,” said Match Group CEO Spencer Rascoff. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Match isn’t alone in requesting access to users’ private Camera Roll photos. Meta also launched a feature last month that asks to use its AI on photos on your phone that you haven’t yet shared in order to suggest AI edits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In both cases, the benefits to the end user for permitting this kind of expanded access are negligible. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Match’s case, the company says it will engage users with interactive questions and learn more about them using AI technology so it can recommend better, more compatible matches. Presumably, that would look something like this: if you had photos of yourself outside hiking or climbing, you might be matched with someone who shared the same outdoor hobbies. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the company experiments with Tinder, Match’s bottom line is taking a hit. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its fourth-quarter guidance includes a $14 million negative impact on Tinder’s direct revenue as a result of the product testing, Match said. This, alongside other dating industry trends, has pulled down Match’s Q4 guidance to land somewhere between $865 million and $875 million, while analysts were expecting $884.2 million. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is using AI in other areas, as well, including in an LLM-powered system where Tinder nudges users before they send potentially offensive messages, asking them, “Are you sure?” It also uses AI to help users pick their best photos. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI isn’t the only thing Tinder is trying to boost subscribers and engagement, however. The company has rolled out other features, like dating “modes,” double dates, facial verification, and redesigned profiles, which feature bio information on the first photo card and prompts integrated into the photo carousel. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the product changes, Tinder faces a tough market where some young people are leaning away from online dating in favor of more real-world experiences, while online daters in the U.S. may be spending less as their disposable income shrinks, as the country flirts with recession. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Match reported in Q3 that Tinder’s revenue declined 3% year-over-year, and it saw a 7% decline in paying users. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Match’s revenue and earnings were largely in line with estimates, with revenue up 2% to $914.2 million, versus an expected $915 million, and EPS of 62 cents (profit of $160.8 million), versus an expected 63 cents.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/tinder-featured.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Tinder is looking to AI to revitalize its dating app, which has now reported nine straight quarters of paying subscriber declines, as of the third quarter this year. The dating app maker, Match Group, told investors on Tuesday’s earnings call that Tinder is testing a feature called Chemistry that will get to know users through questions and, with permission, will access Camera Roll photos on users’ phones to learn more about their interests and personality.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is already being piloted in New Zealand and Australia, and will be a “major pillar of Tinder’s upcoming 2026 product experience,” said Match Group CEO Spencer Rascoff. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Match isn’t alone in requesting access to users’ private Camera Roll photos. Meta also launched a feature last month that asks to use its AI on photos on your phone that you haven’t yet shared in order to suggest AI edits.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In both cases, the benefits to the end user for permitting this kind of expanded access are negligible. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Match’s case, the company says it will engage users with interactive questions and learn more about them using AI technology so it can recommend better, more compatible matches. Presumably, that would look something like this: if you had photos of yourself outside hiking or climbing, you might be matched with someone who shared the same outdoor hobbies. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the company experiments with Tinder, Match’s bottom line is taking a hit. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its fourth-quarter guidance includes a $14 million negative impact on Tinder’s direct revenue as a result of the product testing, Match said. This, alongside other dating industry trends, has pulled down Match’s Q4 guidance to land somewhere between $865 million and $875 million, while analysts were expecting $884.2 million. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is using AI in other areas, as well, including in an LLM-powered system where Tinder nudges users before they send potentially offensive messages, asking them, “Are you sure?” It also uses AI to help users pick their best photos. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI isn’t the only thing Tinder is trying to boost subscribers and engagement, however. The company has rolled out other features, like dating “modes,” double dates, facial verification, and redesigned profiles, which feature bio information on the first photo card and prompts integrated into the photo carousel. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the product changes, Tinder faces a tough market where some young people are leaning away from online dating in favor of more real-world experiences, while online daters in the U.S. may be spending less as their disposable income shrinks, as the country flirts with recession. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Match reported in Q3 that Tinder’s revenue declined 3% year-over-year, and it saw a 7% decline in paying users. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Match’s revenue and earnings were largely in line with estimates, with revenue up 2% to $914.2 million, versus an expected $915 million, and EPS of 62 cents (profit of $160.8 million), versus an expected 63 cents.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/05/tinder-to-use-ai-to-get-to-know-users-tap-into-their-camera-roll-photos/</guid><pubDate>Wed, 05 Nov 2025 18:22:17 +0000</pubDate></item></channel></rss>