<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 06 Oct 2025 01:39:28 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>The Reinforcement Gap — or why some AI skills improve faster than others (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/05/the-reinforcement-gap-or-why-some-ai-skills-improve-faster-than-others/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1575097396.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI coding tools are getting better fast. If you don’t work in code, it can be hard to notice how much things are changing, but GPT-5 and Gemini 2.5 have made a whole new set of developer tricks possible to automate, and last week Sonnet 4.5 did it again.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, other skills are progressing more slowly. If you are using AI to write emails, you’re probably getting the same value out of it you did a year ago. Even when the model gets better, the product doesn’t always benefit — particularly when the product is a chatbot that’s doing a dozen different jobs at the same time. AI is still making progress, but it’s not as evenly distributed as it used to be.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The difference in progress is simpler than it seems. Coding apps are benefitting from billions of easily measurable tests, which can train them to produce workable code. This is reinforcement learning (RL), arguably the biggest driver of AI progress over the past six months and getting more intricate all the time. You can do reinforcement learning with human graders, but it works best if there’s a clear pass-fail metric, so you can repeat it billions of times without having to stop for human input.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the industry relies increasingly on reinforcement learning to improve products, we’re seeing a real difference between capabilities that can be automatically graded and the ones that can’t. RL-friendly skills like bug-fixing and competitive math are getting better fast, while skills like writing make only incremental progress.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, there’s a reinforcement gap — and it’s becoming one of the most important factors for what AI systems can and can’t do.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some ways, software development is the perfect subject for reinforcement learning. Even before AI, there was a whole sub-discipline devoted to testing how software would hold up under pressure — largely because developers needed to make sure their code wouldn’t break before they deployed it. So even the most elegant code still needs to pass through unit testing, integration testing, security testing, and so on. Human developers use these tests routinely to validate their code and,&amp;nbsp;as Google’s senior director for dev tools recently told me, they’re just as useful for validating AI-generated code. Even more than that, they’re useful for reinforcement learning, since they’re already systematized and repeatable at a massive scale.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s no easy way to validate a well-written email or a good chatbot response; these skills are inherently subjective and harder to measure at scale. But not every task falls neatly into “easy to test” or “hard to test” categories. We don’t have an out-of-the-box testing kit for quarterly financial reports or actuarial science, but a well-capitalized accounting startup could probably build one from scratch. Some testing kits will work better than others, of course, and some companies will be smarter about how to approach the problem. But the testability of the underlying process is going to be the deciding factor in whether the underlying process can be made into a functional product instead of just an exciting demo.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Some processes turn out to be more testable than you might think. If you’d asked me last week, I would have put AI-generated video in the “hard to test” category, but the immense progress made by OpenAI’s new Sora 2 model shows it may not be as hard as it looks. In Sora 2, objects no longer appear and disappear out of nowhere. Faces hold their shape, looking like a specific person rather than just a collection of features. Sora 2 footage respects the laws of physics in both obvious and subtle ways. I suspect that, if you peeked behind the curtain, you’d find a robust reinforcement learning system for each of these qualities. Put together, they make the difference between photorealism and an entertaining hallucination.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To be clear, this isn’t a hard and fast rule of artificial intelligence. It’s a result of the central role reinforcement learning is playing in AI development, which could easily change as models develop. But as long as RL is the primary tool for bringing AI products to market, the reinforcement gap will only grow bigger — with serious implications for both startups and the economy at large. If a process ends up on the right side of the reinforcement gap, startups will probably succeed in automating it — and anyone doing that work now may end up looking for a new career. The question of which healthcare services are RL-trainable, for instance, has enormous implications for the shape of the economy over the next 20 years. And if surprises like Sora 2 are any indication, we may not have to wait long for an answer.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1575097396.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI coding tools are getting better fast. If you don’t work in code, it can be hard to notice how much things are changing, but GPT-5 and Gemini 2.5 have made a whole new set of developer tricks possible to automate, and last week Sonnet 4.5 did it again.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, other skills are progressing more slowly. If you are using AI to write emails, you’re probably getting the same value out of it you did a year ago. Even when the model gets better, the product doesn’t always benefit — particularly when the product is a chatbot that’s doing a dozen different jobs at the same time. AI is still making progress, but it’s not as evenly distributed as it used to be.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The difference in progress is simpler than it seems. Coding apps are benefitting from billions of easily measurable tests, which can train them to produce workable code. This is reinforcement learning (RL), arguably the biggest driver of AI progress over the past six months and getting more intricate all the time. You can do reinforcement learning with human graders, but it works best if there’s a clear pass-fail metric, so you can repeat it billions of times without having to stop for human input.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the industry relies increasingly on reinforcement learning to improve products, we’re seeing a real difference between capabilities that can be automatically graded and the ones that can’t. RL-friendly skills like bug-fixing and competitive math are getting better fast, while skills like writing make only incremental progress.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, there’s a reinforcement gap — and it’s becoming one of the most important factors for what AI systems can and can’t do.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some ways, software development is the perfect subject for reinforcement learning. Even before AI, there was a whole sub-discipline devoted to testing how software would hold up under pressure — largely because developers needed to make sure their code wouldn’t break before they deployed it. So even the most elegant code still needs to pass through unit testing, integration testing, security testing, and so on. Human developers use these tests routinely to validate their code and,&amp;nbsp;as Google’s senior director for dev tools recently told me, they’re just as useful for validating AI-generated code. Even more than that, they’re useful for reinforcement learning, since they’re already systematized and repeatable at a massive scale.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s no easy way to validate a well-written email or a good chatbot response; these skills are inherently subjective and harder to measure at scale. But not every task falls neatly into “easy to test” or “hard to test” categories. We don’t have an out-of-the-box testing kit for quarterly financial reports or actuarial science, but a well-capitalized accounting startup could probably build one from scratch. Some testing kits will work better than others, of course, and some companies will be smarter about how to approach the problem. But the testability of the underlying process is going to be the deciding factor in whether the underlying process can be made into a functional product instead of just an exciting demo.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Some processes turn out to be more testable than you might think. If you’d asked me last week, I would have put AI-generated video in the “hard to test” category, but the immense progress made by OpenAI’s new Sora 2 model shows it may not be as hard as it looks. In Sora 2, objects no longer appear and disappear out of nowhere. Faces hold their shape, looking like a specific person rather than just a collection of features. Sora 2 footage respects the laws of physics in both obvious and subtle ways. I suspect that, if you peeked behind the curtain, you’d find a robust reinforcement learning system for each of these qualities. Put together, they make the difference between photorealism and an entertaining hallucination.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To be clear, this isn’t a hard and fast rule of artificial intelligence. It’s a result of the central role reinforcement learning is playing in AI development, which could easily change as models develop. But as long as RL is the primary tool for bringing AI products to market, the reinforcement gap will only grow bigger — with serious implications for both startups and the economy at large. If a process ends up on the right side of the reinforcement gap, startups will probably succeed in automating it — and anyone doing that work now may end up looking for a new career. The question of which healthcare services are RL-trainable, for instance, has enormous implications for the shape of the economy over the next 20 years. And if surprises like Sora 2 are any indication, we may not have to wait long for an answer.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/05/the-reinforcement-gap-or-why-some-ai-skills-improve-faster-than-others/</guid><pubDate>Sun, 05 Oct 2025 15:00:00 +0000</pubDate></item><item><title>OpenAI and Jony Ive may be struggling to figure out their AI device (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/05/openai-and-jony-ive-may-be-struggling-to-figure-out-their-ai-device/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/portrait.jpg?w=1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI and Jony Ive face significant technical challenges as they work to develop a screen-less, AI-powered device, according to the Financial Times.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Back in May, OpenAI acquired io, the device startup founded by the legendary Apple designer with OpenAI CEO Sam Altman, for $6.5 billion. At the time, Altman declared that Ive and his team would help the company “create a new generation of AI-powered computers,” while Bloomberg reported that the first devices to emerge from the deal were scheduled to launch in 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The FT now says that OpenAI and Ive aim to create “a palm-sized device without a screen that can take audio and visual cues from the physical environment and respond to users’ requests.” But unresolved issues around the device’s “personality,” how it handles privacy, and computing infrastructure might delay the launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, one source told the FT that rather than waiting for a specific verbal prompt, the device would take an “always on” approach — but the team has reportedly struggled to ensure it only speaks up when useful and ends its conversations at the appropriate time.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/portrait.jpg?w=1000" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI and Jony Ive face significant technical challenges as they work to develop a screen-less, AI-powered device, according to the Financial Times.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Back in May, OpenAI acquired io, the device startup founded by the legendary Apple designer with OpenAI CEO Sam Altman, for $6.5 billion. At the time, Altman declared that Ive and his team would help the company “create a new generation of AI-powered computers,” while Bloomberg reported that the first devices to emerge from the deal were scheduled to launch in 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The FT now says that OpenAI and Ive aim to create “a palm-sized device without a screen that can take audio and visual cues from the physical environment and respond to users’ requests.” But unresolved issues around the device’s “personality,” how it handles privacy, and computing infrastructure might delay the launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, one source told the FT that rather than waiting for a specific verbal prompt, the device would take an “always on” approach — but the team has reportedly struggled to ensure it only speaks up when useful and ends its conversations at the appropriate time.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/05/openai-and-jony-ive-may-be-struggling-to-figure-out-their-ai-device/</guid><pubDate>Sun, 05 Oct 2025 16:34:26 +0000</pubDate></item><item><title>California’s new AI safety law shows regulation and innovation don’t have to clash (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/05/californias-new-ai-safety-law-shows-regulation-and-innovation-dont-have-to-clash/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;SB 53, the AI&amp;nbsp;safety&amp;nbsp;and transparency&amp;nbsp;bill&amp;nbsp;that California&amp;nbsp;Gov. Gavin Newsom signed into law this week, is proof that state regulation&amp;nbsp;doesn’t&amp;nbsp;have to hinder AI progress.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So says&amp;nbsp;Adam Billen, vice president of public policy at youth-led advocacy group Encode AI,&amp;nbsp;on today’s episode of Equity.&amp;nbsp;&lt;/p&gt;









&lt;p class="wp-block-paragraph"&gt;“The reality is that policy makers themselves know that we have to do something, and they know from working on a million other issues that there is a way to pass legislation that genuinely does protect innovation&amp;nbsp;—&amp;nbsp;which I do care about&amp;nbsp;—&amp;nbsp;while making sure that these products are safe,” Billen told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At its core,&amp;nbsp;SB&amp;nbsp;53 is a&amp;nbsp;first-in-the-nation bill&amp;nbsp;that&amp;nbsp;requires large AI labs to be transparent about their safety and security protocols&amp;nbsp;— specifically around how they prevent their models from catastrophic risks, like being used to commit&amp;nbsp;cyberattacks&amp;nbsp;on critical infrastructure or build&amp;nbsp;bio-weapons.&amp;nbsp;The law also mandates that companies stick to those protocols, which will be enforced by the Office of Emergency Services.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Companies are already doing the stuff that we ask them to do in this bill,” Billen&amp;nbsp;told TechCrunch. “They do safety testing on their models. They release model cards. Are they starting to skimp in some areas at some companies? Yes. And that’s why bills like this are important.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Billen also noted that&amp;nbsp;some AI firms have a policy around relaxing safety standards under competitive pressure. OpenAI, for example, has&amp;nbsp;publicly stated&amp;nbsp;that it&amp;nbsp;may “adjust” its safety requirements&amp;nbsp;if a rival AI lab releases a high-risk system without similar safeguards.&amp;nbsp;Billen argues that policy can enforce companies’ existing safety promises, preventing them from cutting corners under competitive or financial pressure.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While public opposition to SB 53 was muted in&amp;nbsp;comparison to its predecessor SB 1047, which&amp;nbsp;Newsom vetoed last year, the rhetoric in Silicon Valley and among most AI labs has been that almost any&amp;nbsp;AI&amp;nbsp;regulation is anathema to progress and will&amp;nbsp;ultimately hinder&amp;nbsp;the U.S. in its race to beat China.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It’s why companies like Meta, VCs like Andreessen Horowitz, and powerful individuals like OpenAI president Greg Brockman are collectively pumping hundreds of millions into super PACs to back pro-AI politicians in state elections. And&amp;nbsp;it’s&amp;nbsp;why those same forces earlier this year pushed for an&amp;nbsp;AI moratorium&amp;nbsp;that would have banned states from regulating AI for 10 years.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Encode AI ran a coalition of more than 200 organizations to work to&amp;nbsp;strike down the proposal, but Billen says the fight&amp;nbsp;isn’t&amp;nbsp;over. Senator Ted Cruz, who championed the moratorium, is&amp;nbsp;attempting&amp;nbsp;a new strategy to achieve the same goal of federal preemption of state laws. In September, Cruz introduced the&amp;nbsp;SANDBOX Act, which would allow AI companies to apply for waivers to temporarily bypass certain federal regulations for up to 10 years. Billen also anticipates a forthcoming bill establishing a federal AI standard&amp;nbsp;that would be pitched as a middle-ground&amp;nbsp;solution but&amp;nbsp;would in reality&amp;nbsp;override state laws.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He warned that narrowly scoped&amp;nbsp;federal AI legislation could “delete federalism for the most important technology of our time.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“If you told me SB 53 was the bill that would replace all the state bills on everything related to AI and all of the potential risks, I would tell you that’s probably not a very good idea and that this bill is designed for a particular subset of things,” Billen said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3053195" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/adam-billen-headshot.jpg?w=624" width="624" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Adam Billen, vice president of public policy, Encode AI&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Encode AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While he agrees that the AI race with China matters, and that policymakers need to enact regulation that will support American progress, he says killing state bills&amp;nbsp;— which&amp;nbsp;mainly focus on deepfakes, transparency, algorithmic discrimination, children’s safety, and governmental use of AI — isn’t the way to go about doing that.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Are bills like SB 53 the thing that will stop us from beating China? No,” he said. “I think it&amp;nbsp;is just genuinely intellectually dishonest to say that that is the thing that will stop us in the race.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added:&amp;nbsp;“If the thing you care about is beating China in the race on AI — and I do care about that — then the things you would push for are stuff like export controls in Congress,” Billen said. “You would make sure that American companies have the&amp;nbsp;chips. But&amp;nbsp;that’s&amp;nbsp;not what the industry is pushing for.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Legislative proposals like the&amp;nbsp;Chip Security Act&amp;nbsp;aim to prevent the diversion of advanced AI chips to China through export controls and tracking devices, and the existing&amp;nbsp;CHIPS and Science Act&amp;nbsp;seeks&amp;nbsp;to boost domestic chip production. However, some major tech companies, including OpenAI and Nvidia, have expressed reluctance or&amp;nbsp;opposition&amp;nbsp;to certain aspects of these efforts, citing concerns about effectiveness, competitiveness, and security vulnerabilities.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia has its reasons — it has a strong financial incentive to continue selling chips to China, which has historically&amp;nbsp;represented&amp;nbsp;a significant portion&amp;nbsp;of its global revenue.&amp;nbsp;Billen&amp;nbsp;speculated&amp;nbsp;that OpenAI could hold back on chip export advocacy to stay in the good graces of crucial suppliers like Nvidia.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s&amp;nbsp;also been inconsistent messaging from&amp;nbsp;the Trump administration. Three months after expanding an export ban&amp;nbsp;on advanced AI chips to China&amp;nbsp;in April 2025, the administration reversed course, allowing&amp;nbsp;Nvidia and AMD to sell some chips to China in&amp;nbsp;exchange for 15% of the revenue.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You see people on the Hill moving towards bills like the Chip Security Act that would put export controls on China,” Billen said. “In the meantime,&amp;nbsp;there’s going to continue to be this propping up&amp;nbsp;of&amp;nbsp;the narrative to kill state bills that are&amp;nbsp;actually quite&amp;nbsp;light tough.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Billen&amp;nbsp;added&amp;nbsp;that SB 53 is an example of democracy in action — of industry and policymakers working together to get to a version of a bill that everyone can agree on.&amp;nbsp;It’s&amp;nbsp;“very ugly and messy,” but “that process of democracy and federalism is the entire foundation of our country and our economic system, and I hope that we will keep doing that successfully.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think SB 53 is one of the best proof points that that can still work,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was first published on October 1.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;SB 53, the AI&amp;nbsp;safety&amp;nbsp;and transparency&amp;nbsp;bill&amp;nbsp;that California&amp;nbsp;Gov. Gavin Newsom signed into law this week, is proof that state regulation&amp;nbsp;doesn’t&amp;nbsp;have to hinder AI progress.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So says&amp;nbsp;Adam Billen, vice president of public policy at youth-led advocacy group Encode AI,&amp;nbsp;on today’s episode of Equity.&amp;nbsp;&lt;/p&gt;









&lt;p class="wp-block-paragraph"&gt;“The reality is that policy makers themselves know that we have to do something, and they know from working on a million other issues that there is a way to pass legislation that genuinely does protect innovation&amp;nbsp;—&amp;nbsp;which I do care about&amp;nbsp;—&amp;nbsp;while making sure that these products are safe,” Billen told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At its core,&amp;nbsp;SB&amp;nbsp;53 is a&amp;nbsp;first-in-the-nation bill&amp;nbsp;that&amp;nbsp;requires large AI labs to be transparent about their safety and security protocols&amp;nbsp;— specifically around how they prevent their models from catastrophic risks, like being used to commit&amp;nbsp;cyberattacks&amp;nbsp;on critical infrastructure or build&amp;nbsp;bio-weapons.&amp;nbsp;The law also mandates that companies stick to those protocols, which will be enforced by the Office of Emergency Services.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Companies are already doing the stuff that we ask them to do in this bill,” Billen&amp;nbsp;told TechCrunch. “They do safety testing on their models. They release model cards. Are they starting to skimp in some areas at some companies? Yes. And that’s why bills like this are important.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Billen also noted that&amp;nbsp;some AI firms have a policy around relaxing safety standards under competitive pressure. OpenAI, for example, has&amp;nbsp;publicly stated&amp;nbsp;that it&amp;nbsp;may “adjust” its safety requirements&amp;nbsp;if a rival AI lab releases a high-risk system without similar safeguards.&amp;nbsp;Billen argues that policy can enforce companies’ existing safety promises, preventing them from cutting corners under competitive or financial pressure.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While public opposition to SB 53 was muted in&amp;nbsp;comparison to its predecessor SB 1047, which&amp;nbsp;Newsom vetoed last year, the rhetoric in Silicon Valley and among most AI labs has been that almost any&amp;nbsp;AI&amp;nbsp;regulation is anathema to progress and will&amp;nbsp;ultimately hinder&amp;nbsp;the U.S. in its race to beat China.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It’s why companies like Meta, VCs like Andreessen Horowitz, and powerful individuals like OpenAI president Greg Brockman are collectively pumping hundreds of millions into super PACs to back pro-AI politicians in state elections. And&amp;nbsp;it’s&amp;nbsp;why those same forces earlier this year pushed for an&amp;nbsp;AI moratorium&amp;nbsp;that would have banned states from regulating AI for 10 years.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Encode AI ran a coalition of more than 200 organizations to work to&amp;nbsp;strike down the proposal, but Billen says the fight&amp;nbsp;isn’t&amp;nbsp;over. Senator Ted Cruz, who championed the moratorium, is&amp;nbsp;attempting&amp;nbsp;a new strategy to achieve the same goal of federal preemption of state laws. In September, Cruz introduced the&amp;nbsp;SANDBOX Act, which would allow AI companies to apply for waivers to temporarily bypass certain federal regulations for up to 10 years. Billen also anticipates a forthcoming bill establishing a federal AI standard&amp;nbsp;that would be pitched as a middle-ground&amp;nbsp;solution but&amp;nbsp;would in reality&amp;nbsp;override state laws.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He warned that narrowly scoped&amp;nbsp;federal AI legislation could “delete federalism for the most important technology of our time.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“If you told me SB 53 was the bill that would replace all the state bills on everything related to AI and all of the potential risks, I would tell you that’s probably not a very good idea and that this bill is designed for a particular subset of things,” Billen said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3053195" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/adam-billen-headshot.jpg?w=624" width="624" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Adam Billen, vice president of public policy, Encode AI&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Encode AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While he agrees that the AI race with China matters, and that policymakers need to enact regulation that will support American progress, he says killing state bills&amp;nbsp;— which&amp;nbsp;mainly focus on deepfakes, transparency, algorithmic discrimination, children’s safety, and governmental use of AI — isn’t the way to go about doing that.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Are bills like SB 53 the thing that will stop us from beating China? No,” he said. “I think it&amp;nbsp;is just genuinely intellectually dishonest to say that that is the thing that will stop us in the race.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added:&amp;nbsp;“If the thing you care about is beating China in the race on AI — and I do care about that — then the things you would push for are stuff like export controls in Congress,” Billen said. “You would make sure that American companies have the&amp;nbsp;chips. But&amp;nbsp;that’s&amp;nbsp;not what the industry is pushing for.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Legislative proposals like the&amp;nbsp;Chip Security Act&amp;nbsp;aim to prevent the diversion of advanced AI chips to China through export controls and tracking devices, and the existing&amp;nbsp;CHIPS and Science Act&amp;nbsp;seeks&amp;nbsp;to boost domestic chip production. However, some major tech companies, including OpenAI and Nvidia, have expressed reluctance or&amp;nbsp;opposition&amp;nbsp;to certain aspects of these efforts, citing concerns about effectiveness, competitiveness, and security vulnerabilities.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia has its reasons — it has a strong financial incentive to continue selling chips to China, which has historically&amp;nbsp;represented&amp;nbsp;a significant portion&amp;nbsp;of its global revenue.&amp;nbsp;Billen&amp;nbsp;speculated&amp;nbsp;that OpenAI could hold back on chip export advocacy to stay in the good graces of crucial suppliers like Nvidia.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s&amp;nbsp;also been inconsistent messaging from&amp;nbsp;the Trump administration. Three months after expanding an export ban&amp;nbsp;on advanced AI chips to China&amp;nbsp;in April 2025, the administration reversed course, allowing&amp;nbsp;Nvidia and AMD to sell some chips to China in&amp;nbsp;exchange for 15% of the revenue.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You see people on the Hill moving towards bills like the Chip Security Act that would put export controls on China,” Billen said. “In the meantime,&amp;nbsp;there’s going to continue to be this propping up&amp;nbsp;of&amp;nbsp;the narrative to kill state bills that are&amp;nbsp;actually quite&amp;nbsp;light tough.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Billen&amp;nbsp;added&amp;nbsp;that SB 53 is an example of democracy in action — of industry and policymakers working together to get to a version of a bill that everyone can agree on.&amp;nbsp;It’s&amp;nbsp;“very ugly and messy,” but “that process of democracy and federalism is the entire foundation of our country and our economic system, and I hope that we will keep doing that successfully.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think SB 53 is one of the best proof points that that can still work,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was first published on October 1.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/05/californias-new-ai-safety-law-shows-regulation-and-innovation-dont-have-to-clash/</guid><pubDate>Sun, 05 Oct 2025 17:51:48 +0000</pubDate></item></channel></rss>