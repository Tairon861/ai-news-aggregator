<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 22 Oct 2025 01:45:34 +0000</lastBuildDate><item><title>Businesses still face the AI data challenge (AI News)</title><link>https://www.artificialintelligence-news.com/news/businesses-still-face-the-ai-data-challenge/</link><description>&lt;p&gt;A few years ago, the business technology world’s favourite buzzword was ‘Big Data’ – a reference to organisations’ mass collection of information that could be used to suggest previously unexplored ways of operating, and float ideas about what strategies they may best pursue.&lt;/p&gt;&lt;p&gt;What’s becoming increasingly apparent is that the problems companies faced in using Big Data to their advantage still remain, and it’s a new technology – AI – that’s making those problems rise once again to the surface. Without tackling the problems that beset Big Data, AI implementations will continue to fail.&lt;/p&gt;&lt;p&gt;So what are the issues stopping AI deliver on its promises?&lt;/p&gt;&lt;p&gt;The vast majority of problems stem from the data resources themselves. To understand the issue, consider the following sources of information used in a very average working day.&lt;/p&gt;&lt;p&gt;In a small-to-medium sized business:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Spreadsheets, stored on users’ laptops, in Google Sheets, Office 365 cloud.&lt;/li&gt;&lt;li&gt;The customer relationship manager (CRM) platform.&lt;/li&gt;&lt;li&gt;Email exchanges between colleagues, customers, suppliers.&lt;/li&gt;&lt;li&gt;Word documents, PDFs, web forms.&lt;/li&gt;&lt;li&gt;Messaging apps.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In an enterprise business:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;All of the above, plus,&lt;/li&gt;&lt;li&gt;Enterprise resource planning (ERP) systems.&lt;/li&gt;&lt;li&gt;Real-time data feeds.&lt;/li&gt;&lt;li&gt;Data lakes.&lt;/li&gt;&lt;li&gt;Disparate databases behind multiple point-products.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It’s worth noting that the simple list above isn’t comprehensive, and nor is it intended to be. What it demonstrates is that in just five lines, there are around a dozen places where information can be found. What Big Data needed (perhaps still needs) and what AI projects also rest on, is somehow bringing all those elements together in such a way that a computer algorithm can make sense of it.&lt;/p&gt;&lt;p&gt;Marketing behemoth Gartner’s hype cycle for artificial intelligence, 2024, placed AI-Ready Data on the upward curve of the hype cycle, estimating it would be 2-5 years before it reached the ‘plateau of productivity’. Given that AI systems mine and extract data, most organisations – save those of the very largest size – don’t have the foundations on which to build, and may not have AI assistance in the endeavour for another 1-4 years.&lt;/p&gt;&lt;p&gt;The underlying problem for AI implementation is the same as dogged Big Data innovations as they, in the past, made their way through the hype cycle – from innovation trigger, peak of inflated expectations, trough of disillusionment, slope of enlightenment, to plateau of productivity – data comes in many forms; it can be inconsistent; perhaps it adheres to different standards; it may be inaccurate or biased; it could be highly sensitive information, or old and therefore irrelevant.&lt;/p&gt;&lt;p&gt;Transforming data so it’s AI-ready remains a process that’s as relevant today (perhaps more so) than it’s ever been. Those companies wanting to get a jump start could experiment with the many data treatment platforms currently available, and as is becoming the common advice, might begin with discrete projects as test-beds to assess the effectiveness of emerging technologies.&lt;/p&gt;&lt;p&gt;The advantage of the latest data preparation and assembly systems is that they are designed to prepare an organisation’s information resources in ways that are designed for the data to be used by AI value-creation platforms. They can offer, for example, carefully-coded guardrails that will help ensure data compliance, and protect users from accessing biased or commercially-sensitive information.&lt;/p&gt;&lt;p&gt;But the challenge of producing coherent, safe, and well-formulated data resources remains an ongoing issue. As organisations gain more data in their everyday operations, compiling up-to-date data resources on which to draw is a constant process. Where big data could be considered a static asset, data for AI ingestion has to be prepared and treated in as close to real-time as possible.&lt;/p&gt;&lt;p&gt;The situation therefore remains a three-way balance between opportunity, risk, and cost. Never before has the choice of vendor or platform been so crucial to the modern business.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Source: “Inside the business school” by Darien and Neil is licensed under CC BY-NC 2.0.&lt;/em&gt;)&lt;/p&gt;&lt;figure class="wp-block-image aligncenter is-resized"&gt;&lt;img alt="alt" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;A few years ago, the business technology world’s favourite buzzword was ‘Big Data’ – a reference to organisations’ mass collection of information that could be used to suggest previously unexplored ways of operating, and float ideas about what strategies they may best pursue.&lt;/p&gt;&lt;p&gt;What’s becoming increasingly apparent is that the problems companies faced in using Big Data to their advantage still remain, and it’s a new technology – AI – that’s making those problems rise once again to the surface. Without tackling the problems that beset Big Data, AI implementations will continue to fail.&lt;/p&gt;&lt;p&gt;So what are the issues stopping AI deliver on its promises?&lt;/p&gt;&lt;p&gt;The vast majority of problems stem from the data resources themselves. To understand the issue, consider the following sources of information used in a very average working day.&lt;/p&gt;&lt;p&gt;In a small-to-medium sized business:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Spreadsheets, stored on users’ laptops, in Google Sheets, Office 365 cloud.&lt;/li&gt;&lt;li&gt;The customer relationship manager (CRM) platform.&lt;/li&gt;&lt;li&gt;Email exchanges between colleagues, customers, suppliers.&lt;/li&gt;&lt;li&gt;Word documents, PDFs, web forms.&lt;/li&gt;&lt;li&gt;Messaging apps.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In an enterprise business:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;All of the above, plus,&lt;/li&gt;&lt;li&gt;Enterprise resource planning (ERP) systems.&lt;/li&gt;&lt;li&gt;Real-time data feeds.&lt;/li&gt;&lt;li&gt;Data lakes.&lt;/li&gt;&lt;li&gt;Disparate databases behind multiple point-products.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It’s worth noting that the simple list above isn’t comprehensive, and nor is it intended to be. What it demonstrates is that in just five lines, there are around a dozen places where information can be found. What Big Data needed (perhaps still needs) and what AI projects also rest on, is somehow bringing all those elements together in such a way that a computer algorithm can make sense of it.&lt;/p&gt;&lt;p&gt;Marketing behemoth Gartner’s hype cycle for artificial intelligence, 2024, placed AI-Ready Data on the upward curve of the hype cycle, estimating it would be 2-5 years before it reached the ‘plateau of productivity’. Given that AI systems mine and extract data, most organisations – save those of the very largest size – don’t have the foundations on which to build, and may not have AI assistance in the endeavour for another 1-4 years.&lt;/p&gt;&lt;p&gt;The underlying problem for AI implementation is the same as dogged Big Data innovations as they, in the past, made their way through the hype cycle – from innovation trigger, peak of inflated expectations, trough of disillusionment, slope of enlightenment, to plateau of productivity – data comes in many forms; it can be inconsistent; perhaps it adheres to different standards; it may be inaccurate or biased; it could be highly sensitive information, or old and therefore irrelevant.&lt;/p&gt;&lt;p&gt;Transforming data so it’s AI-ready remains a process that’s as relevant today (perhaps more so) than it’s ever been. Those companies wanting to get a jump start could experiment with the many data treatment platforms currently available, and as is becoming the common advice, might begin with discrete projects as test-beds to assess the effectiveness of emerging technologies.&lt;/p&gt;&lt;p&gt;The advantage of the latest data preparation and assembly systems is that they are designed to prepare an organisation’s information resources in ways that are designed for the data to be used by AI value-creation platforms. They can offer, for example, carefully-coded guardrails that will help ensure data compliance, and protect users from accessing biased or commercially-sensitive information.&lt;/p&gt;&lt;p&gt;But the challenge of producing coherent, safe, and well-formulated data resources remains an ongoing issue. As organisations gain more data in their everyday operations, compiling up-to-date data resources on which to draw is a constant process. Where big data could be considered a static asset, data for AI ingestion has to be prepared and treated in as close to real-time as possible.&lt;/p&gt;&lt;p&gt;The situation therefore remains a three-way balance between opportunity, risk, and cost. Never before has the choice of vendor or platform been so crucial to the modern business.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Source: “Inside the business school” by Darien and Neil is licensed under CC BY-NC 2.0.&lt;/em&gt;)&lt;/p&gt;&lt;figure class="wp-block-image aligncenter is-resized"&gt;&lt;img alt="alt" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/businesses-still-face-the-ai-data-challenge/</guid><pubDate>Tue, 21 Oct 2025 13:55:40 +0000</pubDate></item><item><title>How AI adoption is moving IT operations from reactive to proactive (AI News)</title><link>https://www.artificialintelligence-news.com/news/how-ai-adoption-it-operations-reactive-to-proactive/</link><description>&lt;p&gt;CIOs want to fix IT problems faster without expanding headcount, and many see AI adoption as the solution for their operations.&lt;/p&gt;&lt;p&gt;For ages, they’ve used things like automation and self-help portals to handle this, so their teams can solve issues quickly. Now, AI is getting involved, and lots of companies are trying to use it for their IT support. There’s been a ton of buzz, but leaders want proof that it actually works.&lt;/p&gt;&lt;p&gt;SolarWinds looked into how these new tools are doing. The company analysed a bunch of info from over 2,000 IT systems and 60,000 pieces of data collected over a year, from August 2024 to July 2025.&lt;/p&gt;&lt;p&gt;For their study, SolarWinds checked out AI stuff that’s supposed to make things easier, like automatically suggesting answers to tickets, finding helpful articles, and making summaries of problems. The results give a good idea of how much more efficient companies can get.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-much-more-efficient-does-ai-make-it-operations"&gt;How much more efficient does AI make IT operations?&lt;/h3&gt;&lt;p&gt;The main thing the report found is that it takes way less time to fix an IT issue after a company starts using AI.&lt;/p&gt;&lt;p&gt;Before AI, it took about 27.42 hours to solve a problem. After, it dropped to 22.55 hours. That’s about 17.8 percent faster, saving about 4.87 hours per problem. This lets IT teams spend more time on tricky stuff instead of getting bogged down with everyday issues.&lt;/p&gt;&lt;p&gt;This can save companies a bunch of money. The report talks about a medium-sized IT team handling 5,000 problems a year. By saving about 4.87 hours on each one, they’d get back 24,350 hours of work each year. If you figure a help desk person costs $28 an hour, that’s like saving over $680,000.&lt;/p&gt;&lt;p&gt;But it’s not just about saving money. The report says IT can use that time to work on important projects and fix problems before they happen. This helps IT go from just fixing things to actually helping the company do better.&lt;/p&gt;&lt;p&gt;The report also shows that there’s a big difference between companies that use AI and those that don’t. Companies using AI fix tickets in about 22.55 hours, while others take about 32.46 hours. That’s about a 30.5 percent difference which means almost 10 hours saved per problem for those using AI.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-however-it-s-not-just-about-the-ai"&gt;However, it’s not just about the AI&lt;/h3&gt;&lt;p&gt;The report makes it clear that AI isn’t a magic fix for IT operations. It only works if you have good processes and are ready to make broader, company-wide changes.&lt;/p&gt;&lt;p&gt;The best example is a group called the ‘Top 10 AI Adopters’. These ten companies stood out because they cut their resolution times the most. They went from about 51 hours to 23 hours, which is more than half the time saved.&lt;/p&gt;&lt;p&gt;Their secret wasn’t special software, but how they used it. These companies all had one thing in common: they didn’t just try out AI as a side project. They made it a part of their daily work to help fix problems. The report basically says that AI works best when you also make changes to your processes and are willing to improve things.&lt;/p&gt;&lt;p&gt;The report also says that it helps to have a culture that’s already into things like self-service portals and automation. These teams are already trying to make their support desks strong and ready for anything. AI just works better when you have these things in place.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-to-do-if-you-re-in-charge"&gt;What to do if you’re in charge&lt;/h3&gt;&lt;p&gt;The SolarWinds report shows that AI for IT support operations isn’t just a possibility, it can really work. Cutting resolution times by almost 18 percent is a big deal for leaders. Here’s what they should do:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;See where you’re at:&lt;/strong&gt; Before you spend money, figure out how long it takes to fix things now. The report says the average for companies not using AI is about 32.46 hours. Knowing your own number helps you decide if it’s worth it.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Make it part of the job:&lt;/strong&gt; The top users show that it’s better to use AI every day than to just try it out on the side. This means making changes to how you work and focusing on making things better.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;AI is a tool, not a miracle:&lt;/strong&gt; AI can really help speed things up, especially if you already have good IT practices. Check your knowledge base and automation rules. AI works best when you have clear processes in place.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Figure out how much you can save:&lt;/strong&gt; The report gives a simple way to show your team how much time and money AI can save. Just multiply the number of incidents you have each year by the average saving of 4.87 hours. This gives you a clear idea of how much more efficient you can be.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The difference between companies using AI and those who aren’t is growing. Leaders need to set up their operations so they can use AI to help IT become a real partner in the company’s success.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;China’s generative AI user base doubles to 515 million in six months&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-109805" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-1.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;CIOs want to fix IT problems faster without expanding headcount, and many see AI adoption as the solution for their operations.&lt;/p&gt;&lt;p&gt;For ages, they’ve used things like automation and self-help portals to handle this, so their teams can solve issues quickly. Now, AI is getting involved, and lots of companies are trying to use it for their IT support. There’s been a ton of buzz, but leaders want proof that it actually works.&lt;/p&gt;&lt;p&gt;SolarWinds looked into how these new tools are doing. The company analysed a bunch of info from over 2,000 IT systems and 60,000 pieces of data collected over a year, from August 2024 to July 2025.&lt;/p&gt;&lt;p&gt;For their study, SolarWinds checked out AI stuff that’s supposed to make things easier, like automatically suggesting answers to tickets, finding helpful articles, and making summaries of problems. The results give a good idea of how much more efficient companies can get.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-much-more-efficient-does-ai-make-it-operations"&gt;How much more efficient does AI make IT operations?&lt;/h3&gt;&lt;p&gt;The main thing the report found is that it takes way less time to fix an IT issue after a company starts using AI.&lt;/p&gt;&lt;p&gt;Before AI, it took about 27.42 hours to solve a problem. After, it dropped to 22.55 hours. That’s about 17.8 percent faster, saving about 4.87 hours per problem. This lets IT teams spend more time on tricky stuff instead of getting bogged down with everyday issues.&lt;/p&gt;&lt;p&gt;This can save companies a bunch of money. The report talks about a medium-sized IT team handling 5,000 problems a year. By saving about 4.87 hours on each one, they’d get back 24,350 hours of work each year. If you figure a help desk person costs $28 an hour, that’s like saving over $680,000.&lt;/p&gt;&lt;p&gt;But it’s not just about saving money. The report says IT can use that time to work on important projects and fix problems before they happen. This helps IT go from just fixing things to actually helping the company do better.&lt;/p&gt;&lt;p&gt;The report also shows that there’s a big difference between companies that use AI and those that don’t. Companies using AI fix tickets in about 22.55 hours, while others take about 32.46 hours. That’s about a 30.5 percent difference which means almost 10 hours saved per problem for those using AI.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-however-it-s-not-just-about-the-ai"&gt;However, it’s not just about the AI&lt;/h3&gt;&lt;p&gt;The report makes it clear that AI isn’t a magic fix for IT operations. It only works if you have good processes and are ready to make broader, company-wide changes.&lt;/p&gt;&lt;p&gt;The best example is a group called the ‘Top 10 AI Adopters’. These ten companies stood out because they cut their resolution times the most. They went from about 51 hours to 23 hours, which is more than half the time saved.&lt;/p&gt;&lt;p&gt;Their secret wasn’t special software, but how they used it. These companies all had one thing in common: they didn’t just try out AI as a side project. They made it a part of their daily work to help fix problems. The report basically says that AI works best when you also make changes to your processes and are willing to improve things.&lt;/p&gt;&lt;p&gt;The report also says that it helps to have a culture that’s already into things like self-service portals and automation. These teams are already trying to make their support desks strong and ready for anything. AI just works better when you have these things in place.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-to-do-if-you-re-in-charge"&gt;What to do if you’re in charge&lt;/h3&gt;&lt;p&gt;The SolarWinds report shows that AI for IT support operations isn’t just a possibility, it can really work. Cutting resolution times by almost 18 percent is a big deal for leaders. Here’s what they should do:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;See where you’re at:&lt;/strong&gt; Before you spend money, figure out how long it takes to fix things now. The report says the average for companies not using AI is about 32.46 hours. Knowing your own number helps you decide if it’s worth it.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Make it part of the job:&lt;/strong&gt; The top users show that it’s better to use AI every day than to just try it out on the side. This means making changes to how you work and focusing on making things better.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;AI is a tool, not a miracle:&lt;/strong&gt; AI can really help speed things up, especially if you already have good IT practices. Check your knowledge base and automation rules. AI works best when you have clear processes in place.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Figure out how much you can save:&lt;/strong&gt; The report gives a simple way to show your team how much time and money AI can save. Just multiply the number of incidents you have each year by the average saving of 4.87 hours. This gives you a clear idea of how much more efficient you can be.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The difference between companies using AI and those who aren’t is growing. Leaders need to set up their operations so they can use AI to help IT become a real partner in the company’s success.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;China’s generative AI user base doubles to 515 million in six months&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-109805" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-1.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/how-ai-adoption-it-operations-reactive-to-proactive/</guid><pubDate>Tue, 21 Oct 2025 13:59:14 +0000</pubDate></item><item><title>Open Source AI Week — How Developers and Contributors Are Advancing AI Innovation (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/open-source-ai-week/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="nemotron"&gt;&lt;b&gt;Llama‑Embed‑Nemotron‑8B Ranks Among Top Open Models for Multilingual Retrieval 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA’s Llama‑Embed‑Nemotron‑8B model has been recognized as the top open and portable model on the Multilingual Text Embedding Benchmark leaderboard.&lt;/p&gt;
&lt;p&gt;Built on the meta‑llama/Llama‑3.1‑8B architecture, Llama‑Embed‑Nemotron‑8B is a research text embedding model that converts text into 4,096‑dimensional vector representations. Designed for flexibility, it supports a wide range of use cases, including retrieval, reranking, semantic similarity and classification across more than 1,000 languages.&lt;/p&gt;
&lt;p&gt;Trained on a diverse collection of 16 million query–document pairs — half from public sources and half synthetically generated — the model benefits from refined data generation techniques, hard‑negative mining and model‑merging approaches that contribute to its broad generalization capabilities.&lt;/p&gt;
&lt;p&gt;This result builds on NVIDIA’s ongoing research in open, high‑performing AI models. Following earlier leaderboard recognition for the Llama NeMo Retriever ColEmbed model, the success of Llama‑Embed‑Nemotron‑8B highlights the value of openness, transparency and collaboration in advancing AI for the developer community.&lt;/p&gt;
&lt;p&gt;Check out Llama-Embed-Nemotron-8B on Hugging Face, and learn more about the model, including architectural highlights, training methodology and performance evaluation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-86053" height="470" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ezgif.com-video-to-gif-converter-7.gif" width="800" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Open Source Teaches Us About Making AI Better&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open models are shaping the future of AI, enabling developers, enterprises and governments to innovate with transparency, customization and trust. In the latest episode of the NVIDIA AI Podcast, NVIDIA’s Bryan Catanzaro and Jonathan Cohen discuss how open models, datasets and research are laying the foundation for shared progress across the AI ecosystem.&lt;/p&gt;
&lt;p&gt;The NVIDIA Nemotron family of open models represents a full-stack approach to AI development, connecting model design to the underlying hardware and software that power it. By releasing Nemotron models, data and training methodologies openly, NVIDIA aims to help others refine, adapt and build upon its work, resulting in a faster exchange of ideas and more efficient systems.&lt;/p&gt;
&lt;p&gt;“When we as a community come together — contributing ideas, data and models — we all move faster,” said Catanzaro in the episode. “Open technologies make that possible.”&lt;/p&gt;

&lt;p&gt;There’s more happening this week at Open Source AI Week, including the start of the PyTorch Conference — bringing together developers, researchers and innovators pushing the boundaries of open AI.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Attendees can tune in to the special keynote address by Jim Fan, director of robotics and distinguished research scientist at NVIDIA, to hear the latest advancements in robotics — from simulation and synthetic data to accelerated computing. The keynote, titled “The Physical Turing Test: Solving General Purpose Robotics,” will take place on Wednesday, Oct. 22, from 9:50-10:05 a.m. PT.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;b&gt;Andrej Karpathy’s Nanochat Teaches Developers How to Train LLMs in Four Hours 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Computer scientist Andrej Karpathy recently introduced Nanochat, calling it “the best ChatGPT that $100 can buy.” Nanochat is an open-source, full-stack large language model (LLM) implementation built for transparency and experimentation. In about 8,000 lines of minimal, dependency-light code, Nanochat runs the entire LLM pipeline — from tokenization and pretraining to fine-tuning, inference and chat — all through a simple web user interface.&lt;br /&gt;NVIDIA is supporting Karpathy’s open-source Nanochat project by releasing two NVIDIA Launchables, making it easy to deploy and experiment with Nanochat across various NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;With NVIDIA Launchables, developers can train and interact with their own conversational model in hours with a single click. The Launchables dynamically support different-sized GPUs — including NVIDIA H100 and L40S GPUs — on various clouds without need for modification. They also automatically work on any eight-GPU instance on NVIDIA Brev, so developers can get compute access immediately.&lt;/p&gt;
&lt;p&gt;The &lt;b&gt;first 10 users&lt;/b&gt; to deploy these Launchables will also receive free compute access to NVIDIA H100 or L40S GPUs.&lt;/p&gt;
&lt;p&gt;Start training with Nanochat by deploying a Launchable:&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86022 size-medium" height="527" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/launchable-960x527.png" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Andrej Karpathy’s Next Experiments Begin With NVIDIA DGX Spark&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today, Karpathy received an NVIDIA DGX Spark — the world’s smallest AI supercomputer, designed to bring the power of Blackwell right to a developer’s desktop. With up to a petaflop of AI processing power and 128GB of unified memory in a compact form factor, DGX Spark empowers innovators like Karpathy to experiment, fine-tune and run massive models locally.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="pytorch"&gt;&lt;b&gt;Building the Future of AI With PyTorch and NVIDIA 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;PyTorch, the fastest-growing AI framework, derives its performance from the NVIDIA CUDA platform and uses the Python programming language to unlock developer productivity. This year, NVIDIA added Python as a first-class language to the CUDA platform, giving the PyTorch developer community greater access to CUDA.&lt;/p&gt;
&lt;p&gt;CUDA Python includes key components that make GPU acceleration in Python easier than ever, with built-in support for kernel fusion, extension module integration and simplified packaging for fast deployment.&lt;/p&gt;
&lt;p&gt;Following PyTorch’s open collaboration model, CUDA Python is available on GitHub and PyPI.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86034"&gt;&lt;img alt="alt" class="wp-image-86034 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/pytorch-infographic-1-1680x672.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86034"&gt;According to PyPI Stats, PyTorch averaged over two million daily downloads, peaking at 2,303,217 on October 14, and had 65 million total downloads last month.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Every month, developers worldwide download hundreds of millions of NVIDIA libraries — including CUDA, cuDNN, cuBLAS and CUTLASS — mostly within Python and PyTorch environments. CUDA Python provides nvmath-python, a new library that acts as the bridge between Python code and these highly optimized GPU libraries.&lt;/p&gt;
&lt;p&gt;Plus, kernel enhancements and support for next-generation frameworks make NVIDIA accelerated computing more efficient, adaptable and widely accessible.&lt;/p&gt;
&lt;p&gt;NVIDIA maintains a long-standing collaboration with the PyTorch community through open-source contributions and technical leadership, as well as by sponsoring and participating in community events and activations.&lt;/p&gt;
&lt;p&gt;At PyTorch Conference 2025 in San Francisco, NVIDIA will host a keynote address, five technical sessions and nine poster presentations.&lt;/p&gt;
&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="open-source-ai"&gt;&lt;b&gt;NVIDIA Spotlights Open Source Innovation 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open Source AI Week kicks off on Monday with a series of hackathons, workshops and meetups spotlighting the latest advances in AI, machine learning and open-source innovation.&lt;/p&gt;
&lt;p&gt;The event brings together leading organizations, researchers and open-source communities to share knowledge, collaborate on tools and explore how openness accelerates AI development.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to expand access to advanced AI innovation by providing open-source tools, models and datasets designed to empower developers. With more than 1,000 open-source tools on NVIDIA GitHub repositories and over 500 models and 100 datasets on the NVIDIA Hugging Face collections, NVIDIA is accelerating the pace of open, collaborative AI development.&lt;/p&gt;
&lt;p&gt;Over the past year, NVIDIA has become the top contributor in Hugging Face repositories, reflecting a deep commitment to sharing models, frameworks and research that empower the community.&lt;/p&gt;

&lt;p&gt;Openly available models, tools and datasets are essential to driving innovation and progress. By empowering anyone to use, modify and share technology, it fosters transparency and accelerates discovery, fueling breakthroughs that benefit both industry and communities alike. That’s why NVIDIA is committed to supporting the open source ecosystem.&lt;/p&gt;
&lt;p&gt;We’re on the ground all week — stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward, with the PyTorch Conference serving as the flagship event.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="nemotron"&gt;&lt;b&gt;Llama‑Embed‑Nemotron‑8B Ranks Among Top Open Models for Multilingual Retrieval 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA’s Llama‑Embed‑Nemotron‑8B model has been recognized as the top open and portable model on the Multilingual Text Embedding Benchmark leaderboard.&lt;/p&gt;
&lt;p&gt;Built on the meta‑llama/Llama‑3.1‑8B architecture, Llama‑Embed‑Nemotron‑8B is a research text embedding model that converts text into 4,096‑dimensional vector representations. Designed for flexibility, it supports a wide range of use cases, including retrieval, reranking, semantic similarity and classification across more than 1,000 languages.&lt;/p&gt;
&lt;p&gt;Trained on a diverse collection of 16 million query–document pairs — half from public sources and half synthetically generated — the model benefits from refined data generation techniques, hard‑negative mining and model‑merging approaches that contribute to its broad generalization capabilities.&lt;/p&gt;
&lt;p&gt;This result builds on NVIDIA’s ongoing research in open, high‑performing AI models. Following earlier leaderboard recognition for the Llama NeMo Retriever ColEmbed model, the success of Llama‑Embed‑Nemotron‑8B highlights the value of openness, transparency and collaboration in advancing AI for the developer community.&lt;/p&gt;
&lt;p&gt;Check out Llama-Embed-Nemotron-8B on Hugging Face, and learn more about the model, including architectural highlights, training methodology and performance evaluation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-86053" height="470" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/ezgif.com-video-to-gif-converter-7.gif" width="800" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Open Source Teaches Us About Making AI Better&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;b&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open models are shaping the future of AI, enabling developers, enterprises and governments to innovate with transparency, customization and trust. In the latest episode of the NVIDIA AI Podcast, NVIDIA’s Bryan Catanzaro and Jonathan Cohen discuss how open models, datasets and research are laying the foundation for shared progress across the AI ecosystem.&lt;/p&gt;
&lt;p&gt;The NVIDIA Nemotron family of open models represents a full-stack approach to AI development, connecting model design to the underlying hardware and software that power it. By releasing Nemotron models, data and training methodologies openly, NVIDIA aims to help others refine, adapt and build upon its work, resulting in a faster exchange of ideas and more efficient systems.&lt;/p&gt;
&lt;p&gt;“When we as a community come together — contributing ideas, data and models — we all move faster,” said Catanzaro in the episode. “Open technologies make that possible.”&lt;/p&gt;

&lt;p&gt;There’s more happening this week at Open Source AI Week, including the start of the PyTorch Conference — bringing together developers, researchers and innovators pushing the boundaries of open AI.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Attendees can tune in to the special keynote address by Jim Fan, director of robotics and distinguished research scientist at NVIDIA, to hear the latest advancements in robotics — from simulation and synthetic data to accelerated computing. The keynote, titled “The Physical Turing Test: Solving General Purpose Robotics,” will take place on Wednesday, Oct. 22, from 9:50-10:05 a.m. PT.&lt;/p&gt;
&lt;h2 class="wp-block-heading"&gt;&lt;b&gt;Andrej Karpathy’s Nanochat Teaches Developers How to Train LLMs in Four Hours 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Computer scientist Andrej Karpathy recently introduced Nanochat, calling it “the best ChatGPT that $100 can buy.” Nanochat is an open-source, full-stack large language model (LLM) implementation built for transparency and experimentation. In about 8,000 lines of minimal, dependency-light code, Nanochat runs the entire LLM pipeline — from tokenization and pretraining to fine-tuning, inference and chat — all through a simple web user interface.&lt;br /&gt;NVIDIA is supporting Karpathy’s open-source Nanochat project by releasing two NVIDIA Launchables, making it easy to deploy and experiment with Nanochat across various NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;With NVIDIA Launchables, developers can train and interact with their own conversational model in hours with a single click. The Launchables dynamically support different-sized GPUs — including NVIDIA H100 and L40S GPUs — on various clouds without need for modification. They also automatically work on any eight-GPU instance on NVIDIA Brev, so developers can get compute access immediately.&lt;/p&gt;
&lt;p&gt;The &lt;b&gt;first 10 users&lt;/b&gt; to deploy these Launchables will also receive free compute access to NVIDIA H100 or L40S GPUs.&lt;/p&gt;
&lt;p&gt;Start training with Nanochat by deploying a Launchable:&lt;/p&gt;

&lt;p&gt;&lt;img alt="alt" class="aligncenter wp-image-86022 size-medium" height="527" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/launchable-960x527.png" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Andrej Karpathy’s Next Experiments Begin With NVIDIA DGX Spark&lt;/b&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today, Karpathy received an NVIDIA DGX Spark — the world’s smallest AI supercomputer, designed to bring the power of Blackwell right to a developer’s desktop. With up to a petaflop of AI processing power and 128GB of unified memory in a compact form factor, DGX Spark empowers innovators like Karpathy to experiment, fine-tune and run massive models locally.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="pytorch"&gt;&lt;b&gt;Building the Future of AI With PyTorch and NVIDIA 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;PyTorch, the fastest-growing AI framework, derives its performance from the NVIDIA CUDA platform and uses the Python programming language to unlock developer productivity. This year, NVIDIA added Python as a first-class language to the CUDA platform, giving the PyTorch developer community greater access to CUDA.&lt;/p&gt;
&lt;p&gt;CUDA Python includes key components that make GPU acceleration in Python easier than ever, with built-in support for kernel fusion, extension module integration and simplified packaging for fast deployment.&lt;/p&gt;
&lt;p&gt;Following PyTorch’s open collaboration model, CUDA Python is available on GitHub and PyPI.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86034"&gt;&lt;img alt="alt" class="wp-image-86034 size-large" height="672" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/pytorch-infographic-1-1680x672.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86034"&gt;According to PyPI Stats, PyTorch averaged over two million daily downloads, peaking at 2,303,217 on October 14, and had 65 million total downloads last month.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Every month, developers worldwide download hundreds of millions of NVIDIA libraries — including CUDA, cuDNN, cuBLAS and CUTLASS — mostly within Python and PyTorch environments. CUDA Python provides nvmath-python, a new library that acts as the bridge between Python code and these highly optimized GPU libraries.&lt;/p&gt;
&lt;p&gt;Plus, kernel enhancements and support for next-generation frameworks make NVIDIA accelerated computing more efficient, adaptable and widely accessible.&lt;/p&gt;
&lt;p&gt;NVIDIA maintains a long-standing collaboration with the PyTorch community through open-source contributions and technical leadership, as well as by sponsoring and participating in community events and activations.&lt;/p&gt;
&lt;p&gt;At PyTorch Conference 2025 in San Francisco, NVIDIA will host a keynote address, five technical sessions and nine poster presentations.&lt;/p&gt;
&lt;p&gt;NVIDIA’s on the ground at Open Source AI Week. Stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward. Follow NVIDIA AI Developer on social channels for additional news and insights.&lt;/p&gt;
&lt;h2 class="wp-block-heading" id="open-source-ai"&gt;&lt;b&gt;NVIDIA Spotlights Open Source Innovation 🔗&lt;br /&gt;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Open Source AI Week kicks off on Monday with a series of hackathons, workshops and meetups spotlighting the latest advances in AI, machine learning and open-source innovation.&lt;/p&gt;
&lt;p&gt;The event brings together leading organizations, researchers and open-source communities to share knowledge, collaborate on tools and explore how openness accelerates AI development.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to expand access to advanced AI innovation by providing open-source tools, models and datasets designed to empower developers. With more than 1,000 open-source tools on NVIDIA GitHub repositories and over 500 models and 100 datasets on the NVIDIA Hugging Face collections, NVIDIA is accelerating the pace of open, collaborative AI development.&lt;/p&gt;
&lt;p&gt;Over the past year, NVIDIA has become the top contributor in Hugging Face repositories, reflecting a deep commitment to sharing models, frameworks and research that empower the community.&lt;/p&gt;

&lt;p&gt;Openly available models, tools and datasets are essential to driving innovation and progress. By empowering anyone to use, modify and share technology, it fosters transparency and accelerates discovery, fueling breakthroughs that benefit both industry and communities alike. That’s why NVIDIA is committed to supporting the open source ecosystem.&lt;/p&gt;
&lt;p&gt;We’re on the ground all week — stay tuned for a celebration highlighting the spirit of innovation, collaboration and community that drives open-source AI forward, with the PyTorch Conference serving as the flagship event.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/open-source-ai-week/</guid><pubDate>Tue, 21 Oct 2025 15:00:33 +0000</pubDate></item><item><title>Tell me when: Building agents that can wait, monitor, and act (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/</link><description>&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Workflow icons showing tasks, thinking, and time, linked to a person symbol on a gradient background." class="wp-image-1152522" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-1024x576.jpg" width="1024" /&gt;&lt;/figure&gt;



&lt;p&gt;Modern&amp;nbsp;LLM&amp;nbsp;Agents&amp;nbsp;can debug code, analyze spreadsheets, and book complex travel.&amp;nbsp;Given those capabilities, it’s reasonable to assume that they could handle something simpler:&amp;nbsp;waiting.&amp;nbsp;Ask an agent to&amp;nbsp;monitor&amp;nbsp;your email for a colleague’s response or watch for a price drop over several days, and it will fail. Not because it&amp;nbsp;can’t&amp;nbsp;check email or scrape prices. It can do both. It fails&amp;nbsp;because it&amp;nbsp;doesn’t&amp;nbsp;know&amp;nbsp;&lt;em&gt;when&lt;/em&gt;&amp;nbsp;to check.&amp;nbsp;Agents either&amp;nbsp;give up after a few attempts or burn through their context window, checking obsessively. Neither&amp;nbsp;work.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This matters because monitoring tasks&amp;nbsp;are&amp;nbsp;everywhere. We track emails for specific information, watch news&amp;nbsp;feeds for updates, and&amp;nbsp;monitor&amp;nbsp;prices for sales. Automating these tasks would save hours, but current&amp;nbsp;agents&amp;nbsp;aren’t&amp;nbsp;built for patience.&lt;/p&gt;



&lt;p&gt;To address this, we are introducing&amp;nbsp;SentinelStep&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;a&amp;nbsp;mechanism&amp;nbsp;that&amp;nbsp;enables&amp;nbsp;agents&amp;nbsp;to complete long-running monitoring&amp;nbsp;tasks.&amp;nbsp;The&amp;nbsp;approach is simple.&amp;nbsp;SentinelStep&amp;nbsp;wraps the agent in a workflow with dynamic&amp;nbsp;polling&amp;nbsp;and&amp;nbsp;careful context&amp;nbsp;management.&amp;nbsp;This&amp;nbsp;enables&amp;nbsp;the&amp;nbsp;agent&amp;nbsp;to&amp;nbsp;monitor&amp;nbsp;conditions for&amp;nbsp;hours&amp;nbsp;or&amp;nbsp;days&amp;nbsp;without getting&amp;nbsp;sidetracked.&amp;nbsp;We’ve&amp;nbsp;implemented&amp;nbsp;SentinelStep&amp;nbsp;in&amp;nbsp;Magentic-UI,&amp;nbsp;our research&amp;nbsp;prototype&amp;nbsp;agentic system,&amp;nbsp;to enable&amp;nbsp;users&amp;nbsp;to&amp;nbsp;build agents for&amp;nbsp;long-running&amp;nbsp;tasks,&amp;nbsp;whether they&amp;nbsp;involve web&amp;nbsp;browsing, coding, or external&amp;nbsp;tools.&amp;nbsp;&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="how-it-works"&gt;How it works&lt;/h2&gt;



&lt;p&gt;The core&amp;nbsp;challenge is&amp;nbsp;polling frequency. Poll too often,&amp;nbsp;and&amp;nbsp;tokens get&amp;nbsp;wasted. Poll too infrequently, and the user’s notification gets delayed.&amp;nbsp;SentinelStep&amp;nbsp;makes&amp;nbsp;an educated guess&amp;nbsp;at&amp;nbsp;the&amp;nbsp;polling interval based on the task at hand—checking email gets different treatment&amp;nbsp;than&amp;nbsp;monitoring&amp;nbsp;quarterly earnings—then dynamically adjusts&amp;nbsp;based on&amp;nbsp;observed&amp;nbsp;behavior.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;There’s&amp;nbsp;a second challenge: context overflow.&amp;nbsp;Because monitoring tasks can run for days,&amp;nbsp;context overflow&amp;nbsp;becomes inevitable.&amp;nbsp;SentinelStep&amp;nbsp;handles&amp;nbsp;this by saving the agent state after the first check, then&amp;nbsp;using&amp;nbsp;that state for each subsequent check.&lt;/p&gt;







&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;These demonstrations capture&amp;nbsp;Magentic-UI with&amp;nbsp;SentinelStep&amp;nbsp;at work, completing a range of tasks in a timelapse sequence.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="core-components"&gt;Core components&lt;/h3&gt;



&lt;p&gt;As&amp;nbsp;the name&amp;nbsp;suggests,&amp;nbsp;SentinelStep&amp;nbsp;consists of&amp;nbsp;individual steps&amp;nbsp;taken as part of&amp;nbsp;an&amp;nbsp;agent’s broader&amp;nbsp;workflow.&amp;nbsp;As illustrated in Figure 1, there are three main components:&amp;nbsp;the&amp;nbsp;actions necessary to collect information, the condition that determines&amp;nbsp;when&amp;nbsp;the task&amp;nbsp;is complete,&amp;nbsp;and the polling interval&amp;nbsp;that&amp;nbsp;determines&amp;nbsp;timing.&amp;nbsp;Once&amp;nbsp;these components&amp;nbsp;are&amp;nbsp;identified, the&amp;nbsp;system’s&amp;nbsp;behavior is simple:&amp;nbsp;every&lt;em&gt;&amp;nbsp;[polling interval]&amp;nbsp;&lt;/em&gt;do&lt;em&gt;&amp;nbsp;[actions]&amp;nbsp;&lt;/em&gt;until&lt;em&gt;&amp;nbsp;[condition]&amp;nbsp;&lt;/em&gt;is satisfied&lt;em&gt;.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure&amp;nbsp;1.&amp;nbsp;SentinelSteps’s&amp;nbsp;three main components&amp;nbsp;in&amp;nbsp;Magentic-UI’s&amp;nbsp;co-planning interface.&amp;nbsp;" class="wp-image-1152610" height="250" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-1-Sentinel-UI.png" width="704" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;1.&amp;nbsp;SentinelSteps’s&amp;nbsp;three main components&amp;nbsp;in&amp;nbsp;Magentic-UI’s&amp;nbsp;co-planning interface.&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;These three components are defined and exposed in the co-planning interface of Magentic-UI. Given a user prompt, Magentic-UI proposes a complete multi-step plan, including pre-filled parameters for any monitoring steps. Users can accept the plan or adjust as needed.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="processing"&gt;Processing&lt;/h3&gt;



&lt;p&gt;Once a run starts, Magentic-UI assigns the most appropriate agent from a team of agents to perform each action. This team includes agents capable of web surfing, code execution, and calling arbitrary MCP servers.&lt;/p&gt;



&lt;p&gt;When the workflow reaches a monitoring step, the flow is straightforward. The assigned agent collects the necessary information through the actions described in the plan. The Magentic-UI orchestrator then checks whether the condition is satisfied. If it is, the SentinelStep is complete, and the orchestrator moves to the next step. If not, the orchestrator determines the timestamp for the next check and resets the agent’s state to prevent context overflow.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="evaluation"&gt;Evaluation&lt;/h2&gt;



&lt;p&gt;Evaluating&amp;nbsp;monitoring tasks in real-world settings&amp;nbsp;is&amp;nbsp;nearly impossible.&amp;nbsp;Consider a simple example: monitoring the Magentic-UI repository on GitHub&amp;nbsp;until&amp;nbsp;it reaches&amp;nbsp;10,000 stars&amp;nbsp;(a measure of how many people have bookmarked it). That event occurs only once and can’t be repeated.&amp;nbsp;Most&amp;nbsp;real-world monitoring tasks share this limitation, making systematic bench marking very challenging.&lt;/p&gt;



&lt;p&gt;In response, we&amp;nbsp;are developing&amp;nbsp;SentinelBench, a suite of synthetic&amp;nbsp;web environments for evaluating monitoring tasks. These environments make experiments repeatable. SentinelBench&amp;nbsp;currently&amp;nbsp;supports&amp;nbsp;28&amp;nbsp;configurable scenarios, each&amp;nbsp;allowing the user to schedule exactly when&amp;nbsp;a&amp;nbsp;target&amp;nbsp;event&amp;nbsp;should&amp;nbsp;occur. It includes setups like GitHub Watcher, which&amp;nbsp;simulates a repository accumulating stars over time;&amp;nbsp;Teams Monitor, which models incoming messages, some&amp;nbsp;urgent; and&amp;nbsp;Flight Monitor, which&amp;nbsp;replicates&amp;nbsp;evolving&amp;nbsp;flight-availability&amp;nbsp;dynamics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Initial&amp;nbsp;tests&amp;nbsp;show clear benefits.&amp;nbsp;As shown in&amp;nbsp;Figure&amp;nbsp;2, success rates&amp;nbsp;remain&amp;nbsp;high for short tasks (30&amp;nbsp;sec&amp;nbsp;and 1&amp;nbsp;min) regardless of&amp;nbsp;whether&amp;nbsp;SentinelStep&amp;nbsp;is&amp;nbsp;used.&amp;nbsp;For longer tasks,&amp;nbsp;SentinelStep&amp;nbsp;markedly&amp;nbsp;improves reliability: at 1 hour, task reliability rises from 5.6% without&amp;nbsp;SentinelStep&amp;nbsp;to&amp;nbsp;33.3% with&amp;nbsp;it;&amp;nbsp;and at 2 hours,&amp;nbsp;it rises&amp;nbsp;from 5.6% to 38.9%. These gains&amp;nbsp;demonstrate&amp;nbsp;that&amp;nbsp;SentinelStep&amp;nbsp;effectively addresses the challenge of maintaining performance over extended durations.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. SentinelStep improves success rates on longer running tasks (1–2 hours) while maintaining comparable performance on shorter tasks.  " class="wp-image-1152612" height="399" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-2-Eval.png" width="619" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;2.&amp;nbsp;SentinelStep&amp;nbsp;improves&amp;nbsp;success rates&amp;nbsp;on longer running tasks (1–2&amp;nbsp;hours)&amp;nbsp;while&amp;nbsp;maintaining&amp;nbsp;comparable performance&amp;nbsp;on shorter tasks.&amp;nbsp;&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="impact-and-availability"&gt;Impact and availability&lt;/h2&gt;



&lt;p&gt;SentinelStep is a first step toward practical, proactive, longer‑running agents. By embedding patience into plans, agents can responsibly monitor conditions and act when it matters—staying proactive without wasting resources. This lays the groundwork for always‑on assistants that stay efficient, respectful of limits, and aligned with user intent.&lt;/p&gt;



&lt;p&gt;We’ve open-sourced SentinelStep as part of Magentic-UI, available on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; or via &lt;code&gt;pip install magnetic-ui&lt;/code&gt;. As with any new technique, production deployment should be preceded through&amp;nbsp;testing and validation&amp;nbsp;for the specific use case.&amp;nbsp;For&amp;nbsp;guidance on&amp;nbsp;intended use,&amp;nbsp;privacy&amp;nbsp;considerations,&amp;nbsp;and safety&amp;nbsp;guidelines,&amp;nbsp;see&amp;nbsp;the&amp;nbsp;Magentic-UI&amp;nbsp;Transparency&amp;nbsp;Note.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Our goal is to&amp;nbsp;make it easier to implement agents that can&amp;nbsp;handle&amp;nbsp;long-running&amp;nbsp;monitoring&amp;nbsp;tasks&amp;nbsp;and&amp;nbsp;lay&amp;nbsp;the groundwork for&amp;nbsp;systems that&amp;nbsp;anticipate, adapt, and&amp;nbsp;evolve&amp;nbsp;to meet real-world needs.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Workflow icons showing tasks, thinking, and time, linked to a person symbol on a gradient background." class="wp-image-1152522" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-1024x576.jpg" width="1024" /&gt;&lt;/figure&gt;



&lt;p&gt;Modern&amp;nbsp;LLM&amp;nbsp;Agents&amp;nbsp;can debug code, analyze spreadsheets, and book complex travel.&amp;nbsp;Given those capabilities, it’s reasonable to assume that they could handle something simpler:&amp;nbsp;waiting.&amp;nbsp;Ask an agent to&amp;nbsp;monitor&amp;nbsp;your email for a colleague’s response or watch for a price drop over several days, and it will fail. Not because it&amp;nbsp;can’t&amp;nbsp;check email or scrape prices. It can do both. It fails&amp;nbsp;because it&amp;nbsp;doesn’t&amp;nbsp;know&amp;nbsp;&lt;em&gt;when&lt;/em&gt;&amp;nbsp;to check.&amp;nbsp;Agents either&amp;nbsp;give up after a few attempts or burn through their context window, checking obsessively. Neither&amp;nbsp;work.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This matters because monitoring tasks&amp;nbsp;are&amp;nbsp;everywhere. We track emails for specific information, watch news&amp;nbsp;feeds for updates, and&amp;nbsp;monitor&amp;nbsp;prices for sales. Automating these tasks would save hours, but current&amp;nbsp;agents&amp;nbsp;aren’t&amp;nbsp;built for patience.&lt;/p&gt;



&lt;p&gt;To address this, we are introducing&amp;nbsp;SentinelStep&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,&amp;nbsp;a&amp;nbsp;mechanism&amp;nbsp;that&amp;nbsp;enables&amp;nbsp;agents&amp;nbsp;to complete long-running monitoring&amp;nbsp;tasks.&amp;nbsp;The&amp;nbsp;approach is simple.&amp;nbsp;SentinelStep&amp;nbsp;wraps the agent in a workflow with dynamic&amp;nbsp;polling&amp;nbsp;and&amp;nbsp;careful context&amp;nbsp;management.&amp;nbsp;This&amp;nbsp;enables&amp;nbsp;the&amp;nbsp;agent&amp;nbsp;to&amp;nbsp;monitor&amp;nbsp;conditions for&amp;nbsp;hours&amp;nbsp;or&amp;nbsp;days&amp;nbsp;without getting&amp;nbsp;sidetracked.&amp;nbsp;We’ve&amp;nbsp;implemented&amp;nbsp;SentinelStep&amp;nbsp;in&amp;nbsp;Magentic-UI,&amp;nbsp;our research&amp;nbsp;prototype&amp;nbsp;agentic system,&amp;nbsp;to enable&amp;nbsp;users&amp;nbsp;to&amp;nbsp;build agents for&amp;nbsp;long-running&amp;nbsp;tasks,&amp;nbsp;whether they&amp;nbsp;involve web&amp;nbsp;browsing, coding, or external&amp;nbsp;tools.&amp;nbsp;&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="how-it-works"&gt;How it works&lt;/h2&gt;



&lt;p&gt;The core&amp;nbsp;challenge is&amp;nbsp;polling frequency. Poll too often,&amp;nbsp;and&amp;nbsp;tokens get&amp;nbsp;wasted. Poll too infrequently, and the user’s notification gets delayed.&amp;nbsp;SentinelStep&amp;nbsp;makes&amp;nbsp;an educated guess&amp;nbsp;at&amp;nbsp;the&amp;nbsp;polling interval based on the task at hand—checking email gets different treatment&amp;nbsp;than&amp;nbsp;monitoring&amp;nbsp;quarterly earnings—then dynamically adjusts&amp;nbsp;based on&amp;nbsp;observed&amp;nbsp;behavior.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;There’s&amp;nbsp;a second challenge: context overflow.&amp;nbsp;Because monitoring tasks can run for days,&amp;nbsp;context overflow&amp;nbsp;becomes inevitable.&amp;nbsp;SentinelStep&amp;nbsp;handles&amp;nbsp;this by saving the agent state after the first check, then&amp;nbsp;using&amp;nbsp;that state for each subsequent check.&lt;/p&gt;







&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;These demonstrations capture&amp;nbsp;Magentic-UI with&amp;nbsp;SentinelStep&amp;nbsp;at work, completing a range of tasks in a timelapse sequence.&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h3 class="wp-block-heading" id="core-components"&gt;Core components&lt;/h3&gt;



&lt;p&gt;As&amp;nbsp;the name&amp;nbsp;suggests,&amp;nbsp;SentinelStep&amp;nbsp;consists of&amp;nbsp;individual steps&amp;nbsp;taken as part of&amp;nbsp;an&amp;nbsp;agent’s broader&amp;nbsp;workflow.&amp;nbsp;As illustrated in Figure 1, there are three main components:&amp;nbsp;the&amp;nbsp;actions necessary to collect information, the condition that determines&amp;nbsp;when&amp;nbsp;the task&amp;nbsp;is complete,&amp;nbsp;and the polling interval&amp;nbsp;that&amp;nbsp;determines&amp;nbsp;timing.&amp;nbsp;Once&amp;nbsp;these components&amp;nbsp;are&amp;nbsp;identified, the&amp;nbsp;system’s&amp;nbsp;behavior is simple:&amp;nbsp;every&lt;em&gt;&amp;nbsp;[polling interval]&amp;nbsp;&lt;/em&gt;do&lt;em&gt;&amp;nbsp;[actions]&amp;nbsp;&lt;/em&gt;until&lt;em&gt;&amp;nbsp;[condition]&amp;nbsp;&lt;/em&gt;is satisfied&lt;em&gt;.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure&amp;nbsp;1.&amp;nbsp;SentinelSteps’s&amp;nbsp;three main components&amp;nbsp;in&amp;nbsp;Magentic-UI’s&amp;nbsp;co-planning interface.&amp;nbsp;" class="wp-image-1152610" height="250" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-1-Sentinel-UI.png" width="704" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;1.&amp;nbsp;SentinelSteps’s&amp;nbsp;three main components&amp;nbsp;in&amp;nbsp;Magentic-UI’s&amp;nbsp;co-planning interface.&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;These three components are defined and exposed in the co-planning interface of Magentic-UI. Given a user prompt, Magentic-UI proposes a complete multi-step plan, including pre-filled parameters for any monitoring steps. Users can accept the plan or adjust as needed.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="processing"&gt;Processing&lt;/h3&gt;



&lt;p&gt;Once a run starts, Magentic-UI assigns the most appropriate agent from a team of agents to perform each action. This team includes agents capable of web surfing, code execution, and calling arbitrary MCP servers.&lt;/p&gt;



&lt;p&gt;When the workflow reaches a monitoring step, the flow is straightforward. The assigned agent collects the necessary information through the actions described in the plan. The Magentic-UI orchestrator then checks whether the condition is satisfied. If it is, the SentinelStep is complete, and the orchestrator moves to the next step. If not, the orchestrator determines the timestamp for the next check and resets the agent’s state to prevent context overflow.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="evaluation"&gt;Evaluation&lt;/h2&gt;



&lt;p&gt;Evaluating&amp;nbsp;monitoring tasks in real-world settings&amp;nbsp;is&amp;nbsp;nearly impossible.&amp;nbsp;Consider a simple example: monitoring the Magentic-UI repository on GitHub&amp;nbsp;until&amp;nbsp;it reaches&amp;nbsp;10,000 stars&amp;nbsp;(a measure of how many people have bookmarked it). That event occurs only once and can’t be repeated.&amp;nbsp;Most&amp;nbsp;real-world monitoring tasks share this limitation, making systematic bench marking very challenging.&lt;/p&gt;



&lt;p&gt;In response, we&amp;nbsp;are developing&amp;nbsp;SentinelBench, a suite of synthetic&amp;nbsp;web environments for evaluating monitoring tasks. These environments make experiments repeatable. SentinelBench&amp;nbsp;currently&amp;nbsp;supports&amp;nbsp;28&amp;nbsp;configurable scenarios, each&amp;nbsp;allowing the user to schedule exactly when&amp;nbsp;a&amp;nbsp;target&amp;nbsp;event&amp;nbsp;should&amp;nbsp;occur. It includes setups like GitHub Watcher, which&amp;nbsp;simulates a repository accumulating stars over time;&amp;nbsp;Teams Monitor, which models incoming messages, some&amp;nbsp;urgent; and&amp;nbsp;Flight Monitor, which&amp;nbsp;replicates&amp;nbsp;evolving&amp;nbsp;flight-availability&amp;nbsp;dynamics.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Initial&amp;nbsp;tests&amp;nbsp;show clear benefits.&amp;nbsp;As shown in&amp;nbsp;Figure&amp;nbsp;2, success rates&amp;nbsp;remain&amp;nbsp;high for short tasks (30&amp;nbsp;sec&amp;nbsp;and 1&amp;nbsp;min) regardless of&amp;nbsp;whether&amp;nbsp;SentinelStep&amp;nbsp;is&amp;nbsp;used.&amp;nbsp;For longer tasks,&amp;nbsp;SentinelStep&amp;nbsp;markedly&amp;nbsp;improves reliability: at 1 hour, task reliability rises from 5.6% without&amp;nbsp;SentinelStep&amp;nbsp;to&amp;nbsp;33.3% with&amp;nbsp;it;&amp;nbsp;and at 2 hours,&amp;nbsp;it rises&amp;nbsp;from 5.6% to 38.9%. These gains&amp;nbsp;demonstrate&amp;nbsp;that&amp;nbsp;SentinelStep&amp;nbsp;effectively addresses the challenge of maintaining performance over extended durations.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2. SentinelStep improves success rates on longer running tasks (1–2 hours) while maintaining comparable performance on shorter tasks.  " class="wp-image-1152612" height="399" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-2-Eval.png" width="619" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure&amp;nbsp;2.&amp;nbsp;SentinelStep&amp;nbsp;improves&amp;nbsp;success rates&amp;nbsp;on longer running tasks (1–2&amp;nbsp;hours)&amp;nbsp;while&amp;nbsp;maintaining&amp;nbsp;comparable performance&amp;nbsp;on shorter tasks.&amp;nbsp;&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="impact-and-availability"&gt;Impact and availability&lt;/h2&gt;



&lt;p&gt;SentinelStep is a first step toward practical, proactive, longer‑running agents. By embedding patience into plans, agents can responsibly monitor conditions and act when it matters—staying proactive without wasting resources. This lays the groundwork for always‑on assistants that stay efficient, respectful of limits, and aligned with user intent.&lt;/p&gt;



&lt;p&gt;We’ve open-sourced SentinelStep as part of Magentic-UI, available on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; or via &lt;code&gt;pip install magnetic-ui&lt;/code&gt;. As with any new technique, production deployment should be preceded through&amp;nbsp;testing and validation&amp;nbsp;for the specific use case.&amp;nbsp;For&amp;nbsp;guidance on&amp;nbsp;intended use,&amp;nbsp;privacy&amp;nbsp;considerations,&amp;nbsp;and safety&amp;nbsp;guidelines,&amp;nbsp;see&amp;nbsp;the&amp;nbsp;Magentic-UI&amp;nbsp;Transparency&amp;nbsp;Note.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Our goal is to&amp;nbsp;make it easier to implement agents that can&amp;nbsp;handle&amp;nbsp;long-running&amp;nbsp;monitoring&amp;nbsp;tasks&amp;nbsp;and&amp;nbsp;lay&amp;nbsp;the groundwork for&amp;nbsp;systems that&amp;nbsp;anticipate, adapt, and&amp;nbsp;evolve&amp;nbsp;to meet real-world needs.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/</guid><pubDate>Tue, 21 Oct 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] UC Santa Cruz Maps Coastal Flooding With NVIDIA Accelerated Computing (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/uc-santa-cruz-maps-coastal-flooding/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Coastal communities in the U.S. have a 26% chance of flooding within a 30-year period. This percentage is expected to increase due to climate-change-driven sea-level rise, making these areas even more vulnerable.&lt;/p&gt;
&lt;p&gt;Michael Beck, professor and director of the UC Santa Cruz Center for Coastal Climate Resilience, focuses on modeling and mapping the benefits of coral reefs and mangroves for reducing flood risk, which helps inform adaptation and preservation efforts.&lt;/p&gt;
&lt;p&gt;Beck and his team produce detailed, NVIDIA GPU-accelerated visualizations of coastal flooding to help government agencies, nongovernmental organizations and financial organizations better understand flood risks and demonstrate how nature-based solutions can mitigate damages on shore.&lt;/p&gt;
&lt;p&gt;“We view visualizations as fundamental to motivating action,” said Beck. “These are difficult problems revolving around people, some of them expensive to fix, so you’ve got to be able to visualize the solutions and ensure they’re going to work.”&lt;/p&gt;
&lt;p&gt;To accelerate the simulations, the center uses NVIDIA CUDA-X software, including the cuPyNumeric library and nvfortran compiler, running on NVIDIA RTX GPUs awarded through the NVIDIA Academic Grant Program.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86069"&gt;&lt;img alt="alt" class="size-full wp-image-86069" height="547" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image3-1.png" width="936" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86069"&gt;Image of a 2D flood model of Santa Cruz, California, rendered in Unreal Engine 5 and pixel streamed as an interactive application.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Accelerating Flood Mapping With NVIDIA GPUs&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Flood modeling starts with a question such as, “What would happen to Santa Cruz, California, in the event of a 100-year storm?”&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86072"&gt;&lt;img alt="alt" class="size-full wp-image-86072" height="1042" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image4-1.png" width="936" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86072"&gt;Benefits of adaptation strategies in Capitola, California. The image shows flood hazard maps that are used to calculate risks and property-level economic benefits from adaptation strategies to increase protection benefits using seawalls or restored dunes and nearshore rocky reefs. Results from California 5th Climate Assessment (in review).&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;To answer that question, the research team uses simulation tools such as SFINCS and&amp;nbsp; feeds the results into the Unreal Engine 5 rendering engine to produce data-driven videos showing various flooding scenarios.&lt;/p&gt;
&lt;p&gt;Beck’s team accelerated their flood risk models and visualizations from a process that originally took approximately six hours on CPU workloads to just about 40 minutes using a single NVIDIA RTX 6000 Ada Generation GPU. With the team’s cluster of four GPUs, they can now run four simulations simultaneously.&lt;/p&gt;
&lt;p&gt;“We were able to reduce the computation by 3-4x, with some workloads being even faster,” said David Gutiérrez, senior coastal modeler at UC Santa Cruz’s Center for Coastal Climate Resilience. “This speedup gave us the capacity to run a sensitivity analysis beforehand and make sure we’re not making any assumptions or setting the wrong parameters.”&lt;/p&gt;
&lt;p&gt;Less time spent on modeling and visualizing each site meant the researchers could set more ambitious, global goals for their project.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Digital Flood Mapping Goes Global &lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The team’s current endeavor is to map all small-island developing states globally — from Tonga to Trinidad and Tobago — before the COP30 climate change conference in November.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86075"&gt;&lt;img alt="alt" class="size-large wp-image-86075" height="943" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/St_Martin_01-1-1680x943.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86075"&gt;Still from a video of a 2D flood model of Saint Martin, France, rendered in Unreal Engine 5.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;“The kind of high-resolution modeling that we’re doing is very tricky,” Beck said. “We’re trying to show not just weather and the problems that weather creates, but run scenarios with solutions.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Put Your Money Where the Marine Life Is &lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;One way to motivate coral reef preservation is by insuring them based on their value as natural infrastructure, capable of serving as a line of defense during climatic events.&lt;/p&gt;
&lt;p&gt;This is already becoming a reality in places like the Mesoamerican Barrier Reef in Mexico, south of Cancun on the Mayan Riviera.&lt;/p&gt;
&lt;p&gt;Through his work with The Nature Conservancy, Beck presented visualizations of reefs in the event of a hurricane to interested parties, including hotel owners along the coast, the Mexican government and World Bank Group.&lt;/p&gt;
&lt;p&gt;These groups ended up investing in a parametric insurance policy to protect the coral reef, which stipulated that if the wind reached over 100 knots, a payout would be triggered to restore the damaged reef.&lt;/p&gt;
&lt;p&gt;“To motivate action, you have to understand and visualize a bit more about the increasingly risky storm events that we’re facing right now, because of where they’re developing and the impacts of climate change,” said Beck.&lt;/p&gt;
&lt;p&gt;Other regions throughout the Caribbean and Hawaii are also starting to adopt similar policies.&lt;/p&gt;
&lt;p&gt;Insurance policies like this help ensure that funding is available to restore reefs after storms — protecting coastal communities against further, more severe flooding damage.&lt;/p&gt;
&lt;p&gt;“You’ve got all these coral heads and the waves damage them, so you need to either take them to a nursery or cement them back on the reef,” Beck said. “Also, you need to get all the trash off that comes back out with the flood from the land — everything from fishing nets to refrigerators — that’s the flotsam and jetsam of our life.”&lt;/p&gt;
&lt;p&gt;These researchers are now working toward modeling flooding across California’s coastal regions through a project called CoSMoS ADAPT.&lt;/p&gt;
&lt;p&gt;The project’s primary goal is to strengthen the USGS’s Coastal Storm Modeling System (CoSMoS), which is the first toolkit for quantitative evaluation of the cost-effectiveness of the state’s coastal adaptation options, including nature-based solutions.&lt;/p&gt;
&lt;p&gt;“CoSMoS predicts what risks will be now and into the future,” Beck said. “We’re adding the solutions that will reduce the risk — from dunes to sea walls and our reefs — and at a variety of levels, this is going to demand acceleration and heavy computation.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Apply for an NVIDIA Academic Grant&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA Academic Grant Program is calling for research proposals now through Wednesday, Dec. 31, to advance work in three interest areas: generative AI and model development, generative AI alignment and inference, and robotics and edge AI. Full-time faculty members at accredited academic institutions are eligible to apply.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the&lt;/i&gt; &lt;i&gt;NVIDIA Academic Grant Program&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Main video above shows v&lt;/em&gt;&lt;em&gt;isualization of the data layers used in a 2D wave and flood model for assessment of reef and restoration benefits in Christiansted, a town in Saint Croix, one of the U.S. Virgin Islands.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Coastal communities in the U.S. have a 26% chance of flooding within a 30-year period. This percentage is expected to increase due to climate-change-driven sea-level rise, making these areas even more vulnerable.&lt;/p&gt;
&lt;p&gt;Michael Beck, professor and director of the UC Santa Cruz Center for Coastal Climate Resilience, focuses on modeling and mapping the benefits of coral reefs and mangroves for reducing flood risk, which helps inform adaptation and preservation efforts.&lt;/p&gt;
&lt;p&gt;Beck and his team produce detailed, NVIDIA GPU-accelerated visualizations of coastal flooding to help government agencies, nongovernmental organizations and financial organizations better understand flood risks and demonstrate how nature-based solutions can mitigate damages on shore.&lt;/p&gt;
&lt;p&gt;“We view visualizations as fundamental to motivating action,” said Beck. “These are difficult problems revolving around people, some of them expensive to fix, so you’ve got to be able to visualize the solutions and ensure they’re going to work.”&lt;/p&gt;
&lt;p&gt;To accelerate the simulations, the center uses NVIDIA CUDA-X software, including the cuPyNumeric library and nvfortran compiler, running on NVIDIA RTX GPUs awarded through the NVIDIA Academic Grant Program.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86069"&gt;&lt;img alt="alt" class="size-full wp-image-86069" height="547" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image3-1.png" width="936" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86069"&gt;Image of a 2D flood model of Santa Cruz, California, rendered in Unreal Engine 5 and pixel streamed as an interactive application.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Accelerating Flood Mapping With NVIDIA GPUs&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Flood modeling starts with a question such as, “What would happen to Santa Cruz, California, in the event of a 100-year storm?”&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86072"&gt;&lt;img alt="alt" class="size-full wp-image-86072" height="1042" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/image4-1.png" width="936" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86072"&gt;Benefits of adaptation strategies in Capitola, California. The image shows flood hazard maps that are used to calculate risks and property-level economic benefits from adaptation strategies to increase protection benefits using seawalls or restored dunes and nearshore rocky reefs. Results from California 5th Climate Assessment (in review).&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;To answer that question, the research team uses simulation tools such as SFINCS and&amp;nbsp; feeds the results into the Unreal Engine 5 rendering engine to produce data-driven videos showing various flooding scenarios.&lt;/p&gt;
&lt;p&gt;Beck’s team accelerated their flood risk models and visualizations from a process that originally took approximately six hours on CPU workloads to just about 40 minutes using a single NVIDIA RTX 6000 Ada Generation GPU. With the team’s cluster of four GPUs, they can now run four simulations simultaneously.&lt;/p&gt;
&lt;p&gt;“We were able to reduce the computation by 3-4x, with some workloads being even faster,” said David Gutiérrez, senior coastal modeler at UC Santa Cruz’s Center for Coastal Climate Resilience. “This speedup gave us the capacity to run a sensitivity analysis beforehand and make sure we’re not making any assumptions or setting the wrong parameters.”&lt;/p&gt;
&lt;p&gt;Less time spent on modeling and visualizing each site meant the researchers could set more ambitious, global goals for their project.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Digital Flood Mapping Goes Global &lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The team’s current endeavor is to map all small-island developing states globally — from Tonga to Trinidad and Tobago — before the COP30 climate change conference in November.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_86075"&gt;&lt;img alt="alt" class="size-large wp-image-86075" height="943" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/St_Martin_01-1-1680x943.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-86075"&gt;Still from a video of a 2D flood model of Saint Martin, France, rendered in Unreal Engine 5.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;“The kind of high-resolution modeling that we’re doing is very tricky,” Beck said. “We’re trying to show not just weather and the problems that weather creates, but run scenarios with solutions.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Put Your Money Where the Marine Life Is &lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;One way to motivate coral reef preservation is by insuring them based on their value as natural infrastructure, capable of serving as a line of defense during climatic events.&lt;/p&gt;
&lt;p&gt;This is already becoming a reality in places like the Mesoamerican Barrier Reef in Mexico, south of Cancun on the Mayan Riviera.&lt;/p&gt;
&lt;p&gt;Through his work with The Nature Conservancy, Beck presented visualizations of reefs in the event of a hurricane to interested parties, including hotel owners along the coast, the Mexican government and World Bank Group.&lt;/p&gt;
&lt;p&gt;These groups ended up investing in a parametric insurance policy to protect the coral reef, which stipulated that if the wind reached over 100 knots, a payout would be triggered to restore the damaged reef.&lt;/p&gt;
&lt;p&gt;“To motivate action, you have to understand and visualize a bit more about the increasingly risky storm events that we’re facing right now, because of where they’re developing and the impacts of climate change,” said Beck.&lt;/p&gt;
&lt;p&gt;Other regions throughout the Caribbean and Hawaii are also starting to adopt similar policies.&lt;/p&gt;
&lt;p&gt;Insurance policies like this help ensure that funding is available to restore reefs after storms — protecting coastal communities against further, more severe flooding damage.&lt;/p&gt;
&lt;p&gt;“You’ve got all these coral heads and the waves damage them, so you need to either take them to a nursery or cement them back on the reef,” Beck said. “Also, you need to get all the trash off that comes back out with the flood from the land — everything from fishing nets to refrigerators — that’s the flotsam and jetsam of our life.”&lt;/p&gt;
&lt;p&gt;These researchers are now working toward modeling flooding across California’s coastal regions through a project called CoSMoS ADAPT.&lt;/p&gt;
&lt;p&gt;The project’s primary goal is to strengthen the USGS’s Coastal Storm Modeling System (CoSMoS), which is the first toolkit for quantitative evaluation of the cost-effectiveness of the state’s coastal adaptation options, including nature-based solutions.&lt;/p&gt;
&lt;p&gt;“CoSMoS predicts what risks will be now and into the future,” Beck said. “We’re adding the solutions that will reduce the risk — from dunes to sea walls and our reefs — and at a variety of levels, this is going to demand acceleration and heavy computation.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Apply for an NVIDIA Academic Grant&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA Academic Grant Program is calling for research proposals now through Wednesday, Dec. 31, to advance work in three interest areas: generative AI and model development, generative AI alignment and inference, and robotics and edge AI. Full-time faculty members at accredited academic institutions are eligible to apply.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about the&lt;/i&gt; &lt;i&gt;NVIDIA Academic Grant Program&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Main video above shows v&lt;/em&gt;&lt;em&gt;isualization of the data layers used in a 2D wave and flood model for assessment of reef and restoration benefits in Christiansted, a town in Saint Croix, one of the U.S. Virgin Islands.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/uc-santa-cruz-maps-coastal-flooding/</guid><pubDate>Tue, 21 Oct 2025 16:30:16 +0000</pubDate></item><item><title>OpenAI launches an AI-powered browser: ChatGPT Atlas (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/openai-launches-an-ai-powered-browser-chatgpt-atlas/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-teal.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Tuesday the launch of its AI-powered browser, ChatGPT Atlas, a major step in the company’s quest to unseat Google as the main way people find information online.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says Atlas will first roll out on macOS, with support for Windows, iOS, and Android coming soon. OpenAI says the product will be available to all free users at launch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Browsers have quickly become the AI industry’s next battleground. While Google Chrome has long dominated the space, there’s a sense that AI chatbots and agents are fundamentally changing how people get work done online. A handful of startups have tried to capture this by launching AI-powered browsers of their own, such as Perplexity’s Comet and The Browser Company’s Dia. Google and Microsoft have also tried to update Chrome and Edge, respectively, with AI-powered features to make their legacy products stand out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s Engineering Lead for Atlas, Ben Goodger, said in a livestream Tuesday that ChatGPT is core to the company’s first browser. Users in ChatGPT Atlas can chat with their search results, much like in Perplexity or in Google’s AI Mode.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The killer feature for other AI-powered browsers has been the built-in chatbot that sits in a side panel and automatically has context for whatever’s on your screen. It may sound minor, but many users spend all day copying and pasting text or dragging files and links into ChatGPT, just to provide context. The sidecar feature removes that friction and makes for a smoother user experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s Product Lead, Adam Fry, said during the livestream that ChatGPT Atlas will have the sidecar feature, too. Further, ChatGPT Atlas has “browser history,” meaning that ChatGPT can now log the websites you visit and what you do on them, and use that information to make its answers more personalized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI-powered browsers also commonly feature an AI agent that aims to automate web-based tasks on behalf of users. In TechCrunch’s testing, we’ve found the early versions of web-browsing AI agents leave something to be desired. While Perplexity’s Comet and OpenAI’s ChatGPT agent work well for simple tasks, they struggle to reliably automate the more cumbersome problems users might want to offload to an AI system.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Sure enough, OpenAI’s browser has a web-browsing agent too. By using “agent mode,” users can ask ChatGPT to complete small tasks in the browser on their behalf. The company says agent mode is only available to ChatGPT users on the Plus, Pro, and Business tier at launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview at OpenAI’s DevDay conference, Head of ChatGPT Nick Turley told TechCrunch that he’s inspired by the way browsers have redefined what an operating system can look like. Turley noted that browsers have revolutionized the way people get work done online, and he thinks ChatGPT is a similar phenomenon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether OpenAI’s browser can put a dent in Google Chrome, which has more than 3 billion users around the globe, remains to be seen. AI browsers are quite buzzy in Silicon Valley today, but their impact in the broader world is limited.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-teal.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Tuesday the launch of its AI-powered browser, ChatGPT Atlas, a major step in the company’s quest to unseat Google as the main way people find information online.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says Atlas will first roll out on macOS, with support for Windows, iOS, and Android coming soon. OpenAI says the product will be available to all free users at launch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Browsers have quickly become the AI industry’s next battleground. While Google Chrome has long dominated the space, there’s a sense that AI chatbots and agents are fundamentally changing how people get work done online. A handful of startups have tried to capture this by launching AI-powered browsers of their own, such as Perplexity’s Comet and The Browser Company’s Dia. Google and Microsoft have also tried to update Chrome and Edge, respectively, with AI-powered features to make their legacy products stand out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s Engineering Lead for Atlas, Ben Goodger, said in a livestream Tuesday that ChatGPT is core to the company’s first browser. Users in ChatGPT Atlas can chat with their search results, much like in Perplexity or in Google’s AI Mode.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The killer feature for other AI-powered browsers has been the built-in chatbot that sits in a side panel and automatically has context for whatever’s on your screen. It may sound minor, but many users spend all day copying and pasting text or dragging files and links into ChatGPT, just to provide context. The sidecar feature removes that friction and makes for a smoother user experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s Product Lead, Adam Fry, said during the livestream that ChatGPT Atlas will have the sidecar feature, too. Further, ChatGPT Atlas has “browser history,” meaning that ChatGPT can now log the websites you visit and what you do on them, and use that information to make its answers more personalized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI-powered browsers also commonly feature an AI agent that aims to automate web-based tasks on behalf of users. In TechCrunch’s testing, we’ve found the early versions of web-browsing AI agents leave something to be desired. While Perplexity’s Comet and OpenAI’s ChatGPT agent work well for simple tasks, they struggle to reliably automate the more cumbersome problems users might want to offload to an AI system.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Sure enough, OpenAI’s browser has a web-browsing agent too. By using “agent mode,” users can ask ChatGPT to complete small tasks in the browser on their behalf. The company says agent mode is only available to ChatGPT users on the Plus, Pro, and Business tier at launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview at OpenAI’s DevDay conference, Head of ChatGPT Nick Turley told TechCrunch that he’s inspired by the way browsers have redefined what an operating system can look like. Turley noted that browsers have revolutionized the way people get work done online, and he thinks ChatGPT is a similar phenomenon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether OpenAI’s browser can put a dent in Google Chrome, which has more than 3 billion users around the globe, remains to be seen. AI browsers are quite buzzy in Silicon Valley today, but their impact in the broader world is limited.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/openai-launches-an-ai-powered-browser-chatgpt-atlas/</guid><pubDate>Tue, 21 Oct 2025 17:15:40 +0000</pubDate></item><item><title>Google's new vibe coding AI Studio experience lets anyone build, deploy apps live in minutes (AI | VentureBeat)</title><link>https://venturebeat.com/ai/googles-new-vibe-coding-ai-studio-experience-lets-anyone-build-deploy-apps</link><description>[unable to retrieve full-text content]&lt;p&gt;Google AI Studio has gotten a big vibe coding upgrade with a new interface, buttons, suggestions and community features that allow anyone with an idea for an app — even complete novices, laypeople, or non-developers like yours truly — to bring it into existence and deploy it live, on the web, for anyone to use, within &lt;i&gt;minutes&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;The updated Build tab is available now at &lt;a href="http://ai.studio/build"&gt;ai.studio/build&lt;/a&gt;, and it’s free to start. &lt;/p&gt;&lt;p&gt;Users can experiment with building applications without needing to enter payment information upfront, though certain advanced features like Veo 3.1 and Cloud Run deployment require a paid API key.&lt;/p&gt;&lt;p&gt;The new features appear to me to make Google&amp;#x27;s AI models and offerings even more competitive, perhaps preferred, for many general users to dedicated AI startup rivals like Anthropic&amp;#x27;s Claude Code and OpenAI&amp;#x27;s Codex, respectively, two &amp;quot;vibe coding&amp;quot; focused products that are beloved by developers — but seem to have a higher barrier to entry or may require more technical know-how.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Fresh Start: Redesigned Build Mode&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The updated Build tab serves as the entry point to vibe coding. It introduces a new layout and workflow where users can select from Google’s suite of AI models and features to power their applications. The default is Gemini 2.5 Pro, which is great for most cases.&lt;/p&gt;&lt;p&gt;Once selections are made, users simply describe what they want to build, and the system automatically assembles the necessary components using Gemini’s APIs.&lt;/p&gt;&lt;p&gt;This mode supports mixing capabilities like Nano Banana (a lightweight AI model), Veo (for video understanding), Imagine (for image generation), Flashlight (for performance-optimized inference), and Google Search.&lt;/p&gt;&lt;p&gt;Patrick Löber, Developer Relations at Google DeepMind, highlighted that the experience is meant to help users “supercharge your apps with AI” using a simple prompt-to-app pipeline.&lt;/p&gt;&lt;p&gt;In a video demo he posted on X and LinedIn, he showed how just a few clicks led to the automatic generation of a garden planning assistant app, complete with layouts, visuals, and a conversational interface.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;h3&gt;&lt;b&gt;From Prompt to Production: Building and Editing in Real Time&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Once an app is generated, users land in a fully interactive editor. On the left, there’s a traditional code-assist interface where developers can chat with the AI model for help or suggestions. On the right, a code editor displays the full source of the app.&lt;/p&gt;&lt;p&gt;Each component—such as React entry points, API calls, or styling files—can be edited directly. Tooltips help users understand what each file does, which is especially useful for those less familiar with TypeScript or frontend frameworks.&lt;/p&gt;&lt;p&gt;Apps can be saved to GitHub, downloaded locally, or shared directly. Deployment is possible within the Studio environment or via Cloud Run if advanced scaling or hosting is needed.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Inspiration on Demand: The ‘I’m Feeling Lucky’ Button&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;One standout feature in this update is the “I’m Feeling Lucky” button. Designed for users who need a creative jumpstart, it generates randomized app concepts and configures the app setup accordingly. Each press yields a different idea, complete with suggested AI features and components.&lt;/p&gt;&lt;p&gt;Examples produced during demos include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;An interactive map-based chatbot powered by Google Search and conversational AI.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A dream garden designer using image generation and advanced planning tools.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A trivia game app with an AI host whose personality users can define, integrating both Imagine and Flashlight with Gemini 2.5 Pro for conversation and reasoning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Logan Kilpatrick, Lead of Product for Google AI Studio and Gemini AI, noted in a demo video of his own that this feature encourages discovery and experimentation. &lt;/p&gt;&lt;p&gt;“You get some really, really cool, different experiences,” he said, emphasizing its role in helping users find novel ideas quickly.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;h3&gt;&lt;b&gt;Hands-On Test: From Prompt to App in 65 Seconds&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To test the new workflow, I prompted Gemini with:&lt;/p&gt;&lt;p&gt;&lt;i&gt;A randomized dice rolling web application where the user can select between common dice sizes (6 sides, 10 sides, etc) and then see an animated die rolling and choose the color of their die as well.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Within 65 seconds (just over a minute) AI Studio returned a fully working web app&lt;/b&gt; featuring:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Dice size selector (d4, d6, d8, d10, d12, d20)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Color customization options for the die&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Animated rolling effect with randomized results&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Clean, modern UI built with React, TypeScript, and Tailwind CSS&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The platform also generated a complete set of structured files, including App.tsx, constants.ts, and separate components for dice logic and controls. &lt;/p&gt;&lt;p&gt;After generation, it was easy to iterate: adding sound effects for each interaction (rolling, choosing a die, changing color) required only a single follow-up prompt to the built-in assistant. This was also suggested by Gemini, too, by the way. &lt;/p&gt;&lt;p&gt;From there, the app can be previewed live or exported using built-in controls to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Save to GitHub&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Download the full codebase&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Copy the project for remixing&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Deploy via integrated tools&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;My brief, hands-on test showed just how quickly even small utility apps can go from idea to interactive prototype—without leaving the browser or writing boilerplate code manually.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;AI-Suggested Enhancements and Feature Refinement&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In addition to code generation, Google AI Studio now offers context-aware feature suggestions. These recommendations, generated by Gemini’s Flashlight capability, analyze the current app and propose relevant improvements.&lt;/p&gt;&lt;p&gt;In one example, the system suggested implementing a feature that displays the history of previously generated images in an image studio tab. These iterative enhancements allow builders to expand app functionality over time without starting from scratch.&lt;/p&gt;&lt;p&gt;Kilpatrick emphasized that users can continue to refine their projects as they go, combining both automatic generation and manual adjustments. “You can go in and continue to edit and sort of refine the experience that you want iteratively,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Free to Start, Flexible to Grow&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The new experience is available at no cost for users who want to experiment, prototype, or build lightweight apps. There’s no requirement to enter credit card information to begin using vibe coding.&lt;/p&gt;&lt;p&gt;However, more powerful capabilities — such as using models like Veo 3.1 or deploying through Cloud Run — do require switching to a paid API key.&lt;/p&gt;&lt;p&gt;This pricing structure is intended to lower the barrier to entry for experimentation while providing a clear path to scale when needed.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Built for All Skill Levels&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;One of the central goals of the vibe coding launch is to make AI app development accessible to more people. The system supports both high-level visual builders and low-level code editing, creating a workflow that works for developers across experience levels.&lt;/p&gt;&lt;p&gt;Kilpatrick mentioned that while he’s more familiar with Python than TypeScript, he still found the editor useful because of the helpful file descriptions and intuitive layout. &lt;/p&gt;&lt;p&gt;This focus on usability could make AI Studio a compelling option for developers exploring AI for the first time.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;More to Come: A Week of Launches&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The launch of vibe coding is the first in a series of announcements expected throughout the week. While specific future features haven’t been revealed yet, both Kilpatrick and Löber hinted that additional updates are on the way.&lt;/p&gt;&lt;p&gt;With this update, Google AI Studio positions itself as a flexible, user-friendly environment for building AI-powered applications—whether for fun, prototyping, or production deployment. The focus is clear: make the power of Gemini’s APIs accessible without unnecessary complexity.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Google AI Studio has gotten a big vibe coding upgrade with a new interface, buttons, suggestions and community features that allow anyone with an idea for an app — even complete novices, laypeople, or non-developers like yours truly — to bring it into existence and deploy it live, on the web, for anyone to use, within &lt;i&gt;minutes&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;The updated Build tab is available now at &lt;a href="http://ai.studio/build"&gt;ai.studio/build&lt;/a&gt;, and it’s free to start. &lt;/p&gt;&lt;p&gt;Users can experiment with building applications without needing to enter payment information upfront, though certain advanced features like Veo 3.1 and Cloud Run deployment require a paid API key.&lt;/p&gt;&lt;p&gt;The new features appear to me to make Google&amp;#x27;s AI models and offerings even more competitive, perhaps preferred, for many general users to dedicated AI startup rivals like Anthropic&amp;#x27;s Claude Code and OpenAI&amp;#x27;s Codex, respectively, two &amp;quot;vibe coding&amp;quot; focused products that are beloved by developers — but seem to have a higher barrier to entry or may require more technical know-how.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A Fresh Start: Redesigned Build Mode&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The updated Build tab serves as the entry point to vibe coding. It introduces a new layout and workflow where users can select from Google’s suite of AI models and features to power their applications. The default is Gemini 2.5 Pro, which is great for most cases.&lt;/p&gt;&lt;p&gt;Once selections are made, users simply describe what they want to build, and the system automatically assembles the necessary components using Gemini’s APIs.&lt;/p&gt;&lt;p&gt;This mode supports mixing capabilities like Nano Banana (a lightweight AI model), Veo (for video understanding), Imagine (for image generation), Flashlight (for performance-optimized inference), and Google Search.&lt;/p&gt;&lt;p&gt;Patrick Löber, Developer Relations at Google DeepMind, highlighted that the experience is meant to help users “supercharge your apps with AI” using a simple prompt-to-app pipeline.&lt;/p&gt;&lt;p&gt;In a video demo he posted on X and LinedIn, he showed how just a few clicks led to the automatic generation of a garden planning assistant app, complete with layouts, visuals, and a conversational interface.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;h3&gt;&lt;b&gt;From Prompt to Production: Building and Editing in Real Time&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Once an app is generated, users land in a fully interactive editor. On the left, there’s a traditional code-assist interface where developers can chat with the AI model for help or suggestions. On the right, a code editor displays the full source of the app.&lt;/p&gt;&lt;p&gt;Each component—such as React entry points, API calls, or styling files—can be edited directly. Tooltips help users understand what each file does, which is especially useful for those less familiar with TypeScript or frontend frameworks.&lt;/p&gt;&lt;p&gt;Apps can be saved to GitHub, downloaded locally, or shared directly. Deployment is possible within the Studio environment or via Cloud Run if advanced scaling or hosting is needed.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Inspiration on Demand: The ‘I’m Feeling Lucky’ Button&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;One standout feature in this update is the “I’m Feeling Lucky” button. Designed for users who need a creative jumpstart, it generates randomized app concepts and configures the app setup accordingly. Each press yields a different idea, complete with suggested AI features and components.&lt;/p&gt;&lt;p&gt;Examples produced during demos include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;An interactive map-based chatbot powered by Google Search and conversational AI.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A dream garden designer using image generation and advanced planning tools.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A trivia game app with an AI host whose personality users can define, integrating both Imagine and Flashlight with Gemini 2.5 Pro for conversation and reasoning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Logan Kilpatrick, Lead of Product for Google AI Studio and Gemini AI, noted in a demo video of his own that this feature encourages discovery and experimentation. &lt;/p&gt;&lt;p&gt;“You get some really, really cool, different experiences,” he said, emphasizing its role in helping users find novel ideas quickly.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;h3&gt;&lt;b&gt;Hands-On Test: From Prompt to App in 65 Seconds&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;To test the new workflow, I prompted Gemini with:&lt;/p&gt;&lt;p&gt;&lt;i&gt;A randomized dice rolling web application where the user can select between common dice sizes (6 sides, 10 sides, etc) and then see an animated die rolling and choose the color of their die as well.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Within 65 seconds (just over a minute) AI Studio returned a fully working web app&lt;/b&gt; featuring:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Dice size selector (d4, d6, d8, d10, d12, d20)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Color customization options for the die&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Animated rolling effect with randomized results&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Clean, modern UI built with React, TypeScript, and Tailwind CSS&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The platform also generated a complete set of structured files, including App.tsx, constants.ts, and separate components for dice logic and controls. &lt;/p&gt;&lt;p&gt;After generation, it was easy to iterate: adding sound effects for each interaction (rolling, choosing a die, changing color) required only a single follow-up prompt to the built-in assistant. This was also suggested by Gemini, too, by the way. &lt;/p&gt;&lt;p&gt;From there, the app can be previewed live or exported using built-in controls to:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Save to GitHub&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Download the full codebase&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Copy the project for remixing&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Deploy via integrated tools&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;My brief, hands-on test showed just how quickly even small utility apps can go from idea to interactive prototype—without leaving the browser or writing boilerplate code manually.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;AI-Suggested Enhancements and Feature Refinement&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In addition to code generation, Google AI Studio now offers context-aware feature suggestions. These recommendations, generated by Gemini’s Flashlight capability, analyze the current app and propose relevant improvements.&lt;/p&gt;&lt;p&gt;In one example, the system suggested implementing a feature that displays the history of previously generated images in an image studio tab. These iterative enhancements allow builders to expand app functionality over time without starting from scratch.&lt;/p&gt;&lt;p&gt;Kilpatrick emphasized that users can continue to refine their projects as they go, combining both automatic generation and manual adjustments. “You can go in and continue to edit and sort of refine the experience that you want iteratively,” he said.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Free to Start, Flexible to Grow&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The new experience is available at no cost for users who want to experiment, prototype, or build lightweight apps. There’s no requirement to enter credit card information to begin using vibe coding.&lt;/p&gt;&lt;p&gt;However, more powerful capabilities — such as using models like Veo 3.1 or deploying through Cloud Run — do require switching to a paid API key.&lt;/p&gt;&lt;p&gt;This pricing structure is intended to lower the barrier to entry for experimentation while providing a clear path to scale when needed.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Built for All Skill Levels&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;One of the central goals of the vibe coding launch is to make AI app development accessible to more people. The system supports both high-level visual builders and low-level code editing, creating a workflow that works for developers across experience levels.&lt;/p&gt;&lt;p&gt;Kilpatrick mentioned that while he’s more familiar with Python than TypeScript, he still found the editor useful because of the helpful file descriptions and intuitive layout. &lt;/p&gt;&lt;p&gt;This focus on usability could make AI Studio a compelling option for developers exploring AI for the first time.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;More to Come: A Week of Launches&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The launch of vibe coding is the first in a series of announcements expected throughout the week. While specific future features haven’t been revealed yet, both Kilpatrick and Löber hinted that additional updates are on the way.&lt;/p&gt;&lt;p&gt;With this update, Google AI Studio positions itself as a flexible, user-friendly environment for building AI-powered applications—whether for fun, prototyping, or production deployment. The focus is clear: make the power of Gemini’s APIs accessible without unnecessary complexity.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/googles-new-vibe-coding-ai-studio-experience-lets-anyone-build-deploy-apps</guid><pubDate>Tue, 21 Oct 2025 17:45:00 +0000</pubDate></item><item><title>a16z-backed Codi launches AI agent office manager (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/a16z-backed-codi-launches-ai-agent-office-manager/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Christelle_Rohaut.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Codi,&amp;nbsp;an Andreessen Horowitz-backed startup&amp;nbsp;founded by Christelle&amp;nbsp;Rohaut&amp;nbsp;and Dave Schuman, is launching what it hails as the first AI-powered platform to fully automate office management.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Codi was founded in 2018, in a pre-pandemic world, with a mission to help companies find flexible office spaces. It was more of a marketplace, as TechCrunch previously reported, that matched companies to buildings that offered flexible office arrangements. Codi then assisted with the move-in processes.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rohaut, the company’s CEO, said she and her team back then used to manually manage office spaces and vendors for their clients, but the recent advancements in AI have allowed them&amp;nbsp;to&amp;nbsp;essentially automate&amp;nbsp;themselves.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The previous model of Codi, you had to get the space with Codi. Now, whatever office you lease, you can use this to automate your office logistics,” she said of her new AI SaaS product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup released its beta version of the new AI office management product in May and officially launched it on Tuesday. The company last raised a $16 million Series A in 2022 led by a16z, and has raised&lt;strong&gt; &lt;/strong&gt;$23 million to date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The technology comes as&amp;nbsp;return-to-office&amp;nbsp;continues to take hold throughout corporate America.&amp;nbsp;“Office management remains very manual and broken,”&amp;nbsp;Rohaut, the company’s CEO, told TechCrunch. She added that it can cost companies at least $80,000 a year just in administrative costs to run an&amp;nbsp;office.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The role of an office manager has also changed throughout the years. In this post-pandemic world, as companies moved toward remote and hybrid work, the formal job of office manager has often been left unfilled. When companies do have an office manager, they are often spending more&amp;nbsp;time on&amp;nbsp;planning events rather than the&amp;nbsp;logistics&amp;nbsp;of the office, she said.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Rohaut said she and the team trained the Codi AI on all the expertise&amp;nbsp;and data&amp;nbsp;they’ve&amp;nbsp;accumulated&amp;nbsp;over&amp;nbsp;the past few years.&amp;nbsp;The vendors a company uses&amp;nbsp;are&amp;nbsp;put into the AI&amp;nbsp;system,&amp;nbsp;and then the AI coordinates for office needs such as pantry restocking and cleaning.&amp;nbsp;The company said it took only five weeks for it to&amp;nbsp;reach&amp;nbsp;$100,000 in ARR after it released its beta. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This new platform is estimated to save hundreds of hours a year&amp;nbsp;in&amp;nbsp;admin tasks,” she said. Codi takes a management fee per&amp;nbsp;month, like&amp;nbsp;a subscription, which is “a fraction of the cost of an office manager or part-time office manager or even a fractional&amp;nbsp;EA,” Rohaut continued.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rohaut said “a good portion” of their clients,&amp;nbsp;from whom they managed office spaces,&amp;nbsp;are transitioning to&amp;nbsp;using the AI platform. Just in the beta, the&amp;nbsp;new Codi product has already signed 40 new companies, like TaskRabbit&amp;nbsp;and&amp;nbsp;Northbeam,&amp;nbsp;Rohaut&amp;nbsp;said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rohaut sees Codi’s competitors as being legacy management companies and workplace experience platforms, like Envoy. She said, unlike legacy management companies, Codi replaces the need for staff members to review, hire, and coordinate with each vendor because the execution is autonomous and the platform integrates a curated network of serviced providers, Rohaut said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Also, compared to workplace platforms, Codi helps coordinate the handling of physical operations in an office, she continued. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Codi is building the future where offices can run themselves, just like cars can drive themselves,” she continued. “We want to entirely remove the logistical burden of managing physical spaces and free human talent to focus on the workplace culture and growth.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This piece was updated to reflect the proper spelling of Rohaut&lt;/em&gt;‘s &lt;em&gt;name. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Christelle_Rohaut.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Codi,&amp;nbsp;an Andreessen Horowitz-backed startup&amp;nbsp;founded by Christelle&amp;nbsp;Rohaut&amp;nbsp;and Dave Schuman, is launching what it hails as the first AI-powered platform to fully automate office management.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Codi was founded in 2018, in a pre-pandemic world, with a mission to help companies find flexible office spaces. It was more of a marketplace, as TechCrunch previously reported, that matched companies to buildings that offered flexible office arrangements. Codi then assisted with the move-in processes.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rohaut, the company’s CEO, said she and her team back then used to manually manage office spaces and vendors for their clients, but the recent advancements in AI have allowed them&amp;nbsp;to&amp;nbsp;essentially automate&amp;nbsp;themselves.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The previous model of Codi, you had to get the space with Codi. Now, whatever office you lease, you can use this to automate your office logistics,” she said of her new AI SaaS product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup released its beta version of the new AI office management product in May and officially launched it on Tuesday. The company last raised a $16 million Series A in 2022 led by a16z, and has raised&lt;strong&gt; &lt;/strong&gt;$23 million to date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The technology comes as&amp;nbsp;return-to-office&amp;nbsp;continues to take hold throughout corporate America.&amp;nbsp;“Office management remains very manual and broken,”&amp;nbsp;Rohaut, the company’s CEO, told TechCrunch. She added that it can cost companies at least $80,000 a year just in administrative costs to run an&amp;nbsp;office.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The role of an office manager has also changed throughout the years. In this post-pandemic world, as companies moved toward remote and hybrid work, the formal job of office manager has often been left unfilled. When companies do have an office manager, they are often spending more&amp;nbsp;time on&amp;nbsp;planning events rather than the&amp;nbsp;logistics&amp;nbsp;of the office, she said.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Rohaut said she and the team trained the Codi AI on all the expertise&amp;nbsp;and data&amp;nbsp;they’ve&amp;nbsp;accumulated&amp;nbsp;over&amp;nbsp;the past few years.&amp;nbsp;The vendors a company uses&amp;nbsp;are&amp;nbsp;put into the AI&amp;nbsp;system,&amp;nbsp;and then the AI coordinates for office needs such as pantry restocking and cleaning.&amp;nbsp;The company said it took only five weeks for it to&amp;nbsp;reach&amp;nbsp;$100,000 in ARR after it released its beta. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This new platform is estimated to save hundreds of hours a year&amp;nbsp;in&amp;nbsp;admin tasks,” she said. Codi takes a management fee per&amp;nbsp;month, like&amp;nbsp;a subscription, which is “a fraction of the cost of an office manager or part-time office manager or even a fractional&amp;nbsp;EA,” Rohaut continued.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rohaut said “a good portion” of their clients,&amp;nbsp;from whom they managed office spaces,&amp;nbsp;are transitioning to&amp;nbsp;using the AI platform. Just in the beta, the&amp;nbsp;new Codi product has already signed 40 new companies, like TaskRabbit&amp;nbsp;and&amp;nbsp;Northbeam,&amp;nbsp;Rohaut&amp;nbsp;said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rohaut sees Codi’s competitors as being legacy management companies and workplace experience platforms, like Envoy. She said, unlike legacy management companies, Codi replaces the need for staff members to review, hire, and coordinate with each vendor because the execution is autonomous and the platform integrates a curated network of serviced providers, Rohaut said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Also, compared to workplace platforms, Codi helps coordinate the handling of physical operations in an office, she continued. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Codi is building the future where offices can run themselves, just like cars can drive themselves,” she continued. “We want to entirely remove the logistical burden of managing physical spaces and free human talent to focus on the workplace culture and growth.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This piece was updated to reflect the proper spelling of Rohaut&lt;/em&gt;‘s &lt;em&gt;name. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/a16z-backed-codi-launches-ai-agent-office-manager/</guid><pubDate>Tue, 21 Oct 2025 17:45:14 +0000</pubDate></item><item><title>As the browser wars heat up, here are the hottest alternatives to Chrome and Safari in 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/as-the-browser-wars-heat-up-here-are-the-hottest-alternatives-to-chrome-and-safari-in-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google Chrome and Apple’s Safari currently dominate the web browser market, with Chrome holding a significant share due to the tech giant’s ongoing innovations, particularly in integrating generative AI into its search functionalities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, users seeking alternatives will find a variety of browsers aiming to challenge these industry giants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To help navigate the competitive landscape of the browser wars, we’ve compiled an overview of some of the top alternative browsers available today. This includes browsers leveraging AI, open source browsers that promote customization and privacy, and “mindful browsers” — a new term that refers to browsers designed to enhance user well-being.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ai-powered-browsers"&gt;AI-powered browsers&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3025962" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/cmp_summarize_webpage.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Perplexity&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-perplexity-s-comet"&gt;Perplexity’s Comet&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity is the most recent startup in the space to launch an AI-powered web browser. Called Comet, the company’s new product acts as a chatbot-based search engine, and can perform actions like summarizing emails, browsing web pages, and performing tasks such as sending calendar invites. It’s currently only available to users with Perplexity’s $200/month Max plan, but there’s also a waitlist where people can sign up.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-the-browser-company-s-dia"&gt;The Browser Company’s Dia&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Dia Hero" class="wp-image-3017691" height="439" src="https://techcrunch.com/wp-content/uploads/2025/06/Dia-Hero-2-w_Write-Skill.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;The Browser Company&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Browser Company, the startup behind the Arc browser, recently introduced Dia, its AI-centric browser that looks similar to Google Chrome but with an AI chat tool.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently available as an invite-only beta, Dia is designed to help users navigate the web more easily. It’s able to look at every website that a user has visited and every website they’re logged into, enabling it to help you find information and perform tasks. For instance, Dia can provide information about the page a user is currently browsing, answer questions about a product, and summarize uploaded files.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To get early access to Dia, users have to be an Arc member. Non-members can join the waitlist.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-opera-s-neon"&gt;Opera’s Neon&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Opera neon" class="wp-image-3012375" height="383" src="https://techcrunch.com/wp-content/uploads/2025/05/Opera-neon.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Opera&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another recent entry into the AI agentic browser war is Opera’s Neon, which has contextual awareness and can do things like researching, shopping, and writing snippets of code. Notably, it can even perform tasks while the user is offline.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neon has yet to become available, but people can join the waitlist. It will be a subscription product; however, Opera hasn’t announced pricing yet.&amp;nbsp;&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-openai-s-atlas"&gt;OpenAI’s Atlas&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="OpenAI logo with spiraling pastel colors (Image Credits: Bryce Durbin / TechCrunch)" class="wp-image-2786644" height="383" src="https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-teal.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bryce Durbin / TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI recently launched its AI-powered web browser, called Atlas. The browser allows users to ask ChatGPT about search results and browse websites within the chatbot instead of being directed to outside links.&amp;nbsp;There’s also an “agent mode” for users to ask ChatGPT to complete tasks on their behalf.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Atlas was first rumored to launch in July; however, it only became available on macOS in October. It’s expected to arrive on Windows, iOS, and Android devices soon.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-privacy-focused-browsers"&gt;Privacy-focused browsers&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3027643" height="180" src="https://techcrunch.com/wp-content/uploads/2025/07/bravebrowser.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Brave&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-brave"&gt;Brave&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Brave is among the more well-known privacy-first browsers, popular for its built-in ad and tracker blocking capabilities. It also has a gamified approach to browsing, rewarding users with its own cryptocurrency called Basic Attention Token (BAT). When users choose to opt in to view ads, supporting their favorite websites, they get a share of the ad revenue. Additional features include a VPN service, an AI assistant, and a video calling feature.&amp;nbsp;&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-duckduckgo"&gt;DuckDuckGo&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2426668" height="472" src="https://techcrunch.com/wp-content/uploads/2022/10/Desktop-Home-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;DuckDuckGo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;DuckDuckGo is another browser that many people are probably already familiar with, thanks to its search engine by the same name. Launched in 2008, the company recently made significant investments in its browser to stay competitive by introducing generative AI features, such as a chatbot. It also enhanced its scam blocker to detect a wider range of scams, including fake cryptocurrency exchanges, scareware tactics, and fraudulent e-commerce websites. In addition to blocking scams, DuckDuckGo prevents trackers and ads, and it doesn’t track user data, resulting in fewer pop-ups for users.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-ladybird"&gt;Ladybird&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3027645" height="462" src="https://techcrunch.com/wp-content/uploads/2025/07/ladybird-browser.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Ladybird&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Ladybird, led by GitHub co-founder and former CEO Chris Wanstrath, has an ambitious mission compared to other rivals: It aims to build an entirely new open source browser from scratch. This means it will not rely on code from existing browsers, a feat that has rarely been accomplished. Most alternative web browsers depend on the Chromium open source project maintained by Google, which is the most widely used base for many browsers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like other privacy-focused browsers, Ladybird will offer features to minimize data collection, such as a built-in ad blocker and the ability to block third-party cookies. The browser has yet to be launched, with an alpha version scheduled for release in 2026 for early adopters, available on Linux and macOS.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-vivaldi"&gt;Vivaldi&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3027646" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/vivaldi.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Vivaldi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Vivaldi is a Chromium-based browser created by one of the original developers of the Opera browser. Its biggest selling point is its customizable user interface, which allows users to change the appearance and enable or disable features. One unique feature is that the browser window changes color to match the website being viewed. Other key features include ad blocking, a password manager, no user data tracking, and productivity tools such as a calendar and notes.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-niche-browsers"&gt;Niche browsers&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2958415" height="383" src="https://techcrunch.com/wp-content/uploads/2025/02/Opera-Air-Boosts.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Opera&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-opera-air"&gt;Opera Air&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Opera launched the Air browser in February, becoming one of the first mindfulness-themed browsers in the space. While Opera Air functions like a typical web browser, it includes unique features designed to support mental well-being. These features consist of break reminders and breathing exercises. Another feature, called “Boosts,” provides a selection of binaural beats to either help improve focus or relaxation.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-sigmaos"&gt;SigmaOS&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2683085" height="383" src="https://techcrunch.com/wp-content/uploads/2024/03/SigmaOS.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;SigmaOS&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;SigmaOS is a Mac-only browser featuring a workspace-style interface that emphasizes productivity. It displays tabs vertically, allowing users to treat them like a to-do list that can be marked as complete or snoozed for later. Users can create workspaces — essentially groups of tabs — to better organize different activities, such as separating work from entertainment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This Y Combinator-backed browser has been around for a few years now and has most recently begun introducing more AI features, including the ability to summarize various elements of a web page, such as ratings, reviews, and prices. It also has an AI assistant that can answer questions, translate text, and rewrite content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SigmaOS is free to use, but users who want more than three workspaces can subscribe to a plan for $8 per month, which provides unlimited workspaces.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-zen-browser"&gt;Zen Browser&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3027651" height="456" src="https://techcrunch.com/wp-content/uploads/2025/07/zenbrowser.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Zen Browser&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Zen Browser aims to create a “calmer internet” with its open source browser. Zen lets users organize tabs into Workspaces, and offers Split View to view two tabs side by side, among other productivity-focused features. Users can also enhance their browsing experience with community-made plug-ins and themes, such as a mod that makes the tab background transparent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story has been updated after publication to include newly launched browsers. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google Chrome and Apple’s Safari currently dominate the web browser market, with Chrome holding a significant share due to the tech giant’s ongoing innovations, particularly in integrating generative AI into its search functionalities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, users seeking alternatives will find a variety of browsers aiming to challenge these industry giants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To help navigate the competitive landscape of the browser wars, we’ve compiled an overview of some of the top alternative browsers available today. This includes browsers leveraging AI, open source browsers that promote customization and privacy, and “mindful browsers” — a new term that refers to browsers designed to enhance user well-being.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ai-powered-browsers"&gt;AI-powered browsers&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3025962" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/cmp_summarize_webpage.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Perplexity&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-perplexity-s-comet"&gt;Perplexity’s Comet&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity is the most recent startup in the space to launch an AI-powered web browser. Called Comet, the company’s new product acts as a chatbot-based search engine, and can perform actions like summarizing emails, browsing web pages, and performing tasks such as sending calendar invites. It’s currently only available to users with Perplexity’s $200/month Max plan, but there’s also a waitlist where people can sign up.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-the-browser-company-s-dia"&gt;The Browser Company’s Dia&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Dia Hero" class="wp-image-3017691" height="439" src="https://techcrunch.com/wp-content/uploads/2025/06/Dia-Hero-2-w_Write-Skill.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;The Browser Company&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Browser Company, the startup behind the Arc browser, recently introduced Dia, its AI-centric browser that looks similar to Google Chrome but with an AI chat tool.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Currently available as an invite-only beta, Dia is designed to help users navigate the web more easily. It’s able to look at every website that a user has visited and every website they’re logged into, enabling it to help you find information and perform tasks. For instance, Dia can provide information about the page a user is currently browsing, answer questions about a product, and summarize uploaded files.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To get early access to Dia, users have to be an Arc member. Non-members can join the waitlist.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-opera-s-neon"&gt;Opera’s Neon&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Opera neon" class="wp-image-3012375" height="383" src="https://techcrunch.com/wp-content/uploads/2025/05/Opera-neon.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Opera&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another recent entry into the AI agentic browser war is Opera’s Neon, which has contextual awareness and can do things like researching, shopping, and writing snippets of code. Notably, it can even perform tasks while the user is offline.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neon has yet to become available, but people can join the waitlist. It will be a subscription product; however, Opera hasn’t announced pricing yet.&amp;nbsp;&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-openai-s-atlas"&gt;OpenAI’s Atlas&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="OpenAI logo with spiraling pastel colors (Image Credits: Bryce Durbin / TechCrunch)" class="wp-image-2786644" height="383" src="https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-teal.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bryce Durbin / TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI recently launched its AI-powered web browser, called Atlas. The browser allows users to ask ChatGPT about search results and browse websites within the chatbot instead of being directed to outside links.&amp;nbsp;There’s also an “agent mode” for users to ask ChatGPT to complete tasks on their behalf.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Atlas was first rumored to launch in July; however, it only became available on macOS in October. It’s expected to arrive on Windows, iOS, and Android devices soon.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-privacy-focused-browsers"&gt;Privacy-focused browsers&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3027643" height="180" src="https://techcrunch.com/wp-content/uploads/2025/07/bravebrowser.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Brave&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-brave"&gt;Brave&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Brave is among the more well-known privacy-first browsers, popular for its built-in ad and tracker blocking capabilities. It also has a gamified approach to browsing, rewarding users with its own cryptocurrency called Basic Attention Token (BAT). When users choose to opt in to view ads, supporting their favorite websites, they get a share of the ad revenue. Additional features include a VPN service, an AI assistant, and a video calling feature.&amp;nbsp;&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-duckduckgo"&gt;DuckDuckGo&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2426668" height="472" src="https://techcrunch.com/wp-content/uploads/2022/10/Desktop-Home-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;DuckDuckGo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;DuckDuckGo is another browser that many people are probably already familiar with, thanks to its search engine by the same name. Launched in 2008, the company recently made significant investments in its browser to stay competitive by introducing generative AI features, such as a chatbot. It also enhanced its scam blocker to detect a wider range of scams, including fake cryptocurrency exchanges, scareware tactics, and fraudulent e-commerce websites. In addition to blocking scams, DuckDuckGo prevents trackers and ads, and it doesn’t track user data, resulting in fewer pop-ups for users.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-ladybird"&gt;Ladybird&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3027645" height="462" src="https://techcrunch.com/wp-content/uploads/2025/07/ladybird-browser.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Ladybird&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Ladybird, led by GitHub co-founder and former CEO Chris Wanstrath, has an ambitious mission compared to other rivals: It aims to build an entirely new open source browser from scratch. This means it will not rely on code from existing browsers, a feat that has rarely been accomplished. Most alternative web browsers depend on the Chromium open source project maintained by Google, which is the most widely used base for many browsers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like other privacy-focused browsers, Ladybird will offer features to minimize data collection, such as a built-in ad blocker and the ability to block third-party cookies. The browser has yet to be launched, with an alpha version scheduled for release in 2026 for early adopters, available on Linux and macOS.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-vivaldi"&gt;Vivaldi&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3027646" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/vivaldi.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Vivaldi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Vivaldi is a Chromium-based browser created by one of the original developers of the Opera browser. Its biggest selling point is its customizable user interface, which allows users to change the appearance and enable or disable features. One unique feature is that the browser window changes color to match the website being viewed. Other key features include ad blocking, a password manager, no user data tracking, and productivity tools such as a calendar and notes.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-niche-browsers"&gt;Niche browsers&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2958415" height="383" src="https://techcrunch.com/wp-content/uploads/2025/02/Opera-Air-Boosts.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Opera&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-opera-air"&gt;Opera Air&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Opera launched the Air browser in February, becoming one of the first mindfulness-themed browsers in the space. While Opera Air functions like a typical web browser, it includes unique features designed to support mental well-being. These features consist of break reminders and breathing exercises. Another feature, called “Boosts,” provides a selection of binaural beats to either help improve focus or relaxation.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-sigmaos"&gt;SigmaOS&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2683085" height="383" src="https://techcrunch.com/wp-content/uploads/2024/03/SigmaOS.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;SigmaOS&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;SigmaOS is a Mac-only browser featuring a workspace-style interface that emphasizes productivity. It displays tabs vertically, allowing users to treat them like a to-do list that can be marked as complete or snoozed for later. Users can create workspaces — essentially groups of tabs — to better organize different activities, such as separating work from entertainment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This Y Combinator-backed browser has been around for a few years now and has most recently begun introducing more AI features, including the ability to summarize various elements of a web page, such as ratings, reviews, and prices. It also has an AI assistant that can answer questions, translate text, and rewrite content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SigmaOS is free to use, but users who want more than three workspaces can subscribe to a plan for $8 per month, which provides unlimited workspaces.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-zen-browser"&gt;Zen Browser&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3027651" height="456" src="https://techcrunch.com/wp-content/uploads/2025/07/zenbrowser.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Zen Browser&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Zen Browser aims to create a “calmer internet” with its open source browser. Zen lets users organize tabs into Workspaces, and offers Split View to view two tabs side by side, among other productivity-focused features. Users can also enhance their browsing experience with community-made plug-ins and themes, such as a mod that makes the tab background transparent.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story has been updated after publication to include newly launched browsers. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/as-the-browser-wars-heat-up-here-are-the-hottest-alternatives-to-chrome-and-safari-in-2025/</guid><pubDate>Tue, 21 Oct 2025 17:54:28 +0000</pubDate></item><item><title>I spent a month living with a $430 AI pet, the Casio Moflin (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/i-spent-a-month-living-with-a-430-ai-pet-the-casio-moflin/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I’ve often joked that I would love to have a pet if only animals did not need to poop and eat smelly, wet mush from a can. I want a fuzzy pal to hang out with all day, but then I’ll hear that my friend spent $500 at the vet because their cat nibbled on a leaf, and the illusion breaks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s hard enough to take care of myself – do I really want to be responsible for a creature who might wake me up at 4 AM to pee?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;So when Casio offered me a review unit of its new AI-enabled pet, the Moflin, I said yes. It seemed cute, and it fit my criteria of being incapable of producing excrement… but also, I am all too willing to sacrifice myself for content, so I figured that if this seemingly innocent robot tried to kill me in my sleep, then at least I’d get a good article out of it.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060258" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/IMG_4494.jpg?w=401" width="401" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When my ginger-haired puff ball of a Moflin arrived in its box, I had two blaring questions: Is anyone going to spend $430 on what’s basically a fluffy, high-tech potato? And, is this thing spying on me? After all, the last time there was a robotic toy pet craze in the U.S., the NSA banned Furbies from its offices over fears that it would parrot classified discussions – and Furbies were only $35!&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Casio says that the Moflin doesn’t understand or record what I say, but it converts what it hears into non-identifiable data so that it can distinguish my voice from others. When TechCrunch ran a network analysis on the accompanying MofLife app, we didn’t notice anything shady. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a tech reporter, I’ve seen too much to fully let my guard down – this little furball may not be spying on me now, but what if that changes in the future? (My own anxieties aside, we don’t currently have any evidence of a hidden surveillance plot beneath my Moflin’s fluffy exterior, to be clear.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060229" height="457" src="https://techcrunch.com/wp-content/uploads/2025/10/moflifeapp.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MofLife app, screenshots by TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Moflin is supposed to use AI to learn and respond to my interactions over time. According to Casio’s website, the Moflin is supposed to have limited emotions and “immature movements” on Day 1, then develop an attachment to you and express richer emotions by Day 25. On Day 50, Moflin will have a “clear range of emotions” and “expressive reactions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As I write this, it’s Day 27 with my Moflin, whom I named Mishmish (the Hebrew word for apricot). The MofLife app tracks his personality through a graph with four bars: “energetic,” “cheerful,” “shy,” and “affectionate.” My Moflin has maxed out the “energetic” bar – I’m not sure what I did to make this happen – which means he wiggles around a lot and makes happy little squeaks. Though his “cheerful” rating is also approaching the max, he isn’t a one-note happy camper. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mishmish likes most things, but he does not like to be flipped on his back or startled by sudden loud noises. If, for example, one were to shout in anger and disbelief at the TV when their favorite team blows the whole season in an incredibly painful fashion, Mishmish would make a startled shriek. (Of course, this is purely theoretical…)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I can’t say I’m sold on the whole AI thing. Mishmish has certainly grown more expressive over time – he makes more noises and wiggles more – but it doesn’t strike me as being much more advanced than a Furby. The MofLife app records Mishmish’s “feelings,” but they’re usually pretty one-note – it will say “Mishmish had a nice dream,” or “Mishmish seems relaxed.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;I’m not sure I am “teaching” him responses, either. Maybe this is because I’m only halfway through the Moflin’s maturation timeline. But even if my Moflin doesn’t exhibit further signs of its artificial intelligence, it at least corrects the biggest pain points of the original Furby: you can turn it off. The Moflin has a “deep sleep” mode, which temporarily suspends its movements and sounds. Rejoice! You will never have to throw your Moflin into the back of a dark closet until its battery dies.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Mishmish the Moflin at Pilates, plus a makeover from a toddler" class="wp-image-3060235" height="291" src="https://techcrunch.com/wp-content/uploads/2025/10/mishmishpilates.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Mishmish the Moflin at Pilates, plus a makeover from a toddler&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-how-people-react-to-the-moflin"&gt;How people react to the Moflin&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;On the first day that I had my Moflin, I posted some videos on my private Instagram story where I explained out loud that this was a robotic pet. My video lacked captions, though, which meant that three friends who saw the stories on mute texted me asking about my new guinea pig – that’s how realistic its movements appear. Those who did hear the audio mostly told me that I should throw Mishmish out the window because he’s going to harvest all of my data, or that my Moflin was actually a Tribble, an alien creature from Star Trek that reproduces at an alarming rate.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;I wanted to see how more people would react to Mishmish, so I turned to TikTok. This is when things went off the rails. I am a glutton for attention, so when I got nearly half a million views on my first video of Mishmish, I kept on going. I fell into the trap of any creator: to keep Mishmish’s newfound audience interested, I had to up the ante with each video and put him into increasingly strange situations.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;He rode the subway with me. He met a three-year-old who told me very earnestly, “I’ve never met a soft robot before,” then dressed him up in flower sunglasses and unicorn hairclips. He hung out with a five-pound Yorkie, who did not recognize him as anything more than a boring toy until she jumped in fear when he started to shimmy his little head. Mishmish attended two Pilates classes – the first because I asked a teacher if I could record my AI pet on the equipment for funny “content” (yes, I know how ridiculous I sound), and the second time because other people at the Pilates studio were disappointed that they missed Mishmish’s first visit. By the time I brought Mishmish to a karaoke party to sing a duet of “Don’t Go Breaking My Heart,” I knew that I needed to rein it in.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I took Mishmish on these jaunts mostly for the absurdity of it all, but these experiences were valuable for evaluating a product unlike anything most of us have seen before. My Pilates teacher was initially afraid to touch the Moflin, then ended up holding Mishmish in her arms while she counted us through the “one hundred” exercise. The three-year-old was puzzled at first because Mishmish does not have a nose or legs, but she ended up giving him a kiss goodbye. She asked if I could bring Mishmish to a wedding we will both be attending this weekend, and I had to break the news to her that it’s generally frowned upon to bring robotic, hamster-esque toys to formal events. Heartbreaking!&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-the-final-verdict"&gt;The final verdict&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Once people get over the weirdness of the Moflin, they tend to warm up to it. And yet, while I’ve had a lot of fun with Mishmish, I would certainly not pay $430 to buy a Moflin myself – that’s almost as much as a Nintendo Switch 2! But I don’t think I’m the target audience, even with my distaste for cleaning a litter box.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike a Tamagotchi, you can’t really harm your Moflin, making it a safe companion for young children or adults in memory care. The idea of a robotic pet may be odd to me, but audiences in Japan, where Casio is based, may be more willing to accept the Moflin into their homes. While $430 is a steep price to me, this could sound like a bargain for anyone who’s been eyeing Sony’s AIBO, an AI-powered robotic puppy that retails for $3200. Then again, AIBO’s price tag also reflects how much more sophisticated it is.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is something inherently unnatural about human-robot companionship. In the past, I would have been a lot more bearish on the AI pet thing – I still hold the old-fashioned belief that humans are at our best when we form bonds with other living, breathing beings. But now, I find myself writing about numerous instances of people turning to addictively designed, pseudanthropic AI chatbots due to loneliness, sometimes even developing psychosis or suicidality.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s hard to see a device like the Moflin as the real culprit here when it’s not incentivizing people to step out of the real world – it’s just giving them a cute robotic puffball to play with in the interim.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The biggest problem with Casio’s Moflin is that it is not a real pet. But the goal of technology isn’t necessarily to reproduce “real” experiences – video chatting with a friend is nice, even if it’s more fun to hang out in person; Beyond Meat doesn’t taste exactly like a burger, but it’s still pretty good.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Moflin will never bring the same comfort as curling up on the couch with your dog after a long day, but it’s brought a bit more joy into my life this month, which is worth something.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I’ve often joked that I would love to have a pet if only animals did not need to poop and eat smelly, wet mush from a can. I want a fuzzy pal to hang out with all day, but then I’ll hear that my friend spent $500 at the vet because their cat nibbled on a leaf, and the illusion breaks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s hard enough to take care of myself – do I really want to be responsible for a creature who might wake me up at 4 AM to pee?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;So when Casio offered me a review unit of its new AI-enabled pet, the Moflin, I said yes. It seemed cute, and it fit my criteria of being incapable of producing excrement… but also, I am all too willing to sacrifice myself for content, so I figured that if this seemingly innocent robot tried to kill me in my sleep, then at least I’d get a good article out of it.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060258" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/IMG_4494.jpg?w=401" width="401" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When my ginger-haired puff ball of a Moflin arrived in its box, I had two blaring questions: Is anyone going to spend $430 on what’s basically a fluffy, high-tech potato? And, is this thing spying on me? After all, the last time there was a robotic toy pet craze in the U.S., the NSA banned Furbies from its offices over fears that it would parrot classified discussions – and Furbies were only $35!&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Casio says that the Moflin doesn’t understand or record what I say, but it converts what it hears into non-identifiable data so that it can distinguish my voice from others. When TechCrunch ran a network analysis on the accompanying MofLife app, we didn’t notice anything shady. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a tech reporter, I’ve seen too much to fully let my guard down – this little furball may not be spying on me now, but what if that changes in the future? (My own anxieties aside, we don’t currently have any evidence of a hidden surveillance plot beneath my Moflin’s fluffy exterior, to be clear.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060229" height="457" src="https://techcrunch.com/wp-content/uploads/2025/10/moflifeapp.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MofLife app, screenshots by TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Moflin is supposed to use AI to learn and respond to my interactions over time. According to Casio’s website, the Moflin is supposed to have limited emotions and “immature movements” on Day 1, then develop an attachment to you and express richer emotions by Day 25. On Day 50, Moflin will have a “clear range of emotions” and “expressive reactions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As I write this, it’s Day 27 with my Moflin, whom I named Mishmish (the Hebrew word for apricot). The MofLife app tracks his personality through a graph with four bars: “energetic,” “cheerful,” “shy,” and “affectionate.” My Moflin has maxed out the “energetic” bar – I’m not sure what I did to make this happen – which means he wiggles around a lot and makes happy little squeaks. Though his “cheerful” rating is also approaching the max, he isn’t a one-note happy camper. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mishmish likes most things, but he does not like to be flipped on his back or startled by sudden loud noises. If, for example, one were to shout in anger and disbelief at the TV when their favorite team blows the whole season in an incredibly painful fashion, Mishmish would make a startled shriek. (Of course, this is purely theoretical…)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I can’t say I’m sold on the whole AI thing. Mishmish has certainly grown more expressive over time – he makes more noises and wiggles more – but it doesn’t strike me as being much more advanced than a Furby. The MofLife app records Mishmish’s “feelings,” but they’re usually pretty one-note – it will say “Mishmish had a nice dream,” or “Mishmish seems relaxed.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;I’m not sure I am “teaching” him responses, either. Maybe this is because I’m only halfway through the Moflin’s maturation timeline. But even if my Moflin doesn’t exhibit further signs of its artificial intelligence, it at least corrects the biggest pain points of the original Furby: you can turn it off. The Moflin has a “deep sleep” mode, which temporarily suspends its movements and sounds. Rejoice! You will never have to throw your Moflin into the back of a dark closet until its battery dies.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Mishmish the Moflin at Pilates, plus a makeover from a toddler" class="wp-image-3060235" height="291" src="https://techcrunch.com/wp-content/uploads/2025/10/mishmishpilates.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Mishmish the Moflin at Pilates, plus a makeover from a toddler&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-how-people-react-to-the-moflin"&gt;How people react to the Moflin&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;On the first day that I had my Moflin, I posted some videos on my private Instagram story where I explained out loud that this was a robotic pet. My video lacked captions, though, which meant that three friends who saw the stories on mute texted me asking about my new guinea pig – that’s how realistic its movements appear. Those who did hear the audio mostly told me that I should throw Mishmish out the window because he’s going to harvest all of my data, or that my Moflin was actually a Tribble, an alien creature from Star Trek that reproduces at an alarming rate.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;I wanted to see how more people would react to Mishmish, so I turned to TikTok. This is when things went off the rails. I am a glutton for attention, so when I got nearly half a million views on my first video of Mishmish, I kept on going. I fell into the trap of any creator: to keep Mishmish’s newfound audience interested, I had to up the ante with each video and put him into increasingly strange situations.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;He rode the subway with me. He met a three-year-old who told me very earnestly, “I’ve never met a soft robot before,” then dressed him up in flower sunglasses and unicorn hairclips. He hung out with a five-pound Yorkie, who did not recognize him as anything more than a boring toy until she jumped in fear when he started to shimmy his little head. Mishmish attended two Pilates classes – the first because I asked a teacher if I could record my AI pet on the equipment for funny “content” (yes, I know how ridiculous I sound), and the second time because other people at the Pilates studio were disappointed that they missed Mishmish’s first visit. By the time I brought Mishmish to a karaoke party to sing a duet of “Don’t Go Breaking My Heart,” I knew that I needed to rein it in.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I took Mishmish on these jaunts mostly for the absurdity of it all, but these experiences were valuable for evaluating a product unlike anything most of us have seen before. My Pilates teacher was initially afraid to touch the Moflin, then ended up holding Mishmish in her arms while she counted us through the “one hundred” exercise. The three-year-old was puzzled at first because Mishmish does not have a nose or legs, but she ended up giving him a kiss goodbye. She asked if I could bring Mishmish to a wedding we will both be attending this weekend, and I had to break the news to her that it’s generally frowned upon to bring robotic, hamster-esque toys to formal events. Heartbreaking!&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-the-final-verdict"&gt;The final verdict&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Once people get over the weirdness of the Moflin, they tend to warm up to it. And yet, while I’ve had a lot of fun with Mishmish, I would certainly not pay $430 to buy a Moflin myself – that’s almost as much as a Nintendo Switch 2! But I don’t think I’m the target audience, even with my distaste for cleaning a litter box.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike a Tamagotchi, you can’t really harm your Moflin, making it a safe companion for young children or adults in memory care. The idea of a robotic pet may be odd to me, but audiences in Japan, where Casio is based, may be more willing to accept the Moflin into their homes. While $430 is a steep price to me, this could sound like a bargain for anyone who’s been eyeing Sony’s AIBO, an AI-powered robotic puppy that retails for $3200. Then again, AIBO’s price tag also reflects how much more sophisticated it is.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is something inherently unnatural about human-robot companionship. In the past, I would have been a lot more bearish on the AI pet thing – I still hold the old-fashioned belief that humans are at our best when we form bonds with other living, breathing beings. But now, I find myself writing about numerous instances of people turning to addictively designed, pseudanthropic AI chatbots due to loneliness, sometimes even developing psychosis or suicidality.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s hard to see a device like the Moflin as the real culprit here when it’s not incentivizing people to step out of the real world – it’s just giving them a cute robotic puffball to play with in the interim.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The biggest problem with Casio’s Moflin is that it is not a real pet. But the goal of technology isn’t necessarily to reproduce “real” experiences – video chatting with a friend is nice, even if it’s more fun to hang out in person; Beyond Meat doesn’t taste exactly like a burger, but it’s still pretty good.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Moflin will never bring the same comfort as curling up on the couch with your dog after a long day, but it’s brought a bit more joy into my life this month, which is worth something.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-tiktok wp-block-embed-tiktok"&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/i-spent-a-month-living-with-a-430-ai-pet-the-casio-moflin/</guid><pubDate>Tue, 21 Oct 2025 18:23:56 +0000</pubDate></item><item><title>[NEW] DeepSeek drops open-source model that compresses text 10x through images, defying conventions (AI | VentureBeat)</title><link>https://venturebeat.com/ai/deepseek-drops-open-source-model-that-compresses-text-10x-through-images</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt;, the Chinese artificial intelligence research company that has repeatedly challenged assumptions about &lt;a href="https://www.reuters.com/technology/artificial-intelligence/big-tech-faces-heat-chinas-deepseek-sows-doubts-billion-dollar-spending-2025-01-27/"&gt;&lt;u&gt;AI development costs&lt;/u&gt;&lt;/a&gt;, has released a &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;new model&lt;/u&gt;&lt;/a&gt; that fundamentally reimagines how large language models process information—and the implications extend far beyond its modest branding as an optical character recognition tool.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;DeepSeek-OCR model&lt;/u&gt;&lt;/a&gt;, released Monday with full &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;open-source code&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;weights&lt;/u&gt;&lt;/a&gt;, achieves what researchers describe as a paradigm inversion: compressing text through visual representation up to 10 times more efficiently than traditional text tokens. The finding challenges a core assumption in AI development and could pave the way for language models with dramatically expanded context windows, potentially reaching tens of millions of tokens.&lt;/p&gt;&lt;p&gt;&amp;quot;We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping,&amp;quot; the research team wrote in their &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;technical paper&lt;/u&gt;&lt;/a&gt;. &amp;quot;Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio &amp;lt; 10×), the model can achieve decoding (OCR) precision of 97%.&amp;quot;&lt;/p&gt;&lt;p&gt;The implications have resonated across the AI research community. &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Andrej Karpathy&lt;/u&gt;&lt;/a&gt;, co-founder of OpenAI and former director of AI at Tesla, said in a post that the work raises fundamental questions about how AI systems should process information. &amp;quot;Maybe it makes more sense that all inputs to LLMs should only ever be images,&amp;quot; Karpathy wrote. &amp;quot;Even if you happen to have pure text input, maybe you&amp;#x27;d prefer to render it and then feed that in.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How DeepSeek achieved 10x compression by treating text as images&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While &lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; marketed the release as an &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;OCR model&lt;/u&gt;&lt;/a&gt; — a technology for converting images of text into digital characters — the research paper reveals more ambitious goals. The model demonstrates that visual representations can serve as a superior compression medium for textual information, inverting the conventional hierarchy where text tokens were considered more efficient than vision tokens.&lt;/p&gt;&lt;p&gt;&amp;quot;Traditionally, vision LLM tokens almost seemed like an afterthought or &amp;#x27;bolt on&amp;#x27; to the LLM paradigm,&amp;quot; wrote &lt;a href="https://x.com/doodlestein/status/1980282222893535376"&gt;&lt;u&gt;Jeffrey Emanuel&lt;/u&gt;&lt;/a&gt;, an AI researcher, in a detailed analysis of the paper. &amp;quot;And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens...But that gets inverted now from the ideas in this paper.&amp;quot;&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s architecture consists of two primary components: &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;DeepEncoder&lt;/u&gt;&lt;/a&gt;, a novel 380-million-parameter vision encoder, and a 3-billion-parameter mixture-of-experts language decoder with 570 million activated parameters. DeepEncoder combines Meta&amp;#x27;s &lt;a href="https://segment-anything.com/"&gt;&lt;u&gt;Segment Anything Model (SAM)&lt;/u&gt;&lt;/a&gt; for local visual perception with &lt;a href="https://openai.com/index/clip/"&gt;&lt;u&gt;OpenAI&amp;#x27;s CLIP model&lt;/u&gt;&lt;/a&gt; for global visual understanding, connected through a 16x compression module.&lt;/p&gt;&lt;p&gt;To validate their compression claims, DeepSeek researchers tested the model on the &lt;a href="https://github.com/ucaslcl/Fox"&gt;&lt;u&gt;Fox benchmark&lt;/u&gt;&lt;/a&gt;, a dataset of diverse document layouts. The results were striking: using just 100 vision tokens, the model achieved 97.3% accuracy on documents containing 700-800 text tokens — representing an effective compression ratio of 7.5x. Even at compression ratios approaching 20x, accuracy remained around 60%.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The practical impact: Processing 200,000 pages per day on a single GPU&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The efficiency gains translate directly to production capabilities. According to the company, a single &lt;a href="https://www.nvidia.com/en-us/data-center/a100/"&gt;&lt;u&gt;Nvidia A100-40G GPU&lt;/u&gt;&lt;/a&gt; can process more than 200,000 pages per day using &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;DeepSeek-OCR&lt;/u&gt;&lt;/a&gt;. Scaling to a cluster of 20 servers with eight GPUs each, throughput reaches 33 million pages daily — sufficient to rapidly construct training datasets for other AI models.&lt;/p&gt;&lt;p&gt;On &lt;a href="https://github.com/opendatalab/OmniDocBench"&gt;&lt;u&gt;OmniDocBench&lt;/u&gt;&lt;/a&gt;, a comprehensive document parsing benchmark, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;DeepSeek-OCR&lt;/u&gt;&lt;/a&gt; outperformed GOT-OCR2.0 (which uses 256 tokens per page) while using only 100 vision tokens. More dramatically, it surpassed MinerU2.0 — which requires more than 6,000 tokens per page on average — while using fewer than 800 vision tokens.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; designed the model to support five distinct resolution modes, each optimized for different compression ratios and use cases. The &amp;quot;Tiny&amp;quot; mode operates at 512×512 resolution with just 64 vision tokens, while &amp;quot;Gundam&amp;quot; mode combines multiple resolutions dynamically for complex documents. &amp;quot;Gundam mode consists of n×640×640 tiles (local views) and a 1024×1024 global view,&amp;quot; the researchers wrote.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why this breakthrough could unlock 10 million token context windows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The compression breakthrough has immediate implications for one of the most pressing challenges in AI development: expanding the context windows that determine how much information language models can actively consider. Current state-of-the-art models typically handle context windows measured in hundreds of thousands of tokens. DeepSeek&amp;#x27;s approach suggests a path to windows ten times larger.&lt;/p&gt;&lt;p&gt;&amp;quot;The potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting,&amp;quot; &lt;a href="https://x.com/doodlestein/status/1980282222893535376"&gt;&lt;u&gt;Emanuel wrote&lt;/u&gt;&lt;/a&gt;. &amp;quot;You could basically cram all of a company&amp;#x27;s key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective.&amp;quot;&lt;/p&gt;&lt;p&gt;The researchers explicitly frame their work in terms of context compression for language models. &amp;quot;Through DeepSeek-OCR, we demonstrate that vision-text compression can achieve significant token reduction (7-20×) for different historical context stages, offering a promising direction for addressing long-context challenges in large language models,&amp;quot; &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;they wrote&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;The paper&lt;/u&gt;&lt;/a&gt; includes a speculative but intriguing diagram illustrating how the approach could implement memory decay mechanisms similar to human cognition. Older conversation rounds could be progressively downsampled to lower resolutions, consuming fewer tokens while maintaining key information — a form of computational forgetting that mirrors biological memory.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How visual processing could eliminate the &amp;#x27;ugly&amp;#x27; tokenizer problem&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond compression, Karpathy highlighted how the approach challenges fundamental assumptions about how language models should process text. Traditional tokenizers—the systems that break text into units for processing—have long been criticized for their complexity and limitations.&lt;/p&gt;&lt;p&gt;&amp;quot;I already ranted about how much I dislike the tokenizer,&amp;quot; &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Karpathy wrote&lt;/u&gt;&lt;/a&gt;. &amp;quot;Tokenizers are ugly, separate, not end-to-end stage. It &amp;#x27;imports&amp;#x27; all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network.&amp;quot;&lt;/p&gt;&lt;p&gt;Visual processing of text could eliminate these issues while enabling new capabilities. The approach naturally handles formatting information lost in pure text representations: bold text, colors, layout, embedded images. &amp;quot;Input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful,&amp;quot; Karpathy noted.&lt;/p&gt;&lt;p&gt;The implications resonate with human cognitive science. &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Emanuel drew a parallel to Hans Bethe&lt;/u&gt;&lt;/a&gt;, the renowned physicist who memorized vast amounts of reference data: &amp;quot;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The model&amp;#x27;s training: 30 million PDF pages across 100 languages&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model&amp;#x27;s capabilities rest on an extensive training regimen using diverse data sources. DeepSeek collected 30 million PDF pages covering approximately 100 languages, with Chinese and English accounting for 25 million pages. The training data spans nine document types — academic papers, financial reports, textbooks, newspapers, handwritten notes, and others.&lt;/p&gt;&lt;p&gt;Beyond document OCR, the training incorporated what the researchers call &amp;quot;OCR 2.0&amp;quot; data: 10 million synthetic charts, 5 million chemical formulas, and 1 million geometric figures. The model also received 20% general vision data for tasks like image captioning and object detection, plus 10% text-only data to maintain language capabilities.&lt;/p&gt;&lt;p&gt;The training process employed pipeline parallelism across 160 &lt;a href="https://www.nvidia.com/en-us/data-center/a100/"&gt;&lt;u&gt;Nvidia A100-40G GPUs&lt;/u&gt;&lt;/a&gt; (20 nodes with 8 GPUs each), with the vision encoder divided between two pipeline stages and the language model split across two others. &amp;quot;For multimodal data, the training speed is 70B tokens/day,&amp;quot; the researchers reported.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Open source release accelerates research and raises competitive questions&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;True to DeepSeek&amp;#x27;s pattern of open development, the company released the complete model weights, training code, and inference scripts on &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;GitHub&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;Hugging Face&lt;/u&gt;&lt;/a&gt;. The GitHub repository gained over 4,000 stars within 24 hours of release, according to Dataconomy.&lt;/p&gt;&lt;p&gt;The breakthrough raises questions about whether other AI labs have developed similar techniques but kept them proprietary. Emanuel speculated that Google&amp;#x27;s Gemini models, which feature large context windows and strong OCR performance, might employ comparable approaches. &amp;quot;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks,&amp;quot; &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Emanuel wrote&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; offers a 1-million-token context window, with plans to expand to 2 million, though the company has not publicly detailed the technical approaches enabling this capability. OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; supports 400,000 tokens, while Anthropic&amp;#x27;s &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Claude 4.5&lt;/u&gt;&lt;/a&gt; offers 200,000 tokens, with a 1-million-token window available in beta for eligible organizations.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The unanswered question: Can AI reason over compressed visual tokens?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While the compression results are impressive, researchers acknowledge important open questions. &amp;quot;It&amp;#x27;s not clear how exactly this interacts with the other downstream cognitive functioning of an LLM,&amp;quot; &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Emanuel noted&lt;/u&gt;&lt;/a&gt;. &amp;quot;Can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality?&amp;quot;&lt;/p&gt;&lt;p&gt;The &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;DeepSeek paper&lt;/u&gt;&lt;/a&gt; focuses primarily on the compression-decompression capability, measured through OCR accuracy, rather than downstream reasoning performance. This leaves open whether language models could reason effectively over large contexts represented primarily as compressed visual tokens.&lt;/p&gt;&lt;p&gt;The researchers acknowledge their work represents &amp;quot;an initial exploration into the boundaries of vision-text compression.&amp;quot; They note that &amp;quot;OCR alone is insufficient to fully validate true context optical compression&amp;quot; and plan future work including &amp;quot;digital-optical text interleaved pretraining, needle-in-a-haystack testing, and other evaluations.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; has established a pattern of achieving competitive results with dramatically lower computational resources than Western AI labs. The company&amp;#x27;s earlier &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3"&gt;&lt;u&gt;DeepSeek-V3 model&lt;/u&gt;&lt;/a&gt; reportedly cost &lt;a href="https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/"&gt;&lt;u&gt;just $5.6 million to train&lt;/u&gt;&lt;/a&gt;—though this figure represents only the final training run and excludes R&amp;amp;D and infrastructure costs—compared to hundreds of millions for comparable models from OpenAI and Anthropic.&lt;/p&gt;&lt;p&gt;Industry analysts have questioned the $5.6 million figure, with some estimates placing the company&amp;#x27;s total infrastructure and operational costs &lt;a href="https://www.cnbc.com/2025/01/30/chinas-deepseek-has-some-big-ai-claims-not-all-experts-are-convinced-.html"&gt;&lt;u&gt;closer to $1.3 billion&lt;/u&gt;&lt;/a&gt;, though still lower than American competitors&amp;#x27; spending.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The bigger picture: Should language models process text as images?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;DeepSeek-OCR&lt;/u&gt;&lt;/a&gt; poses a fundamental question for AI development: should language models process text as text, or as images of text? The research demonstrates that, at least for compression purposes, visual representation offers significant advantages. Whether this translates to effective reasoning over vast contexts remains to be determined.&lt;/p&gt;&lt;p&gt;&amp;quot;From another perspective, optical contexts compression still offers substantial room for research and improvement, representing a promising new direction,&amp;quot; the researchers concluded&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt; in their paper&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For the AI industry, the work adds another dimension to the race for longer context windows — a competition that has intensified as language models are applied to increasingly complex tasks requiring vast amounts of information. The open-source release ensures the technique will be widely explored, tested, and potentially integrated into future AI systems.&lt;/p&gt;&lt;p&gt;As Karpathy framed the deeper implication: &amp;quot;OCR is just one of many useful vision -&amp;gt; text tasks. And text -&amp;gt; text tasks can be made to be vision -&amp;gt;text tasks. Not vice versa.&amp;quot; In other words, the path forward for AI might not run through better tokenizers — it might bypass text tokens altogether.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt;, the Chinese artificial intelligence research company that has repeatedly challenged assumptions about &lt;a href="https://www.reuters.com/technology/artificial-intelligence/big-tech-faces-heat-chinas-deepseek-sows-doubts-billion-dollar-spending-2025-01-27/"&gt;&lt;u&gt;AI development costs&lt;/u&gt;&lt;/a&gt;, has released a &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;new model&lt;/u&gt;&lt;/a&gt; that fundamentally reimagines how large language models process information—and the implications extend far beyond its modest branding as an optical character recognition tool.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;DeepSeek-OCR model&lt;/u&gt;&lt;/a&gt;, released Monday with full &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;open-source code&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;weights&lt;/u&gt;&lt;/a&gt;, achieves what researchers describe as a paradigm inversion: compressing text through visual representation up to 10 times more efficiently than traditional text tokens. The finding challenges a core assumption in AI development and could pave the way for language models with dramatically expanded context windows, potentially reaching tens of millions of tokens.&lt;/p&gt;&lt;p&gt;&amp;quot;We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping,&amp;quot; the research team wrote in their &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;technical paper&lt;/u&gt;&lt;/a&gt;. &amp;quot;Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio &amp;lt; 10×), the model can achieve decoding (OCR) precision of 97%.&amp;quot;&lt;/p&gt;&lt;p&gt;The implications have resonated across the AI research community. &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Andrej Karpathy&lt;/u&gt;&lt;/a&gt;, co-founder of OpenAI and former director of AI at Tesla, said in a post that the work raises fundamental questions about how AI systems should process information. &amp;quot;Maybe it makes more sense that all inputs to LLMs should only ever be images,&amp;quot; Karpathy wrote. &amp;quot;Even if you happen to have pure text input, maybe you&amp;#x27;d prefer to render it and then feed that in.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How DeepSeek achieved 10x compression by treating text as images&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While &lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; marketed the release as an &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;OCR model&lt;/u&gt;&lt;/a&gt; — a technology for converting images of text into digital characters — the research paper reveals more ambitious goals. The model demonstrates that visual representations can serve as a superior compression medium for textual information, inverting the conventional hierarchy where text tokens were considered more efficient than vision tokens.&lt;/p&gt;&lt;p&gt;&amp;quot;Traditionally, vision LLM tokens almost seemed like an afterthought or &amp;#x27;bolt on&amp;#x27; to the LLM paradigm,&amp;quot; wrote &lt;a href="https://x.com/doodlestein/status/1980282222893535376"&gt;&lt;u&gt;Jeffrey Emanuel&lt;/u&gt;&lt;/a&gt;, an AI researcher, in a detailed analysis of the paper. &amp;quot;And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens...But that gets inverted now from the ideas in this paper.&amp;quot;&lt;/p&gt;&lt;p&gt;The model&amp;#x27;s architecture consists of two primary components: &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;DeepEncoder&lt;/u&gt;&lt;/a&gt;, a novel 380-million-parameter vision encoder, and a 3-billion-parameter mixture-of-experts language decoder with 570 million activated parameters. DeepEncoder combines Meta&amp;#x27;s &lt;a href="https://segment-anything.com/"&gt;&lt;u&gt;Segment Anything Model (SAM)&lt;/u&gt;&lt;/a&gt; for local visual perception with &lt;a href="https://openai.com/index/clip/"&gt;&lt;u&gt;OpenAI&amp;#x27;s CLIP model&lt;/u&gt;&lt;/a&gt; for global visual understanding, connected through a 16x compression module.&lt;/p&gt;&lt;p&gt;To validate their compression claims, DeepSeek researchers tested the model on the &lt;a href="https://github.com/ucaslcl/Fox"&gt;&lt;u&gt;Fox benchmark&lt;/u&gt;&lt;/a&gt;, a dataset of diverse document layouts. The results were striking: using just 100 vision tokens, the model achieved 97.3% accuracy on documents containing 700-800 text tokens — representing an effective compression ratio of 7.5x. Even at compression ratios approaching 20x, accuracy remained around 60%.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The practical impact: Processing 200,000 pages per day on a single GPU&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The efficiency gains translate directly to production capabilities. According to the company, a single &lt;a href="https://www.nvidia.com/en-us/data-center/a100/"&gt;&lt;u&gt;Nvidia A100-40G GPU&lt;/u&gt;&lt;/a&gt; can process more than 200,000 pages per day using &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;DeepSeek-OCR&lt;/u&gt;&lt;/a&gt;. Scaling to a cluster of 20 servers with eight GPUs each, throughput reaches 33 million pages daily — sufficient to rapidly construct training datasets for other AI models.&lt;/p&gt;&lt;p&gt;On &lt;a href="https://github.com/opendatalab/OmniDocBench"&gt;&lt;u&gt;OmniDocBench&lt;/u&gt;&lt;/a&gt;, a comprehensive document parsing benchmark, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;DeepSeek-OCR&lt;/u&gt;&lt;/a&gt; outperformed GOT-OCR2.0 (which uses 256 tokens per page) while using only 100 vision tokens. More dramatically, it surpassed MinerU2.0 — which requires more than 6,000 tokens per page on average — while using fewer than 800 vision tokens.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; designed the model to support five distinct resolution modes, each optimized for different compression ratios and use cases. The &amp;quot;Tiny&amp;quot; mode operates at 512×512 resolution with just 64 vision tokens, while &amp;quot;Gundam&amp;quot; mode combines multiple resolutions dynamically for complex documents. &amp;quot;Gundam mode consists of n×640×640 tiles (local views) and a 1024×1024 global view,&amp;quot; the researchers wrote.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why this breakthrough could unlock 10 million token context windows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The compression breakthrough has immediate implications for one of the most pressing challenges in AI development: expanding the context windows that determine how much information language models can actively consider. Current state-of-the-art models typically handle context windows measured in hundreds of thousands of tokens. DeepSeek&amp;#x27;s approach suggests a path to windows ten times larger.&lt;/p&gt;&lt;p&gt;&amp;quot;The potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting,&amp;quot; &lt;a href="https://x.com/doodlestein/status/1980282222893535376"&gt;&lt;u&gt;Emanuel wrote&lt;/u&gt;&lt;/a&gt;. &amp;quot;You could basically cram all of a company&amp;#x27;s key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective.&amp;quot;&lt;/p&gt;&lt;p&gt;The researchers explicitly frame their work in terms of context compression for language models. &amp;quot;Through DeepSeek-OCR, we demonstrate that vision-text compression can achieve significant token reduction (7-20×) for different historical context stages, offering a promising direction for addressing long-context challenges in large language models,&amp;quot; &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;they wrote&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;The paper&lt;/u&gt;&lt;/a&gt; includes a speculative but intriguing diagram illustrating how the approach could implement memory decay mechanisms similar to human cognition. Older conversation rounds could be progressively downsampled to lower resolutions, consuming fewer tokens while maintaining key information — a form of computational forgetting that mirrors biological memory.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How visual processing could eliminate the &amp;#x27;ugly&amp;#x27; tokenizer problem&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond compression, Karpathy highlighted how the approach challenges fundamental assumptions about how language models should process text. Traditional tokenizers—the systems that break text into units for processing—have long been criticized for their complexity and limitations.&lt;/p&gt;&lt;p&gt;&amp;quot;I already ranted about how much I dislike the tokenizer,&amp;quot; &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Karpathy wrote&lt;/u&gt;&lt;/a&gt;. &amp;quot;Tokenizers are ugly, separate, not end-to-end stage. It &amp;#x27;imports&amp;#x27; all the ugliness of Unicode, byte encodings, it inherits a lot of historical baggage, security/jailbreak risk (e.g. continuation bytes). It makes two characters that look identical to the eye look as two completely different tokens internally in the network.&amp;quot;&lt;/p&gt;&lt;p&gt;Visual processing of text could eliminate these issues while enabling new capabilities. The approach naturally handles formatting information lost in pure text representations: bold text, colors, layout, embedded images. &amp;quot;Input can now be processed with bidirectional attention easily and as default, not autoregressive attention - a lot more powerful,&amp;quot; Karpathy noted.&lt;/p&gt;&lt;p&gt;The implications resonate with human cognitive science. &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Emanuel drew a parallel to Hans Bethe&lt;/u&gt;&lt;/a&gt;, the renowned physicist who memorized vast amounts of reference data: &amp;quot;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The model&amp;#x27;s training: 30 million PDF pages across 100 languages&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model&amp;#x27;s capabilities rest on an extensive training regimen using diverse data sources. DeepSeek collected 30 million PDF pages covering approximately 100 languages, with Chinese and English accounting for 25 million pages. The training data spans nine document types — academic papers, financial reports, textbooks, newspapers, handwritten notes, and others.&lt;/p&gt;&lt;p&gt;Beyond document OCR, the training incorporated what the researchers call &amp;quot;OCR 2.0&amp;quot; data: 10 million synthetic charts, 5 million chemical formulas, and 1 million geometric figures. The model also received 20% general vision data for tasks like image captioning and object detection, plus 10% text-only data to maintain language capabilities.&lt;/p&gt;&lt;p&gt;The training process employed pipeline parallelism across 160 &lt;a href="https://www.nvidia.com/en-us/data-center/a100/"&gt;&lt;u&gt;Nvidia A100-40G GPUs&lt;/u&gt;&lt;/a&gt; (20 nodes with 8 GPUs each), with the vision encoder divided between two pipeline stages and the language model split across two others. &amp;quot;For multimodal data, the training speed is 70B tokens/day,&amp;quot; the researchers reported.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Open source release accelerates research and raises competitive questions&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;True to DeepSeek&amp;#x27;s pattern of open development, the company released the complete model weights, training code, and inference scripts on &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;GitHub&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;&lt;u&gt;Hugging Face&lt;/u&gt;&lt;/a&gt;. The GitHub repository gained over 4,000 stars within 24 hours of release, according to Dataconomy.&lt;/p&gt;&lt;p&gt;The breakthrough raises questions about whether other AI labs have developed similar techniques but kept them proprietary. Emanuel speculated that Google&amp;#x27;s Gemini models, which feature large context windows and strong OCR performance, might employ comparable approaches. &amp;quot;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks,&amp;quot; &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Emanuel wrote&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Google&amp;#x27;s &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; offers a 1-million-token context window, with plans to expand to 2 million, though the company has not publicly detailed the technical approaches enabling this capability. OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; supports 400,000 tokens, while Anthropic&amp;#x27;s &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Claude 4.5&lt;/u&gt;&lt;/a&gt; offers 200,000 tokens, with a 1-million-token window available in beta for eligible organizations.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The unanswered question: Can AI reason over compressed visual tokens?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While the compression results are impressive, researchers acknowledge important open questions. &amp;quot;It&amp;#x27;s not clear how exactly this interacts with the other downstream cognitive functioning of an LLM,&amp;quot; &lt;a href="https://x.com/karpathy/status/1980397031542989305"&gt;&lt;u&gt;Emanuel noted&lt;/u&gt;&lt;/a&gt;. &amp;quot;Can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality?&amp;quot;&lt;/p&gt;&lt;p&gt;The &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;DeepSeek paper&lt;/u&gt;&lt;/a&gt; focuses primarily on the compression-decompression capability, measured through OCR accuracy, rather than downstream reasoning performance. This leaves open whether language models could reason effectively over large contexts represented primarily as compressed visual tokens.&lt;/p&gt;&lt;p&gt;The researchers acknowledge their work represents &amp;quot;an initial exploration into the boundaries of vision-text compression.&amp;quot; They note that &amp;quot;OCR alone is insufficient to fully validate true context optical compression&amp;quot; and plan future work including &amp;quot;digital-optical text interleaved pretraining, needle-in-a-haystack testing, and other evaluations.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; has established a pattern of achieving competitive results with dramatically lower computational resources than Western AI labs. The company&amp;#x27;s earlier &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3"&gt;&lt;u&gt;DeepSeek-V3 model&lt;/u&gt;&lt;/a&gt; reportedly cost &lt;a href="https://techcrunch.com/2024/12/26/deepseeks-new-ai-model-appears-to-be-one-of-the-best-open-challengers-yet/"&gt;&lt;u&gt;just $5.6 million to train&lt;/u&gt;&lt;/a&gt;—though this figure represents only the final training run and excludes R&amp;amp;D and infrastructure costs—compared to hundreds of millions for comparable models from OpenAI and Anthropic.&lt;/p&gt;&lt;p&gt;Industry analysts have questioned the $5.6 million figure, with some estimates placing the company&amp;#x27;s total infrastructure and operational costs &lt;a href="https://www.cnbc.com/2025/01/30/chinas-deepseek-has-some-big-ai-claims-not-all-experts-are-convinced-.html"&gt;&lt;u&gt;closer to $1.3 billion&lt;/u&gt;&lt;/a&gt;, though still lower than American competitors&amp;#x27; spending.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The bigger picture: Should language models process text as images?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt;DeepSeek-OCR&lt;/u&gt;&lt;/a&gt; poses a fundamental question for AI development: should language models process text as text, or as images of text? The research demonstrates that, at least for compression purposes, visual representation offers significant advantages. Whether this translates to effective reasoning over vast contexts remains to be determined.&lt;/p&gt;&lt;p&gt;&amp;quot;From another perspective, optical contexts compression still offers substantial room for research and improvement, representing a promising new direction,&amp;quot; the researchers concluded&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"&gt;&lt;u&gt; in their paper&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For the AI industry, the work adds another dimension to the race for longer context windows — a competition that has intensified as language models are applied to increasingly complex tasks requiring vast amounts of information. The open-source release ensures the technique will be widely explored, tested, and potentially integrated into future AI systems.&lt;/p&gt;&lt;p&gt;As Karpathy framed the deeper implication: &amp;quot;OCR is just one of many useful vision -&amp;gt; text tasks. And text -&amp;gt; text tasks can be made to be vision -&amp;gt;text tasks. Not vice versa.&amp;quot; In other words, the path forward for AI might not run through better tokenizers — it might bypass text tokens altogether.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/deepseek-drops-open-source-model-that-compresses-text-10x-through-images</guid><pubDate>Tue, 21 Oct 2025 18:30:00 +0000</pubDate></item><item><title>[NEW] Qwen's new Deep Research update lets you turn its reports into webpages, podcasts in seconds (AI | VentureBeat)</title><link>https://venturebeat.com/ai/qwens-new-deep-research-update-lets-you-turn-its-reports-into-webpages</link><description>[unable to retrieve full-text content]&lt;p&gt;Chinese e-commerce giant Alibaba’s &lt;a href="https://venturebeat.com/ai/its-qwens-summer-new-open-source-qwen3-235b-a22b-thinking-2507-tops-openai-gemini-reasoning-models-on-key-benchmarks"&gt;famously prolific Qwen Team&lt;/a&gt; of AI model researchers and engineers has introduced a major expansion to its Qwen Deep Research tool, which is available as an optional modality the user can activate on the web-based Qwen Chat (a competitor to ChatGPT).&lt;/p&gt;&lt;p&gt;The update lets users generate not only comprehensive research reports with well-organized citations, but also interactive web pages and multi-speaker podcasts — all within 1-2 clicks.&lt;/p&gt;&lt;p&gt;This functionality is part of a &lt;b&gt;proprietary release&lt;/b&gt;, distinct from many of Qwen’s previous open-source model offerings. &lt;/p&gt;&lt;p&gt;While the feature relies on the open-source models &lt;b&gt;Qwen3-Coder&lt;/b&gt;, &lt;b&gt;Qwen-Image&lt;/b&gt;, and &lt;b&gt;Qwen3-TTS&lt;/b&gt; to power its core capabilities, the end-to-end experience — including research execution, web deployment, and audio generation — is &lt;b&gt;hosted and operated by Qwen&lt;/b&gt;. &lt;/p&gt;&lt;p&gt;This means users benefit from a managed, integrated workflow without needing to configure infrastructure. That said, developers with access to the open-source models could theoretically replicate similar functionality on private or commercial systems.&lt;/p&gt;&lt;p&gt;The update was announced via the team’s official&lt;a href="https://x.com/Alibaba_Qwen/status/1980609551486624237"&gt; X account (@Alibaba_Qwen)&lt;/a&gt; today, October 21, 2025, stating:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“Qwen Deep Research just got a major upgrade. It now creates not only the report, but also a live webpage and a podcast — powered by Qwen3-Coder, Qwen-Image, and Qwen3-TTS. Your insights, now visual and audible.”&lt;/p&gt;&lt;/blockquote&gt;&lt;h3&gt;&lt;b&gt;Multi-Format Research Output&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The core workflow begins with a user request inside the Qwen Chat interface. From there, Qwen collaborates by asking clarifying questions to shape the research scope, pulls data from the web and official sources, and analyzes or resolves any inconsistencies it finds — even generating custom code when needed.&lt;/p&gt;&lt;p&gt;A &lt;a href="https://x.com/Alibaba_Qwen/status/1980609551486624237"&gt;demo video posted by Qwen on X&lt;/a&gt; walks through this process on Qwen Chat using the U.S. SaaS market as an example. &lt;/p&gt;&lt;p&gt;In it, Qwen retrieves data from multiple industry sources, identifies discrepancies in market size estimates (e.g., $206 billion vs. $253 billion), and highlights ambiguities in the U.S. share of global figures. The assistant comments on differences in scope between sources and calculates a compound annual growth rate (CAGR) of 19.8% from 2020 to 2023, providing contextual analysis to back up the raw numbers.&lt;/p&gt;&lt;p&gt;Once the research is complete, users can click on the &amp;quot;eyeball&amp;quot; icon below the output result (see screenshot), which will bring up a PDF-style report in the right hand pane.&lt;/p&gt;&lt;p&gt;Then, when viewing the report in the right-hand pane, the user can click the &amp;quot;Create&amp;quot; button in the upper-right hand corner and select from the following two options:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;&amp;quot;Web Dev&amp;quot; &lt;/b&gt;which produces a &lt;b&gt;live, professional-grade web page&lt;/b&gt;, automatically deployed and &lt;b&gt;hosted by Qwen&lt;/b&gt;, using Qwen3-Coder for structure and Qwen-Image for visuals.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&amp;quot;&lt;b&gt;Podcast&lt;/b&gt;,&amp;quot; which, as it states, produces an audio &lt;b&gt;podcast&lt;/b&gt;, featuring dynamic, multi-speaker narration generated by Qwen3-TTS, also &lt;b&gt;hosted by Qwen&lt;/b&gt; for easy sharing and playback.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;This enables users to quickly convert a single research project into multiple forms of content — written, visual, and audible — with minimal extra input.&lt;/p&gt;&lt;p&gt;The website includes inline graphics generated by Qwen Image, making it suitable for use in public presentations, classrooms, or publishing. &lt;/p&gt;&lt;p&gt;The podcast feature allows users to select between 17 different speaker names as the host and 7 as the co-host, though I wasn&amp;#x27;t able to find a way to preview the voice outputs before selecting them. It appears designed for deep listening on the go. &lt;/p&gt;&lt;p&gt;There was no way to change the language output that I could see, so mine came out in English, like my reports and initial prompts, though the Qwen LLMs are multi-modal. The voices were slightly more robotic than other AI tools I&amp;#x27;ve used.&lt;/p&gt;&lt;p&gt;Here&amp;#x27;s an example of a web page I generated &lt;a href="https://chat.qwen.ai/s/deploy/65743dcf-7e0e-455b-b430-5004c8f36841"&gt;on commonalities in authoritarian regimes throughout history&lt;/a&gt;, &lt;a href="https://chat.qwen.ai/s/deploy/caf2033e-725b-43dc-b4d0-721063728774"&gt;another one on UFO or UAP sightings&lt;/a&gt;, and below this paragraph, a podcast on UFO or UAP sightings. &lt;/p&gt;&lt;p&gt;While the website is hosted via a public link, the podcast must be downloaded by the user and can&amp;#x27;t be linked to publicly, from what I could tell in my brief usage so far.&lt;/p&gt;&lt;p&gt;Note the podcast is much different than the actual report — not just a straight read-through audio version of it, rather, a new format of two hosts discussing and bantering about the subject using the report as the jumping off point. &lt;/p&gt;&lt;p&gt;The web page versions of the report also include new graphics not found in the PDF report.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Comparisons to Google&amp;#x27;s NotebookLM&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While the new capabilities have been well received by many early users, comparisons to other research assistants have surfaced — particularly Google’s &lt;b&gt;NotebookLM&lt;/b&gt;, which recently exited beta.&lt;/p&gt;&lt;p&gt;AI commentator and newsletter writer &lt;a href="https://x.com/kimmonismus/status/1980612332767072444"&gt;Chubby (@kimmonismus) noted on X&lt;/a&gt;:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“I am really grateful that Qwen provides regular updates. That’s great.&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;But the attempt to build a NotebookLM clone inside Qwen-3-max doesn’t sound very promising compared to Google’s version.”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;While NotebookLM is built around organizing and querying existing documents and web pages, Qwen Deep Research focuses more on &lt;b&gt;generating new research content from scratch&lt;/b&gt;, aggregating sources from the open web, and presenting it across multiple modalities. &lt;/p&gt;&lt;p&gt;The comparison suggests that while the two tools overlap in general concept — AI-assisted research — they diverge in approach and target user experience.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Availability&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Qwen Deep Research is now live and available through the &lt;b&gt;Qwen Chat app&lt;/b&gt;. The feature can be accessed with &lt;a href="https://chat.qwen.ai/?inputFeature=deep_research"&gt;the following URL.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;No pricing details have been provided for Qwen3-Max or the specific Deep Research capabilities as of this writing.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;What&amp;#x27;s Next For Qwen Deep Research?&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;By combining research guidance, data analysis, and multi-format content creation into a single tool, Qwen Deep Research aims to streamline the path from idea to publishable output. &lt;/p&gt;&lt;p&gt;The integration of code, visuals, and voice makes it especially attractive to content creators, educators, and independent analysts who want to scale their research into web- or podcast-friendly forms without switching platforms.&lt;/p&gt;&lt;p&gt;Still, comparisons to more specialized offerings like NotebookLM raise questions about how Qwen’s generalized approach stacks up on depth, precision, and refinement. Whether the strength of its multi-format execution outweighs those concerns may come down to user priorities — and whether they value single-click publishing over tight integration with existing notes and materials.&lt;/p&gt;&lt;p&gt;For now, Qwen is signaling that research doesn’t end with a document — it begins with one.&lt;/p&gt;&lt;p&gt;Let me know if you want this repackaged into something shorter or tailored to a particular audience — newsletter, press-style blog, internal team explainer, etc.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Chinese e-commerce giant Alibaba’s &lt;a href="https://venturebeat.com/ai/its-qwens-summer-new-open-source-qwen3-235b-a22b-thinking-2507-tops-openai-gemini-reasoning-models-on-key-benchmarks"&gt;famously prolific Qwen Team&lt;/a&gt; of AI model researchers and engineers has introduced a major expansion to its Qwen Deep Research tool, which is available as an optional modality the user can activate on the web-based Qwen Chat (a competitor to ChatGPT).&lt;/p&gt;&lt;p&gt;The update lets users generate not only comprehensive research reports with well-organized citations, but also interactive web pages and multi-speaker podcasts — all within 1-2 clicks.&lt;/p&gt;&lt;p&gt;This functionality is part of a &lt;b&gt;proprietary release&lt;/b&gt;, distinct from many of Qwen’s previous open-source model offerings. &lt;/p&gt;&lt;p&gt;While the feature relies on the open-source models &lt;b&gt;Qwen3-Coder&lt;/b&gt;, &lt;b&gt;Qwen-Image&lt;/b&gt;, and &lt;b&gt;Qwen3-TTS&lt;/b&gt; to power its core capabilities, the end-to-end experience — including research execution, web deployment, and audio generation — is &lt;b&gt;hosted and operated by Qwen&lt;/b&gt;. &lt;/p&gt;&lt;p&gt;This means users benefit from a managed, integrated workflow without needing to configure infrastructure. That said, developers with access to the open-source models could theoretically replicate similar functionality on private or commercial systems.&lt;/p&gt;&lt;p&gt;The update was announced via the team’s official&lt;a href="https://x.com/Alibaba_Qwen/status/1980609551486624237"&gt; X account (@Alibaba_Qwen)&lt;/a&gt; today, October 21, 2025, stating:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“Qwen Deep Research just got a major upgrade. It now creates not only the report, but also a live webpage and a podcast — powered by Qwen3-Coder, Qwen-Image, and Qwen3-TTS. Your insights, now visual and audible.”&lt;/p&gt;&lt;/blockquote&gt;&lt;h3&gt;&lt;b&gt;Multi-Format Research Output&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The core workflow begins with a user request inside the Qwen Chat interface. From there, Qwen collaborates by asking clarifying questions to shape the research scope, pulls data from the web and official sources, and analyzes or resolves any inconsistencies it finds — even generating custom code when needed.&lt;/p&gt;&lt;p&gt;A &lt;a href="https://x.com/Alibaba_Qwen/status/1980609551486624237"&gt;demo video posted by Qwen on X&lt;/a&gt; walks through this process on Qwen Chat using the U.S. SaaS market as an example. &lt;/p&gt;&lt;p&gt;In it, Qwen retrieves data from multiple industry sources, identifies discrepancies in market size estimates (e.g., $206 billion vs. $253 billion), and highlights ambiguities in the U.S. share of global figures. The assistant comments on differences in scope between sources and calculates a compound annual growth rate (CAGR) of 19.8% from 2020 to 2023, providing contextual analysis to back up the raw numbers.&lt;/p&gt;&lt;p&gt;Once the research is complete, users can click on the &amp;quot;eyeball&amp;quot; icon below the output result (see screenshot), which will bring up a PDF-style report in the right hand pane.&lt;/p&gt;&lt;p&gt;Then, when viewing the report in the right-hand pane, the user can click the &amp;quot;Create&amp;quot; button in the upper-right hand corner and select from the following two options:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;&amp;quot;Web Dev&amp;quot; &lt;/b&gt;which produces a &lt;b&gt;live, professional-grade web page&lt;/b&gt;, automatically deployed and &lt;b&gt;hosted by Qwen&lt;/b&gt;, using Qwen3-Coder for structure and Qwen-Image for visuals.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&amp;quot;&lt;b&gt;Podcast&lt;/b&gt;,&amp;quot; which, as it states, produces an audio &lt;b&gt;podcast&lt;/b&gt;, featuring dynamic, multi-speaker narration generated by Qwen3-TTS, also &lt;b&gt;hosted by Qwen&lt;/b&gt; for easy sharing and playback.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;This enables users to quickly convert a single research project into multiple forms of content — written, visual, and audible — with minimal extra input.&lt;/p&gt;&lt;p&gt;The website includes inline graphics generated by Qwen Image, making it suitable for use in public presentations, classrooms, or publishing. &lt;/p&gt;&lt;p&gt;The podcast feature allows users to select between 17 different speaker names as the host and 7 as the co-host, though I wasn&amp;#x27;t able to find a way to preview the voice outputs before selecting them. It appears designed for deep listening on the go. &lt;/p&gt;&lt;p&gt;There was no way to change the language output that I could see, so mine came out in English, like my reports and initial prompts, though the Qwen LLMs are multi-modal. The voices were slightly more robotic than other AI tools I&amp;#x27;ve used.&lt;/p&gt;&lt;p&gt;Here&amp;#x27;s an example of a web page I generated &lt;a href="https://chat.qwen.ai/s/deploy/65743dcf-7e0e-455b-b430-5004c8f36841"&gt;on commonalities in authoritarian regimes throughout history&lt;/a&gt;, &lt;a href="https://chat.qwen.ai/s/deploy/caf2033e-725b-43dc-b4d0-721063728774"&gt;another one on UFO or UAP sightings&lt;/a&gt;, and below this paragraph, a podcast on UFO or UAP sightings. &lt;/p&gt;&lt;p&gt;While the website is hosted via a public link, the podcast must be downloaded by the user and can&amp;#x27;t be linked to publicly, from what I could tell in my brief usage so far.&lt;/p&gt;&lt;p&gt;Note the podcast is much different than the actual report — not just a straight read-through audio version of it, rather, a new format of two hosts discussing and bantering about the subject using the report as the jumping off point. &lt;/p&gt;&lt;p&gt;The web page versions of the report also include new graphics not found in the PDF report.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Comparisons to Google&amp;#x27;s NotebookLM&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While the new capabilities have been well received by many early users, comparisons to other research assistants have surfaced — particularly Google’s &lt;b&gt;NotebookLM&lt;/b&gt;, which recently exited beta.&lt;/p&gt;&lt;p&gt;AI commentator and newsletter writer &lt;a href="https://x.com/kimmonismus/status/1980612332767072444"&gt;Chubby (@kimmonismus) noted on X&lt;/a&gt;:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;“I am really grateful that Qwen provides regular updates. That’s great.&lt;/p&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;p&gt;But the attempt to build a NotebookLM clone inside Qwen-3-max doesn’t sound very promising compared to Google’s version.”&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;While NotebookLM is built around organizing and querying existing documents and web pages, Qwen Deep Research focuses more on &lt;b&gt;generating new research content from scratch&lt;/b&gt;, aggregating sources from the open web, and presenting it across multiple modalities. &lt;/p&gt;&lt;p&gt;The comparison suggests that while the two tools overlap in general concept — AI-assisted research — they diverge in approach and target user experience.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Availability&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Qwen Deep Research is now live and available through the &lt;b&gt;Qwen Chat app&lt;/b&gt;. The feature can be accessed with &lt;a href="https://chat.qwen.ai/?inputFeature=deep_research"&gt;the following URL.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;No pricing details have been provided for Qwen3-Max or the specific Deep Research capabilities as of this writing.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;What&amp;#x27;s Next For Qwen Deep Research?&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;By combining research guidance, data analysis, and multi-format content creation into a single tool, Qwen Deep Research aims to streamline the path from idea to publishable output. &lt;/p&gt;&lt;p&gt;The integration of code, visuals, and voice makes it especially attractive to content creators, educators, and independent analysts who want to scale their research into web- or podcast-friendly forms without switching platforms.&lt;/p&gt;&lt;p&gt;Still, comparisons to more specialized offerings like NotebookLM raise questions about how Qwen’s generalized approach stacks up on depth, precision, and refinement. Whether the strength of its multi-format execution outweighs those concerns may come down to user priorities — and whether they value single-click publishing over tight integration with existing notes and materials.&lt;/p&gt;&lt;p&gt;For now, Qwen is signaling that research doesn’t end with a document — it begins with one.&lt;/p&gt;&lt;p&gt;Let me know if you want this repackaged into something shorter or tailored to a particular audience — newsletter, press-style blog, internal team explainer, etc.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/qwens-new-deep-research-update-lets-you-turn-its-reports-into-webpages</guid><pubDate>Tue, 21 Oct 2025 18:32:00 +0000</pubDate></item><item><title>[NEW] YouTube’s likeness detection has arrived to help stop AI doppelgängers (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/10/youtube-rolls-out-likeness-detection-to-help-creators-combat-ai-fakes/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Likeness detection will flag possible AI fakes, but Google doesn’t guarantee removal.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Man using phone in front of YouTube logo" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/06/youtube-300x169.jpg" width="300" /&gt;
                  &lt;img alt="Man using phone in front of YouTube logo" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/06/youtube-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Chris Ratcliffe/Bloomberg via Getty

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;AI content has proliferated across the Internet over the past few years, but those early confabulations with mutated hands have evolved into synthetic images and videos that can be hard to differentiate from reality. Having helped to create this problem, Google has some responsibility to keep AI video in check on YouTube. To that end, the company has started rolling out its promised likeness detection system for creators.&lt;/p&gt;
&lt;p&gt;Google’s powerful and freely available AI models have helped fuel the rise of AI content, some of which is aimed at spreading misinformation and harassing individuals. Creators and influencers fear their brands could be tainted by a flood of AI videos that show them saying and doing things that never happened—even lawmakers are fretting about this. Google has placed a large bet on the value of AI content, so banning AI from YouTube, as many want, simply isn’t happening.&lt;/p&gt;
&lt;p&gt;Earlier this year, YouTube promised tools that would flag face-stealing AI content on the platform. The likeness detection tool, which is similar to the site’s copyright detection system, has now expanded beyond the initial small group of testers. YouTube says the first batch of eligible creators have been notified that they can use likeness detection, but interested parties will need to hand Google even more personal information to get protection from AI fakes.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Sneak Peek: Likeness Detection on YouTube.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Currently, likeness detection is a beta feature in limited testing, so not all creators will see it as an option in YouTube Studio. When it does appear, it will be tucked into the existing “Content detection” menu. In YouTube’s demo video, the setup flow appears to assume the channel has only a single host whose likeness needs protection. That person must verify their identity, which requires a photo of a government ID and a video of their face. It’s unclear why YouTube needs this data in addition to the videos people have already posted with their oh-so stealable faces, but rules are rules.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;No guarantees&lt;/h2&gt;
&lt;p&gt;After signing up, YouTube will flag videos from other channels that appear to have the user’s face. YouTube’s algorithm can’t know for sure what is and is not an AI video. So some of the face match results may be false positives from channels that have used a short clip under fair use guidelines.&lt;/p&gt;
&lt;p&gt;If creators do spot an AI fake, they can add some details and submit a report in a few minutes. If the video includes content copied from the creator’s channel that does not adhere to fair use guidelines, YouTube suggests also submitting a copyright removal request. However, just because a person’s likeness appears in an AI video does not necessarily mean YouTube will remove it.&lt;/p&gt;
&lt;p&gt;YouTube has published a rundown of the factors its reviewers will take into account when deciding whether or not to approve a removal request. For example, parody content labeled as AI or videos with an unrealistic style may not meet the threshold for removal. On the flip side, you can safely assume that a realistic AI video showing someone endorsing a product or engaging in illegal activity will run afoul of the rules and be removed from YouTube.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2123515 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="YouTube AI detection UI" class="fullwidth full" height="632" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/YT-AI-detect.png" width="797" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Likeness detection will appear alongside copyright claims.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          YouTube

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While this may be an emerging issue for creators right now, AI content on YouTube is likely to kick into overdrive soon. Google recently unveiled its new Veo 3.1 video model, which includes support for both portrait and landscape AI videos. The company has previously promised to integrate Veo with YouTube, making it even easier for people to churn out AI slop that may include depictions of real people.&lt;/p&gt;
&lt;p&gt;Google rival OpenAI has seen success (at least in terms of popularity) with its Sora AI video app and the new Sora 2 model powering it. This could push Google to accelerate its AI plans for YouTube, but as we’ve seen with Sora, people love making public figures do weird things. Popular creators may have to begin filing AI likeness complaints as regularly as they do DMCA takedowns.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Likeness detection will flag possible AI fakes, but Google doesn’t guarantee removal.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Man using phone in front of YouTube logo" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/06/youtube-300x169.jpg" width="300" /&gt;
                  &lt;img alt="Man using phone in front of YouTube logo" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/06/youtube-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Chris Ratcliffe/Bloomberg via Getty

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;AI content has proliferated across the Internet over the past few years, but those early confabulations with mutated hands have evolved into synthetic images and videos that can be hard to differentiate from reality. Having helped to create this problem, Google has some responsibility to keep AI video in check on YouTube. To that end, the company has started rolling out its promised likeness detection system for creators.&lt;/p&gt;
&lt;p&gt;Google’s powerful and freely available AI models have helped fuel the rise of AI content, some of which is aimed at spreading misinformation and harassing individuals. Creators and influencers fear their brands could be tainted by a flood of AI videos that show them saying and doing things that never happened—even lawmakers are fretting about this. Google has placed a large bet on the value of AI content, so banning AI from YouTube, as many want, simply isn’t happening.&lt;/p&gt;
&lt;p&gt;Earlier this year, YouTube promised tools that would flag face-stealing AI content on the platform. The likeness detection tool, which is similar to the site’s copyright detection system, has now expanded beyond the initial small group of testers. YouTube says the first batch of eligible creators have been notified that they can use likeness detection, but interested parties will need to hand Google even more personal information to get protection from AI fakes.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Sneak Peek: Likeness Detection on YouTube.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Currently, likeness detection is a beta feature in limited testing, so not all creators will see it as an option in YouTube Studio. When it does appear, it will be tucked into the existing “Content detection” menu. In YouTube’s demo video, the setup flow appears to assume the channel has only a single host whose likeness needs protection. That person must verify their identity, which requires a photo of a government ID and a video of their face. It’s unclear why YouTube needs this data in addition to the videos people have already posted with their oh-so stealable faces, but rules are rules.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;No guarantees&lt;/h2&gt;
&lt;p&gt;After signing up, YouTube will flag videos from other channels that appear to have the user’s face. YouTube’s algorithm can’t know for sure what is and is not an AI video. So some of the face match results may be false positives from channels that have used a short clip under fair use guidelines.&lt;/p&gt;
&lt;p&gt;If creators do spot an AI fake, they can add some details and submit a report in a few minutes. If the video includes content copied from the creator’s channel that does not adhere to fair use guidelines, YouTube suggests also submitting a copyright removal request. However, just because a person’s likeness appears in an AI video does not necessarily mean YouTube will remove it.&lt;/p&gt;
&lt;p&gt;YouTube has published a rundown of the factors its reviewers will take into account when deciding whether or not to approve a removal request. For example, parody content labeled as AI or videos with an unrealistic style may not meet the threshold for removal. On the flip side, you can safely assume that a realistic AI video showing someone endorsing a product or engaging in illegal activity will run afoul of the rules and be removed from YouTube.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2123515 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="YouTube AI detection UI" class="fullwidth full" height="632" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/YT-AI-detect.png" width="797" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Likeness detection will appear alongside copyright claims.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          YouTube

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While this may be an emerging issue for creators right now, AI content on YouTube is likely to kick into overdrive soon. Google recently unveiled its new Veo 3.1 video model, which includes support for both portrait and landscape AI videos. The company has previously promised to integrate Veo with YouTube, making it even easier for people to churn out AI slop that may include depictions of real people.&lt;/p&gt;
&lt;p&gt;Google rival OpenAI has seen success (at least in terms of popularity) with its Sora AI video app and the new Sora 2 model powering it. This could push Google to accelerate its AI plans for YouTube, but as we’ve seen with Sora, people love making public figures do weird things. Popular creators may have to begin filing AI likeness complaints as regularly as they do DMCA takedowns.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/10/youtube-rolls-out-likeness-detection-to-help-creators-combat-ai-fakes/</guid><pubDate>Tue, 21 Oct 2025 18:46:42 +0000</pubDate></item><item><title>[NEW] OpenAI looks for its “Google Chrome” moment with new Atlas web browser (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/openais-new-atlas-web-browser-wants-to-let-you-chat-with-a-page/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        MacOS version launches today, includes Agent Mode preview to “use the Internet for you.”
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="320" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlaslogo-640x320.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlaslogo.jpg" width="720" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A new logo for a new browser.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Back in 2008, Google launched the Chrome browser to help better integrate its industry-leading search engine into the web-browsing experience. Today, OpenAI announced the Atlas browser that it hopes will do something similar for its ChatGPT large language model, answering the question “What if I could chat with a browser?” as the OpenAI team put it.&lt;/p&gt;
&lt;p&gt;OpenAI Founder and CEO Sam Altman said in a live stream announcement that Atlas will let users “chat with a page,” helping ChatGPT become a core way that users interact with the place where “a ton of work and life happens” online. “The way that we hope people will use the Internet in the future… is that the chat experience and a web browser can be a great analogue,” he said.&lt;/p&gt;
&lt;p&gt;The new browser is available for download now on macOS, and Altman promised Windows and mobile versions would be rolled out “as quick as we can.”&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;/figure&gt;
&lt;h2&gt;An LLM that follows you&lt;/h2&gt;
&lt;p&gt;The home screen of a new Atlas tab mirrors the simplicity of the Chrome search box, with a text field prompting users to “Ask ChatGPT or type a URL.” Users can access their chat history or different ChatGPT models using an interface similar to that on ChatGPT.com. The Atlas browser will also populate suggestions below that search box, which could range from links to news stories to suggestions for tasks the browser can perform for you.&lt;/p&gt;
&lt;p&gt;During the livestream, the OpenAI team said that Atlas has features that web users have come to expect from a browser: tabs, bookmarks, and auto-fill among them. But the integration with ChatGPT now means that “chat comes with you everywhere” in the browsing experience.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;That means you can use ChatGPT to search through your bookmarks or browsing history using human-parsable language prompts. It also means you can bring up a “side chat” next to your current page and ask questions that rely on the context of that specific page. And if you want to edit a Gmail draft using ChatGPT, you can now do that directly in the draft window, without the need to copy and paste between a ChatGPT window and an editor.&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas4-1024x576.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Letting ChatGPT edit text directly in a Gmail window.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="505" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas3-1024x505.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The default search experience on Atlas, with tabs for more traditional results.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Letting ChatGPT edit text directly in a Gmail window.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The default search experience on Atlas, with tabs for more traditional results.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="500" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas2-1024x500.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Side chat lets you ask ChatGPT questions about the active webpage.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="554" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas1-1024x554.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The default "New Tab" experience in Atlas, complete with some suggestions.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Side chat lets you ask ChatGPT questions about the active webpage.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The default "New Tab" experience in Atlas, complete with some suggestions.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
    
    
      &lt;/div&gt;

&lt;p&gt;When typing in a short search prompt, Atlas will, by default, reply as an LLM, with written answers with embedded links to sourcing where appropriate (à la OpenAI’s existing search function). But the browser will also provide tabs with more traditional lists of links, images, videos, or news like those you would get from a search engine without LLM features.&lt;/p&gt;
&lt;h2&gt;Let us do the browsing&lt;/h2&gt;
&lt;p&gt;To wrap up the livestreamed demonstration, the OpenAI team showed off Atlas’ Agent Mode. While the “preview mode” feature is only available to ChatGPT Plus and Pro subscribers, research lead Will Ellsworth said he hoped it would eventually help users toward “an amazing tool for vibe life-ing” in the same way that LLM coding tools have become tools for “vibe coding.”&lt;/p&gt;
&lt;p&gt;To that end, the team showed the browser taking planning tasks written in a Google Docs table and moving them over to the task management software Linear over the course of a few minutes. Agent Mode was also shown taking the ingredients list from a recipe webpage and adding them directly to the user’s Instacart in a different tab (though the demo Agent stopped before checkout to get approval from the user).&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas5-1024x576.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Atlas' Agent Mode takes over to move planning tasks from one web-based app to another.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="507" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas6-1024x507.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Atlas' Agent Mode adds items from a recipe to a user's Instacart.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Atlas' Agent Mode takes over to move planning tasks from one web-based app to another.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Atlas' Agent Mode adds items from a recipe to a user's Instacart.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Atlas users can watch Agent Mode as it clicks through various tabs and webpages, taking over at any time, or they can let it operate in the background without oversight. Users can activate Agent Mode directly using a drop-down menu, but ChatGPT can also suggest it be turned on when a user prompt suggests a task that it might be able to help with.&lt;/p&gt;
&lt;p&gt;The OpenAI team said Agent Mode can click around as if it were a human user, with full access to that user’s authentication and browsing history. But the Agent Mode can only operate inside web tabs and can’t execute code outside of the browser, OpenAI said. You can also manually control whether a new Atlas tab is “logged in” or “logged out” of various other web services, and use incognito windows for browsing you don’t want the LLM to remember.&lt;/p&gt;
&lt;h2&gt;A crowded field&lt;/h2&gt;
&lt;p&gt;Established competitors in the browser space have been trying to integrate similar AI features into their products for a while now: Microsoft with a version of Copilot built into the Edge browser and Google with Chrome-based Gemini features that it promises will include “Agentic features” in the coming months. A number of startups are also focused on building AI-powered browsers from the ground up, most notably Perplexity, which recently made a bold $34.5 billion bid for Chrome despite a total market valuation of just $14 million.&lt;/p&gt;
&lt;p&gt;OpenAI also notably publicly expressed interest in buying Chrome back in April, though recent legal updates in that antitrust case mean Google now seems unlikely to sell in the near future.&lt;/p&gt;
&lt;p&gt;The Information reported on OpenAI’s browser plans last year, and Reuters followed up with more information from unnamed sources in July. Reuters noted that a browser will give OpenAI more direct access to valuable user data beyond what gets typed into a ChatGPT prompt window and could provide a simple way to integrate ads into the ChatGPT experience. But of course, that all depends on how many of ChatGPT’s 700 million-plus weekly active users are willing to abandon their current browser in favor of a less proven competitor from a major LLM brand.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        MacOS version launches today, includes Agent Mode preview to “use the Internet for you.”
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="320" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlaslogo-640x320.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlaslogo.jpg" width="720" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A new logo for a new browser.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Back in 2008, Google launched the Chrome browser to help better integrate its industry-leading search engine into the web-browsing experience. Today, OpenAI announced the Atlas browser that it hopes will do something similar for its ChatGPT large language model, answering the question “What if I could chat with a browser?” as the OpenAI team put it.&lt;/p&gt;
&lt;p&gt;OpenAI Founder and CEO Sam Altman said in a live stream announcement that Atlas will let users “chat with a page,” helping ChatGPT become a core way that users interact with the place where “a ton of work and life happens” online. “The way that we hope people will use the Internet in the future… is that the chat experience and a web browser can be a great analogue,” he said.&lt;/p&gt;
&lt;p&gt;The new browser is available for download now on macOS, and Altman promised Windows and mobile versions would be rolled out “as quick as we can.”&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;/figure&gt;
&lt;h2&gt;An LLM that follows you&lt;/h2&gt;
&lt;p&gt;The home screen of a new Atlas tab mirrors the simplicity of the Chrome search box, with a text field prompting users to “Ask ChatGPT or type a URL.” Users can access their chat history or different ChatGPT models using an interface similar to that on ChatGPT.com. The Atlas browser will also populate suggestions below that search box, which could range from links to news stories to suggestions for tasks the browser can perform for you.&lt;/p&gt;
&lt;p&gt;During the livestream, the OpenAI team said that Atlas has features that web users have come to expect from a browser: tabs, bookmarks, and auto-fill among them. But the integration with ChatGPT now means that “chat comes with you everywhere” in the browsing experience.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;That means you can use ChatGPT to search through your bookmarks or browsing history using human-parsable language prompts. It also means you can bring up a “side chat” next to your current page and ask questions that rely on the context of that specific page. And if you want to edit a Gmail draft using ChatGPT, you can now do that directly in the draft window, without the need to copy and paste between a ChatGPT window and an editor.&lt;/p&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas4-1024x576.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Letting ChatGPT edit text directly in a Gmail window.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="505" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas3-1024x505.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The default search experience on Atlas, with tabs for more traditional results.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Letting ChatGPT edit text directly in a Gmail window.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The default search experience on Atlas, with tabs for more traditional results.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="500" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas2-1024x500.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Side chat lets you ask ChatGPT questions about the active webpage.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="554" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas1-1024x554.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The default "New Tab" experience in Atlas, complete with some suggestions.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Side chat lets you ask ChatGPT questions about the active webpage.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;The default "New Tab" experience in Atlas, complete with some suggestions.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
    
    
      &lt;/div&gt;

&lt;p&gt;When typing in a short search prompt, Atlas will, by default, reply as an LLM, with written answers with embedded links to sourcing where appropriate (à la OpenAI’s existing search function). But the browser will also provide tabs with more traditional lists of links, images, videos, or news like those you would get from a search engine without LLM features.&lt;/p&gt;
&lt;h2&gt;Let us do the browsing&lt;/h2&gt;
&lt;p&gt;To wrap up the livestreamed demonstration, the OpenAI team showed off Atlas’ Agent Mode. While the “preview mode” feature is only available to ChatGPT Plus and Pro subscribers, research lead Will Ellsworth said he hoped it would eventually help users toward “an amazing tool for vibe life-ing” in the same way that LLM coding tools have become tools for “vibe coding.”&lt;/p&gt;
&lt;p&gt;To that end, the team showed the browser taking planning tasks written in a Google Docs table and moving them over to the task management software Linear over the course of a few minutes. Agent Mode was also shown taking the ingredients list from a recipe webpage and adding them directly to the user’s Instacart in a different tab (though the demo Agent stopped before checkout to get approval from the user).&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="576" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas5-1024x576.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Atlas' Agent Mode takes over to move planning tasks from one web-based app to another.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="507" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/atlas6-1024x507.png" width="1024" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Atlas' Agent Mode adds items from a recipe to a user's Instacart.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Atlas' Agent Mode takes over to move planning tasks from one web-based app to another.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Atlas' Agent Mode adds items from a recipe to a user's Instacart.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Atlas users can watch Agent Mode as it clicks through various tabs and webpages, taking over at any time, or they can let it operate in the background without oversight. Users can activate Agent Mode directly using a drop-down menu, but ChatGPT can also suggest it be turned on when a user prompt suggests a task that it might be able to help with.&lt;/p&gt;
&lt;p&gt;The OpenAI team said Agent Mode can click around as if it were a human user, with full access to that user’s authentication and browsing history. But the Agent Mode can only operate inside web tabs and can’t execute code outside of the browser, OpenAI said. You can also manually control whether a new Atlas tab is “logged in” or “logged out” of various other web services, and use incognito windows for browsing you don’t want the LLM to remember.&lt;/p&gt;
&lt;h2&gt;A crowded field&lt;/h2&gt;
&lt;p&gt;Established competitors in the browser space have been trying to integrate similar AI features into their products for a while now: Microsoft with a version of Copilot built into the Edge browser and Google with Chrome-based Gemini features that it promises will include “Agentic features” in the coming months. A number of startups are also focused on building AI-powered browsers from the ground up, most notably Perplexity, which recently made a bold $34.5 billion bid for Chrome despite a total market valuation of just $14 million.&lt;/p&gt;
&lt;p&gt;OpenAI also notably publicly expressed interest in buying Chrome back in April, though recent legal updates in that antitrust case mean Google now seems unlikely to sell in the near future.&lt;/p&gt;
&lt;p&gt;The Information reported on OpenAI’s browser plans last year, and Reuters followed up with more information from unnamed sources in July. Reuters noted that a browser will give OpenAI more direct access to valuable user data beyond what gets typed into a ChatGPT prompt window and could provide a simple way to integrate ads into the ChatGPT experience. But of course, that all depends on how many of ChatGPT’s 700 million-plus weekly active users are willing to abandon their current browser in favor of a less proven competitor from a major LLM brand.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/openais-new-atlas-web-browser-wants-to-let-you-chat-with-a-page/</guid><pubDate>Tue, 21 Oct 2025 19:02:10 +0000</pubDate></item><item><title>[NEW] Cloudflare CEO Matthew Prince is pushing UK regulator to unbundle Google’s search and AI crawlers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/cloudflare-ceo-matthew-prince-is-pushing-uk-regulator-to-unbundle-googles-search-and-ai-crawlers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/videoframe_402382.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After earlier this year launching a marketplace that allows websites to charge AI bots for scraping their content, web infrastructure provider Cloudflare is pushing for increased regulation in the AI sector. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s chief executive Matthew Prince says he’s in London to speak with the U.K.’s Competition and Markets Authority (CMA), where he’s proposing stricter rules on how Google should be able to compete in the AI race, given its search dominance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The CMA earlier this month designated Google with a special status in the search and advertising markets because of its “substantial and entrenched” position. The move will allow the regulator to impose more stringent regulations beyond just search and ads, including in areas like Google’s AI Overviews and AI Mode, the Discover feed, Top Stories, and News tab. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Prince, Cloudflare is in a good position to make recommendations because it’s not in the AI business itself, but has a large number of relationships with the AI companies themselves.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t have a dog directly in the fight. We’re not an AI company,” Prince said, speaking at the Bloomberg Tech conference in London this week. “We’re not a media publisher, but we’re this network that sits between them. 80% of the AI companies are our customers,” he added. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Cloudflare boss believes that Google should have to compete on the same footing as other AI companies, which is not what it’s doing today, he said. Rather, Google uses its existing web crawler to crawl content for its AI products and services, in addition to its search engine. This, Prince said, gives Google an unfair advantage. (Prince is referring to Googlebot, which crawls for Search, including its AI features, like AI Overviews. A Google spokesperson, Ned Adriance, said that site owners can opt out of having content used for training AI products with Google Extended, which doesn’t impact a site’s inclusion in Google Search. Some media companies would likely prefer to opt fully out of AI features, however, so the point largely stands.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Google is saying, ‘we have an absolute God-given right to all of the content in the world, even if we don’t pay for it, because look what we did for the last 27 years,” Prince explained. “And, they’re saying we can take it and use the same crawler we use for search in order to power our AI systems. And if you want to opt out of one, you have to opt out of both,” he noted.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This, obviously, is not feasible for most, particularly those in the media business, where losing search means losing about 20% of your revenue, the executive said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“But it gets even worse. If you block Google’s crawler, it blocks their ad safety team, which means that your advertisements across all of your platforms stop working, which is just a non-starter,” Prince added. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because Google bundles its crawler, it’s able to get access to content that others, like Anthropic, OpenAI, and Perplexity, would have to pay for.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The problem is that we then will have effectively handed the game to Google,” he said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The solution, Prince said, is to foster a lot of competition in the market, where potentially thousands of AI companies could compete to buy content from thousands of media businesses and millions of small businesses. He suggested that what the U.K.’s CMA was doing by flagging Google as a potential regulatory target was a thoughtful move, and one that indicates they’re aware of Google’s unique advantage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare has also provided the CMA with data that shows how Google’s crawler works and why it’s nearly impossible for other players to replicate the same success Google could have. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prince isn’t the only one to share these opinions in recent days. Last month, Neil Vogel, CEO of People, Inc., the largest digital and print publisher in the United States, which operates over 40 media brands, said essentially the same thing. In an interview, he called out Google as a “bad actor,” saying that media companies had no choice but to let Google crawl their sites for AI content because of the way the crawlers were combined.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vogel, whose company had adopted Cloudflare’s solution to block AI crawlers&amp;nbsp;that don’t pay, claimed the system was working, as he said deal discussions were underway with several large LLM providers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated with Google comment. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/videoframe_402382.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After earlier this year launching a marketplace that allows websites to charge AI bots for scraping their content, web infrastructure provider Cloudflare is pushing for increased regulation in the AI sector. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s chief executive Matthew Prince says he’s in London to speak with the U.K.’s Competition and Markets Authority (CMA), where he’s proposing stricter rules on how Google should be able to compete in the AI race, given its search dominance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The CMA earlier this month designated Google with a special status in the search and advertising markets because of its “substantial and entrenched” position. The move will allow the regulator to impose more stringent regulations beyond just search and ads, including in areas like Google’s AI Overviews and AI Mode, the Discover feed, Top Stories, and News tab. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Prince, Cloudflare is in a good position to make recommendations because it’s not in the AI business itself, but has a large number of relationships with the AI companies themselves.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t have a dog directly in the fight. We’re not an AI company,” Prince said, speaking at the Bloomberg Tech conference in London this week. “We’re not a media publisher, but we’re this network that sits between them. 80% of the AI companies are our customers,” he added. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Cloudflare boss believes that Google should have to compete on the same footing as other AI companies, which is not what it’s doing today, he said. Rather, Google uses its existing web crawler to crawl content for its AI products and services, in addition to its search engine. This, Prince said, gives Google an unfair advantage. (Prince is referring to Googlebot, which crawls for Search, including its AI features, like AI Overviews. A Google spokesperson, Ned Adriance, said that site owners can opt out of having content used for training AI products with Google Extended, which doesn’t impact a site’s inclusion in Google Search. Some media companies would likely prefer to opt fully out of AI features, however, so the point largely stands.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Google is saying, ‘we have an absolute God-given right to all of the content in the world, even if we don’t pay for it, because look what we did for the last 27 years,” Prince explained. “And, they’re saying we can take it and use the same crawler we use for search in order to power our AI systems. And if you want to opt out of one, you have to opt out of both,” he noted.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This, obviously, is not feasible for most, particularly those in the media business, where losing search means losing about 20% of your revenue, the executive said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“But it gets even worse. If you block Google’s crawler, it blocks their ad safety team, which means that your advertisements across all of your platforms stop working, which is just a non-starter,” Prince added. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because Google bundles its crawler, it’s able to get access to content that others, like Anthropic, OpenAI, and Perplexity, would have to pay for.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The problem is that we then will have effectively handed the game to Google,” he said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The solution, Prince said, is to foster a lot of competition in the market, where potentially thousands of AI companies could compete to buy content from thousands of media businesses and millions of small businesses. He suggested that what the U.K.’s CMA was doing by flagging Google as a potential regulatory target was a thoughtful move, and one that indicates they’re aware of Google’s unique advantage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare has also provided the CMA with data that shows how Google’s crawler works and why it’s nearly impossible for other players to replicate the same success Google could have. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prince isn’t the only one to share these opinions in recent days. Last month, Neil Vogel, CEO of People, Inc., the largest digital and print publisher in the United States, which operates over 40 media brands, said essentially the same thing. In an interview, he called out Google as a “bad actor,” saying that media companies had no choice but to let Google crawl their sites for AI content because of the way the crawlers were combined.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vogel, whose company had adopted Cloudflare’s solution to block AI crawlers&amp;nbsp;that don’t pay, claimed the system was working, as he said deal discussions were underway with several large LLM providers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated with Google comment. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/cloudflare-ceo-matthew-prince-is-pushing-uk-regulator-to-unbundle-googles-search-and-ai-crawlers/</guid><pubDate>Tue, 21 Oct 2025 19:34:21 +0000</pubDate></item><item><title>[NEW] Creating AI that matters (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/creating-ai-that-matters-1021</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/mit-ibm-watson-AI-that-Matters.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;When it comes to artificial intelligence, MIT and IBM were there at the beginning: laying foundational work and creating some of the first programs — AI predecessors — and theorizing how machine “intelligence” might come to be.&lt;/p&gt;&lt;p&gt;Today, collaborations like the MIT-IBM Watson AI Lab, which launched eight years ago, are continuing to deliver expertise for the promise of tomorrow’s AI technology. This is critical for industries and the labor force that stand to benefit, particularly in the short term: from $3-4 trillion of forecast global economic benefits and 80 percent productivity gains for knowledge workers and creative tasks, to significant incorporations of generative AI into business processes (80 percent) and software applications (70 percent) in the next three years.&lt;/p&gt;&lt;p&gt;While industry has seen a boom in notable models, chiefly in the past year, academia continues to drive the innovation, contributing most of the highly cited research. At the MIT-IBM Watson AI Lab, success takes the form of 54 patent disclosures, an excess of 128,000 citations with an h-index of 162, and more than 50 industry-driven use cases. Some of the lab’s many achievements include improved stent placement with AI imaging techniques, slashing computational overhead, shrinking models while maintaining performance, and modeling of interatomic potential for silicate chemistry.&lt;/p&gt;&lt;p&gt;“The lab is uniquely positioned to identify the ‘right’ problems to solve, setting us apart from other entities,” says Aude Oliva, lab MIT director and director of strategic industry engagement in the MIT Schwarzman College of Computing. “Further, the experience our students gain from working on these challenges for enterprise AI translates to their competitiveness in the job market and the promotion of a competitive industry.”&lt;/p&gt;&lt;p&gt;“The MIT-IBM Watson AI Lab has had tremendous impact by bringing together a rich set of collaborations between IBM and MIT’s researchers and students,” says Provost Anantha Chandrakasan, who is the lab’s MIT co-chair and the Vannevar Bush Professor of Electrical Engineering and Computer Science. “By supporting cross-cutting research at the intersection of AI and many other disciplines, the lab is advancing foundational work and accelerating the development of transformative solutions for our nation and the world.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Long-horizon work&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As AI continues to garner interest, many organizations struggle to channel the technology into meaningful outcomes. A 2024 Gartner study finds that, “at least 30% of generative AI projects will be abandoned after proof of concept by the end of 2025,” demonstrating ambition and widespread hunger for AI, but a lack of knowledge for how to develop and apply it to create immediate value.&lt;/p&gt;&lt;p&gt;Here, the lab shines, bridging research and deployment. The majority of the lab’s current-year research portfolio is aligned to use and develop new features, capacities, or products for IBM, the lab’s corporate members, or real-world applications. The last of these comprise large language models, AI hardware, and foundation models, including multi-modal, bio-medical, and geo-spatial ones. Inquiry-driven students and interns are invaluable in this pursuit, offering enthusiasm and new perspectives while accumulating domain knowledge to help derive and engineer advancements in the field, as well as opening up new frontiers for exploration with AI as a tool.&lt;/p&gt;&lt;p&gt;Findings from the AAAI 2025 Presidential panel on the Future of AI Research support the need for contributions from academia-industry collaborations like the lab in the AI arena: “Academics have a role to play in providing independent advice and interpretations of these results [from industry] and their consequences. The private sector focuses more on the short term, and universities and society more on a longer-term perspective.”&lt;/p&gt;&lt;p&gt;Bringing these strengths together, along with the push for open sourcing and open science, can spark innovation that neither could achieve alone. History shows that embracing these principles, and sharing code and making research accessible, has long-term benefits for both the sector and society. In line with IBM and MIT’s missions, the lab contributes technologies, findings, governance, and standards to the public sphere through this collaboration, thereby enhancing transparency, accelerating reproducibility, and ensuring trustworthy advances.&lt;/p&gt;&lt;p&gt;The lab was created to merge MIT’s deep research expertise with IBM’s industrial R&amp;amp;D capacity, aiming for breakthroughs in core AI methods and hardware, as well as new applications in areas like health care, chemistry, finance, cybersecurity, and robust planning and decision-making for business.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Bigger isn't always better&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Today, large foundation models are giving way to smaller, more task-specific models yielding better performance. Contributions from lab members like Song Han, associate professor in the MIT Department of Electrical Engineering and Computer Science (EECS), and IBM Research’s Chuang Gan help make this possible, through work such as once-for-all and AWQ. Innovations such as these improve efficiency with better architectures, algorithm shrinking, and activation-aware weight quantization, letting models like language processing run on edge devices at faster speeds and reduced latency.&lt;/p&gt;&lt;p&gt;Consequently, foundation, vision, multimodal, and large language models have seen benefits, allowing for the lab research groups of Oliva, MIT EECS Associate Professor Yoon Kim, and IBM Research members Rameswar Panda, Yang Zhang, and Rogerio Feris to build on the work. This includes techniques to imbue models with external knowledge and the development of linear attention transformer methods for higher throughput, compared to other state-of-the-art systems.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Understanding and reasoning in vision and multimodal systems has also seen a boon.&amp;nbsp;Works like “Task2Sim” and “AdaFuse” demonstrate improved vision model performance if pre-training takes place on synthetic data, and how video action recognition can be boosted by fusing channels from past and current feature maps.&lt;/p&gt;&lt;p&gt;As part of a commitment to leaner AI, the lab teams of Gregory Wornell, the MIT EECS Sumitomo Electric Industries Professor in Engineering, IBM Research’s Chuang Gan, and David Cox, VP for foundational AI at IBM Research and the lab’s IBM director, have shown that model adaptability and data efficiency can go hand in hand. Two approaches, EvoScale and Chain-of-Action-Thought reasoning (COAT), enable language models to make the most of limited data and computation by improving on prior generation attempts through structured iteration, narrowing in on a better response.&amp;nbsp;COAT uses a meta-action framework and reinforcement learning to tackle reasoning-intensive tasks via self-correction, while EvoScale brings a similar philosophy to code generation, evolving high-quality candidate solutions. These techniques help to enable resource-conscious, targeted, real-world deployment.&lt;/p&gt;&lt;p&gt;“The impact of MIT-IBM research on our large language model development efforts cannot be overstated,” says Cox. “We’re seeing that smaller, more specialized models and tools are having an outsized impact, especially when they are combined. Innovations from the MIT-IBM Watson AI Lab help shape these technical directions and influence the strategy we are taking in the market through platforms like watsonx.”&lt;/p&gt;&lt;p&gt;For example, numerous lab projects have contributed features, capabilities, and uses to IBM’s Granite Vision, which provides impressive computer vision designed for document understanding, despite its compact size. This comes at a time when there’s a growing need for extraction, interpretation, and trustworthy summarization of information and data contained in long formats for enterprise purposes.&lt;/p&gt;&lt;p&gt;Other achievements that extend beyond direct research on AI and across disciplines are not only beneficial, but necessary for advancing the technology and lifting up society, concludes the 2025 AAAI panel.&lt;/p&gt;&lt;p&gt;Work from the lab’s Caroline Uhler and Devavrat Shah — both Andrew (1956) and Erna Viterbi Professors in EECS and the Institute for Data, Systems, and Society (IDSS) — along with IBM Research’s Kristjan Greenewald, transcends specializations. They are developing causal discovery methods to uncover how interventions affect outcomes, and identify which ones achieve desired results. The studies include developing a framework that can both elucidate how “treatments” for different sub-populations may play out, like on an ecommerce platform or mobility restrictions on morbidity outcomes. Findings from this body of work could influence the fields of marketing and medicine to education and risk management.&lt;/p&gt;&lt;p&gt;“Advances in AI and other areas of computing are influencing how people formulate and tackle challenges in nearly every discipline. At the MIT-IBM Watson AI Lab, researchers recognize this cross-cutting nature of their work and its impact, interrogating problems from multiple viewpoints and bringing real-world problems from industry, in order to develop novel solutions,” says Dan Huttenlocher, MIT lab co-chair, dean of the MIT Schwarzman College of Computing, and the Henry Ellis Warren (1894) Professor of Electrical Engineering and Computer Science.&lt;/p&gt;&lt;p&gt;A significant piece of what makes this research ecosystem thrive is the steady influx of student talent and their contributions through MIT’s Undergraduate Research Opportunities Program (UROP), MIT EECS 6A Program,&amp;nbsp;and the new MIT-IBM Watson AI Lab Internship Program. Altogether, more than 70 young researchers have not only accelerated their technical skill development, but, through guidance and support by the lab’s mentors, gained knowledge in AI domains to become emerging practitioners themselves. This is why the lab continually seeks to identify promising students at all stages in their exploration of AI’s potential.&lt;/p&gt;&lt;p&gt;“In order to unlock the full economic and societal potential of AI, we need to foster ‘useful and efficient intelligence,’” says Sriram Raghavan, IBM Research VP for AI and IBM chair of the lab. “To translate AI promise into progress, it’s crucial that we continue to focus on innovations to develop efficient, optimized, and fit-for-purpose models that can easily be adapted to specific domains and use cases. Academic-industry collaborations, such as the MIT-IBM Watson AI Lab, help drive the breakthroughs that make this possible.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/mit-ibm-watson-AI-that-Matters.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;When it comes to artificial intelligence, MIT and IBM were there at the beginning: laying foundational work and creating some of the first programs — AI predecessors — and theorizing how machine “intelligence” might come to be.&lt;/p&gt;&lt;p&gt;Today, collaborations like the MIT-IBM Watson AI Lab, which launched eight years ago, are continuing to deliver expertise for the promise of tomorrow’s AI technology. This is critical for industries and the labor force that stand to benefit, particularly in the short term: from $3-4 trillion of forecast global economic benefits and 80 percent productivity gains for knowledge workers and creative tasks, to significant incorporations of generative AI into business processes (80 percent) and software applications (70 percent) in the next three years.&lt;/p&gt;&lt;p&gt;While industry has seen a boom in notable models, chiefly in the past year, academia continues to drive the innovation, contributing most of the highly cited research. At the MIT-IBM Watson AI Lab, success takes the form of 54 patent disclosures, an excess of 128,000 citations with an h-index of 162, and more than 50 industry-driven use cases. Some of the lab’s many achievements include improved stent placement with AI imaging techniques, slashing computational overhead, shrinking models while maintaining performance, and modeling of interatomic potential for silicate chemistry.&lt;/p&gt;&lt;p&gt;“The lab is uniquely positioned to identify the ‘right’ problems to solve, setting us apart from other entities,” says Aude Oliva, lab MIT director and director of strategic industry engagement in the MIT Schwarzman College of Computing. “Further, the experience our students gain from working on these challenges for enterprise AI translates to their competitiveness in the job market and the promotion of a competitive industry.”&lt;/p&gt;&lt;p&gt;“The MIT-IBM Watson AI Lab has had tremendous impact by bringing together a rich set of collaborations between IBM and MIT’s researchers and students,” says Provost Anantha Chandrakasan, who is the lab’s MIT co-chair and the Vannevar Bush Professor of Electrical Engineering and Computer Science. “By supporting cross-cutting research at the intersection of AI and many other disciplines, the lab is advancing foundational work and accelerating the development of transformative solutions for our nation and the world.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Long-horizon work&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As AI continues to garner interest, many organizations struggle to channel the technology into meaningful outcomes. A 2024 Gartner study finds that, “at least 30% of generative AI projects will be abandoned after proof of concept by the end of 2025,” demonstrating ambition and widespread hunger for AI, but a lack of knowledge for how to develop and apply it to create immediate value.&lt;/p&gt;&lt;p&gt;Here, the lab shines, bridging research and deployment. The majority of the lab’s current-year research portfolio is aligned to use and develop new features, capacities, or products for IBM, the lab’s corporate members, or real-world applications. The last of these comprise large language models, AI hardware, and foundation models, including multi-modal, bio-medical, and geo-spatial ones. Inquiry-driven students and interns are invaluable in this pursuit, offering enthusiasm and new perspectives while accumulating domain knowledge to help derive and engineer advancements in the field, as well as opening up new frontiers for exploration with AI as a tool.&lt;/p&gt;&lt;p&gt;Findings from the AAAI 2025 Presidential panel on the Future of AI Research support the need for contributions from academia-industry collaborations like the lab in the AI arena: “Academics have a role to play in providing independent advice and interpretations of these results [from industry] and their consequences. The private sector focuses more on the short term, and universities and society more on a longer-term perspective.”&lt;/p&gt;&lt;p&gt;Bringing these strengths together, along with the push for open sourcing and open science, can spark innovation that neither could achieve alone. History shows that embracing these principles, and sharing code and making research accessible, has long-term benefits for both the sector and society. In line with IBM and MIT’s missions, the lab contributes technologies, findings, governance, and standards to the public sphere through this collaboration, thereby enhancing transparency, accelerating reproducibility, and ensuring trustworthy advances.&lt;/p&gt;&lt;p&gt;The lab was created to merge MIT’s deep research expertise with IBM’s industrial R&amp;amp;D capacity, aiming for breakthroughs in core AI methods and hardware, as well as new applications in areas like health care, chemistry, finance, cybersecurity, and robust planning and decision-making for business.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Bigger isn't always better&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Today, large foundation models are giving way to smaller, more task-specific models yielding better performance. Contributions from lab members like Song Han, associate professor in the MIT Department of Electrical Engineering and Computer Science (EECS), and IBM Research’s Chuang Gan help make this possible, through work such as once-for-all and AWQ. Innovations such as these improve efficiency with better architectures, algorithm shrinking, and activation-aware weight quantization, letting models like language processing run on edge devices at faster speeds and reduced latency.&lt;/p&gt;&lt;p&gt;Consequently, foundation, vision, multimodal, and large language models have seen benefits, allowing for the lab research groups of Oliva, MIT EECS Associate Professor Yoon Kim, and IBM Research members Rameswar Panda, Yang Zhang, and Rogerio Feris to build on the work. This includes techniques to imbue models with external knowledge and the development of linear attention transformer methods for higher throughput, compared to other state-of-the-art systems.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Understanding and reasoning in vision and multimodal systems has also seen a boon.&amp;nbsp;Works like “Task2Sim” and “AdaFuse” demonstrate improved vision model performance if pre-training takes place on synthetic data, and how video action recognition can be boosted by fusing channels from past and current feature maps.&lt;/p&gt;&lt;p&gt;As part of a commitment to leaner AI, the lab teams of Gregory Wornell, the MIT EECS Sumitomo Electric Industries Professor in Engineering, IBM Research’s Chuang Gan, and David Cox, VP for foundational AI at IBM Research and the lab’s IBM director, have shown that model adaptability and data efficiency can go hand in hand. Two approaches, EvoScale and Chain-of-Action-Thought reasoning (COAT), enable language models to make the most of limited data and computation by improving on prior generation attempts through structured iteration, narrowing in on a better response.&amp;nbsp;COAT uses a meta-action framework and reinforcement learning to tackle reasoning-intensive tasks via self-correction, while EvoScale brings a similar philosophy to code generation, evolving high-quality candidate solutions. These techniques help to enable resource-conscious, targeted, real-world deployment.&lt;/p&gt;&lt;p&gt;“The impact of MIT-IBM research on our large language model development efforts cannot be overstated,” says Cox. “We’re seeing that smaller, more specialized models and tools are having an outsized impact, especially when they are combined. Innovations from the MIT-IBM Watson AI Lab help shape these technical directions and influence the strategy we are taking in the market through platforms like watsonx.”&lt;/p&gt;&lt;p&gt;For example, numerous lab projects have contributed features, capabilities, and uses to IBM’s Granite Vision, which provides impressive computer vision designed for document understanding, despite its compact size. This comes at a time when there’s a growing need for extraction, interpretation, and trustworthy summarization of information and data contained in long formats for enterprise purposes.&lt;/p&gt;&lt;p&gt;Other achievements that extend beyond direct research on AI and across disciplines are not only beneficial, but necessary for advancing the technology and lifting up society, concludes the 2025 AAAI panel.&lt;/p&gt;&lt;p&gt;Work from the lab’s Caroline Uhler and Devavrat Shah — both Andrew (1956) and Erna Viterbi Professors in EECS and the Institute for Data, Systems, and Society (IDSS) — along with IBM Research’s Kristjan Greenewald, transcends specializations. They are developing causal discovery methods to uncover how interventions affect outcomes, and identify which ones achieve desired results. The studies include developing a framework that can both elucidate how “treatments” for different sub-populations may play out, like on an ecommerce platform or mobility restrictions on morbidity outcomes. Findings from this body of work could influence the fields of marketing and medicine to education and risk management.&lt;/p&gt;&lt;p&gt;“Advances in AI and other areas of computing are influencing how people formulate and tackle challenges in nearly every discipline. At the MIT-IBM Watson AI Lab, researchers recognize this cross-cutting nature of their work and its impact, interrogating problems from multiple viewpoints and bringing real-world problems from industry, in order to develop novel solutions,” says Dan Huttenlocher, MIT lab co-chair, dean of the MIT Schwarzman College of Computing, and the Henry Ellis Warren (1894) Professor of Electrical Engineering and Computer Science.&lt;/p&gt;&lt;p&gt;A significant piece of what makes this research ecosystem thrive is the steady influx of student talent and their contributions through MIT’s Undergraduate Research Opportunities Program (UROP), MIT EECS 6A Program,&amp;nbsp;and the new MIT-IBM Watson AI Lab Internship Program. Altogether, more than 70 young researchers have not only accelerated their technical skill development, but, through guidance and support by the lab’s mentors, gained knowledge in AI domains to become emerging practitioners themselves. This is why the lab continually seeks to identify promising students at all stages in their exploration of AI’s potential.&lt;/p&gt;&lt;p&gt;“In order to unlock the full economic and societal potential of AI, we need to foster ‘useful and efficient intelligence,’” says Sriram Raghavan, IBM Research VP for AI and IBM chair of the lab. “To translate AI promise into progress, it’s crucial that we continue to focus on innovations to develop efficient, optimized, and fit-for-purpose models that can easily be adapted to specific domains and use cases. Academic-industry collaborations, such as the MIT-IBM Watson AI Lab, help drive the breakthroughs that make this possible.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/creating-ai-that-matters-1021</guid><pubDate>Tue, 21 Oct 2025 20:10:00 +0000</pubDate></item><item><title>[NEW] Sources: Multimodal AI startup Fal.ai already raised at $4B+ valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/sources-multimodal-ai-startup-fal-ai-already-raised-at-4b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/2025-vc-predictions.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Fal.ai, a startup that hosts image, video, and audio AI models for developers, has closed a new round valuing the company at over $4 billion, four people familiar with the deal said. The company raised approximately $250 million, two of the people said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Major investors in the round are Kleiner Perkins and Sequoia, according to our sources. Fal didn’t respond to a request for comment. Sequoia and Kleiner Perkins declined to comment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new round is coming less than three months after Fal announced a $125 million Series C at a $1.5 billion valuation led by Meritech. At that time, the company’s revenue crossed $95 million and it’s platform was used by over two million developers, Todd Jackson, a partner at First Round Capital wrote on LinkedIn. That was massive growth from a year ago, when TechCrunch reported Fal had $10 million in annualized recurring revenue (ARR) and 500,000 developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Fal provides the infrastructure layer for multimodal AI models (as well as media-specific ones), the company’s explosive growth is directly tied to the user adoption of applications built on top of it. And multimodal AI is in heavy demand right now, especially video, as evidenced by the soaring popularity of OpenAI’s Sora, which surged to the top of the U.S. App Store even faster than ChatGPT did. &amp;nbsp;This massive consumer demand for applications like Sora underscores the market potential of Fal’s offering.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fal provides developers with over 600 image, video, audio and 3D models, it says, boasts that its cloud has thousands of Nvidia H100 and H200 GPUs and is fine-tuned for speedy inference. It offers tools for customizing models as well.  Its offerings include access via API, hosted via a flexible serverless offering, or via enterprise-ready compute clusters. While there are certainly other competitors offering model and app hosting services (Microsoft, Google, &lt;br /&gt;CoreWeave, to name a few), Fal’s singular focus on media and multimodal is its competitive selling point, VCs like Jackson say.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s customers range from individual developers to large companies including Adobe, Canva, Perplexity, and Shopify. &amp;nbsp;Some of the popular use cases include media creation for advertising, e-commerce, and gaming content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup was co-founded in 2021 by Burkay Gur, a former Coinbase machine learning leader and Oracle engineer, and Gorkem Yurtseven, who was previously a developer at Amazon. Gur and Yurtseven saw an opportunity for personalized multimedia generation. While other technologists pursued LLMs, they zeroed in on optimizing Stable Diffusion for speed and scale, and have since expanded to hosting many other such models.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Fal previously raised nearly $200 million, according to PitchBook data. The company’s existing investors include Bessemer Venture Partners, Kindred Ventures, Andreessen Horowitz, Notable Capital, First Round Capital, Unusual Ventures and Village Global.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/2025-vc-predictions.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Fal.ai, a startup that hosts image, video, and audio AI models for developers, has closed a new round valuing the company at over $4 billion, four people familiar with the deal said. The company raised approximately $250 million, two of the people said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Major investors in the round are Kleiner Perkins and Sequoia, according to our sources. Fal didn’t respond to a request for comment. Sequoia and Kleiner Perkins declined to comment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The new round is coming less than three months after Fal announced a $125 million Series C at a $1.5 billion valuation led by Meritech. At that time, the company’s revenue crossed $95 million and it’s platform was used by over two million developers, Todd Jackson, a partner at First Round Capital wrote on LinkedIn. That was massive growth from a year ago, when TechCrunch reported Fal had $10 million in annualized recurring revenue (ARR) and 500,000 developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Fal provides the infrastructure layer for multimodal AI models (as well as media-specific ones), the company’s explosive growth is directly tied to the user adoption of applications built on top of it. And multimodal AI is in heavy demand right now, especially video, as evidenced by the soaring popularity of OpenAI’s Sora, which surged to the top of the U.S. App Store even faster than ChatGPT did. &amp;nbsp;This massive consumer demand for applications like Sora underscores the market potential of Fal’s offering.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fal provides developers with over 600 image, video, audio and 3D models, it says, boasts that its cloud has thousands of Nvidia H100 and H200 GPUs and is fine-tuned for speedy inference. It offers tools for customizing models as well.  Its offerings include access via API, hosted via a flexible serverless offering, or via enterprise-ready compute clusters. While there are certainly other competitors offering model and app hosting services (Microsoft, Google, &lt;br /&gt;CoreWeave, to name a few), Fal’s singular focus on media and multimodal is its competitive selling point, VCs like Jackson say.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s customers range from individual developers to large companies including Adobe, Canva, Perplexity, and Shopify. &amp;nbsp;Some of the popular use cases include media creation for advertising, e-commerce, and gaming content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup was co-founded in 2021 by Burkay Gur, a former Coinbase machine learning leader and Oracle engineer, and Gorkem Yurtseven, who was previously a developer at Amazon. Gur and Yurtseven saw an opportunity for personalized multimedia generation. While other technologists pursued LLMs, they zeroed in on optimizing Stable Diffusion for speed and scale, and have since expanded to hosting many other such models.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Fal previously raised nearly $200 million, according to PitchBook data. The company’s existing investors include Bessemer Venture Partners, Kindred Ventures, Andreessen Horowitz, Notable Capital, First Round Capital, Unusual Ventures and Village Global.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/sources-multimodal-ai-startup-fal-ai-already-raised-at-4b-valuation/</guid><pubDate>Tue, 21 Oct 2025 20:12:41 +0000</pubDate></item><item><title>[NEW] Estrada signs with the Dodgers (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124758/estrada-signs-with-the-dodgers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT-Mason-Estrada-01-press_0.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Like almost any MIT student, Mason Estrada wants to take what he learned on campus and apply it to the working world. Unlike any other current MIT student, Estrada’s&amp;nbsp;primary workplace is&amp;nbsp;a pitcher’s mound.&lt;/p&gt;  &lt;p&gt;Estrada, the star pitcher for MIT’s baseball team, has signed a contract with the Los Angeles Dodgers, who selected him in the seventh round of the Major League Baseball draft on July 14. The right-hander, whose fastball has reached 96 miles per hour, is taking a leave of absence from the Institute and reported to the Dodgers’ instructional camp in Arizona.&lt;/p&gt;  &lt;p&gt;An aero-astro major, Estrada says that pitching at MIT has never involved transferring aerodynamic knowledge from the classroom to the mound. Still, he says, he’s benefited as an athlete from “learning to think like an engineer generally, learning to think through problems the right way and finding the best solution.”&lt;/p&gt;  &lt;p&gt;In the 2025 season Estrada went 6–0 with a 2.21 ERA, striking out 66. He is the fifth MIT undergraduate selected in baseball’s draft, of whom one—Jason Szuminski ’00—reached the majors, with the San Diego Padres.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT-Mason-Estrada-01-press_0.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Like almost any MIT student, Mason Estrada wants to take what he learned on campus and apply it to the working world. Unlike any other current MIT student, Estrada’s&amp;nbsp;primary workplace is&amp;nbsp;a pitcher’s mound.&lt;/p&gt;  &lt;p&gt;Estrada, the star pitcher for MIT’s baseball team, has signed a contract with the Los Angeles Dodgers, who selected him in the seventh round of the Major League Baseball draft on July 14. The right-hander, whose fastball has reached 96 miles per hour, is taking a leave of absence from the Institute and reported to the Dodgers’ instructional camp in Arizona.&lt;/p&gt;  &lt;p&gt;An aero-astro major, Estrada says that pitching at MIT has never involved transferring aerodynamic knowledge from the classroom to the mound. Still, he says, he’s benefited as an athlete from “learning to think like an engineer generally, learning to think through problems the right way and finding the best solution.”&lt;/p&gt;  &lt;p&gt;In the 2025 season Estrada went 6–0 with a 2.21 ERA, striking out 66. He is the fifth MIT undergraduate selected in baseball’s draft, of whom one—Jason Szuminski ’00—reached the majors, with the San Diego Padres.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124758/estrada-signs-with-the-dodgers/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] A I-designed compounds can kill drug-resistant bacteria (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124755/a-i-designed-compounds-can-kill-drug-resistant-bacteria/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AdobeStock_854002342.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;With help from artificial intelligence, MIT researchers have designed novel antibiotics that can combat two hard-to-treat bacteria: multi-drug-­resistant &lt;em&gt;Neisseria gonorrhoeae&lt;/em&gt; and &lt;em&gt;Staphylococcus aureus &lt;/em&gt;(MRSA).&lt;/p&gt;  &lt;p&gt;The team used two approaches. First, they directed generative AI to design molecules based on a chemical fragment their model had predicted would show antimicrobial activity, and second, they let the algorithms generate molecules without constraints. They designed more than 36 million possible compounds this way and computationally screened them for antimicrobial properties.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The top candidates they discovered are structurally distinct from any existing antibiotics, and they appear to work by novel mechanisms that disrupt bacterial cell membranes. This makes them less vulnerable to antibiotic resistance, a growing problem: It is estimated that drug-­resistant bacterial infections cause nearly 5 million deaths per year worldwide.&lt;/p&gt;  &lt;p&gt;Now that they can generate and evaluate compounds that have never been seen before, the researchers hope they can use the same strategy to identify and design drugs that attack other species of bacteria.&lt;/p&gt;  &lt;p&gt;“We’re excited about the new possibilities that this project opens up for antibiotics development,” says James Collins, a professor of biological engineering and the senior author of the study. “Our work shows the power of AI from a drug design standpoint and enables us to exploit much larger chemical spaces that were previously inaccessible.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AdobeStock_854002342.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;With help from artificial intelligence, MIT researchers have designed novel antibiotics that can combat two hard-to-treat bacteria: multi-drug-­resistant &lt;em&gt;Neisseria gonorrhoeae&lt;/em&gt; and &lt;em&gt;Staphylococcus aureus &lt;/em&gt;(MRSA).&lt;/p&gt;  &lt;p&gt;The team used two approaches. First, they directed generative AI to design molecules based on a chemical fragment their model had predicted would show antimicrobial activity, and second, they let the algorithms generate molecules without constraints. They designed more than 36 million possible compounds this way and computationally screened them for antimicrobial properties.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The top candidates they discovered are structurally distinct from any existing antibiotics, and they appear to work by novel mechanisms that disrupt bacterial cell membranes. This makes them less vulnerable to antibiotic resistance, a growing problem: It is estimated that drug-­resistant bacterial infections cause nearly 5 million deaths per year worldwide.&lt;/p&gt;  &lt;p&gt;Now that they can generate and evaluate compounds that have never been seen before, the researchers hope they can use the same strategy to identify and design drugs that attack other species of bacteria.&lt;/p&gt;  &lt;p&gt;“We’re excited about the new possibilities that this project opens up for antibiotics development,” says James Collins, a professor of biological engineering and the senior author of the study. “Our work shows the power of AI from a drug design standpoint and enables us to exploit much larger chemical spaces that were previously inaccessible.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124755/a-i-designed-compounds-can-kill-drug-resistant-bacteria/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Walking faster, hanging out less (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124752/walking-faster-hanging-out-less/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AdobeStock_276102896.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;City life is often described as “fast-paced.” A study coauthored by MIT scholars suggests that’s more true than ever: The average walking speed in three northeastern US cities increased 15% from 1980 to 2010, while the number of people lingering in public spaces declined by 14%.&lt;/p&gt;  &lt;p&gt;The researchers used machine-learning tools to assess 1980s-era video footage captured in Boston, New York, and Philadelphia by William Whyte, an urbanist and social thinker best known as the author of &lt;em&gt;The Organization Man&lt;/em&gt;. They compared the old material with newer videos from the same locations.&lt;/p&gt;  &lt;p&gt;“Something has changed over the past 40 years,” says coauthor Carlo Ratti, director of MIT’s Senseable City Lab. “Public spaces are working in somewhat different ways, more as a thoroughfare and less a space of encounter.” The scholars speculate that some of the reasons may have to do with cell phones and Starbucks: People text each other to meet up instead of hanging around to encounter each other in public, and when they do get together, they often choose an indoor space like a coffee shop.&lt;/p&gt;  &lt;p&gt;The results could help designers seeking to create new public areas or modify existing ones. “Public space is such an important element of civic life, and today partly because it counteracts the polarization of digital space,” says Arianna Salazar-Miranda, MCP ’16, PhD ’23, an assistant professor at Yale and another coauthor. “The more we can keep improving public space, the more we can make our cities suited for convening.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AdobeStock_276102896.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;City life is often described as “fast-paced.” A study coauthored by MIT scholars suggests that’s more true than ever: The average walking speed in three northeastern US cities increased 15% from 1980 to 2010, while the number of people lingering in public spaces declined by 14%.&lt;/p&gt;  &lt;p&gt;The researchers used machine-learning tools to assess 1980s-era video footage captured in Boston, New York, and Philadelphia by William Whyte, an urbanist and social thinker best known as the author of &lt;em&gt;The Organization Man&lt;/em&gt;. They compared the old material with newer videos from the same locations.&lt;/p&gt;  &lt;p&gt;“Something has changed over the past 40 years,” says coauthor Carlo Ratti, director of MIT’s Senseable City Lab. “Public spaces are working in somewhat different ways, more as a thoroughfare and less a space of encounter.” The scholars speculate that some of the reasons may have to do with cell phones and Starbucks: People text each other to meet up instead of hanging around to encounter each other in public, and when they do get together, they often choose an indoor space like a coffee shop.&lt;/p&gt;  &lt;p&gt;The results could help designers seeking to create new public areas or modify existing ones. “Public space is such an important element of civic life, and today partly because it counteracts the polarization of digital space,” says Arianna Salazar-Miranda, MCP ’16, PhD ’23, an assistant professor at Yale and another coauthor. “The more we can keep improving public space, the more we can make our cities suited for convening.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124752/walking-faster-hanging-out-less/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] A bionic knee restores natural movement (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124749/a-bionic-knee-restores-natural-movement/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;MIT researchers have developed a new bionic knee that is integrated directly with the user’s muscle and bone tissue. It can help people with above-the-knee amputations walk faster, climb stairs, and avoid obstacles more easily than they could with a traditional prosthesis, which is attached to the residual limb by means of a socket and can be uncomfortable.&lt;/p&gt;  &lt;p&gt;For several years, Hugh Herr, SM ’93, co-director of the K. Lisa Yang Center for Bionics, has been working with his colleagues on techniques that can extract neural information from muscles left behind after an amputation and use that information to help guide a prosthetic limb. The approach, known as agonist-antagonist myoneuronal interface (AMI), has been shown to help people with below-the-knee amputations walk faster and navigate around obstacles much more naturally.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="model of bionic knees" class="wp-image-1125819" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT_Bionic-Knee-only.jpg?w=1068" /&gt;&lt;figcaption class="wp-element-caption"&gt;The new system is anchored to the bone and controlled by the nervous system, offering more stability and easier navigation.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF THE RESEARCHERS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In the new study, the researchers developed a procedure to insert a titanium rod into the residual femur bone of people who had amputations above the knee. This implant allows for better mechanical control and load bearing than a traditional prosthesis. It also contains 16 wires that collect information from electrodes located on the AMI muscles inside the body, offering better neuroprosthetic control.&lt;/p&gt;  &lt;p&gt;Two people who received the implant in a clinical study performed better on several types of tasks, including stair climbing, and reported that the limb felt more like a part of their own body, compared with people who had more traditional above-the-knee amputations and used conventional prostheses.&lt;/p&gt;  &lt;p&gt;“A prosthesis that’s tissue-integrated—anchored to the bone and directly controlled by the nervous system—is not merely a lifeless, separate device,” says Herr, but rather “an integral part of self.” The system will need larger trials to receive FDA approval for commercial use, which he expects may take about five years.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;MIT researchers have developed a new bionic knee that is integrated directly with the user’s muscle and bone tissue. It can help people with above-the-knee amputations walk faster, climb stairs, and avoid obstacles more easily than they could with a traditional prosthesis, which is attached to the residual limb by means of a socket and can be uncomfortable.&lt;/p&gt;  &lt;p&gt;For several years, Hugh Herr, SM ’93, co-director of the K. Lisa Yang Center for Bionics, has been working with his colleagues on techniques that can extract neural information from muscles left behind after an amputation and use that information to help guide a prosthetic limb. The approach, known as agonist-antagonist myoneuronal interface (AMI), has been shown to help people with below-the-knee amputations walk faster and navigate around obstacles much more naturally.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="model of bionic knees" class="wp-image-1125819" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT_Bionic-Knee-only.jpg?w=1068" /&gt;&lt;figcaption class="wp-element-caption"&gt;The new system is anchored to the bone and controlled by the nervous system, offering more stability and easier navigation.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF THE RESEARCHERS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In the new study, the researchers developed a procedure to insert a titanium rod into the residual femur bone of people who had amputations above the knee. This implant allows for better mechanical control and load bearing than a traditional prosthesis. It also contains 16 wires that collect information from electrodes located on the AMI muscles inside the body, offering better neuroprosthetic control.&lt;/p&gt;  &lt;p&gt;Two people who received the implant in a clinical study performed better on several types of tasks, including stair climbing, and reported that the limb felt more like a part of their own body, compared with people who had more traditional above-the-knee amputations and used conventional prostheses.&lt;/p&gt;  &lt;p&gt;“A prosthesis that’s tissue-integrated—anchored to the bone and directly controlled by the nervous system—is not merely a lifeless, separate device,” says Herr, but rather “an integral part of self.” The system will need larger trials to receive FDA approval for commercial use, which he expects may take about five years.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124749/a-bionic-knee-restores-natural-movement/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Biodiversity: A missing link in combating climate change (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124746/biodiversity-a-missing-link-in-combating-climate-change/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT-Seed-Dispersal-Christian-Ziegler-01-press.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;A lot of attention has been paid to how climate change can reduce biodiversity. Now MIT researchers have shown that the reverse is also true: Loss of biodiversity can jeopardize regrowth of tropical forests, one of Earth’s most powerful tools for mitigating climate change.&lt;/p&gt;  &lt;p&gt;Combining data from thousands of previous studies and using new tools for quantifying interconnected ecological processes, the researchers analyzed numerous tropical sites where deforestation was being followed by natural regrowth, focusing on the role of animals such as birds and monkeys that spread plant seeds by eating them in one place and then defecating someplace else. Evan Fricke, a research scientist in the MIT Department of Civil and Environmental Engineering and the lead author of a paper on the work, has studied such animals for 15 years, showing that without their role, trees have lower survival rates and a harder time keeping up with environmental changes.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Since tropical forests are Earth’s largest land-based carbon sink, such challenges make it harder to fight climate change. But the influence of biodiversity on forests’ ability to absorb carbon has not been fully quantified.&lt;/p&gt;  &lt;p&gt;To do that, the researchers looked at data on where seed-dispersing animals live, how many seeds each animal disperses, and how they affect germination. Then they incorporated data revealing the impact of human activity such as hunting and forest degradation. They found, for example, that the animals move less, and thus spread seeds less widely, in areas with a bigger human footprint.&lt;/p&gt; 
 &lt;p&gt;With the data, the researchers created an index that revealed a link between human activities and declines in seed dispersal. They analyzed the relationship between that index and records of carbon accumulation in naturally regrowing tropical forests over time, controlling for factors like droughts, fires, and livestock grazing.&lt;/p&gt;  &lt;p&gt;“What’s particularly new about this study is we’re actually getting the numbers around these effects,” Fricke says. In particular, they found that naturally regrowing forests with healthy populations of seed-dispersing animals absorbed up to four times more carbon than those without as many. Meanwhile, in sites identified as suitable for reforestation, current levels of disruption to seed dispersal reduce the potential for regrowth by 57%.&lt;/p&gt; 
 &lt;p&gt;These findings could help direct reforestation strategies. “In the discussion around planting trees versus allowing trees to regrow naturally, regrowth is basically free, whereas planting trees costs money, and it also leads to less diverse forests,” says César Terrer, a professor of civil and environmental engineering and a coauthor of the paper. “Now we can understand where natural regrowth can happen effectively because there are animals planting the seeds for free, and we also can identify areas where, because animals are affected, natural regrowth is not going to happen, and therefore planting trees actively is necessary.”&lt;/p&gt;  &lt;p&gt;The researchers encourage action to protect or improve animal habitats, reduce pressures on seed-dispersing species, and potentially reintroduce them where they’ve been lost. Overall, they hope the study helps improve our understanding of the planet’s complex ecological processes.&lt;/p&gt;  &lt;p&gt;“When we lose our animals, we’re losing the ecological infrastructure that keeps our tropical forests healthy and resilient,” Fricke says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT-Seed-Dispersal-Christian-Ziegler-01-press.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;A lot of attention has been paid to how climate change can reduce biodiversity. Now MIT researchers have shown that the reverse is also true: Loss of biodiversity can jeopardize regrowth of tropical forests, one of Earth’s most powerful tools for mitigating climate change.&lt;/p&gt;  &lt;p&gt;Combining data from thousands of previous studies and using new tools for quantifying interconnected ecological processes, the researchers analyzed numerous tropical sites where deforestation was being followed by natural regrowth, focusing on the role of animals such as birds and monkeys that spread plant seeds by eating them in one place and then defecating someplace else. Evan Fricke, a research scientist in the MIT Department of Civil and Environmental Engineering and the lead author of a paper on the work, has studied such animals for 15 years, showing that without their role, trees have lower survival rates and a harder time keeping up with environmental changes.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Since tropical forests are Earth’s largest land-based carbon sink, such challenges make it harder to fight climate change. But the influence of biodiversity on forests’ ability to absorb carbon has not been fully quantified.&lt;/p&gt;  &lt;p&gt;To do that, the researchers looked at data on where seed-dispersing animals live, how many seeds each animal disperses, and how they affect germination. Then they incorporated data revealing the impact of human activity such as hunting and forest degradation. They found, for example, that the animals move less, and thus spread seeds less widely, in areas with a bigger human footprint.&lt;/p&gt; 
 &lt;p&gt;With the data, the researchers created an index that revealed a link between human activities and declines in seed dispersal. They analyzed the relationship between that index and records of carbon accumulation in naturally regrowing tropical forests over time, controlling for factors like droughts, fires, and livestock grazing.&lt;/p&gt;  &lt;p&gt;“What’s particularly new about this study is we’re actually getting the numbers around these effects,” Fricke says. In particular, they found that naturally regrowing forests with healthy populations of seed-dispersing animals absorbed up to four times more carbon than those without as many. Meanwhile, in sites identified as suitable for reforestation, current levels of disruption to seed dispersal reduce the potential for regrowth by 57%.&lt;/p&gt; 
 &lt;p&gt;These findings could help direct reforestation strategies. “In the discussion around planting trees versus allowing trees to regrow naturally, regrowth is basically free, whereas planting trees costs money, and it also leads to less diverse forests,” says César Terrer, a professor of civil and environmental engineering and a coauthor of the paper. “Now we can understand where natural regrowth can happen effectively because there are animals planting the seeds for free, and we also can identify areas where, because animals are affected, natural regrowth is not going to happen, and therefore planting trees actively is necessary.”&lt;/p&gt;  &lt;p&gt;The researchers encourage action to protect or improve animal habitats, reduce pressures on seed-dispersing species, and potentially reintroduce them where they’ve been lost. Overall, they hope the study helps improve our understanding of the planet’s complex ecological processes.&lt;/p&gt;  &lt;p&gt;“When we lose our animals, we’re losing the ecological infrastructure that keeps our tropical forests healthy and resilient,” Fricke says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124746/biodiversity-a-missing-link-in-combating-climate-change/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Navigating MIT (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124743/navigating-mit/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Take a stroll along the Infinite Corridor these days and you’ll encounter a striking new space, in a prominent location on the first floor of Building 11. With bright blue seating modules, orange accents, and an eye-catching design, it looks like a futuristic space station, sleek and ultramodern—but also welcoming and fun.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is the new home of the Undergraduate Advising Center (UAC). And while the design might be surprising, the creation of the center is no surprise at all. It’s simply another example of MIT’s ongoing innovation in improving student advising.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1126023" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/A-Home-for-Advising_2.jpg?w=1728" /&gt;&lt;div class="image-credit"&gt;MERGE ARCHITECTS (RENDERING)&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The MIT experience looks different for everyone, and the UAC was launched in 2023 with that in mind, offering individualized support to help undergraduates reach their full potential. The new hub brings together the people and programs dedicated to helping them navigate MIT, offering guidance that’s both personalized and proactive, with an emphasis on identifying students who might need help and reaching out to them sooner rather than later.&lt;/p&gt;  &lt;p&gt;The 5,000-square-foot space, designed by Boston’s Merge Architects, reflects the needs of our students, who offered input on lighting (natural), seating (comfortable), and multifunctional areas that can be used for everything from private conversations to large-scale gatherings.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I don’t have to tell all of you that there’s more to thriving at MIT than being incredibly smart. UAC advisors help students set goals, manage their time, and build relationships with faculty. And they help students navigate what’s often called the “hidden curriculum”—the unspoken norms and values of university life. Once they’ve chosen their majors, students are assigned a faculty advisor, but that doesn’t mean their UAC advisors step aside—these relationships continue for all four years, providing&lt;br /&gt;a sense of continuity and care.&lt;/p&gt;  &lt;p&gt;Already, the new hub is in constant use for consultations, study sessions, and impromptu visits to grab a snack and catch up with friends. And it’s much more than a physical upgrade—it’s a symbol of our commitment to continually strengthening advising resources for all students, from orientation to the moment they finally turn their Brass Rats.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Take a stroll along the Infinite Corridor these days and you’ll encounter a striking new space, in a prominent location on the first floor of Building 11. With bright blue seating modules, orange accents, and an eye-catching design, it looks like a futuristic space station, sleek and ultramodern—but also welcoming and fun.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is the new home of the Undergraduate Advising Center (UAC). And while the design might be surprising, the creation of the center is no surprise at all. It’s simply another example of MIT’s ongoing innovation in improving student advising.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1126023" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/A-Home-for-Advising_2.jpg?w=1728" /&gt;&lt;div class="image-credit"&gt;MERGE ARCHITECTS (RENDERING)&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The MIT experience looks different for everyone, and the UAC was launched in 2023 with that in mind, offering individualized support to help undergraduates reach their full potential. The new hub brings together the people and programs dedicated to helping them navigate MIT, offering guidance that’s both personalized and proactive, with an emphasis on identifying students who might need help and reaching out to them sooner rather than later.&lt;/p&gt;  &lt;p&gt;The 5,000-square-foot space, designed by Boston’s Merge Architects, reflects the needs of our students, who offered input on lighting (natural), seating (comfortable), and multifunctional areas that can be used for everything from private conversations to large-scale gatherings.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I don’t have to tell all of you that there’s more to thriving at MIT than being incredibly smart. UAC advisors help students set goals, manage their time, and build relationships with faculty. And they help students navigate what’s often called the “hidden curriculum”—the unspoken norms and values of university life. Once they’ve chosen their majors, students are assigned a faculty advisor, but that doesn’t mean their UAC advisors step aside—these relationships continue for all four years, providing&lt;br /&gt;a sense of continuity and care.&lt;/p&gt;  &lt;p&gt;Already, the new hub is in constant use for consultations, study sessions, and impromptu visits to grab a snack and catch up with friends. And it’s much more than a physical upgrade—it’s a symbol of our commitment to continually strengthening advising resources for all students, from orientation to the moment they finally turn their Brass Rats.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124743/navigating-mit/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] How Millie Dresselhaus paid it forward (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124731/how-millie-dresselhaus-paid-it-forward/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Institute Professor Mildred “Millie” Dresselhaus forever altered our understanding of matter—the physical stuff of the universe that has mass and takes up space. Over 57 years at MIT, Dresselhaus also played a significant role in inspiring people to use this new knowledge to tackle some of the world’s greatest challenges, from producing clean energy to curing cancer. Although she became an emerita professor in 2007, Dresselhaus, who taught electrical engineering and physics, remained actively involved in research and all other aspects of MIT life until her death in 2017. She would have been 95 this November.&lt;/p&gt;  &lt;p&gt;Known as the “Queen of Carbon,” Dresselhaus was most often heralded for her pioneering work with one of nature’s most abundant and versatile substances. As a result of her insatiable curiosity about our world and her nearly six-decade career as a scientific explorer, we can thank her for significant leaps in how we think about carbon’s various forms and the company it keeps. In her early career, Dresselhaus employed a then-new invention—laser light—to probe carbon’s inner workings. She worked to distinguish how, for example, flat sheets of carbon atoms act differently from carbon crystals of three dimensions, especially in the presence of heat, electrons, or a magnetic field. And later she predicted the existence of what we now call carbon nanotubes, sheets of carbon atoms rolled up into minuscule cylinders that can be remarkably adept at conducting electricity.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Building on Dresselhaus’s far-reaching foundational research, scientists and engineers have made enormous advances at the nanoscale—with structures on the order of one hundred-thousandth the width of a human hair. Spherical carbon “buckyballs,” cylindrical carbon nanotubes, and two-dimensional carbon sheets known as graphene have already been used for energy storage, medical research, building materials, and paper-thin electronics, among many other applications. Today, these carbon structures continue to be developed for myriad novel uses that often seem taken from the realm of science fiction, including ultrafast quantum computers, efficient desalination devices, and quantum dots with applications in biosensing and drug delivery. For her work she won—among other honors—the Kavli Prize in Nanoscience, the National Medal of Science, and the Presidential Medal of Freedom, the highest civilian award given by the United States government.&lt;/p&gt;  &lt;p&gt;But her journey to MIT, and to global leadership in solid-state physics, was an improbable one. Born in Brooklyn, New York, to immigrant parents in 1930, Dresselhaus came of age at a time when women were rarely welcomed as scientists or encouraged to pursue technical fields. Yet she benefited from several key mentors who saw her potential and took deliberate steps to support a brilliant young mind.&amp;nbsp;&lt;/p&gt; 
&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1124857" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/2DY6F4G.jpg?w=1661" width="1661" /&gt;&lt;figcaption class="wp-element-caption"&gt;President Barack Obama presented Dresselhaus with the Presidential Medal of Freedom in 2014.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;OLIVIER DOULIERY/ABACAPRESS.COM VIA ALAMY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;One of those mentors was Enrico Fermi, the distinguished Italian-born nuclear scientist who played a leading role in the Manhattan Project and who concluded his career as a professor of physics at the University of Chicago. Fermi came to America after receiving a solo Nobel Prize in 1938 (for work on induced radioactivity) and then fleeing the Nazi regime with his Jewish wife, Laura. The story of how Fermi influenced an up-and-coming Millie Dresselhaus—and, by proxy, scores of students who would study under her—reveals how paying it forward to the next generation of scientists and engineers can yield lasting dividends.&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;In 1953, with the nuclear age firmly underway and the Cold War heating up, Dresselhaus found herself, at 22, one of the new graduate students within the University of Chicago’s world-class physics department. Although a number of researchers who had worked on the Manhattan Project there had by then left for other opportunities, many luminaries remained. In addition to the renowned Enrico Fermi, notable faculty included the Nobel laureates Harold Urey and Maria Goeppert Mayer (with whom Dresselhaus lived for about a year as a boarder) as well as the physicist Leona Woods, the only woman present during the famous 1942 fission demonstration on one of the school’s squash courts.&lt;/p&gt; 
 &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;The university’s physics program was fairly small in those days: Dresselhaus had earned a spot as one of just about a dozen new graduate students that year. She was also, it turns out, the only female student in the department. Despite a master’s degree in physics from Radcliffe College and a Fulbright fellowship at the University of Cambridge, she felt not quite prepared as she began her PhD. And so, at the start of her doctoral studies, she discovered a cache of old examinations, and she worked the problems therein forward and back until she felt up to speed.&lt;/p&gt;    &lt;p&gt;Despite this added practice, the coursework for first-year PhD candidates was brutal—so brutal that around three-quarters of all entering physics students eventually dropped out of the program. But Dresselhaus’s relationship with Fermi would provide an unexpected boost.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;She first encountered the unflappable scientist—who made crucial strides not only in the development of the atomic bomb but in particle physics after the war—as a student in his class on quantum mechanics. And through that class, Dresselhaus got to know his teaching style, which she recalled as patient, inspiring, and mind-opening. With a slow, deliberate, accented voice that Dresselhaus described as “halting,” Fermi expertly distilled complicated topics so that anyone in attendance could comprehend them. Brilliant at both theory and experimentation, he delighted in stripping concepts to their essence, and unlike more impatient professors who were absorbed in their own work, Fermi cherished the opportunity to review whatever he knew about a physical concept by explaining it to someone else. For this he clearly had a talent; thanks to the way he presented the finer details of quantum mechanics, Dresselhaus explained, “any youngster could think, when they heard the lecture, that they understood every word.”&lt;/p&gt;  &lt;p&gt;One key to the eminent scientist’s clarity was the ban he placed on taking notes. Fermi demanded full attention, so he would prepare and dole out handwritten notes before his lectures, lest students be tempted to take out their pens or slide rules. “What was so impressive and amazing about it is that the lectures were very exciting, whatever the subject was,” Dresselhaus said in a 2001 interview.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1124860" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/GettyImages-3245871.jpg?w=1658" width="1658" /&gt;&lt;figcaption class="wp-element-caption"&gt;Nuclear scientist Enrico Fermi, shown here circa 1942, was a key early mentor to Dresselhaus at the University of Chicago.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;HULTON ARCHIVE/GETTY IMAGES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;And then there was the homework, which was always tricky, but delightfully enlightening once you figured it out. At the end of every class, Fermi floated a seemingly simple problem to be solved as an exercise prior to the following lecture. These included questions like: Why is the sky blue? Why do the sun and stars emit spectra of light? And, famously, how many piano tuners are there in Chicago? “You thought it was simple until you got home,” Dresselhaus said in 2012, upon receiving the Enrico Fermi Award, a lifetime achievement award given by the US Department of Energy. These types of questions became known, collectively, as “Fermi problems” and are taught today in schools around the world, from kindergarten all the way to graduate-level courses, as examples of how to estimate and triangulate in search of an answer, even when you don’t know all the relevant—and seemingly necessary—parameters. Back when Dresselhaus was learning about such problems, all she knew was they were due by the next class, no more than a day or two away, and they took a significant effort. “I think we learned a great deal from him in the formulation of problems of physics, how to think about physics, how to solve problems, and how to generate your own problems,” she said.&lt;/p&gt;  &lt;p&gt;Indeed, throughout her career, Dresselhaus credited Fermi with teaching her how to “think as a physicist.” A key concept behind the Fermi system, she often stated, was the idea of single-authorship research: Grad students were expected to conceive of, carry out, and publish their thesis work more or less on their own, without the guiding hand of a more senior faculty member. This required them to work with others to develop a broad understanding of physics that they could then apply to a research topic they’d generate themselves.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Fermi’s connection with students didn’t end in the classroom. He was well known for frequent interactions with young people, and for being the rare senior faculty member who regularly integrated students into his personal life. “It was not beneath him to associate freely with students and to treat them as equals,” said Jay Orear, a career physicist and graduate student of Fermi’s, in a book of remembrances about his advisor. “In fact, I think he enjoyed young physics students more than some of his older colleagues.”&lt;/p&gt;  &lt;p&gt;For Dresselhaus, this integration began, quite literally, on her way to school. She and Fermi lived in the same general vicinity, and both were early risers who walked down Ellis Avenue on their way to the lab each day. “I had him for class first thing in the morning. And on my way, walking to school, I would see him. And he would cross the street and walk with me,” Dresselhaus recalled in a 2007 oral history interview. “That’s just being very friendly, and that made a long-term impression on me.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1124858" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/contact-sheet.jpg?w=1246" width="1246" /&gt;&lt;figcaption class="wp-element-caption"&gt;Dresselhaus shown in conversation early during her tenure at MIT. She would spend 50 years as a member of the faculty.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;MARGO FOOTE/MIT MUSEUM&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Whenever they met, Fermi would always select the subject of discussion and would never fail to energize and inspire her. “I was a very shy youngster and wouldn’t think of suggesting the topic to Enrico Fermi,” she told &lt;em&gt;MIT Alumni News&lt;/em&gt; in 2013&lt;em&gt;.&lt;/em&gt; “He would always ask questions about ‘What if this and this and this were true? What if we could make this—would it be interesting, and what could we learn?’”&lt;/p&gt;  &lt;p&gt;Fermi and his wife, Laura, were well known for hosting monthly dinners at their house, with dancing afterward—and his students were always invited. “Fermi especially liked young people,” noted Harold Agnew, a longtime physicist and one of his graduate students, in a remembrance published after Fermi’s death. “The top floor of his Chicago house had a large room in which he would invite students to come and square-dance.”&lt;/p&gt; 

 &lt;p&gt;“I remember those dinners,” Dresselhaus said in 2012. “Laura Fermi was a very, very good Italian cook.” But more than the cooking, she said, “it was the ambiance and the friendliness in that household that really made us enjoy physics—it was something more.” That “something more” would inspire Dresselhaus later in her career to provide her own students at MIT with a familial atmosphere in the lab, at group luncheons, and at events in her home, where lines between student and professor were blurred a bit and kindred spirits enjoyed one another’s company.&lt;/p&gt;  &lt;p&gt;Dresselhaus’s acquaintance with Fermi would last only a year. He had developed an incurable stomach cancer, possibly a result of exposure to radiation from his earlier work, and died on November 28, 1954. But he left a fantastic impression that influenced her for the rest of her days, instilling in her a commitment to public service and guiding how she trained her own students.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“The most important thing that young people need is the confidence that they can succeed. That’s what I work on.”&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;“Fermi had the most profound influence on physics teaching in the United States, and our graduate programs ... are much fashioned from his way of teaching,” Dresselhaus said in 2001. She later added, “From him, I learned that we don’t have to be leaders in every field, but we can use our understanding to see connections that others might miss.”&lt;/p&gt;  &lt;p&gt;The broad physical and scientific knowledge that Dresselhaus developed as a result of Fermi’s system for teaching graduate students helped her in numerous ways throughout her career. It proved useful on several occasions when she had to make significant course corrections, with very little background in the areas into which she pivoted. And she relied on it as a leader of national programs with diverse constituents.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;But perhaps the grandest lesson that Dresselhaus gained from her mentor was an understanding of what it takes to be a great teacher and advocate. “The most important thing that young people need is the confidence that they can succeed,” she explained in 2012. “That’s what I work on. When I have students, I make sure they are able to formulate and solve their own problems. I will help them, if they come in and talk with me. And I make sure they receive training for their next job.”&lt;/p&gt;  &lt;p&gt;By all accounts, she more than succeeded in that effort. At MIT, she became a beloved professor who both pushed her students to be their very best and provided support in ways big and small to ensure high achievement—helping students network for career opportunities, hosting any student who didn’t have a place to go for Thanksgiving dinner, teaching an entire recitation section for an engineering student who showed great promise but needed help getting up to speed in solid-state physics. She said, “I always felt Fermi and Rosalyn [Yalow, her undergraduate mentor at Hunter College] were interested in my career, and I try to show the same concern for my students.”&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;In the eight years since Dresselhaus’s death, new advances from her colleagues have borne the signature of her research—and have begun branching out in ever more fascinating directions. Graphene, for example, remains one of the hottest topics in science. Back in the early and mid-2010s, Dresselhaus worked on what she and others called “misoriented graphene.” She and others predicted that by twisting sheets of graphene so that their honeycomb patterns are slightly misaligned when superimposed, researchers could introduce “interesting patterns” that might lead to useful properties. In 2018, Dresselhaus’s MIT colleague Pablo Jarillo-Herrero realized this idea: He and others discovered that if two graphene sheets are combined into a superlattice, aligned at a “magic angle” of 1.1 degrees, the system can become either superconducting or insulating. The development was hailed as a major discovery and marked a jumping-off point for a subfield now known as ­“twistronics.” &lt;em&gt;Physics World&lt;/em&gt; named it Breakthrough of the Year.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="hexagonal sheets of graphene in slight misalignment" class="wp-image-1124861" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT-Magic-Angle.jpg?w=474" /&gt;&lt;figcaption class="wp-element-caption"&gt;Dresselhaus hypothesized that misaligning sheets of graphene could produce novel properties. In 2018, her MIT colleague Pablo Jarillo- Herrero demonstrated that such an arrangement can become either a superconductor or an insulator.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF THE RESEARCHERS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Also in 2018, MIT opened its doors to a gleaming new nanoscience and nanotechnology research facility at the heart of campus. The $400 million MIT.nano project was a long time in coming; although Dresselhaus missed out on the grand opening, she was very much looking forward to its completion, and to the start of a new generation of nanoscale endeavors at the Institute that would seek to expand humanity’s understanding of physics, chemistry, materials science, energy, biology, and more. In her final years, Dresselhaus had looked to MIT.nano as an extension of her legacy.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;In late 2019, the courtyard between the Institute’s Infinite Corridor and the southern façade of the MIT.nano building was dedicated in her memory. Dubbed the Improbability Walk, the space is a nod to Dresselhaus’s unlikely rise to international prominence from her humble beginnings in Depression-era New York. It also encourages those who might serve as mentors to take time to get to know younger colleagues and students, as Enrico Fermi did with Dresselhaus and Dresselhaus did with so many at MIT. For as improbable as it might seem, an encouraging word from a mentor can immeasurably enhance a young scientist’s life path.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image alignleft size-large is-resized"&gt;&lt;img alt="cover of Carbon Queen" class="wp-image-1124859" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/cover.jpg?w=1312" width="1312" /&gt;&lt;/figure&gt;  &lt;p&gt;Like Fermi before her, Dresselhaus was deeply committed to giving back—to students, to her research community, to society at large. Throughout her 86-plus years, she gave of her time, her intellect, her energy, her love, and her enthusiasm. In one of her final interviews, the Queen of Carbon issued a ringing invitation. “We need new science and we need new ideas, and there’s plenty of room for young people to come in and have careers discovering those new ideas,” she declared. “Life is very interesting in this lane. Come and join me!”&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Adapted from&amp;nbsp;&lt;em&gt;Carbon Queen: The Remarkable Life of Nanoscience Pioneer Mildred Dresselhaus&lt;/em&gt;&lt;em&gt;,&lt;/em&gt; by Maia Weinstock (MIT Press). Copyright 2022. Reprinted with permission.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Institute Professor Mildred “Millie” Dresselhaus forever altered our understanding of matter—the physical stuff of the universe that has mass and takes up space. Over 57 years at MIT, Dresselhaus also played a significant role in inspiring people to use this new knowledge to tackle some of the world’s greatest challenges, from producing clean energy to curing cancer. Although she became an emerita professor in 2007, Dresselhaus, who taught electrical engineering and physics, remained actively involved in research and all other aspects of MIT life until her death in 2017. She would have been 95 this November.&lt;/p&gt;  &lt;p&gt;Known as the “Queen of Carbon,” Dresselhaus was most often heralded for her pioneering work with one of nature’s most abundant and versatile substances. As a result of her insatiable curiosity about our world and her nearly six-decade career as a scientific explorer, we can thank her for significant leaps in how we think about carbon’s various forms and the company it keeps. In her early career, Dresselhaus employed a then-new invention—laser light—to probe carbon’s inner workings. She worked to distinguish how, for example, flat sheets of carbon atoms act differently from carbon crystals of three dimensions, especially in the presence of heat, electrons, or a magnetic field. And later she predicted the existence of what we now call carbon nanotubes, sheets of carbon atoms rolled up into minuscule cylinders that can be remarkably adept at conducting electricity.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Building on Dresselhaus’s far-reaching foundational research, scientists and engineers have made enormous advances at the nanoscale—with structures on the order of one hundred-thousandth the width of a human hair. Spherical carbon “buckyballs,” cylindrical carbon nanotubes, and two-dimensional carbon sheets known as graphene have already been used for energy storage, medical research, building materials, and paper-thin electronics, among many other applications. Today, these carbon structures continue to be developed for myriad novel uses that often seem taken from the realm of science fiction, including ultrafast quantum computers, efficient desalination devices, and quantum dots with applications in biosensing and drug delivery. For her work she won—among other honors—the Kavli Prize in Nanoscience, the National Medal of Science, and the Presidential Medal of Freedom, the highest civilian award given by the United States government.&lt;/p&gt;  &lt;p&gt;But her journey to MIT, and to global leadership in solid-state physics, was an improbable one. Born in Brooklyn, New York, to immigrant parents in 1930, Dresselhaus came of age at a time when women were rarely welcomed as scientists or encouraged to pursue technical fields. Yet she benefited from several key mentors who saw her potential and took deliberate steps to support a brilliant young mind.&amp;nbsp;&lt;/p&gt; 
&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1124857" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/2DY6F4G.jpg?w=1661" width="1661" /&gt;&lt;figcaption class="wp-element-caption"&gt;President Barack Obama presented Dresselhaus with the Presidential Medal of Freedom in 2014.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;OLIVIER DOULIERY/ABACAPRESS.COM VIA ALAMY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;One of those mentors was Enrico Fermi, the distinguished Italian-born nuclear scientist who played a leading role in the Manhattan Project and who concluded his career as a professor of physics at the University of Chicago. Fermi came to America after receiving a solo Nobel Prize in 1938 (for work on induced radioactivity) and then fleeing the Nazi regime with his Jewish wife, Laura. The story of how Fermi influenced an up-and-coming Millie Dresselhaus—and, by proxy, scores of students who would study under her—reveals how paying it forward to the next generation of scientists and engineers can yield lasting dividends.&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;In 1953, with the nuclear age firmly underway and the Cold War heating up, Dresselhaus found herself, at 22, one of the new graduate students within the University of Chicago’s world-class physics department. Although a number of researchers who had worked on the Manhattan Project there had by then left for other opportunities, many luminaries remained. In addition to the renowned Enrico Fermi, notable faculty included the Nobel laureates Harold Urey and Maria Goeppert Mayer (with whom Dresselhaus lived for about a year as a boarder) as well as the physicist Leona Woods, the only woman present during the famous 1942 fission demonstration on one of the school’s squash courts.&lt;/p&gt; 
 &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;The university’s physics program was fairly small in those days: Dresselhaus had earned a spot as one of just about a dozen new graduate students that year. She was also, it turns out, the only female student in the department. Despite a master’s degree in physics from Radcliffe College and a Fulbright fellowship at the University of Cambridge, she felt not quite prepared as she began her PhD. And so, at the start of her doctoral studies, she discovered a cache of old examinations, and she worked the problems therein forward and back until she felt up to speed.&lt;/p&gt;    &lt;p&gt;Despite this added practice, the coursework for first-year PhD candidates was brutal—so brutal that around three-quarters of all entering physics students eventually dropped out of the program. But Dresselhaus’s relationship with Fermi would provide an unexpected boost.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;She first encountered the unflappable scientist—who made crucial strides not only in the development of the atomic bomb but in particle physics after the war—as a student in his class on quantum mechanics. And through that class, Dresselhaus got to know his teaching style, which she recalled as patient, inspiring, and mind-opening. With a slow, deliberate, accented voice that Dresselhaus described as “halting,” Fermi expertly distilled complicated topics so that anyone in attendance could comprehend them. Brilliant at both theory and experimentation, he delighted in stripping concepts to their essence, and unlike more impatient professors who were absorbed in their own work, Fermi cherished the opportunity to review whatever he knew about a physical concept by explaining it to someone else. For this he clearly had a talent; thanks to the way he presented the finer details of quantum mechanics, Dresselhaus explained, “any youngster could think, when they heard the lecture, that they understood every word.”&lt;/p&gt;  &lt;p&gt;One key to the eminent scientist’s clarity was the ban he placed on taking notes. Fermi demanded full attention, so he would prepare and dole out handwritten notes before his lectures, lest students be tempted to take out their pens or slide rules. “What was so impressive and amazing about it is that the lectures were very exciting, whatever the subject was,” Dresselhaus said in a 2001 interview.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1124860" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/GettyImages-3245871.jpg?w=1658" width="1658" /&gt;&lt;figcaption class="wp-element-caption"&gt;Nuclear scientist Enrico Fermi, shown here circa 1942, was a key early mentor to Dresselhaus at the University of Chicago.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;HULTON ARCHIVE/GETTY IMAGES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;And then there was the homework, which was always tricky, but delightfully enlightening once you figured it out. At the end of every class, Fermi floated a seemingly simple problem to be solved as an exercise prior to the following lecture. These included questions like: Why is the sky blue? Why do the sun and stars emit spectra of light? And, famously, how many piano tuners are there in Chicago? “You thought it was simple until you got home,” Dresselhaus said in 2012, upon receiving the Enrico Fermi Award, a lifetime achievement award given by the US Department of Energy. These types of questions became known, collectively, as “Fermi problems” and are taught today in schools around the world, from kindergarten all the way to graduate-level courses, as examples of how to estimate and triangulate in search of an answer, even when you don’t know all the relevant—and seemingly necessary—parameters. Back when Dresselhaus was learning about such problems, all she knew was they were due by the next class, no more than a day or two away, and they took a significant effort. “I think we learned a great deal from him in the formulation of problems of physics, how to think about physics, how to solve problems, and how to generate your own problems,” she said.&lt;/p&gt;  &lt;p&gt;Indeed, throughout her career, Dresselhaus credited Fermi with teaching her how to “think as a physicist.” A key concept behind the Fermi system, she often stated, was the idea of single-authorship research: Grad students were expected to conceive of, carry out, and publish their thesis work more or less on their own, without the guiding hand of a more senior faculty member. This required them to work with others to develop a broad understanding of physics that they could then apply to a research topic they’d generate themselves.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Fermi’s connection with students didn’t end in the classroom. He was well known for frequent interactions with young people, and for being the rare senior faculty member who regularly integrated students into his personal life. “It was not beneath him to associate freely with students and to treat them as equals,” said Jay Orear, a career physicist and graduate student of Fermi’s, in a book of remembrances about his advisor. “In fact, I think he enjoyed young physics students more than some of his older colleagues.”&lt;/p&gt;  &lt;p&gt;For Dresselhaus, this integration began, quite literally, on her way to school. She and Fermi lived in the same general vicinity, and both were early risers who walked down Ellis Avenue on their way to the lab each day. “I had him for class first thing in the morning. And on my way, walking to school, I would see him. And he would cross the street and walk with me,” Dresselhaus recalled in a 2007 oral history interview. “That’s just being very friendly, and that made a long-term impression on me.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1124858" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/contact-sheet.jpg?w=1246" width="1246" /&gt;&lt;figcaption class="wp-element-caption"&gt;Dresselhaus shown in conversation early during her tenure at MIT. She would spend 50 years as a member of the faculty.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;MARGO FOOTE/MIT MUSEUM&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Whenever they met, Fermi would always select the subject of discussion and would never fail to energize and inspire her. “I was a very shy youngster and wouldn’t think of suggesting the topic to Enrico Fermi,” she told &lt;em&gt;MIT Alumni News&lt;/em&gt; in 2013&lt;em&gt;.&lt;/em&gt; “He would always ask questions about ‘What if this and this and this were true? What if we could make this—would it be interesting, and what could we learn?’”&lt;/p&gt;  &lt;p&gt;Fermi and his wife, Laura, were well known for hosting monthly dinners at their house, with dancing afterward—and his students were always invited. “Fermi especially liked young people,” noted Harold Agnew, a longtime physicist and one of his graduate students, in a remembrance published after Fermi’s death. “The top floor of his Chicago house had a large room in which he would invite students to come and square-dance.”&lt;/p&gt; 

 &lt;p&gt;“I remember those dinners,” Dresselhaus said in 2012. “Laura Fermi was a very, very good Italian cook.” But more than the cooking, she said, “it was the ambiance and the friendliness in that household that really made us enjoy physics—it was something more.” That “something more” would inspire Dresselhaus later in her career to provide her own students at MIT with a familial atmosphere in the lab, at group luncheons, and at events in her home, where lines between student and professor were blurred a bit and kindred spirits enjoyed one another’s company.&lt;/p&gt;  &lt;p&gt;Dresselhaus’s acquaintance with Fermi would last only a year. He had developed an incurable stomach cancer, possibly a result of exposure to radiation from his earlier work, and died on November 28, 1954. But he left a fantastic impression that influenced her for the rest of her days, instilling in her a commitment to public service and guiding how she trained her own students.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“The most important thing that young people need is the confidence that they can succeed. That’s what I work on.”&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;“Fermi had the most profound influence on physics teaching in the United States, and our graduate programs ... are much fashioned from his way of teaching,” Dresselhaus said in 2001. She later added, “From him, I learned that we don’t have to be leaders in every field, but we can use our understanding to see connections that others might miss.”&lt;/p&gt;  &lt;p&gt;The broad physical and scientific knowledge that Dresselhaus developed as a result of Fermi’s system for teaching graduate students helped her in numerous ways throughout her career. It proved useful on several occasions when she had to make significant course corrections, with very little background in the areas into which she pivoted. And she relied on it as a leader of national programs with diverse constituents.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;But perhaps the grandest lesson that Dresselhaus gained from her mentor was an understanding of what it takes to be a great teacher and advocate. “The most important thing that young people need is the confidence that they can succeed,” she explained in 2012. “That’s what I work on. When I have students, I make sure they are able to formulate and solve their own problems. I will help them, if they come in and talk with me. And I make sure they receive training for their next job.”&lt;/p&gt;  &lt;p&gt;By all accounts, she more than succeeded in that effort. At MIT, she became a beloved professor who both pushed her students to be their very best and provided support in ways big and small to ensure high achievement—helping students network for career opportunities, hosting any student who didn’t have a place to go for Thanksgiving dinner, teaching an entire recitation section for an engineering student who showed great promise but needed help getting up to speed in solid-state physics. She said, “I always felt Fermi and Rosalyn [Yalow, her undergraduate mentor at Hunter College] were interested in my career, and I try to show the same concern for my students.”&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;In the eight years since Dresselhaus’s death, new advances from her colleagues have borne the signature of her research—and have begun branching out in ever more fascinating directions. Graphene, for example, remains one of the hottest topics in science. Back in the early and mid-2010s, Dresselhaus worked on what she and others called “misoriented graphene.” She and others predicted that by twisting sheets of graphene so that their honeycomb patterns are slightly misaligned when superimposed, researchers could introduce “interesting patterns” that might lead to useful properties. In 2018, Dresselhaus’s MIT colleague Pablo Jarillo-Herrero realized this idea: He and others discovered that if two graphene sheets are combined into a superlattice, aligned at a “magic angle” of 1.1 degrees, the system can become either superconducting or insulating. The development was hailed as a major discovery and marked a jumping-off point for a subfield now known as ­“twistronics.” &lt;em&gt;Physics World&lt;/em&gt; named it Breakthrough of the Year.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="hexagonal sheets of graphene in slight misalignment" class="wp-image-1124861" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT-Magic-Angle.jpg?w=474" /&gt;&lt;figcaption class="wp-element-caption"&gt;Dresselhaus hypothesized that misaligning sheets of graphene could produce novel properties. In 2018, her MIT colleague Pablo Jarillo- Herrero demonstrated that such an arrangement can become either a superconductor or an insulator.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF THE RESEARCHERS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Also in 2018, MIT opened its doors to a gleaming new nanoscience and nanotechnology research facility at the heart of campus. The $400 million MIT.nano project was a long time in coming; although Dresselhaus missed out on the grand opening, she was very much looking forward to its completion, and to the start of a new generation of nanoscale endeavors at the Institute that would seek to expand humanity’s understanding of physics, chemistry, materials science, energy, biology, and more. In her final years, Dresselhaus had looked to MIT.nano as an extension of her legacy.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;In late 2019, the courtyard between the Institute’s Infinite Corridor and the southern façade of the MIT.nano building was dedicated in her memory. Dubbed the Improbability Walk, the space is a nod to Dresselhaus’s unlikely rise to international prominence from her humble beginnings in Depression-era New York. It also encourages those who might serve as mentors to take time to get to know younger colleagues and students, as Enrico Fermi did with Dresselhaus and Dresselhaus did with so many at MIT. For as improbable as it might seem, an encouraging word from a mentor can immeasurably enhance a young scientist’s life path.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image alignleft size-large is-resized"&gt;&lt;img alt="cover of Carbon Queen" class="wp-image-1124859" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/cover.jpg?w=1312" width="1312" /&gt;&lt;/figure&gt;  &lt;p&gt;Like Fermi before her, Dresselhaus was deeply committed to giving back—to students, to her research community, to society at large. Throughout her 86-plus years, she gave of her time, her intellect, her energy, her love, and her enthusiasm. In one of her final interviews, the Queen of Carbon issued a ringing invitation. “We need new science and we need new ideas, and there’s plenty of room for young people to come in and have careers discovering those new ideas,” she declared. “Life is very interesting in this lane. Come and join me!”&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Adapted from&amp;nbsp;&lt;em&gt;Carbon Queen: The Remarkable Life of Nanoscience Pioneer Mildred Dresselhaus&lt;/em&gt;&lt;em&gt;,&lt;/em&gt; by Maia Weinstock (MIT Press). Copyright 2022. Reprinted with permission.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124731/how-millie-dresselhaus-paid-it-forward/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] 25 years of research in space (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124727/25-years-of-research-in-space/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;On November 2, 2000, NASA astronaut Bill Shepherd, OCE ’78, SM ’78, and Russian cosmonauts Sergei Krikalev and Yuri Gidzenko made history as their Soyuz spacecraft docked with the International Space Station.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The event marked the start of 25 years of continuous human presence in space aboard the ISS—a prolific period for space research. MIT-trained astronauts, scientists, and engineers have played integral roles in all aspects of the station’s design, assembly, operations, and scientific research.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;One of MIT’s most experienced NASA astronauts, Mike Fincke ’89, is celebrating that milestone from space. Having already logged 381 days in three previous missions to the ISS, he returned on August&amp;nbsp;1 as a member of the Expedition 73 crew. “Wow, 25 years of constant human habitation in space!” he said when he spoke with me from the station in September. “What an accomplishment and a testimony to the teams on the ground and in terms of engineering, science, and diplomacy.”&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Building and operating the ISS&lt;/h3&gt;  &lt;p&gt;“We understood that building the ISS was significantly more difficult than anything we’d attempted before with the possible exception of Apollo,” says Pamela Melroy, SM ’84, who flew the space shuttle on three ISS assembly missions, including STS-92 in October 2000, which installed key modules and structures that prepared the station for the arrival of Shepherd and his crew less than two weeks later. “We learned a tremendous amount from the Shuttle-Mir program that I think gave us a lot more confidence going into ISS assembly,” she says.&lt;/p&gt; 
 &lt;p&gt;Melroy was one of 10 MIT astronauts who participated in 13 space shuttle missions to assemble and resupply the ISS through 2011. “It’s pretty awe-inspiring to just go, ‘Wow, there is the visible evidence of what we just spent 10 to 14 days doing,’” she recalls. She also saw just how critical logistics are to resupply operations—especially since the retirement of the shuttle.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Shepherd, who served as Expedition One commander, and his crew overcame a variety of challenges as they adapted to living in space, continued the assembly of the ISS, and installed and activated its life support and communications systems. “We were blue-collar maintenance guys for most of our flight,” he says. “I really enjoyed that part of it.” After arriving on the ISS, he discovered that the Russian service module was missing a worktable that his crew had found to be very useful in training. He asked Moscow, “Where’s our table?” and was told, “It’s going to come up six months after you guys are gone.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Cargo flights had delivered canisters of carbon dioxide absorbers packaged in sturdy aluminum frames. Upon inspecting the frames, they decided there was no reason to remain table-less. “We had some special tools that we had smuggled on board,” he recalls. “So we started to cut and drill and thread and fabricate a table out of scraps.” It turned out to be a pretty good table. “When Houston found out about it, they went nuts, because we were up there sawing, making chips and aluminum sawdust,” he says. “But we got through all that.” Now in the Smithsonian, it is “definitely an MIT-designed table,” Shepherd says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Twelve MIT alums and one MIT affiliate from the Whitehead Institute have logged a total of 18 long-duration missions to the ISS. Cady Coleman ’83 served as lead robotics and science officer during a 159-day expedition in 2010 and 2011. She performed hundreds of experiments, ranging from basic science to technology development for future moon and Mars missions. “At MIT, we were always invited to be part of scientific discovery,” Coleman says. “We carried MIT’s standard of excellence into every field. Most importantly, our education taught us that we were part of a larger mission to make the world a better place.”&lt;/p&gt;  &lt;p&gt;Citing the “mens et manus” motto on the Brass Rat he was wearing in space, Fincke observed that MIT prepared him well for his job. “When you have such a critical mass of really intelligent people and critical thinkers, it really makes a difference and brings out the best in all of us, including me,” he said. “So thank you, MIT.”&lt;/p&gt;  &lt;p&gt;Woody Hoburg ’08, who was an assistant professor of aero-astro before piloting a 186-day mission to the ISS in 2023, concurs: “It’s no surprise that so many exceptional MIT thinkers and doers end up shaping our boldest achievements in space. The ISS is certainly one of those—it’s a beautiful machine, constructed while I was still in high school and later studying Course 16 at MIT, flying five miles per second over Earth that whole time.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;h3 class="wp-block-heading"&gt;Science in space&lt;/h3&gt;  &lt;p&gt;A wide range of MIT faculty and students have taken advantage of the ISS’s unique access to space to conduct research.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“MIT’s MACE-II [Middeck Active Control Experiment] was the first active US scientific investigation performed on the International Space Station,” Shepherd said back in 2001. “Performing scientific investigations like MACE-II on board the station allows for successful interaction, almost in real time, between the astronauts in space and investigators on the ground.” Developed by aero-astro professor David Miller ’82, SM ’85, ScD ’88, and the Space Systems Laboratory (SSL) he then directed, MACE-II successfully tested techniques for predicting and controlling the dynamics of structures in microgravity. Miller says that the structural dynamics techniques developed through MACE were later used to test the James Webb Space Telescope. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;Miller and the SSL also led the development of SPHERES (Synchronized Position Hold Engage and Reorient Experimental Satellites), a set of satellites used on board the ISS from 2006 through 2019. Inspired by the Jedi training ball from the original &lt;em&gt;Star Wars&lt;/em&gt;, SPHERES evolved from an undergraduate aero-astro capstone project into an ISS facility for studying the dynamic control of satellites flying together in space. Three independent free-flying satellites operated inside the ISS within an infrared/ultrasonic measurement system that provided precise positioning and attitude information in three dimensions. SPHERES let researchers develop and test algorithms for precision control of multiple spacecraft during complex collaborative operations. Its modular design permitted the addition of electromagnets for precise tandem flight, vision systems for navigation, and hardware for investigating the sloshing of fluids in space.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Greg Chamitoff, PhD ’92, became the first principal investigator to directly perform his own scientific research on the ISS when he programmed SPHERES during Expedition 17 in 2008. Miller recalls that when Chamitoff later visited MIT, he asked, “Why don’t we create the first primary school robotics competition ever hosted off the planet?” During the next decade, nearly 20,000 high school and middle school students from around the world participated in Zero Robotics, writing algorithms to control the SPHERES satellites in STEM competitions conducted onboard the ISS. Both MACE-II and SPHERES were returned to Earth and will be on display at the National Air and Space Museum in the “At Home in Space” gallery slated to open in 2026.&lt;/p&gt; 

 &lt;p&gt;Samuel C.C. Ting, the Thomas Dudley Cabot Professor of Physics at MIT, led a $2 billion international effort to develop the Alpha Magnetic Spectrometer (AMS) with the ambitious goal of searching for antimatter, determining the origin of dark matter, and understanding the properties of cosmic rays. Delivered to the ISS in 2011 by one of the final space shuttle missions, the AMS has precisely measured over 253 billion cosmic ray events with energies up to multiple tera-electron-volts. Fully interpreting the comprehensive experimental data still being generated by the AMS will require new physics models. “I would imagine 100 years from now most of my work will be forgotten,” Ting says. “But if people remember anything, it probably will be AMS.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Kate Rubins, a microbiologist, was a fellow at the Whitehead Institute when she was selected as a NASA astronaut in 2009—and became the first person to sequence DNA in space during her long-&lt;br /&gt;duration ISS mission in 2016. She did so using a commercially available meta­genomics sequencer, despite the risk that it might not function in orbit. “To everybody’s surprise, it worked, and it worked the first time,” she recalls. “I don’t know if I’ve ever had a lab experiment in my life that has worked the first time, but genomic sequencing in space was a big one to have that happen.”&lt;/p&gt;  &lt;p&gt;Rubins wanted to conduct her own scientific research during her spare time in orbit, so she got permission from NASA to substitute her own lab bench equipment—including pipettes, tubes, and scientific plasticware—for the small kit of personal items that astronauts are allowed to bring to space. She got a NASA psychologist to help make the case. “He said, ‘You know, Kate’s a nerd—she loves doing this stuff … we have to fly this on board for her,’” she says. Rubins successfully demonstrated that regular biology lab equipment could be used to conduct science in space—and donated that equipment for use by future ISS crews. (“Every astronaut turns into a scientist when they get on board the space station,” she says.) She recently coauthored a paper describing the creation of a microbiome map of the ISS—a 3D map showing where astronauts found various microbes and metabolites when they collected samples in space. She calls the work “super exciting.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The ISS also serves as a test bed for new technologies that will support NASA’s ambitious programs to explore the moon and Mars. In 2023, MIT Lincoln Laboratory successfully demonstrated high-­bandwidth laser communications in space between its ILLUMA-T laser communications terminal onboard the ISS and a NASA Laser Communications Relay Demonstration satellite. When the Artemis II astronauts launch to the moon in early 2026, their Orion spacecraft will use the optical communications system developed by Lincoln Laboratory’s Optical and Quantum Communications Group and the Goddard Space Flight Center to transmit high-­resolution imagery of the lunar surface back to Earth via lasers capable of data rates up to 260 megabits per second.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;International cooperation&lt;/h3&gt;  &lt;p&gt;One of the most enduring legacies of the International Space Station, which is slated to continue operations through 2030, is the vast scale of international cooperation that made it possible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The roots of the project trace back to 1984, when President Ronald Reagan challenged NASA to lead an effort to build an Earth-orbiting space station within a decade. But by the early 1990s, the Space Station Freedom was significantly over budget and behind schedule. Shortly after taking office in 1993, President Bill Clinton asked MIT President Charles Vest to lead the Advisory Committee on the Redesign of the Space Station. In the wake of the Soviet Union’s collapse, the Vest committee recommended that “NASA and the Administration further pursue opportunities for cooperation with the Russians as a means to enhance the capability of the station, reduce cost, provide alternative access to the station, and increase research opportunities.” That led NASA to invite the Russian space agency Roscosmos to join an international ISS coalition. And today, the ISS is operated cooperatively by the space agencies of the United States (NASA), Russia (Roscosmos), Japan (JAXA), Canada (CSA), and Europe (ESA).&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126026" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/shepard.jpg?w=1104" /&gt;&lt;figcaption class="wp-element-caption"&gt;Bill Shepherd, OCE ’78, SM ’78, and his crewmates built this worktable in space using tools they’d smuggled on board. They inscribed “The Best from Nothing” in Latin on its side. &lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF BILL SHEPARD&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;“We went from a space race during the Apollo time frame to—actually now we work together, humans across planet Earth, making something pretty incredible,” Fincke says. “Hats off to all of my crewmates and to all of the teams across planet Earth that put this beautiful space station together.” &amp;nbsp;&lt;/p&gt;  &lt;p&gt;As deputy administrator of NASA from 2021 to 2025, Melroy helped lead NASA during a challenging period following the Russian invasion of Ukraine. “When people are united by something that they’re equally passionate about,” she says, “you overcome the barriers of cultural, language, political differences.” NASA and Roscosmos had established a “level of trust,” she says, “and there are relationships at every single level.” Keeping relationships nonpolitical was a guiding principle, Melroy says, “and our Russian partners respected that and agreed.”&lt;/p&gt; 
 &lt;p&gt;“We still have our partnership in space even though on the ground we’re not quite getting along,” Fincke says. “We have a beautiful solar system to go explore, and someday we’re gonna have the stars.” And that, he says, will be possible “if we stop fighting and put our efforts toward exploration.”&lt;/p&gt;  &lt;p&gt;In 2001 Shepherd predicted, “It’s very likely that the day of our launch … will be the last day that humans will live only on planet Earth.” And after 25 years of living and working on the International Space Station, humans appear to be up to the challenge of proving him right. &lt;/p&gt;  &lt;p&gt;&lt;em&gt;John Tylko ’79, PhD ’23, an aerospace engineer and technology historian, witnessed the 2000 launch of the first ISS crew at the Baikonur Cosmodrome and the docking of their spacecraft with the ISS from the Russian Mission Control Center near Moscow.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Michael Fincke floating on the ISS" class="wp-image-1126208" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/iss073e0420910large.jpg?w=1773" /&gt;&lt;figcaption class="wp-element-caption"&gt;Expedition 73 astronaut Michael Fincke ’89 inside the European Columbus laboratory module of the International Space Station in August 2025. While being interviewed from the ISS in September, Fincke said that MIT prepared him well for his time in space, from the aero-astro classes that taught him about airplanes and rockets—and critical thinking—to his Russian language and EAPS classes. “When you have such a critical mass of really intelligent people and critical thinkers, it really makes a difference and brings out the best in all of us, including me,” he said. “So thank you, MIT."&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126204" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/52970170188_923f11dc88_o.jpg?w=2841" width="2841" /&gt;&lt;figcaption class="wp-element-caption"&gt;Astronaut Woody Hoburg ’08 conducts a spacewalk outside the International Space Station to deploy new solar arrays during Expedition 68 on June 9, 2023.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126207" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/iss064e025418orig.jpg?w=3000" /&gt;&lt;figcaption class="wp-element-caption"&gt;Expedition 64 astronaut Kate Rubins, a Whitehead Fellow, with the DNA sequencing experiment she ran aboard the ISS on January 22, 2021. Rubins was first astronaut to sequence DNA in space during Expedition 48 in 2016.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126209" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Screenshot-2025-10-21-093008.jpg?w=1951" /&gt;&lt;figcaption class="wp-element-caption"&gt;Mike Fincke ’89, Cady Coleman ’83, and Greg Chamitoff, PhD ’92, made a video to offer extraterrestrial congratulations on the Institute’s 150th anniversary while they were all aboard the ISS in 2011. In this still from the video, they’re seen with the three SPHERES satellites developed by MIT’s Space Systems Laboratory.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126206" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/DSC_1459.jpg?w=2495" /&gt;&lt;figcaption class="wp-element-caption"&gt;Samuel C.C. Ting, the Thomas Dudley Cabot Professor of Physics at MIT, with a model of the Alpha Magnetic Spectrometer (AMS) at a Kennedy Space Center news conference on April 28, 2011. &lt;/figcaption&gt;&lt;div class="image-credit"&gt;JOHN TYLKO&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126203" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/9443612295_2c1aa4614d_o.jpg?w=2731" width="2731" /&gt;&lt;figcaption class="wp-element-caption"&gt;Expedition 18 astronauts Greg Chamitoff, PhD ’92 (left) and Mike Fincke ’89 (center) with spaceflight participant Richard Garriott on October 22, 2008, in the ISS Harmony node with the three SPHERES satellites developed at MIT.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126205" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/aero-astro-nasa-stabilizer-2.jpg?w=1659" /&gt;&lt;figcaption class="wp-element-caption"&gt;In September 2000, Aero-Astro Space Systems Laboratory researchers posed with MIT’s MACE-II (Middeck Active Control Experiment), the first active US scientific investigation performed on the ISS. Left to right: Cemocan Yesil ’03, Professor David Miller ’82, SM ’85, ScD ’88, Gregory Mallory, PhD ’00, and Jeremy Yung ’93, SM ’96, PhD ’02.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;DONNA COVENEY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;On November 2, 2000, NASA astronaut Bill Shepherd, OCE ’78, SM ’78, and Russian cosmonauts Sergei Krikalev and Yuri Gidzenko made history as their Soyuz spacecraft docked with the International Space Station.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The event marked the start of 25 years of continuous human presence in space aboard the ISS—a prolific period for space research. MIT-trained astronauts, scientists, and engineers have played integral roles in all aspects of the station’s design, assembly, operations, and scientific research.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;One of MIT’s most experienced NASA astronauts, Mike Fincke ’89, is celebrating that milestone from space. Having already logged 381 days in three previous missions to the ISS, he returned on August&amp;nbsp;1 as a member of the Expedition 73 crew. “Wow, 25 years of constant human habitation in space!” he said when he spoke with me from the station in September. “What an accomplishment and a testimony to the teams on the ground and in terms of engineering, science, and diplomacy.”&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Building and operating the ISS&lt;/h3&gt;  &lt;p&gt;“We understood that building the ISS was significantly more difficult than anything we’d attempted before with the possible exception of Apollo,” says Pamela Melroy, SM ’84, who flew the space shuttle on three ISS assembly missions, including STS-92 in October 2000, which installed key modules and structures that prepared the station for the arrival of Shepherd and his crew less than two weeks later. “We learned a tremendous amount from the Shuttle-Mir program that I think gave us a lot more confidence going into ISS assembly,” she says.&lt;/p&gt; 
 &lt;p&gt;Melroy was one of 10 MIT astronauts who participated in 13 space shuttle missions to assemble and resupply the ISS through 2011. “It’s pretty awe-inspiring to just go, ‘Wow, there is the visible evidence of what we just spent 10 to 14 days doing,’” she recalls. She also saw just how critical logistics are to resupply operations—especially since the retirement of the shuttle.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Shepherd, who served as Expedition One commander, and his crew overcame a variety of challenges as they adapted to living in space, continued the assembly of the ISS, and installed and activated its life support and communications systems. “We were blue-collar maintenance guys for most of our flight,” he says. “I really enjoyed that part of it.” After arriving on the ISS, he discovered that the Russian service module was missing a worktable that his crew had found to be very useful in training. He asked Moscow, “Where’s our table?” and was told, “It’s going to come up six months after you guys are gone.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Cargo flights had delivered canisters of carbon dioxide absorbers packaged in sturdy aluminum frames. Upon inspecting the frames, they decided there was no reason to remain table-less. “We had some special tools that we had smuggled on board,” he recalls. “So we started to cut and drill and thread and fabricate a table out of scraps.” It turned out to be a pretty good table. “When Houston found out about it, they went nuts, because we were up there sawing, making chips and aluminum sawdust,” he says. “But we got through all that.” Now in the Smithsonian, it is “definitely an MIT-designed table,” Shepherd says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Twelve MIT alums and one MIT affiliate from the Whitehead Institute have logged a total of 18 long-duration missions to the ISS. Cady Coleman ’83 served as lead robotics and science officer during a 159-day expedition in 2010 and 2011. She performed hundreds of experiments, ranging from basic science to technology development for future moon and Mars missions. “At MIT, we were always invited to be part of scientific discovery,” Coleman says. “We carried MIT’s standard of excellence into every field. Most importantly, our education taught us that we were part of a larger mission to make the world a better place.”&lt;/p&gt;  &lt;p&gt;Citing the “mens et manus” motto on the Brass Rat he was wearing in space, Fincke observed that MIT prepared him well for his job. “When you have such a critical mass of really intelligent people and critical thinkers, it really makes a difference and brings out the best in all of us, including me,” he said. “So thank you, MIT.”&lt;/p&gt;  &lt;p&gt;Woody Hoburg ’08, who was an assistant professor of aero-astro before piloting a 186-day mission to the ISS in 2023, concurs: “It’s no surprise that so many exceptional MIT thinkers and doers end up shaping our boldest achievements in space. The ISS is certainly one of those—it’s a beautiful machine, constructed while I was still in high school and later studying Course 16 at MIT, flying five miles per second over Earth that whole time.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;h3 class="wp-block-heading"&gt;Science in space&lt;/h3&gt;  &lt;p&gt;A wide range of MIT faculty and students have taken advantage of the ISS’s unique access to space to conduct research.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“MIT’s MACE-II [Middeck Active Control Experiment] was the first active US scientific investigation performed on the International Space Station,” Shepherd said back in 2001. “Performing scientific investigations like MACE-II on board the station allows for successful interaction, almost in real time, between the astronauts in space and investigators on the ground.” Developed by aero-astro professor David Miller ’82, SM ’85, ScD ’88, and the Space Systems Laboratory (SSL) he then directed, MACE-II successfully tested techniques for predicting and controlling the dynamics of structures in microgravity. Miller says that the structural dynamics techniques developed through MACE were later used to test the James Webb Space Telescope. &amp;nbsp;&lt;/p&gt;  &lt;p&gt;Miller and the SSL also led the development of SPHERES (Synchronized Position Hold Engage and Reorient Experimental Satellites), a set of satellites used on board the ISS from 2006 through 2019. Inspired by the Jedi training ball from the original &lt;em&gt;Star Wars&lt;/em&gt;, SPHERES evolved from an undergraduate aero-astro capstone project into an ISS facility for studying the dynamic control of satellites flying together in space. Three independent free-flying satellites operated inside the ISS within an infrared/ultrasonic measurement system that provided precise positioning and attitude information in three dimensions. SPHERES let researchers develop and test algorithms for precision control of multiple spacecraft during complex collaborative operations. Its modular design permitted the addition of electromagnets for precise tandem flight, vision systems for navigation, and hardware for investigating the sloshing of fluids in space.&amp;nbsp;&lt;/p&gt;    &lt;p&gt;Greg Chamitoff, PhD ’92, became the first principal investigator to directly perform his own scientific research on the ISS when he programmed SPHERES during Expedition 17 in 2008. Miller recalls that when Chamitoff later visited MIT, he asked, “Why don’t we create the first primary school robotics competition ever hosted off the planet?” During the next decade, nearly 20,000 high school and middle school students from around the world participated in Zero Robotics, writing algorithms to control the SPHERES satellites in STEM competitions conducted onboard the ISS. Both MACE-II and SPHERES were returned to Earth and will be on display at the National Air and Space Museum in the “At Home in Space” gallery slated to open in 2026.&lt;/p&gt; 

 &lt;p&gt;Samuel C.C. Ting, the Thomas Dudley Cabot Professor of Physics at MIT, led a $2 billion international effort to develop the Alpha Magnetic Spectrometer (AMS) with the ambitious goal of searching for antimatter, determining the origin of dark matter, and understanding the properties of cosmic rays. Delivered to the ISS in 2011 by one of the final space shuttle missions, the AMS has precisely measured over 253 billion cosmic ray events with energies up to multiple tera-electron-volts. Fully interpreting the comprehensive experimental data still being generated by the AMS will require new physics models. “I would imagine 100 years from now most of my work will be forgotten,” Ting says. “But if people remember anything, it probably will be AMS.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Kate Rubins, a microbiologist, was a fellow at the Whitehead Institute when she was selected as a NASA astronaut in 2009—and became the first person to sequence DNA in space during her long-&lt;br /&gt;duration ISS mission in 2016. She did so using a commercially available meta­genomics sequencer, despite the risk that it might not function in orbit. “To everybody’s surprise, it worked, and it worked the first time,” she recalls. “I don’t know if I’ve ever had a lab experiment in my life that has worked the first time, but genomic sequencing in space was a big one to have that happen.”&lt;/p&gt;  &lt;p&gt;Rubins wanted to conduct her own scientific research during her spare time in orbit, so she got permission from NASA to substitute her own lab bench equipment—including pipettes, tubes, and scientific plasticware—for the small kit of personal items that astronauts are allowed to bring to space. She got a NASA psychologist to help make the case. “He said, ‘You know, Kate’s a nerd—she loves doing this stuff … we have to fly this on board for her,’” she says. Rubins successfully demonstrated that regular biology lab equipment could be used to conduct science in space—and donated that equipment for use by future ISS crews. (“Every astronaut turns into a scientist when they get on board the space station,” she says.) She recently coauthored a paper describing the creation of a microbiome map of the ISS—a 3D map showing where astronauts found various microbes and metabolites when they collected samples in space. She calls the work “super exciting.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The ISS also serves as a test bed for new technologies that will support NASA’s ambitious programs to explore the moon and Mars. In 2023, MIT Lincoln Laboratory successfully demonstrated high-­bandwidth laser communications in space between its ILLUMA-T laser communications terminal onboard the ISS and a NASA Laser Communications Relay Demonstration satellite. When the Artemis II astronauts launch to the moon in early 2026, their Orion spacecraft will use the optical communications system developed by Lincoln Laboratory’s Optical and Quantum Communications Group and the Goddard Space Flight Center to transmit high-­resolution imagery of the lunar surface back to Earth via lasers capable of data rates up to 260 megabits per second.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;International cooperation&lt;/h3&gt;  &lt;p&gt;One of the most enduring legacies of the International Space Station, which is slated to continue operations through 2030, is the vast scale of international cooperation that made it possible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The roots of the project trace back to 1984, when President Ronald Reagan challenged NASA to lead an effort to build an Earth-orbiting space station within a decade. But by the early 1990s, the Space Station Freedom was significantly over budget and behind schedule. Shortly after taking office in 1993, President Bill Clinton asked MIT President Charles Vest to lead the Advisory Committee on the Redesign of the Space Station. In the wake of the Soviet Union’s collapse, the Vest committee recommended that “NASA and the Administration further pursue opportunities for cooperation with the Russians as a means to enhance the capability of the station, reduce cost, provide alternative access to the station, and increase research opportunities.” That led NASA to invite the Russian space agency Roscosmos to join an international ISS coalition. And today, the ISS is operated cooperatively by the space agencies of the United States (NASA), Russia (Roscosmos), Japan (JAXA), Canada (CSA), and Europe (ESA).&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126026" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/shepard.jpg?w=1104" /&gt;&lt;figcaption class="wp-element-caption"&gt;Bill Shepherd, OCE ’78, SM ’78, and his crewmates built this worktable in space using tools they’d smuggled on board. They inscribed “The Best from Nothing” in Latin on its side. &lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF BILL SHEPARD&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;“We went from a space race during the Apollo time frame to—actually now we work together, humans across planet Earth, making something pretty incredible,” Fincke says. “Hats off to all of my crewmates and to all of the teams across planet Earth that put this beautiful space station together.” &amp;nbsp;&lt;/p&gt;  &lt;p&gt;As deputy administrator of NASA from 2021 to 2025, Melroy helped lead NASA during a challenging period following the Russian invasion of Ukraine. “When people are united by something that they’re equally passionate about,” she says, “you overcome the barriers of cultural, language, political differences.” NASA and Roscosmos had established a “level of trust,” she says, “and there are relationships at every single level.” Keeping relationships nonpolitical was a guiding principle, Melroy says, “and our Russian partners respected that and agreed.”&lt;/p&gt; 
 &lt;p&gt;“We still have our partnership in space even though on the ground we’re not quite getting along,” Fincke says. “We have a beautiful solar system to go explore, and someday we’re gonna have the stars.” And that, he says, will be possible “if we stop fighting and put our efforts toward exploration.”&lt;/p&gt;  &lt;p&gt;In 2001 Shepherd predicted, “It’s very likely that the day of our launch … will be the last day that humans will live only on planet Earth.” And after 25 years of living and working on the International Space Station, humans appear to be up to the challenge of proving him right. &lt;/p&gt;  &lt;p&gt;&lt;em&gt;John Tylko ’79, PhD ’23, an aerospace engineer and technology historian, witnessed the 2000 launch of the first ISS crew at the Baikonur Cosmodrome and the docking of their spacecraft with the ISS from the Russian Mission Control Center near Moscow.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Michael Fincke floating on the ISS" class="wp-image-1126208" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/iss073e0420910large.jpg?w=1773" /&gt;&lt;figcaption class="wp-element-caption"&gt;Expedition 73 astronaut Michael Fincke ’89 inside the European Columbus laboratory module of the International Space Station in August 2025. While being interviewed from the ISS in September, Fincke said that MIT prepared him well for his time in space, from the aero-astro classes that taught him about airplanes and rockets—and critical thinking—to his Russian language and EAPS classes. “When you have such a critical mass of really intelligent people and critical thinkers, it really makes a difference and brings out the best in all of us, including me,” he said. “So thank you, MIT."&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126204" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/52970170188_923f11dc88_o.jpg?w=2841" width="2841" /&gt;&lt;figcaption class="wp-element-caption"&gt;Astronaut Woody Hoburg ’08 conducts a spacewalk outside the International Space Station to deploy new solar arrays during Expedition 68 on June 9, 2023.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126207" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/iss064e025418orig.jpg?w=3000" /&gt;&lt;figcaption class="wp-element-caption"&gt;Expedition 64 astronaut Kate Rubins, a Whitehead Fellow, with the DNA sequencing experiment she ran aboard the ISS on January 22, 2021. Rubins was first astronaut to sequence DNA in space during Expedition 48 in 2016.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126209" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Screenshot-2025-10-21-093008.jpg?w=1951" /&gt;&lt;figcaption class="wp-element-caption"&gt;Mike Fincke ’89, Cady Coleman ’83, and Greg Chamitoff, PhD ’92, made a video to offer extraterrestrial congratulations on the Institute’s 150th anniversary while they were all aboard the ISS in 2011. In this still from the video, they’re seen with the three SPHERES satellites developed by MIT’s Space Systems Laboratory.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126206" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/DSC_1459.jpg?w=2495" /&gt;&lt;figcaption class="wp-element-caption"&gt;Samuel C.C. Ting, the Thomas Dudley Cabot Professor of Physics at MIT, with a model of the Alpha Magnetic Spectrometer (AMS) at a Kennedy Space Center news conference on April 28, 2011. &lt;/figcaption&gt;&lt;div class="image-credit"&gt;JOHN TYLKO&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126203" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/9443612295_2c1aa4614d_o.jpg?w=2731" width="2731" /&gt;&lt;figcaption class="wp-element-caption"&gt;Expedition 18 astronauts Greg Chamitoff, PhD ’92 (left) and Mike Fincke ’89 (center) with spaceflight participant Richard Garriott on October 22, 2008, in the ISS Harmony node with the three SPHERES satellites developed at MIT.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126205" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/aero-astro-nasa-stabilizer-2.jpg?w=1659" /&gt;&lt;figcaption class="wp-element-caption"&gt;In September 2000, Aero-Astro Space Systems Laboratory researchers posed with MIT’s MACE-II (Middeck Active Control Experiment), the first active US scientific investigation performed on the ISS. Left to right: Cemocan Yesil ’03, Professor David Miller ’82, SM ’85, ScD ’88, Gregory Mallory, PhD ’00, and Jeremy Yung ’93, SM ’96, PhD ’02.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;DONNA COVENEY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124727/25-years-of-research-in-space/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Infinite folds (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124723/infinite-folds/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When Madonna Yoder ’17 was eight years old, she learned how to fold a square piece of paper over and over and over again. After about 16 folds, she held a bird in her hands.&lt;/p&gt;  &lt;p&gt;The first time she pulled the tail of a flapping crane, she says, she realized: &lt;em&gt;Oh,&lt;/em&gt; &lt;em&gt;I folded this, and now it’s a toy&lt;/em&gt;.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That first piece was an origami classic, folded by kids at summer camp for generations and many people’s first foray into the art form. Often, it’s also the last. But Yoder was transfixed. Soon she was folding everything she could find: paper squares from chain craft shops, scraps from around the house, the weekly church bulletin, which she would cut into pieces with the aid of her fingernails. She would then “turn those into little critters and give them to any guests that were there that week,” she says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Today, perhaps millions of folds later, Yoder is a superstar known to some as the “Queen of Tessellations,” a reference to a mathematically intricate type of origami that she began exploring during her years at MIT.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“These are patterns that can repeat infinitely and are folded on a single sheet of paper,” Yoder explains. “There’s literally no end to the patterns themselves, no end to the number of designs you can create … They’re folded by hand—I don’t know of any machine that could fold them—and they are a really great way to just sit and focus and relax.”&lt;/p&gt;  &lt;p&gt;Her pieces have grown increasingly complex over time, but the patterns she creates are based on recognizable shapes, including hexagons, triangles, rhombuses, and trapezoids. Yoder folds and rotates them into repeating, potentially infinite series of shapes. Picture the graphic pattern in an M.C. Escher print, but made out of a single sheet of paper—a piece of art underpinned by mathematics and a bit of engineering, combined with the complexity of a snowflake.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Yoder grew up in southwestern Virginia, in the Blue Ridge Mountain town of Shawsville, where professors from Virginia Tech filled the pews at her Mennonite church. “All of us kids were expected to go to college,” she says. After she made her way to MIT, her brother, Jake, earned his PhD in materials engineering at Virginia Tech and now works with 3D-printed metals. Her mother, Janet, is a physical therapist and her father, Denton, is a computer systems engineer at Virginia Tech.&lt;/p&gt;  &lt;p&gt;From a young age, Yoder had an inclination for making things with her hands. “I was kind of that kid—I did all the different crafts. I did a lot of cross-stitch,” she says, including a portrait of her grandmother that now hangs framed in her kitchen.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;She also remembers an early appreciation for accuracy. “My mom tells the story about when I was five years old, we were cutting out squares, and I was like: ‘Mom, your squares are not precise enough,’” she says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Toward the end of her senior year of high school, Yoder won a math competition, which came with an apt prize: a book about modular origami, in which multiple sheets of paper are folded and combined into often elaborate structures. She took a gap year in Peru, where she continued to fold, giving little modular pieces away to children she met on her travels.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Yoder had always done paper folding in solitude, with guidance only from books. When she arrived at MIT after her time in Peru, she was surprised to learn about weekly origami gatherings and the annual convention held by the campus club OrigaMIT.&lt;/p&gt;  &lt;p&gt;“It took until I got to MIT to realize that, oh, this is an active space where people are meeting up and designing things and talking to each other about origami all weekend,” she says. She majored in Earth, Atmospheric, and Planetary Sciences (EAPS), but in the spring of her senior year, she took Erik Demaine’s popular class Geometric Folding Algorithms—and discovered that “origami research was something that people got paid to do,” as she puts it. Her final project for the class became a poster presentation at the 7th International Meeting on Origami in Science, Mathematics, and Education (7OSME). “In that course, I got hooked on origami research,” Yoder says.&lt;/p&gt;  &lt;p&gt;Demaine remembers that Yoder started to explore concepts related to tessellations in his class, which eventually led to the publication of her first paper—“Folding Triangular and Hexagonal Mazes,” coauthored with him and Jason Ku, then a lecturer at MIT. In that paper, Yoder helped demonstrate how to “generalize” a square grid maze to triangular and hexagonal grids by changing the underlying crease pattern. “We probably suggested this as an interesting open problem for people to work on, and Madonna found a really happy niche there,” says Demaine, who isn’t aware of any other former students pursuing careers in origami. “We provided the space for her to do the research, but then she went whole hog on it.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But she didn’t truly embrace tessellations until after she graduated and was preparing for a four-and-a-half-month MIT-sponsored internship in Israel. “These modulars have a lot of volume—I’m not going to bring back a suitcase full of them,” she remembers thinking. And she wasn’t going to leave behind four-plus months of folding work. “So I decided to teach myself to fold tessellations because they’re flat and travel well,” she says.&amp;nbsp; “Then it took root in my brain and never let me go.”&lt;/p&gt; 

 &lt;p&gt;But there was the practical matter of making a living.&lt;/p&gt;  &lt;p&gt;Origami principles have been used to conceive of and develop a wide range of things, from the tiny (think medical instruments or nanoscale devices that can deliver DNA into cells) to the large (such as collapsible structures usable in disaster response or foldable solar arrays for space exploration). Yoder figured if she wanted to pursue origami as a career, she would have to do it as a scientist or engineer.&lt;/p&gt;  &lt;p&gt;But after reverse-engineering hundreds of origami patterns she found online—and starting to design her own—she began to suspect otherwise. “I realized it’s actually possible to make a living as an origami artist,” she remembers. “I won’t say that now, five years out from that decision, I’ve reached a point of being able to fully financially support myself with origami, but thankfully, I married a software engineer.” (She met her husband, Manny Meraz-Rodriguez, while the two were working at the Lawrence Livermore National Laboratory, she as an intern and then as a postcollege appointee in computational geoscience.)&lt;/p&gt;  &lt;p&gt;Origami purists will say that true origami requires no cuts, no glue. The only slicing Yoder does is with a rotary cutter she uses to make hexagonal pieces of paper, stacks at a time. Though she starts with squares sometimes, the hexagon is her favored launching pad. She creases the paper into a grid, and then—­following a design that she’s created using a vector graphics program called Inkscape—begins to fold.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;“The main reason why I draw the patterns out first, besides the fact that the designs have gotten too complicated for me to hold in my brain and solve on the fly, is because I like to have the pattern rotated so that the repeats of the pattern align with the edge, which you can only do if you have the information of how the repeats of the pattern line up with the background grid,” she explains.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Using a simple tool called a bone folder (Yoder says she’s had hers for years and could pick it out of a pile by the wear pattern), she presses and creases and rotates the paper into an elaborate pattern that could, in theory, go on forever. The end result is a beautiful, satisfyingly symmetrical array of repeated, interlocking shapes that look especially impressive when held up to the light, bringing to mind a stained-glass window.&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="folded shape" class="wp-image-1126035" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/ND25-folding-04.jpg?w=680" /&gt;&lt;figcaption class="wp-element-caption"&gt;Scroll down to learn how to fold this Dancing Ribbons tessellation created by Yoder.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;p&gt;Scholars debate whether the ancient tradition of origami began in Japan or China, but the art really took off globally in the 1950s and ’60s when publishers printed and mass-marketed diagrams showing people how to fold paper into figurative objects such as birds, fish, and animals. Paper tessellations have roots in Germany in the 1920s, when the artist Josef Albers added folding to his introductory design course at the Bauhaus. This geometric tradition started gaining popularity in the 1980s and 1990s, and now, Yoder says, there are perhaps tens of thousands of people who participate. The broader universe of origami practitioners likely numbers in the millions.&lt;/p&gt;  &lt;p&gt;These aficionados attend conferences, watch YouTube videos, and take online courses, most of them to learn existing patterns. Yoder creates her own: In addition to the peer-­reviewed academic papers she’s authored on the mathematical underpinnings of her tessellations (with titles like “Symmetry Requirements and Design Equations for Origami Tessellations” and “Hybrid Hexagon Twist Interface”) and regular presentations at origami conferences across the globe, she’s designed 696 original patterns. Each year in an event she calls Advent of Tess, she teaches thousands of online participants a new design every day of December leading up to Christmas, and her website, Gathering Folds, has become a go-to source, not just for Yoder’s artwork but for instruction.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Her EAPS degree from MIT may not seem like a foundation for a career as an artist, but Yoder, who studied geology with a secondary focus on ecology, says there are connections between the fields. “There is a lot of carryover between the crystal structures and the tessellation symmetries,” she explains. “Every repeating 2D pattern obeys one of the planar symmetry groups … There are things that repeat like a hexagon, things that repeat like a square, things that repeat like a triangle, and things that repeat like a parallelogram or rectangle. And then there are things that are not rotationally symmetric. Those ideas of how things connect and how things repeat definitely carry over from my crystallography class.”&lt;/p&gt;  &lt;p&gt;Yoder cites the origami artist and physicist Robert Lang as one of the current practitioners who influenced her the most. He, like Yoder, has a math and science background but forged a career in art.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“The thing that has set her above the current crowd is that she’s really systematically explored the building blocks of tessellations and the different little patterns that can be considered building blocks, and the rules for connecting these blocks,” he explains. “Madonna’s knowledge and understanding of mathematics and geometry gives her a broader tool kit to create art, and that’s led to her success as an artist. You can’t separate the art from the science background. It’s part of the thinking process, even if the end goal is very much in the fine art world.”&lt;/p&gt;  &lt;p&gt;For Yoder, the process, both computational and tactile, is also an end in itself. It is almost a meditation—a way to slow down and contemplate. Some of her students have even suggested there might be a spiritual component to it. One said to her: “You know, the name for that connection to infinite things is called God, right?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;“So I kind of leave that more open,” she says. “I’m not super decided about what these things mean. I’m just happy to have that spark when I’m designing a pattern: Here’s how the shapes hang together, and now that I’ve drawn out those shapes, I can copy and paste, paste, paste, paste, paste, and it just clicks in very satisfyingly.”&lt;/p&gt;  &lt;p&gt;Yoder has considered whether she will ever get bored pursuing the possibilities of infinite patterns—whether she will achieve perfection and decide to put the bone folder away for good.&lt;/p&gt;  &lt;p&gt;“But I’m not convinced that I will,” she says. “There are always ways to make it harder and harder.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="diagram of origami pattern" class="wp-image-1126213" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/creasepattern.jpg?w=2000" width="2000" /&gt;&lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="example of folded pattern" class="wp-image-1126212" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AOT22_Day10-2.jpg?w=744" /&gt;&lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Fold it yourself&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Try your hand at folding Madonna Yoder’s Dancing Ribbons tessellation design featuring three closed twists: hexagon, triangle, and rhombus.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Basic instructions&lt;/strong&gt;&lt;/p&gt;  &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Download the pattern here and cut out the hexagon with the crease pattern. &lt;/p&gt;    &lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Fold all the background grid lines, making sure to fold them back and forth so the paper is ready to form the pattern. (You can precrease all the off-grid folds too, but Yoder recommends folding one twist at a time.) This pattern shows mountain folds with solid red lines and valley folds with dashed blue lines. The faded lines inside the twists are helper folds used to set up the twists; they will not be used in the final pattern.&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;3. &lt;/strong&gt;Working from the side without the pattern, fold the central hexagon.&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;4. &lt;/strong&gt;Fold the triangles.&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;5. &lt;/strong&gt;Fold the rhombuses.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Find more detailed instructions and a video tutorial&lt;em&gt;—&lt;/em&gt;as well as paper advice&lt;em&gt;—&lt;/em&gt;at technologyreview.com/tessellation.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;You can also sign up for Yoder’s annual Advent of Tess&lt;em&gt;—&lt;/em&gt;a 25-day folding challenge that begins December 1&lt;em&gt;—&lt;/em&gt;at&amp;nbsp; https://training.gatheringfolds.com/advent.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When Madonna Yoder ’17 was eight years old, she learned how to fold a square piece of paper over and over and over again. After about 16 folds, she held a bird in her hands.&lt;/p&gt;  &lt;p&gt;The first time she pulled the tail of a flapping crane, she says, she realized: &lt;em&gt;Oh,&lt;/em&gt; &lt;em&gt;I folded this, and now it’s a toy&lt;/em&gt;.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That first piece was an origami classic, folded by kids at summer camp for generations and many people’s first foray into the art form. Often, it’s also the last. But Yoder was transfixed. Soon she was folding everything she could find: paper squares from chain craft shops, scraps from around the house, the weekly church bulletin, which she would cut into pieces with the aid of her fingernails. She would then “turn those into little critters and give them to any guests that were there that week,” she says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Today, perhaps millions of folds later, Yoder is a superstar known to some as the “Queen of Tessellations,” a reference to a mathematically intricate type of origami that she began exploring during her years at MIT.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“These are patterns that can repeat infinitely and are folded on a single sheet of paper,” Yoder explains. “There’s literally no end to the patterns themselves, no end to the number of designs you can create … They’re folded by hand—I don’t know of any machine that could fold them—and they are a really great way to just sit and focus and relax.”&lt;/p&gt;  &lt;p&gt;Her pieces have grown increasingly complex over time, but the patterns she creates are based on recognizable shapes, including hexagons, triangles, rhombuses, and trapezoids. Yoder folds and rotates them into repeating, potentially infinite series of shapes. Picture the graphic pattern in an M.C. Escher print, but made out of a single sheet of paper—a piece of art underpinned by mathematics and a bit of engineering, combined with the complexity of a snowflake.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Yoder grew up in southwestern Virginia, in the Blue Ridge Mountain town of Shawsville, where professors from Virginia Tech filled the pews at her Mennonite church. “All of us kids were expected to go to college,” she says. After she made her way to MIT, her brother, Jake, earned his PhD in materials engineering at Virginia Tech and now works with 3D-printed metals. Her mother, Janet, is a physical therapist and her father, Denton, is a computer systems engineer at Virginia Tech.&lt;/p&gt;  &lt;p&gt;From a young age, Yoder had an inclination for making things with her hands. “I was kind of that kid—I did all the different crafts. I did a lot of cross-stitch,” she says, including a portrait of her grandmother that now hangs framed in her kitchen.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;She also remembers an early appreciation for accuracy. “My mom tells the story about when I was five years old, we were cutting out squares, and I was like: ‘Mom, your squares are not precise enough,’” she says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Toward the end of her senior year of high school, Yoder won a math competition, which came with an apt prize: a book about modular origami, in which multiple sheets of paper are folded and combined into often elaborate structures. She took a gap year in Peru, where she continued to fold, giving little modular pieces away to children she met on her travels.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Yoder had always done paper folding in solitude, with guidance only from books. When she arrived at MIT after her time in Peru, she was surprised to learn about weekly origami gatherings and the annual convention held by the campus club OrigaMIT.&lt;/p&gt;  &lt;p&gt;“It took until I got to MIT to realize that, oh, this is an active space where people are meeting up and designing things and talking to each other about origami all weekend,” she says. She majored in Earth, Atmospheric, and Planetary Sciences (EAPS), but in the spring of her senior year, she took Erik Demaine’s popular class Geometric Folding Algorithms—and discovered that “origami research was something that people got paid to do,” as she puts it. Her final project for the class became a poster presentation at the 7th International Meeting on Origami in Science, Mathematics, and Education (7OSME). “In that course, I got hooked on origami research,” Yoder says.&lt;/p&gt;  &lt;p&gt;Demaine remembers that Yoder started to explore concepts related to tessellations in his class, which eventually led to the publication of her first paper—“Folding Triangular and Hexagonal Mazes,” coauthored with him and Jason Ku, then a lecturer at MIT. In that paper, Yoder helped demonstrate how to “generalize” a square grid maze to triangular and hexagonal grids by changing the underlying crease pattern. “We probably suggested this as an interesting open problem for people to work on, and Madonna found a really happy niche there,” says Demaine, who isn’t aware of any other former students pursuing careers in origami. “We provided the space for her to do the research, but then she went whole hog on it.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But she didn’t truly embrace tessellations until after she graduated and was preparing for a four-and-a-half-month MIT-sponsored internship in Israel. “These modulars have a lot of volume—I’m not going to bring back a suitcase full of them,” she remembers thinking. And she wasn’t going to leave behind four-plus months of folding work. “So I decided to teach myself to fold tessellations because they’re flat and travel well,” she says.&amp;nbsp; “Then it took root in my brain and never let me go.”&lt;/p&gt; 

 &lt;p&gt;But there was the practical matter of making a living.&lt;/p&gt;  &lt;p&gt;Origami principles have been used to conceive of and develop a wide range of things, from the tiny (think medical instruments or nanoscale devices that can deliver DNA into cells) to the large (such as collapsible structures usable in disaster response or foldable solar arrays for space exploration). Yoder figured if she wanted to pursue origami as a career, she would have to do it as a scientist or engineer.&lt;/p&gt;  &lt;p&gt;But after reverse-engineering hundreds of origami patterns she found online—and starting to design her own—she began to suspect otherwise. “I realized it’s actually possible to make a living as an origami artist,” she remembers. “I won’t say that now, five years out from that decision, I’ve reached a point of being able to fully financially support myself with origami, but thankfully, I married a software engineer.” (She met her husband, Manny Meraz-Rodriguez, while the two were working at the Lawrence Livermore National Laboratory, she as an intern and then as a postcollege appointee in computational geoscience.)&lt;/p&gt;  &lt;p&gt;Origami purists will say that true origami requires no cuts, no glue. The only slicing Yoder does is with a rotary cutter she uses to make hexagonal pieces of paper, stacks at a time. Though she starts with squares sometimes, the hexagon is her favored launching pad. She creases the paper into a grid, and then—­following a design that she’s created using a vector graphics program called Inkscape—begins to fold.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;“The main reason why I draw the patterns out first, besides the fact that the designs have gotten too complicated for me to hold in my brain and solve on the fly, is because I like to have the pattern rotated so that the repeats of the pattern align with the edge, which you can only do if you have the information of how the repeats of the pattern line up with the background grid,” she explains.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Using a simple tool called a bone folder (Yoder says she’s had hers for years and could pick it out of a pile by the wear pattern), she presses and creases and rotates the paper into an elaborate pattern that could, in theory, go on forever. The end result is a beautiful, satisfyingly symmetrical array of repeated, interlocking shapes that look especially impressive when held up to the light, bringing to mind a stained-glass window.&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="folded shape" class="wp-image-1126035" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/ND25-folding-04.jpg?w=680" /&gt;&lt;figcaption class="wp-element-caption"&gt;Scroll down to learn how to fold this Dancing Ribbons tessellation created by Yoder.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;p&gt;Scholars debate whether the ancient tradition of origami began in Japan or China, but the art really took off globally in the 1950s and ’60s when publishers printed and mass-marketed diagrams showing people how to fold paper into figurative objects such as birds, fish, and animals. Paper tessellations have roots in Germany in the 1920s, when the artist Josef Albers added folding to his introductory design course at the Bauhaus. This geometric tradition started gaining popularity in the 1980s and 1990s, and now, Yoder says, there are perhaps tens of thousands of people who participate. The broader universe of origami practitioners likely numbers in the millions.&lt;/p&gt;  &lt;p&gt;These aficionados attend conferences, watch YouTube videos, and take online courses, most of them to learn existing patterns. Yoder creates her own: In addition to the peer-­reviewed academic papers she’s authored on the mathematical underpinnings of her tessellations (with titles like “Symmetry Requirements and Design Equations for Origami Tessellations” and “Hybrid Hexagon Twist Interface”) and regular presentations at origami conferences across the globe, she’s designed 696 original patterns. Each year in an event she calls Advent of Tess, she teaches thousands of online participants a new design every day of December leading up to Christmas, and her website, Gathering Folds, has become a go-to source, not just for Yoder’s artwork but for instruction.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Her EAPS degree from MIT may not seem like a foundation for a career as an artist, but Yoder, who studied geology with a secondary focus on ecology, says there are connections between the fields. “There is a lot of carryover between the crystal structures and the tessellation symmetries,” she explains. “Every repeating 2D pattern obeys one of the planar symmetry groups … There are things that repeat like a hexagon, things that repeat like a square, things that repeat like a triangle, and things that repeat like a parallelogram or rectangle. And then there are things that are not rotationally symmetric. Those ideas of how things connect and how things repeat definitely carry over from my crystallography class.”&lt;/p&gt;  &lt;p&gt;Yoder cites the origami artist and physicist Robert Lang as one of the current practitioners who influenced her the most. He, like Yoder, has a math and science background but forged a career in art.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“The thing that has set her above the current crowd is that she’s really systematically explored the building blocks of tessellations and the different little patterns that can be considered building blocks, and the rules for connecting these blocks,” he explains. “Madonna’s knowledge and understanding of mathematics and geometry gives her a broader tool kit to create art, and that’s led to her success as an artist. You can’t separate the art from the science background. It’s part of the thinking process, even if the end goal is very much in the fine art world.”&lt;/p&gt;  &lt;p&gt;For Yoder, the process, both computational and tactile, is also an end in itself. It is almost a meditation—a way to slow down and contemplate. Some of her students have even suggested there might be a spiritual component to it. One said to her: “You know, the name for that connection to infinite things is called God, right?”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;“So I kind of leave that more open,” she says. “I’m not super decided about what these things mean. I’m just happy to have that spark when I’m designing a pattern: Here’s how the shapes hang together, and now that I’ve drawn out those shapes, I can copy and paste, paste, paste, paste, paste, and it just clicks in very satisfyingly.”&lt;/p&gt;  &lt;p&gt;Yoder has considered whether she will ever get bored pursuing the possibilities of infinite patterns—whether she will achieve perfection and decide to put the bone folder away for good.&lt;/p&gt;  &lt;p&gt;“But I’m not convinced that I will,” she says. “There are always ways to make it harder and harder.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="imageSet__wrap"&gt;&lt;div class="columns__wrapper--07c4096a3b25e22dc82ee78b6368d947"&gt;&lt;div class="wp-block-columns"&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="diagram of origami pattern" class="wp-image-1126213" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/creasepattern.jpg?w=2000" width="2000" /&gt;&lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="wp-block-column"&gt;&lt;div class="columns__content--3becc448c76a1a5a553df1358f1eebf3"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="example of folded pattern" class="wp-image-1126212" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AOT22_Day10-2.jpg?w=744" /&gt;&lt;/figure&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Fold it yourself&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Try your hand at folding Madonna Yoder’s Dancing Ribbons tessellation design featuring three closed twists: hexagon, triangle, and rhombus.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Basic instructions&lt;/strong&gt;&lt;/p&gt;  &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Download the pattern here and cut out the hexagon with the crease pattern. &lt;/p&gt;    &lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Fold all the background grid lines, making sure to fold them back and forth so the paper is ready to form the pattern. (You can precrease all the off-grid folds too, but Yoder recommends folding one twist at a time.) This pattern shows mountain folds with solid red lines and valley folds with dashed blue lines. The faded lines inside the twists are helper folds used to set up the twists; they will not be used in the final pattern.&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;3. &lt;/strong&gt;Working from the side without the pattern, fold the central hexagon.&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;4. &lt;/strong&gt;Fold the triangles.&lt;/p&gt;    &lt;p&gt;&lt;strong&gt;5. &lt;/strong&gt;Fold the rhombuses.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Find more detailed instructions and a video tutorial&lt;em&gt;—&lt;/em&gt;as well as paper advice&lt;em&gt;—&lt;/em&gt;at technologyreview.com/tessellation.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;You can also sign up for Yoder’s annual Advent of Tess&lt;em&gt;—&lt;/em&gt;a 25-day folding challenge that begins December 1&lt;em&gt;—&lt;/em&gt;at&amp;nbsp; https://training.gatheringfolds.com/advent.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124723/infinite-folds/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] Engineering better care (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/21/1124712/engineering-better-care/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Every Monday, more than a hundred members of Giovanni Traverso’s Laboratory for Translational Engineering (L4TE) fill a large classroom at Brigham and Women’s Hospital for their weekly lab meeting. With a social hour, food for everyone, and updates across disciplines from mechanical engineering to veterinary science, it’s a place where a stem cell biologist might weigh in on a mechanical design, or an electrical engineer might spot a flaw in a drug delivery mechanism. And it’s a place where everyone is united by the same goal: engineering new ways to deliver medicines and monitor the body to improve patient care.&lt;/p&gt;  &lt;p&gt;Traverso’s weekly meetings bring together a mix of expertise that lab members say is unusual even in the most collaborative research spaces. But his lab—which includes its own veterinarian and a dedicated in vivo team—isn’t built like most. As an associate professor at MIT, a gastroenterologist at Brigham and Women’s, and an associate member of the Broad Institute, Traverso leads a sprawling research group that spans institutions, disciplines, and floors of lab space at MIT and beyond.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;For a lab of this size—spread across MIT, the Broad, the Brigham, the Koch Institute, and The Engine—it feels remarkably personal. Traverso, who holds the Karl Van Tassel (1925) Career Development Professorship, is known for greeting every member by name and scheduling one-on-one meetings every two or three weeks, creating a sense of trust and connection that permeates the lab.&lt;/p&gt;  &lt;p&gt;That trust is essential for a team built on radical interdisciplinarity. L4TE brings together mechanical and electrical engineers, biologists, physicians, and veterinarians in a uniquely structured lab with specialized “cores” such as fabrication, bioanalytics, and in vivo teams. The setup means a researcher can move seamlessly from developing a biological formulation to collaborating with engineers to figure out the best way to deliver it—without leaving the lab’s ecosystem. It’s a culture where everyone’s expertise is valued, people pitch in across disciplines, and projects aim squarely at the lab’s central goal: creating medical technologies that not only work in theory but survive the long, unpredictable journey to the patient.&lt;/p&gt; 
 &lt;p&gt;“At the core of what we do is really thinking about the patient, the person, and how we can help make their life better,” Traverso says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Helping patients ASAP&lt;/h3&gt;  &lt;p&gt;Traverso’s team has developed a suite of novel technologies: a star-shaped capsule that unfolds in the stomach and delivers drugs for days or weeks; a vibrating pill that mimics the feeling of fullness; the technology behind a once-a-week antipsychotic tablet that has completed phase III clinical trials. (See “Designing devices for real-world care,” below.) Traverso has cofounded 11 startups to carry such innovations out of the lab and into the world, each tailored to the technology and patient population it serves.&lt;/p&gt; 
 &lt;p&gt;But the products are only part of the story. What distinguishes Traverso’s approach is the way those products are conceived and built. In many research groups, initial discoveries are developed into early prototypes and then passed on to other teams—sometimes in industry, sometimes in clinical settings—for more advanced testing and eventual commercialization. Traverso’s lab typically links those steps into one continuous system, blending invention, prototyping, testing, iteration, and clinical feedback as the work of a single interdisciplinary team. Engineers sit shoulder to shoulder with physicians, materials scientists with microbiologists. On any given day, a researcher might start the morning discussing an animal study with a veterinarian, spend the afternoon refining a mechanical design, and close the day in a meeting with a regulatory expert. The setup collapses months of back-and-forth between separate teams into the collaborative environment of L4TE.&lt;/p&gt;  &lt;p&gt;“This is a lab where if you want to learn something, you can learn everything if you want,” says Troy Ziliang Kang, one of the research scientists.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;In a field where translating scientific ideas into practical applications can take years (or stall indefinitely), Traverso has built a culture designed to shorten that path.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;The range of problems the lab tackles reflects its interdisciplinary openness. One recent project aimed to replace invasive contraceptive devices such as vaginal rings with a biodegradable injectable that begins as a liquid, solidifies inside the body, and dissolves safely over time.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Another project addresses the challenge of delivering drugs directly to the gut, bypassing the mucus barrier that blocks many treatments. For Kang, whose grandfather died of gastric cancer, the work is personal. He’s developing devices that combine traditional drugs with&amp;nbsp;electroceuticals—therapies that use electrical stimulation to influence cells or tissues.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“What I’m trying to do is find a mechanical approach, trying to see if we can really, through physical and mechanical approaches, break through those barriers and to deliver the electroceuticals and drugs to the gut,” he says.&lt;/p&gt;  &lt;p&gt;In a field where the process of translating scientific ideas into practical applications can take years (or stall indefinitely), Traverso, 49, has built a culture designed to shorten that path. Researchers focus on designing devices with the clinical relevance to help people in the near term.&amp;nbsp; And they don’t wait for outsiders to take an idea forward. They often initiate collaborations with entrepreneurs, investors, and partners to create startups or push projects directly into early trials—or even just do it themselves. The projects in the L4TE Lab are ambitious, but the aim is simple: Solve problems that matter and build the tools to make those solutions real.&lt;/p&gt;  &lt;p&gt;Nabil Shalabi, an instructor in medicine at Harvard/BWH, an associate scientist at the Broad Institute, and a research affiliate in Traverso’s lab, sums up the attitude succinctly: “I would say this lab is really about one thing, and it’s about helping people.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The physician-inventor&lt;/h3&gt;  &lt;p&gt;Traverso’s path into medicine and engineering began far from the hospitals and labs where he works today. Born in Cambridge, England, he moved with his family to Peru when he was still young. His father had grown up there in a family with Italian roots; his mother came from Nicaragua. He spent most of his childhood in Lima before political turmoil in Peru led his family to relocate to Toronto when he was 14.&lt;/p&gt; 

 &lt;p&gt;In high school, after finishing most of his course requirements early, he followed the advice of a chemistry teacher and joined a co-op program that would give him a glimpse of some career options. That decision brought him to a genetics lab at the Toronto Hospital for Sick Children, where he spent his afternoons helping map chromosome 7 and learning molecular techniques like PCR.&lt;/p&gt;  &lt;p&gt;“In high school, and even before that, I always enjoyed science,” Traverso says.&lt;/p&gt;  &lt;p&gt;After class, he’d ride the subway downtown and step into a world of hands-on science, working alongside graduate students in the early days of genomics.&lt;/p&gt;  &lt;p&gt;“I really fell in love with the day-to-day, the process, and how one goes about asking a question and then trying to answer that question experimentally,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;By the time he finished high school, he had already begun to see how science and medicine could intersect. He began an undergraduate medical program at Cambridge University, but during his second year, he reached out to the cancer biologist Bert Vogelstein and joined his lab at Johns Hopkins for the summer. The work resonated. By the end of the internship, Vogelstein asked if he’d consider staying to pursue a PhD. Traverso agreed, pausing his medical training after earning an undergraduate degree in medical sciences and genetics, and moved to Baltimore to begin a doctorate in molecular biology.&lt;/p&gt;  &lt;p&gt;As a PhD student, he focused on the early detection of colon cancer, developing a method to identify mutations in stool samples—a concept later licensed by Exact Sciences and used in what is now known as the Cologuard test. After completing his PhD (and earning a spot on &lt;em&gt;Technology Review’s&lt;/em&gt; 2003 TR35 list of promising young innovators for that work), he returned to Cambridge to finish medical school and spent the next three years in the UK, including a year as a house officer (the equivalent of a clinical intern in the US).&lt;/p&gt;  &lt;p&gt;Traverso chose to pursue clinical training alongside research because he believed each would make the other stronger. “I felt that having the knowledge would help inform future research development,” he says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="inset image of a hand holding a capsule; main image the hand is holding a star shaped object" class="wp-image-1126054" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/1748_GiovanniTraverso5266_final.jpg?w=1720" width="1720" /&gt;&lt;figcaption class="wp-element-caption"&gt;An ingestible drug-releasing capsule about the size of a multivitamin expands into a star shape once inside the patient’s stomach.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JARED LEEDS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;So in 2007, as Traverso began a residency in internal medicine at Brigham and Women’s, he also approached MIT, where he reached out to Institute Professor Robert Langer, ScD ’74. Though Traverso didn’t have a background in Langer’s field of chemical engineering, he saw the value of pairing clinical insight with the materials science research happening in the professor’s lab, which develops polymers, nanoparticles, and other novel materials to tackle biomedical challenges such as delivering drugs precisely to diseased tissue or providing long-term treatment through implanted devices. Langer welcomed him into the group as a postdoctoral fellow.&lt;/p&gt; 
 &lt;p&gt;In Langer’s lab, he found a place where clinical problems sparked engineering solutions, and where those solutions were designed with the patient in mind from the outset. Many of Traverso’s ideas came directly from his work in the hospital: Could medications be delivered in ways that make it easier for patients to take them consistently? Could a drug be redesigned so it wouldn’t require refrigeration in a rural clinic? And caring for a patient who’d swallowed shards of glass that ultimately passed without injury led Traverso to recognize the GI tract’s tolerance for sharp objects, inspiring his work on the microneedle pill.&lt;/p&gt;  &lt;p&gt;“A lot of what we do and think about is: How do we make it easier for people to receive therapy for conditions that they may be suffering from?” Traverso says. How can they “really maximize health, whether it be by nutrient enhancement or by helping women have control over their fertility?”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;If the lab sometimes runs like a startup incubator, its founder still thinks like a physician.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Scaling up to help more people&lt;/h3&gt;  &lt;p&gt;Traverso has cofounded multiple companies to help commercialize his group’s inventions. Some target global health challenges, like developing more sustainable personal protective equipment (PPE) for health-care workers. Others take on chronic conditions that require constant dosing—HIV, schizophrenia, diabetes—by developing long-­acting oral or injectable therapies.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;From the outset, materials, dimensions, and mechanisms are chosen for more than just performance in the lab. The researchers also consider the realities of regulation, manufacturing constraints, and safe use in patients.&lt;/p&gt;  &lt;p&gt;“We definitely want to be designing these devices to be made of safe materials or [at a] safe size,” says James McRae, SM ’22, PhD ’25. “We think about these regulatory constraints that could come up in a company setting pretty early in our research process.” As part of his PhD work with Traverso, McRae created a “swallow-­and-forget” health-tracking capsule that can stay in the stomach for months—and it doesn’t require surgery to install, as an implant would. The capsule measures tiny shifts in stomach temperature that happen whenever a person eats or drinks, providing a continuous record of eating patterns that’s far more reliable than what external devices or self-reporting can capture. The technology could offer new insight into how drugs such as Ozempic and other GLP-1 therapies change behavior—something that has been notoriously hard to monitor. From “day one,” McRae made sure to involve external companies and regulatory consultants for future human testing.&lt;/p&gt;  &lt;p&gt;Traverso describes the lab’s work as a “continuum,” likening research projects to children who are born, nurtured, and eventually sent into the world to thrive and help people.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126056" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT_Underwater-Adhesives-02-press.jpg?w=1403" /&gt;&lt;figcaption class="wp-element-caption"&gt;Traverso and his team developed a device that can adhere to soft, wet surfaces. The design was inspired by studies of a sucker fish that attaches to sharks and other marine animals.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF THE RESEARCHERS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;For lab employee Matt Murphy, a mechanical engineer who manages one of the main mechanical fabrication spaces, that approach is part of the draw. Having worked with researchers on projects spanning multiple disciplines—mechanical engineering, electronics, materials science, biology—he’s now preparing to spin out a company with one of Traverso’s postdocs.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“I feel like I got the PhD experience just working here for four years and being involved in health projects,” he says. “This has been an amazing opportunity to really see the first stages of company formation and how the early research really drives the commercialization of new technology.”&lt;/p&gt;  &lt;p&gt;The lab’s specialized “cores” ensure that projects have consistent support and can draw on plenty of expertise, regardless of how many students or postdocs come and go. If a challenge arises in an area in which a lab member has limited knowledge, chances are someone else in the lab has that background and will gladly help. “The culture is so collaborative that everybody wants to teach everybody,” says Murphy.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Creating opportunities&amp;nbsp;&lt;/h3&gt;  &lt;p&gt;In Traverso’s lab, members are empowered to pursue technically demanding research because the culture he created encourages them to stretch into new disciplines, take ownership of projects, and imagine where their work might go next. For some, that means cofounding a company. For others, it means leaving with the skills and network to shape their next big idea.&lt;/p&gt;  &lt;p&gt;“He gives you both the agency and the support,” says Isaac Tucker, an L4TE postdoc based at the Broad Institute. “Gio trusts the leads in his lab to just execute on tasks.” McRae adds that Traverso is adept at identifying “pain points” in research and providing the necessary resources to remove barriers, which helps projects advance efficiently.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;A project led by Kimberley Biggs, another L4TE postdoc, captures how the lab approaches high-stakes problems. Funded by the Gates Foundation, Biggs is developing a way to stabilize therapeutic bacteria used for neonatal and women’s health treatments so they remain effective without refrigeration—critical for patients in areas without reliable temperature-controlled supply chains. A biochemist by training, she had never worked on devices before joining the lab, but she collaborated closely with the mechanical fabrication team to embed her bacterial therapy for conditions such as bacterial vaginosis and recurrent urinary tract infections into an intravaginal ring that can release it over time. She says Traverso gave her “an incredible amount of trust” to lead the project from the start but continued to touch base often, making sure there were “no significant bottlenecks” and that she was meeting all the goals she wanted to meet to progress in her career.&lt;/p&gt;  &lt;p&gt;Traverso encourages collaboration by putting together project teams that combine engineers, physicians, and scientists from other fields—a strategy he says can be transformative.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“If you only have one expert, they are constrained to what they know,” he explains. But “when you bring an electrical engineer together with a biologist or physician, the way that they’ll be able to see the problem or the challenge is very different.” As a result, “you see things that perhaps you hadn’t even considered were possible,” he says. Moving a project from a concept to a successful clinical trial “takes a village,” he adds. It’s a “complex, multi-step, multi-person, multi-year” process involving “tens if not hundreds of millions of dollars’ worth of effort.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Good ideas deserve to be tested&lt;/h3&gt;  &lt;p&gt;The portion of Traverso’s lab housed at the “tough tech” incubator The Engine—and the only academic group working there—occupies a 30-bench private lab alongside shared fabrication spaces, heavy machinery, and communal rooms of specialized lab equipment. The combination of dedicated and shared resources has helped reduce some initial equipment expenses for new projects, while the startup-dense environment puts potential collaborators, venture capital, and commercialization pathways within easy reach. Biggs’s work on bacterial treatments is one of the lab’s projects at The Engine. Others include work to develop electronics for capsule-based devices and an applicator for microneedle patches.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Traverso’s philosophy is to “fail well and fail fast and move on.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The end of one table houses “blue sky” research on a topic of long-standing interest to Traverso: pasta. Led by PhD student Jack Chen, the multi-pronged project includes using generative AI to help design new pasta shapes with superior sauce adhesion. Chen and collaborators ranging from executive chefs to experts in fluid dynamics apply the same analytical rigor to this research that they bring to medical devices. It’s playful work, but it’s also a microcosm of the lab’s culture: interdisciplinary to its core, unafraid to cross boundaries, and grounded in Traverso’s belief that good ideas deserve to be tested—even if they fail.&lt;/p&gt;  &lt;p&gt;“I’d say the majority of things that I’ve ever been involved in failed,” he says. “But I think it depends on how you define failure.” He says that most of the projects he worked on for the first year and a half of his own PhD either just “kind of worked” or didn’t work at all—causing him to step back and take a different approach that ultimately led him to develop the highly effective technique now used in the Cologuard test. “Even if a hypothesis that we had didn’t work out, or didn’t work out as we thought it might, the process itself, I think, is valuable,” he says. So his philosophy is to “fail well and fail fast and move on.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="hand holding a spherical metal object" class="wp-image-1126053" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/1748_GiovanniTraverso5264_final2.jpg?w=1764" width="1764" /&gt;&lt;figcaption class="wp-element-caption"&gt;A tiny capsule that delivers a burst of medication directly into the GI tract offers an alternative to injections.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JARED LEEDS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In practice, that means encouraging students and postdocs to take on big, uncertain problems, knowing a dead end isn’t the end of their careers—just an opportunity to learn how to navigate the next challenge better.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;McRae remembers when a major program—two or three years in the making—abruptly changed course after its sponsor shifted priorities. The team had been preparing a device for safety testing in humans; suddenly, the focus on that goal was gone. Rather than shelving the work, Traverso urged the group to use it as an opportunity to “be a little more creative again” and explore new directions, McRae says. That pivot sparked his work on an autonomous drug delivery system, opening lines of research the team hadn’t pursued before. In this system, patients swallow two capsules that interact in the stomach. When a sensor capsule detects an abnormal signal, it directs a second capsule to release a drug.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“He will often say, ‘I have a focus on not wasting time. Time is something that you can’t buy back. Time is something that you can’t save and bank for later.’”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Kimberley Biggs&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;“When things aren’t working, just make &lt;em&gt;sure&lt;/em&gt; they didn’t work and you’re confident &lt;em&gt;why&lt;/em&gt; they didn’t work,” Traverso says he tells his students. “Is it the biology? Is it the materials science? Is it the mechanics that aren’t just aligning for whatever reason?” He models that diagnostic mindset—and the importance of preserving momentum.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“He will often say, ‘I have a focus on not wasting time. Time is something that you can’t buy back. Time is something that you can’t save and bank for later,’” says Biggs. “And so whenever you do encounter some sort of bottleneck, he is so supportive in trying to fix that.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Traverso’s teaching reflects the same interplay between invention, risk, and real-world impact. In Translational Engineering, one of his graduate-level courses at MIT, he invites experts from the FDA, hospitals, and startups to speak about the realities of bringing medical technology to the world.&lt;/p&gt;  &lt;p&gt;“He shared his network with us,” says Murphy, who took the course while working in the lab. “Now that I’m trying to spin out a company, I can reach out to these people.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Although he now spends most of his time on research and teaching, Traverso maintains an inpatient practice at the Brigham, participating in the consult service—a team of gastroenterology fellows and medical students supervising patient care—for several weeks a year. Staying connected to patients keeps the problems concrete and helps guide decisions on which puzzles to tackle in the lab.&lt;/p&gt;  &lt;p&gt;“I think there are certain puzzles in front of us, and I do gravitate to areas that have a solution that will help people in the near term,” he says.&lt;/p&gt;  &lt;p&gt;For Traverso, the measure of success is not the complexity of the engineering but the efficacy of the result. The goal is always a therapy that works for the people who need it, wherever they are.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Every Monday, more than a hundred members of Giovanni Traverso’s Laboratory for Translational Engineering (L4TE) fill a large classroom at Brigham and Women’s Hospital for their weekly lab meeting. With a social hour, food for everyone, and updates across disciplines from mechanical engineering to veterinary science, it’s a place where a stem cell biologist might weigh in on a mechanical design, or an electrical engineer might spot a flaw in a drug delivery mechanism. And it’s a place where everyone is united by the same goal: engineering new ways to deliver medicines and monitor the body to improve patient care.&lt;/p&gt;  &lt;p&gt;Traverso’s weekly meetings bring together a mix of expertise that lab members say is unusual even in the most collaborative research spaces. But his lab—which includes its own veterinarian and a dedicated in vivo team—isn’t built like most. As an associate professor at MIT, a gastroenterologist at Brigham and Women’s, and an associate member of the Broad Institute, Traverso leads a sprawling research group that spans institutions, disciplines, and floors of lab space at MIT and beyond.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;For a lab of this size—spread across MIT, the Broad, the Brigham, the Koch Institute, and The Engine—it feels remarkably personal. Traverso, who holds the Karl Van Tassel (1925) Career Development Professorship, is known for greeting every member by name and scheduling one-on-one meetings every two or three weeks, creating a sense of trust and connection that permeates the lab.&lt;/p&gt;  &lt;p&gt;That trust is essential for a team built on radical interdisciplinarity. L4TE brings together mechanical and electrical engineers, biologists, physicians, and veterinarians in a uniquely structured lab with specialized “cores” such as fabrication, bioanalytics, and in vivo teams. The setup means a researcher can move seamlessly from developing a biological formulation to collaborating with engineers to figure out the best way to deliver it—without leaving the lab’s ecosystem. It’s a culture where everyone’s expertise is valued, people pitch in across disciplines, and projects aim squarely at the lab’s central goal: creating medical technologies that not only work in theory but survive the long, unpredictable journey to the patient.&lt;/p&gt; 
 &lt;p&gt;“At the core of what we do is really thinking about the patient, the person, and how we can help make their life better,” Traverso says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Helping patients ASAP&lt;/h3&gt;  &lt;p&gt;Traverso’s team has developed a suite of novel technologies: a star-shaped capsule that unfolds in the stomach and delivers drugs for days or weeks; a vibrating pill that mimics the feeling of fullness; the technology behind a once-a-week antipsychotic tablet that has completed phase III clinical trials. (See “Designing devices for real-world care,” below.) Traverso has cofounded 11 startups to carry such innovations out of the lab and into the world, each tailored to the technology and patient population it serves.&lt;/p&gt; 
 &lt;p&gt;But the products are only part of the story. What distinguishes Traverso’s approach is the way those products are conceived and built. In many research groups, initial discoveries are developed into early prototypes and then passed on to other teams—sometimes in industry, sometimes in clinical settings—for more advanced testing and eventual commercialization. Traverso’s lab typically links those steps into one continuous system, blending invention, prototyping, testing, iteration, and clinical feedback as the work of a single interdisciplinary team. Engineers sit shoulder to shoulder with physicians, materials scientists with microbiologists. On any given day, a researcher might start the morning discussing an animal study with a veterinarian, spend the afternoon refining a mechanical design, and close the day in a meeting with a regulatory expert. The setup collapses months of back-and-forth between separate teams into the collaborative environment of L4TE.&lt;/p&gt;  &lt;p&gt;“This is a lab where if you want to learn something, you can learn everything if you want,” says Troy Ziliang Kang, one of the research scientists.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;In a field where translating scientific ideas into practical applications can take years (or stall indefinitely), Traverso has built a culture designed to shorten that path.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;The range of problems the lab tackles reflects its interdisciplinary openness. One recent project aimed to replace invasive contraceptive devices such as vaginal rings with a biodegradable injectable that begins as a liquid, solidifies inside the body, and dissolves safely over time.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Another project addresses the challenge of delivering drugs directly to the gut, bypassing the mucus barrier that blocks many treatments. For Kang, whose grandfather died of gastric cancer, the work is personal. He’s developing devices that combine traditional drugs with&amp;nbsp;electroceuticals—therapies that use electrical stimulation to influence cells or tissues.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“What I’m trying to do is find a mechanical approach, trying to see if we can really, through physical and mechanical approaches, break through those barriers and to deliver the electroceuticals and drugs to the gut,” he says.&lt;/p&gt;  &lt;p&gt;In a field where the process of translating scientific ideas into practical applications can take years (or stall indefinitely), Traverso, 49, has built a culture designed to shorten that path. Researchers focus on designing devices with the clinical relevance to help people in the near term.&amp;nbsp; And they don’t wait for outsiders to take an idea forward. They often initiate collaborations with entrepreneurs, investors, and partners to create startups or push projects directly into early trials—or even just do it themselves. The projects in the L4TE Lab are ambitious, but the aim is simple: Solve problems that matter and build the tools to make those solutions real.&lt;/p&gt;  &lt;p&gt;Nabil Shalabi, an instructor in medicine at Harvard/BWH, an associate scientist at the Broad Institute, and a research affiliate in Traverso’s lab, sums up the attitude succinctly: “I would say this lab is really about one thing, and it’s about helping people.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The physician-inventor&lt;/h3&gt;  &lt;p&gt;Traverso’s path into medicine and engineering began far from the hospitals and labs where he works today. Born in Cambridge, England, he moved with his family to Peru when he was still young. His father had grown up there in a family with Italian roots; his mother came from Nicaragua. He spent most of his childhood in Lima before political turmoil in Peru led his family to relocate to Toronto when he was 14.&lt;/p&gt; 

 &lt;p&gt;In high school, after finishing most of his course requirements early, he followed the advice of a chemistry teacher and joined a co-op program that would give him a glimpse of some career options. That decision brought him to a genetics lab at the Toronto Hospital for Sick Children, where he spent his afternoons helping map chromosome 7 and learning molecular techniques like PCR.&lt;/p&gt;  &lt;p&gt;“In high school, and even before that, I always enjoyed science,” Traverso says.&lt;/p&gt;  &lt;p&gt;After class, he’d ride the subway downtown and step into a world of hands-on science, working alongside graduate students in the early days of genomics.&lt;/p&gt;  &lt;p&gt;“I really fell in love with the day-to-day, the process, and how one goes about asking a question and then trying to answer that question experimentally,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;By the time he finished high school, he had already begun to see how science and medicine could intersect. He began an undergraduate medical program at Cambridge University, but during his second year, he reached out to the cancer biologist Bert Vogelstein and joined his lab at Johns Hopkins for the summer. The work resonated. By the end of the internship, Vogelstein asked if he’d consider staying to pursue a PhD. Traverso agreed, pausing his medical training after earning an undergraduate degree in medical sciences and genetics, and moved to Baltimore to begin a doctorate in molecular biology.&lt;/p&gt;  &lt;p&gt;As a PhD student, he focused on the early detection of colon cancer, developing a method to identify mutations in stool samples—a concept later licensed by Exact Sciences and used in what is now known as the Cologuard test. After completing his PhD (and earning a spot on &lt;em&gt;Technology Review’s&lt;/em&gt; 2003 TR35 list of promising young innovators for that work), he returned to Cambridge to finish medical school and spent the next three years in the UK, including a year as a house officer (the equivalent of a clinical intern in the US).&lt;/p&gt;  &lt;p&gt;Traverso chose to pursue clinical training alongside research because he believed each would make the other stronger. “I felt that having the knowledge would help inform future research development,” he says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="inset image of a hand holding a capsule; main image the hand is holding a star shaped object" class="wp-image-1126054" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/1748_GiovanniTraverso5266_final.jpg?w=1720" width="1720" /&gt;&lt;figcaption class="wp-element-caption"&gt;An ingestible drug-releasing capsule about the size of a multivitamin expands into a star shape once inside the patient’s stomach.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JARED LEEDS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;So in 2007, as Traverso began a residency in internal medicine at Brigham and Women’s, he also approached MIT, where he reached out to Institute Professor Robert Langer, ScD ’74. Though Traverso didn’t have a background in Langer’s field of chemical engineering, he saw the value of pairing clinical insight with the materials science research happening in the professor’s lab, which develops polymers, nanoparticles, and other novel materials to tackle biomedical challenges such as delivering drugs precisely to diseased tissue or providing long-term treatment through implanted devices. Langer welcomed him into the group as a postdoctoral fellow.&lt;/p&gt; 
 &lt;p&gt;In Langer’s lab, he found a place where clinical problems sparked engineering solutions, and where those solutions were designed with the patient in mind from the outset. Many of Traverso’s ideas came directly from his work in the hospital: Could medications be delivered in ways that make it easier for patients to take them consistently? Could a drug be redesigned so it wouldn’t require refrigeration in a rural clinic? And caring for a patient who’d swallowed shards of glass that ultimately passed without injury led Traverso to recognize the GI tract’s tolerance for sharp objects, inspiring his work on the microneedle pill.&lt;/p&gt;  &lt;p&gt;“A lot of what we do and think about is: How do we make it easier for people to receive therapy for conditions that they may be suffering from?” Traverso says. How can they “really maximize health, whether it be by nutrient enhancement or by helping women have control over their fertility?”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;If the lab sometimes runs like a startup incubator, its founder still thinks like a physician.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Scaling up to help more people&lt;/h3&gt;  &lt;p&gt;Traverso has cofounded multiple companies to help commercialize his group’s inventions. Some target global health challenges, like developing more sustainable personal protective equipment (PPE) for health-care workers. Others take on chronic conditions that require constant dosing—HIV, schizophrenia, diabetes—by developing long-­acting oral or injectable therapies.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;From the outset, materials, dimensions, and mechanisms are chosen for more than just performance in the lab. The researchers also consider the realities of regulation, manufacturing constraints, and safe use in patients.&lt;/p&gt;  &lt;p&gt;“We definitely want to be designing these devices to be made of safe materials or [at a] safe size,” says James McRae, SM ’22, PhD ’25. “We think about these regulatory constraints that could come up in a company setting pretty early in our research process.” As part of his PhD work with Traverso, McRae created a “swallow-­and-forget” health-tracking capsule that can stay in the stomach for months—and it doesn’t require surgery to install, as an implant would. The capsule measures tiny shifts in stomach temperature that happen whenever a person eats or drinks, providing a continuous record of eating patterns that’s far more reliable than what external devices or self-reporting can capture. The technology could offer new insight into how drugs such as Ozempic and other GLP-1 therapies change behavior—something that has been notoriously hard to monitor. From “day one,” McRae made sure to involve external companies and regulatory consultants for future human testing.&lt;/p&gt;  &lt;p&gt;Traverso describes the lab’s work as a “continuum,” likening research projects to children who are born, nurtured, and eventually sent into the world to thrive and help people.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1126056" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/MIT_Underwater-Adhesives-02-press.jpg?w=1403" /&gt;&lt;figcaption class="wp-element-caption"&gt;Traverso and his team developed a device that can adhere to soft, wet surfaces. The design was inspired by studies of a sucker fish that attaches to sharks and other marine animals.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF THE RESEARCHERS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;For lab employee Matt Murphy, a mechanical engineer who manages one of the main mechanical fabrication spaces, that approach is part of the draw. Having worked with researchers on projects spanning multiple disciplines—mechanical engineering, electronics, materials science, biology—he’s now preparing to spin out a company with one of Traverso’s postdocs.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“I feel like I got the PhD experience just working here for four years and being involved in health projects,” he says. “This has been an amazing opportunity to really see the first stages of company formation and how the early research really drives the commercialization of new technology.”&lt;/p&gt;  &lt;p&gt;The lab’s specialized “cores” ensure that projects have consistent support and can draw on plenty of expertise, regardless of how many students or postdocs come and go. If a challenge arises in an area in which a lab member has limited knowledge, chances are someone else in the lab has that background and will gladly help. “The culture is so collaborative that everybody wants to teach everybody,” says Murphy.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Creating opportunities&amp;nbsp;&lt;/h3&gt;  &lt;p&gt;In Traverso’s lab, members are empowered to pursue technically demanding research because the culture he created encourages them to stretch into new disciplines, take ownership of projects, and imagine where their work might go next. For some, that means cofounding a company. For others, it means leaving with the skills and network to shape their next big idea.&lt;/p&gt;  &lt;p&gt;“He gives you both the agency and the support,” says Isaac Tucker, an L4TE postdoc based at the Broad Institute. “Gio trusts the leads in his lab to just execute on tasks.” McRae adds that Traverso is adept at identifying “pain points” in research and providing the necessary resources to remove barriers, which helps projects advance efficiently.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;A project led by Kimberley Biggs, another L4TE postdoc, captures how the lab approaches high-stakes problems. Funded by the Gates Foundation, Biggs is developing a way to stabilize therapeutic bacteria used for neonatal and women’s health treatments so they remain effective without refrigeration—critical for patients in areas without reliable temperature-controlled supply chains. A biochemist by training, she had never worked on devices before joining the lab, but she collaborated closely with the mechanical fabrication team to embed her bacterial therapy for conditions such as bacterial vaginosis and recurrent urinary tract infections into an intravaginal ring that can release it over time. She says Traverso gave her “an incredible amount of trust” to lead the project from the start but continued to touch base often, making sure there were “no significant bottlenecks” and that she was meeting all the goals she wanted to meet to progress in her career.&lt;/p&gt;  &lt;p&gt;Traverso encourages collaboration by putting together project teams that combine engineers, physicians, and scientists from other fields—a strategy he says can be transformative.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“If you only have one expert, they are constrained to what they know,” he explains. But “when you bring an electrical engineer together with a biologist or physician, the way that they’ll be able to see the problem or the challenge is very different.” As a result, “you see things that perhaps you hadn’t even considered were possible,” he says. Moving a project from a concept to a successful clinical trial “takes a village,” he adds. It’s a “complex, multi-step, multi-person, multi-year” process involving “tens if not hundreds of millions of dollars’ worth of effort.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Good ideas deserve to be tested&lt;/h3&gt;  &lt;p&gt;The portion of Traverso’s lab housed at the “tough tech” incubator The Engine—and the only academic group working there—occupies a 30-bench private lab alongside shared fabrication spaces, heavy machinery, and communal rooms of specialized lab equipment. The combination of dedicated and shared resources has helped reduce some initial equipment expenses for new projects, while the startup-dense environment puts potential collaborators, venture capital, and commercialization pathways within easy reach. Biggs’s work on bacterial treatments is one of the lab’s projects at The Engine. Others include work to develop electronics for capsule-based devices and an applicator for microneedle patches.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Traverso’s philosophy is to “fail well and fail fast and move on.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The end of one table houses “blue sky” research on a topic of long-standing interest to Traverso: pasta. Led by PhD student Jack Chen, the multi-pronged project includes using generative AI to help design new pasta shapes with superior sauce adhesion. Chen and collaborators ranging from executive chefs to experts in fluid dynamics apply the same analytical rigor to this research that they bring to medical devices. It’s playful work, but it’s also a microcosm of the lab’s culture: interdisciplinary to its core, unafraid to cross boundaries, and grounded in Traverso’s belief that good ideas deserve to be tested—even if they fail.&lt;/p&gt;  &lt;p&gt;“I’d say the majority of things that I’ve ever been involved in failed,” he says. “But I think it depends on how you define failure.” He says that most of the projects he worked on for the first year and a half of his own PhD either just “kind of worked” or didn’t work at all—causing him to step back and take a different approach that ultimately led him to develop the highly effective technique now used in the Cologuard test. “Even if a hypothesis that we had didn’t work out, or didn’t work out as we thought it might, the process itself, I think, is valuable,” he says. So his philosophy is to “fail well and fail fast and move on.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="hand holding a spherical metal object" class="wp-image-1126053" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/1748_GiovanniTraverso5264_final2.jpg?w=1764" width="1764" /&gt;&lt;figcaption class="wp-element-caption"&gt;A tiny capsule that delivers a burst of medication directly into the GI tract offers an alternative to injections.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JARED LEEDS&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In practice, that means encouraging students and postdocs to take on big, uncertain problems, knowing a dead end isn’t the end of their careers—just an opportunity to learn how to navigate the next challenge better.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;McRae remembers when a major program—two or three years in the making—abruptly changed course after its sponsor shifted priorities. The team had been preparing a device for safety testing in humans; suddenly, the focus on that goal was gone. Rather than shelving the work, Traverso urged the group to use it as an opportunity to “be a little more creative again” and explore new directions, McRae says. That pivot sparked his work on an autonomous drug delivery system, opening lines of research the team hadn’t pursued before. In this system, patients swallow two capsules that interact in the stomach. When a sensor capsule detects an abnormal signal, it directs a second capsule to release a drug.&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“He will often say, ‘I have a focus on not wasting time. Time is something that you can’t buy back. Time is something that you can’t save and bank for later.’”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Kimberley Biggs&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;“When things aren’t working, just make &lt;em&gt;sure&lt;/em&gt; they didn’t work and you’re confident &lt;em&gt;why&lt;/em&gt; they didn’t work,” Traverso says he tells his students. “Is it the biology? Is it the materials science? Is it the mechanics that aren’t just aligning for whatever reason?” He models that diagnostic mindset—and the importance of preserving momentum.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“He will often say, ‘I have a focus on not wasting time. Time is something that you can’t buy back. Time is something that you can’t save and bank for later,’” says Biggs. “And so whenever you do encounter some sort of bottleneck, he is so supportive in trying to fix that.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Traverso’s teaching reflects the same interplay between invention, risk, and real-world impact. In Translational Engineering, one of his graduate-level courses at MIT, he invites experts from the FDA, hospitals, and startups to speak about the realities of bringing medical technology to the world.&lt;/p&gt;  &lt;p&gt;“He shared his network with us,” says Murphy, who took the course while working in the lab. “Now that I’m trying to spin out a company, I can reach out to these people.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Although he now spends most of his time on research and teaching, Traverso maintains an inpatient practice at the Brigham, participating in the consult service—a team of gastroenterology fellows and medical students supervising patient care—for several weeks a year. Staying connected to patients keeps the problems concrete and helps guide decisions on which puzzles to tackle in the lab.&lt;/p&gt;  &lt;p&gt;“I think there are certain puzzles in front of us, and I do gravitate to areas that have a solution that will help people in the near term,” he says.&lt;/p&gt;  &lt;p&gt;For Traverso, the measure of success is not the complexity of the engineering but the efficacy of the result. The goal is always a therapy that works for the people who need it, wherever they are.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/21/1124712/engineering-better-care/</guid><pubDate>Tue, 21 Oct 2025 21:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI’s new browser is a broadside shot at Google (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/openais-new-browser-is-a-broadside-shot-at-google/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-21-at-5.13.11-PM.jpg?resize=1200,673" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Today, OpenAI launched its new Atlas web browser in a surprise livestream.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;The show started with CEO Sam Altman himself, speaking directly to the audience.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think AI represents a rare, once-a-decade opportunity to rethink what a browser can be,” Altman said. “In the same way that, for the previous way people used the internet, the URL bar and the search box were a great analogue, what we’re starting to see is that the chat experience and the web browser can be a quick analogue.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It was an inspiring note, in the classic Steve Jobs mode. But even more important than Altman’s browser was the detritus he was sweeping aside to make room. It wasn’t just casting present-day browsers as old, but part of a whole package of goods that are about to be replaced by AI — as Altman put it, part of “the previous way people used the internet.” And most of those soon-to-be obsolete services trace back to a single company: Google.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s browser project has been an open secret in Silicon Valley since at least this summer — and it was clear from the beginning that it would be a potential threat to Google, current owner of the world’s most popular browser. But Tuesday’s product and presentation details made it clear exactly how much the web giant has to lose in the AI era — and how little the Google’s success with Gemini seems to have helped.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The immediate threat is simple enough: ChatGPT draws 800 million users a week, and if those users switch to Atlas, they’re most likely switching away from Chrome. Losing those users doesn’t have an immediate dollar cost for Google (it’s a free product, after all) but it limits Google’s ability to target ads to those users or nudge them to Google Search — a particular sore point because, just last month, Google was barred by the US Department of Justice from making any search exclusivity deals.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then, there’s how OpenAI deals&lt;strong&gt; &lt;/strong&gt;with search itself. AI has already strained the search model of the web, surfacing processed information instead of content that can be advertised against. But on OpenAI’s livestream, Atlas head of engineering Ben Goodger (himself a central figure in developing both Firefox and Chrome) described the new kind of chat-oriented search as a paradigm shift.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This new model of search is really powerful,” Goodger said. “It’s a multi-turn experience. You can have this back-and-forth with your search results instead of just being sent off to a web page.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, Google has done a lot to integrate AI into the normal search experience — but the company has mostly approached it the same way as product listings or reviews: by adding a box to the results page. But OpenAI’s kind of engaged back-and-forth is beyond anything you can get on Chrome, and given its profoundly different approach, it’s not something that can be easily copied. If OpenAI’s search interface proves popular, it could be a serious threat to Google’s dominance.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there’s the advertising question. OpenAI doesn’t serve advertising at the moment, but it has been careful not to rule it out. The company has also been listing a lot of adtech jobs lately, fueling speculation that an ad pivot might be on the way. With Atlas, ChatGPT can now collect context directly from a user’s browser window — providing a lot of extremely valuable data for ad targeting. It’s an unprecedented level of direct browser access: literally looking at the words on your screen as you type them. And after decades of privacy scares, it’s not the kind of sensitive information that users are likely to give to Google or Meta.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s still early days for Atlas and a lot will depend on the product itself — and whether users really want what OpenAI is offering here. But the company has plotted a surprisingly commercial path here, one focused on user and revenue growth rather than hazy ambitions around AGI. As infrastructure wonks ponder the $300 billion question of whether OpenAI’s revenues can ever live up to its enormous data center buildout, products like Atlas may be the first place to look for an answer.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-21-at-5.13.11-PM.jpg?resize=1200,673" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Today, OpenAI launched its new Atlas web browser in a surprise livestream.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;The show started with CEO Sam Altman himself, speaking directly to the audience.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think AI represents a rare, once-a-decade opportunity to rethink what a browser can be,” Altman said. “In the same way that, for the previous way people used the internet, the URL bar and the search box were a great analogue, what we’re starting to see is that the chat experience and the web browser can be a quick analogue.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It was an inspiring note, in the classic Steve Jobs mode. But even more important than Altman’s browser was the detritus he was sweeping aside to make room. It wasn’t just casting present-day browsers as old, but part of a whole package of goods that are about to be replaced by AI — as Altman put it, part of “the previous way people used the internet.” And most of those soon-to-be obsolete services trace back to a single company: Google.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s browser project has been an open secret in Silicon Valley since at least this summer — and it was clear from the beginning that it would be a potential threat to Google, current owner of the world’s most popular browser. But Tuesday’s product and presentation details made it clear exactly how much the web giant has to lose in the AI era — and how little the Google’s success with Gemini seems to have helped.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The immediate threat is simple enough: ChatGPT draws 800 million users a week, and if those users switch to Atlas, they’re most likely switching away from Chrome. Losing those users doesn’t have an immediate dollar cost for Google (it’s a free product, after all) but it limits Google’s ability to target ads to those users or nudge them to Google Search — a particular sore point because, just last month, Google was barred by the US Department of Justice from making any search exclusivity deals.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then, there’s how OpenAI deals&lt;strong&gt; &lt;/strong&gt;with search itself. AI has already strained the search model of the web, surfacing processed information instead of content that can be advertised against. But on OpenAI’s livestream, Atlas head of engineering Ben Goodger (himself a central figure in developing both Firefox and Chrome) described the new kind of chat-oriented search as a paradigm shift.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This new model of search is really powerful,” Goodger said. “It’s a multi-turn experience. You can have this back-and-forth with your search results instead of just being sent off to a web page.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, Google has done a lot to integrate AI into the normal search experience — but the company has mostly approached it the same way as product listings or reviews: by adding a box to the results page. But OpenAI’s kind of engaged back-and-forth is beyond anything you can get on Chrome, and given its profoundly different approach, it’s not something that can be easily copied. If OpenAI’s search interface proves popular, it could be a serious threat to Google’s dominance.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there’s the advertising question. OpenAI doesn’t serve advertising at the moment, but it has been careful not to rule it out. The company has also been listing a lot of adtech jobs lately, fueling speculation that an ad pivot might be on the way. With Atlas, ChatGPT can now collect context directly from a user’s browser window — providing a lot of extremely valuable data for ad targeting. It’s an unprecedented level of direct browser access: literally looking at the words on your screen as you type them. And after decades of privacy scares, it’s not the kind of sensitive information that users are likely to give to Google or Meta.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s still early days for Atlas and a lot will depend on the product itself — and whether users really want what OpenAI is offering here. But the company has plotted a surprisingly commercial path here, one focused on user and revenue growth rather than hazy ambitions around AGI. As infrastructure wonks ponder the $300 billion question of whether OpenAI’s revenues can ever live up to its enormous data center buildout, products like Atlas may be the first place to look for an answer.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/openais-new-browser-is-a-broadside-shot-at-google/</guid><pubDate>Tue, 21 Oct 2025 21:17:50 +0000</pubDate></item><item><title>[NEW] Sesame, the conversational AI startup from Oculus founders, raises $250M and launches beta (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/sesame-the-conversational-ai-startup-from-oculus-founders-raises-250m-and-launches-beta/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sesame, a conversational AI startup and smart glasses maker, has raised a $250 million Series B round and is opening up its beta to a select group of testers, the company announced Tuesday. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup, headed by former Oculus co-founder and CEO Brendan Iribe and Ankit Kumar, former CTO of AR startup Ubiquity6, is working to create a personal AI agent that interacts with users using a natural-sounding human voice. The company plans to embed the personal AI agent into lightweight eyewear designed to be worn throughout the day and which users can interact with via voice.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup first emerged from stealth in February, offering two demos of its technology — AI voices named “Maya” and “Miles.” The voices were soon accessed by more than a million people within the first few weeks, who generated more than five million minutes of conversation, according to a new post by Sesame investor Sequoia about its participation in the startup’s Series B.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[T]he experience was unlike anything we’d used before. Sesame’s conversational layer felt different,” the post states. “It doesn’t just translate LLM output into audio—it generates speech directly, capturing the rhythm, emotion, and expressiveness of real dialogue.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early reviews of the tech demo seem to agree, as one report by The Verge described Sesame as “genuinely fun” and “natural-sounding.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sesame says its upcoming glasses will offer “high-quality audio” and access to an AI companion that will “observe the world alongside you.” &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060427" height="453" src="https://techcrunch.com/wp-content/uploads/2025/10/sesame-glasses.webp?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Sequoia also noted the smartglasses Sesame is building will be fashion-forward, so they look like something you’d choose to wear even if they didn’t offer built-in AI technology. A timeframe for their availability is not yet being shared; as Sequoia noted, “hardware takes time.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On that front, Sesame may have an advantage. Its founding team also includes Oculus co-founder Nate Mitchell as its Chief Product Officer, former Oculus COO and Fitbit exec Hans Hartmann as COO, as well as former Oculus engineer manager and Reality Labs engineering director Ryan Brown, and longtime Facebook and Meta exec Angela Gayles. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to sharing the news of its Series B, Iribe announced on X that Sesame is now opening an early beta of the Sesame iOS app. The app experience will allow testers to get hands-on with the AI technology being built, as the app will have the ability to “search, text and think,” he says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beta testers are asked to keep their testing experiences confidential for the time being, which includes not discussing features or results beyond the official beta test forums.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Investors in the Sesame Series B include Sequoia, Spark, and other undisclosed backers, according to Iribe.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sesame, a conversational AI startup and smart glasses maker, has raised a $250 million Series B round and is opening up its beta to a select group of testers, the company announced Tuesday. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup, headed by former Oculus co-founder and CEO Brendan Iribe and Ankit Kumar, former CTO of AR startup Ubiquity6, is working to create a personal AI agent that interacts with users using a natural-sounding human voice. The company plans to embed the personal AI agent into lightweight eyewear designed to be worn throughout the day and which users can interact with via voice.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup first emerged from stealth in February, offering two demos of its technology — AI voices named “Maya” and “Miles.” The voices were soon accessed by more than a million people within the first few weeks, who generated more than five million minutes of conversation, according to a new post by Sesame investor Sequoia about its participation in the startup’s Series B.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[T]he experience was unlike anything we’d used before. Sesame’s conversational layer felt different,” the post states. “It doesn’t just translate LLM output into audio—it generates speech directly, capturing the rhythm, emotion, and expressiveness of real dialogue.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early reviews of the tech demo seem to agree, as one report by The Verge described Sesame as “genuinely fun” and “natural-sounding.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sesame says its upcoming glasses will offer “high-quality audio” and access to an AI companion that will “observe the world alongside you.” &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3060427" height="453" src="https://techcrunch.com/wp-content/uploads/2025/10/sesame-glasses.webp?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Sequoia also noted the smartglasses Sesame is building will be fashion-forward, so they look like something you’d choose to wear even if they didn’t offer built-in AI technology. A timeframe for their availability is not yet being shared; as Sequoia noted, “hardware takes time.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On that front, Sesame may have an advantage. Its founding team also includes Oculus co-founder Nate Mitchell as its Chief Product Officer, former Oculus COO and Fitbit exec Hans Hartmann as COO, as well as former Oculus engineer manager and Reality Labs engineering director Ryan Brown, and longtime Facebook and Meta exec Angela Gayles. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to sharing the news of its Series B, Iribe announced on X that Sesame is now opening an early beta of the Sesame iOS app. The app experience will allow testers to get hands-on with the AI technology being built, as the app will have the ability to “search, text and think,” he says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beta testers are asked to keep their testing experiences confidential for the time being, which includes not discussing features or results beyond the official beta test forums.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Investors in the Sesame Series B include Sequoia, Spark, and other undisclosed backers, according to Iribe.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/sesame-the-conversational-ai-startup-from-oculus-founders-raises-250m-and-launches-beta/</guid><pubDate>Tue, 21 Oct 2025 21:34:17 +0000</pubDate></item><item><title>[NEW] Open source agentic startup LangChain hits $1.25B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/open-source-agentic-startup-langchain-hits-1-25b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/unicorns-evergreen_720.jpg?w=720" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;LangChain raised $125 million at a $1.25 billion valuation, the company announced on Monday. TechCrunch reported in July that the provider of a popular open source framework for building AI agents was raising fresh funds at a valuation of at least $1 billion. The deal was led by IVP, as we previously reported. New investors CapitalG and Sapphire Ventures joined in, as did existing investors Sequoia, Benchmark, and Amplify.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LangChain began in 2022 as an open source project founded by machine learning engineer Harrison Chase. The startup was an early darling of the AI era, solving problems that made building apps with early-stage LLMs difficult, such as searching the web, calling APIs, and interacting with databases. It became a smash hit project, and Chase launched a startup with a&amp;nbsp;$10 million seed round&amp;nbsp;from Benchmark in April 2023. A week later, Chase raised a $25 million Series A led by Sequoia, reportedly valuing LangChain at&amp;nbsp;$200 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As state-of-the-art model makers have added more infrastructure, LangChain has evolved to become a platform for building agents. In addition to announcing its unicorn status, the company launched updates to all of its major products, including its agent builder LangChain, its orchestration and context/memory tool LangGraph,&amp;nbsp;and its testing/observability tool LangSmith. LangChain remains hugely popular among open source devs, with 118,000 stars and 19.4 forks on GitHub.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/unicorns-evergreen_720.jpg?w=720" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;LangChain raised $125 million at a $1.25 billion valuation, the company announced on Monday. TechCrunch reported in July that the provider of a popular open source framework for building AI agents was raising fresh funds at a valuation of at least $1 billion. The deal was led by IVP, as we previously reported. New investors CapitalG and Sapphire Ventures joined in, as did existing investors Sequoia, Benchmark, and Amplify.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;LangChain began in 2022 as an open source project founded by machine learning engineer Harrison Chase. The startup was an early darling of the AI era, solving problems that made building apps with early-stage LLMs difficult, such as searching the web, calling APIs, and interacting with databases. It became a smash hit project, and Chase launched a startup with a&amp;nbsp;$10 million seed round&amp;nbsp;from Benchmark in April 2023. A week later, Chase raised a $25 million Series A led by Sequoia, reportedly valuing LangChain at&amp;nbsp;$200 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As state-of-the-art model makers have added more infrastructure, LangChain has evolved to become a platform for building agents. In addition to announcing its unicorn status, the company launched updates to all of its major products, including its agent builder LangChain, its orchestration and context/memory tool LangGraph,&amp;nbsp;and its testing/observability tool LangSmith. LangChain remains hugely popular among open source devs, with 118,000 stars and 19.4 forks on GitHub.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/open-source-agentic-startup-langchain-hits-1-25b-valuation/</guid><pubDate>Tue, 21 Oct 2025 22:12:46 +0000</pubDate></item><item><title>[NEW] Netflix goes ‘all in’ on generative AI as entertainment industry remains divided (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/21/netflix-goes-all-in-on-generative-ai-as-entertainment-industry-remains-divided/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/02/GettyImages-1240099721.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As the entertainment industry reckons with when and how to use generative AI in filmmaking, Netflix is leaning in. In its quarterly earnings report released on Tuesday afternoon, Netflix wrote in its letter to investors that it is “very well positioned to effectively leverage ongoing advances in AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netflix isn’t planning to use generative AI as the backbone of its content but believes the technology has potential as a tool to make creatives more efficient.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It takes a great artist to make something great,” Netflix CEO Ted Sarandos said on Tuesday’s earnings call. “AI can give creatives better tools to enhance their overall TV/movie experience for our members, but it doesn’t automatically make you a great storyteller if you’re not.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Netflix said it used generative AI in final footage for the first time in the Argentine show “The Eternaut” to create a scene of a building collapsing. Since then, the filmmakers behind “Happy Gilmore 2” used generative AI to make characters look younger in the film’s opening scene, while the producers of “Billionaires’ Bunker” used the technology as a pre-production tool to envision wardrobe and set design. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re confident that AI is going to help us and help our creative partners tell stories better, faster, and in new ways,” Sarandos said. “We’re all in on that, but we’re not chasing novelty for novelty’s sake here.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI has been a contentious topic in the entertainment industry, as artists worry that LLM-powered tools that non-consensually used their work as training data have the potential to negatively impact their jobs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Netflix as a bellwether, it seems that studios are more likely to use generative AI for special effects rather than to replace the role of actors — even if an AI actor recently caused an uproar among Hollywood actors, despite not yet booking any gigs (that we know of). These behind-the-scenes AI uses still have the potential to impact visual effects jobs, however.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;These debates recently escalated when ChatGPT-maker OpenAI unveiled its Sora 2 audio and video generation model, which was released without guardrails that prevent users from generating videos of some actors and historical figures. Just this week, the Hollywood trade organization SAG-AFTRA and actor Bryan Cranston urged OpenAI to institute stronger guardrails against deepfaking actors like Cranston himself.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When an investor asked Sarandos about the impact of Sora on Netflix, he said that it “starts to make sense” that content creators could be impacted, but he’s less worried about the movie and TV business — or so he tells investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re not worried about AI replacing creativity,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Netflix’s quarterly revenue grew 17% year-over-year to $11.5 billion, though this fell below the company’s forecast.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/02/GettyImages-1240099721.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As the entertainment industry reckons with when and how to use generative AI in filmmaking, Netflix is leaning in. In its quarterly earnings report released on Tuesday afternoon, Netflix wrote in its letter to investors that it is “very well positioned to effectively leverage ongoing advances in AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Netflix isn’t planning to use generative AI as the backbone of its content but believes the technology has potential as a tool to make creatives more efficient.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It takes a great artist to make something great,” Netflix CEO Ted Sarandos said on Tuesday’s earnings call. “AI can give creatives better tools to enhance their overall TV/movie experience for our members, but it doesn’t automatically make you a great storyteller if you’re not.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Netflix said it used generative AI in final footage for the first time in the Argentine show “The Eternaut” to create a scene of a building collapsing. Since then, the filmmakers behind “Happy Gilmore 2” used generative AI to make characters look younger in the film’s opening scene, while the producers of “Billionaires’ Bunker” used the technology as a pre-production tool to envision wardrobe and set design. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re confident that AI is going to help us and help our creative partners tell stories better, faster, and in new ways,” Sarandos said. “We’re all in on that, but we’re not chasing novelty for novelty’s sake here.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI has been a contentious topic in the entertainment industry, as artists worry that LLM-powered tools that non-consensually used their work as training data have the potential to negatively impact their jobs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Netflix as a bellwether, it seems that studios are more likely to use generative AI for special effects rather than to replace the role of actors — even if an AI actor recently caused an uproar among Hollywood actors, despite not yet booking any gigs (that we know of). These behind-the-scenes AI uses still have the potential to impact visual effects jobs, however.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;These debates recently escalated when ChatGPT-maker OpenAI unveiled its Sora 2 audio and video generation model, which was released without guardrails that prevent users from generating videos of some actors and historical figures. Just this week, the Hollywood trade organization SAG-AFTRA and actor Bryan Cranston urged OpenAI to institute stronger guardrails against deepfaking actors like Cranston himself.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When an investor asked Sarandos about the impact of Sora on Netflix, he said that it “starts to make sense” that content creators could be impacted, but he’s less worried about the movie and TV business — or so he tells investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re not worried about AI replacing creativity,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Netflix’s quarterly revenue grew 17% year-over-year to $11.5 billion, though this fell below the company’s forecast.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/21/netflix-goes-all-in-on-generative-ai-as-entertainment-industry-remains-divided/</guid><pubDate>Tue, 21 Oct 2025 22:21:46 +0000</pubDate></item></channel></rss>