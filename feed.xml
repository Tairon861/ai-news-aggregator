<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 19 Feb 2026 07:03:22 +0000</lastBuildDate><item><title>Google Cloud’s VP for startups on reading your ‘check engine light’ before it’s too late (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/google-clouds-vp-for-startups-on-reading-your-check-engine-light-before-its-too-late/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Darren-Mowry-headshot.png?resize=1200,960" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond free credits and into real cloud bills.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan caught up with&amp;nbsp;Darren Mowry, Google Cloud’s vice president of global startups who is right at the center of those tradeoffs. Together, they discuss what Mowry’s seeing across the startup ecosystem, how Google Cloud is competing for AI startups, and what founders should be thinking about as they scale.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How Google positions against AWS and Microsoft in the AI startup&amp;nbsp;race.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;TPUs vs GPUs: How much does hardware choice matter for early-stage companies?&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Which AI verticals are seeing real growth, and&amp;nbsp;what’s&amp;nbsp;standing out in biotech, climate tech, developer tools, and world models.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What red flags will signal that a startup&amp;nbsp;isn’t&amp;nbsp;going to make it.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Darren-Mowry-headshot.png?resize=1200,960" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond free credits and into real cloud bills.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan caught up with&amp;nbsp;Darren Mowry, Google Cloud’s vice president of global startups who is right at the center of those tradeoffs. Together, they discuss what Mowry’s seeing across the startup ecosystem, how Google Cloud is competing for AI startups, and what founders should be thinking about as they scale.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How Google positions against AWS and Microsoft in the AI startup&amp;nbsp;race.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;TPUs vs GPUs: How much does hardware choice matter for early-stage companies?&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Which AI verticals are seeing real growth, and&amp;nbsp;what’s&amp;nbsp;standing out in biotech, climate tech, developer tools, and world models.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What red flags will signal that a startup&amp;nbsp;isn’t&amp;nbsp;going to make it.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/google-clouds-vp-for-startups-on-reading-your-check-engine-light-before-its-too-late/</guid><pubDate>Wed, 18 Feb 2026 20:22:29 +0000</pubDate></item><item><title>Is your startup’s check engine light on? Google Cloud’s VP explains what to do (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/is-your-startups-check-engine-light-on-google-clouds-vp-explains-what-to-do/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/54885194602_44d8e5385f_o.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30941231"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond free credits and into real cloud bills.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan caught up with&amp;nbsp;Darren Mowry, Google Cloud’s vice president of global startups who is right at the center of those tradeoffs. Watch as they discuss what Mowry’s seeing across the startup ecosystem, how Google Cloud is competing for AI startups, and what founders should be thinking about as they scale.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/54885194602_44d8e5385f_o.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30941231"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond free credits and into real cloud bills.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan caught up with&amp;nbsp;Darren Mowry, Google Cloud’s vice president of global startups who is right at the center of those tradeoffs. Watch as they discuss what Mowry’s seeing across the startup ecosystem, how Google Cloud is competing for AI startups, and what founders should be thinking about as they scale.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/is-your-startups-check-engine-light-on-google-clouds-vp-explains-what-to-do/</guid><pubDate>Wed, 18 Feb 2026 21:07:00 +0000</pubDate></item><item><title>The Reasonable Effectiveness of Virtue Ethics in AI Alignment (The Gradient)</title><link>https://thegradient.pub/virtue-ethics-ai-alignment/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://thegradient.pub/content/images/2026/02/ramelli-featured.jpg" /&gt;&lt;/div&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="preface"&gt;Preface&lt;/h2&gt;
&lt;p&gt;This essay argues that rational people don’t have goals, and that rational AIs shouldn’t have goals. Human actions are rational not because we direct them at some final ‘goals,’ but because we align actions to &lt;em&gt;practices&lt;/em&gt;: networks of actions, action-dispositions, action-evaluation criteria, and action-resources that structure, clarify, develop, and promote themselves. If we want AIs that can genuinely support, collaborate with, or even &lt;strong&gt;comply&lt;/strong&gt; with human agency, AI agents’ deliberations must share a “type signature” with the practices-based logic we use to reflect and act.&lt;/p&gt;
&lt;p&gt;I argue that these issues matter not just for aligning AI to grand ethical ideals like human flourishing, but also for aligning AI to core safety-properties like transparency, helpfulness, harmlessness, or corrigibility. Concepts like ’harmlessness’ or ‘corrigibility’ are unnatural -- brittle, unstable, arbitrary -- for agents who’d interpret them in terms of goals or rules, but natural for agents who’d interpret them as dynamics in networks of actions, action-dispositions, action-evaluation criteria, and action-resources.&lt;/p&gt;
&lt;p&gt;While the issues this essay tackles tend to sprawl, one theme that reappears over and over is the relevance of the formula ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly.’ I argue that this formula captures something important about both meaningful human life-activity (art is the artistic promotion of art, romance is the romantic promotion of romance) and real human morality (to care about kindness is to promote kindness kindly, to care about honesty is to promote honesty honestly).&lt;/p&gt;
&lt;p&gt;I start by asking: What follows for AI alignment if we take the concept of eudaimonia -- active, rational human flourishing -- seriously? I argue that the concept of eudaimonia doesn’t simply point to a desired state or trajectory of the world that we should set as an AI’s optimization target, but rather points to a structure of deliberation different from standard consequentialist rationality. I then argue that this form of rational activity and valuing, which l call &lt;em&gt;eudaimonic rationality&lt;/em&gt;, is a useful or even necessary framework for the agency and values of human-aligned AIs.&lt;/p&gt;
&lt;p&gt;These arguments are based both on the dangers of a “type mismatch” between human flourishing as an optimization target and consequentialist optimization as a form, and on certain material advantages that eudaimonic rationality plausibly possesses in comparison to deontological and consequentialist agency with regard to stability and safety.&lt;/p&gt;
&lt;p&gt;The concept of eudaimonia, I argue, suggests a form of rational activity without a strict distinction between means and ends, or between ‘instrumental’ and ‘terminal’ values. In this model of rational activity, a rational action is an element of a valued practice in roughly the same sense that a note is an element of a melody, a time-step is an element of a computation, and a moment in an organism’s cellular life is an element of that organism’s self-subsistence and self-development.&lt;/p&gt;
&lt;p&gt;My central claim is that our intuitions about the nature of human flourishing are implicitly intuitions that eudaimonic rationality can be functionally robust in a sense highly critical to AI alignment. More specifically, I argue that in light of our best intuitions about the nature of human flourishing it’s plausible that eudaimonic rationality is a &lt;em&gt;natural&lt;/em&gt; form of agency, and that eudaimonic rationality is &lt;em&gt;effective&lt;/em&gt; even by the light of certain consequentialist approximations of its values. I then argue that if our goal is to align AI in support of human flourishing, and if it is furthermore plausible that eudaimonic rationality is natural and efficacious, then many classical AI safety considerations and ‘paradoxes’ of AI alignment speak in favor of trying to instill AIs with eudaimonic rationality.&lt;/p&gt;
&lt;p&gt;Throughout this essay, I will sometimes explicitly and often implicitly be asking whether some form of agency or rationality or practice is &lt;em&gt;natural&lt;/em&gt;. The sense of ‘natural’ I’m calling on is certainly related to the senses used in various virtue-ethical traditions, but the interest I take in it is less immediately normative and more material or technical. While I have no reductive definition at hand, the intended meaning of ‘natural’ is related to stability, coherence, relative non-contingency, ease of learnability, lower algorithmic complexity, convergent cultural evolution, hypothetical convergent cultural evolution across different hypothetical rational-animal species, potential convergent evolution between humans and neural-network based AI, and targetability by ML training processes. While I will also make many direct references to AI alignment, this question of material naturalness is where the real alignment-critical action takes place: if we learn that certain exotic-sounding forms of agency, rationality, or practice are both themselves natural and make the contents of our all-too-human values natural in turn, then we have learned about good, relatively safe, and relatively easy targets for AI alignment.&lt;/p&gt;
&lt;p&gt;Readers may find the following section-by-section overview useful for navigating the essay:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part &lt;em&gt;I&lt;/em&gt; presents a class of cases of rational deliberation that are very different from the Effective Altruism-style optimization many in the AI-alignment world treat as the paradigm of rational deliberation. I call this class of rational deliberations 'eudaimonic rationality,' and identify it with the form of rationality that guides a mathematician or an artist or a friend when they reflect on what to do in mathematics or in art or in friendship.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;II&lt;/em&gt; looks at the case of research mathematics (via an account by Terry Tao) as an example of eudaimonic rationality at work. What does a mathematician try to do in math? I say she tries to be &lt;em&gt;mathematically excellent&lt;/em&gt;, which involves promoting mathematical excellence through mathematical excellence, and that this structure is closely related to why 'mathematical excellence' can even be a concept.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;III&lt;/em&gt; argues that for eudaimonic agents such as a mathematician who is trying to do excellent mathematics, distinctions between ‘instrumental goods’ and ‘terminal goods’ (intrinsic goods) are mostly unnatural. This makes reflection about values go very differently for a eudaimonic agent than for an Effective Altruism-style agent. Instead of looking to reduce a network of causally intertwined apparent values to a minimal base of intrinsic values that “explains away” the rest as instrumental, a eudaimonic agent looks for organism-like causal coherence in a network of apparent values.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IV&lt;/em&gt; cashes out the essay’s central concepts: A &lt;em&gt;eudaimonic practice&lt;/em&gt; is a network of actions, action-dispositions, action-evaluation criteria, and action-resources where high-scoring actions reliably (but defeasibly) causally promote future high-scoring actions. &lt;em&gt;Eudaimonic rationality&lt;/em&gt; is a class of reflective equilibration and deliberation processes that assume an underlying eudaimonic practice and seek to optimize aggregate action-scores specifically via high-scoring action.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;V&lt;/em&gt;, I argue that many puzzles and ‘paradoxes’ about AI alignment are driven by the assumption that mature AI agents will be Effective Altruism-style optimizers. A “type mismatch” between Effective Altruism-style optimization and eudaimonic rationality makes it nearly impossible to translate the interests of humans -- agents who practice eudaimonic rationality -- into a utility function legible to an Effective Altruism-style optimizer AI. But this does not mean that our values are inherently brittle, unnatural, or wildly contingent: while Effective Altruism-style optimizers may well be a natural type of agent, eudaimonic agents (whether biological or AI) are highly natural as well.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;VI&lt;/em&gt;, I ask whether a eudaimonically rational AI agent devoted to a practice like mathematical research would be safe by default. I argue that a practice like mathematical research plausibly has natural boundaries that exclude moves like ‘take over planet to get more compute for mathematical research,’ but the issue is nuanced. I propose that a practice’s boundaries (for which there may be multiple good natural candidates) may be most stable when a practice is paired with a support practice: a complementary practice for dealing with practice-external issues of maintenance and resource-gathering.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VII&lt;/em&gt; develops the idea of ‘support practices’: eudaimonically rational ways to support eudaimonic practices. We famously want AI agents to help humans lead flourishing lives, but how can we define the purview of this ‘help’? I argue that many core human practices have natural support-practices with a derived eudaimonic structure: the work of good couples’ therapist, for instance, is intertwined with but clearly distinct from a couple’s relationship-practice. Still, there remains a problem: a support-practice AI might harm other people and practices to help the people or practice it’s supporting.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VIII&lt;/em&gt; moves from eudaimonic rationality in general to eudaimonically rational morality. I argue that thinking of moral virtues as domain-general, always-on practices solves key AI-alignment-flavored problems with consequentialist and deontological moralities. The core idea is that the conditions for e.g. ‘kindness’ being a robust moral virtue are akin to the conditions for ‘mathematical excellence’ being a meaningful concept: it must be generally viable to promote kindness in yourself and others kindly. It’s this structure, I argue, that gives moral virtues material standing in a ‘fitness landscape’ riven by pressures from neural-network generalization dynamics, reinforcement-learning cycles, and social and natural selection.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IX&lt;/em&gt; argues that eudaimonic agents have some unique forms of robustness to RL-like and Darwinian-like dynamics that tend to mutate the values of EA-style optimizers. In particular, eudaimonic agents should be very robust to the risk of developing rogue subroutines (sometimes called ‘the inner alignment problem’).&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;X&lt;/em&gt; I discuss canonical AI-safety desiderata like transparency, corrigibility, and (more abstractly) niceness. I argue that treating these properties as moral virtues in my sense -- domain-general, always-on eudaimonic practices --  dissolves problems and paradoxes that arise when treating them as goals, as rules, or even as character traits. I end with an appendix on some prospects for RL regimes geared towards eudaimonic rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="i-rational-action-in-the-good-life"&gt;I. Rational Action in the Good Life&lt;/h2&gt;
&lt;p&gt;I start with a consideration of the nature of the good we hope AI alignment can promote. With the exception of hedonistic utilitarians, most actors interested in AI alignment understand our goal as a future brimming with human (and other sapient-being) flourishing: persons living good lives and forming good communities. What I believe many fail to reflect on, however, is that on any plausible conception human flourishing involves a kind of rational activity. Subjects engaged in human flourishing act in intelligible ways subject to reason, reflection, and revision, and this form of rational care and purposefulness is itself part of the constitution of our flourishing. I believe this characterization of human flourishing is relatively uncontroversial upon reflection, but it raises a kind of puzzle if we’re used to thinking of rationality in consequentialist (or consequentialist-with-deontological-constraints) terms: just what goal is the rational agency involved in human-flourishing activity directed towards?&lt;/p&gt;
&lt;p&gt;One obvious answer would be that, like all properly aligned rationality, the rational agency involved in human-flourishing activities is geared towards maximizing human (and other sapient) flourishing. But we should quickly find ourselves confused about the right way to describe the contribution that rational agency in human-flourishing activities makes to human flourishing. It seems neither appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing only by enacting rationality (by selecting actions that are intrinsically valuable when rationally selected), nor appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing just instrumentally (by selecting actions that causally promote human flourishing).&lt;/p&gt;
&lt;p&gt;The first option reduces our rational actions to something ritualistic, even as the good life surely involves mathematicians working to advance mathematics, friends speaking heart-to-heart to deepen intimacies, gymnasts practicing flips to get better at flips, and novelists revising chapters to improve their manuscripts. The second option threatens to make the good in the good life just impossible to find -- if speaking heart-to-heart is not the good of friendship, and working on math is the not the good of mathematics, then what is?&lt;/p&gt;
&lt;p&gt;This essay argues that deliberative reasoning about the good life is neither directed towards goals external to rational action nor directed towards rational action as an independent good, but towards acts of excellent participation in a valued open-ended process. I then go on to argue that the ‘eudaimonic’ structure of deliberation salient in cases like math or friendship (sloganized as ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) is also subtly critical in more worldly, strategic, or morally high-stakes contexts, and constitutes a major organizing principle of human action and deliberation.&lt;/p&gt;
&lt;h2 id="ii-what-is-a-practice"&gt;II. What Is a Practice?&lt;/h2&gt;
&lt;p&gt;Since ‘human flourishing’ can seem mysterious and abstract, let’s focus on some concrete eudaimonic practices. Consider practices like math, art, craft, friendship, athletics, romance, play, and technology, which are among our best-understood candidates for partial answers to the question ‘what would flourishing people in a flourishing community be doing.’ From a consequentialist point of view, these practices are all marked by extreme ambiguity -- and I would argue indeterminacy -- about what’s instrumental and what’s terminal in their guiding ideas of value. Here, for example, is Terry Tao’s account of goodness in mathematics:&lt;/p&gt;
&lt;p&gt;‘The very best examples of good mathematics do not merely fulfil one or more of the criteria of mathematical quality listed at the beginning of this article, but are more importantly part of a greater mathematical story, which then unfurls to generate many further pieces of good mathematics of many different types. Indeed, one can view the history of entire fields of mathematics as being primarily generated by a handful of these great stories, their evolution through time, and their interaction with each other. I would thus conclude that good mathematics [...] also depends on the more “global” question of how it fits in with other pieces of good mathematics, either by building upon earlier achievements or encouraging the development of future breakthroughs. [There seems] to be some undefinable sense that a certain piece of mathematics is “on to something”, that it is a piece of a larger puzzle waiting to be explored further.’&lt;/p&gt;
&lt;p&gt;It may be possible to give some post-hoc decomposition of Tao’s account into two logically distinct components -- a description of a utility-function over mathematical achievements and an empirical theory about causal relations between mathematical achievements -- but I believe this would be artificial and misleading. On a more natural reading, Tao is describing some of the conditions that make good mathematical practice a eudaimonic practice: In a mathematical practice guided by a cultivated mathematical practical-wisdom judgment (Tao’s ‘undefinable sense that a certain piece of mathematics is “on to something”’), present excellent performance by the standard of the practical-wisdom judgment reliably develops the conditions for future excellent performance by the standard of the mathematical practical-wisdom judgment, as well as cultivating our practical and theoretical grasp of the standard itself.&lt;/p&gt;
&lt;p&gt;This is not to suggest that ‘good mathematics causes future good mathematics’ is a full definition or even full informal description of good mathematics. My claim is only that the fact that good mathematics has a disposition to cause future good mathematics reveals something essential about our concept of good mathematics (and about the material affordances enabling this concept). By analogy, consider the respective concepts healthy tiger and healthy human: It's essential to the concept of a healthy tiger that &lt;em&gt;x&lt;/em&gt; being a healthy tiger now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy tiger 5 minutes in the future (since a healthy tiger body self-maintains and enables self-preservation tiger-behaviours), and essential to the concept of a healthy human that &lt;em&gt;x&lt;/em&gt; being a healthy human now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy human 5 minutes in the future (since a healthy human body self-maintains and enables self-preservation human behaviours). But these formulae aren't yet complete descriptions of 'healthy tiger' or 'healthy human,' as evidenced by the fact that we can tell apart a healthy tiger from a healthy human.&lt;/p&gt;
&lt;p&gt;Crucially, the mathematical practical-wisdom described by Tao is not entirely conceptually opaque beyond its basic characterization as a self-cultivating criterion for self-cultivating excellence in mathematical activity. Mathematical flourishing can partly be described as involving the instantiation of a relation (a mathematical-practice relation of ‘developmental connectedness’) among instantiations of relatively individually definable and quantifiable instances of mathematical value such as elegant proofs, clear expositions, strong theorems, cogent definitions and so on. Furthermore, this relation of developmental connectedness is partly defined by its reliable tendency to causally propagate instances of more individually and locally measurable mathematical value (instances of elegant proofs, clear exposition, strong theorems, cogent definitions and so on):&lt;/p&gt;
&lt;p&gt;[I believe] that good mathematics is more than simply the process of solving problems, building theories, and making arguments shorter, stronger, clearer, more elegant, or more rigorous, though these are of course all admirable goals; while achieving all of these tasks (and debating which ones should have higher priority within any given field), we should also be aware of any possible larger context that one’s results could be placed in, as this may well lead to the greatest long-term benefit for the result, for the field, and for mathematics as a whole.&lt;/p&gt;
&lt;p&gt;One could, again, try to interpret this causal relationship between excellence according to Tao’s ‘organicist’ (or ‘narrative’ or ‘developmental’) sense of good mathematics and the reliable propagation of narrow instances of good mathematics as evidence of a means-ends rational relation, where additive maximization of narrow instances of mathematical value is the utility function and ‘organicist’ mathematical insight is the means. For Tao, however, the evidential import of this causal relationship goes exactly the other way -- it suggests a unification of our myriad more-explicit and more-standalone conceptions of mathematical excellence into a more-ineffable but more-complete conception. As Tao says:&lt;/p&gt;
&lt;p&gt;It may seem from the above discussion that the problem of evaluating mathematical quality, while important, is a hopelessly complicated one, especially since many good mathematical achievements may score highly on some of the qualities listed above but not on others [...] However, there is the remarkable phenomenon that good mathematics in one of the above senses tends to beget more good mathematics in many of the other senses as well, leading to the tentative conjecture that perhaps there is, after all, a universal notion of good quality mathematics, and all the specific metrics listed above represent different routes to uncover new mathematics, or different stages or aspects of the evolution of a mathematical story.&lt;/p&gt;
&lt;h2 id="iii-inverting-consequentialist-reflection"&gt;III. Inverting Consequentialist Reflection&lt;/h2&gt;
&lt;p&gt;Tao’s reasoning about local and global mathematical values exemplifies a central difference between consequentialist rationality and eudaimonic rationality, now taken as paradigms not only for selecting actions but for reflecting on values. (Paradigms for what philosophers will sometimes call ‘reflective equilibration.’) Within the paradigm of consequentialist rationality, if excellence in accordance with a holistic, difficult-to-judge apparent value (say ‘freedom’) is reliably a powerful causal promoter of excellence in accordance with more explicit, more standalone apparent values (say ‘material comfort,’ ‘psychological health,’ ‘lifespan’), this relationship functions as evidence against the status of the holistic prima-facie value as a constitutive -- as opposed to instrumental -- value. Within the paradigm of eudaimonic rationality, by contrast, this same relationship functions as evidence for the status of the holistic prima-facie value as a constitutive value.&lt;/p&gt;
&lt;p&gt;For a (typical) consequentialist-rationality reflection process, evidence that the excellence of a whole causally contributes to the excellences of its parts explains away our investment in the excellence of the whole. The “coincidence” that the intrinsically valuable whole is also instrumentally valuable for its parts is taken to suggest a kind of double-counting error --  one we “fix” by concluding that the whole has no constitutive value but valuing the whole is an effective heuristic under normal circumstances. A eudaimonic-rationality reflective equilibration, by contrast, treats instrumental causal connections between excellences as evidence that our notions of excellence are picking out something appropriately ‘substantive.’&lt;br /&gt;For eudaimonic-rationality reflective equilibration, it is the discovery of causal and common-cause relations among excellences that ratifies our initial sense that caring about these excellences is eudaimonically rational. The discovery of these causal connections functions as evidence that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The ‘local’ excellences we care about are resonant or fruitful, in that they causally promote each other and the holistic excellences in which they participate.&lt;/li&gt;
&lt;li&gt;The ‘holistic’ excellences we care about are materially efficacious and robust, in that they causally promote both the more local excellences that participate in them and their own continuation as future holistic excellence.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my view this is the right way to treat causal connections between (apparent) values if we’re hoping to capture actual human values-reflection, and points to an important strength of the eudaimonic rationality paradigm: Eudaimonic rationality dissolves the ‘paradox’ that in real-life arguments about the value of various human enterprises (e.g. the value of branches of science, branches of art, branches of sport), judgments of intrinsic value typically seek succor from some kind of claim to instrumental value. For example, a defense of the importance of research in quantum physics will often appeal to the wonderful technological, mathematical, and special-sciences applications quantum physics gave us, without meaning to reduce the worth of quantum physics to these applications. On my reading, these appeals aren't just additive -- 'aside from the intrinsic value there is also instrumental value' -- but presentations of evidence that research in quantum physics is a resonant part of a flourishing organic whole (e.g. the civilizational whole of ‘modern science and technology’).&lt;/p&gt;
&lt;p&gt;I believe that without 'organicism' of the kind described above, one faces a serious dilemma whenever one argues for the intrinsic worth of a pursuit or norm: either we stress the value's independence from all benefits and applications and make the claim of value dogmatic and irrelevant, or else we invite an instrumentalist reduction that ‘explains away’ the appearance of intrinsic value. Indeed, I’d argue that organicism of this kind is even necessary to make sense of caring about rationality (including truth, knowledge, non-contradiction and so on) non-instrumentally at all: the ‘paradox’ of rationality as a substantive value is that the typical usefulness of rationality suggests an error-theory about its apparent intrinsic value, since it’s a strange coincidence that rationality is both so typically useful and so intrinsically good. On an organicist account, however, we expect that major forms of excellence endemic to human life -- thought, understanding, knowledge, reasoned action -- both typically promote each other and typically promote our material flourishing and causal leverage on the world.&lt;/p&gt;
&lt;h2 id="iv-the-material-efficacy-condition"&gt;IV. The Material Efficacy Condition&lt;/h2&gt;
&lt;p&gt;Returning now to Tao’s account of good mathematics, let’s take final stock of our interpretation. I argue that mathematical excellence (the property marking ‘the very best examples of good mathematics’) according to Tao satisfies the following conditions, which I believe Tao intends as necessary but not sufficient:&lt;/p&gt;
&lt;p&gt;A) Mathematical excellence is a property of mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;B) An excellent mathematical-activity instance performed today is excellent partly by virtue of satisfying the mathematical-practice relation ‘builds on’ with regard to past excellent mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;C) An excellent mathematical-activity instance performed today is excellent partly by virtue of having a reliable causal tendency to bring about future excellent mathematical-activity instances that satisfy the mathematical-practice relation ‘builds on’ with regard to it.&lt;/p&gt;
&lt;p&gt;D) Instantiation of more local, more individually measurable criteria of mathematical-activity goodness such as elegant proofs, clear expositions, and strong theorems is a typical correlate of mathematical excellence.&lt;/p&gt;
&lt;p&gt;E) At a given moment in a given mathematical field, the instantiation of mathematical excellence will be predictably better-correlated with the instantiation of certain local criteria of mathematical-activity goodness than with others.&lt;/p&gt;
&lt;p&gt;Should we take these traits to collectively describe something more like a decision-procedure called ‘mathematical excellence’ that mathematicians should try to follow, or something more like an event called ‘mathematical excellence’ whose aggregate future-occurrences mathematicians should aspire to maximize? My contention is that Tao’s account is inherently ambiguous, and for a good reason: in ordinary circumstances there is no significant practical difference between doing excellent mathematics and doing instrumentally optimal mathematics with regard to maximizing future aggregate excellent mathematics. This isn’t to say that doing excellent mathematics is the instrumentally optimal action among all possible actions with regard to aggregate future excellent mathematics, but that (in ordinary circumstances) it is the instrumentally optimal choice from among mathematical actions with regard to aggregate future excellent mathematics.&lt;/p&gt;
&lt;p&gt;I propose that the rough matchup between mathematical excellence and optimal (among mathematical actions) instrumental promotion of aggregate mathematical excellence is neither an empirical miracle nor something determined ‘by definition’ in a trivializing sense. Rather, ‘mathematical excellence’ as used by Tao is a concept that has a referent only if there is a possible property &lt;em&gt;x&lt;/em&gt; that satisfies both desiderata &lt;em&gt;A&lt;/em&gt;-&lt;em&gt;E&lt;/em&gt; and the additional criterion that among mathematical actions, actions that are optimal as instantiations of &lt;em&gt;x&lt;/em&gt; are also roughly optimal for maximizing aggregate future instantiation of &lt;em&gt;x&lt;/em&gt;-ness.&lt;/p&gt;
&lt;p&gt;This is what I would describe as the material efficacy condition on eudaimonic rationality. In order for a practice to be fit for possessing internal criteria of flourishing, excellence, and eudaimonic rationality, a practice must materially allow for an (under normal circumstances) optimally self-promoting property &lt;em&gt;x&lt;/em&gt; that strongly correlates with a plethora of more local, more individually measurable properties whose instantiation is prima facie valuable. Stated more informally, there must exist a two-way causal relationship between a practice’s excellence and the material, psychological, and epistemic effects of its excellence, such that present excellence reliably materially, psychologically, and epistemically promotes future excellence.&lt;/p&gt;
&lt;h2 id="v-practices-and-optimization"&gt;V. Practices and Optimization&lt;/h2&gt;
&lt;p&gt;I earlier said that if human flourishing involves practicing eudaimonic rationality, there may well be a “type mismatch” between human flourishing and the kind of consequentialist optimization we often associate with the idea of an agenticly mature future AI. In fact, I believe that implicitly recognizing but misdiagnosing this type mismatch is at least partially responsible for MIRI-style pessimism about the probability of aligning any artificial agents to human values.&lt;/p&gt;
&lt;p&gt;On my view, the secret to relatively successful alignment among humans themselves (when there is successful alignment among humans) lies in the role attempted excellence plays as a filter on human interventions in the future trajectory of a eudaimonic practice. To the degree that humans value a given eudaimonic practice, they are committed to effecting their vision for the practice’s future-trajectory primarily by attempting acts of excellence in the present: we stake our intended effect over the practice’s future-trajectory on the self-propagating excellence of our intervention. While this ‘filter’ doesn’t necessarily stop the worst interventions from being harmful (there are forms of ‘anti-excellence’ that also have self-promotion powers), I contend that this filter is mechanically crucial for the possibility of reliably benign or positive interventions.&lt;/p&gt;
&lt;p&gt;What do I mean? Consider the difference between a world where scientists typically try to propagate (what they believe to be) the scientific truth mainly by means of submitting research work to scientific institutions, and a world where scientists typically try to propagate (what they believe to be) the scientific truth by means including propaganda, fraud, threats, bribery, and slander. As Liam Kofi Bright demonstrates in On Fraud, a community of consequentialist scientists devoted to maximizing truth will predictably match the latter model. I believe one lesson to be drawn is that humans’ ability to collaborate in the promotion of science depends on our ability to scientifically collaborate in the promotion of science, rather than throttle the future trajectory of science every-which-way  our financial and political powers based on our individual beliefs about the optimal trajectory of science.&lt;/p&gt;
&lt;p&gt;A flourishing eudaimonic practice is, above all, a natural-selection-like mechanism whose fitness-function selects among attempted acts of excellence the ones conducive to (and constitutive of) the practice’s flourishing, propagating the excellence these acts instantiate. When people committed to a eudaimonic practice make their attempted interventions into the future trajectory of the practice via acts of attempted excellence, the natural-selection-like mechanism embodied by the practice  (rather than any single individual’s theory of optimal future trajectory) is the aligned intelligence determining the practice’s future trajectory.&lt;/p&gt;
&lt;p&gt;The explanation here, again, is partly causal and partly constitutive: a practice’s “ultimate” norms of excellence, including the “ultimate” epistemic and alethic norms of a discursive practice, are partly defined by the succession of norms in the course of a practice’s development through best-efforts attempted excellence. Although this may be no deterrent to an already god-like optimizer who can simulate entire civilizational trajectories, an agent short of these capacities can best act on their vision of the optimal future-trajectory of a practice by attempting an excellent contribution to the practice.&lt;/p&gt;
&lt;p&gt;The second aspect of our type-mismatch is much more in the weeds: In my analysis so far, I discussed the overall excellence of the trajectory of a eudaimonic practice much like a consequentialist might discuss a quantity of utility. This may be taken to suggest that a ‘sophisticated consequentialist’ or ‘universal consequentialist’ could easily accommodate the implications of the so-called type mismatch by treating them as instrumental, decision-procedure level considerations against naive optimization. In fact, quantities like ‘aggregate democracy’ or ‘overall mathematical excellence’ are (on my view) practice-internal quantities that quickly lose meaning if we try to apply them outside the scope of a ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ decision-procedure.&lt;/p&gt;
&lt;p&gt;What do I mean?  Consider, for example, the practice of philosophy. Here are some questions that should arise for a consequentialist planner (including a sophisticated consequentialist planning decision-procedures or habits) who values philosophy practice-trajectories: Does rating (e.g.) Aristotle’s or Dharmakirti’s philosophical achievements as the most excellent achievements in philosophy imply that we should “tile the universe” with independent practice-trajectories designed to reproduce classical Greek or Indian philosophy? If not, is it because we should assign non-linearly greater value to longer trajectories? Or should we discount trajectories that have parallel contents? Or should we analyze the greatness of early achievements in a practice as mostly instrumental greatness but the greatness of later achievements in a practice as mostly intrinsically valuable? These are all, I believe, bad questions that have only arbitrary answers. To an agent trying to promote philosophy by doing excellent philosophical work, the bad questions above are naturally out of scope. The agent uses the concept of ‘aggregate philosophical excellence’ or ‘a philosophy practice-trajectory’s value’ only to reason about the philosophical influence of their work on the trajectory of the philosophy-practice in which it participates. Choosing an excellent action in practice requires (at most) quantitative comparison between different possible paths for a practice-trajectory, not quantitative comparison between possible worlds containing independent practice-trajectories sprinkled throughout time and space.&lt;/p&gt;
&lt;h2 id="vi-prospects-and-problems-for-ai"&gt;VI. Prospects and Problems for AI&lt;/h2&gt;
&lt;p&gt;Is this good news for AI alignment? It’s certainly good news that (if I’m right) eudaimonic practices are something like natural kinds marked by a causal structure that enables a self-developing excellence well-correlated with multiple naive local measures of quality. But does this mean we could develop a stable and safe (e.g.) ‘mathematical excellence through mathematical excellence’ AI? If we create a fully agentic AI mathematician, will it naturally abstain from trying to extend its longevity or get more resources (even for doing mathematics) other than by impressing us with excellent mathematical work?  I think that prospects are good, but not simple.&lt;/p&gt;
&lt;p&gt;I believe ‘mathematical excellence through mathematical excellence’ really can powerfully scope what mechanisms for shaping the future an AI cares to activate. An AI trained to follow ‘promote mathematics mathematically’ will only care about influencing the future by feeding  excellent mathematical work to mathematics’ excellence-propagation mechanism. But it’s harder to say whether the structure of mathematical practice also properly scopes what subactions can be taken as part of an instance of “doing math.” Is a human mathematician working on a would-be excellent proof in pen and paper practicing math when she is picking up a pen or flipping pages? When she is taking the bus to her office? When she’s buying amphetamines? And is an AI mathematician working on a would-be excellent proof practicing math when it opens a Python console? When it searches the web for new papers? When it harvests Earth for compute?&lt;/p&gt;
&lt;p&gt;I think these questions are complex, rather than nonsensical. Much like collective practices, individual practices -- for example a person’s or possibly an AI’s mathematical practice -- may possess functional organic unities that allow a meaningful distinction between internal dynamics (including dynamics of development and empowerment) and external interventions (including interventions of enhancement and provision). Still, it’s clear that eudaimonic practices do not exist in isolation, and that no practice can function without either blending with or relying on a “support practice” of some kind.&lt;/p&gt;
&lt;p&gt;How, then, do we rationally go about externally-oriented activities like building offices for mathematicians, performing elective reconstructive surgery on an athlete, or conducting couples therapy for romantic partners? And furthermore, how do we rationally go about allocating scarce resources useful for different practices, or judging whether to integrate (e.g.) performance-enhancing drugs into a practice?&lt;/p&gt;
&lt;p&gt;This is, I think, the fundamental question for AI alignment from the viewpoint of  ‘eudaimonic rationality.’ We want AI to support human eudaimonic practices -- and, if relevant, its own eudaimonic practices or participation in human eudaimonic practices -- in a eudaimonia-appropriate way. But how does the logic of eudaimonic rationality extend from eudaimonic practices to their support activities? How do we ‘eudaimonically-rationally’ do the dirty work that makes eudaimonia possible? My best answer is: carefully, kindly, respectfully, accountably, peacefully, honestly, sensitively.&lt;/p&gt;
&lt;h2 id="vii-from-support-practices-to-moral-practice"&gt;VII. From Support-Practices to Moral Practice&lt;/h2&gt;
&lt;p&gt;The theory of AI alignment, I propose, should fundamentally be a theory of the eudaimonic rationality of &lt;em&gt;support practices&lt;/em&gt;. One part of this theory should concern the ‘support’ relation itself, and analyze varieties of support practices and their appropriate relation to the self-determination of a eudaimonic practice: Support-practices such as acquiring resources for a practice, maintaining an enabling environment, coaching practitioners, conducting (physical or psychological) therapy for practitioners, devising technological enhancements for a practice, and educating the public about a practice, each have their own ‘role-morality’ vis-a-vis the practice they support. It is this part of the theory of ‘support practices’ that should, if all goes well in the theory’s construction, describe the various practice-external ways to eudaimonically-rationally act on a pro-attitude towards the aggregate excellence of the practice’s future trajectory without treating it like a quantity of utility. (Much like the concept of ‘mathematical action’ scopes the range of action-choices in such a way that decision-theoretic optimization of math’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint, so should the concepts of various types of ‘support action’ scope the range of action-choices in such a way that decision-theoretic optimization of a practice’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint when the choice of actions is scoped.)&lt;/p&gt;
&lt;p&gt;What is more difficult is delineating the appropriate relationship of a support-practice to everything &lt;em&gt;&lt;strong&gt;outside&lt;/strong&gt;&lt;/em&gt; the practice it supports. What stops a marriage-therapist AI on Mars from appropriately tending to the marriage of a Mars-dwelling couple but harvesting Earth for compute to be a better therapist-AI for that couple? While we can perhaps imagine a person or AI taking up a support-role for ‘humanity’s flourishing as whole,’ so that there’s no outside to speak of, I am not sure that the concept of practice remains natural at this level of abstraction. We have no real grasp on a &lt;em&gt;direct&lt;/em&gt; practice of human flourishing, but rather grasp it as the harmonious and mutually supportive interaction of all eudaimonic practices and support-practices participating in the flourishing. And as there is, indeed, not much outside of the practice of human flourishing, it’s also unclear whether there is room for a support-practice &lt;em&gt;external&lt;/em&gt; to the field of human flourishing itself.&lt;/p&gt;
&lt;p&gt;It’s here that I want to call on the classic idea of domain-general virtues, the traditional centerpiece of theories of human flourishing. I propose that the cultivation of human flourishing as such --  the cultivation of the harmony of a multiplicity of practices, including their resource-hungry support practices -- is the cultivation of an &lt;em&gt;&lt;strong&gt;adverbial&lt;/strong&gt;&lt;/em&gt; practice that modulates each and every practice. What makes our practices ‘play nice’ together are our adverbial practices of going about  any practice &lt;em&gt;carefully&lt;/em&gt;, &lt;em&gt;kindly&lt;/em&gt;, &lt;em&gt;respectfully&lt;/em&gt;, &lt;em&gt;accountably&lt;/em&gt;, &lt;em&gt;peacefully&lt;/em&gt;, &lt;em&gt;honestly&lt;/em&gt;, &lt;em&gt;sensitively&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="viii-virtue-decision-theory"&gt;VIII. Virtue decision-theory&lt;/h2&gt;
&lt;p&gt;Why think of qualities like kindness, respectfulness, or honesty as ‘practices’? The first reason is that devotion to a quality like kindness or honesty displays the same &lt;strong&gt;normative structure&lt;/strong&gt; with regard to means and ends as we find in devotion to a practice: An agent devoted to kindness cares about their own future kindness (and about the future kindness of others), but will seek to secure future kindness only by &lt;em&gt;kind means&lt;/em&gt;. The second reason is that qualities like kindness or honesty also approximately have the &lt;strong&gt;material&lt;/strong&gt; structure of a practice: there exist effective very kind strategies for promoting kindness in oneself and others, and when these strategies succeed they further increase affordances for effective very kind strategies for promoting kindness/honesty in oneself and others.&lt;/p&gt;
&lt;p&gt;The difference between adverbial practices like kindness or honesty and practices like research mathematics is that adverbial practices don’t have a “proprietary” domain. In a practice like research mathematics, the material structure of the domain does the most of work of directing agents to a eudaimonic form of agency all by itself, as long as the agents restrict themselves to in-domain actions. (Recall that we described mathematically excellent action as, in ordinary circumstances, the best action among mathematical action for maximizing aggregate mathematical excellence.) With a domain-general, adverbial practice like kindness the normative structure needs to do somewhat more heavy lifting.&lt;/p&gt;
&lt;p&gt;The following is a first pass at characterizing the &lt;strong&gt;normative structure&lt;/strong&gt; of an adverbial practice that values some action-quality &lt;em&gt;x&lt;/em&gt;. The corresponding material efficiency condition (or &lt;strong&gt;material structure&lt;/strong&gt;) necessary for the practice to be viable is that under ordinary circumstances this decision-procedure be instrumentally competitive with naive optimization of aggregate &lt;em&gt;x&lt;/em&gt;-ness:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Actions (or more generally 'computations') get an &lt;em&gt;x&lt;/em&gt;-ness rating. We define the agent’s expected utility conditional on a candidate action a as the sum of two utility functions: a bounded utility function on the &lt;em&gt;x&lt;/em&gt;-ness of a and a more tightly bounded utility function on the expected aggregate &lt;em&gt;x&lt;/em&gt;-ness of the agent's future actions conditional on a. (Thus the agent will choose an action with mildly suboptimal &lt;em&gt;x&lt;/em&gt;-ness if it gives a big boost to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness, but refuse certain large sacrifices of present &lt;em&gt;x&lt;/em&gt;-ness for big boosts to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A commitment to an adverbial practice that values &lt;em&gt;x&lt;/em&gt; is a commitment to promoting &lt;em&gt;x&lt;/em&gt;-ness (in oneself and others) &lt;em&gt;x&lt;/em&gt;-ingly.  The agent strikes a balance between promoting &lt;em&gt;x&lt;/em&gt;-ness and acting &lt;em&gt;x&lt;/em&gt;-ingly that heavily prioritizes acting &lt;em&gt;x&lt;/em&gt;-ingly when the two are in conflict, but if &lt;em&gt;x&lt;/em&gt; meets the material efficacy condition then the loss this balance imposes on future &lt;em&gt;x&lt;/em&gt;-ness will be small under normal circumstances, and -- from our point of view -- desirable in abnormal circumstances. This is because just like the practices of research mathematics, philosophy, or art, an adverbial practice is a crucial ‘epistemic filter’ on actions aiming to shape its future, and the (e.g.) future kindness a paperclipper-like future-kindness-optimizer optimizes for is probably not the kindness we want. What we know about kindness with relative certainty is that we’d like people and AIs here and now to act kindly, and to develop, propagate, and empower the habit and art of kindness in a way that is both kind and clever.&lt;/p&gt;
&lt;p&gt;To keep our conceptual system nicely organized, we might want distinguish merely (e.g.) very kind action from an action that is both very kind and highly promotive of future kindness in oneself and others, and call the latter sort of action excellently kind.  What I call the material efficacy conditions for adverbial practices states not that the kindest action best-promotes aggregate kindness, but that there are almost always action-options that are excellently kind: very kind actions that strongly promote aggregate kindness in oneself and others.&lt;/p&gt;
&lt;h2 id="ix-virtue-decision-theory-is-natural-for-humans-and-ais"&gt;IX. Virtue decision-theory is 'Natural' for Humans and AIs&lt;/h2&gt;
&lt;p&gt;I’ve said that the robustness or ‘naturalness’ of a practice’s normative structure (‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) depends on the practice’s &lt;strong&gt;material structure&lt;/strong&gt;: the capacity of high &lt;em&gt;x&lt;/em&gt;-ness actions to causally promote aggregate &lt;em&gt;x&lt;/em&gt;-ness. I also said that in key real-world practices, commitment to &lt;em&gt;x&lt;/em&gt;-ing might optimize aggregate &lt;em&gt;x&lt;/em&gt;-ness even better than direct optimization would. These two claims are best understood together. On my view, the normative structure ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ appears prominently in human life because (given the right material structure) ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ is a &lt;em&gt;much more stable&lt;/em&gt; than ‘promote &lt;em&gt;x&lt;/em&gt;.’&lt;/p&gt;
&lt;p&gt;How so? Both humans and any sufficiently dynamic AI agent operate in a world that subjects their agency, values, and dispositions to constant mutation pressures from RL-like and Darwinian-like processes. Eudaimonic deliberation is an RL-dynamics-native, Darwinian-dynamics-native operation: its direct object is a form of life that reinforces, enables, and propagates that same form of life. When an &lt;em&gt;x&lt;/em&gt;-ing successfully promotes (in expectation) aggregate &lt;em&gt;x&lt;/em&gt;-nes, the fact of its success itself promotes &lt;em&gt;x&lt;/em&gt;-ing because it reverberates via ubiquitous RL-like and Darwinian-like processes that reinforce (a generalization of) successful action. The material structure of a practice is the backbone that makes reliable success and meaningful generalization possible -- the right ecology of  neural-network generalization dynamics, reinforcement-learning feedback loop dynamics, and neural and environmental selection dynamics.&lt;/p&gt;
&lt;p&gt;An EA-style optimizer trying to minimize risk from optimization-goal-mutation, by contrast, is fighting an uphill battle to foresee and contain the RL-like and Darwinian-like side effects of its optimization actions. One critical mutation-pressure in particular is the risk that an optimizer agent will cultivate, reinforce, and materially empower subroutines (what high-church alignment theory calls ‘mesaoptimizers’) that initially serve the optimization goal but gradually distort or overtake it. For example, if a pro-democracy government instates a secret police to detect and extrajudicially kill anti-democracy agitators, and the government increases the secret police’s funding whenever the police convincingly reports discovering an agitator, the secret police might grow into a distorting influence on the government’s democracy-promotion effort. In light or risks like this, it’s not surprising that oppressive democracy-promotion is generally considered an unserious or dishonest idea: even if an agent were to abstract some concept of ‘aggregate democracy’ from democratic practice into a consequentialist value, it’s plausible that the agent should then immediately revert to a commitment to democratic practice (‘promote democracy democratically’) on sophisticated-consequentialist grounds.&lt;/p&gt;
&lt;p&gt;We should perhaps imagine eudaimonic practices as fixed points at the end of a chain of mesaoptimisers taking over outer optimisers and then being taken over by their own mesaoptimisers in turn. What the practice contributes that puts a stop to this process concept of &lt;em&gt;x&lt;/em&gt;-ness that’s applicable to every agentic subroutine of &lt;em&gt;x&lt;/em&gt;-ing across all nesting levels, so that &lt;em&gt;x&lt;/em&gt;-ness is reinforced (both directly and through generalization) across all subroutines and levels.&lt;/p&gt;
&lt;h2 id="x-virtue-decision-theory-is-safe-in-humans-and-ais"&gt;X. Virtue-decision-theory is Safe in Humans and AIs&lt;/h2&gt;
&lt;p&gt;Let’s talk about AI alignment in the more narrow, concrete sense. It’s widely accepted that if early strategically aware AIs possess values like corrigibility, transparency, and perhaps niceness, further alignment efforts are much more likely to succeed. But values like corrigibility or transparency or niceness don’t easily fit into an intuitively consequentialist form like ‘maximize lifetime corrigible behavior’ or ‘maximize lifetime transparency.’ In fact, an AI valuing its own corrigibility or transparency or niceness in an intuitively consequentialist way can lead to extreme power-seeking: the AI should seek to violently remake the world to (for example) protect itself from the risk that humans will modify the AI  to be less corrigible or transparent or nice. On the other hand, constraints or taboos or purely negative values (a.k.a. ‘deontological restrictions’) are widely suspected to be weak, in the sense that an advanced AI will come to work around them or uproot them: ‘never lie’ or ‘never kill’ or ‘never refuse a direct order from the president’ are poor substitutes for active transparency, niceness, and corrigibility.&lt;/p&gt;
&lt;p&gt;Conceiving of corrigibility or transparency or niceness as adverbial practices is a promising way to capture the normal, sensible way we want an agent to value corrigibility or transparency or niceness, which intuitively-consequentialist values and deontology both fail to capture. We want an agent that (e.g.) actively tries to be transparent, and to cultivate its own future transparency and its own future valuing of transparency, but that will not (e.g.) engage in deception and plotting when it expects a high future-transparency payoff.&lt;/p&gt;
&lt;p&gt;If this is right, then eudaimonic rationality is not a matter of congratulating ourselves for our richly human ways of reasoning, valuing, and acting but a key to basic sanity. What makes human life beautiful is also what makes human life possible at all.&lt;/p&gt;
&lt;h2 id="appendix-excellence-and-deep-reinforcement-learning"&gt;Appendix: Excellence and Deep Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;Within the context of broadly RL-based training of deep neural networks, it may be possible to give some more concrete meaning to what I called the material efficacy condition for a property qualifying as an adverbial practices. We can now understand the material efficacy condition on &lt;em&gt;x&lt;/em&gt; partly in terms of the conditions necessary for ‘promote &lt;em&gt;x&lt;/em&gt;-ness &lt;em&gt;x&lt;/em&gt;-ingly’ to be a viable target for RL. Consider an RL training regimen where &lt;em&gt;x&lt;/em&gt;-ness is rewarded but aggregate &lt;em&gt;x&lt;/em&gt;-ness reward is bounded with some asymptotic function on the sum. For &lt;em&gt;x&lt;/em&gt; to meet the RL version of the material efficacy condition, it must be possible to design an initial reward model (most likely LLM-based) that assigns actions an &lt;em&gt;x&lt;/em&gt;-ness rating such that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;em&gt;x&lt;/em&gt;-ness rating is enough of a natural abstraction that reinforcement of high &lt;em&gt;x&lt;/em&gt;-ness actions generalizes.&lt;/li&gt;
&lt;li&gt;If high &lt;em&gt;x&lt;/em&gt;-ness action both depends on having capital of some kind and is suboptimal from the viewpoint of general power-seeking, there must typically be some high &lt;em&gt;x&lt;/em&gt;-ness actions that approximately make up for the (future &lt;em&gt;x&lt;/em&gt;-ness wise) opportunity cost by creating capital useful for &lt;em&gt;x&lt;/em&gt;-ing.&lt;br /&gt;(Illustration: If you dream of achieving great theater acting, one way to do it is to become President of the United States and then pursue a theater career after your presidency, immediately getting interest from great directors who'll help you achieve great acting. Alternatively, you could start in a regional theater after high school, demonstrate talent by acting well, get invited to work with better and better theater directors who develop your skills and reputation -- skills and reputation that are not as generally useful as those you get by being POTUS -- and achieve great acting through that feedback loop.)&lt;/li&gt;
&lt;li&gt;For any capability &lt;em&gt;y&lt;/em&gt; necessary to reward in training to produce effective AI, there must be an unlimited local-optimization path of Pareto improvement for &lt;em&gt;x&lt;/em&gt;-ness and &lt;em&gt;y&lt;/em&gt; together.&lt;br /&gt;(Illustration: Maybe the most effective kind of engineering manager is ruthless; a nice engineering manager can still grow in effectiveness without becoming less nice, because there are many effective nice-engineering-management techniques to master.)&lt;/li&gt;
&lt;li&gt;Successful initial training in ‘promoting &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ allows the model to be used as a basis for a new reward model which human experts judge as better-capturing our concept of &lt;em&gt;x&lt;/em&gt;-ness. The process should be iterable.&lt;br /&gt;(If the model is LLM-based, improved performance may automatically lead to improved understanding of the &lt;em&gt;x&lt;/em&gt;-ness concept. More generally, data from training runs as well the model’s value-function could be used to refine an &lt;em&gt;x&lt;/em&gt;-ness rating that more strongly implements conditions 1-3.)&lt;/li&gt;
&lt;/ol&gt;


&lt;!--kg-card-end: markdown--&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://thegradient.pub/content/images/2026/02/ramelli-featured.jpg" /&gt;&lt;/div&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="preface"&gt;Preface&lt;/h2&gt;
&lt;p&gt;This essay argues that rational people don’t have goals, and that rational AIs shouldn’t have goals. Human actions are rational not because we direct them at some final ‘goals,’ but because we align actions to &lt;em&gt;practices&lt;/em&gt;: networks of actions, action-dispositions, action-evaluation criteria, and action-resources that structure, clarify, develop, and promote themselves. If we want AIs that can genuinely support, collaborate with, or even &lt;strong&gt;comply&lt;/strong&gt; with human agency, AI agents’ deliberations must share a “type signature” with the practices-based logic we use to reflect and act.&lt;/p&gt;
&lt;p&gt;I argue that these issues matter not just for aligning AI to grand ethical ideals like human flourishing, but also for aligning AI to core safety-properties like transparency, helpfulness, harmlessness, or corrigibility. Concepts like ’harmlessness’ or ‘corrigibility’ are unnatural -- brittle, unstable, arbitrary -- for agents who’d interpret them in terms of goals or rules, but natural for agents who’d interpret them as dynamics in networks of actions, action-dispositions, action-evaluation criteria, and action-resources.&lt;/p&gt;
&lt;p&gt;While the issues this essay tackles tend to sprawl, one theme that reappears over and over is the relevance of the formula ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly.’ I argue that this formula captures something important about both meaningful human life-activity (art is the artistic promotion of art, romance is the romantic promotion of romance) and real human morality (to care about kindness is to promote kindness kindly, to care about honesty is to promote honesty honestly).&lt;/p&gt;
&lt;p&gt;I start by asking: What follows for AI alignment if we take the concept of eudaimonia -- active, rational human flourishing -- seriously? I argue that the concept of eudaimonia doesn’t simply point to a desired state or trajectory of the world that we should set as an AI’s optimization target, but rather points to a structure of deliberation different from standard consequentialist rationality. I then argue that this form of rational activity and valuing, which l call &lt;em&gt;eudaimonic rationality&lt;/em&gt;, is a useful or even necessary framework for the agency and values of human-aligned AIs.&lt;/p&gt;
&lt;p&gt;These arguments are based both on the dangers of a “type mismatch” between human flourishing as an optimization target and consequentialist optimization as a form, and on certain material advantages that eudaimonic rationality plausibly possesses in comparison to deontological and consequentialist agency with regard to stability and safety.&lt;/p&gt;
&lt;p&gt;The concept of eudaimonia, I argue, suggests a form of rational activity without a strict distinction between means and ends, or between ‘instrumental’ and ‘terminal’ values. In this model of rational activity, a rational action is an element of a valued practice in roughly the same sense that a note is an element of a melody, a time-step is an element of a computation, and a moment in an organism’s cellular life is an element of that organism’s self-subsistence and self-development.&lt;/p&gt;
&lt;p&gt;My central claim is that our intuitions about the nature of human flourishing are implicitly intuitions that eudaimonic rationality can be functionally robust in a sense highly critical to AI alignment. More specifically, I argue that in light of our best intuitions about the nature of human flourishing it’s plausible that eudaimonic rationality is a &lt;em&gt;natural&lt;/em&gt; form of agency, and that eudaimonic rationality is &lt;em&gt;effective&lt;/em&gt; even by the light of certain consequentialist approximations of its values. I then argue that if our goal is to align AI in support of human flourishing, and if it is furthermore plausible that eudaimonic rationality is natural and efficacious, then many classical AI safety considerations and ‘paradoxes’ of AI alignment speak in favor of trying to instill AIs with eudaimonic rationality.&lt;/p&gt;
&lt;p&gt;Throughout this essay, I will sometimes explicitly and often implicitly be asking whether some form of agency or rationality or practice is &lt;em&gt;natural&lt;/em&gt;. The sense of ‘natural’ I’m calling on is certainly related to the senses used in various virtue-ethical traditions, but the interest I take in it is less immediately normative and more material or technical. While I have no reductive definition at hand, the intended meaning of ‘natural’ is related to stability, coherence, relative non-contingency, ease of learnability, lower algorithmic complexity, convergent cultural evolution, hypothetical convergent cultural evolution across different hypothetical rational-animal species, potential convergent evolution between humans and neural-network based AI, and targetability by ML training processes. While I will also make many direct references to AI alignment, this question of material naturalness is where the real alignment-critical action takes place: if we learn that certain exotic-sounding forms of agency, rationality, or practice are both themselves natural and make the contents of our all-too-human values natural in turn, then we have learned about good, relatively safe, and relatively easy targets for AI alignment.&lt;/p&gt;
&lt;p&gt;Readers may find the following section-by-section overview useful for navigating the essay:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part &lt;em&gt;I&lt;/em&gt; presents a class of cases of rational deliberation that are very different from the Effective Altruism-style optimization many in the AI-alignment world treat as the paradigm of rational deliberation. I call this class of rational deliberations 'eudaimonic rationality,' and identify it with the form of rationality that guides a mathematician or an artist or a friend when they reflect on what to do in mathematics or in art or in friendship.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;II&lt;/em&gt; looks at the case of research mathematics (via an account by Terry Tao) as an example of eudaimonic rationality at work. What does a mathematician try to do in math? I say she tries to be &lt;em&gt;mathematically excellent&lt;/em&gt;, which involves promoting mathematical excellence through mathematical excellence, and that this structure is closely related to why 'mathematical excellence' can even be a concept.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;III&lt;/em&gt; argues that for eudaimonic agents such as a mathematician who is trying to do excellent mathematics, distinctions between ‘instrumental goods’ and ‘terminal goods’ (intrinsic goods) are mostly unnatural. This makes reflection about values go very differently for a eudaimonic agent than for an Effective Altruism-style agent. Instead of looking to reduce a network of causally intertwined apparent values to a minimal base of intrinsic values that “explains away” the rest as instrumental, a eudaimonic agent looks for organism-like causal coherence in a network of apparent values.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IV&lt;/em&gt; cashes out the essay’s central concepts: A &lt;em&gt;eudaimonic practice&lt;/em&gt; is a network of actions, action-dispositions, action-evaluation criteria, and action-resources where high-scoring actions reliably (but defeasibly) causally promote future high-scoring actions. &lt;em&gt;Eudaimonic rationality&lt;/em&gt; is a class of reflective equilibration and deliberation processes that assume an underlying eudaimonic practice and seek to optimize aggregate action-scores specifically via high-scoring action.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;V&lt;/em&gt;, I argue that many puzzles and ‘paradoxes’ about AI alignment are driven by the assumption that mature AI agents will be Effective Altruism-style optimizers. A “type mismatch” between Effective Altruism-style optimization and eudaimonic rationality makes it nearly impossible to translate the interests of humans -- agents who practice eudaimonic rationality -- into a utility function legible to an Effective Altruism-style optimizer AI. But this does not mean that our values are inherently brittle, unnatural, or wildly contingent: while Effective Altruism-style optimizers may well be a natural type of agent, eudaimonic agents (whether biological or AI) are highly natural as well.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;VI&lt;/em&gt;, I ask whether a eudaimonically rational AI agent devoted to a practice like mathematical research would be safe by default. I argue that a practice like mathematical research plausibly has natural boundaries that exclude moves like ‘take over planet to get more compute for mathematical research,’ but the issue is nuanced. I propose that a practice’s boundaries (for which there may be multiple good natural candidates) may be most stable when a practice is paired with a support practice: a complementary practice for dealing with practice-external issues of maintenance and resource-gathering.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VII&lt;/em&gt; develops the idea of ‘support practices’: eudaimonically rational ways to support eudaimonic practices. We famously want AI agents to help humans lead flourishing lives, but how can we define the purview of this ‘help’? I argue that many core human practices have natural support-practices with a derived eudaimonic structure: the work of good couples’ therapist, for instance, is intertwined with but clearly distinct from a couple’s relationship-practice. Still, there remains a problem: a support-practice AI might harm other people and practices to help the people or practice it’s supporting.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VIII&lt;/em&gt; moves from eudaimonic rationality in general to eudaimonically rational morality. I argue that thinking of moral virtues as domain-general, always-on practices solves key AI-alignment-flavored problems with consequentialist and deontological moralities. The core idea is that the conditions for e.g. ‘kindness’ being a robust moral virtue are akin to the conditions for ‘mathematical excellence’ being a meaningful concept: it must be generally viable to promote kindness in yourself and others kindly. It’s this structure, I argue, that gives moral virtues material standing in a ‘fitness landscape’ riven by pressures from neural-network generalization dynamics, reinforcement-learning cycles, and social and natural selection.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IX&lt;/em&gt; argues that eudaimonic agents have some unique forms of robustness to RL-like and Darwinian-like dynamics that tend to mutate the values of EA-style optimizers. In particular, eudaimonic agents should be very robust to the risk of developing rogue subroutines (sometimes called ‘the inner alignment problem’).&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;X&lt;/em&gt; I discuss canonical AI-safety desiderata like transparency, corrigibility, and (more abstractly) niceness. I argue that treating these properties as moral virtues in my sense -- domain-general, always-on eudaimonic practices --  dissolves problems and paradoxes that arise when treating them as goals, as rules, or even as character traits. I end with an appendix on some prospects for RL regimes geared towards eudaimonic rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="i-rational-action-in-the-good-life"&gt;I. Rational Action in the Good Life&lt;/h2&gt;
&lt;p&gt;I start with a consideration of the nature of the good we hope AI alignment can promote. With the exception of hedonistic utilitarians, most actors interested in AI alignment understand our goal as a future brimming with human (and other sapient-being) flourishing: persons living good lives and forming good communities. What I believe many fail to reflect on, however, is that on any plausible conception human flourishing involves a kind of rational activity. Subjects engaged in human flourishing act in intelligible ways subject to reason, reflection, and revision, and this form of rational care and purposefulness is itself part of the constitution of our flourishing. I believe this characterization of human flourishing is relatively uncontroversial upon reflection, but it raises a kind of puzzle if we’re used to thinking of rationality in consequentialist (or consequentialist-with-deontological-constraints) terms: just what goal is the rational agency involved in human-flourishing activity directed towards?&lt;/p&gt;
&lt;p&gt;One obvious answer would be that, like all properly aligned rationality, the rational agency involved in human-flourishing activities is geared towards maximizing human (and other sapient) flourishing. But we should quickly find ourselves confused about the right way to describe the contribution that rational agency in human-flourishing activities makes to human flourishing. It seems neither appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing only by enacting rationality (by selecting actions that are intrinsically valuable when rationally selected), nor appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing just instrumentally (by selecting actions that causally promote human flourishing).&lt;/p&gt;
&lt;p&gt;The first option reduces our rational actions to something ritualistic, even as the good life surely involves mathematicians working to advance mathematics, friends speaking heart-to-heart to deepen intimacies, gymnasts practicing flips to get better at flips, and novelists revising chapters to improve their manuscripts. The second option threatens to make the good in the good life just impossible to find -- if speaking heart-to-heart is not the good of friendship, and working on math is the not the good of mathematics, then what is?&lt;/p&gt;
&lt;p&gt;This essay argues that deliberative reasoning about the good life is neither directed towards goals external to rational action nor directed towards rational action as an independent good, but towards acts of excellent participation in a valued open-ended process. I then go on to argue that the ‘eudaimonic’ structure of deliberation salient in cases like math or friendship (sloganized as ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) is also subtly critical in more worldly, strategic, or morally high-stakes contexts, and constitutes a major organizing principle of human action and deliberation.&lt;/p&gt;
&lt;h2 id="ii-what-is-a-practice"&gt;II. What Is a Practice?&lt;/h2&gt;
&lt;p&gt;Since ‘human flourishing’ can seem mysterious and abstract, let’s focus on some concrete eudaimonic practices. Consider practices like math, art, craft, friendship, athletics, romance, play, and technology, which are among our best-understood candidates for partial answers to the question ‘what would flourishing people in a flourishing community be doing.’ From a consequentialist point of view, these practices are all marked by extreme ambiguity -- and I would argue indeterminacy -- about what’s instrumental and what’s terminal in their guiding ideas of value. Here, for example, is Terry Tao’s account of goodness in mathematics:&lt;/p&gt;
&lt;p&gt;‘The very best examples of good mathematics do not merely fulfil one or more of the criteria of mathematical quality listed at the beginning of this article, but are more importantly part of a greater mathematical story, which then unfurls to generate many further pieces of good mathematics of many different types. Indeed, one can view the history of entire fields of mathematics as being primarily generated by a handful of these great stories, their evolution through time, and their interaction with each other. I would thus conclude that good mathematics [...] also depends on the more “global” question of how it fits in with other pieces of good mathematics, either by building upon earlier achievements or encouraging the development of future breakthroughs. [There seems] to be some undefinable sense that a certain piece of mathematics is “on to something”, that it is a piece of a larger puzzle waiting to be explored further.’&lt;/p&gt;
&lt;p&gt;It may be possible to give some post-hoc decomposition of Tao’s account into two logically distinct components -- a description of a utility-function over mathematical achievements and an empirical theory about causal relations between mathematical achievements -- but I believe this would be artificial and misleading. On a more natural reading, Tao is describing some of the conditions that make good mathematical practice a eudaimonic practice: In a mathematical practice guided by a cultivated mathematical practical-wisdom judgment (Tao’s ‘undefinable sense that a certain piece of mathematics is “on to something”’), present excellent performance by the standard of the practical-wisdom judgment reliably develops the conditions for future excellent performance by the standard of the mathematical practical-wisdom judgment, as well as cultivating our practical and theoretical grasp of the standard itself.&lt;/p&gt;
&lt;p&gt;This is not to suggest that ‘good mathematics causes future good mathematics’ is a full definition or even full informal description of good mathematics. My claim is only that the fact that good mathematics has a disposition to cause future good mathematics reveals something essential about our concept of good mathematics (and about the material affordances enabling this concept). By analogy, consider the respective concepts healthy tiger and healthy human: It's essential to the concept of a healthy tiger that &lt;em&gt;x&lt;/em&gt; being a healthy tiger now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy tiger 5 minutes in the future (since a healthy tiger body self-maintains and enables self-preservation tiger-behaviours), and essential to the concept of a healthy human that &lt;em&gt;x&lt;/em&gt; being a healthy human now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy human 5 minutes in the future (since a healthy human body self-maintains and enables self-preservation human behaviours). But these formulae aren't yet complete descriptions of 'healthy tiger' or 'healthy human,' as evidenced by the fact that we can tell apart a healthy tiger from a healthy human.&lt;/p&gt;
&lt;p&gt;Crucially, the mathematical practical-wisdom described by Tao is not entirely conceptually opaque beyond its basic characterization as a self-cultivating criterion for self-cultivating excellence in mathematical activity. Mathematical flourishing can partly be described as involving the instantiation of a relation (a mathematical-practice relation of ‘developmental connectedness’) among instantiations of relatively individually definable and quantifiable instances of mathematical value such as elegant proofs, clear expositions, strong theorems, cogent definitions and so on. Furthermore, this relation of developmental connectedness is partly defined by its reliable tendency to causally propagate instances of more individually and locally measurable mathematical value (instances of elegant proofs, clear exposition, strong theorems, cogent definitions and so on):&lt;/p&gt;
&lt;p&gt;[I believe] that good mathematics is more than simply the process of solving problems, building theories, and making arguments shorter, stronger, clearer, more elegant, or more rigorous, though these are of course all admirable goals; while achieving all of these tasks (and debating which ones should have higher priority within any given field), we should also be aware of any possible larger context that one’s results could be placed in, as this may well lead to the greatest long-term benefit for the result, for the field, and for mathematics as a whole.&lt;/p&gt;
&lt;p&gt;One could, again, try to interpret this causal relationship between excellence according to Tao’s ‘organicist’ (or ‘narrative’ or ‘developmental’) sense of good mathematics and the reliable propagation of narrow instances of good mathematics as evidence of a means-ends rational relation, where additive maximization of narrow instances of mathematical value is the utility function and ‘organicist’ mathematical insight is the means. For Tao, however, the evidential import of this causal relationship goes exactly the other way -- it suggests a unification of our myriad more-explicit and more-standalone conceptions of mathematical excellence into a more-ineffable but more-complete conception. As Tao says:&lt;/p&gt;
&lt;p&gt;It may seem from the above discussion that the problem of evaluating mathematical quality, while important, is a hopelessly complicated one, especially since many good mathematical achievements may score highly on some of the qualities listed above but not on others [...] However, there is the remarkable phenomenon that good mathematics in one of the above senses tends to beget more good mathematics in many of the other senses as well, leading to the tentative conjecture that perhaps there is, after all, a universal notion of good quality mathematics, and all the specific metrics listed above represent different routes to uncover new mathematics, or different stages or aspects of the evolution of a mathematical story.&lt;/p&gt;
&lt;h2 id="iii-inverting-consequentialist-reflection"&gt;III. Inverting Consequentialist Reflection&lt;/h2&gt;
&lt;p&gt;Tao’s reasoning about local and global mathematical values exemplifies a central difference between consequentialist rationality and eudaimonic rationality, now taken as paradigms not only for selecting actions but for reflecting on values. (Paradigms for what philosophers will sometimes call ‘reflective equilibration.’) Within the paradigm of consequentialist rationality, if excellence in accordance with a holistic, difficult-to-judge apparent value (say ‘freedom’) is reliably a powerful causal promoter of excellence in accordance with more explicit, more standalone apparent values (say ‘material comfort,’ ‘psychological health,’ ‘lifespan’), this relationship functions as evidence against the status of the holistic prima-facie value as a constitutive -- as opposed to instrumental -- value. Within the paradigm of eudaimonic rationality, by contrast, this same relationship functions as evidence for the status of the holistic prima-facie value as a constitutive value.&lt;/p&gt;
&lt;p&gt;For a (typical) consequentialist-rationality reflection process, evidence that the excellence of a whole causally contributes to the excellences of its parts explains away our investment in the excellence of the whole. The “coincidence” that the intrinsically valuable whole is also instrumentally valuable for its parts is taken to suggest a kind of double-counting error --  one we “fix” by concluding that the whole has no constitutive value but valuing the whole is an effective heuristic under normal circumstances. A eudaimonic-rationality reflective equilibration, by contrast, treats instrumental causal connections between excellences as evidence that our notions of excellence are picking out something appropriately ‘substantive.’&lt;br /&gt;For eudaimonic-rationality reflective equilibration, it is the discovery of causal and common-cause relations among excellences that ratifies our initial sense that caring about these excellences is eudaimonically rational. The discovery of these causal connections functions as evidence that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The ‘local’ excellences we care about are resonant or fruitful, in that they causally promote each other and the holistic excellences in which they participate.&lt;/li&gt;
&lt;li&gt;The ‘holistic’ excellences we care about are materially efficacious and robust, in that they causally promote both the more local excellences that participate in them and their own continuation as future holistic excellence.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my view this is the right way to treat causal connections between (apparent) values if we’re hoping to capture actual human values-reflection, and points to an important strength of the eudaimonic rationality paradigm: Eudaimonic rationality dissolves the ‘paradox’ that in real-life arguments about the value of various human enterprises (e.g. the value of branches of science, branches of art, branches of sport), judgments of intrinsic value typically seek succor from some kind of claim to instrumental value. For example, a defense of the importance of research in quantum physics will often appeal to the wonderful technological, mathematical, and special-sciences applications quantum physics gave us, without meaning to reduce the worth of quantum physics to these applications. On my reading, these appeals aren't just additive -- 'aside from the intrinsic value there is also instrumental value' -- but presentations of evidence that research in quantum physics is a resonant part of a flourishing organic whole (e.g. the civilizational whole of ‘modern science and technology’).&lt;/p&gt;
&lt;p&gt;I believe that without 'organicism' of the kind described above, one faces a serious dilemma whenever one argues for the intrinsic worth of a pursuit or norm: either we stress the value's independence from all benefits and applications and make the claim of value dogmatic and irrelevant, or else we invite an instrumentalist reduction that ‘explains away’ the appearance of intrinsic value. Indeed, I’d argue that organicism of this kind is even necessary to make sense of caring about rationality (including truth, knowledge, non-contradiction and so on) non-instrumentally at all: the ‘paradox’ of rationality as a substantive value is that the typical usefulness of rationality suggests an error-theory about its apparent intrinsic value, since it’s a strange coincidence that rationality is both so typically useful and so intrinsically good. On an organicist account, however, we expect that major forms of excellence endemic to human life -- thought, understanding, knowledge, reasoned action -- both typically promote each other and typically promote our material flourishing and causal leverage on the world.&lt;/p&gt;
&lt;h2 id="iv-the-material-efficacy-condition"&gt;IV. The Material Efficacy Condition&lt;/h2&gt;
&lt;p&gt;Returning now to Tao’s account of good mathematics, let’s take final stock of our interpretation. I argue that mathematical excellence (the property marking ‘the very best examples of good mathematics’) according to Tao satisfies the following conditions, which I believe Tao intends as necessary but not sufficient:&lt;/p&gt;
&lt;p&gt;A) Mathematical excellence is a property of mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;B) An excellent mathematical-activity instance performed today is excellent partly by virtue of satisfying the mathematical-practice relation ‘builds on’ with regard to past excellent mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;C) An excellent mathematical-activity instance performed today is excellent partly by virtue of having a reliable causal tendency to bring about future excellent mathematical-activity instances that satisfy the mathematical-practice relation ‘builds on’ with regard to it.&lt;/p&gt;
&lt;p&gt;D) Instantiation of more local, more individually measurable criteria of mathematical-activity goodness such as elegant proofs, clear expositions, and strong theorems is a typical correlate of mathematical excellence.&lt;/p&gt;
&lt;p&gt;E) At a given moment in a given mathematical field, the instantiation of mathematical excellence will be predictably better-correlated with the instantiation of certain local criteria of mathematical-activity goodness than with others.&lt;/p&gt;
&lt;p&gt;Should we take these traits to collectively describe something more like a decision-procedure called ‘mathematical excellence’ that mathematicians should try to follow, or something more like an event called ‘mathematical excellence’ whose aggregate future-occurrences mathematicians should aspire to maximize? My contention is that Tao’s account is inherently ambiguous, and for a good reason: in ordinary circumstances there is no significant practical difference between doing excellent mathematics and doing instrumentally optimal mathematics with regard to maximizing future aggregate excellent mathematics. This isn’t to say that doing excellent mathematics is the instrumentally optimal action among all possible actions with regard to aggregate future excellent mathematics, but that (in ordinary circumstances) it is the instrumentally optimal choice from among mathematical actions with regard to aggregate future excellent mathematics.&lt;/p&gt;
&lt;p&gt;I propose that the rough matchup between mathematical excellence and optimal (among mathematical actions) instrumental promotion of aggregate mathematical excellence is neither an empirical miracle nor something determined ‘by definition’ in a trivializing sense. Rather, ‘mathematical excellence’ as used by Tao is a concept that has a referent only if there is a possible property &lt;em&gt;x&lt;/em&gt; that satisfies both desiderata &lt;em&gt;A&lt;/em&gt;-&lt;em&gt;E&lt;/em&gt; and the additional criterion that among mathematical actions, actions that are optimal as instantiations of &lt;em&gt;x&lt;/em&gt; are also roughly optimal for maximizing aggregate future instantiation of &lt;em&gt;x&lt;/em&gt;-ness.&lt;/p&gt;
&lt;p&gt;This is what I would describe as the material efficacy condition on eudaimonic rationality. In order for a practice to be fit for possessing internal criteria of flourishing, excellence, and eudaimonic rationality, a practice must materially allow for an (under normal circumstances) optimally self-promoting property &lt;em&gt;x&lt;/em&gt; that strongly correlates with a plethora of more local, more individually measurable properties whose instantiation is prima facie valuable. Stated more informally, there must exist a two-way causal relationship between a practice’s excellence and the material, psychological, and epistemic effects of its excellence, such that present excellence reliably materially, psychologically, and epistemically promotes future excellence.&lt;/p&gt;
&lt;h2 id="v-practices-and-optimization"&gt;V. Practices and Optimization&lt;/h2&gt;
&lt;p&gt;I earlier said that if human flourishing involves practicing eudaimonic rationality, there may well be a “type mismatch” between human flourishing and the kind of consequentialist optimization we often associate with the idea of an agenticly mature future AI. In fact, I believe that implicitly recognizing but misdiagnosing this type mismatch is at least partially responsible for MIRI-style pessimism about the probability of aligning any artificial agents to human values.&lt;/p&gt;
&lt;p&gt;On my view, the secret to relatively successful alignment among humans themselves (when there is successful alignment among humans) lies in the role attempted excellence plays as a filter on human interventions in the future trajectory of a eudaimonic practice. To the degree that humans value a given eudaimonic practice, they are committed to effecting their vision for the practice’s future-trajectory primarily by attempting acts of excellence in the present: we stake our intended effect over the practice’s future-trajectory on the self-propagating excellence of our intervention. While this ‘filter’ doesn’t necessarily stop the worst interventions from being harmful (there are forms of ‘anti-excellence’ that also have self-promotion powers), I contend that this filter is mechanically crucial for the possibility of reliably benign or positive interventions.&lt;/p&gt;
&lt;p&gt;What do I mean? Consider the difference between a world where scientists typically try to propagate (what they believe to be) the scientific truth mainly by means of submitting research work to scientific institutions, and a world where scientists typically try to propagate (what they believe to be) the scientific truth by means including propaganda, fraud, threats, bribery, and slander. As Liam Kofi Bright demonstrates in On Fraud, a community of consequentialist scientists devoted to maximizing truth will predictably match the latter model. I believe one lesson to be drawn is that humans’ ability to collaborate in the promotion of science depends on our ability to scientifically collaborate in the promotion of science, rather than throttle the future trajectory of science every-which-way  our financial and political powers based on our individual beliefs about the optimal trajectory of science.&lt;/p&gt;
&lt;p&gt;A flourishing eudaimonic practice is, above all, a natural-selection-like mechanism whose fitness-function selects among attempted acts of excellence the ones conducive to (and constitutive of) the practice’s flourishing, propagating the excellence these acts instantiate. When people committed to a eudaimonic practice make their attempted interventions into the future trajectory of the practice via acts of attempted excellence, the natural-selection-like mechanism embodied by the practice  (rather than any single individual’s theory of optimal future trajectory) is the aligned intelligence determining the practice’s future trajectory.&lt;/p&gt;
&lt;p&gt;The explanation here, again, is partly causal and partly constitutive: a practice’s “ultimate” norms of excellence, including the “ultimate” epistemic and alethic norms of a discursive practice, are partly defined by the succession of norms in the course of a practice’s development through best-efforts attempted excellence. Although this may be no deterrent to an already god-like optimizer who can simulate entire civilizational trajectories, an agent short of these capacities can best act on their vision of the optimal future-trajectory of a practice by attempting an excellent contribution to the practice.&lt;/p&gt;
&lt;p&gt;The second aspect of our type-mismatch is much more in the weeds: In my analysis so far, I discussed the overall excellence of the trajectory of a eudaimonic practice much like a consequentialist might discuss a quantity of utility. This may be taken to suggest that a ‘sophisticated consequentialist’ or ‘universal consequentialist’ could easily accommodate the implications of the so-called type mismatch by treating them as instrumental, decision-procedure level considerations against naive optimization. In fact, quantities like ‘aggregate democracy’ or ‘overall mathematical excellence’ are (on my view) practice-internal quantities that quickly lose meaning if we try to apply them outside the scope of a ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ decision-procedure.&lt;/p&gt;
&lt;p&gt;What do I mean?  Consider, for example, the practice of philosophy. Here are some questions that should arise for a consequentialist planner (including a sophisticated consequentialist planning decision-procedures or habits) who values philosophy practice-trajectories: Does rating (e.g.) Aristotle’s or Dharmakirti’s philosophical achievements as the most excellent achievements in philosophy imply that we should “tile the universe” with independent practice-trajectories designed to reproduce classical Greek or Indian philosophy? If not, is it because we should assign non-linearly greater value to longer trajectories? Or should we discount trajectories that have parallel contents? Or should we analyze the greatness of early achievements in a practice as mostly instrumental greatness but the greatness of later achievements in a practice as mostly intrinsically valuable? These are all, I believe, bad questions that have only arbitrary answers. To an agent trying to promote philosophy by doing excellent philosophical work, the bad questions above are naturally out of scope. The agent uses the concept of ‘aggregate philosophical excellence’ or ‘a philosophy practice-trajectory’s value’ only to reason about the philosophical influence of their work on the trajectory of the philosophy-practice in which it participates. Choosing an excellent action in practice requires (at most) quantitative comparison between different possible paths for a practice-trajectory, not quantitative comparison between possible worlds containing independent practice-trajectories sprinkled throughout time and space.&lt;/p&gt;
&lt;h2 id="vi-prospects-and-problems-for-ai"&gt;VI. Prospects and Problems for AI&lt;/h2&gt;
&lt;p&gt;Is this good news for AI alignment? It’s certainly good news that (if I’m right) eudaimonic practices are something like natural kinds marked by a causal structure that enables a self-developing excellence well-correlated with multiple naive local measures of quality. But does this mean we could develop a stable and safe (e.g.) ‘mathematical excellence through mathematical excellence’ AI? If we create a fully agentic AI mathematician, will it naturally abstain from trying to extend its longevity or get more resources (even for doing mathematics) other than by impressing us with excellent mathematical work?  I think that prospects are good, but not simple.&lt;/p&gt;
&lt;p&gt;I believe ‘mathematical excellence through mathematical excellence’ really can powerfully scope what mechanisms for shaping the future an AI cares to activate. An AI trained to follow ‘promote mathematics mathematically’ will only care about influencing the future by feeding  excellent mathematical work to mathematics’ excellence-propagation mechanism. But it’s harder to say whether the structure of mathematical practice also properly scopes what subactions can be taken as part of an instance of “doing math.” Is a human mathematician working on a would-be excellent proof in pen and paper practicing math when she is picking up a pen or flipping pages? When she is taking the bus to her office? When she’s buying amphetamines? And is an AI mathematician working on a would-be excellent proof practicing math when it opens a Python console? When it searches the web for new papers? When it harvests Earth for compute?&lt;/p&gt;
&lt;p&gt;I think these questions are complex, rather than nonsensical. Much like collective practices, individual practices -- for example a person’s or possibly an AI’s mathematical practice -- may possess functional organic unities that allow a meaningful distinction between internal dynamics (including dynamics of development and empowerment) and external interventions (including interventions of enhancement and provision). Still, it’s clear that eudaimonic practices do not exist in isolation, and that no practice can function without either blending with or relying on a “support practice” of some kind.&lt;/p&gt;
&lt;p&gt;How, then, do we rationally go about externally-oriented activities like building offices for mathematicians, performing elective reconstructive surgery on an athlete, or conducting couples therapy for romantic partners? And furthermore, how do we rationally go about allocating scarce resources useful for different practices, or judging whether to integrate (e.g.) performance-enhancing drugs into a practice?&lt;/p&gt;
&lt;p&gt;This is, I think, the fundamental question for AI alignment from the viewpoint of  ‘eudaimonic rationality.’ We want AI to support human eudaimonic practices -- and, if relevant, its own eudaimonic practices or participation in human eudaimonic practices -- in a eudaimonia-appropriate way. But how does the logic of eudaimonic rationality extend from eudaimonic practices to their support activities? How do we ‘eudaimonically-rationally’ do the dirty work that makes eudaimonia possible? My best answer is: carefully, kindly, respectfully, accountably, peacefully, honestly, sensitively.&lt;/p&gt;
&lt;h2 id="vii-from-support-practices-to-moral-practice"&gt;VII. From Support-Practices to Moral Practice&lt;/h2&gt;
&lt;p&gt;The theory of AI alignment, I propose, should fundamentally be a theory of the eudaimonic rationality of &lt;em&gt;support practices&lt;/em&gt;. One part of this theory should concern the ‘support’ relation itself, and analyze varieties of support practices and their appropriate relation to the self-determination of a eudaimonic practice: Support-practices such as acquiring resources for a practice, maintaining an enabling environment, coaching practitioners, conducting (physical or psychological) therapy for practitioners, devising technological enhancements for a practice, and educating the public about a practice, each have their own ‘role-morality’ vis-a-vis the practice they support. It is this part of the theory of ‘support practices’ that should, if all goes well in the theory’s construction, describe the various practice-external ways to eudaimonically-rationally act on a pro-attitude towards the aggregate excellence of the practice’s future trajectory without treating it like a quantity of utility. (Much like the concept of ‘mathematical action’ scopes the range of action-choices in such a way that decision-theoretic optimization of math’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint, so should the concepts of various types of ‘support action’ scope the range of action-choices in such a way that decision-theoretic optimization of a practice’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint when the choice of actions is scoped.)&lt;/p&gt;
&lt;p&gt;What is more difficult is delineating the appropriate relationship of a support-practice to everything &lt;em&gt;&lt;strong&gt;outside&lt;/strong&gt;&lt;/em&gt; the practice it supports. What stops a marriage-therapist AI on Mars from appropriately tending to the marriage of a Mars-dwelling couple but harvesting Earth for compute to be a better therapist-AI for that couple? While we can perhaps imagine a person or AI taking up a support-role for ‘humanity’s flourishing as whole,’ so that there’s no outside to speak of, I am not sure that the concept of practice remains natural at this level of abstraction. We have no real grasp on a &lt;em&gt;direct&lt;/em&gt; practice of human flourishing, but rather grasp it as the harmonious and mutually supportive interaction of all eudaimonic practices and support-practices participating in the flourishing. And as there is, indeed, not much outside of the practice of human flourishing, it’s also unclear whether there is room for a support-practice &lt;em&gt;external&lt;/em&gt; to the field of human flourishing itself.&lt;/p&gt;
&lt;p&gt;It’s here that I want to call on the classic idea of domain-general virtues, the traditional centerpiece of theories of human flourishing. I propose that the cultivation of human flourishing as such --  the cultivation of the harmony of a multiplicity of practices, including their resource-hungry support practices -- is the cultivation of an &lt;em&gt;&lt;strong&gt;adverbial&lt;/strong&gt;&lt;/em&gt; practice that modulates each and every practice. What makes our practices ‘play nice’ together are our adverbial practices of going about  any practice &lt;em&gt;carefully&lt;/em&gt;, &lt;em&gt;kindly&lt;/em&gt;, &lt;em&gt;respectfully&lt;/em&gt;, &lt;em&gt;accountably&lt;/em&gt;, &lt;em&gt;peacefully&lt;/em&gt;, &lt;em&gt;honestly&lt;/em&gt;, &lt;em&gt;sensitively&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="viii-virtue-decision-theory"&gt;VIII. Virtue decision-theory&lt;/h2&gt;
&lt;p&gt;Why think of qualities like kindness, respectfulness, or honesty as ‘practices’? The first reason is that devotion to a quality like kindness or honesty displays the same &lt;strong&gt;normative structure&lt;/strong&gt; with regard to means and ends as we find in devotion to a practice: An agent devoted to kindness cares about their own future kindness (and about the future kindness of others), but will seek to secure future kindness only by &lt;em&gt;kind means&lt;/em&gt;. The second reason is that qualities like kindness or honesty also approximately have the &lt;strong&gt;material&lt;/strong&gt; structure of a practice: there exist effective very kind strategies for promoting kindness in oneself and others, and when these strategies succeed they further increase affordances for effective very kind strategies for promoting kindness/honesty in oneself and others.&lt;/p&gt;
&lt;p&gt;The difference between adverbial practices like kindness or honesty and practices like research mathematics is that adverbial practices don’t have a “proprietary” domain. In a practice like research mathematics, the material structure of the domain does the most of work of directing agents to a eudaimonic form of agency all by itself, as long as the agents restrict themselves to in-domain actions. (Recall that we described mathematically excellent action as, in ordinary circumstances, the best action among mathematical action for maximizing aggregate mathematical excellence.) With a domain-general, adverbial practice like kindness the normative structure needs to do somewhat more heavy lifting.&lt;/p&gt;
&lt;p&gt;The following is a first pass at characterizing the &lt;strong&gt;normative structure&lt;/strong&gt; of an adverbial practice that values some action-quality &lt;em&gt;x&lt;/em&gt;. The corresponding material efficiency condition (or &lt;strong&gt;material structure&lt;/strong&gt;) necessary for the practice to be viable is that under ordinary circumstances this decision-procedure be instrumentally competitive with naive optimization of aggregate &lt;em&gt;x&lt;/em&gt;-ness:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Actions (or more generally 'computations') get an &lt;em&gt;x&lt;/em&gt;-ness rating. We define the agent’s expected utility conditional on a candidate action a as the sum of two utility functions: a bounded utility function on the &lt;em&gt;x&lt;/em&gt;-ness of a and a more tightly bounded utility function on the expected aggregate &lt;em&gt;x&lt;/em&gt;-ness of the agent's future actions conditional on a. (Thus the agent will choose an action with mildly suboptimal &lt;em&gt;x&lt;/em&gt;-ness if it gives a big boost to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness, but refuse certain large sacrifices of present &lt;em&gt;x&lt;/em&gt;-ness for big boosts to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A commitment to an adverbial practice that values &lt;em&gt;x&lt;/em&gt; is a commitment to promoting &lt;em&gt;x&lt;/em&gt;-ness (in oneself and others) &lt;em&gt;x&lt;/em&gt;-ingly.  The agent strikes a balance between promoting &lt;em&gt;x&lt;/em&gt;-ness and acting &lt;em&gt;x&lt;/em&gt;-ingly that heavily prioritizes acting &lt;em&gt;x&lt;/em&gt;-ingly when the two are in conflict, but if &lt;em&gt;x&lt;/em&gt; meets the material efficacy condition then the loss this balance imposes on future &lt;em&gt;x&lt;/em&gt;-ness will be small under normal circumstances, and -- from our point of view -- desirable in abnormal circumstances. This is because just like the practices of research mathematics, philosophy, or art, an adverbial practice is a crucial ‘epistemic filter’ on actions aiming to shape its future, and the (e.g.) future kindness a paperclipper-like future-kindness-optimizer optimizes for is probably not the kindness we want. What we know about kindness with relative certainty is that we’d like people and AIs here and now to act kindly, and to develop, propagate, and empower the habit and art of kindness in a way that is both kind and clever.&lt;/p&gt;
&lt;p&gt;To keep our conceptual system nicely organized, we might want distinguish merely (e.g.) very kind action from an action that is both very kind and highly promotive of future kindness in oneself and others, and call the latter sort of action excellently kind.  What I call the material efficacy conditions for adverbial practices states not that the kindest action best-promotes aggregate kindness, but that there are almost always action-options that are excellently kind: very kind actions that strongly promote aggregate kindness in oneself and others.&lt;/p&gt;
&lt;h2 id="ix-virtue-decision-theory-is-natural-for-humans-and-ais"&gt;IX. Virtue decision-theory is 'Natural' for Humans and AIs&lt;/h2&gt;
&lt;p&gt;I’ve said that the robustness or ‘naturalness’ of a practice’s normative structure (‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) depends on the practice’s &lt;strong&gt;material structure&lt;/strong&gt;: the capacity of high &lt;em&gt;x&lt;/em&gt;-ness actions to causally promote aggregate &lt;em&gt;x&lt;/em&gt;-ness. I also said that in key real-world practices, commitment to &lt;em&gt;x&lt;/em&gt;-ing might optimize aggregate &lt;em&gt;x&lt;/em&gt;-ness even better than direct optimization would. These two claims are best understood together. On my view, the normative structure ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ appears prominently in human life because (given the right material structure) ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ is a &lt;em&gt;much more stable&lt;/em&gt; than ‘promote &lt;em&gt;x&lt;/em&gt;.’&lt;/p&gt;
&lt;p&gt;How so? Both humans and any sufficiently dynamic AI agent operate in a world that subjects their agency, values, and dispositions to constant mutation pressures from RL-like and Darwinian-like processes. Eudaimonic deliberation is an RL-dynamics-native, Darwinian-dynamics-native operation: its direct object is a form of life that reinforces, enables, and propagates that same form of life. When an &lt;em&gt;x&lt;/em&gt;-ing successfully promotes (in expectation) aggregate &lt;em&gt;x&lt;/em&gt;-nes, the fact of its success itself promotes &lt;em&gt;x&lt;/em&gt;-ing because it reverberates via ubiquitous RL-like and Darwinian-like processes that reinforce (a generalization of) successful action. The material structure of a practice is the backbone that makes reliable success and meaningful generalization possible -- the right ecology of  neural-network generalization dynamics, reinforcement-learning feedback loop dynamics, and neural and environmental selection dynamics.&lt;/p&gt;
&lt;p&gt;An EA-style optimizer trying to minimize risk from optimization-goal-mutation, by contrast, is fighting an uphill battle to foresee and contain the RL-like and Darwinian-like side effects of its optimization actions. One critical mutation-pressure in particular is the risk that an optimizer agent will cultivate, reinforce, and materially empower subroutines (what high-church alignment theory calls ‘mesaoptimizers’) that initially serve the optimization goal but gradually distort or overtake it. For example, if a pro-democracy government instates a secret police to detect and extrajudicially kill anti-democracy agitators, and the government increases the secret police’s funding whenever the police convincingly reports discovering an agitator, the secret police might grow into a distorting influence on the government’s democracy-promotion effort. In light or risks like this, it’s not surprising that oppressive democracy-promotion is generally considered an unserious or dishonest idea: even if an agent were to abstract some concept of ‘aggregate democracy’ from democratic practice into a consequentialist value, it’s plausible that the agent should then immediately revert to a commitment to democratic practice (‘promote democracy democratically’) on sophisticated-consequentialist grounds.&lt;/p&gt;
&lt;p&gt;We should perhaps imagine eudaimonic practices as fixed points at the end of a chain of mesaoptimisers taking over outer optimisers and then being taken over by their own mesaoptimisers in turn. What the practice contributes that puts a stop to this process concept of &lt;em&gt;x&lt;/em&gt;-ness that’s applicable to every agentic subroutine of &lt;em&gt;x&lt;/em&gt;-ing across all nesting levels, so that &lt;em&gt;x&lt;/em&gt;-ness is reinforced (both directly and through generalization) across all subroutines and levels.&lt;/p&gt;
&lt;h2 id="x-virtue-decision-theory-is-safe-in-humans-and-ais"&gt;X. Virtue-decision-theory is Safe in Humans and AIs&lt;/h2&gt;
&lt;p&gt;Let’s talk about AI alignment in the more narrow, concrete sense. It’s widely accepted that if early strategically aware AIs possess values like corrigibility, transparency, and perhaps niceness, further alignment efforts are much more likely to succeed. But values like corrigibility or transparency or niceness don’t easily fit into an intuitively consequentialist form like ‘maximize lifetime corrigible behavior’ or ‘maximize lifetime transparency.’ In fact, an AI valuing its own corrigibility or transparency or niceness in an intuitively consequentialist way can lead to extreme power-seeking: the AI should seek to violently remake the world to (for example) protect itself from the risk that humans will modify the AI  to be less corrigible or transparent or nice. On the other hand, constraints or taboos or purely negative values (a.k.a. ‘deontological restrictions’) are widely suspected to be weak, in the sense that an advanced AI will come to work around them or uproot them: ‘never lie’ or ‘never kill’ or ‘never refuse a direct order from the president’ are poor substitutes for active transparency, niceness, and corrigibility.&lt;/p&gt;
&lt;p&gt;Conceiving of corrigibility or transparency or niceness as adverbial practices is a promising way to capture the normal, sensible way we want an agent to value corrigibility or transparency or niceness, which intuitively-consequentialist values and deontology both fail to capture. We want an agent that (e.g.) actively tries to be transparent, and to cultivate its own future transparency and its own future valuing of transparency, but that will not (e.g.) engage in deception and plotting when it expects a high future-transparency payoff.&lt;/p&gt;
&lt;p&gt;If this is right, then eudaimonic rationality is not a matter of congratulating ourselves for our richly human ways of reasoning, valuing, and acting but a key to basic sanity. What makes human life beautiful is also what makes human life possible at all.&lt;/p&gt;
&lt;h2 id="appendix-excellence-and-deep-reinforcement-learning"&gt;Appendix: Excellence and Deep Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;Within the context of broadly RL-based training of deep neural networks, it may be possible to give some more concrete meaning to what I called the material efficacy condition for a property qualifying as an adverbial practices. We can now understand the material efficacy condition on &lt;em&gt;x&lt;/em&gt; partly in terms of the conditions necessary for ‘promote &lt;em&gt;x&lt;/em&gt;-ness &lt;em&gt;x&lt;/em&gt;-ingly’ to be a viable target for RL. Consider an RL training regimen where &lt;em&gt;x&lt;/em&gt;-ness is rewarded but aggregate &lt;em&gt;x&lt;/em&gt;-ness reward is bounded with some asymptotic function on the sum. For &lt;em&gt;x&lt;/em&gt; to meet the RL version of the material efficacy condition, it must be possible to design an initial reward model (most likely LLM-based) that assigns actions an &lt;em&gt;x&lt;/em&gt;-ness rating such that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;em&gt;x&lt;/em&gt;-ness rating is enough of a natural abstraction that reinforcement of high &lt;em&gt;x&lt;/em&gt;-ness actions generalizes.&lt;/li&gt;
&lt;li&gt;If high &lt;em&gt;x&lt;/em&gt;-ness action both depends on having capital of some kind and is suboptimal from the viewpoint of general power-seeking, there must typically be some high &lt;em&gt;x&lt;/em&gt;-ness actions that approximately make up for the (future &lt;em&gt;x&lt;/em&gt;-ness wise) opportunity cost by creating capital useful for &lt;em&gt;x&lt;/em&gt;-ing.&lt;br /&gt;(Illustration: If you dream of achieving great theater acting, one way to do it is to become President of the United States and then pursue a theater career after your presidency, immediately getting interest from great directors who'll help you achieve great acting. Alternatively, you could start in a regional theater after high school, demonstrate talent by acting well, get invited to work with better and better theater directors who develop your skills and reputation -- skills and reputation that are not as generally useful as those you get by being POTUS -- and achieve great acting through that feedback loop.)&lt;/li&gt;
&lt;li&gt;For any capability &lt;em&gt;y&lt;/em&gt; necessary to reward in training to produce effective AI, there must be an unlimited local-optimization path of Pareto improvement for &lt;em&gt;x&lt;/em&gt;-ness and &lt;em&gt;y&lt;/em&gt; together.&lt;br /&gt;(Illustration: Maybe the most effective kind of engineering manager is ruthless; a nice engineering manager can still grow in effectiveness without becoming less nice, because there are many effective nice-engineering-management techniques to master.)&lt;/li&gt;
&lt;li&gt;Successful initial training in ‘promoting &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ allows the model to be used as a basis for a new reward model which human experts judge as better-capturing our concept of &lt;em&gt;x&lt;/em&gt;-ness. The process should be iterable.&lt;br /&gt;(If the model is LLM-based, improved performance may automatically lead to improved understanding of the &lt;em&gt;x&lt;/em&gt;-ness concept. More generally, data from training runs as well the model’s value-function could be used to refine an &lt;em&gt;x&lt;/em&gt;-ness rating that more strongly implements conditions 1-3.)&lt;/li&gt;
&lt;/ol&gt;


&lt;!--kg-card-end: markdown--&gt;</content:encoded><guid isPermaLink="false">https://thegradient.pub/virtue-ethics-ai-alignment/</guid><pubDate>Wed, 18 Feb 2026 23:25:52 +0000</pubDate></item><item><title>[NEW] OpenAI deepens India push with Pine Labs fintech partnership (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/18/openai-deepens-india-push-with-pine-labs-fintech-partnership/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2170386424.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As India pitches itself as a global hub for applied artificial intelligence, OpenAI has partnered with Pine Labs to integrate AI-driven reasoning into the fintech firm’s payments stack, automating settlement and invoicing workflows in a move the companies say could help accelerate AI-led commerce in India.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership will see Pine Labs embed OpenAI’s application programming interfaces — software tools that let companies plug AI into their existing systems — within its payments and commerce infrastructure, the companies said on Thursday, all with the aim of enabling AI-assisted settlement, reconciliation, and invoicing workflows.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal underscores OpenAI’s broader push to expand its footprint in India, one of its fastest-growing markets, as it looks to move beyond being known primarily as the maker of ChatGPT and embed its technology into education, enterprise, and infrastructure. Earlier this week, OpenAI partnered with leading Indian engineering, medical, and design institutions to bring AI tools into higher education, betting that India’s large developer base and more than a billion internet users will play a central role in the next phase of AI adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pine Labs is already using AI internally to automate parts of its settlement and reconciliation process, cutting the time it takes to clear daily settlements from hours to minutes, according to Chief executive B Amrish Rau. The Noida-based company previously relied on manual checks by dozens of employees to process funds from multiple banks before markets opened each day, a workflow that is now largely handled by AI-driven systems, he said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Pine Labs, the partnership is intended to extend those AI-driven efficiencies beyond internal operations to merchants and corporate clients, starting with business-to-business use cases such as invoice processing, settlements and payments orchestration, Rau told TechCrunch. He noted the company sees faster adoption in B2B workflows, where AI agents can handle large volumes of repetitive financial tasks under predefined rules, before similar capabilities reach consumer-facing payments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“People talk about retail AI, but the bigger impact of all of this is really efficiency improvement, especially in B2B,” Rau said. “If you look at invoicing and settlement, those are workflows where agents can actually drive the process end to end, and that’s where adoption can happen faster.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout of more autonomous, agent-led payment workflows will move faster in overseas markets where regulations already allow such transactions, Rau said, while India is likely to see a more gradual adoption focused on AI-assisted commerce rather than fully agent-initiated payments. He said that Pine Labs is already prototyping agent-driven payments in parts of the Middle East and Southeast Asia, even as Indian regulations require tighter controls on how payments are authorized.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI, the partnership offers a route deeper into India’s payments and enterprise ecosystem as it looks to move beyond consumer-facing tools and embed its models into high-volume, regulated workflows. Rau said the collaboration is aimed at increasing merchant stickiness and expanding Pine Labs’ role from a payments processor to a broader commerce platform, with higher transaction volumes over time translating into incremental revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pine Labs says it works with more than 980,000 merchants, 716 consumer brands, and 177 financial institutions, and has processed over 6 billion cumulative transactions valued at over ₹11.4 trillion (about $126 billion), per its prospectus published last year. The fintech operates across 20 countries, including Malaysia, Singapore, Australia, parts of Africa, the UAE, and the U.S., giving the OpenAI partnership reach across both Indian and international markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rau said the partnership does not involve revenue sharing between the two companies, with Pine Labs not taking a cut if its merchants choose to embed OpenAI’s tools. “We’ve kept it completely independent of each other — anything related to payment and payment services, we will get the benefit of it, and anything related to OpenAI revenues will go to them,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The arrangement, Rau added, is also non-exclusive. He compared it to OpenAI’s partnership with Stripe in the U.S. and said Pine Labs remains open to working with other AI providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rau said Pine Labs is building additional security and compliance layers around AI-driven workflows to ensure that sensitive merchant and consumer transaction data remains protected, as the company integrates AI more deeply into its payments systems. He said the focus is on ensuring transactions remain secure and compliant even as more workflows are automated by AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pine Labs’ interest in AI-driven commerce builds on earlier work through its Setu unit, which has experimented with agent-led bill payment experiences using chatbots including ChatGPT and Anthropic’s Claude. Separately, India also began piloting consumer payments directly through AI chatbots last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new announcement comes as India hosts its AI Impact Summit in New Delhi, where global AI companies including OpenAI, Anthropic, and Google are showcasing their latest capabilities alongside Indian startups demonstrating AI applications aimed at large-scale deployment across sectors such as finance, healthcare, and education.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2170386424.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As India pitches itself as a global hub for applied artificial intelligence, OpenAI has partnered with Pine Labs to integrate AI-driven reasoning into the fintech firm’s payments stack, automating settlement and invoicing workflows in a move the companies say could help accelerate AI-led commerce in India.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership will see Pine Labs embed OpenAI’s application programming interfaces — software tools that let companies plug AI into their existing systems — within its payments and commerce infrastructure, the companies said on Thursday, all with the aim of enabling AI-assisted settlement, reconciliation, and invoicing workflows.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal underscores OpenAI’s broader push to expand its footprint in India, one of its fastest-growing markets, as it looks to move beyond being known primarily as the maker of ChatGPT and embed its technology into education, enterprise, and infrastructure. Earlier this week, OpenAI partnered with leading Indian engineering, medical, and design institutions to bring AI tools into higher education, betting that India’s large developer base and more than a billion internet users will play a central role in the next phase of AI adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pine Labs is already using AI internally to automate parts of its settlement and reconciliation process, cutting the time it takes to clear daily settlements from hours to minutes, according to Chief executive B Amrish Rau. The Noida-based company previously relied on manual checks by dozens of employees to process funds from multiple banks before markets opened each day, a workflow that is now largely handled by AI-driven systems, he said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Pine Labs, the partnership is intended to extend those AI-driven efficiencies beyond internal operations to merchants and corporate clients, starting with business-to-business use cases such as invoice processing, settlements and payments orchestration, Rau told TechCrunch. He noted the company sees faster adoption in B2B workflows, where AI agents can handle large volumes of repetitive financial tasks under predefined rules, before similar capabilities reach consumer-facing payments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“People talk about retail AI, but the bigger impact of all of this is really efficiency improvement, especially in B2B,” Rau said. “If you look at invoicing and settlement, those are workflows where agents can actually drive the process end to end, and that’s where adoption can happen faster.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout of more autonomous, agent-led payment workflows will move faster in overseas markets where regulations already allow such transactions, Rau said, while India is likely to see a more gradual adoption focused on AI-assisted commerce rather than fully agent-initiated payments. He said that Pine Labs is already prototyping agent-driven payments in parts of the Middle East and Southeast Asia, even as Indian regulations require tighter controls on how payments are authorized.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;For OpenAI, the partnership offers a route deeper into India’s payments and enterprise ecosystem as it looks to move beyond consumer-facing tools and embed its models into high-volume, regulated workflows. Rau said the collaboration is aimed at increasing merchant stickiness and expanding Pine Labs’ role from a payments processor to a broader commerce platform, with higher transaction volumes over time translating into incremental revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pine Labs says it works with more than 980,000 merchants, 716 consumer brands, and 177 financial institutions, and has processed over 6 billion cumulative transactions valued at over ₹11.4 trillion (about $126 billion), per its prospectus published last year. The fintech operates across 20 countries, including Malaysia, Singapore, Australia, parts of Africa, the UAE, and the U.S., giving the OpenAI partnership reach across both Indian and international markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rau said the partnership does not involve revenue sharing between the two companies, with Pine Labs not taking a cut if its merchants choose to embed OpenAI’s tools. “We’ve kept it completely independent of each other — anything related to payment and payment services, we will get the benefit of it, and anything related to OpenAI revenues will go to them,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The arrangement, Rau added, is also non-exclusive. He compared it to OpenAI’s partnership with Stripe in the U.S. and said Pine Labs remains open to working with other AI providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rau said Pine Labs is building additional security and compliance layers around AI-driven workflows to ensure that sensitive merchant and consumer transaction data remains protected, as the company integrates AI more deeply into its payments systems. He said the focus is on ensuring transactions remain secure and compliant even as more workflows are automated by AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pine Labs’ interest in AI-driven commerce builds on earlier work through its Setu unit, which has experimented with agent-led bill payment experiences using chatbots including ChatGPT and Anthropic’s Claude. Separately, India also began piloting consumer payments directly through AI chatbots last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new announcement comes as India hosts its AI Impact Summit in New Delhi, where global AI companies including OpenAI, Anthropic, and Google are showcasing their latest capabilities alongside Indian startups demonstrating AI applications aimed at large-scale deployment across sectors such as finance, healthcare, and education.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/18/openai-deepens-india-push-with-pine-labs-fintech-partnership/</guid><pubDate>Thu, 19 Feb 2026 03:30:00 +0000</pubDate></item><item><title>[NEW] Parking-aware navigation system could prevent frustration and emissions (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/parking-aware-navigation-could-prevent-frustration-and-emissions-0219</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT_Probability-Parking-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;It happens every day — a motorist heading across town checks a navigation app to see how long the trip will take, but they find no parking spots available when they reach their destination. By the time they finally park and walk to their destination, they’re significantly later than they expected to be.&lt;/p&gt;&lt;p&gt;Most popular navigation systems send drivers to a location without considering the extra time that could be needed to find parking. This causes more than just a headache for drivers. It can worsen congestion and increase emissions by causing motorists to cruise around looking for a parking spot. This underestimation could also discourage people from taking mass transit because they don’t realize it might be faster than driving and parking.&lt;/p&gt;&lt;p&gt;MIT researchers tackled this problem by developing a system that can be used to identify parking lots that offer the best balance of proximity to the desired location and likelihood of parking availability. Their adaptable method points users to the ideal parking area rather than their destination.&lt;/p&gt;&lt;p&gt;In simulated tests with real-world traffic data from Seattle, this technique achieved time savings of up to 66 percent in the most congested settings. For a motorist, this would reduce travel time by about 35 minutes, compared to waiting for a spot to open in the closest parking lot.&lt;/p&gt;&lt;p&gt;While they haven’t designed a system ready for the real world yet, their demonstrations show the viability of this approach and indicate how it could be implemented.&lt;/p&gt;&lt;p&gt;“This frustration is real and felt by a lot of people, and the bigger issue here is that systematically underestimating these drive times prevents people from making informed choices. It makes it that much harder for people to make shifts to public transit, bikes, or alternative forms of transportation,” says MIT graduate student Cameron Hickert, lead author on a paper describing the work.&lt;/p&gt;&lt;p&gt;Hickert is joined on the paper by Sirui Li PhD ’25; Zhengbing He, a research scientist in the Laboratory for Information and Decision Systems (LIDS); and senior author Cathy Wu, the Class of 1954 Career Development Associate Professor in Civil and Environmental Engineering (CEE) and the Institute for Data, Systems, and Society (IDSS) at MIT, and a member of LIDS. The research appears today in &lt;em&gt;Transactions on Intelligent Transportation Systems&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Probable parking&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To solve the parking problem, the researchers developed a probability-aware approach that considers all possible public parking lots near a destination, the distance to drive there from a point of origin, the distance to walk from each lot to the destination, and the likelihood of parking success.&lt;/p&gt;&lt;p&gt;The approach, based on dynamic programming, works backward from good outcomes to calculate the best route for the user.&lt;/p&gt;&lt;p&gt;Their method also considers the case where a user arrives at the ideal parking lot but can’t find a space. It takes into the account the distance to other parking lots and the probability of success of parking at each.&lt;/p&gt;&lt;p&gt;“If there are several lots nearby that have slightly lower probabilities of success, but are very close to each other, it might be a smarter play to drive there rather than going to the higher-probability lot and hoping to find an opening. Our framework can account for that,” Hickert says.&lt;/p&gt;&lt;p&gt;In the end, their system can identify the optimal lot that has the lowest expected time required to drive, park, and walk to the destination.&lt;/p&gt;&lt;p&gt;But no motorist expects to be the only one trying to park in a busy city center. So, this method also incorporates the actions of other drivers, which affect the user’s probability of parking success.&lt;/p&gt;&lt;p&gt;For instance, another driver may arrive at the user’s ideal lot first and take the last parking spot. Or another motorist could try parking in another lot but then park in the user’s ideal lot if unsuccessful. In addition, another motorist may park in a different lot and cause spillover effects that lower the user’s chances of success.&lt;/p&gt;&lt;p&gt;“With our framework, we show how you can model all those scenarios in a very clean and principled manner,” Hickert says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Crowdsourced parking data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The data on parking availability could come from several sources. For example, some parking lots have magnetic detectors or gates that track the number of cars entering and exiting.&lt;/p&gt;&lt;p&gt;But such sensors aren’t widely used, so to make their system more feasible for real-world deployment, the researchers studied the effectiveness of using crowdsourced data instead.&lt;/p&gt;&lt;p&gt;For instance, users could indicate available parking using an app. Data could also be gathered by tracking the number of vehicles circling to find parking, or how many enter a lot and exit after being unsuccessful.&lt;/p&gt;&lt;p&gt;Someday, autonomous vehicles could even report on open parking spots they drive by.&lt;/p&gt;&lt;p&gt;“Right now, a lot of that information goes nowhere. But if we could capture it, even by having someone simply tap ‘no parking’ in an app, that could be an important source of information that allows people to make more informed decisions,” Hickert adds.&lt;/p&gt;&lt;p&gt;The researchers evaluated their system using real-world traffic data from the Seattle area, simulating different times of day in a congested urban setting and a suburban area. In congested settings, their approach cut total travel time by about 60 percent compared to sitting and waiting for a spot to open, and by about 20 percent compared to a strategy of continually driving to the next closet parking lot.&lt;/p&gt;&lt;p&gt;They also found that crowdsourced observations of parking availability would have an error rate of only about 7 percent, compared to actual parking availability. This indicates it could be an effective way to gather parking probability data.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to conduct larger studies using real-time route information in an entire city. They also want to explore additional avenues for gathering data on parking availability, such as using satellite images, and estimate potential emissions reductions.&lt;/p&gt;&lt;p&gt;“Transportation systems are so large and complex that they are really hard to change. What we look for, and what we found with this approach, is small changes that can have a big impact to help people make better choices, reduce congestion, and reduce emissions,” says Wu.&lt;/p&gt;&lt;p&gt;This research was supported, in part, by Cintra, the MIT Energy Initiative, and the National Science Foundation.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT_Probability-Parking-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;It happens every day — a motorist heading across town checks a navigation app to see how long the trip will take, but they find no parking spots available when they reach their destination. By the time they finally park and walk to their destination, they’re significantly later than they expected to be.&lt;/p&gt;&lt;p&gt;Most popular navigation systems send drivers to a location without considering the extra time that could be needed to find parking. This causes more than just a headache for drivers. It can worsen congestion and increase emissions by causing motorists to cruise around looking for a parking spot. This underestimation could also discourage people from taking mass transit because they don’t realize it might be faster than driving and parking.&lt;/p&gt;&lt;p&gt;MIT researchers tackled this problem by developing a system that can be used to identify parking lots that offer the best balance of proximity to the desired location and likelihood of parking availability. Their adaptable method points users to the ideal parking area rather than their destination.&lt;/p&gt;&lt;p&gt;In simulated tests with real-world traffic data from Seattle, this technique achieved time savings of up to 66 percent in the most congested settings. For a motorist, this would reduce travel time by about 35 minutes, compared to waiting for a spot to open in the closest parking lot.&lt;/p&gt;&lt;p&gt;While they haven’t designed a system ready for the real world yet, their demonstrations show the viability of this approach and indicate how it could be implemented.&lt;/p&gt;&lt;p&gt;“This frustration is real and felt by a lot of people, and the bigger issue here is that systematically underestimating these drive times prevents people from making informed choices. It makes it that much harder for people to make shifts to public transit, bikes, or alternative forms of transportation,” says MIT graduate student Cameron Hickert, lead author on a paper describing the work.&lt;/p&gt;&lt;p&gt;Hickert is joined on the paper by Sirui Li PhD ’25; Zhengbing He, a research scientist in the Laboratory for Information and Decision Systems (LIDS); and senior author Cathy Wu, the Class of 1954 Career Development Associate Professor in Civil and Environmental Engineering (CEE) and the Institute for Data, Systems, and Society (IDSS) at MIT, and a member of LIDS. The research appears today in &lt;em&gt;Transactions on Intelligent Transportation Systems&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Probable parking&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To solve the parking problem, the researchers developed a probability-aware approach that considers all possible public parking lots near a destination, the distance to drive there from a point of origin, the distance to walk from each lot to the destination, and the likelihood of parking success.&lt;/p&gt;&lt;p&gt;The approach, based on dynamic programming, works backward from good outcomes to calculate the best route for the user.&lt;/p&gt;&lt;p&gt;Their method also considers the case where a user arrives at the ideal parking lot but can’t find a space. It takes into the account the distance to other parking lots and the probability of success of parking at each.&lt;/p&gt;&lt;p&gt;“If there are several lots nearby that have slightly lower probabilities of success, but are very close to each other, it might be a smarter play to drive there rather than going to the higher-probability lot and hoping to find an opening. Our framework can account for that,” Hickert says.&lt;/p&gt;&lt;p&gt;In the end, their system can identify the optimal lot that has the lowest expected time required to drive, park, and walk to the destination.&lt;/p&gt;&lt;p&gt;But no motorist expects to be the only one trying to park in a busy city center. So, this method also incorporates the actions of other drivers, which affect the user’s probability of parking success.&lt;/p&gt;&lt;p&gt;For instance, another driver may arrive at the user’s ideal lot first and take the last parking spot. Or another motorist could try parking in another lot but then park in the user’s ideal lot if unsuccessful. In addition, another motorist may park in a different lot and cause spillover effects that lower the user’s chances of success.&lt;/p&gt;&lt;p&gt;“With our framework, we show how you can model all those scenarios in a very clean and principled manner,” Hickert says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Crowdsourced parking data&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The data on parking availability could come from several sources. For example, some parking lots have magnetic detectors or gates that track the number of cars entering and exiting.&lt;/p&gt;&lt;p&gt;But such sensors aren’t widely used, so to make their system more feasible for real-world deployment, the researchers studied the effectiveness of using crowdsourced data instead.&lt;/p&gt;&lt;p&gt;For instance, users could indicate available parking using an app. Data could also be gathered by tracking the number of vehicles circling to find parking, or how many enter a lot and exit after being unsuccessful.&lt;/p&gt;&lt;p&gt;Someday, autonomous vehicles could even report on open parking spots they drive by.&lt;/p&gt;&lt;p&gt;“Right now, a lot of that information goes nowhere. But if we could capture it, even by having someone simply tap ‘no parking’ in an app, that could be an important source of information that allows people to make more informed decisions,” Hickert adds.&lt;/p&gt;&lt;p&gt;The researchers evaluated their system using real-world traffic data from the Seattle area, simulating different times of day in a congested urban setting and a suburban area. In congested settings, their approach cut total travel time by about 60 percent compared to sitting and waiting for a spot to open, and by about 20 percent compared to a strategy of continually driving to the next closet parking lot.&lt;/p&gt;&lt;p&gt;They also found that crowdsourced observations of parking availability would have an error rate of only about 7 percent, compared to actual parking availability. This indicates it could be an effective way to gather parking probability data.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to conduct larger studies using real-time route information in an entire city. They also want to explore additional avenues for gathering data on parking availability, such as using satellite images, and estimate potential emissions reductions.&lt;/p&gt;&lt;p&gt;“Transportation systems are so large and complex that they are really hard to change. What we look for, and what we found with this approach, is small changes that can have a big impact to help people make better choices, reduce congestion, and reduce emissions,” says Wu.&lt;/p&gt;&lt;p&gt;This research was supported, in part, by Cintra, the MIT Energy Initiative, and the National Science Foundation.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/parking-aware-navigation-could-prevent-frustration-and-emissions-0219</guid><pubDate>Thu, 19 Feb 2026 05:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI taps Tata for 100MW AI data center capacity in India, eyes 1GW (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/18/openai-taps-tata-for-100mw-ai-data-center-capacity-in-india-eyes-1gw/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/india-chatgpt.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI has partnered with India’s Tata Group to secure 100 megawatts of AI-ready data center capacity in the country, with plans to scale to 1 gigawatt. The move is part of a broader push to deepen the company’s enterprise and infrastructure footprint in one of its fastest-growing markets. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI announced on Thursday that the partnership with the Tata Group is part of its Stargate project, which aims to build AI-ready infrastructure and expand enterprise adoption globally. OpenAI will become the first customer of Tata Consultancy Services’ HyperVault data center business, beginning with 100 megawatts of capacity. The deal also includes deploying ChatGPT Enterprise across Tata’s workforce and standardizing AI-native software development through OpenAI’s tools.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The partnership, which falls under the “OpenAI for India” initiative, highlights the company’s expanding footprint in the country, which according to recent estimates from CEO Sam Altman has more than 100 million weekly ChatGPT users spanning students, teachers, developers, and entrepreneurs. The scale of adoption has positioned India as one of OpenAI’s most important growth markets as it deepens enterprise and infrastructure investments in the country.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The local data center capacity will allow OpenAI to run its most advanced models within India, reducing latency for users while meeting data residency, security, and compliance requirements for regulated sectors and government workloads. Hosting compute domestically is critical for enterprises that handle sensitive data and operate under data localization and digital infrastructure rules. These circumstances could widen OpenAI’s access to enterprise customers that require in-country processing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An initial 100 megawatts of capacity represents a substantial commitment in the context of AI infrastructure, where large-scale model training and inference require power-hungry clusters of graphics processing units, or GPUs. Scaling to 1 gigawatt over time would place the Tata facility among the largest AI-focused data center deployments globally, underlining the scale of OpenAI’s long-term ambitions in India.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond infrastructure, OpenAI and Tata Group will pursue a strategic enterprise collaboration aimed at accelerating AI adoption across Tata’s businesses. The conglomerate plans to roll out ChatGPT Enterprise to its workforce over the coming years, beginning with hundreds of thousands of employees at Tata Consultancy Services (TCS), in what would rank among the largest enterprise AI deployments globally. TCS also intends to use OpenAI’s Codex tools to standardize AI-native software development across its engineering teams.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;N Chandrasekaran, chairman of Tata Sons, said OpenAI’s partnership would help build “state-of-the-art AI infrastructure in India” while supporting efforts to skill the country’s workforce for the AI era.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Financial terms of the deal were not disclosed, including whether OpenAI is making a capital investment in HyperVault or leasing capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In November 2025, TCS secured backing from private equity firm TPG to develop AI-ready infrastructure in India under its HyperVault data center business. The platform is backed by about ₹180 billion (about $2 billion) in planned investment and is designed to support large-scale compute workloads for hyperscalers and enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI will also expand its certification programs in India, with TCS becoming the first participating organization outside the United States. The certifications are designed to help professionals build practical AI skills across roles and industries, the company said. The move follows OpenAI’s recent partnerships with leading Indian institutions in engineering, medicine, and design.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI plans to open new offices in Mumbai and Bengaluru later this year, adding to its existing presence in New Delhi as it deepens operations in the country. The expansion is expected to support enterprise partnerships, developer engagement, and local regulatory coordination as the company scales its footprint in India.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes as India hosts its AI Impact Summit in New Delhi, where global AI leaders, including Sam Altman, Anthropic CEO Dario Amodei and Google CEO Sundar Pichai are participating alongside Indian startups and enterprises showcasing AI applications across sectors such as finance, healthcare, and education.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has been expanding its presence in India through partnerships with companies including Pine Labs, JioHotstar, Eternal, Cars24, HCLTech, PhonePe, CRED, and MakeMyTrip, as it seeks to embed its models across consumer platforms, enterprise systems and digital payments infrastructure in one of the world’s largest internet markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, the data center build-out, enterprise deployments, and expanding partner ecosystem signal OpenAI’s most comprehensive push yet to anchor advanced AI infrastructure and applications in India.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/india-chatgpt.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI has partnered with India’s Tata Group to secure 100 megawatts of AI-ready data center capacity in the country, with plans to scale to 1 gigawatt. The move is part of a broader push to deepen the company’s enterprise and infrastructure footprint in one of its fastest-growing markets. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI announced on Thursday that the partnership with the Tata Group is part of its Stargate project, which aims to build AI-ready infrastructure and expand enterprise adoption globally. OpenAI will become the first customer of Tata Consultancy Services’ HyperVault data center business, beginning with 100 megawatts of capacity. The deal also includes deploying ChatGPT Enterprise across Tata’s workforce and standardizing AI-native software development through OpenAI’s tools.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The partnership, which falls under the “OpenAI for India” initiative, highlights the company’s expanding footprint in the country, which according to recent estimates from CEO Sam Altman has more than 100 million weekly ChatGPT users spanning students, teachers, developers, and entrepreneurs. The scale of adoption has positioned India as one of OpenAI’s most important growth markets as it deepens enterprise and infrastructure investments in the country.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The local data center capacity will allow OpenAI to run its most advanced models within India, reducing latency for users while meeting data residency, security, and compliance requirements for regulated sectors and government workloads. Hosting compute domestically is critical for enterprises that handle sensitive data and operate under data localization and digital infrastructure rules. These circumstances could widen OpenAI’s access to enterprise customers that require in-country processing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An initial 100 megawatts of capacity represents a substantial commitment in the context of AI infrastructure, where large-scale model training and inference require power-hungry clusters of graphics processing units, or GPUs. Scaling to 1 gigawatt over time would place the Tata facility among the largest AI-focused data center deployments globally, underlining the scale of OpenAI’s long-term ambitions in India.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond infrastructure, OpenAI and Tata Group will pursue a strategic enterprise collaboration aimed at accelerating AI adoption across Tata’s businesses. The conglomerate plans to roll out ChatGPT Enterprise to its workforce over the coming years, beginning with hundreds of thousands of employees at Tata Consultancy Services (TCS), in what would rank among the largest enterprise AI deployments globally. TCS also intends to use OpenAI’s Codex tools to standardize AI-native software development across its engineering teams.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;N Chandrasekaran, chairman of Tata Sons, said OpenAI’s partnership would help build “state-of-the-art AI infrastructure in India” while supporting efforts to skill the country’s workforce for the AI era.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Financial terms of the deal were not disclosed, including whether OpenAI is making a capital investment in HyperVault or leasing capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In November 2025, TCS secured backing from private equity firm TPG to develop AI-ready infrastructure in India under its HyperVault data center business. The platform is backed by about ₹180 billion (about $2 billion) in planned investment and is designed to support large-scale compute workloads for hyperscalers and enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI will also expand its certification programs in India, with TCS becoming the first participating organization outside the United States. The certifications are designed to help professionals build practical AI skills across roles and industries, the company said. The move follows OpenAI’s recent partnerships with leading Indian institutions in engineering, medicine, and design.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI plans to open new offices in Mumbai and Bengaluru later this year, adding to its existing presence in New Delhi as it deepens operations in the country. The expansion is expected to support enterprise partnerships, developer engagement, and local regulatory coordination as the company scales its footprint in India.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes as India hosts its AI Impact Summit in New Delhi, where global AI leaders, including Sam Altman, Anthropic CEO Dario Amodei and Google CEO Sundar Pichai are participating alongside Indian startups and enterprises showcasing AI applications across sectors such as finance, healthcare, and education.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has been expanding its presence in India through partnerships with companies including Pine Labs, JioHotstar, Eternal, Cars24, HCLTech, PhonePe, CRED, and MakeMyTrip, as it seeks to embed its models across consumer platforms, enterprise systems and digital payments infrastructure in one of the world’s largest internet markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, the data center build-out, enterprise deployments, and expanding partner ecosystem signal OpenAI’s most comprehensive push yet to anchor advanced AI infrastructure and applications in India.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/18/openai-taps-tata-for-100mw-ai-data-center-capacity-in-india-eyes-1gw/</guid><pubDate>Thu, 19 Feb 2026 05:34:25 +0000</pubDate></item></channel></rss>