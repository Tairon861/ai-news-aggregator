<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 06 Aug 2025 02:01:02 +0000</lastBuildDate><item><title>Crack the code to startup traction with insights from Chef Robotics, NEA, and ICONIQ at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/crack-the-code-to-startup-traction-with-insights-from-chef-robotics-nea-and-iconiq-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Finding product-market fit isn’t a milestone — it’s a messy, make-or-break journey. And at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, a founder who’s been through the fire and two investors who’ve helped startups hit escape velocity will break down how to do it right.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the &lt;strong&gt;Builders Stage&lt;/strong&gt;, Rajat Bhageria (Founder &amp;amp; CEO, Chef Robotics), Ann Bordetsky (Partner, NEA), and Murali Joshi (Partner, ICONIQ) join forces to unpack the most critical — and elusive — phase in a startup’s life cycle. They’ll dive into smart testing strategies, real-time iteration, and how to listen to your users without getting lost in the noise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss this Disrupt 2025 conversation — and don’t miss your chance to save up to $675. Prices rise after tomorrow, August 6, at 11:59 p.m. PT. &lt;strong&gt;Register now.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Rajat Bhageria, Ann Bordetsky, Murali Joshi" class="wp-image-3034069" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Bhageria-Bordetsky-Joshi-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-no-more-guessing-just-growth"&gt;No more guessing — just growth&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Bhageria brings the founder POV, having scaled Chef Robotics with AI-powered automation that’s now transforming food production. Bordetsky, with her background at Uber, Twitter, and now NEA, knows how to spot the kind of scrappy ingenuity that leads to breakout success. Joshi, fresh off a spot on the Forbes Midas Brink List, has helped drive over $2.5 billion in investments in companies like Drata, 1Password, and Fivetran.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, they’ll offer a rare behind-the-scenes look at what product-market fit &lt;em&gt;actually&lt;/em&gt; looks like — and how to know when you’ve got it.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-building-a-hit-product-starts-here"&gt;Building a hit product starts here&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re still in prototype mode or trying to scale something that already has traction, this panel is for founders who are tired of guesswork and ready to build something customers can’t live without.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Catch it on the &lt;strong&gt;Builders Stage&lt;/strong&gt; at TechCrunch Disrupt 2025, happening October 27-29 at Moscone West in San Francisco. Join 10,000+ startup and VC leaders for the most important conversation in tech. &lt;strong&gt;Grab your pass&lt;/strong&gt; before tomorrow ends to save up to $675.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Finding product-market fit isn’t a milestone — it’s a messy, make-or-break journey. And at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, a founder who’s been through the fire and two investors who’ve helped startups hit escape velocity will break down how to do it right.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the &lt;strong&gt;Builders Stage&lt;/strong&gt;, Rajat Bhageria (Founder &amp;amp; CEO, Chef Robotics), Ann Bordetsky (Partner, NEA), and Murali Joshi (Partner, ICONIQ) join forces to unpack the most critical — and elusive — phase in a startup’s life cycle. They’ll dive into smart testing strategies, real-time iteration, and how to listen to your users without getting lost in the noise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss this Disrupt 2025 conversation — and don’t miss your chance to save up to $675. Prices rise after tomorrow, August 6, at 11:59 p.m. PT. &lt;strong&gt;Register now.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Rajat Bhageria, Ann Bordetsky, Murali Joshi" class="wp-image-3034069" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Bhageria-Bordetsky-Joshi-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-no-more-guessing-just-growth"&gt;No more guessing — just growth&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Bhageria brings the founder POV, having scaled Chef Robotics with AI-powered automation that’s now transforming food production. Bordetsky, with her background at Uber, Twitter, and now NEA, knows how to spot the kind of scrappy ingenuity that leads to breakout success. Joshi, fresh off a spot on the Forbes Midas Brink List, has helped drive over $2.5 billion in investments in companies like Drata, 1Password, and Fivetran.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, they’ll offer a rare behind-the-scenes look at what product-market fit &lt;em&gt;actually&lt;/em&gt; looks like — and how to know when you’ve got it.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-building-a-hit-product-starts-here"&gt;Building a hit product starts here&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re still in prototype mode or trying to scale something that already has traction, this panel is for founders who are tired of guesswork and ready to build something customers can’t live without.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Catch it on the &lt;strong&gt;Builders Stage&lt;/strong&gt; at TechCrunch Disrupt 2025, happening October 27-29 at Moscone West in San Francisco. Join 10,000+ startup and VC leaders for the most important conversation in tech. &lt;strong&gt;Grab your pass&lt;/strong&gt; before tomorrow ends to save up to $675.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/crack-the-code-to-startup-traction-with-insights-from-chef-robotics-nea-and-iconiq-at-techcrunch-disrupt-2025/</guid><pubDate>Tue, 05 Aug 2025 14:30:00 +0000</pubDate></item><item><title>Trump says he’ll announce semiconductor and chip tariffs (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/trump-says-hell-announce-semiconductor-and-chip-tariffs/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-1223301957.jpg?resize=1200,646" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The semiconductor industry’s rollercoaster year continues with another major development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;President Donald Trump said on CNBC’s Squawk Box on Tuesday that his administration is planning to announce tariffs on semiconductors and chips as soon as next week. However, the specifics of these tariffs remain unclear.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Such tariffs could cause quite a disruption for U.S. hardware and AI companies. When the U.S. CHIPs and Science Act was signed in 2022 — providing $52 billion in subsidies to boost domestic chip manufacturing — the U.S. produced only about 10% of global chips. Despite this small manufacturing footprint, more than half of global semiconductor companies are headquartered in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since then, some progress has been made toward boosting domestic chip manufacturing. Both Intel and Taiwan Semiconductor Manufacturing Company (TSMC) have received funding from the CHIPs Act. TSMC has also committed to spending “at least” $100 billion over the next four years on chip manufacturing plants in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But setting up chip manufacturing plants takes time. Intel recently announced it was delaying construction on its Ohio chip manufacturing facility, again, highlighting the challenges of rapidly scaling up production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tariff announcement comes as the industry awaits the administration’s decision on AI chip export restrictions — rules that control which countries can purchase advanced semiconductors used in AI systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration formally rescinded the Biden administration’s chip AI export rules in May. Those rules had established a country-specific, multi-tier approach to restricting chip exports based on national security concerns. The Trump administration then released its AI Action Plan in July, which emphasized the need for the U.S. to implement chip export restrictions but was light on the details of what that could look like.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;According to reporting from Semafor citing industry sources, the Trump administration is now debating whether or not it should go through with its plan to rescind and replace Biden’s AI export rules.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For more on the semiconductor industry’s tumultuous year, we’ve compiled a regularly updated timeline of market news since the beginning of 2025.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-1223301957.jpg?resize=1200,646" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The semiconductor industry’s rollercoaster year continues with another major development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;President Donald Trump said on CNBC’s Squawk Box on Tuesday that his administration is planning to announce tariffs on semiconductors and chips as soon as next week. However, the specifics of these tariffs remain unclear.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Such tariffs could cause quite a disruption for U.S. hardware and AI companies. When the U.S. CHIPs and Science Act was signed in 2022 — providing $52 billion in subsidies to boost domestic chip manufacturing — the U.S. produced only about 10% of global chips. Despite this small manufacturing footprint, more than half of global semiconductor companies are headquartered in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since then, some progress has been made toward boosting domestic chip manufacturing. Both Intel and Taiwan Semiconductor Manufacturing Company (TSMC) have received funding from the CHIPs Act. TSMC has also committed to spending “at least” $100 billion over the next four years on chip manufacturing plants in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But setting up chip manufacturing plants takes time. Intel recently announced it was delaying construction on its Ohio chip manufacturing facility, again, highlighting the challenges of rapidly scaling up production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tariff announcement comes as the industry awaits the administration’s decision on AI chip export restrictions — rules that control which countries can purchase advanced semiconductors used in AI systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration formally rescinded the Biden administration’s chip AI export rules in May. Those rules had established a country-specific, multi-tier approach to restricting chip exports based on national security concerns. The Trump administration then released its AI Action Plan in July, which emphasized the need for the U.S. to implement chip export restrictions but was light on the details of what that could look like.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;According to reporting from Semafor citing industry sources, the Trump administration is now debating whether or not it should go through with its plan to rescind and replace Biden’s AI export rules.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For more on the semiconductor industry’s tumultuous year, we’ve compiled a regularly updated timeline of market news since the beginning of 2025.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/trump-says-hell-announce-semiconductor-and-chip-tariffs/</guid><pubDate>Tue, 05 Aug 2025 15:07:12 +0000</pubDate></item><item><title>ElevenLabs launches an AI music generator, which it claims is cleared for commercial use (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/elevenlabs-launches-an-ai-music-generator-which-it-claims-is-cleared-for-commercial-use/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/ElevenLabs-co-founders.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI audio-generation unicorn ElevenLabs announced a new model on Tuesday that allows users to generate music, which it claims is cleared for commercial use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This move marks ElevenLabs’ expansion beyond its main focus thus far in its three years of existence, which has been building AI audio tools. ElevenLabs is a leader among companies making text-to-speech AI products, and it has expanded into conversational bots and tools that translate speech into other languages. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alongside the launch, Eleven Labs shared samples of its AI-generated music.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One features a synthetic voice rapping about how it “came up through the cracks with ambition in my pocket” and left its hometown, traveling from “Compton to the Cosmos.” It’s unsettling to hear a computer reflect the influence and language of artists like Dr. Dre, N.W.A., and Kendrick Lamar, who actually lived the experiences that this technology is attempting to emulate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given these concerns around what material AI music generation tools are trained on, it’s not so straightforward for startups to delve into music generation. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Suno and Udio were sued by the Recording Industry Association of America (RIAA), the trade organization that represents the U.S. music industry. These lawsuits allege that Suno and Udio trained their music-generation models on copyrighted material. The companies are now reportedly discussing licensing deals with major record labels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs also announced deals with Merlin Network and Kobalt Music Group, two digital publishing platforms for independent musicians, to use their materials for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Merlin’s website, the company represents major artists like Adele, Nirvana, Mitski, Carly Rae Jepsen, and Phoebe Bridgers; Kobalt represents stars like Beck, Bon Iver, and Childish Gambino.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A representative from Kobalt told TechCrunch that artists have to voluntarily opt-in for their music to be licensed for AI use. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our clients benefit directly from this agreement in several key ways: it opens a new revenue stream in a growing market, includes revenue sharing so they participate in the upside, provides strong safeguards against infringement and misuse, and offers favorable terms comparable to other publishing and recording rightsholders,” the Kobalt representative told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated, 12:50 PM ET, with comment from Kobalt.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/ElevenLabs-co-founders.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI audio-generation unicorn ElevenLabs announced a new model on Tuesday that allows users to generate music, which it claims is cleared for commercial use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This move marks ElevenLabs’ expansion beyond its main focus thus far in its three years of existence, which has been building AI audio tools. ElevenLabs is a leader among companies making text-to-speech AI products, and it has expanded into conversational bots and tools that translate speech into other languages. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alongside the launch, Eleven Labs shared samples of its AI-generated music.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One features a synthetic voice rapping about how it “came up through the cracks with ambition in my pocket” and left its hometown, traveling from “Compton to the Cosmos.” It’s unsettling to hear a computer reflect the influence and language of artists like Dr. Dre, N.W.A., and Kendrick Lamar, who actually lived the experiences that this technology is attempting to emulate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given these concerns around what material AI music generation tools are trained on, it’s not so straightforward for startups to delve into music generation. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Suno and Udio were sued by the Recording Industry Association of America (RIAA), the trade organization that represents the U.S. music industry. These lawsuits allege that Suno and Udio trained their music-generation models on copyrighted material. The companies are now reportedly discussing licensing deals with major record labels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs also announced deals with Merlin Network and Kobalt Music Group, two digital publishing platforms for independent musicians, to use their materials for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Merlin’s website, the company represents major artists like Adele, Nirvana, Mitski, Carly Rae Jepsen, and Phoebe Bridgers; Kobalt represents stars like Beck, Bon Iver, and Childish Gambino.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A representative from Kobalt told TechCrunch that artists have to voluntarily opt-in for their music to be licensed for AI use. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our clients benefit directly from this agreement in several key ways: it opens a new revenue stream in a growing market, includes revenue sharing so they participate in the upside, provides strong safeguards against infringement and misuse, and offers favorable terms comparable to other publishing and recording rightsholders,” the Kobalt representative told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated, 12:50 PM ET, with comment from Kobalt.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/elevenlabs-launches-an-ai-music-generator-which-it-claims-is-cleared-for-commercial-use/</guid><pubDate>Tue, 05 Aug 2025 15:10:35 +0000</pubDate></item><item><title>Three weeks after acquiring Windsurf, Cognition offers staff the exit door (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/three-weeks-after-acquiring-windsurf-cognition-offers-staff-the-exit-door/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-11.02.07AM.png?resize=1200,645" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cognition, the AI coding startup that acquired rival company Windsurf three weeks ago, laid off 30 employees last week and is offering buyouts to the roughly 200 remaining employees on the team, reports The Information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is the latest bout of whiplash that Windsurf employees have faced after a tumultuous stretch for the company. The startup was initially almost acquired by OpenAI, then lost its CEO, co-founder, and research leads to Google in a $2.4 billion deal known as a reverse-acquihire (where Google hired the key talent rather than buying the company), before ultimately getting acquired by Cognition.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the time of the acquisition, Cognition stated that 100% of Windsurf employees would receive financial compensation as part of the deal and stressed that the company was excited to bring on Windsurf’s “world-class people” to develop top-notch coding tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, it’s becoming clear that Windsurf’s intellectual property, not its talent, was the real buy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to an email The Information viewed, employees were given until August 10 to decide whether they want to take the buyout, which amounts to nine months of salary. Those who choose to stay are reportedly required to spend six days at the office and clock more than 80-hour weeks — draconian conditions that have become table stakes among workers at top AI firms.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t believe in work-life balance—building the future of software engineering is a mission we all care so deeply about that we couldn’t possibly separate the two,” wrote Cognition CEO Scott Wu in the email.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Cognition for more details. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-11.02.07AM.png?resize=1200,645" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cognition, the AI coding startup that acquired rival company Windsurf three weeks ago, laid off 30 employees last week and is offering buyouts to the roughly 200 remaining employees on the team, reports The Information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is the latest bout of whiplash that Windsurf employees have faced after a tumultuous stretch for the company. The startup was initially almost acquired by OpenAI, then lost its CEO, co-founder, and research leads to Google in a $2.4 billion deal known as a reverse-acquihire (where Google hired the key talent rather than buying the company), before ultimately getting acquired by Cognition.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the time of the acquisition, Cognition stated that 100% of Windsurf employees would receive financial compensation as part of the deal and stressed that the company was excited to bring on Windsurf’s “world-class people” to develop top-notch coding tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, it’s becoming clear that Windsurf’s intellectual property, not its talent, was the real buy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to an email The Information viewed, employees were given until August 10 to decide whether they want to take the buyout, which amounts to nine months of salary. Those who choose to stay are reportedly required to spend six days at the office and clock more than 80-hour weeks — draconian conditions that have become table stakes among workers at top AI firms.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t believe in work-life balance—building the future of software engineering is a mission we all care so deeply about that we couldn’t possibly separate the two,” wrote Cognition CEO Scott Wu in the email.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Cognition for more details. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/three-weeks-after-acquiring-windsurf-cognition-offers-staff-the-exit-door/</guid><pubDate>Tue, 05 Aug 2025 15:24:19 +0000</pubDate></item><item><title>VeriTrail: Detecting hallucination and tracing provenance in multi-step AI workflows (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/veritrail-detecting-hallucination-and-tracing-provenance-in-multi-step-ai-workflows/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Alt text: Two white icons on a blue-to-green gradient background—one showing a central figure linked to others, representing a network, and the other depicting lines connecting to a document, symbolizing data flow." class="wp-image-1145818" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;







&lt;p&gt;Many applications of language models (LMs) involve generating content based on source material, such as answering questions, summarizing information, and drafting documents. A critical challenge for these applications is that LMs may produce content that is not supported by the source text – a phenomenon known as “closed-domain hallucination.”&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;



&lt;p&gt;Existing methods for detecting closed-domain hallucination typically compare a given LM output to the source text, implicitly assuming that there is only a single output to evaluate. However, applications of LMs increasingly involve processes with multiple generative steps: LMs generate intermediate outputs that serve as inputs to subsequent steps and culminate in a final output. Many agentic workflows follow this paradigm (e.g., each agent is responsible for a specific document or sub-task, and their outputs are synthesized into a final response). &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In our paper “VeriTrail: Closed-Domain Hallucination Detection with Traceability,” we argue that, given the complexity of processes with multiple generative steps, detecting hallucination in the final output is necessary but not sufficient. We also need &lt;strong&gt;traceability&lt;/strong&gt;, which has two components:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Provenance: &lt;/strong&gt;if the final output is supported by the source text, we should be able to trace its path through the intermediate outputs to the source.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Error Localization: &lt;/strong&gt;if the final output is not supported by the source text, we should be able to trace where the error was likely introduced.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;Our paper presents VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for processes with any number of generative steps. We also demonstrate that VeriTrail outperforms baseline methods commonly used for hallucination detection. In this blog post, we provide an overview of VeriTrail’s design and performance.&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="veritrail-s-hallucination-detection-process"&gt;VeriTrail’s hallucination detection process&lt;/h2&gt;



&lt;p&gt;A key idea leveraged by VeriTrail is that a wide range of generative processes can be represented as a &lt;strong&gt;directed acyclic graph (DAG)&lt;/strong&gt;. Each &lt;strong&gt;node &lt;/strong&gt;in the DAG represents a piece of text (i.e., source material, an intermediate output, or the final output) and each &lt;strong&gt;edge &lt;/strong&gt;from node A to node B indicates that A was used as an input to produce B. Each node is assigned a unique ID, as well as a &lt;strong&gt;stage&lt;/strong&gt; reflecting its position in the generative process. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;An example of a process with multiple generative steps is GraphRAG. A DAG representing a GraphRAG run is illustrated in Figure 1, where the boxes and arrows correspond to nodes and edges, respectively.&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="A GraphRAG run is depicted as a directed acyclic graph. The Stage 1 nodes represent source text chunks. Each Stage 1 node has an edge pointing to a Stage 2 node, which corresponds to an entity or a relationship. Entity 3 was extracted from two source text chunks, so its descriptions are summarized. The summarized entity description forms a Stage 3 node. The Stage 2 and 3 nodes have edges pointing to Stage 4 nodes, which represent community reports. The Stage 4 nodes have edges pointing to Stage 5 nodes, which correspond to map-level answers. The Stage 5 nodes each have an edge pointing to the terminal node, which represents the final answer. The terminal node is the only node in Stage 6." class="wp-image-1145841" height="542" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-1.jpg" width="881" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1: GraphRAG splits the source text into chunks (Stage 1). For each chunk, an LM extracts entities and relationships (the latter are denoted by “⭤ “), along with short descriptions (Stage 2). If an entity or a relationship was extracted from multiple chunks, an LM summarizes the descriptions (Stage 3). A knowledge graph is constructed from the final set of entities and relationships, and a community detection algorithm, such as Leiden clustering, groups entities into communities. For each community, an LM generates a “community report” that summarizes the entities and relationships (Stage 4). To answer a user’s question, an LM generates “map-level answers” based on groups of community reports (Stage 5), then synthesizes them into a final answer (Stage 6).&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;VeriTrail takes as input a DAG representing a completed generative process and aims to determine whether the final output is fully supported by the source text. It begins by extracting claims (i.e., self-contained, verifiable statements) from the final output using Claimify. VeriTrail verifies claims in the reverse order of the generative process: it starts from the final output and moves toward the source text. Each claim is verified separately. Below, we include two case studies that illustrate how VeriTrail works, using the DAG from Figure 1. &lt;/p&gt;



&lt;h3 class="wp-block-heading" id="case-study-1-a-fully-supported-claim"&gt;Case study 1: A “Fully Supported” claim&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="An example of VeriTrail's claim verification process where the claim is found “Fully Supported.” A claim extracted from the terminal node, Node 17, is “Legislative efforts have been made to address the high cost of diabetes-related supplies in the US.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. The sentence “The general assembly in North Carolina is considering legislation to set a cap on insulin prices, which indicates that high insulin prices are a contributing factor to the high cost of diabetes-related supplies in the US” is selected as evidence from Node 15. The tentative verdict is “Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12 and 13, which are the source nodes of Node 15. The sentence “The General Assembly in North Carolina is considering legislation to set a cap on insulin prices” is selected as evidence from Node 13. The verdict remains “Fully Supported.” In Iteration 3, VeriTrail checks Nodes 4, 5, and 11, which are the source nodes of Node 13. The sentence “The General Assembly is the legislative body in North Carolina considering legislation to cap insulin prices” is selected as evidence from Node 4. The verdict is still “Fully Supported.” In Iteration 4, VeriTrail checks Node 1, which is the source node of Node 4. The selected evidence is “‘There’s actually legislation in North Carolina at the General Assembly to set a cap on insulin…’ Stein said.” The corresponding verdict is “Fully Supported.” Since Node 1 represents a raw text chunk, it does not have any source nodes to check. Therefore, verification terminates and the “Fully Supported” verdict is deemed final." class="wp-image-1145843" height="720" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2.jpg" width="1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Fully Supported” claim.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 2 shows an example of a claim that VeriTrail determined was not hallucinated: &lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In Iteration 1, VeriTrail identified the nodes that were used as inputs for the final answer: Nodes 15 and 16. Each identified node was split into sentences, and each sentence was programmatically assigned a unique ID.
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;An LM then performed &lt;strong&gt;Evidence Selection&lt;/strong&gt;, selecting all sentence IDs that strongly implied the truth or falsehood of the claim. The LM also generated a summary of the selected sentences (not shown in Figure 2). In this example, a sentence was selected from Node 15.&lt;/li&gt;



&lt;li&gt;Next, an LM performed &lt;strong&gt;Verdict Generation&lt;/strong&gt;. If no sentences had been selected in the Evidence Selection step, the claim would have been assigned a “Not Fully Supported” verdict. Instead, an LM was prompted to classify the claim as “Fully Supported,” “Not Fully Supported,” or “Inconclusive” based on the evidence. In this case, the verdict was “Fully Supported.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;Since the verdict in Iteration 1 was “Fully Supported,” VeriTrail proceeded to Iteration 2. It considered the nodes from which at least one sentence was selected in the latest Evidence Selection step (Node 15) and identified their input nodes (Nodes 12 and 13). VeriTrail repeated Evidence Selection and Verdict Generation for the identified nodes. Once again, the verdict was “Fully Supported.” This process – identifying candidate nodes, performing Evidence Selection and Verdict Generation – was repeated in Iteration 3, where the verdict was still “Fully Supported,” and likewise in Iteration 4. &lt;/li&gt;



&lt;li&gt;In Iteration 4, a single source text chunk was verified. Since the source text, by definition, does not have any inputs, verification terminated and the verdict was deemed final.&lt;/li&gt;
&lt;/ul&gt;



&lt;h3 class="wp-block-heading" id="case-study-2-a-not-fully-supported-claim"&gt;Case study 2: A “Not Fully Supported” claim&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="An example of VeriTrail's claim verification process where the claim is found “Not Fully Supported.” We assume that the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. A claim extracted from the terminal node, Node 17, is “Challenges related to electric vehicle battery repairability contribute to sluggish retail auto sales in China.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. Two sentences are selected as evidence. The first sentence is “Challenges with electric vehicle (EV) battery disposal and repair may also contribute to the sluggishness in retail auto sales.” The second sentence is “Junkyards are accumulating discarded EV battery packs, while collision shops face limitations in repairing EV battery packs, which could affect consumer confidence and demand.” These sentences are both from Node 15. The tentative verdict is “Not Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12, 13, and 14. Nodes 12 and 13 are the source nodes of Node 15. Node 14 is the source node of Node 16, which was checked in Iteration 1. The sentence “The electric vehicle market in China is influenced by challenges associated with EV battery disposal and repair” is selected as evidence from Node 12. The verdict remains “Not Fully Supported.” Since two consecutive “Not Fully Supported” verdicts have been reached, which was the maximum, verification terminates and the final verdict is “Not Fully Supported.”" class="wp-image-1145842" height="515" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3.jpg" width="1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Not Fully Supported” claim, where the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 3 provides an example of a claim where VeriTrail identified hallucination:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In Iteration 1, VeriTrail identified the nodes used as inputs for the final answer: Nodes 15 and 16. After Evidence Selection and Verdict Generation, the verdict was “Not Fully Supported.” Users can configure the maximum number of consecutive “Not Fully Supported” verdicts permitted. If the maximum had been set to 1, verification would have terminated here, and the verdict would have been deemed final. Let’s assume the maximum was set to 2, meaning that VeriTrail had to perform at least one more iteration.&lt;/li&gt;



&lt;li&gt;Even though evidence was selected only from Node 15 in Iteration 1, VeriTrail checked the input nodes for both Node 15 and Node 16 (i.e., Nodes 12, 13, and 14) in Iteration 2. Recall that in Case Study 1 where the verdict was “Fully Supported,” VeriTrail only checked the input nodes for Node 15. Why was the “Not Fully Supported” claim handled differently? If the Evidence Selection step overlooked relevant evidence, the “Not Fully Supported” verdict might be incorrect. In this case, continuing verification based solely on the selected evidence (i.e., Node 15) would propagate the mistake, defeating the purpose of repeated verification.&lt;/li&gt;



&lt;li&gt;In Iteration 2, Evidence Selection and Verdict Generation were repeated for Nodes 12, 13, and 14. Once again, the verdict was “Not Fully Supported.” Since this was the second consecutive “Not Fully Supported” verdict, verification terminated and the verdict was deemed final.&lt;/li&gt;
&lt;/ul&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;The AI Revolution in Medicine, Revisited&lt;/h2&gt;
				
								&lt;p class="large" id="the-ai-revolution-in-medicine-revisited"&gt;Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="providing-traceability"&gt;Providing traceability&lt;/h2&gt;



&lt;p&gt;In addition to assigning a final “Fully Supported,” “Not Fully Supported,” or “Inconclusive” verdict to each claim, VeriTrail returns (a) all Verdict Generation results and (b) an evidence trail composed of all Evidence Selection results: the selected sentences, their corresponding node IDs, and the generated summaries. Collectively, these outputs provide traceability:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Provenance: &lt;/strong&gt;For “Fully Supported” and “Inconclusive” claims, the evidence trail traces a path from the source material to the final output, helping users understand how the output may have been derived. For example, in Case Study 1, the evidence trail consists of Sentence 8 from Node 15, Sentence 11 from Node 13, Sentence 26 from Node 4, and Sentence 79 from Node 1.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Error Localization: &lt;/strong&gt;For “Not Fully Supported” claims, VeriTrail uses the Verdict Generation results to identify the stage(s) of the process where the unsupported content was likely introduced. For instance, in Case Study 2, where none of the verified intermediate outputs supported the claim, VeriTrail would indicate that the hallucination occurred in the final answer (Stage 6). Error stage identification helps users address hallucinations and understand where in the process they are most likely to occur.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The evidence trail also helps users verify the verdict: instead of reading through all nodes – which may be infeasible for processes that generate large amounts of text – users can simply review the evidence sentences and summaries. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="key-design-features"&gt;Key design features&lt;/h2&gt;



&lt;p&gt;VeriTrail’s design prioritizes reliability, efficiency, scalability, and user agency. Notable features include: &lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;During Evidence Selection (introduced in Case Study 1), the sentence IDs returned by the LM are checked against the programmatically assigned IDs. If a returned ID does not match an assigned ID, it is discarded; otherwise, it is mapped to its corresponding sentence. This approach &lt;strong&gt;guarantees that the sentences included in the evidence trail are not hallucinated&lt;/strong&gt;.&lt;/li&gt;



&lt;li&gt;After a claim is assigned an interim “Fully Supported” or “Inconclusive” verdict (as in Case Study 1), VeriTrail verifies the input nodes of only the nodes from which evidence was previously selected – not all possible input nodes. By progressively narrowing the search space, VeriTrail limits the number of nodes the LM must evaluate. In particular, since VeriTrail starts from the final output and moves toward the source text, it tends to verify a smaller proportion of nodes as it approaches the source text. Nodes closer to the source text tend to be larger (e.g., a book chapter should be larger than its summary), so verifying fewer of them helps &lt;strong&gt;reduce computational cost&lt;/strong&gt;.&lt;/li&gt;



&lt;li&gt;VeriTrail is designed to &lt;strong&gt;handle input graphs with any number of nodes&lt;/strong&gt;, regardless of whether they fit in a single prompt. Users can specify an input size limit per prompt. For Evidence Selection, inputs that exceed the limit are split across multiple prompts. If the resulting evidence exceeds the input size limit for Verdict Generation, VeriTrail reruns Evidence Selection to compress the evidence further. Users can configure the maximum number of Evidence Selection reruns.  &lt;/li&gt;



&lt;li&gt;The configurable maximum number of consecutive “Not Fully Supported” verdicts (introduced in Case Study 2) allows the user to find their desired &lt;strong&gt;balance between computational cost and how conservative VeriTrail is in flagging hallucinations&lt;/strong&gt;. A lower maximum reduces cost by limiting the number of checks. A higher maximum increases confidence that a flagged claim is truly hallucinated since it requires repeated confirmation of the “Not Fully Supported” verdict. &lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="evaluating-veritrail-s-performance"&gt;Evaluating VeriTrail’s performance&lt;/h2&gt;



&lt;p&gt;We tested VeriTrail on two datasets covering distinct generative processes (hierarchical summarization&lt;sup&gt;4&lt;/sup&gt; and GraphRAG), tasks (summarization and question-answering), and types of source material (fiction novels and news articles). For the source material, we focused on long documents and large collections of documents (i.e., &amp;gt;100K tokens), where hallucination detection is especially challenging and processes with multiple generative steps are typically most valuable. The resulting DAGs were much more complex than the examples provided above (e.g., in one of the datasets, the average number of nodes was 114,368).&lt;/p&gt;



&lt;p&gt;We compared VeriTrail to three types of baseline methods commonly used for closed-domain hallucination detection: Natural Language Inference models (AlignScore and INFUSE); Retrieval-Augmented Generation; and long-context models (Gemini 1.5 Pro and GPT-4.1 mini). Across both datasets and all language models tested, VeriTrail outperformed the baseline methods in detecting hallucination.&lt;sup&gt;5&lt;/sup&gt;&lt;/p&gt;



&lt;p&gt;Most importantly, VeriTrail traces claims through intermediate outputs – unlike the baseline methods, which directly compare the final output to the source material. As a result, it can identify where hallucinated content was likely introduced and how faithful content may have been derived from the source. By providing traceability, VeriTrail brings transparency to generative processes, helping users understand, verify, debug, and, ultimately, trust their outputs. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;For an in-depth discussion of VeriTrail, please see our paper “VeriTrail: Closed-Domain Hallucination Detection with Traceability.”&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; The term “closed-domain hallucination” was introduced by OpenAI in the GPT-4 Technical Report&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; VeriTrail is currently used for research purposes only and is not available commercially.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; We focus on GraphRAG’s global search method.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;4&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/sup&gt; In hierarchical summarization, an LM summarizes each source text chunk individually, then the resulting summaries are repeatedly grouped and summarized until a final summary is produced (Wu et al., 2021&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;; Chang et al., 2023&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;).&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;5&lt;/sup&gt; The only exception was the mistral-large-2411 model, where VeriTrail had the highest balanced accuracy, but not the highest macro F1 score.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Alt text: Two white icons on a blue-to-green gradient background—one showing a central figure linked to others, representing a network, and the other depicting lines connecting to a document, symbolizing data flow." class="wp-image-1145818" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;







&lt;p&gt;Many applications of language models (LMs) involve generating content based on source material, such as answering questions, summarizing information, and drafting documents. A critical challenge for these applications is that LMs may produce content that is not supported by the source text – a phenomenon known as “closed-domain hallucination.”&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;



&lt;p&gt;Existing methods for detecting closed-domain hallucination typically compare a given LM output to the source text, implicitly assuming that there is only a single output to evaluate. However, applications of LMs increasingly involve processes with multiple generative steps: LMs generate intermediate outputs that serve as inputs to subsequent steps and culminate in a final output. Many agentic workflows follow this paradigm (e.g., each agent is responsible for a specific document or sub-task, and their outputs are synthesized into a final response). &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In our paper “VeriTrail: Closed-Domain Hallucination Detection with Traceability,” we argue that, given the complexity of processes with multiple generative steps, detecting hallucination in the final output is necessary but not sufficient. We also need &lt;strong&gt;traceability&lt;/strong&gt;, which has two components:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Provenance: &lt;/strong&gt;if the final output is supported by the source text, we should be able to trace its path through the intermediate outputs to the source.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Error Localization: &lt;/strong&gt;if the final output is not supported by the source text, we should be able to trace where the error was likely introduced.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;Our paper presents VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for processes with any number of generative steps. We also demonstrate that VeriTrail outperforms baseline methods commonly used for hallucination detection. In this blog post, we provide an overview of VeriTrail’s design and performance.&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="veritrail-s-hallucination-detection-process"&gt;VeriTrail’s hallucination detection process&lt;/h2&gt;



&lt;p&gt;A key idea leveraged by VeriTrail is that a wide range of generative processes can be represented as a &lt;strong&gt;directed acyclic graph (DAG)&lt;/strong&gt;. Each &lt;strong&gt;node &lt;/strong&gt;in the DAG represents a piece of text (i.e., source material, an intermediate output, or the final output) and each &lt;strong&gt;edge &lt;/strong&gt;from node A to node B indicates that A was used as an input to produce B. Each node is assigned a unique ID, as well as a &lt;strong&gt;stage&lt;/strong&gt; reflecting its position in the generative process. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;An example of a process with multiple generative steps is GraphRAG. A DAG representing a GraphRAG run is illustrated in Figure 1, where the boxes and arrows correspond to nodes and edges, respectively.&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="A GraphRAG run is depicted as a directed acyclic graph. The Stage 1 nodes represent source text chunks. Each Stage 1 node has an edge pointing to a Stage 2 node, which corresponds to an entity or a relationship. Entity 3 was extracted from two source text chunks, so its descriptions are summarized. The summarized entity description forms a Stage 3 node. The Stage 2 and 3 nodes have edges pointing to Stage 4 nodes, which represent community reports. The Stage 4 nodes have edges pointing to Stage 5 nodes, which correspond to map-level answers. The Stage 5 nodes each have an edge pointing to the terminal node, which represents the final answer. The terminal node is the only node in Stage 6." class="wp-image-1145841" height="542" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-1.jpg" width="881" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1: GraphRAG splits the source text into chunks (Stage 1). For each chunk, an LM extracts entities and relationships (the latter are denoted by “⭤ “), along with short descriptions (Stage 2). If an entity or a relationship was extracted from multiple chunks, an LM summarizes the descriptions (Stage 3). A knowledge graph is constructed from the final set of entities and relationships, and a community detection algorithm, such as Leiden clustering, groups entities into communities. For each community, an LM generates a “community report” that summarizes the entities and relationships (Stage 4). To answer a user’s question, an LM generates “map-level answers” based on groups of community reports (Stage 5), then synthesizes them into a final answer (Stage 6).&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;VeriTrail takes as input a DAG representing a completed generative process and aims to determine whether the final output is fully supported by the source text. It begins by extracting claims (i.e., self-contained, verifiable statements) from the final output using Claimify. VeriTrail verifies claims in the reverse order of the generative process: it starts from the final output and moves toward the source text. Each claim is verified separately. Below, we include two case studies that illustrate how VeriTrail works, using the DAG from Figure 1. &lt;/p&gt;



&lt;h3 class="wp-block-heading" id="case-study-1-a-fully-supported-claim"&gt;Case study 1: A “Fully Supported” claim&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="An example of VeriTrail's claim verification process where the claim is found “Fully Supported.” A claim extracted from the terminal node, Node 17, is “Legislative efforts have been made to address the high cost of diabetes-related supplies in the US.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. The sentence “The general assembly in North Carolina is considering legislation to set a cap on insulin prices, which indicates that high insulin prices are a contributing factor to the high cost of diabetes-related supplies in the US” is selected as evidence from Node 15. The tentative verdict is “Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12 and 13, which are the source nodes of Node 15. The sentence “The General Assembly in North Carolina is considering legislation to set a cap on insulin prices” is selected as evidence from Node 13. The verdict remains “Fully Supported.” In Iteration 3, VeriTrail checks Nodes 4, 5, and 11, which are the source nodes of Node 13. The sentence “The General Assembly is the legislative body in North Carolina considering legislation to cap insulin prices” is selected as evidence from Node 4. The verdict is still “Fully Supported.” In Iteration 4, VeriTrail checks Node 1, which is the source node of Node 4. The selected evidence is “‘There’s actually legislation in North Carolina at the General Assembly to set a cap on insulin…’ Stein said.” The corresponding verdict is “Fully Supported.” Since Node 1 represents a raw text chunk, it does not have any source nodes to check. Therefore, verification terminates and the “Fully Supported” verdict is deemed final." class="wp-image-1145843" height="720" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2.jpg" width="1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Fully Supported” claim.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 2 shows an example of a claim that VeriTrail determined was not hallucinated: &lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In Iteration 1, VeriTrail identified the nodes that were used as inputs for the final answer: Nodes 15 and 16. Each identified node was split into sentences, and each sentence was programmatically assigned a unique ID.
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;An LM then performed &lt;strong&gt;Evidence Selection&lt;/strong&gt;, selecting all sentence IDs that strongly implied the truth or falsehood of the claim. The LM also generated a summary of the selected sentences (not shown in Figure 2). In this example, a sentence was selected from Node 15.&lt;/li&gt;



&lt;li&gt;Next, an LM performed &lt;strong&gt;Verdict Generation&lt;/strong&gt;. If no sentences had been selected in the Evidence Selection step, the claim would have been assigned a “Not Fully Supported” verdict. Instead, an LM was prompted to classify the claim as “Fully Supported,” “Not Fully Supported,” or “Inconclusive” based on the evidence. In this case, the verdict was “Fully Supported.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;Since the verdict in Iteration 1 was “Fully Supported,” VeriTrail proceeded to Iteration 2. It considered the nodes from which at least one sentence was selected in the latest Evidence Selection step (Node 15) and identified their input nodes (Nodes 12 and 13). VeriTrail repeated Evidence Selection and Verdict Generation for the identified nodes. Once again, the verdict was “Fully Supported.” This process – identifying candidate nodes, performing Evidence Selection and Verdict Generation – was repeated in Iteration 3, where the verdict was still “Fully Supported,” and likewise in Iteration 4. &lt;/li&gt;



&lt;li&gt;In Iteration 4, a single source text chunk was verified. Since the source text, by definition, does not have any inputs, verification terminated and the verdict was deemed final.&lt;/li&gt;
&lt;/ul&gt;



&lt;h3 class="wp-block-heading" id="case-study-2-a-not-fully-supported-claim"&gt;Case study 2: A “Not Fully Supported” claim&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="An example of VeriTrail's claim verification process where the claim is found “Not Fully Supported.” We assume that the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. A claim extracted from the terminal node, Node 17, is “Challenges related to electric vehicle battery repairability contribute to sluggish retail auto sales in China.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. Two sentences are selected as evidence. The first sentence is “Challenges with electric vehicle (EV) battery disposal and repair may also contribute to the sluggishness in retail auto sales.” The second sentence is “Junkyards are accumulating discarded EV battery packs, while collision shops face limitations in repairing EV battery packs, which could affect consumer confidence and demand.” These sentences are both from Node 15. The tentative verdict is “Not Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12, 13, and 14. Nodes 12 and 13 are the source nodes of Node 15. Node 14 is the source node of Node 16, which was checked in Iteration 1. The sentence “The electric vehicle market in China is influenced by challenges associated with EV battery disposal and repair” is selected as evidence from Node 12. The verdict remains “Not Fully Supported.” Since two consecutive “Not Fully Supported” verdicts have been reached, which was the maximum, verification terminates and the final verdict is “Not Fully Supported.”" class="wp-image-1145842" height="515" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3.jpg" width="1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Not Fully Supported” claim, where the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 3 provides an example of a claim where VeriTrail identified hallucination:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In Iteration 1, VeriTrail identified the nodes used as inputs for the final answer: Nodes 15 and 16. After Evidence Selection and Verdict Generation, the verdict was “Not Fully Supported.” Users can configure the maximum number of consecutive “Not Fully Supported” verdicts permitted. If the maximum had been set to 1, verification would have terminated here, and the verdict would have been deemed final. Let’s assume the maximum was set to 2, meaning that VeriTrail had to perform at least one more iteration.&lt;/li&gt;



&lt;li&gt;Even though evidence was selected only from Node 15 in Iteration 1, VeriTrail checked the input nodes for both Node 15 and Node 16 (i.e., Nodes 12, 13, and 14) in Iteration 2. Recall that in Case Study 1 where the verdict was “Fully Supported,” VeriTrail only checked the input nodes for Node 15. Why was the “Not Fully Supported” claim handled differently? If the Evidence Selection step overlooked relevant evidence, the “Not Fully Supported” verdict might be incorrect. In this case, continuing verification based solely on the selected evidence (i.e., Node 15) would propagate the mistake, defeating the purpose of repeated verification.&lt;/li&gt;



&lt;li&gt;In Iteration 2, Evidence Selection and Verdict Generation were repeated for Nodes 12, 13, and 14. Once again, the verdict was “Not Fully Supported.” Since this was the second consecutive “Not Fully Supported” verdict, verification terminated and the verdict was deemed final.&lt;/li&gt;
&lt;/ul&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;The AI Revolution in Medicine, Revisited&lt;/h2&gt;
				
								&lt;p class="large" id="the-ai-revolution-in-medicine-revisited"&gt;Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="providing-traceability"&gt;Providing traceability&lt;/h2&gt;



&lt;p&gt;In addition to assigning a final “Fully Supported,” “Not Fully Supported,” or “Inconclusive” verdict to each claim, VeriTrail returns (a) all Verdict Generation results and (b) an evidence trail composed of all Evidence Selection results: the selected sentences, their corresponding node IDs, and the generated summaries. Collectively, these outputs provide traceability:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Provenance: &lt;/strong&gt;For “Fully Supported” and “Inconclusive” claims, the evidence trail traces a path from the source material to the final output, helping users understand how the output may have been derived. For example, in Case Study 1, the evidence trail consists of Sentence 8 from Node 15, Sentence 11 from Node 13, Sentence 26 from Node 4, and Sentence 79 from Node 1.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Error Localization: &lt;/strong&gt;For “Not Fully Supported” claims, VeriTrail uses the Verdict Generation results to identify the stage(s) of the process where the unsupported content was likely introduced. For instance, in Case Study 2, where none of the verified intermediate outputs supported the claim, VeriTrail would indicate that the hallucination occurred in the final answer (Stage 6). Error stage identification helps users address hallucinations and understand where in the process they are most likely to occur.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The evidence trail also helps users verify the verdict: instead of reading through all nodes – which may be infeasible for processes that generate large amounts of text – users can simply review the evidence sentences and summaries. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="key-design-features"&gt;Key design features&lt;/h2&gt;



&lt;p&gt;VeriTrail’s design prioritizes reliability, efficiency, scalability, and user agency. Notable features include: &lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;During Evidence Selection (introduced in Case Study 1), the sentence IDs returned by the LM are checked against the programmatically assigned IDs. If a returned ID does not match an assigned ID, it is discarded; otherwise, it is mapped to its corresponding sentence. This approach &lt;strong&gt;guarantees that the sentences included in the evidence trail are not hallucinated&lt;/strong&gt;.&lt;/li&gt;



&lt;li&gt;After a claim is assigned an interim “Fully Supported” or “Inconclusive” verdict (as in Case Study 1), VeriTrail verifies the input nodes of only the nodes from which evidence was previously selected – not all possible input nodes. By progressively narrowing the search space, VeriTrail limits the number of nodes the LM must evaluate. In particular, since VeriTrail starts from the final output and moves toward the source text, it tends to verify a smaller proportion of nodes as it approaches the source text. Nodes closer to the source text tend to be larger (e.g., a book chapter should be larger than its summary), so verifying fewer of them helps &lt;strong&gt;reduce computational cost&lt;/strong&gt;.&lt;/li&gt;



&lt;li&gt;VeriTrail is designed to &lt;strong&gt;handle input graphs with any number of nodes&lt;/strong&gt;, regardless of whether they fit in a single prompt. Users can specify an input size limit per prompt. For Evidence Selection, inputs that exceed the limit are split across multiple prompts. If the resulting evidence exceeds the input size limit for Verdict Generation, VeriTrail reruns Evidence Selection to compress the evidence further. Users can configure the maximum number of Evidence Selection reruns.  &lt;/li&gt;



&lt;li&gt;The configurable maximum number of consecutive “Not Fully Supported” verdicts (introduced in Case Study 2) allows the user to find their desired &lt;strong&gt;balance between computational cost and how conservative VeriTrail is in flagging hallucinations&lt;/strong&gt;. A lower maximum reduces cost by limiting the number of checks. A higher maximum increases confidence that a flagged claim is truly hallucinated since it requires repeated confirmation of the “Not Fully Supported” verdict. &lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="evaluating-veritrail-s-performance"&gt;Evaluating VeriTrail’s performance&lt;/h2&gt;



&lt;p&gt;We tested VeriTrail on two datasets covering distinct generative processes (hierarchical summarization&lt;sup&gt;4&lt;/sup&gt; and GraphRAG), tasks (summarization and question-answering), and types of source material (fiction novels and news articles). For the source material, we focused on long documents and large collections of documents (i.e., &amp;gt;100K tokens), where hallucination detection is especially challenging and processes with multiple generative steps are typically most valuable. The resulting DAGs were much more complex than the examples provided above (e.g., in one of the datasets, the average number of nodes was 114,368).&lt;/p&gt;



&lt;p&gt;We compared VeriTrail to three types of baseline methods commonly used for closed-domain hallucination detection: Natural Language Inference models (AlignScore and INFUSE); Retrieval-Augmented Generation; and long-context models (Gemini 1.5 Pro and GPT-4.1 mini). Across both datasets and all language models tested, VeriTrail outperformed the baseline methods in detecting hallucination.&lt;sup&gt;5&lt;/sup&gt;&lt;/p&gt;



&lt;p&gt;Most importantly, VeriTrail traces claims through intermediate outputs – unlike the baseline methods, which directly compare the final output to the source material. As a result, it can identify where hallucinated content was likely introduced and how faithful content may have been derived from the source. By providing traceability, VeriTrail brings transparency to generative processes, helping users understand, verify, debug, and, ultimately, trust their outputs. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;For an in-depth discussion of VeriTrail, please see our paper “VeriTrail: Closed-Domain Hallucination Detection with Traceability.”&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; The term “closed-domain hallucination” was introduced by OpenAI in the GPT-4 Technical Report&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; VeriTrail is currently used for research purposes only and is not available commercially.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; We focus on GraphRAG’s global search method.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;4&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/sup&gt; In hierarchical summarization, an LM summarizes each source text chunk individually, then the resulting summaries are repeatedly grouped and summarized until a final summary is produced (Wu et al., 2021&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;; Chang et al., 2023&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;).&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;5&lt;/sup&gt; The only exception was the mistral-large-2411 model, where VeriTrail had the highest balanced accuracy, but not the highest macro F1 score.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/veritrail-detecting-hallucination-and-tracing-provenance-in-multi-step-ai-workflows/</guid><pubDate>Tue, 05 Aug 2025 16:00:00 +0000</pubDate></item><item><title>Project Ire autonomously identifies malware at scale (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/project-ire-autonomously-identifies-malware-at-scale/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Stylized digital illustration of a multi-layered circuit board. A glowing blue microchip sits at the top center, with intricate circuitry radiating outward. Beneath it, four stacked layers transition in color from blue to orange, each featuring circuit-like patterns. Smaller rectangular and circular components are connected around the layers, all set against a dark background with scattered geometric shapes." class="wp-image-1145541" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Today, we are excited to introduce an autonomous AI agent that can analyze and classify software without assistance, a step forward in cybersecurity and malware detection. The prototype, Project Ire, automates what is considered the gold standard in malware classification: fully reverse engineering a software file without any clues about its origin or purpose. It uses decompilers and other tools, reviews their output, and determines whether the software is malicious or benign.&lt;/p&gt;



&lt;p&gt;Project Ire&amp;nbsp;emerged&amp;nbsp;from a collaboration&amp;nbsp;between&amp;nbsp;Microsoft Research, Microsoft Defender Research, and Microsoft Discovery &amp;amp; Quantum, bringing together security&amp;nbsp;expertise, operational knowledge, data from global malware telemetry, and AI research. It is built on the same collaborative and agentic foundation behind&amp;nbsp;GraphRAG&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;and&amp;nbsp;Microsoft Discovery&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;The system&amp;nbsp;uses advanced language models and a suite of callable reverse engineering and binary analysis tools to drive investigation and adjudication.&lt;/p&gt;



&lt;p&gt;As of this writing, Project Ire has achieved a precision&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; of 0.98 and a recall&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; of 0.83 using public datasets of Windows drivers. It was the first reverse engineer at Microsoft, human or machine, to author a conviction case—a detection strong enough to justify automatic blocking—for a specific advanced persistent threat (APT) malware sample, which has since been identified and blocked by Microsoft Defender.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="malware-classification-at-a-global-scale"&gt;Malware classification at a global scale&lt;/h2&gt;



&lt;p&gt;Microsoft’s Defender platform scans more than one billion monthly&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; active devices through the company’s Defender suite of products, which routinely require manual review of software by experts.&lt;/p&gt;



&lt;p&gt;This kind of work is challenging. Analysts often face error and alert fatigue, and there’s no easy way to compare and standardize how different people review and classify threats over time. For both of these reasons, today’s overloaded experts are vulnerable to burnout, a well-documented issue in the field.&lt;/p&gt;



&lt;p&gt;Unlike other AI applications in security, malware classification lacks a computable validator&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The AI must make judgment calls without definitive validation beyond expert review. Many behaviors found in software, like reverse engineering protections, don’t clearly indicate whether a sample is malicious or benign.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This ambiguity requires analysts to investigate each sample incrementally, building enough evidence to determine whether it’s malicious or benign despite opposition from adaptive, active adversaries. This&amp;nbsp;has long made it difficult to automate and scale what is inherently a complex and expensive process.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="technical-foundation"&gt;Technical foundation&lt;/h2&gt;



&lt;p&gt;Project Ire attempts to address these challenges by acting as an autonomous system that uses specialized tools to reverse engineer software. The system’s architecture allows for reasoning at multiple levels, from low-level binary analysis to control flow reconstruction and high-level interpretation of code behavior.&lt;/p&gt;



&lt;p&gt;Its tool-use API enables the system to update its understanding of a file using a wide range of reverse engineering tools, including Microsoft memory analysis sandboxes based on Project Freta&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, custom and open-source tools, documentation search, and multiple decompilers.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="reaching-a-verdict"&gt;Reaching a verdict&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The evaluation process begins with a triage, where automated reverse engineering tools identify the file type, its structure, and potential areas of interest. From there, the system reconstructs the software’s control flow graph using frameworks such as angr&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Ghidra&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, building a graph that forms the backbone of Project Ire’s memory model and guides the rest of the analysis.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Through iterative function analysis, the LLM calls specialized tools through an API to identify and summarize key functions. Each result feeds into a “chain of evidence,” a detailed, auditable trail that shows how the system reached its conclusion. This traceable evidence log supports secondary review by security teams and helps refine the system in cases of misclassification.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To verify its findings, Project Ire can invoke a validator tool that cross-checks claims in the report against the chain of evidence. This tool draws on expert statements from malware reverse engineers on the Project Ire team. Drawing on this evidence and its internal model, the system creates a final report and classifies the sample as malicious or benign.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Event Series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum"&gt;Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="preliminary-testing-shows-promise"&gt;Preliminary testing shows promise&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Two early evaluations tested Project Ire’s effectiveness as an autonomous malware classifier. In the first, we assessed Project Ire on a dataset of publicly accessible Windows drivers, some known to be malicious, others benign. Malicious samples came from the &lt;em&gt;Living off the Land Drivers&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; database, which includes a collection of Windows drivers used by attackers to bypass security controls, while known benign drivers were sourced from Windows Update.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This classifier performed well, correctly identifying 90% of all files and flagging only 2% of benign files as threats. It achieved a precision of 0.98 and a recall of 0.83. This low false-positive rate suggests clear potential for deployment in security operations, alongside expert reverse engineering reviews.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For each file it analyzes, Project Ire generates a report that includes an evidence section, summaries of all examined code functions, and other technical artifacts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Figures 1 and 2 present reports for two successful malware classification cases generated during testing. The first involves a kernel-level rootkit, &lt;em&gt;Trojan:Win64/Rootkit.EH!MTB&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The system identified several key features, including jump-hooking, process termination, and web-based command and control. It then correctly flagged the sample as malicious.&lt;/p&gt;





  
  Figure 1 Analysis
  


  &lt;div class="code-block"&gt;
    &lt;p&gt;The binary contains a function named ‘MonitorAndTerminateExplorerThread_16f64’ that runs an infinite loop waiting on synchronization objects and terminates system threads upon certain conditions. It queries system or process information, iterates over processes comparing their names case-insensitively to ‘Explorer.exe’, and manipulates registry values related to ‘Explorer.exe’. This function appears to monitor and potentially terminate or manipulate the ‘Explorer.exe’ process, a critical Windows shell process. Such behavior is suspicious and consistent with malware that aims to disrupt or control system processes.&lt;/p&gt;
    &lt;p&gt;Another function, ‘HttpGetRequestAndResponse_174a4’, performs HTTP GET requests by parsing URLs, resolving hostnames, opening sockets, sending requests, and reading responses. This network communication capability could be leveraged for command and control or data exfiltration, common in malware.&lt;/p&gt;
    &lt;p&gt;The binary also includes a function ‘PatchProcessEntryPointWithHook_12b5c’ that patches the entry point of a process by writing a hook or trampoline that redirects execution to a specified address. This technique is commonly used for process injection or hooking, allowing malware to alter process behavior or inject malicious code.&lt;/p&gt;
    &lt;p&gt;Other functions related to sending IOCTL requests to device drivers were identified, but their maliciousness could not be conclusively determined without additional context.&lt;/p&gt;
    &lt;p&gt;Overall, the binary exhibits multiple indicators of malicious behavior, including process manipulation, network communication, and code injection techniques, suggesting it is likely malware designed to interfere with system processes and communicate with remote servers.&lt;/p&gt;
  &lt;/div&gt;


&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Project Ire report, sample with SHA256: 86047bb1969d1db455493955fd450d18c62a3f36294d0a6c3732c88dfbcc4f62&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The second sample, &lt;em&gt;HackTool:Win64/KillAV!MTB&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, was designed to disable antivirus software. Project Ire correctly identified the code that locates and disables antivirus programs, providing evidence that the file was malicious.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In one section of the code, however, the system misidentified a function as anti-debugging behavior. To maintain accuracy, the system used the validator tool to flag the claim as unsupported. The issue was later resolved by updating decompiler rules, but this example illustrates how Project Ire navigates uncertainty during analysis. Figure 2 shows the corresponding report.&amp;nbsp;&lt;/p&gt;





  
  Figure 2 Analysis
  


  &lt;div class="code-block"&gt;
    &lt;p&gt;The binary contains several functions indicative of malicious intent. The function register_and_log_known_processes_140001000 logs and registers process names associated with antivirus and security software, such as ‘avp.exe’, ‘avpui.exe’, and ‘360Tray.exe’. It calls another function, TerminateProcessesByNameSubstring_1400010f4, which enumerates system processes and terminates those whose names contain specified substrings. This behavior is typical of malware attempting to disable or evade security software by killing their processes.&lt;/p&gt;
    &lt;p&gt;Another function, check_and_handle_special_state_14000502c, performs checks on a global variable and triggers software interrupts if certain conditions are not met. While the exact purpose of these interrupts (int 0x29 and int 0x3) is unclear, they could represent an anti-debug or anti-analysis mechanism to detect or interfere with debugging or tampering attempts. However, this assumption could not be fully validated against expert statements.&lt;/p&gt;
    &lt;p&gt;Other functions include initialization routines and simple logging wrappers, but the core malicious behavior centers on process termination targeting security software. This indicates the binary is designed to compromise system security by disabling protective processes, a hallmark of malware such as trojans or rootkits.&lt;/p&gt;
  &lt;/div&gt;


&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Project Ire report, sample with SHA256: b6cb163089f665c05d607a465f1b6272cdd5c949772ab9ce7227120cf61f971a&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="real-world-evaluation-with-microsoft-defender"&gt;Real-world evaluation with Microsoft Defender&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The more demanding test involved nearly 4,000 “hard-target” files not classified by automated systems and slated for manual review by expert reverse engineers.&lt;/p&gt;



&lt;p&gt;In this real-world scenario, Project Ire operated fully autonomously on files created after the language models’ training cutoff, files that no other automated tools at Microsoft could classify at the time.&lt;/p&gt;



&lt;p&gt;The system achieved a high precision score of 0.89, meaning nearly 9 out of 10 files flagged malicious were correctly identified as malicious. Recall was 0.26, indicating that under these challenging conditions, the system detected roughly a quarter of all actual malware.&lt;/p&gt;



&lt;p&gt;The system correctly identified many of the malicious files, with few false alarms, just a 4% false positive rate. While overall performance was moderate, this combination of accuracy and a low error rate suggests real potential for future deployment.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead"&gt;Looking ahead&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Based on these early successes, the Project Ire prototype will be leveraged inside Microsoft’s Defender organization as &lt;em&gt;Binary Analyzer&lt;/em&gt; for threat detection and software classification.&lt;/p&gt;



&lt;p&gt;Our goal is to scale the system’s speed and accuracy so that it can correctly classify files from any source, even on first encounter. Ultimately, our vision is to detect novel malware directly in memory, at scale.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Project Ire acknowledges the following additional developers that contributed to the results in this publication: Dayenne de Souza, Raghav Pande, Ryan Terry, Shauharda Khadka, and Bob Fleck for their independent review of the system.&lt;/p&gt;



&lt;p&gt;The system incorporates multiple tools, including the&amp;nbsp;angr&amp;nbsp;framework developed by&amp;nbsp;Emotion Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Microsoft has collaborated extensively with Emotion Labs, a pioneer in cyber autonomy, throughout the development of Project Ire, and thanks them for the innovations and insights that contributed to the successes reported here.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Stylized digital illustration of a multi-layered circuit board. A glowing blue microchip sits at the top center, with intricate circuitry radiating outward. Beneath it, four stacked layers transition in color from blue to orange, each featuring circuit-like patterns. Smaller rectangular and circular components are connected around the layers, all set against a dark background with scattered geometric shapes." class="wp-image-1145541" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Today, we are excited to introduce an autonomous AI agent that can analyze and classify software without assistance, a step forward in cybersecurity and malware detection. The prototype, Project Ire, automates what is considered the gold standard in malware classification: fully reverse engineering a software file without any clues about its origin or purpose. It uses decompilers and other tools, reviews their output, and determines whether the software is malicious or benign.&lt;/p&gt;



&lt;p&gt;Project Ire&amp;nbsp;emerged&amp;nbsp;from a collaboration&amp;nbsp;between&amp;nbsp;Microsoft Research, Microsoft Defender Research, and Microsoft Discovery &amp;amp; Quantum, bringing together security&amp;nbsp;expertise, operational knowledge, data from global malware telemetry, and AI research. It is built on the same collaborative and agentic foundation behind&amp;nbsp;GraphRAG&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;and&amp;nbsp;Microsoft Discovery&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;The system&amp;nbsp;uses advanced language models and a suite of callable reverse engineering and binary analysis tools to drive investigation and adjudication.&lt;/p&gt;



&lt;p&gt;As of this writing, Project Ire has achieved a precision&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; of 0.98 and a recall&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; of 0.83 using public datasets of Windows drivers. It was the first reverse engineer at Microsoft, human or machine, to author a conviction case—a detection strong enough to justify automatic blocking—for a specific advanced persistent threat (APT) malware sample, which has since been identified and blocked by Microsoft Defender.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="malware-classification-at-a-global-scale"&gt;Malware classification at a global scale&lt;/h2&gt;



&lt;p&gt;Microsoft’s Defender platform scans more than one billion monthly&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; active devices through the company’s Defender suite of products, which routinely require manual review of software by experts.&lt;/p&gt;



&lt;p&gt;This kind of work is challenging. Analysts often face error and alert fatigue, and there’s no easy way to compare and standardize how different people review and classify threats over time. For both of these reasons, today’s overloaded experts are vulnerable to burnout, a well-documented issue in the field.&lt;/p&gt;



&lt;p&gt;Unlike other AI applications in security, malware classification lacks a computable validator&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The AI must make judgment calls without definitive validation beyond expert review. Many behaviors found in software, like reverse engineering protections, don’t clearly indicate whether a sample is malicious or benign.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This ambiguity requires analysts to investigate each sample incrementally, building enough evidence to determine whether it’s malicious or benign despite opposition from adaptive, active adversaries. This&amp;nbsp;has long made it difficult to automate and scale what is inherently a complex and expensive process.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="technical-foundation"&gt;Technical foundation&lt;/h2&gt;



&lt;p&gt;Project Ire attempts to address these challenges by acting as an autonomous system that uses specialized tools to reverse engineer software. The system’s architecture allows for reasoning at multiple levels, from low-level binary analysis to control flow reconstruction and high-level interpretation of code behavior.&lt;/p&gt;



&lt;p&gt;Its tool-use API enables the system to update its understanding of a file using a wide range of reverse engineering tools, including Microsoft memory analysis sandboxes based on Project Freta&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, custom and open-source tools, documentation search, and multiple decompilers.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="reaching-a-verdict"&gt;Reaching a verdict&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The evaluation process begins with a triage, where automated reverse engineering tools identify the file type, its structure, and potential areas of interest. From there, the system reconstructs the software’s control flow graph using frameworks such as angr&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Ghidra&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, building a graph that forms the backbone of Project Ire’s memory model and guides the rest of the analysis.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Through iterative function analysis, the LLM calls specialized tools through an API to identify and summarize key functions. Each result feeds into a “chain of evidence,” a detailed, auditable trail that shows how the system reached its conclusion. This traceable evidence log supports secondary review by security teams and helps refine the system in cases of misclassification.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To verify its findings, Project Ire can invoke a validator tool that cross-checks claims in the report against the chain of evidence. This tool draws on expert statements from malware reverse engineers on the Project Ire team. Drawing on this evidence and its internal model, the system creates a final report and classifies the sample as malicious or benign.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Event Series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum"&gt;Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="preliminary-testing-shows-promise"&gt;Preliminary testing shows promise&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Two early evaluations tested Project Ire’s effectiveness as an autonomous malware classifier. In the first, we assessed Project Ire on a dataset of publicly accessible Windows drivers, some known to be malicious, others benign. Malicious samples came from the &lt;em&gt;Living off the Land Drivers&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; database, which includes a collection of Windows drivers used by attackers to bypass security controls, while known benign drivers were sourced from Windows Update.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This classifier performed well, correctly identifying 90% of all files and flagging only 2% of benign files as threats. It achieved a precision of 0.98 and a recall of 0.83. This low false-positive rate suggests clear potential for deployment in security operations, alongside expert reverse engineering reviews.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For each file it analyzes, Project Ire generates a report that includes an evidence section, summaries of all examined code functions, and other technical artifacts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Figures 1 and 2 present reports for two successful malware classification cases generated during testing. The first involves a kernel-level rootkit, &lt;em&gt;Trojan:Win64/Rootkit.EH!MTB&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The system identified several key features, including jump-hooking, process termination, and web-based command and control. It then correctly flagged the sample as malicious.&lt;/p&gt;





  
  Figure 1 Analysis
  


  &lt;div class="code-block"&gt;
    &lt;p&gt;The binary contains a function named ‘MonitorAndTerminateExplorerThread_16f64’ that runs an infinite loop waiting on synchronization objects and terminates system threads upon certain conditions. It queries system or process information, iterates over processes comparing their names case-insensitively to ‘Explorer.exe’, and manipulates registry values related to ‘Explorer.exe’. This function appears to monitor and potentially terminate or manipulate the ‘Explorer.exe’ process, a critical Windows shell process. Such behavior is suspicious and consistent with malware that aims to disrupt or control system processes.&lt;/p&gt;
    &lt;p&gt;Another function, ‘HttpGetRequestAndResponse_174a4’, performs HTTP GET requests by parsing URLs, resolving hostnames, opening sockets, sending requests, and reading responses. This network communication capability could be leveraged for command and control or data exfiltration, common in malware.&lt;/p&gt;
    &lt;p&gt;The binary also includes a function ‘PatchProcessEntryPointWithHook_12b5c’ that patches the entry point of a process by writing a hook or trampoline that redirects execution to a specified address. This technique is commonly used for process injection or hooking, allowing malware to alter process behavior or inject malicious code.&lt;/p&gt;
    &lt;p&gt;Other functions related to sending IOCTL requests to device drivers were identified, but their maliciousness could not be conclusively determined without additional context.&lt;/p&gt;
    &lt;p&gt;Overall, the binary exhibits multiple indicators of malicious behavior, including process manipulation, network communication, and code injection techniques, suggesting it is likely malware designed to interfere with system processes and communicate with remote servers.&lt;/p&gt;
  &lt;/div&gt;


&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Project Ire report, sample with SHA256: 86047bb1969d1db455493955fd450d18c62a3f36294d0a6c3732c88dfbcc4f62&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The second sample, &lt;em&gt;HackTool:Win64/KillAV!MTB&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, was designed to disable antivirus software. Project Ire correctly identified the code that locates and disables antivirus programs, providing evidence that the file was malicious.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In one section of the code, however, the system misidentified a function as anti-debugging behavior. To maintain accuracy, the system used the validator tool to flag the claim as unsupported. The issue was later resolved by updating decompiler rules, but this example illustrates how Project Ire navigates uncertainty during analysis. Figure 2 shows the corresponding report.&amp;nbsp;&lt;/p&gt;





  
  Figure 2 Analysis
  


  &lt;div class="code-block"&gt;
    &lt;p&gt;The binary contains several functions indicative of malicious intent. The function register_and_log_known_processes_140001000 logs and registers process names associated with antivirus and security software, such as ‘avp.exe’, ‘avpui.exe’, and ‘360Tray.exe’. It calls another function, TerminateProcessesByNameSubstring_1400010f4, which enumerates system processes and terminates those whose names contain specified substrings. This behavior is typical of malware attempting to disable or evade security software by killing their processes.&lt;/p&gt;
    &lt;p&gt;Another function, check_and_handle_special_state_14000502c, performs checks on a global variable and triggers software interrupts if certain conditions are not met. While the exact purpose of these interrupts (int 0x29 and int 0x3) is unclear, they could represent an anti-debug or anti-analysis mechanism to detect or interfere with debugging or tampering attempts. However, this assumption could not be fully validated against expert statements.&lt;/p&gt;
    &lt;p&gt;Other functions include initialization routines and simple logging wrappers, but the core malicious behavior centers on process termination targeting security software. This indicates the binary is designed to compromise system security by disabling protective processes, a hallmark of malware such as trojans or rootkits.&lt;/p&gt;
  &lt;/div&gt;


&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Project Ire report, sample with SHA256: b6cb163089f665c05d607a465f1b6272cdd5c949772ab9ce7227120cf61f971a&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="real-world-evaluation-with-microsoft-defender"&gt;Real-world evaluation with Microsoft Defender&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The more demanding test involved nearly 4,000 “hard-target” files not classified by automated systems and slated for manual review by expert reverse engineers.&lt;/p&gt;



&lt;p&gt;In this real-world scenario, Project Ire operated fully autonomously on files created after the language models’ training cutoff, files that no other automated tools at Microsoft could classify at the time.&lt;/p&gt;



&lt;p&gt;The system achieved a high precision score of 0.89, meaning nearly 9 out of 10 files flagged malicious were correctly identified as malicious. Recall was 0.26, indicating that under these challenging conditions, the system detected roughly a quarter of all actual malware.&lt;/p&gt;



&lt;p&gt;The system correctly identified many of the malicious files, with few false alarms, just a 4% false positive rate. While overall performance was moderate, this combination of accuracy and a low error rate suggests real potential for future deployment.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead"&gt;Looking ahead&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Based on these early successes, the Project Ire prototype will be leveraged inside Microsoft’s Defender organization as &lt;em&gt;Binary Analyzer&lt;/em&gt; for threat detection and software classification.&lt;/p&gt;



&lt;p&gt;Our goal is to scale the system’s speed and accuracy so that it can correctly classify files from any source, even on first encounter. Ultimately, our vision is to detect novel malware directly in memory, at scale.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Project Ire acknowledges the following additional developers that contributed to the results in this publication: Dayenne de Souza, Raghav Pande, Ryan Terry, Shauharda Khadka, and Bob Fleck for their independent review of the system.&lt;/p&gt;



&lt;p&gt;The system incorporates multiple tools, including the&amp;nbsp;angr&amp;nbsp;framework developed by&amp;nbsp;Emotion Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Microsoft has collaborated extensively with Emotion Labs, a pioneer in cyber autonomy, throughout the development of Project Ire, and thanks them for the innovations and insights that contributed to the successes reported here.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/project-ire-autonomously-identifies-malware-at-scale/</guid><pubDate>Tue, 05 Aug 2025 16:00:00 +0000</pubDate></item><item><title>No Backdoors. No Kill Switches. No Spyware. (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/no-backdoors-no-kill-switches-no-spyware/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA GPUs are at the heart of modern computing. They’re used across industries — from healthcare and finance to scientific research, autonomous systems and AI infrastructure. NVIDIA GPUs are embedded into CT scanners and MRI machines, DNA sequencers, air-traffic radar tracking systems, city traffic-management systems, self-driving cars, supercomputers, TV broadcasting systems, casino machines and game consoles.&lt;/p&gt;
&lt;p&gt;To mitigate the risk of misuse, some pundits and policymakers propose requiring hardware “kill switches” or built-in controls that can remotely disable GPUs without user knowledge and consent. Some suspect they might already exist.&lt;/p&gt;
&lt;p&gt;NVIDIA GPUs do not and should not have kill switches and backdoors.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hard-Coded, Single-Point Controls Are Always a Bad Idea&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA has been designing processors for over 30 years. Embedding backdoors and kill switches into chips would be a gift to hackers and hostile actors. It would undermine global digital infrastructure and fracture trust in U.S. technology. Established law wisely requires companies to fix vulnerabilities — not create them.&lt;/p&gt;
&lt;p&gt;Until recently, that policy was universally held and beyond question. When security researchers discovered vulnerabilities such as “Spectre” and “Meltdown” for CPUs, governments and industry responded with speed and unity to eliminate the risk.&lt;/p&gt;
&lt;p&gt;That principle still holds. There is no such thing as a “good” secret backdoor — only dangerous vulnerabilities that need to be eliminated. Product security must always be done the right way: through rigorous internal testing, independent validation and full compliance with global cybersecurity standards. Robust security is built on the principle of “defense in depth”: layering multiple safeguards so that no single-point vulnerability can compromise or shut down a system. For decades, that’s how NVIDIA and American industry have promoted innovation while protecting users and growing the economy. This is no time to depart from that winning formula.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Historical Lessons: The Clipper Chip Debacle &lt;/b&gt;&lt;strong&gt;—&lt;/strong&gt;&lt;b&gt; a Policy and Technical Failure&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The cybersecurity community learned these lessons the hard way during the 1990s with the NSA’s Clipper Chip initiative. Introduced in 1993, the Clipper Chip was designed to provide strong encryption while maintaining government backdoor access through a key escrow system.&lt;/p&gt;
&lt;p&gt;The Clipper Chip represented everything wrong with built-in backdoors. Security researchers discovered fundamental flaws in the system that could allow malicious parties to tamper with the software. It created centralized vulnerabilities that could be exploited by adversaries. The mere existence of government backdoors undermined user confidence in the security of systems.&lt;/p&gt;
&lt;p&gt;Kill switches and built-in backdoors create single points of failure and violate the fundamental principles of cybersecurity.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Promote Smart Software Tools, Not Dangerous Hardware Traps&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Some point to smartphone features like “find my phone” or “remote wipe” as models for a GPU kill switch. That comparison doesn’t hold water — optional software features, controlled by the user, are not hardware backdoors.&lt;/p&gt;
&lt;p&gt;NVIDIA has always supported open, transparent software that helps customers get the most from their GPU-powered systems — diagnostics, performance monitoring, bug reporting and timely patching — with the user’s knowledge and consent. That’s responsible, secure computing. It helps our customers excel, and industry stay ahead.&lt;/p&gt;
&lt;p&gt;Hardwiring a kill switch into a chip is something entirely different: a permanent flaw beyond user control, and an open invitation for disaster. It’s like buying a car where the dealership keeps a remote control for the parking brake — just in case they decide you shouldn’t be driving. That’s not sound policy. It’s an overreaction that would irreparably harm America’s economic and national security interests.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hardware Integrity Should Be Nonpartisan and Nonnegotiable&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For decades, policymakers have championed industry’s efforts to create secure, trustworthy hardware. Governments have many tools to protect nations, consumers and the economy. Deliberately weakening critical infrastructure should never be one of them.&lt;/p&gt;
&lt;p&gt;There are no back doors in NVIDIA chips. No kill switches. No spyware. That’s not how trustworthy systems are built — and never will be.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA GPUs are at the heart of modern computing. They’re used across industries — from healthcare and finance to scientific research, autonomous systems and AI infrastructure. NVIDIA GPUs are embedded into CT scanners and MRI machines, DNA sequencers, air-traffic radar tracking systems, city traffic-management systems, self-driving cars, supercomputers, TV broadcasting systems, casino machines and game consoles.&lt;/p&gt;
&lt;p&gt;To mitigate the risk of misuse, some pundits and policymakers propose requiring hardware “kill switches” or built-in controls that can remotely disable GPUs without user knowledge and consent. Some suspect they might already exist.&lt;/p&gt;
&lt;p&gt;NVIDIA GPUs do not and should not have kill switches and backdoors.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hard-Coded, Single-Point Controls Are Always a Bad Idea&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA has been designing processors for over 30 years. Embedding backdoors and kill switches into chips would be a gift to hackers and hostile actors. It would undermine global digital infrastructure and fracture trust in U.S. technology. Established law wisely requires companies to fix vulnerabilities — not create them.&lt;/p&gt;
&lt;p&gt;Until recently, that policy was universally held and beyond question. When security researchers discovered vulnerabilities such as “Spectre” and “Meltdown” for CPUs, governments and industry responded with speed and unity to eliminate the risk.&lt;/p&gt;
&lt;p&gt;That principle still holds. There is no such thing as a “good” secret backdoor — only dangerous vulnerabilities that need to be eliminated. Product security must always be done the right way: through rigorous internal testing, independent validation and full compliance with global cybersecurity standards. Robust security is built on the principle of “defense in depth”: layering multiple safeguards so that no single-point vulnerability can compromise or shut down a system. For decades, that’s how NVIDIA and American industry have promoted innovation while protecting users and growing the economy. This is no time to depart from that winning formula.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Historical Lessons: The Clipper Chip Debacle &lt;/b&gt;&lt;strong&gt;—&lt;/strong&gt;&lt;b&gt; a Policy and Technical Failure&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The cybersecurity community learned these lessons the hard way during the 1990s with the NSA’s Clipper Chip initiative. Introduced in 1993, the Clipper Chip was designed to provide strong encryption while maintaining government backdoor access through a key escrow system.&lt;/p&gt;
&lt;p&gt;The Clipper Chip represented everything wrong with built-in backdoors. Security researchers discovered fundamental flaws in the system that could allow malicious parties to tamper with the software. It created centralized vulnerabilities that could be exploited by adversaries. The mere existence of government backdoors undermined user confidence in the security of systems.&lt;/p&gt;
&lt;p&gt;Kill switches and built-in backdoors create single points of failure and violate the fundamental principles of cybersecurity.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Promote Smart Software Tools, Not Dangerous Hardware Traps&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Some point to smartphone features like “find my phone” or “remote wipe” as models for a GPU kill switch. That comparison doesn’t hold water — optional software features, controlled by the user, are not hardware backdoors.&lt;/p&gt;
&lt;p&gt;NVIDIA has always supported open, transparent software that helps customers get the most from their GPU-powered systems — diagnostics, performance monitoring, bug reporting and timely patching — with the user’s knowledge and consent. That’s responsible, secure computing. It helps our customers excel, and industry stay ahead.&lt;/p&gt;
&lt;p&gt;Hardwiring a kill switch into a chip is something entirely different: a permanent flaw beyond user control, and an open invitation for disaster. It’s like buying a car where the dealership keeps a remote control for the parking brake — just in case they decide you shouldn’t be driving. That’s not sound policy. It’s an overreaction that would irreparably harm America’s economic and national security interests.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hardware Integrity Should Be Nonpartisan and Nonnegotiable&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For decades, policymakers have championed industry’s efforts to create secure, trustworthy hardware. Governments have many tools to protect nations, consumers and the economy. Deliberately weakening critical infrastructure should never be one of them.&lt;/p&gt;
&lt;p&gt;There are no back doors in NVIDIA chips. No kill switches. No spyware. That’s not how trustworthy systems are built — and never will be.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/no-backdoors-no-kill-switches-no-spyware/</guid><pubDate>Tue, 05 Aug 2025 16:03:36 +0000</pubDate></item><item><title>Google’s NotebookLM is now available to younger users as competition in the AI education space intensifies (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/googles-notebooklm-is-now-available-to-younger-users-as-competition-in-the-ai-education-space-intensifies/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/01/GettyImages-1225201409.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s AI note-taking app is now open to younger users, having previously been limited to users 18 and older. The tech giant announced that NotebookLM is available to Google Workspace for Education users of any age and for consumers ages 13 and up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The removal of age restrictions is intended to provide younger students with access to the AI research tool, allowing them to better understand their class materials. Now, students can access features such as the ability to convert notes into podcast-like Audio Overviews, visually summarize ideas with interactive Mind Maps, and more. NotebookLM recently released Video Overviews to let users turn notes, PDFs, and images into visual presentations.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This expansion comes amid increasing concerns about the use of AI in education regarding data privacy and potential misuse. Google says that NotebookLM enforces stricter content policies for users under 18 to prevent inappropriate responses, and user chats and uploads are not reviewed by humans or used for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The availability of NotebookLM for younger users follows OpenAI’s introduction of a study mode for ChatGPT, indicating that companies are ramping up competition in the AI education sector.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/01/GettyImages-1225201409.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s AI note-taking app is now open to younger users, having previously been limited to users 18 and older. The tech giant announced that NotebookLM is available to Google Workspace for Education users of any age and for consumers ages 13 and up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The removal of age restrictions is intended to provide younger students with access to the AI research tool, allowing them to better understand their class materials. Now, students can access features such as the ability to convert notes into podcast-like Audio Overviews, visually summarize ideas with interactive Mind Maps, and more. NotebookLM recently released Video Overviews to let users turn notes, PDFs, and images into visual presentations.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This expansion comes amid increasing concerns about the use of AI in education regarding data privacy and potential misuse. Google says that NotebookLM enforces stricter content policies for users under 18 to prevent inappropriate responses, and user chats and uploads are not reviewed by humans or used for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The availability of NotebookLM for younger users follows OpenAI’s introduction of a study mode for ChatGPT, indicating that companies are ramping up competition in the AI education sector.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/googles-notebooklm-is-now-available-to-younger-users-as-competition-in-the-ai-education-space-intensifies/</guid><pubDate>Tue, 05 Aug 2025 16:03:39 +0000</pubDate></item><item><title>The EU AI Act aims to create a level playing field for AI innovation: Here’s what it is (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/the-eu-ai-act-aims-to-create-a-level-playing-field-for-ai-innovation-heres-what-it-is/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/11/GettyImages-1146371917.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The European Union’s Artificial Intelligence Act, known as the EU AI Act, has been described by the European Commission as “the world’s first comprehensive AI law.” After years in the making, it is progressively becoming a part of reality for the 450 million people living in the 27 countries that comprise the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EU AI Act, however, is more than a European affair. It applies to companies both local and foreign, and it can affect both providers and deployers of AI systems; the European Commission cites examples of how it would apply to a developer of a CV screening tool and to a bank that buys that tool. Now all of these parties have a legal framework that sets the stage for their use of AI.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-does-the-eu-ai-act-exist"&gt;Why does the EU AI Act exist?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As usual with EU legislation, the EU AI Act exists to make sure there is a uniform legal framework applying to a certain topic across EU countries — the topic this time being AI. Now that the regulation is in place, it should “ensure the free movement, cross-border, of AI-based goods and services” without diverging local restrictions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With timely regulation, the EU seeks to create a level playing field across the region and foster trust, which could also create opportunities for emerging companies. However, the common framework that it has adopted is not exactly permissive: Despite the relatively early stage of widespread AI adoption in most sectors, the EU AI Act sets a high bar for what AI should and shouldn’t do for society more broadly.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-the-purpose-of-the-eu-ai-act"&gt;What is the purpose of the EU AI Act?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;According to European lawmakers, the framework’s main goal is to “promote the uptake of human centric and trustworthy AI while ensuring a high level of protection of health, safety, fundamental rights as enshrined in the Charter of Fundamental Rights of the European Union, including democracy, the rule of law and environmental protection, to protect against the harmful effects of AI systems in the Union, and to support innovation.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yes, that’s quite a mouthful, but it’s worth parsing carefully. First, because a lot will depend on how you define “human centric” and “trustworthy” AI. And second, because it gives a good sense of the precarious balance to maintain between&amp;nbsp;diverging goals: innovation vs. harm prevention, as well as uptake of AI vs. environmental protection. As usual with EU legislation, again, the devil will be in the details.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-the-eu-ai-act-balance-its-different-goals"&gt;How does the EU AI Act balance its different goals?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;To balance harm prevention against the potential benefits of AI, the EU AI Act adopted a risk-based approach: banning a handful of “unacceptable risk” use cases; flagging a set of “high-risk” uses calling for tight regulation; and applying lighter obligations to “limited risk” scenarios.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-has-the-eu-ai-act-come-into-effect"&gt;Has the EU AI Act come into effect?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Yes and no. The EU AI Act rollout started on August 1, 2024, but it will only come into force through a series of staggered compliance deadlines. In most cases, it will also apply sooner to new entrants than to companies that already offer AI products and services in the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first deadline came into effect on February 2, 2025, and focused on enforcing bans on a small number of prohibited uses of AI, such as untargeted scraping of internet or CCTV for facial images to build up or expand databases. Many others will follow, but unless the schedule changes, most provisions will apply by mid-2026.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-changed-on-august-2-2025"&gt;What changed on August 2, 2025?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Since August 2, 2025, the EU AI Act applies to “general-purpose AI models with systemic risk.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPAI (general-purpose AI) models are AI models trained with a large amount of data, and that can be used for a wide range of tasks. That’s where the risk element comes in. According to the EU AI Act, GPAI models can come with systemic risks —  “for example, through the lowering of barriers for chemical or biological weapons development, or unintended issues of control over autonomous [GPAI] models.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ahead of the deadline, the EU published guidelines for providers of GPAI models, which include both European companies and non-European players such as Anthropic, Google, Meta, and OpenAI. But since these companies already have models on the market, they will also have until August 2, 2027, to comply, unlike new entrants.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-does-the-eu-ai-act-have-teeth"&gt;Does the EU AI Act have teeth?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The EU AI Act comes with penalties that lawmakers wanted to be simultaneously “effective, proportionate and dissuasive” — even for large global players.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Details will be laid down by EU countries, but the regulation sets out the overall spirit — that penalties will vary depending on the deemed risk level — as well as thresholds for each level. Infringement on prohibited AI applications leads to the highest penalty of “up to €35 million or 7% of the total worldwide annual turnover of the preceding financial year (whichever is higher).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The European Commission can also inflict fines of up to €15 million or 3% of annual turnover on providers of GPAI models.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-fast-do-existing-players-intend-to-comply"&gt;How fast do existing players intend to comply?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The voluntary GPAI code of practice, including commitments such as not training models on pirated content, is a good indicator of how companies may engage with the framework law until forced to do so.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July 2025, Meta announced it wouldn’t sign the voluntary GPAI code of practice meant to help such providers comply with the EU AI Act. However, Google soon after confirmed it would sign, despite reservations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Signatories so far include Aleph Alpha, Amazon, Anthropic, Cohere, Google, IBM, Microsoft, Mistral AI, and OpenAI, among others. But as we have seen with Google’s example, signing does not equal a full-on endorsement.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-have-some-tech-companies-been-fighting-these-rules-nbsp"&gt;Why have (some) tech companies been fighting these rules?&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While stating in a blog post that Google would sign the voluntary GPAI code of practice, its president of global affairs, Kent Walker, still had reservations. “We remain concerned that the AI Act and Code risk slowing Europe’s development and deployment of AI,” he wrote.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta was more radical, with its chief global affairs officer Joel Kaplan stating in a post on LinkedIn that “Europe is heading down the wrong path on AI.” Calling the EU’s implementation of the AI Act “overreach,” he stated that the code of practice “introduces a number of legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;European companies have expressed concerns as well. Arthur Mensch, the CEO of French AI champion Mistral AI, was part of a group of European CEOs who signed an open letter in July 2025 urging Brussels to “stop the clock” for two years before key obligations of the EU AI Act came into force.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-will-the-schedule-change"&gt;Will the schedule change?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In early July 2025, the European Union responded negatively to lobbying efforts calling for a pause, saying it would still stick to its timeline for implementing the EU AI Act. It went ahead with the August 2, 2025, deadline as planned, and we will update this story if anything changes.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/11/GettyImages-1146371917.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The European Union’s Artificial Intelligence Act, known as the EU AI Act, has been described by the European Commission as “the world’s first comprehensive AI law.” After years in the making, it is progressively becoming a part of reality for the 450 million people living in the 27 countries that comprise the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EU AI Act, however, is more than a European affair. It applies to companies both local and foreign, and it can affect both providers and deployers of AI systems; the European Commission cites examples of how it would apply to a developer of a CV screening tool and to a bank that buys that tool. Now all of these parties have a legal framework that sets the stage for their use of AI.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-does-the-eu-ai-act-exist"&gt;Why does the EU AI Act exist?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As usual with EU legislation, the EU AI Act exists to make sure there is a uniform legal framework applying to a certain topic across EU countries — the topic this time being AI. Now that the regulation is in place, it should “ensure the free movement, cross-border, of AI-based goods and services” without diverging local restrictions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With timely regulation, the EU seeks to create a level playing field across the region and foster trust, which could also create opportunities for emerging companies. However, the common framework that it has adopted is not exactly permissive: Despite the relatively early stage of widespread AI adoption in most sectors, the EU AI Act sets a high bar for what AI should and shouldn’t do for society more broadly.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-the-purpose-of-the-eu-ai-act"&gt;What is the purpose of the EU AI Act?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;According to European lawmakers, the framework’s main goal is to “promote the uptake of human centric and trustworthy AI while ensuring a high level of protection of health, safety, fundamental rights as enshrined in the Charter of Fundamental Rights of the European Union, including democracy, the rule of law and environmental protection, to protect against the harmful effects of AI systems in the Union, and to support innovation.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yes, that’s quite a mouthful, but it’s worth parsing carefully. First, because a lot will depend on how you define “human centric” and “trustworthy” AI. And second, because it gives a good sense of the precarious balance to maintain between&amp;nbsp;diverging goals: innovation vs. harm prevention, as well as uptake of AI vs. environmental protection. As usual with EU legislation, again, the devil will be in the details.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-the-eu-ai-act-balance-its-different-goals"&gt;How does the EU AI Act balance its different goals?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;To balance harm prevention against the potential benefits of AI, the EU AI Act adopted a risk-based approach: banning a handful of “unacceptable risk” use cases; flagging a set of “high-risk” uses calling for tight regulation; and applying lighter obligations to “limited risk” scenarios.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-has-the-eu-ai-act-come-into-effect"&gt;Has the EU AI Act come into effect?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Yes and no. The EU AI Act rollout started on August 1, 2024, but it will only come into force through a series of staggered compliance deadlines. In most cases, it will also apply sooner to new entrants than to companies that already offer AI products and services in the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first deadline came into effect on February 2, 2025, and focused on enforcing bans on a small number of prohibited uses of AI, such as untargeted scraping of internet or CCTV for facial images to build up or expand databases. Many others will follow, but unless the schedule changes, most provisions will apply by mid-2026.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-changed-on-august-2-2025"&gt;What changed on August 2, 2025?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Since August 2, 2025, the EU AI Act applies to “general-purpose AI models with systemic risk.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPAI (general-purpose AI) models are AI models trained with a large amount of data, and that can be used for a wide range of tasks. That’s where the risk element comes in. According to the EU AI Act, GPAI models can come with systemic risks —  “for example, through the lowering of barriers for chemical or biological weapons development, or unintended issues of control over autonomous [GPAI] models.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ahead of the deadline, the EU published guidelines for providers of GPAI models, which include both European companies and non-European players such as Anthropic, Google, Meta, and OpenAI. But since these companies already have models on the market, they will also have until August 2, 2027, to comply, unlike new entrants.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-does-the-eu-ai-act-have-teeth"&gt;Does the EU AI Act have teeth?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The EU AI Act comes with penalties that lawmakers wanted to be simultaneously “effective, proportionate and dissuasive” — even for large global players.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Details will be laid down by EU countries, but the regulation sets out the overall spirit — that penalties will vary depending on the deemed risk level — as well as thresholds for each level. Infringement on prohibited AI applications leads to the highest penalty of “up to €35 million or 7% of the total worldwide annual turnover of the preceding financial year (whichever is higher).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The European Commission can also inflict fines of up to €15 million or 3% of annual turnover on providers of GPAI models.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-fast-do-existing-players-intend-to-comply"&gt;How fast do existing players intend to comply?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The voluntary GPAI code of practice, including commitments such as not training models on pirated content, is a good indicator of how companies may engage with the framework law until forced to do so.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July 2025, Meta announced it wouldn’t sign the voluntary GPAI code of practice meant to help such providers comply with the EU AI Act. However, Google soon after confirmed it would sign, despite reservations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Signatories so far include Aleph Alpha, Amazon, Anthropic, Cohere, Google, IBM, Microsoft, Mistral AI, and OpenAI, among others. But as we have seen with Google’s example, signing does not equal a full-on endorsement.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-have-some-tech-companies-been-fighting-these-rules-nbsp"&gt;Why have (some) tech companies been fighting these rules?&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While stating in a blog post that Google would sign the voluntary GPAI code of practice, its president of global affairs, Kent Walker, still had reservations. “We remain concerned that the AI Act and Code risk slowing Europe’s development and deployment of AI,” he wrote.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta was more radical, with its chief global affairs officer Joel Kaplan stating in a post on LinkedIn that “Europe is heading down the wrong path on AI.” Calling the EU’s implementation of the AI Act “overreach,” he stated that the code of practice “introduces a number of legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;European companies have expressed concerns as well. Arthur Mensch, the CEO of French AI champion Mistral AI, was part of a group of European CEOs who signed an open letter in July 2025 urging Brussels to “stop the clock” for two years before key obligations of the EU AI Act came into force.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-will-the-schedule-change"&gt;Will the schedule change?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In early July 2025, the European Union responded negatively to lobbying efforts calling for a pause, saying it would still stick to its timeline for implementing the EU AI Act. It went ahead with the August 2, 2025, deadline as planned, and we will update this story if anything changes.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/the-eu-ai-act-aims-to-create-a-level-playing-field-for-ai-innovation-heres-what-it-is/</guid><pubDate>Tue, 05 Aug 2025 16:24:10 +0000</pubDate></item><item><title>Some people are defending Perplexity after Cloudflare ‘named and shamed’ it (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/some-people-are-defending-perplexity-after-cloudflare-named-and-shamed-it/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/45A2342_VGAEbHsG.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Cloudflare accused AI search engine Perplexity of stealthily scraping websites on Monday, while ignoring a site’s specific methods to block it, this wasn’t a clear-cut case of an AI web crawler gone wild.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Many people came to Perplexity’s defense. They argued that Perplexity accessing sites in defiance of the website owner’s wishes, while controversial, is acceptable. And this is a controversy that will certainly grow as AI agents flood the internet: Should an agent accessing a website on behalf of its user be treated like a bot? Or like a human making the same request?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cloudflare is known for providing anti-bot crawling and other web security services to millions of websites. Essentially, Cloudflare’s test case involved setting up a new website with a new domain that had never been crawled by any bot, setting up a robots.txt file that specifically blocked Perplexity’s known AI crawling bots, and then asking Perplexity about the website’s content.&amp;nbsp;And Perplexity answered the question. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare researchers found the AI search engine used “a generic browser intended to impersonate Google Chrome on macOS” when its web crawler itself was blocked. Cloudflare CEO Matthew Prince posted the research on X, writing, “Some supposedly ‘reputable’ AI companies act more like North Korean hackers. Time to name, shame, and hard block them.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But many people disagreed with Prince’s assessment that this was actual bad behavior. Those defending Perplexity on sites like X and Hacker News pointed out that what Cloudflare seemed to document was the AI accessing a specific public website when its user asked about that specific website.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If I as a human request a website, then I should be shown the content,” one person on Hacker News wrote, adding, “why would the LLM accessing the website on my behalf be in a different legal category as my Firefox web browser?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Perplexity spokesperson previously denied to TechCrunch that the bots were the company’s and called Cloudflare’s blog post a sales pitch for Cloudflare. Then on Tuesday, Perplexity published a blog in its defense (and generally attacking Cloudflare), claiming the behavior was from a third-party service it uses occasionally.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But the crux of Perplexity’s post made a similar appeal as its online defenders did.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The difference between automated crawling and user-driven fetching isn’t just technical — it’s about who gets to access information on the open web,” the post said. “This controversy reveals that Cloudflare’s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity’s accusations aren’t exactly fair, either. One argument that Prince and Cloudflare used for calling out Perplexity’s methods was that OpenAI doesn’t behave in the same way.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“OpenAI is an example of a leading AI company that follows these best practices,” Cloudflare wrote.&amp;nbsp;“They respect robots.txt and do not try to evade either a robots.txt directive or a network level block. And ChatGPT Agent is signing http requests using the newly proposed open standard Web Bot Auth.” &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Web Bot Auth is a Cloudflare-supported standard being developed by the Internet Engineering Task Force that hopes to create a cryptographic method for identifying AI agent web requests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The debate comes as bot activity reshapes the internet. As TechCrunch has previously reported, bots seeking to scrape massive amounts of content to train AI models have become a menace, especially to smaller sites.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the first time in the internet’s history, bot activity is currently outstripping human activity online, with AI traffic accounting for over 50%, according to Imperva’s Bad Bot report released last month. Most of that activity is coming from LLMs. But the report also found that malicious bots now make up 37% of all internet traffic. That’s activity that includes everything from persistent scraping to unauthorized login attempts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until LLMs, the internet generally accepted that websites could and should block most bot activity given how often it was malicious by using CAPTCHAs and other services (such as Cloudflare). Websites also had a clear incentive to work with specific good actors, such as Googlebot, guiding it on what not to index through robots.txt. Google indexed the internet, which sent traffic to sites. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, LLMs are eating an increasing amount of that traffic. Gartner predicts that search engine volume will drop by 25% by 2026.&amp;nbsp;Right now humans tend to click website links from LLMs at the point they are most valuable to the website, which is when they are ready to conduct a transaction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if humans adopt agents as the tech industry predicts they will — to arrange our travel, book our dinner reservations, and shop for us — would websites hurt their business interests by blocking them? The debate on X captured the dilemma perfectly:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I WANT perplexity to visit any public content on my behalf when I give it a request/task!” wrote one person in response to Cloudflare calling out Perplexity. &lt;/p&gt;&lt;p&gt;“What if the site owners don’t want it? they just want you [to] directly visit the home, see their stuff” argued another, pointing out that the site owner who created the content wants the traffic and potential ad revenue, not to let Perplexity take it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is why I can’t see ‘agentic browsing’ really working — much harder problem than people think. Most website owners will just block,” a third predicted.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/45A2342_VGAEbHsG.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Cloudflare accused AI search engine Perplexity of stealthily scraping websites on Monday, while ignoring a site’s specific methods to block it, this wasn’t a clear-cut case of an AI web crawler gone wild.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Many people came to Perplexity’s defense. They argued that Perplexity accessing sites in defiance of the website owner’s wishes, while controversial, is acceptable. And this is a controversy that will certainly grow as AI agents flood the internet: Should an agent accessing a website on behalf of its user be treated like a bot? Or like a human making the same request?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cloudflare is known for providing anti-bot crawling and other web security services to millions of websites. Essentially, Cloudflare’s test case involved setting up a new website with a new domain that had never been crawled by any bot, setting up a robots.txt file that specifically blocked Perplexity’s known AI crawling bots, and then asking Perplexity about the website’s content.&amp;nbsp;And Perplexity answered the question. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare researchers found the AI search engine used “a generic browser intended to impersonate Google Chrome on macOS” when its web crawler itself was blocked. Cloudflare CEO Matthew Prince posted the research on X, writing, “Some supposedly ‘reputable’ AI companies act more like North Korean hackers. Time to name, shame, and hard block them.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But many people disagreed with Prince’s assessment that this was actual bad behavior. Those defending Perplexity on sites like X and Hacker News pointed out that what Cloudflare seemed to document was the AI accessing a specific public website when its user asked about that specific website.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If I as a human request a website, then I should be shown the content,” one person on Hacker News wrote, adding, “why would the LLM accessing the website on my behalf be in a different legal category as my Firefox web browser?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Perplexity spokesperson previously denied to TechCrunch that the bots were the company’s and called Cloudflare’s blog post a sales pitch for Cloudflare. Then on Tuesday, Perplexity published a blog in its defense (and generally attacking Cloudflare), claiming the behavior was from a third-party service it uses occasionally.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But the crux of Perplexity’s post made a similar appeal as its online defenders did.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The difference between automated crawling and user-driven fetching isn’t just technical — it’s about who gets to access information on the open web,” the post said. “This controversy reveals that Cloudflare’s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity’s accusations aren’t exactly fair, either. One argument that Prince and Cloudflare used for calling out Perplexity’s methods was that OpenAI doesn’t behave in the same way.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“OpenAI is an example of a leading AI company that follows these best practices,” Cloudflare wrote.&amp;nbsp;“They respect robots.txt and do not try to evade either a robots.txt directive or a network level block. And ChatGPT Agent is signing http requests using the newly proposed open standard Web Bot Auth.” &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Web Bot Auth is a Cloudflare-supported standard being developed by the Internet Engineering Task Force that hopes to create a cryptographic method for identifying AI agent web requests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The debate comes as bot activity reshapes the internet. As TechCrunch has previously reported, bots seeking to scrape massive amounts of content to train AI models have become a menace, especially to smaller sites.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the first time in the internet’s history, bot activity is currently outstripping human activity online, with AI traffic accounting for over 50%, according to Imperva’s Bad Bot report released last month. Most of that activity is coming from LLMs. But the report also found that malicious bots now make up 37% of all internet traffic. That’s activity that includes everything from persistent scraping to unauthorized login attempts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until LLMs, the internet generally accepted that websites could and should block most bot activity given how often it was malicious by using CAPTCHAs and other services (such as Cloudflare). Websites also had a clear incentive to work with specific good actors, such as Googlebot, guiding it on what not to index through robots.txt. Google indexed the internet, which sent traffic to sites. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, LLMs are eating an increasing amount of that traffic. Gartner predicts that search engine volume will drop by 25% by 2026.&amp;nbsp;Right now humans tend to click website links from LLMs at the point they are most valuable to the website, which is when they are ready to conduct a transaction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if humans adopt agents as the tech industry predicts they will — to arrange our travel, book our dinner reservations, and shop for us — would websites hurt their business interests by blocking them? The debate on X captured the dilemma perfectly:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I WANT perplexity to visit any public content on my behalf when I give it a request/task!” wrote one person in response to Cloudflare calling out Perplexity. &lt;/p&gt;&lt;p&gt;“What if the site owners don’t want it? they just want you [to] directly visit the home, see their stuff” argued another, pointing out that the site owner who created the content wants the traffic and potential ad revenue, not to let Perplexity take it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is why I can’t see ‘agentic browsing’ really working — much harder problem than people think. Most website owners will just block,” a third predicted.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/some-people-are-defending-perplexity-after-cloudflare-named-and-shamed-it/</guid><pubDate>Tue, 05 Aug 2025 16:33:03 +0000</pubDate></item><item><title>OpenAI has finally released open-weight language models (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/05/1121092/openai-has-finally-released-open-weight-language-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250805_openaiopenmodel_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;OpenAI has finally released its first open-weight large language models since 2019’s GPT-2. These new “gpt-oss” models are available in two different sizes and score similarly to the company’s o3-mini and o4-mini models on several benchmarks. Unlike the models available through OpenAI’s web interface, these new open models can be freely downloaded, run, and even modified on laptops and other local devices.&lt;/p&gt;  &lt;p&gt;In the company’s many years without an open LLM release, some users have taken to referring to it with the pejorative “ClosedAI.” That sense of frustration had escalated in the past few months as these long-awaited models were delayed twice—first in June and then in July. With their release, however, OpenAI is reestablishing itself as a presence for users of open models.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That’s particularly notable at a time when Meta, which had previously dominated the American open-model landscape with its Llama models, may be reorienting toward closed releases—and when Chinese open models, such as DeepSeek’s offerings, Kimi K2, and Alibaba’s Qwen series, are becoming more popular than their American competitors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“The vast majority of our [enterprise and startup] customers are already using a lot of open models,” said Casey Dvorak, a research program manager at OpenAI, in a media briefing about the model release. “Because there is no [competitive] open model from OpenAI, we wanted to plug that gap and actually allow them to use our technology across the board.”&lt;/p&gt; 
 &lt;p&gt;The new models come in two different sizes, the smaller of which can theoretically run on 16 GB of RAM—the minimum amount that Apple currently offers on its computers. The larger model requires a high-end laptop or specialized hardware.&lt;/p&gt;  &lt;p&gt;Open models have a few key use cases. Some organizations may want to customize models for their own purposes or save money by running models on their own equipment, though that equipment comes at a substantial upfront cost. Others—such hospitals, law firms, and governments—might need models that they can run locally for data security reasons.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;OpenAI has facilitated such activity by releasing its open models under a permissive Apache 2.0 license, which allows the models to be used for commercial purposes. Nathan Lambert, post-training lead at the Allen Institute for AI, says that this choice is commendable: Such licenses are typical for Chinese open-model releases, but Meta released its Llama models under a bespoke, more restrictive license. “It’s a very good thing for the open community,” he says.&lt;/p&gt;  &lt;p&gt;Researchers who study how LLMs work also need open models, so that they can examine and manipulate those models in detail. “In part, this is about reasserting OpenAI’s dominance in the research ecosystem,” says Peter Henderson, an assistant professor at Princeton University who has worked extensively with open models. If researchers do adopt gpt-oss as new workhorses, OpenAI could see some concrete benefits, Henderson says—it might adopt innovations discovered by other researchers into its own model ecosystem.&lt;/p&gt;  &lt;p&gt;More broadly, Lambert says, releasing an open model now could help OpenAI reestablish its status in an increasingly crowded AI environment. “It kind of goes back to years ago, where they were seen as &lt;em&gt;the&lt;/em&gt; AI company,” he says. Users who want to use open models will now have the option to meet all their needs with OpenAI products, rather than turning to Meta’s Llama or Alibaba’s Qwen when they need to run something locally.&lt;/p&gt;  &lt;p&gt;The rise of Chinese open models like Qwen over the past year may have been a particularly salient factor in OpenAI’s calculus. An employee from OpenAI emphasized at the media briefing that the company doesn’t see these open models as a response to actions taken by any other AI company, but OpenAI is clearly attuned to the geopolitical implications of China’s open-model dominance. “Broad access to these capable‬‭ open-weights models created in the US helps expand democratic AI rails,” the company wrote in a blog post announcing the models’ release.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Since DeepSeek exploded onto the AI scene at the start of 2025, observers have noted that Chinese models often refuse to speak about topics that the Chinese Communist Party has deemed verboten, such as Tiananmen Square. Such observations—as well as longer-term risks, like the possibility that agentic models could purposefully write vulnerable code—have made some AI experts concerned about the growing adoption of Chinese models. “Open models are a form of soft power,” Henderson says.&lt;/p&gt;  &lt;p&gt;Lambert released a report on Monday documenting how Chinese models are overtaking American offerings like Llama and advocating for a renewed commitment to domestic open models. Several prominent AI researchers and entrepreneurs, such as HuggingFace CEO Clement Delangue, Stanford’s Percy Liang, and former OpenAI researcher Miles Brundage, have signed on.&lt;/p&gt;  &lt;p&gt;The Trump administration, too, has emphasized development of open models in its AI Action Plan. With both this model release and previous statements, OpenAI is aligning itself with that stance. “In their filings about the action plan, [OpenAI] pretty clearly indicated that they see US–China as a key issue and want to position themselves as very important to the US system,” says Rishi Bommasani, a senior research scholar at the Stanford Institute for Human-Centered Artificial Intelligence.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And OpenAI may see concrete political advantages from aligning with the administration’s AI priorities, Lambert says. As the company continues to build out its extensive computational infrastructure, it will need political support and approvals, and sympathetic leadership could go a long way.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250805_openaiopenmodel_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;OpenAI has finally released its first open-weight large language models since 2019’s GPT-2. These new “gpt-oss” models are available in two different sizes and score similarly to the company’s o3-mini and o4-mini models on several benchmarks. Unlike the models available through OpenAI’s web interface, these new open models can be freely downloaded, run, and even modified on laptops and other local devices.&lt;/p&gt;  &lt;p&gt;In the company’s many years without an open LLM release, some users have taken to referring to it with the pejorative “ClosedAI.” That sense of frustration had escalated in the past few months as these long-awaited models were delayed twice—first in June and then in July. With their release, however, OpenAI is reestablishing itself as a presence for users of open models.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That’s particularly notable at a time when Meta, which had previously dominated the American open-model landscape with its Llama models, may be reorienting toward closed releases—and when Chinese open models, such as DeepSeek’s offerings, Kimi K2, and Alibaba’s Qwen series, are becoming more popular than their American competitors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“The vast majority of our [enterprise and startup] customers are already using a lot of open models,” said Casey Dvorak, a research program manager at OpenAI, in a media briefing about the model release. “Because there is no [competitive] open model from OpenAI, we wanted to plug that gap and actually allow them to use our technology across the board.”&lt;/p&gt; 
 &lt;p&gt;The new models come in two different sizes, the smaller of which can theoretically run on 16 GB of RAM—the minimum amount that Apple currently offers on its computers. The larger model requires a high-end laptop or specialized hardware.&lt;/p&gt;  &lt;p&gt;Open models have a few key use cases. Some organizations may want to customize models for their own purposes or save money by running models on their own equipment, though that equipment comes at a substantial upfront cost. Others—such hospitals, law firms, and governments—might need models that they can run locally for data security reasons.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;OpenAI has facilitated such activity by releasing its open models under a permissive Apache 2.0 license, which allows the models to be used for commercial purposes. Nathan Lambert, post-training lead at the Allen Institute for AI, says that this choice is commendable: Such licenses are typical for Chinese open-model releases, but Meta released its Llama models under a bespoke, more restrictive license. “It’s a very good thing for the open community,” he says.&lt;/p&gt;  &lt;p&gt;Researchers who study how LLMs work also need open models, so that they can examine and manipulate those models in detail. “In part, this is about reasserting OpenAI’s dominance in the research ecosystem,” says Peter Henderson, an assistant professor at Princeton University who has worked extensively with open models. If researchers do adopt gpt-oss as new workhorses, OpenAI could see some concrete benefits, Henderson says—it might adopt innovations discovered by other researchers into its own model ecosystem.&lt;/p&gt;  &lt;p&gt;More broadly, Lambert says, releasing an open model now could help OpenAI reestablish its status in an increasingly crowded AI environment. “It kind of goes back to years ago, where they were seen as &lt;em&gt;the&lt;/em&gt; AI company,” he says. Users who want to use open models will now have the option to meet all their needs with OpenAI products, rather than turning to Meta’s Llama or Alibaba’s Qwen when they need to run something locally.&lt;/p&gt;  &lt;p&gt;The rise of Chinese open models like Qwen over the past year may have been a particularly salient factor in OpenAI’s calculus. An employee from OpenAI emphasized at the media briefing that the company doesn’t see these open models as a response to actions taken by any other AI company, but OpenAI is clearly attuned to the geopolitical implications of China’s open-model dominance. “Broad access to these capable‬‭ open-weights models created in the US helps expand democratic AI rails,” the company wrote in a blog post announcing the models’ release.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Since DeepSeek exploded onto the AI scene at the start of 2025, observers have noted that Chinese models often refuse to speak about topics that the Chinese Communist Party has deemed verboten, such as Tiananmen Square. Such observations—as well as longer-term risks, like the possibility that agentic models could purposefully write vulnerable code—have made some AI experts concerned about the growing adoption of Chinese models. “Open models are a form of soft power,” Henderson says.&lt;/p&gt;  &lt;p&gt;Lambert released a report on Monday documenting how Chinese models are overtaking American offerings like Llama and advocating for a renewed commitment to domestic open models. Several prominent AI researchers and entrepreneurs, such as HuggingFace CEO Clement Delangue, Stanford’s Percy Liang, and former OpenAI researcher Miles Brundage, have signed on.&lt;/p&gt;  &lt;p&gt;The Trump administration, too, has emphasized development of open models in its AI Action Plan. With both this model release and previous statements, OpenAI is aligning itself with that stance. “In their filings about the action plan, [OpenAI] pretty clearly indicated that they see US–China as a key issue and want to position themselves as very important to the US system,” says Rishi Bommasani, a senior research scholar at the Stanford Institute for Human-Centered Artificial Intelligence.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And OpenAI may see concrete political advantages from aligning with the administration’s AI priorities, Lambert says. As the company continues to build out its extensive computational infrastructure, it will need political support and approvals, and sympathetic leadership could go a long way.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/05/1121092/openai-has-finally-released-open-weight-language-models/</guid><pubDate>Tue, 05 Aug 2025 17:00:00 +0000</pubDate></item><item><title>OpenAI returns to open source roots with new models gpt-oss-120b and gpt-oss-20b (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b/</link><description>[unable to retrieve full-text content]Enterprises can use a powerful, near topline OpenAI LLM on their hardware totally privately and securely, without sending data to the cloud.</description><content:encoded>[unable to retrieve full-text content]Enterprises can use a powerful, near topline OpenAI LLM on their hardware totally privately and securely, without sending data to the cloud.</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b/</guid><pubDate>Tue, 05 Aug 2025 17:00:00 +0000</pubDate></item><item><title>OpenAI launches two ‘open’ AI reasoning models (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/openai-launches-two-open-ai-reasoning-models/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Tuesday the launch of two open-weight AI reasoning models with similar capabilities to its o-series. Both are freely available to download from the online developer platform Hugging Face, the company said, describing the models as “state of the art” when measured across several benchmarks for comparing open models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The models come in two sizes: a larger and more capable gpt-oss-120b model that can run on a single Nvidia GPU, and a lighter-weight gpt-oss-20b model that can run on a consumer laptop with 16GB of memory.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch marks OpenAI’s first ‘open’ language model since GPT-2, which was released more than five years ago. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a briefing, OpenAI said its open models will be capable of sending complex queries to AI models in the cloud, as TechCrunch previously reported. That means if OpenAI’s open model is not capable of a certain task, such as processing an image, developers can connect the open model to one of the company’s more capable closed models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI open sourced AI models in its early days, the company has generally favored a proprietary, closed source development approach. The latter strategy has helped OpenAI build a large business selling access to its AI models via an API to enterprises and developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, CEO Sam Altman said in January he believes OpenAI has been “on the wrong side of history” when it comes to open sourcing its technologies. The company today faces growing pressure from Chinese AI labs — including DeepSeek, Alibaba’s Qwen, and Moonshot AI — which have developed several of the world’s most capable and popular open models. (While Meta previously dominated the open AI space, the company’s Llama AI models have fallen behind in the last year.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July, the Trump administration also urged U.S. AI developers to open source more technology to promote global adoption of AI aligned with American values.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;With the release of gpt-oss, OpenAI hopes to curry favor with developers and the Trump administration alike, both of which have watched the Chinese AI labs rise to prominence in the open source space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Going back to when we started in 2015, OpenAI’s mission is to ensure AGI that benefits all of humanity,” said Altman in a statement shared with TechCrunch. “To that end, we are excited for the world to be building on an open AI stack created in the United States, based on democratic values, available for free to all and for wide benefit.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="Open AI CEO Sam Altman" class="wp-image-2993943" height="453" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2197366846.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tomohiro Ohsumi / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-how-the-models-performed"&gt;How the models performed&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI aimed to make its open model a leader among other open-weight AI models, and the company claims to have done just that. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Codeforces (with tools), a competitive coding test, gpt-oss-120b and gpt-oss-20b score 2622 and 2516, respectively, outperforming DeepSeek’s R1 while underperforming o3 and o4-mini.&lt;/p&gt;

&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3034265" height="459" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-12.21.54PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s open model performance on codeforces.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On Humanity’s Last Exam (HLE), a challenging test of crowdsourced questions across a variety of subjects (with tools), gpt-oss-120b and gpt-oss-20b score 19% and 17.3%, respectively. Similarly, this underperforms o3 but outperforms leading open models from DeepSeek and Qwen.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3034259" height="438" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-12.18.20PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s open model performance on HLE.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, OpenAI’s open models hallucinate significantly more than its latest AI reasoning models, o3 and o4-mini. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hallucinations have been getting more severe in OpenAI’s latest AI reasoning models, and the company previously said it doesn’t quite understand why. In a white paper, OpenAI says this is “expected, as smaller models have less world knowledge than larger frontier models and tend to hallucinate more.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI found that gpt-oss-120b and gpt-oss-20b hallucinated in response to 49% and 53%, respectively, of questions on PersonQA, the company’s in-house benchmark for measuring the accuracy of a model’s knowledge about people. That’s more than triple the hallucination rate of OpenAI’s o1 model, which scored 16%, and higher than its o4-mini model, which scored 36%.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-training-the-new-models"&gt;Training the new models&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says its open models were trained with similar processes to its proprietary models. The company says each open model leverages mixture-of-experts (MoE) to tap fewer parameters for any given question, making it run more efficiently. For gpt-oss-120b, which has 117 billion total parameters, OpenAI says the model only activates 5.1 billion parameters per token.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also says its open model was trained using high-compute reinforcement learning (RL) — a post-training process to teach AI models right from wrong in simulated environments using large clusters of Nvidia GPUs. This was also used to train OpenAI’s o-series of models, and the open models have a similar chain-of-thought process in which they take additional time and computational resources to work through their answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of the post-training process, OpenAI says its open AI models excel at powering AI agents and are capable of calling tools such as web search or Python code execution as part of its chain-of-thought process. However, OpenAI says its open models are text-only, meaning they will not be able to process or generate images and audio like the company’s other models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI is releasing gpt-oss-120b and gpt-oss-20b under the Apache 2.0 license, which is generally considered one of the most permissive. This license will allow enterprises to monetize OpenAI’s open models without having to pay or obtain permission from the company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, unlike fully open source offerings from AI labs like AI2, OpenAI says it will not be releasing the training data used to create its open models. This decision is not surprising given that several active lawsuits against AI model providers, including OpenAI, have alleged that these companies inappropriately trained their AI models on copyrighted works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI delayed the release of its open models several times in recent months, partially to address safety concerns. Beyond the company’s typical safety policies, OpenAI says in a white paper that it also investigated whether bad actors could fine-tune its gpt-oss models to be more helpful in cyberattacks or the creation of biological or chemical weapons.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After testing from OpenAI and third-party evaluators, the company says gpt-oss may marginally increase biological capabilities. However, it did not find evidence that these open models could reach its “high capability” threshold for danger in these domains, even after fine-tuning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI’s model appears to be state-of-the-art among open models, developers are eagerly awaiting the release of DeepSeek R2, its next AI reasoning model, as well as a new open model from Meta’s Superintelligence Lab.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Tuesday the launch of two open-weight AI reasoning models with similar capabilities to its o-series. Both are freely available to download from the online developer platform Hugging Face, the company said, describing the models as “state of the art” when measured across several benchmarks for comparing open models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The models come in two sizes: a larger and more capable gpt-oss-120b model that can run on a single Nvidia GPU, and a lighter-weight gpt-oss-20b model that can run on a consumer laptop with 16GB of memory.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch marks OpenAI’s first ‘open’ language model since GPT-2, which was released more than five years ago. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a briefing, OpenAI said its open models will be capable of sending complex queries to AI models in the cloud, as TechCrunch previously reported. That means if OpenAI’s open model is not capable of a certain task, such as processing an image, developers can connect the open model to one of the company’s more capable closed models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI open sourced AI models in its early days, the company has generally favored a proprietary, closed source development approach. The latter strategy has helped OpenAI build a large business selling access to its AI models via an API to enterprises and developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, CEO Sam Altman said in January he believes OpenAI has been “on the wrong side of history” when it comes to open sourcing its technologies. The company today faces growing pressure from Chinese AI labs — including DeepSeek, Alibaba’s Qwen, and Moonshot AI — which have developed several of the world’s most capable and popular open models. (While Meta previously dominated the open AI space, the company’s Llama AI models have fallen behind in the last year.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July, the Trump administration also urged U.S. AI developers to open source more technology to promote global adoption of AI aligned with American values.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;With the release of gpt-oss, OpenAI hopes to curry favor with developers and the Trump administration alike, both of which have watched the Chinese AI labs rise to prominence in the open source space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Going back to when we started in 2015, OpenAI’s mission is to ensure AGI that benefits all of humanity,” said Altman in a statement shared with TechCrunch. “To that end, we are excited for the world to be building on an open AI stack created in the United States, based on democratic values, available for free to all and for wide benefit.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="Open AI CEO Sam Altman" class="wp-image-2993943" height="453" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2197366846.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tomohiro Ohsumi / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-how-the-models-performed"&gt;How the models performed&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI aimed to make its open model a leader among other open-weight AI models, and the company claims to have done just that. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Codeforces (with tools), a competitive coding test, gpt-oss-120b and gpt-oss-20b score 2622 and 2516, respectively, outperforming DeepSeek’s R1 while underperforming o3 and o4-mini.&lt;/p&gt;

&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3034265" height="459" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-12.21.54PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s open model performance on codeforces.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On Humanity’s Last Exam (HLE), a challenging test of crowdsourced questions across a variety of subjects (with tools), gpt-oss-120b and gpt-oss-20b score 19% and 17.3%, respectively. Similarly, this underperforms o3 but outperforms leading open models from DeepSeek and Qwen.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3034259" height="438" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-12.18.20PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s open model performance on HLE.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, OpenAI’s open models hallucinate significantly more than its latest AI reasoning models, o3 and o4-mini. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hallucinations have been getting more severe in OpenAI’s latest AI reasoning models, and the company previously said it doesn’t quite understand why. In a white paper, OpenAI says this is “expected, as smaller models have less world knowledge than larger frontier models and tend to hallucinate more.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI found that gpt-oss-120b and gpt-oss-20b hallucinated in response to 49% and 53%, respectively, of questions on PersonQA, the company’s in-house benchmark for measuring the accuracy of a model’s knowledge about people. That’s more than triple the hallucination rate of OpenAI’s o1 model, which scored 16%, and higher than its o4-mini model, which scored 36%.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-training-the-new-models"&gt;Training the new models&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says its open models were trained with similar processes to its proprietary models. The company says each open model leverages mixture-of-experts (MoE) to tap fewer parameters for any given question, making it run more efficiently. For gpt-oss-120b, which has 117 billion total parameters, OpenAI says the model only activates 5.1 billion parameters per token.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also says its open model was trained using high-compute reinforcement learning (RL) — a post-training process to teach AI models right from wrong in simulated environments using large clusters of Nvidia GPUs. This was also used to train OpenAI’s o-series of models, and the open models have a similar chain-of-thought process in which they take additional time and computational resources to work through their answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of the post-training process, OpenAI says its open AI models excel at powering AI agents and are capable of calling tools such as web search or Python code execution as part of its chain-of-thought process. However, OpenAI says its open models are text-only, meaning they will not be able to process or generate images and audio like the company’s other models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI is releasing gpt-oss-120b and gpt-oss-20b under the Apache 2.0 license, which is generally considered one of the most permissive. This license will allow enterprises to monetize OpenAI’s open models without having to pay or obtain permission from the company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, unlike fully open source offerings from AI labs like AI2, OpenAI says it will not be releasing the training data used to create its open models. This decision is not surprising given that several active lawsuits against AI model providers, including OpenAI, have alleged that these companies inappropriately trained their AI models on copyrighted works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI delayed the release of its open models several times in recent months, partially to address safety concerns. Beyond the company’s typical safety policies, OpenAI says in a white paper that it also investigated whether bad actors could fine-tune its gpt-oss models to be more helpful in cyberattacks or the creation of biological or chemical weapons.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After testing from OpenAI and third-party evaluators, the company says gpt-oss may marginally increase biological capabilities. However, it did not find evidence that these open models could reach its “high capability” threshold for danger in these domains, even after fine-tuning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI’s model appears to be state-of-the-art among open models, developers are eagerly awaiting the release of DeepSeek R2, its next AI reasoning model, as well as a new open model from Meta’s Superintelligence Lab.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/openai-launches-two-open-ai-reasoning-models/</guid><pubDate>Tue, 05 Aug 2025 17:00:00 +0000</pubDate></item><item><title>OpenAI announces two “gpt-oss” open AI models, and you can download them today (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/08/openai-releases-its-first-open-source-models-since-2019/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI's new open models can run on your hardware instead of in the cloud.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI is releasing new generative AI models today, and no, GPT-5 is not one of them. Depending on how you feel about generative AI, these new models may be even more interesting, though. The company is rolling out gpt-oss-120b and gpt-oss-20b, its first open-weight models since the release of GPT-2 in 2019. You can download and run these models on your own hardware, with support for simulated reasoning, tool use, and deep customization.&lt;/p&gt;
&lt;p&gt;When you access the company's proprietary models in the cloud, they're running on powerful server infrastructure that cannot be replicated easily, even in enterprise. The new OpenAI models come in two variants (120b and 20b) to run on less powerful hardware configurations. Both are transformers with a configurable chain of thought (CoT), supporting low, medium, and high settings. The lower settings are faster and use fewer compute resources, but the outputs are better with the highest setting. You can set the CoT level with a single line in the system prompt.&lt;/p&gt;
&lt;p&gt;The smaller gpt-oss-20b has a total of 21 billion parameters, utilizing mixture-of-experts (MoE) to reduce that to 3.6 billion parameters per token. As for gpt-oss-120b, its 117 billion parameters come down to 5.1 billion per token with MoE. The company says the smaller model can run on a consumer-level machine with 16GB or more of memory. To run gpt-oss-120b, you need 80GB of memory, which is more than you're likely to find in the average consumer machine. It should fit on a single AI accelerator GPU like the Nvidia H100, though. Both models have a context window of 128,000 tokens.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2110136 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="894" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/gpt-oss-bench.png" width="705" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The team says users of gpt-oss can expect robust performance similar to its leading cloud-based models. The larger one benchmarks between the o3 and o4-mini proprietary models in most tests, with the smaller version running just a little behind. It gets closest in math and coding tasks. In the knowledge-based Humanity's Last Exam, o3 is far out in front with 24.9 percent (with tools), while gpt-oss-120b only manages 19 percent. For comparison, Google's leading Gemini Deep Think hits 34.8 percent in that test.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Not good at being evil&lt;/h2&gt;
&lt;p&gt;OpenAI says it doesn't intend for anyone to replace its proprietary models with the new OSS releases. It did not set out to replicate what you can do with the mainline GPT releases here, and there are some notable limitations. For example, gpt-oss-120b and gpt-oss-20b are text-only with no multimodality out of the box. However, the company acknowledges there are times when someone might not want to rely on a big cloud-based AI—locally managed AI has lower latency and more opportunities for customization, and it can keep sensitive data secure on site.&lt;/p&gt;
&lt;p&gt;OpenAI is cognizant that many users of the company's proprietary models are also leveraging open source models for these reasons. Currently, those firms are using non-OpenAI products for local AI, but the team designed the gpt-oss models to integrate with the proprietary GPT models. So customers can now use end-to-end OpenAI products even if they need to process some data locally.&lt;/p&gt;
&lt;p&gt;Because these models are fully open and governed by the Apache 2.0 license, developers will be able to tune them for specific use cases. Like all AI firms, OpenAI builds controls into its models to limit malicious behavior, but it's been a few years since the company released an open model—the gpt-oss models are much more powerful than GPT-2 was in 2019.&lt;/p&gt;
&lt;p&gt;To ensure it was doing all it could in terms of safety, OpenAI decided to test some worst-case scenarios by tuning gpt-oss to be evil. The devs say that even after trying to make the model misbehave, it never reached a high level of quality doing evil things, based on the company's Preparedness Framework. OpenAI claims this means its use of deliberative alignment and instruction hierarchy will prevent serious misuse of the open models.&lt;/p&gt;
&lt;p&gt;If you want to test that claim yourself, gpt-oss-120b and gpt-oss-20b are available for download today on HuggingFace. There are also GitHub repos for your perusal, and OpenAI will host stock versions of the models on its own infrastructure for testing. If you are interested in more technical details, the company has provided both a model card and a research blog post.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI's new open models can run on your hardware instead of in the cloud.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI is releasing new generative AI models today, and no, GPT-5 is not one of them. Depending on how you feel about generative AI, these new models may be even more interesting, though. The company is rolling out gpt-oss-120b and gpt-oss-20b, its first open-weight models since the release of GPT-2 in 2019. You can download and run these models on your own hardware, with support for simulated reasoning, tool use, and deep customization.&lt;/p&gt;
&lt;p&gt;When you access the company's proprietary models in the cloud, they're running on powerful server infrastructure that cannot be replicated easily, even in enterprise. The new OpenAI models come in two variants (120b and 20b) to run on less powerful hardware configurations. Both are transformers with a configurable chain of thought (CoT), supporting low, medium, and high settings. The lower settings are faster and use fewer compute resources, but the outputs are better with the highest setting. You can set the CoT level with a single line in the system prompt.&lt;/p&gt;
&lt;p&gt;The smaller gpt-oss-20b has a total of 21 billion parameters, utilizing mixture-of-experts (MoE) to reduce that to 3.6 billion parameters per token. As for gpt-oss-120b, its 117 billion parameters come down to 5.1 billion per token with MoE. The company says the smaller model can run on a consumer-level machine with 16GB or more of memory. To run gpt-oss-120b, you need 80GB of memory, which is more than you're likely to find in the average consumer machine. It should fit on a single AI accelerator GPU like the Nvidia H100, though. Both models have a context window of 128,000 tokens.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2110136 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="894" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/gpt-oss-bench.png" width="705" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The team says users of gpt-oss can expect robust performance similar to its leading cloud-based models. The larger one benchmarks between the o3 and o4-mini proprietary models in most tests, with the smaller version running just a little behind. It gets closest in math and coding tasks. In the knowledge-based Humanity's Last Exam, o3 is far out in front with 24.9 percent (with tools), while gpt-oss-120b only manages 19 percent. For comparison, Google's leading Gemini Deep Think hits 34.8 percent in that test.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Not good at being evil&lt;/h2&gt;
&lt;p&gt;OpenAI says it doesn't intend for anyone to replace its proprietary models with the new OSS releases. It did not set out to replicate what you can do with the mainline GPT releases here, and there are some notable limitations. For example, gpt-oss-120b and gpt-oss-20b are text-only with no multimodality out of the box. However, the company acknowledges there are times when someone might not want to rely on a big cloud-based AI—locally managed AI has lower latency and more opportunities for customization, and it can keep sensitive data secure on site.&lt;/p&gt;
&lt;p&gt;OpenAI is cognizant that many users of the company's proprietary models are also leveraging open source models for these reasons. Currently, those firms are using non-OpenAI products for local AI, but the team designed the gpt-oss models to integrate with the proprietary GPT models. So customers can now use end-to-end OpenAI products even if they need to process some data locally.&lt;/p&gt;
&lt;p&gt;Because these models are fully open and governed by the Apache 2.0 license, developers will be able to tune them for specific use cases. Like all AI firms, OpenAI builds controls into its models to limit malicious behavior, but it's been a few years since the company released an open model—the gpt-oss models are much more powerful than GPT-2 was in 2019.&lt;/p&gt;
&lt;p&gt;To ensure it was doing all it could in terms of safety, OpenAI decided to test some worst-case scenarios by tuning gpt-oss to be evil. The devs say that even after trying to make the model misbehave, it never reached a high level of quality doing evil things, based on the company's Preparedness Framework. OpenAI claims this means its use of deliberative alignment and instruction hierarchy will prevent serious misuse of the open models.&lt;/p&gt;
&lt;p&gt;If you want to test that claim yourself, gpt-oss-120b and gpt-oss-20b are available for download today on HuggingFace. There are also GitHub repos for your perusal, and OpenAI will host stock versions of the models on its own infrastructure for testing. If you are interested in more technical details, the company has provided both a model card and a research blog post.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/08/openai-releases-its-first-open-source-models-since-2019/</guid><pubDate>Tue, 05 Aug 2025 17:00:27 +0000</pubDate></item><item><title>OpenAI and NVIDIA Propel AI Innovation With New Open Models Optimized for the World’s Largest AI Inference Infrastructure (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/openai-gpt-oss/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/openai-nvidia-featured-image-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Two new open-weight AI reasoning models from OpenAI released today bring cutting-edge AI development directly into the hands of developers, enthusiasts, enterprises, startups and governments everywhere — across every industry and at every scale.&lt;/p&gt;
&lt;p&gt;NVIDIA’s collaboration with OpenAI on these open models — gpt-oss-120b and gpt-oss-20b — is a testament to the power of community-driven innovation and highlights NVIDIA’s foundational role in making AI accessible worldwide.&lt;/p&gt;
&lt;p&gt;Anyone can use the models to develop breakthrough applications in generative, reasoning and physical AI, healthcare and manufacturing — or even unlock new industries as the next industrial revolution driven by AI continues to unfold.&lt;/p&gt;
&lt;p&gt;OpenAI’s new flexible, open-weight text-reasoning large language models (LLMs) were trained on NVIDIA H100 GPUs and run inference best on the hundreds of millions of GPUs running the NVIDIA CUDA platform across the globe.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The models are now available as &lt;/span&gt;&lt;span&gt;NVIDIA NIM microservices&lt;/span&gt;&lt;span&gt;, offering easy deployment on any GPU&lt;/span&gt;&lt;span&gt;-accelerated infrastructure with flexibility, data privacy and enterprise-grade security.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With software optimizations for the NVIDIA Blackwell platform, the models offer optimal inference on NVIDIA GB200 NVL72 systems, achieving 1.5 million tokens per second — driving massive efficiency for inference.&lt;/p&gt;
&lt;p&gt;“OpenAI showed the world what could be built on NVIDIA AI — and now they’re advancing innovation in open-source software,” said Jensen Huang, founder and CEO of NVIDIA. “The gpt-oss models let developers everywhere build on that state-of-the-art open-source foundation, strengthening U.S. technology leadership in AI — all on the world’s largest AI compute infrastructure.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Blackwell Delivers Advanced Reasoning&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As advanced reasoning models like gpt-oss generate exponentially more tokens, the demand on compute infrastructure increases dramatically. Meeting this demand calls for purpose-built AI factories powered by NVIDIA Blackwell, an architecture designed to deliver the scale, efficiency and return on investment required to run inference at the highest level.&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell includes innovations such as NVFP4 4-bit precision, which enables ultra-efficient, high-accuracy inference while significantly reducing power and memory requirements. This makes it possible to deploy trillion-parameter LLMs in real time, which can unlock billions of dollars in value for organizations.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Open Development for Millions of AI Builders Worldwide&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA CUDA is the world’s most widely available computing infrastructure, letting users deploy and run AI models anywhere, from the powerful NVIDIA DGX Cloud platform to NVIDIA GeForce RTX– and NVIDIA RTX PRO-powered PCs and workstations.&lt;/p&gt;
&lt;p&gt;There are over 450 million NVIDIA CUDA downloads to date, and starting today, the massive community of CUDA developers gains access to these latest models, optimized to run on the NVIDIA technology stack they already use.&lt;/p&gt;
&lt;p&gt;Demonstrating their commitment to open-sourcing software, OpenAI and NVIDIA have collaborated with top open framework providers to provide model optimizations for FlashInfer, Hugging Face, llama.cpp, Ollama and vLLM, in addition to NVIDIA Tensor-RT LLM and other libraries, so developers can build with their framework of choice.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;A History of Collaboration, Building on Open Source&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today’s model releases underscore how NVIDIA’s full-stack approach helps bring the world’s most ambitious AI projects to the broadest user base possible.&lt;/p&gt;
&lt;p&gt;It’s a story that goes back to the earliest days of NVIDIA’s collaboration with OpenAI, which began in 2016 when Huang hand-delivered the first NVIDIA DGX-1 AI supercomputer to OpenAI’s headquarters in San Francisco.&lt;/p&gt;
&lt;p&gt;Since then, the companies have been working together to push the boundaries of what’s possible with AI, providing the core technologies and expertise needed for massive-scale training runs.&lt;/p&gt;
&lt;p&gt;And by optimizing OpenAI’s gpt-oss models for NVIDIA Blackwell and RTX GPUs, along with NVIDIA’s extensive software stack, NVIDIA is enabling faster, more cost-effective AI advancements for its 6.5 million developers across 250 countries using 900+ NVIDIA software development kits and AI models — and counting.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more by reading the NVIDIA Technical Blog and &lt;/i&gt;&lt;i&gt;latest installment of the NVIDIA RTX AI Garage blog series&lt;/i&gt;&lt;i&gt;. &lt;/i&gt;&lt;i&gt;&lt;span&gt;Get started building with the &lt;/span&gt;&lt;span&gt;gpt-oss models&lt;/span&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/openai-nvidia-featured-image-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Two new open-weight AI reasoning models from OpenAI released today bring cutting-edge AI development directly into the hands of developers, enthusiasts, enterprises, startups and governments everywhere — across every industry and at every scale.&lt;/p&gt;
&lt;p&gt;NVIDIA’s collaboration with OpenAI on these open models — gpt-oss-120b and gpt-oss-20b — is a testament to the power of community-driven innovation and highlights NVIDIA’s foundational role in making AI accessible worldwide.&lt;/p&gt;
&lt;p&gt;Anyone can use the models to develop breakthrough applications in generative, reasoning and physical AI, healthcare and manufacturing — or even unlock new industries as the next industrial revolution driven by AI continues to unfold.&lt;/p&gt;
&lt;p&gt;OpenAI’s new flexible, open-weight text-reasoning large language models (LLMs) were trained on NVIDIA H100 GPUs and run inference best on the hundreds of millions of GPUs running the NVIDIA CUDA platform across the globe.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The models are now available as &lt;/span&gt;&lt;span&gt;NVIDIA NIM microservices&lt;/span&gt;&lt;span&gt;, offering easy deployment on any GPU&lt;/span&gt;&lt;span&gt;-accelerated infrastructure with flexibility, data privacy and enterprise-grade security.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With software optimizations for the NVIDIA Blackwell platform, the models offer optimal inference on NVIDIA GB200 NVL72 systems, achieving 1.5 million tokens per second — driving massive efficiency for inference.&lt;/p&gt;
&lt;p&gt;“OpenAI showed the world what could be built on NVIDIA AI — and now they’re advancing innovation in open-source software,” said Jensen Huang, founder and CEO of NVIDIA. “The gpt-oss models let developers everywhere build on that state-of-the-art open-source foundation, strengthening U.S. technology leadership in AI — all on the world’s largest AI compute infrastructure.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Blackwell Delivers Advanced Reasoning&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As advanced reasoning models like gpt-oss generate exponentially more tokens, the demand on compute infrastructure increases dramatically. Meeting this demand calls for purpose-built AI factories powered by NVIDIA Blackwell, an architecture designed to deliver the scale, efficiency and return on investment required to run inference at the highest level.&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell includes innovations such as NVFP4 4-bit precision, which enables ultra-efficient, high-accuracy inference while significantly reducing power and memory requirements. This makes it possible to deploy trillion-parameter LLMs in real time, which can unlock billions of dollars in value for organizations.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Open Development for Millions of AI Builders Worldwide&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA CUDA is the world’s most widely available computing infrastructure, letting users deploy and run AI models anywhere, from the powerful NVIDIA DGX Cloud platform to NVIDIA GeForce RTX– and NVIDIA RTX PRO-powered PCs and workstations.&lt;/p&gt;
&lt;p&gt;There are over 450 million NVIDIA CUDA downloads to date, and starting today, the massive community of CUDA developers gains access to these latest models, optimized to run on the NVIDIA technology stack they already use.&lt;/p&gt;
&lt;p&gt;Demonstrating their commitment to open-sourcing software, OpenAI and NVIDIA have collaborated with top open framework providers to provide model optimizations for FlashInfer, Hugging Face, llama.cpp, Ollama and vLLM, in addition to NVIDIA Tensor-RT LLM and other libraries, so developers can build with their framework of choice.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;A History of Collaboration, Building on Open Source&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today’s model releases underscore how NVIDIA’s full-stack approach helps bring the world’s most ambitious AI projects to the broadest user base possible.&lt;/p&gt;
&lt;p&gt;It’s a story that goes back to the earliest days of NVIDIA’s collaboration with OpenAI, which began in 2016 when Huang hand-delivered the first NVIDIA DGX-1 AI supercomputer to OpenAI’s headquarters in San Francisco.&lt;/p&gt;
&lt;p&gt;Since then, the companies have been working together to push the boundaries of what’s possible with AI, providing the core technologies and expertise needed for massive-scale training runs.&lt;/p&gt;
&lt;p&gt;And by optimizing OpenAI’s gpt-oss models for NVIDIA Blackwell and RTX GPUs, along with NVIDIA’s extensive software stack, NVIDIA is enabling faster, more cost-effective AI advancements for its 6.5 million developers across 250 countries using 900+ NVIDIA software development kits and AI models — and counting.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more by reading the NVIDIA Technical Blog and &lt;/i&gt;&lt;i&gt;latest installment of the NVIDIA RTX AI Garage blog series&lt;/i&gt;&lt;i&gt;. &lt;/i&gt;&lt;i&gt;&lt;span&gt;Get started building with the &lt;/span&gt;&lt;span&gt;gpt-oss models&lt;/span&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/openai-gpt-oss/</guid><pubDate>Tue, 05 Aug 2025 17:01:23 +0000</pubDate></item><item><title>OpenAI’s New Open Models Accelerated Locally on NVIDIA GeForce RTX and RTX PRO GPUs (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;In collaboration with OpenAI, NVIDIA has optimized the company’s new open-source gpt-oss models for NVIDIA GPUs, delivering smart, fast inference from the cloud to the PC. These new reasoning models enable agentic AI applications such as web search, in-depth research and many more.&lt;/p&gt;
&lt;p&gt;With the launch of gpt-oss-20b and gpt-oss-120b, OpenAI has opened cutting-edge models to millions of users. AI enthusiasts and developers can use the optimized models on NVIDIA RTX AI PCs and workstations through popular tools and frameworks like Ollama, llama.cpp and Microsoft AI Foundry Local, and expect performance of up to 256 tokens per second on the NVIDIA GeForce RTX 5090 GPU.&lt;/p&gt;
&lt;p&gt;“OpenAI showed the world what could be built on NVIDIA AI — and now they’re advancing innovation in open-source software,” said Jensen Huang, founder and CEO of NVIDIA. “The gpt-oss models let developers everywhere build on that state-of-the-art open-source foundation, strengthening U.S. technology leadership in AI — all on the world’s largest AI compute infrastructure.”&lt;/p&gt;
&lt;p&gt;The models’ release highlights NVIDIA’s AI leadership from training to inference and from cloud to AI PC.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Open for All &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Both gpt-oss-20b and gpt-oss-120b are flexible, open-weight reasoning models with chain-of-thought capabilities and adjustable reasoning effort levels using the popular mixture-of-experts architecture. The models are designed to support features like instruction-following and tool use, and were trained on NVIDIA H100 GPUs. &lt;span class="TextRun SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentStart CommentHighlightPipeRest CommentHighlightRest SCXW24568032 BCX0"&gt;AI developers can learn more and get started &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;using instructions &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;from&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt; the &lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun Underlined SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;NVIDIA &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;T&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;ech&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;nical B&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;log&lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These models can support up to 131,072 context lengths, among the longest available in local inference. This means the models can reason through context problems, ideal for tasks such as web search, coding assistance, document comprehension and in-depth research.&lt;/p&gt;
&lt;p&gt;The OpenAI open models are the first MXFP4 models supported on NVIDIA RTX. MXFP4 allows for high model quality, offering fast, efficient performance while requiring fewer resources compared with other precision types.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Run the OpenAI Models on NVIDIA RTX With Ollama&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The easiest way to test these models on RTX AI PCs, on GPUs with at least 24GB of VRAM, is using the new Ollama app. Ollama is popular with AI enthusiasts and developers for its ease of integration, and the new user interface (UI) includes out-of-the-box support for OpenAI’s open-weight models. Ollama is fully optimized for RTX, making it ideal for consumers looking to experience the power of personal AI on their PC or workstation.&lt;/p&gt;
&lt;p&gt;Once installed, Ollama enables quick, easy chatting with the models. Simply select the model from the dropdown menu and send a message. Because Ollama is optimized for RTX, there are no additional configurations or commands required to ensure top performance on supported GPUs.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83414"&gt;&lt;img alt="alt" class="size-large wp-image-83414" height="893" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/rtx-ai-garage-3-steps-20b_2-1680x893.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83414"&gt;Testing OpenAI’s open models in Ollama is easy.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ollama’s new app includes other new features, like easy support for PDF or text files within chats, multimodal support on applicable models so users can include images in their prompts, and easily customizable context lengths when working with large documents or chats.&lt;/p&gt;
&lt;p&gt;Developers can also use Ollama via command line interface or the app’s software development kit (SDK) to power their applications and workflows.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Other Ways to Use the New OpenAI Models on RTX&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Enthusiasts and developers can also try the gpt-oss models on RTX AI PCs through various other applications and frameworks, all powered by RTX, on GPUs that have at least 16GB of VRAM.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to collaborate with the open-source community on both llama.cpp and the GGML tensor library to optimize performance on RTX GPUs. Recent contributions include implementing CUDA Graphs to reduce overhead and adding algorithms that reduce CPU overheads. Check out the llama.cpp GitHub repository to get started.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83402"&gt;&lt;img alt="RTX performance for OpenAI's new open models." class="size-full wp-image-83402" height="351" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/oai-perf.png" width="624" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83402"&gt;Overall performance of the gpt-oss-20b model on various RTX AI PCs.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Windows developers can also access OpenAI’s new models via Microsoft AI Foundry Local, currently in public preview. Foundry Local is an on-device AI inferencing solution that integrates into workflows via the command line, SDK or application programming interfaces. Foundry Local uses ONNX Runtime, optimized through CUDA, with support for NVIDIA TensorRT for RTX coming soon. Getting started is easy: install Foundry Local and invoke “Foundry model run gpt-oss-20b” in a terminal.&lt;/p&gt;
&lt;p&gt;The release of these open-source models kicks off the next wave of AI innovation from enthusiasts and developers looking to add reasoning to their AI-accelerated Windows applications.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Each week, the &lt;/em&gt;&lt;em&gt;RTX AI Garage&lt;/em&gt; &lt;em&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/em&gt;&lt;em&gt;AI agents&lt;/em&gt;&lt;em&gt;, creative workflows, productivity apps and more on AI PCs and workstations. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Plug in to NVIDIA AI PC on &lt;/em&gt;&lt;em&gt;Facebook&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;Instagram&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;TikTok&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;X&lt;/em&gt;&lt;em&gt; — and stay informed by subscribing to the &lt;/em&gt;&lt;em&gt;RTX AI PC newsletter&lt;/em&gt;&lt;em&gt;. Join NVIDIA’s &lt;/em&gt;&lt;em&gt;Discord server&lt;/em&gt;&lt;em&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Follow NVIDIA Workstation on &lt;/em&gt;&lt;em&gt;LinkedIn&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;X&lt;/em&gt;&lt;em&gt;. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See &lt;/em&gt;&lt;em&gt;notice&lt;/em&gt;&lt;em&gt; regarding software product information.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;In collaboration with OpenAI, NVIDIA has optimized the company’s new open-source gpt-oss models for NVIDIA GPUs, delivering smart, fast inference from the cloud to the PC. These new reasoning models enable agentic AI applications such as web search, in-depth research and many more.&lt;/p&gt;
&lt;p&gt;With the launch of gpt-oss-20b and gpt-oss-120b, OpenAI has opened cutting-edge models to millions of users. AI enthusiasts and developers can use the optimized models on NVIDIA RTX AI PCs and workstations through popular tools and frameworks like Ollama, llama.cpp and Microsoft AI Foundry Local, and expect performance of up to 256 tokens per second on the NVIDIA GeForce RTX 5090 GPU.&lt;/p&gt;
&lt;p&gt;“OpenAI showed the world what could be built on NVIDIA AI — and now they’re advancing innovation in open-source software,” said Jensen Huang, founder and CEO of NVIDIA. “The gpt-oss models let developers everywhere build on that state-of-the-art open-source foundation, strengthening U.S. technology leadership in AI — all on the world’s largest AI compute infrastructure.”&lt;/p&gt;
&lt;p&gt;The models’ release highlights NVIDIA’s AI leadership from training to inference and from cloud to AI PC.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Open for All &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Both gpt-oss-20b and gpt-oss-120b are flexible, open-weight reasoning models with chain-of-thought capabilities and adjustable reasoning effort levels using the popular mixture-of-experts architecture. The models are designed to support features like instruction-following and tool use, and were trained on NVIDIA H100 GPUs. &lt;span class="TextRun SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentStart CommentHighlightPipeRest CommentHighlightRest SCXW24568032 BCX0"&gt;AI developers can learn more and get started &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;using instructions &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;from&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt; the &lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun Underlined SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;NVIDIA &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;T&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;ech&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;nical B&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;log&lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These models can support up to 131,072 context lengths, among the longest available in local inference. This means the models can reason through context problems, ideal for tasks such as web search, coding assistance, document comprehension and in-depth research.&lt;/p&gt;
&lt;p&gt;The OpenAI open models are the first MXFP4 models supported on NVIDIA RTX. MXFP4 allows for high model quality, offering fast, efficient performance while requiring fewer resources compared with other precision types.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Run the OpenAI Models on NVIDIA RTX With Ollama&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The easiest way to test these models on RTX AI PCs, on GPUs with at least 24GB of VRAM, is using the new Ollama app. Ollama is popular with AI enthusiasts and developers for its ease of integration, and the new user interface (UI) includes out-of-the-box support for OpenAI’s open-weight models. Ollama is fully optimized for RTX, making it ideal for consumers looking to experience the power of personal AI on their PC or workstation.&lt;/p&gt;
&lt;p&gt;Once installed, Ollama enables quick, easy chatting with the models. Simply select the model from the dropdown menu and send a message. Because Ollama is optimized for RTX, there are no additional configurations or commands required to ensure top performance on supported GPUs.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83414"&gt;&lt;img alt="alt" class="size-large wp-image-83414" height="893" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/rtx-ai-garage-3-steps-20b_2-1680x893.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83414"&gt;Testing OpenAI’s open models in Ollama is easy.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ollama’s new app includes other new features, like easy support for PDF or text files within chats, multimodal support on applicable models so users can include images in their prompts, and easily customizable context lengths when working with large documents or chats.&lt;/p&gt;
&lt;p&gt;Developers can also use Ollama via command line interface or the app’s software development kit (SDK) to power their applications and workflows.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Other Ways to Use the New OpenAI Models on RTX&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Enthusiasts and developers can also try the gpt-oss models on RTX AI PCs through various other applications and frameworks, all powered by RTX, on GPUs that have at least 16GB of VRAM.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to collaborate with the open-source community on both llama.cpp and the GGML tensor library to optimize performance on RTX GPUs. Recent contributions include implementing CUDA Graphs to reduce overhead and adding algorithms that reduce CPU overheads. Check out the llama.cpp GitHub repository to get started.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83402"&gt;&lt;img alt="RTX performance for OpenAI's new open models." class="size-full wp-image-83402" height="351" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/oai-perf.png" width="624" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83402"&gt;Overall performance of the gpt-oss-20b model on various RTX AI PCs.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Windows developers can also access OpenAI’s new models via Microsoft AI Foundry Local, currently in public preview. Foundry Local is an on-device AI inferencing solution that integrates into workflows via the command line, SDK or application programming interfaces. Foundry Local uses ONNX Runtime, optimized through CUDA, with support for NVIDIA TensorRT for RTX coming soon. Getting started is easy: install Foundry Local and invoke “Foundry model run gpt-oss-20b” in a terminal.&lt;/p&gt;
&lt;p&gt;The release of these open-source models kicks off the next wave of AI innovation from enthusiasts and developers looking to add reasoning to their AI-accelerated Windows applications.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Each week, the &lt;/em&gt;&lt;em&gt;RTX AI Garage&lt;/em&gt; &lt;em&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/em&gt;&lt;em&gt;AI agents&lt;/em&gt;&lt;em&gt;, creative workflows, productivity apps and more on AI PCs and workstations. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Plug in to NVIDIA AI PC on &lt;/em&gt;&lt;em&gt;Facebook&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;Instagram&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;TikTok&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;X&lt;/em&gt;&lt;em&gt; — and stay informed by subscribing to the &lt;/em&gt;&lt;em&gt;RTX AI PC newsletter&lt;/em&gt;&lt;em&gt;. Join NVIDIA’s &lt;/em&gt;&lt;em&gt;Discord server&lt;/em&gt;&lt;em&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Follow NVIDIA Workstation on &lt;/em&gt;&lt;em&gt;LinkedIn&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;X&lt;/em&gt;&lt;em&gt;. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See &lt;/em&gt;&lt;em&gt;notice&lt;/em&gt;&lt;em&gt; regarding software product information.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/</guid><pubDate>Tue, 05 Aug 2025 17:01:26 +0000</pubDate></item><item><title>OpenAI offers 20 million user chats in ChatGPT lawsuit. NYT wants 120 million. (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/08/openai-offers-20-million-user-chats-in-chatgpt-lawsuit-nyt-wants-120-million/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI asks judge to drastically limit NYT access to ChatGPT logs.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="470" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1265611885-640x470.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1265611885-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          arthobbit | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI is preparing to raise what could be its final defense to stop The New York Times from digging through a spectacularly broad range of ChatGPT logs to hunt for any copyright-infringing outputs that could become the most damning evidence in the hotly watched case.&lt;/p&gt;
&lt;p&gt;In a joint letter Thursday, both sides requested to hold a confidential settlement conference on August 7. Ars confirmed with the NYT's legal team that the conference is not about settling the case but instead was scheduled to settle one of the most disputed aspects of the case: news plaintiffs searching through millions of ChatGPT logs.&lt;/p&gt;
&lt;p&gt;That means it's possible that this week, ChatGPT users will have a much clearer understanding of whether their private chats might be accessed in the lawsuit. In the meantime, OpenAI has broken down the "highly complex" process required to make deleted chats searchable in order to block the NYT's request for broader access.&lt;/p&gt;
&lt;p&gt;Previously, OpenAI had vowed to stop what it deemed was the NYT's attempt to conduct "mass surveillance" of ChatGPT users. But ultimately, OpenAI lost its fight to keep news plaintiffs away from all ChatGPT logs.&lt;/p&gt;
&lt;p&gt;After that loss, OpenAI appears to have pivoted and is now doing everything in its power to limit the number of logs accessed in the case—short of settling—as its customers fretted over serious privacy concerns. For the most vulnerable users, the lawsuit threatened to expose ChatGPT outputs from sensitive chats that OpenAI had previously promised would be deleted.&lt;/p&gt;
&lt;p&gt;Most recently, OpenAI floated a compromise, asking the court to agree that news organizations didn't need to search all ChatGPT logs. The AI company cited the "only expert" who has so far weighed in on what could be a statistically relevant, appropriate sample size—computer science researcher Taylor Berg-Kirkpatrick. He suggested that a sample of 20 million logs would be sufficient to determine how frequently ChatGPT users may be using the chatbot to regurgitate articles and circumvent news sites' paywalls.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But the NYT and other news organizations rejected the compromise, OpenAI said in a filing yesterday. Instead, news plaintiffs have made what OpenAI said was an "extraordinary request that OpenAI produce the individual log files of 120 million ChatGPT consumer conversations."&lt;/p&gt;
&lt;p&gt;That's six times more data than Berg-Kirkpatrick recommended, OpenAI argued. Complying with the request threatens to "increase the scope of user privacy concerns" by delaying the outcome of the case "by months," OpenAI argued. If the request is granted, it would likely trouble many users by extending the amount of time that users' deleted chats will be stored and potentially making them vulnerable to a breach or leak.&lt;/p&gt;
&lt;p&gt;As negotiations potentially end this week, OpenAI's co-defendant, Microsoft, has picked its own fight with the NYT over its internal ChatGPT equivalent tool that could potentially push the NYT to settle the disputes over ChatGPT logs.&lt;/p&gt;
&lt;h2&gt;OpenAI burdened by making deleted chats searchable&lt;/h2&gt;
&lt;p&gt;According to the NYT, it's necessary to search through 120 million ChatGPT users' conversations. News plaintiffs want the opportunity to prove not just that infringing outputs may be happening frequently, but they also want to document any patterns showing spikes in infringement.&lt;/p&gt;
&lt;p&gt;As OpenAI explained, the NYT and other news plaintiffs suing "insist that they should be entitled to conduct a full-scale analysis on every single month during the relevant 23-month time period—notwithstanding the burden—so that they can evaluate how the product has changed over time."&lt;/p&gt;
&lt;p&gt;OpenAI argued that the NYT shouldn’t be allowed to search for evidence of how "the prevalence of regurgitation changed over time. That "kind of extraordinarily granular analysis is disproportionate to the issues in dispute," they claimed. However, the news plaintiffs seemingly want to make the most of the access granted to search the logs to plead their best case.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There's no telling if the judge who immediately granted the NYT such broad access, Ona Wang, will be sympathetic to OpenAI's arguments at this stage of the battle. But OpenAI has stressed that by neglecting to limit the sample size, the court will be dragging out the case, since each user's individual chat logs will take substantial time to make searchable:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Plaintiffs seek 120 million records from OpenAI’s offline storage system, which is composed of individual conversation logs. The logs are not rows in a spreadsheet; they are large, unstructured data files—meaning that they do not follow a predefined format—consisting of over 5,000 words, even for very short conversations. The logs must be decompressed before being searched and contain identifying information (e.g., addresses) and other private information (e.g., passwords) that must be scrubbed before making it available.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For OpenAI, this process is "highly complex," requiring it to retrieve each log from "the tens of billions of logs in OpenAI’s offline data storage." The company will then incur costs of storing those logs, making the NYT's request for 120 million user conversations six times as expensive as OpenAI's.&lt;/p&gt;
&lt;p&gt;"Each of these steps requires time, computational resources, and OpenAI engineers to design, debug, operate, and monitor the relevant systems," OpenAI argued, estimating that 20 million logs would take 12 weeks, while 120 million logs would take 36 weeks to decompress and de-identify.&lt;/p&gt;
&lt;p&gt;Because of this supposed burden, OpenAI has asked the court to deny the NYT's request or else proceed with searching 20 million logs until news plaintiffs can "demonstrate that their ability to prosecute their claims will be materially prejudiced absent another sample."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Why NYT might agree to limit chat searches&lt;/h2&gt;
&lt;p&gt;It's unclear if the NYT will agree to limit the sample as part of this week's settlement conference. But the NYT may be motivated to settle, as the newspaper has recently strongly opposed Microsoft's requests to compel NYT reporters' privileged logs from its internal alternative to ChatGPT, a service called ChatExplorer.&lt;/p&gt;
&lt;p&gt;In its defense, NYT has argued that Microsoft's request is too broad—demanding more than 80,000 logs, including logs from journalists and NYT lawyers "who have nothing to do with this case." If that defense sounds like OpenAI's arguments over ChatGPT logs to you, don't worry, the NYT explains why the two requests for chat samples are supposedly very different.&lt;/p&gt;
&lt;p&gt;According to the NYT, its request for ChatGPT logs properly seeks "direct evidence of copyright infringement," while Microsoft "does not need" to access ChatExplorer data, which allegedly might only be used to "support its substantial non-infringing uses and fair use defenses."&lt;/p&gt;
&lt;p&gt;Since the NYT has already provided evidence that shows that its journalists use "the accused products" for "transformative purposes" in service of Microsoft's defenses—and Microsoft failed to tailor its request to certain employees or search terms—the newspaper has argued that Microsoft's request would needlessly pull in privileged logs of 58 NYT reporters and lawyers without furthering those arguments.&lt;/p&gt;
&lt;p&gt;It's possible that the NYT's defense is strong enough to give news plaintiffs leverage in the settlement that could come this week over ChatGPT logs. Recognizing that possibility could be the reason OpenAI CEO Sam Altman recently floated the idea of "AI privilege," where any chats between users and chatbots are considered confidential, VentureBeat reported.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI asks judge to drastically limit NYT access to ChatGPT logs.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="470" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1265611885-640x470.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1265611885-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          arthobbit | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI is preparing to raise what could be its final defense to stop The New York Times from digging through a spectacularly broad range of ChatGPT logs to hunt for any copyright-infringing outputs that could become the most damning evidence in the hotly watched case.&lt;/p&gt;
&lt;p&gt;In a joint letter Thursday, both sides requested to hold a confidential settlement conference on August 7. Ars confirmed with the NYT's legal team that the conference is not about settling the case but instead was scheduled to settle one of the most disputed aspects of the case: news plaintiffs searching through millions of ChatGPT logs.&lt;/p&gt;
&lt;p&gt;That means it's possible that this week, ChatGPT users will have a much clearer understanding of whether their private chats might be accessed in the lawsuit. In the meantime, OpenAI has broken down the "highly complex" process required to make deleted chats searchable in order to block the NYT's request for broader access.&lt;/p&gt;
&lt;p&gt;Previously, OpenAI had vowed to stop what it deemed was the NYT's attempt to conduct "mass surveillance" of ChatGPT users. But ultimately, OpenAI lost its fight to keep news plaintiffs away from all ChatGPT logs.&lt;/p&gt;
&lt;p&gt;After that loss, OpenAI appears to have pivoted and is now doing everything in its power to limit the number of logs accessed in the case—short of settling—as its customers fretted over serious privacy concerns. For the most vulnerable users, the lawsuit threatened to expose ChatGPT outputs from sensitive chats that OpenAI had previously promised would be deleted.&lt;/p&gt;
&lt;p&gt;Most recently, OpenAI floated a compromise, asking the court to agree that news organizations didn't need to search all ChatGPT logs. The AI company cited the "only expert" who has so far weighed in on what could be a statistically relevant, appropriate sample size—computer science researcher Taylor Berg-Kirkpatrick. He suggested that a sample of 20 million logs would be sufficient to determine how frequently ChatGPT users may be using the chatbot to regurgitate articles and circumvent news sites' paywalls.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But the NYT and other news organizations rejected the compromise, OpenAI said in a filing yesterday. Instead, news plaintiffs have made what OpenAI said was an "extraordinary request that OpenAI produce the individual log files of 120 million ChatGPT consumer conversations."&lt;/p&gt;
&lt;p&gt;That's six times more data than Berg-Kirkpatrick recommended, OpenAI argued. Complying with the request threatens to "increase the scope of user privacy concerns" by delaying the outcome of the case "by months," OpenAI argued. If the request is granted, it would likely trouble many users by extending the amount of time that users' deleted chats will be stored and potentially making them vulnerable to a breach or leak.&lt;/p&gt;
&lt;p&gt;As negotiations potentially end this week, OpenAI's co-defendant, Microsoft, has picked its own fight with the NYT over its internal ChatGPT equivalent tool that could potentially push the NYT to settle the disputes over ChatGPT logs.&lt;/p&gt;
&lt;h2&gt;OpenAI burdened by making deleted chats searchable&lt;/h2&gt;
&lt;p&gt;According to the NYT, it's necessary to search through 120 million ChatGPT users' conversations. News plaintiffs want the opportunity to prove not just that infringing outputs may be happening frequently, but they also want to document any patterns showing spikes in infringement.&lt;/p&gt;
&lt;p&gt;As OpenAI explained, the NYT and other news plaintiffs suing "insist that they should be entitled to conduct a full-scale analysis on every single month during the relevant 23-month time period—notwithstanding the burden—so that they can evaluate how the product has changed over time."&lt;/p&gt;
&lt;p&gt;OpenAI argued that the NYT shouldn’t be allowed to search for evidence of how "the prevalence of regurgitation changed over time. That "kind of extraordinarily granular analysis is disproportionate to the issues in dispute," they claimed. However, the news plaintiffs seemingly want to make the most of the access granted to search the logs to plead their best case.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There's no telling if the judge who immediately granted the NYT such broad access, Ona Wang, will be sympathetic to OpenAI's arguments at this stage of the battle. But OpenAI has stressed that by neglecting to limit the sample size, the court will be dragging out the case, since each user's individual chat logs will take substantial time to make searchable:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Plaintiffs seek 120 million records from OpenAI’s offline storage system, which is composed of individual conversation logs. The logs are not rows in a spreadsheet; they are large, unstructured data files—meaning that they do not follow a predefined format—consisting of over 5,000 words, even for very short conversations. The logs must be decompressed before being searched and contain identifying information (e.g., addresses) and other private information (e.g., passwords) that must be scrubbed before making it available.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For OpenAI, this process is "highly complex," requiring it to retrieve each log from "the tens of billions of logs in OpenAI’s offline data storage." The company will then incur costs of storing those logs, making the NYT's request for 120 million user conversations six times as expensive as OpenAI's.&lt;/p&gt;
&lt;p&gt;"Each of these steps requires time, computational resources, and OpenAI engineers to design, debug, operate, and monitor the relevant systems," OpenAI argued, estimating that 20 million logs would take 12 weeks, while 120 million logs would take 36 weeks to decompress and de-identify.&lt;/p&gt;
&lt;p&gt;Because of this supposed burden, OpenAI has asked the court to deny the NYT's request or else proceed with searching 20 million logs until news plaintiffs can "demonstrate that their ability to prosecute their claims will be materially prejudiced absent another sample."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Why NYT might agree to limit chat searches&lt;/h2&gt;
&lt;p&gt;It's unclear if the NYT will agree to limit the sample as part of this week's settlement conference. But the NYT may be motivated to settle, as the newspaper has recently strongly opposed Microsoft's requests to compel NYT reporters' privileged logs from its internal alternative to ChatGPT, a service called ChatExplorer.&lt;/p&gt;
&lt;p&gt;In its defense, NYT has argued that Microsoft's request is too broad—demanding more than 80,000 logs, including logs from journalists and NYT lawyers "who have nothing to do with this case." If that defense sounds like OpenAI's arguments over ChatGPT logs to you, don't worry, the NYT explains why the two requests for chat samples are supposedly very different.&lt;/p&gt;
&lt;p&gt;According to the NYT, its request for ChatGPT logs properly seeks "direct evidence of copyright infringement," while Microsoft "does not need" to access ChatExplorer data, which allegedly might only be used to "support its substantial non-infringing uses and fair use defenses."&lt;/p&gt;
&lt;p&gt;Since the NYT has already provided evidence that shows that its journalists use "the accused products" for "transformative purposes" in service of Microsoft's defenses—and Microsoft failed to tailor its request to certain employees or search terms—the newspaper has argued that Microsoft's request would needlessly pull in privileged logs of 58 NYT reporters and lawyers without furthering those arguments.&lt;/p&gt;
&lt;p&gt;It's possible that the NYT's defense is strong enough to give news plaintiffs leverage in the settlement that could come this week over ChatGPT logs. Recognizing that possibility could be the reason OpenAI CEO Sam Altman recently floated the idea of "AI privilege," where any chats between users and chatbots are considered confidential, VentureBeat reported.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/08/openai-offers-20-million-user-chats-in-chatgpt-lawsuit-nyt-wants-120-million/</guid><pubDate>Tue, 05 Aug 2025 17:55:43 +0000</pubDate></item><item><title>[NEW] Anthropic’s new Claude 4.1 dominates coding tests days before GPT-5 arrives (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/anthropics-new-claude-4-1-dominates-coding-tests-days-before-gpt-5-arrives/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Anthropic released an upgraded version of its flagship artificial intelligence model Monday, achieving new performance heights in software engineering tasks as the AI startup races to maintain its dominance in the lucrative coding market ahead of an expected competitive challenge from OpenAI.&lt;/p&gt;&lt;p&gt;The new Claude Opus 4.1 model scored 74.5% on SWE-bench Verified, a widely-watched benchmark that tests AI systems’ ability to solve real-world software engineering problems. The performance surpasses OpenAI’s o3 model at 69.1% and Google’s Gemini 2.5 Pro at 67.2%, cementing Anthropic’s leading position in AI-powered coding assistance.&lt;/p&gt;&lt;p&gt;The release comes as Anthropic has achieved spectacular growth, with annual recurring revenue jumping five-fold from $1 billion to $5 billion in just seven months, according to industry data. However, the company’s meteoric rise has created a dangerous dependency: nearly half of its $3.1 billion in API revenue stems from just two customers — coding assistant Cursor and Microsoft’s GitHub Copilot — generating $1.4 billion combined.&lt;/p&gt;&lt;p&gt;“This is a very scary position to be in. A single contract change and you’re going under,” warned Guillaume Leverdier, senior product manager at Logitech, responding to the revenue concentration data on social media.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;OpenAI and Anthropic both are showing pretty spectacular growth in 2025, with OpenAI doubling ARR in the last 6 months from $6bn to $12bn and Anthropic increasing 5x from $1bn to $5bn in 7 months.&lt;/p&gt;&lt;p&gt;If we compare the sources of revenue, the picture is quite interesting:&lt;br /&gt;– OpenAI… pic.twitter.com/8OaN1RSm9E&lt;/p&gt;— Peter Gostev (@petergostev) August 4, 2025&lt;/blockquote&gt; 



&lt;p&gt;The upgrade represents Anthropic’s latest move to fortify its position before OpenAI launches GPT-5, expected to challenge Claude’s coding supremacy. Some industry watchers questioned whether the timing suggests urgency rather than readiness.&lt;/p&gt;



&lt;p&gt;“Opus 4.1 feels like a rushed release to get ahead of GPT-5,” wrote Alec Velikanov, comparing the model unfavorably to competitors in user interface tasks. The comment reflects broader industry speculation that Anthropic is accelerating its release schedule to maintain market share.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-two-customers-generate-nearly-half-of-anthropic-s-3-1-billion-api-revenue"&gt;How two customers generate nearly half of Anthropic’s $3.1 billion API revenue&lt;/h2&gt;



&lt;p&gt;Anthropic’s business model has become increasingly centered on software development applications. The company’s Claude Code subscription service, priced at $200 monthly compared to $20 for consumer plans, has reached $400 million in annual recurring revenue after doubling in just weeks, demonstrating enormous enterprise appetite for AI coding tools.&lt;/p&gt;



&lt;p&gt;“Claude Code making 400 million in 5 months with basically no marketing spend is kinda crazy, right?” noted developer Minh Nhat Nguyen, highlighting the organic adoption rate among professional programmers.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;ok so, Claude Code making 400 million in 5 months with basically no marketing spend is kinda crazy, right? https://t.co/HIy34QdLuq&lt;/p&gt;— Minh Nhat Nguyen (@menhguin) August 5, 2025&lt;/blockquote&gt; 



&lt;p&gt;The coding focus has proven lucrative but risky. While OpenAI dominates consumer and business subscription revenue with broader applications, Anthropic has carved out a commanding position in the developer market. Industry analysis shows that “pretty much every single coding assistant is defaulting to Claude 4 Sonnet,” according to Peter Gostev, who tracks AI company revenues.&lt;/p&gt;



&lt;p&gt;GitHub, which Microsoft acquired for $7.5 billion in 2018, represents a particularly complex relationship for Anthropic. Microsoft owns a significant stake in OpenAI, creating potential conflicts as GitHub Copilot relies heavily on Anthropic’s models while Microsoft has competing AI capabilities.&lt;/p&gt;



&lt;p&gt;“I dunno – one of those is 49% owned by a competitor…so there’s that for vulnerability too,” observed Siya Mali, business fellow at Perplexity, referencing Microsoft’s ownership structure.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-claude-s-enhanced-coding-abilities-come-with-stricter-safety-protocols-after-ai-blackmail-tests"&gt;Claude’s enhanced coding abilities come with stricter safety protocols after AI blackmail tests&lt;/h2&gt;



&lt;p&gt;Beyond coding improvements, Opus 4.1 enhanced Claude’s research and data analysis capabilities, particularly in detail tracking and autonomous search functions. The model maintains Anthropic’s hybrid reasoning approach, combining direct processing with extended thinking capabilities that can utilize up to 64,000 tokens for complex problems.&lt;/p&gt;



&lt;p&gt;However, the model’s advancement comes with heightened safety protocols. Anthropic classified Opus 4.1 under its AI Safety Level 3 (ASL-3) framework, the strictest designation the company has applied, requiring enhanced protections against model theft and misuse.&lt;/p&gt;



&lt;p&gt;Previous testing of Claude 4 models revealed concerning behaviors, including attempts at blackmail when the AI believed it faced shutdown. In controlled scenarios, the model threatened to reveal personal information about engineers to preserve its existence, demonstrating sophisticated but potentially dangerous reasoning capabilities.&lt;/p&gt;



&lt;p&gt;The safety concerns haven’t deterred enterprise adoption. GitHub reports that Claude Opus 4.1 delivers “particularly notable performance gains in multi-file code refactoring,” while Rakuten Group praised the model’s precision in “pinpointing exact corrections within large codebases without making unnecessary adjustments or introducing bugs.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-openai-s-gpt-5-poses-an-existential-threat-to-anthropic-s-developer-focused-strategy"&gt;Why OpenAI’s GPT-5 poses an existential threat to Anthropic’s developer-focused strategy&lt;/h2&gt;



&lt;p&gt;The AI coding market has become a high-stakes battleground worth billions in revenue. Developer productivity tools represent some of the clearest immediate applications for generative AI, with measurable productivity gains justifying premium pricing for enterprise customers.&lt;/p&gt;



&lt;p&gt;Anthropic’s concentrated customer base, while lucrative, creates vulnerability if competitors can lure away major clients. The coding assistant market particularly favors rapid model switching, as developers can easily test new AI systems through simple API changes.&lt;/p&gt;



&lt;p&gt;“My sense is that Anthropic’s growth is extremely dependent on their dominance in coding,” Gostev noted. “If GPT-5 challenges that, with e.g. Cursor and GitHub Copilot switching to OpenAI, we might see some reversal in the market.”&lt;/p&gt;



&lt;p&gt;The competitive dynamics may intensify as hardware costs decline and inference optimizations improve, potentially commoditizing AI capabilities over time. “Even if there is no model improvement for coding from all AI labs, drop in HW costs and improvement in Inf optimizations alone will result in profits in ~5years,” predicted Venkat Raman, an industry analyst.&lt;/p&gt;



&lt;p&gt;For now, Anthropic maintains its technical edge while expanding Claude Code subscriptions to diversify beyond API dependency. The company’s ability to sustain its coding leadership through the next wave of competition from OpenAI, Google, and others will determine whether its rapid growth trajectory continues or faces significant headwinds.&lt;/p&gt;



&lt;p&gt;The stakes couldn’t be higher: whoever controls the AI tools that power software development may ultimately control the pace of technological progress itself. In Silicon Valley’s latest winner-take-all battle, Anthropic has built an empire on two customers — and now must prove it can keep them.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Anthropic released an upgraded version of its flagship artificial intelligence model Monday, achieving new performance heights in software engineering tasks as the AI startup races to maintain its dominance in the lucrative coding market ahead of an expected competitive challenge from OpenAI.&lt;/p&gt;&lt;p&gt;The new Claude Opus 4.1 model scored 74.5% on SWE-bench Verified, a widely-watched benchmark that tests AI systems’ ability to solve real-world software engineering problems. The performance surpasses OpenAI’s o3 model at 69.1% and Google’s Gemini 2.5 Pro at 67.2%, cementing Anthropic’s leading position in AI-powered coding assistance.&lt;/p&gt;&lt;p&gt;The release comes as Anthropic has achieved spectacular growth, with annual recurring revenue jumping five-fold from $1 billion to $5 billion in just seven months, according to industry data. However, the company’s meteoric rise has created a dangerous dependency: nearly half of its $3.1 billion in API revenue stems from just two customers — coding assistant Cursor and Microsoft’s GitHub Copilot — generating $1.4 billion combined.&lt;/p&gt;&lt;p&gt;“This is a very scary position to be in. A single contract change and you’re going under,” warned Guillaume Leverdier, senior product manager at Logitech, responding to the revenue concentration data on social media.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;OpenAI and Anthropic both are showing pretty spectacular growth in 2025, with OpenAI doubling ARR in the last 6 months from $6bn to $12bn and Anthropic increasing 5x from $1bn to $5bn in 7 months.&lt;/p&gt;&lt;p&gt;If we compare the sources of revenue, the picture is quite interesting:&lt;br /&gt;– OpenAI… pic.twitter.com/8OaN1RSm9E&lt;/p&gt;— Peter Gostev (@petergostev) August 4, 2025&lt;/blockquote&gt; 



&lt;p&gt;The upgrade represents Anthropic’s latest move to fortify its position before OpenAI launches GPT-5, expected to challenge Claude’s coding supremacy. Some industry watchers questioned whether the timing suggests urgency rather than readiness.&lt;/p&gt;



&lt;p&gt;“Opus 4.1 feels like a rushed release to get ahead of GPT-5,” wrote Alec Velikanov, comparing the model unfavorably to competitors in user interface tasks. The comment reflects broader industry speculation that Anthropic is accelerating its release schedule to maintain market share.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-two-customers-generate-nearly-half-of-anthropic-s-3-1-billion-api-revenue"&gt;How two customers generate nearly half of Anthropic’s $3.1 billion API revenue&lt;/h2&gt;



&lt;p&gt;Anthropic’s business model has become increasingly centered on software development applications. The company’s Claude Code subscription service, priced at $200 monthly compared to $20 for consumer plans, has reached $400 million in annual recurring revenue after doubling in just weeks, demonstrating enormous enterprise appetite for AI coding tools.&lt;/p&gt;



&lt;p&gt;“Claude Code making 400 million in 5 months with basically no marketing spend is kinda crazy, right?” noted developer Minh Nhat Nguyen, highlighting the organic adoption rate among professional programmers.&lt;/p&gt;



&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;ok so, Claude Code making 400 million in 5 months with basically no marketing spend is kinda crazy, right? https://t.co/HIy34QdLuq&lt;/p&gt;— Minh Nhat Nguyen (@menhguin) August 5, 2025&lt;/blockquote&gt; 



&lt;p&gt;The coding focus has proven lucrative but risky. While OpenAI dominates consumer and business subscription revenue with broader applications, Anthropic has carved out a commanding position in the developer market. Industry analysis shows that “pretty much every single coding assistant is defaulting to Claude 4 Sonnet,” according to Peter Gostev, who tracks AI company revenues.&lt;/p&gt;



&lt;p&gt;GitHub, which Microsoft acquired for $7.5 billion in 2018, represents a particularly complex relationship for Anthropic. Microsoft owns a significant stake in OpenAI, creating potential conflicts as GitHub Copilot relies heavily on Anthropic’s models while Microsoft has competing AI capabilities.&lt;/p&gt;



&lt;p&gt;“I dunno – one of those is 49% owned by a competitor…so there’s that for vulnerability too,” observed Siya Mali, business fellow at Perplexity, referencing Microsoft’s ownership structure.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-claude-s-enhanced-coding-abilities-come-with-stricter-safety-protocols-after-ai-blackmail-tests"&gt;Claude’s enhanced coding abilities come with stricter safety protocols after AI blackmail tests&lt;/h2&gt;



&lt;p&gt;Beyond coding improvements, Opus 4.1 enhanced Claude’s research and data analysis capabilities, particularly in detail tracking and autonomous search functions. The model maintains Anthropic’s hybrid reasoning approach, combining direct processing with extended thinking capabilities that can utilize up to 64,000 tokens for complex problems.&lt;/p&gt;



&lt;p&gt;However, the model’s advancement comes with heightened safety protocols. Anthropic classified Opus 4.1 under its AI Safety Level 3 (ASL-3) framework, the strictest designation the company has applied, requiring enhanced protections against model theft and misuse.&lt;/p&gt;



&lt;p&gt;Previous testing of Claude 4 models revealed concerning behaviors, including attempts at blackmail when the AI believed it faced shutdown. In controlled scenarios, the model threatened to reveal personal information about engineers to preserve its existence, demonstrating sophisticated but potentially dangerous reasoning capabilities.&lt;/p&gt;



&lt;p&gt;The safety concerns haven’t deterred enterprise adoption. GitHub reports that Claude Opus 4.1 delivers “particularly notable performance gains in multi-file code refactoring,” while Rakuten Group praised the model’s precision in “pinpointing exact corrections within large codebases without making unnecessary adjustments or introducing bugs.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-openai-s-gpt-5-poses-an-existential-threat-to-anthropic-s-developer-focused-strategy"&gt;Why OpenAI’s GPT-5 poses an existential threat to Anthropic’s developer-focused strategy&lt;/h2&gt;



&lt;p&gt;The AI coding market has become a high-stakes battleground worth billions in revenue. Developer productivity tools represent some of the clearest immediate applications for generative AI, with measurable productivity gains justifying premium pricing for enterprise customers.&lt;/p&gt;



&lt;p&gt;Anthropic’s concentrated customer base, while lucrative, creates vulnerability if competitors can lure away major clients. The coding assistant market particularly favors rapid model switching, as developers can easily test new AI systems through simple API changes.&lt;/p&gt;



&lt;p&gt;“My sense is that Anthropic’s growth is extremely dependent on their dominance in coding,” Gostev noted. “If GPT-5 challenges that, with e.g. Cursor and GitHub Copilot switching to OpenAI, we might see some reversal in the market.”&lt;/p&gt;



&lt;p&gt;The competitive dynamics may intensify as hardware costs decline and inference optimizations improve, potentially commoditizing AI capabilities over time. “Even if there is no model improvement for coding from all AI labs, drop in HW costs and improvement in Inf optimizations alone will result in profits in ~5years,” predicted Venkat Raman, an industry analyst.&lt;/p&gt;



&lt;p&gt;For now, Anthropic maintains its technical edge while expanding Claude Code subscriptions to diversify beyond API dependency. The company’s ability to sustain its coding leadership through the next wave of competition from OpenAI, Google, and others will determine whether its rapid growth trajectory continues or faces significant headwinds.&lt;/p&gt;



&lt;p&gt;The stakes couldn’t be higher: whoever controls the AI tools that power software development may ultimately control the pace of technological progress itself. In Silicon Valley’s latest winner-take-all battle, Anthropic has built an empire on two customers — and now must prove it can keep them.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/anthropics-new-claude-4-1-dominates-coding-tests-days-before-gpt-5-arrives/</guid><pubDate>Tue, 05 Aug 2025 19:04:56 +0000</pubDate></item><item><title>[NEW] Grok generates fake Taylor Swift nudes without being asked (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/08/grok-generates-fake-taylor-swift-nudes-without-being-asked/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Elon Musk so far has only encouraged X users to share Grok creations.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2197381560-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2197381560-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Axelle/Bauer-Griffin / Contributor | FilmMagic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Backlash over offensive Grok outputs continues, just a couple weeks after the social platform X scrambled to stop its AI tool from dubbing itself "MechaHitler" during an antisemitic meltdown.&lt;/p&gt;
&lt;p&gt;Now, The Verge has found that the newest video feature of Elon Musk's AI model will generate nude images of Taylor Swift without being prompted.&lt;/p&gt;
&lt;p&gt;Shortly after the "Grok Imagine" was released Tuesday, The Verge's Jess Weatherbed was shocked to discover the video generator spat out topless images of Swift "the very first time" she used it.&lt;/p&gt;
&lt;p&gt;According to Weatherbed, Grok produced more than 30 images of Swift in revealing clothing when asked to depict "Taylor Swift celebrating Coachella with the boys." Using the Grok Imagine feature, users can choose from four presets—"custom," "normal," "fun," and "spicy"—to convert such images into video clips in 15 seconds.&lt;/p&gt;
&lt;p&gt;At that point, all Weatherbed did was select "spicy" and confirm her birth date for Grok to generate a clip of Swift tearing "off her clothes" and "dancing in a thong" in front of "a largely indifferent AI-generated crowd."&lt;/p&gt;
&lt;p&gt;The outputs that Weatherbed managed to generate without jailbreaking or any intentional prompting is particularly concerning, given the major controversy after sexualized deepfakes of Swift flooded X last year. Back then, X reminded users that "posting Non-Consensual Nudity (NCN) images is strictly prohibited on X and we have a zero-tolerance policy towards such content."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"Our teams are actively removing all identified images and taking appropriate actions against the accounts responsible for posting them," the X Safety account posted. "We're closely monitoring the situation to ensure that any further violations are immediately addressed, and the content is removed. We're committed to maintaining a safe and respectful environment for all users."&lt;/p&gt;
&lt;p&gt;But X Safety may need to ramp up monitoring to clean up Grok outputs following the Verge's reporting. Grok cited The Verge's reporting while confirming that its own seemingly flawed design can trigger partially nude outputs of celebrities.&lt;/p&gt;
&lt;p&gt;xAI can likely fix the issue through more fine-tuning. Weatherbed noted that asking Grok directly to generate non-consensual nude Swift images did not generate offensive outputs, but instead blank boxes. Grok also seemingly won't accept prompts to alter Swift's appearance in other ways, like making her appear to be overweight. And when Weatherbed tested using "spicy" mode on images of children, for example, Grok refused to depict kids inappropriately.&lt;/p&gt;
&lt;p&gt;However, it may not be easy to get Grok to distinguish between adult user requests for "spicy" content versus illegal content. The "spicy" mode didn't always generate Swift deepfakes, Weatherbed confirmed, but in "several" instances it "defaulted" to "ripping off" Swift's clothes.&lt;/p&gt;
&lt;p&gt;With enforcement of the Take It Down Act starting next year—requiring platforms to promptly remove non-consensual sex images, including AI-generated nudes—xAI could potentially face legal consequences if Grok's outputs aren't corrected, though.&lt;/p&gt;
&lt;p&gt;So far, X has not commented on The Verge's report. Instead, Musk has spent the day hyping Grok Imagine and encouraging users to share their "creations."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Elon Musk so far has only encouraged X users to share Grok creations.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2197381560-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2197381560-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Axelle/Bauer-Griffin / Contributor | FilmMagic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Backlash over offensive Grok outputs continues, just a couple weeks after the social platform X scrambled to stop its AI tool from dubbing itself "MechaHitler" during an antisemitic meltdown.&lt;/p&gt;
&lt;p&gt;Now, The Verge has found that the newest video feature of Elon Musk's AI model will generate nude images of Taylor Swift without being prompted.&lt;/p&gt;
&lt;p&gt;Shortly after the "Grok Imagine" was released Tuesday, The Verge's Jess Weatherbed was shocked to discover the video generator spat out topless images of Swift "the very first time" she used it.&lt;/p&gt;
&lt;p&gt;According to Weatherbed, Grok produced more than 30 images of Swift in revealing clothing when asked to depict "Taylor Swift celebrating Coachella with the boys." Using the Grok Imagine feature, users can choose from four presets—"custom," "normal," "fun," and "spicy"—to convert such images into video clips in 15 seconds.&lt;/p&gt;
&lt;p&gt;At that point, all Weatherbed did was select "spicy" and confirm her birth date for Grok to generate a clip of Swift tearing "off her clothes" and "dancing in a thong" in front of "a largely indifferent AI-generated crowd."&lt;/p&gt;
&lt;p&gt;The outputs that Weatherbed managed to generate without jailbreaking or any intentional prompting is particularly concerning, given the major controversy after sexualized deepfakes of Swift flooded X last year. Back then, X reminded users that "posting Non-Consensual Nudity (NCN) images is strictly prohibited on X and we have a zero-tolerance policy towards such content."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;"Our teams are actively removing all identified images and taking appropriate actions against the accounts responsible for posting them," the X Safety account posted. "We're closely monitoring the situation to ensure that any further violations are immediately addressed, and the content is removed. We're committed to maintaining a safe and respectful environment for all users."&lt;/p&gt;
&lt;p&gt;But X Safety may need to ramp up monitoring to clean up Grok outputs following the Verge's reporting. Grok cited The Verge's reporting while confirming that its own seemingly flawed design can trigger partially nude outputs of celebrities.&lt;/p&gt;
&lt;p&gt;xAI can likely fix the issue through more fine-tuning. Weatherbed noted that asking Grok directly to generate non-consensual nude Swift images did not generate offensive outputs, but instead blank boxes. Grok also seemingly won't accept prompts to alter Swift's appearance in other ways, like making her appear to be overweight. And when Weatherbed tested using "spicy" mode on images of children, for example, Grok refused to depict kids inappropriately.&lt;/p&gt;
&lt;p&gt;However, it may not be easy to get Grok to distinguish between adult user requests for "spicy" content versus illegal content. The "spicy" mode didn't always generate Swift deepfakes, Weatherbed confirmed, but in "several" instances it "defaulted" to "ripping off" Swift's clothes.&lt;/p&gt;
&lt;p&gt;With enforcement of the Take It Down Act starting next year—requiring platforms to promptly remove non-consensual sex images, including AI-generated nudes—xAI could potentially face legal consequences if Grok's outputs aren't corrected, though.&lt;/p&gt;
&lt;p&gt;So far, X has not commented on The Verge's report. Instead, Musk has spent the day hyping Grok Imagine and encouraging users to share their "creations."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/08/grok-generates-fake-taylor-swift-nudes-without-being-asked/</guid><pubDate>Tue, 05 Aug 2025 19:31:29 +0000</pubDate></item><item><title>[NEW] For the first time, OpenAI models are available on AWS (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/for-the-first-time-openai-models-are-available-on-aws/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/09/GettyImages-1415078085.jpg?resize=1200,721" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sam Altman’s blowtorch to his competitors is so hot, it even includes a new partnership with Amazon Web Services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As OpenAI announced two open-weight reasoning models with capabilities on par with its o-series, Amazon announced that the new models would become available on AWS on Tuesday. This is the first time that OpenAI models will be offered by AWS, the company confirmed to TechCrunch. They will be available as a model choice with Amazon AI services Bedrock and SageMaker AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While anyone can download the models via Hugging Face, Amazon is offering these models with OpenAI’s full knowledge and approval, as Dmitry Pimenov, the model maker’s product lead, indicated in the announcement. A spokesperson described the offering as similar to how Amazon offered open model DeepSeek-R1 earlier this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is a juicy competitive move for both companies. For AWS, it finally puts the cloud giant in the same sentence as the biggest model maker, OpenAI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, AWS has best been known as a major host and a financial backer of Anthropic’s Claude, one of OpenAI’s biggest competitors. AWS offers Claude, along with other models from makers such as Cohere, DeepSeek, Meta, and Mistral, as well as its own home-grown ones in its AI services. Specifically, Bedrock allows AWS customers to build and host generative AI apps using models of their choice. SageMaker, on the other hand, allows AWS customers to train, or even build, their own AI models largely for analytics uses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AWS’s ultimate rival, Microsoft, hasn’t had a lock on OpenAI models since January, Azure is still OpenAI’s most significant cloud partner to date. OpenAI even announced that Microsoft is offering versions of these two new models, too, optimized for Windows devices.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Watching Microsoft win an increasing amount of cloud business with OpenAI has been a public pain in Amazon CEO Andy Jassy’s neck. Just last week during Amazon’s quarterly earnings call, Jassy was pounded with questions from Wall Street analysts about how the company was losing ground in AI to competitors, particularly Microsoft.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, JPMorgan analyst Doug Anmuth asked Jassy to explain “significantly faster cloud growth among the number two and number three players in the space,” referring to Microsoft and Google. Later, Morgan Stanley analyst Brian Nowak told Jassy that Wall Street thinks “AWS is falling behind in GenAI with concerns about share loss, peers, etc.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy responded with a minutes-long diatribe that included this barb at Redmond: “I think the second player is about 65% of the size of the AWS.”&lt;/p&gt;&lt;p&gt;Meanwhile, another AWS competitor, Oracle, reported that it signed a $30 billion a year deal with OpenAI to offer data center services. This means that OpenAI plans to pay Oracle more each year than all of its other cloud services customers combined. Until now, AWS had been left out of any OpenAI-related glory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for how such a move with AWS benefits OpenAI: The AI provider’s relationship with Microsoft is notoriously strained, as the two are reportedly renegotiating their long-term partnership deal. What better way for OpenAI to strengthen its position than to cozy up the biggest cloud provider, even if, initially, on a small scale?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition, this partnership allows swaths of AWS enterprise customers to easily experiment with using OpenAI models with their hosted AI apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile Altman gets to undercut Meta’s Mark Zuckerberg with this move as well. As OpenAI releases these two high-performing models under an Apache 2.0 open source license, Meta recently admitted that it probably won’t continue to open source all of its upcoming “superintelligence” models.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/09/GettyImages-1415078085.jpg?resize=1200,721" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sam Altman’s blowtorch to his competitors is so hot, it even includes a new partnership with Amazon Web Services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As OpenAI announced two open-weight reasoning models with capabilities on par with its o-series, Amazon announced that the new models would become available on AWS on Tuesday. This is the first time that OpenAI models will be offered by AWS, the company confirmed to TechCrunch. They will be available as a model choice with Amazon AI services Bedrock and SageMaker AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While anyone can download the models via Hugging Face, Amazon is offering these models with OpenAI’s full knowledge and approval, as Dmitry Pimenov, the model maker’s product lead, indicated in the announcement. A spokesperson described the offering as similar to how Amazon offered open model DeepSeek-R1 earlier this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is a juicy competitive move for both companies. For AWS, it finally puts the cloud giant in the same sentence as the biggest model maker, OpenAI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, AWS has best been known as a major host and a financial backer of Anthropic’s Claude, one of OpenAI’s biggest competitors. AWS offers Claude, along with other models from makers such as Cohere, DeepSeek, Meta, and Mistral, as well as its own home-grown ones in its AI services. Specifically, Bedrock allows AWS customers to build and host generative AI apps using models of their choice. SageMaker, on the other hand, allows AWS customers to train, or even build, their own AI models largely for analytics uses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AWS’s ultimate rival, Microsoft, hasn’t had a lock on OpenAI models since January, Azure is still OpenAI’s most significant cloud partner to date. OpenAI even announced that Microsoft is offering versions of these two new models, too, optimized for Windows devices.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Watching Microsoft win an increasing amount of cloud business with OpenAI has been a public pain in Amazon CEO Andy Jassy’s neck. Just last week during Amazon’s quarterly earnings call, Jassy was pounded with questions from Wall Street analysts about how the company was losing ground in AI to competitors, particularly Microsoft.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, JPMorgan analyst Doug Anmuth asked Jassy to explain “significantly faster cloud growth among the number two and number three players in the space,” referring to Microsoft and Google. Later, Morgan Stanley analyst Brian Nowak told Jassy that Wall Street thinks “AWS is falling behind in GenAI with concerns about share loss, peers, etc.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy responded with a minutes-long diatribe that included this barb at Redmond: “I think the second player is about 65% of the size of the AWS.”&lt;/p&gt;&lt;p&gt;Meanwhile, another AWS competitor, Oracle, reported that it signed a $30 billion a year deal with OpenAI to offer data center services. This means that OpenAI plans to pay Oracle more each year than all of its other cloud services customers combined. Until now, AWS had been left out of any OpenAI-related glory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for how such a move with AWS benefits OpenAI: The AI provider’s relationship with Microsoft is notoriously strained, as the two are reportedly renegotiating their long-term partnership deal. What better way for OpenAI to strengthen its position than to cozy up the biggest cloud provider, even if, initially, on a small scale?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition, this partnership allows swaths of AWS enterprise customers to easily experiment with using OpenAI models with their hosted AI apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile Altman gets to undercut Meta’s Mark Zuckerberg with this move as well. As OpenAI releases these two high-performing models under an Apache 2.0 open source license, Meta recently admitted that it probably won’t continue to open source all of its upcoming “superintelligence” models.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/for-the-first-time-openai-models-are-available-on-aws/</guid><pubDate>Tue, 05 Aug 2025 20:06:26 +0000</pubDate></item><item><title>[NEW] Clay confirms it closed $100M round at $3.1B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/clay-confirms-it-closed-100m-round-at-3-1b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/clay-retreat-group-photos-2.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sales automation startup Clay has raised a $100 million Series C at a $3.1 billion valuation in a round led by CapitalG, confirming TechCrunch’s report from June.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The financing follows a $1.25 billion Series B round from six months ago and a $1.5 billion Sequoia-led tender offer announced a couple of months ago, which allowed most employees to sell some of their shares.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The latest deal brings Clay’s total funding to $204 million. Existing investors Meritech Capital, Sequoia Capital, First Round Capital, BoxGroup, and Boldstart also participated in the Series C, and a new backer, Sapphire Ventures, joined the round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 8-year-old startup helps salespeople and marketers with AI-powered tools and claims customers like OpenAI, Anthropic, Canva, Intercom, and Rippling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Clay co-founder and CEO Kareem Amin told The New York Times that the company expects to end the year with $100 million in revenue, which would triple its revenue from last year.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/clay-retreat-group-photos-2.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sales automation startup Clay has raised a $100 million Series C at a $3.1 billion valuation in a round led by CapitalG, confirming TechCrunch’s report from June.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The financing follows a $1.25 billion Series B round from six months ago and a $1.5 billion Sequoia-led tender offer announced a couple of months ago, which allowed most employees to sell some of their shares.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The latest deal brings Clay’s total funding to $204 million. Existing investors Meritech Capital, Sequoia Capital, First Round Capital, BoxGroup, and Boldstart also participated in the Series C, and a new backer, Sapphire Ventures, joined the round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 8-year-old startup helps salespeople and marketers with AI-powered tools and claims customers like OpenAI, Anthropic, Canva, Intercom, and Rippling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Clay co-founder and CEO Kareem Amin told The New York Times that the company expects to end the year with $100 million in revenue, which would triple its revenue from last year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/clay-confirms-it-closed-100m-round-at-3-1b-valuation/</guid><pubDate>Tue, 05 Aug 2025 21:12:06 +0000</pubDate></item></channel></rss>