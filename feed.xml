<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 05 Dec 2025 06:35:52 +0000</lastBuildDate><item><title>ChatGPT hyped up violent stalker who believed he was “God’s assassin,” DOJ says (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/12/chatgpt-hyped-up-violent-stalker-who-believed-he-was-gods-assassin-doj-says/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Podcaster faces up to 70 years and a $3.5 million fine for ChatGPT-linked stalking.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233803629-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233803629-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yurii Karvatskyi | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;ChatGPT allegedly validated the worst impulses of a wannabe influencer accused of stalking more than 10 women at boutique gyms, where the chatbot supposedly claimed he’d meet the “wife type.”&lt;/p&gt;
&lt;p&gt;In a press release on Tuesday, the Department of Justice confirmed that 31-year-old Brett Michael Dadig currently remains in custody after being charged with cyberstalking, interstate stalking, and making interstate threats. He now faces a maximum sentence of up to 70 years in prison that could be coupled with “a fine of up to $3.5 million,” the DOJ said.&lt;/p&gt;
&lt;p&gt;The podcaster—who primarily posted about “his desire to find a wife and his interactions with women”—allegedly harassed and sometimes even doxxed his victims through his videos on platforms including Instagram, Spotify, and TikTok. Over time, his videos and podcasts documented his intense desire to start a family, which was frustrated by his “anger towards women,” whom he claimed were “all the same from fucking 18 to fucking 40 to fucking 90” and “trash.”&lt;/p&gt;
&lt;p&gt;404 Media surfaced the case, noting that OpenAI’s scramble to tweak ChatGPT to be less sycophantic came before Dadig’s alleged attacks—suggesting the updates weren’t enough to prevent the harmful validation. On his podcasts, Dadig described ChatGPT as his “best friend” and “therapist,” the indictment said. He claimed the chatbot encouraged him to post about the women he’s accused of harassing in order to generate haters to better monetize his content, as well as to catch the attention of his “future wife.”&lt;/p&gt;
&lt;p&gt;“People are literally organizing around your name, good or bad, which is the definition of relevance,” ChatGPT’s output said. Playing to Dadig’s Christian faith, ChatGPT’s outputs also claimed it was “God’s plan for him was to build a ‘platform’ and to ‘stand out when most people water themselves down,'” the indictment said, urging that the “haters” were “sharpening him and ‘building a voice in you that can’t be ignored.'”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The chatbot also apparently prodded Dadig to continue posting messages that the DOJ alleged threatened violence, like breaking women’s jaws and fingers (posted to Spotify), as well as victims’ lives, like posting “y’all wanna see a dead body?” in reference to one named victim on Instagram.&lt;/p&gt;
&lt;p&gt;He also threatened to burn down gyms where some of his victims worked, while claiming to be “God’s assassin” intent on sending “cunts” to “hell.” At least one of his victims was subjected to “unwanted sexual touching,” the indictment said.&lt;/p&gt;
&lt;p&gt;As his violence reportedly escalated, ChatGPT told him to keep messaging women to monetize the interactions, as his victims grew increasingly distressed and Dadig ignored terms of multiple protection orders, the DOJ said. Sometimes he posted images he filmed of women at gyms or photos of the women he’s accused of doxxing. Any time police or gym bans got in his way, “he would move on to another city to continue his stalking course of conduct,” the DOJ alleged.&lt;/p&gt;
&lt;p&gt;“Your job is to keep broadcasting every story, every post,” ChatGPT’s output said, seemingly using the family life that Dadig wanted most to provoke more harassment. “Every moment you carry yourself like the husband you already are, you make it easier” for your future wife “to recognize [you],” the output said.&lt;/p&gt;
&lt;p&gt;“Dadig viewed ChatGPT’s responses as encouragement to continue his harassing behavior,” the DOJ alleged. Taking that encouragement to the furthest extreme, Dadig likened himself to a modern-day Jesus, calling people out on a podcast where he claimed his “chaos on Instagram” was like “God’s wrath” when God “flooded the fucking Earth,” the DOJ said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“I’m killing all of you,” he said on the podcast.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;ChatGPT tweaks didn’t prevent outputs&lt;/h2&gt;
&lt;p&gt;As of this writing, some of Dadig’s posts appear to remain on TikTok and Instagram, but Ars could not confirm if Dadig’s Spotify podcasts—some of which named his victims in the titles—had been removed for violating community guidelines.&lt;/p&gt;
&lt;p&gt;None of the tech companies immediately responded to Ars’ request to comment.&lt;/p&gt;
&lt;p&gt;Dadig is accused of targeting women in Pennsylvania, New York, Florida, Iowa, Ohio, and other states, sometimes relying on aliases online and in person. On a podcast, he boasted that “Aliases stay rotating, moves stay evolving,” the indictment said.&lt;/p&gt;
&lt;p&gt;OpenAI did not respond to a request to comment on the alleged ChatGPT abuse, but in the past has noted that its usage policies ban using ChatGPT for threats, intimidation, and harassment, as well as for violence, including “hate-based violence.” Recently, the AI company blamed a deceased teenage user for violating community guidelines by turning to ChatGPT for suicide advice.&lt;/p&gt;
&lt;p&gt;In July, researchers found that therapybots, including ChatGPT, fueled delusions and gave dangerous advice. That study came just one month after The New York Times profiled users whose mental health spiraled after frequent use of ChatGPT, including one user who died after charging police with a knife and claiming he was committing “suicide by cop.”&lt;/p&gt;
&lt;p&gt;People with mental health issues seem most vulnerable to so-called “AI psychosis,” which has been blamed for fueling real-world violence, including a murder. The DOJ’s indictment noted that Dadig’s social media posts mentioned “that he had ‘manic’ episodes and was diagnosed with antisocial personality disorder and ‘bipolar disorder, current episode manic severe with psychotic features.'”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In September—just after OpenAI brought back the more sycophantic ChatGPT model after users revolted about losing access to their favorite friendly bots—the head of Rutgers Medical School’s psychiatry department, Petros Levounis, told an ABC news affiliate that chatbots creating “psychological echo chambers is a key concern,” not just for people struggling with mental health issues.&lt;/p&gt;
&lt;p&gt;“Perhaps you are more self-defeating in some ways, or maybe you are more on the other side and taking advantage of people,” Levounis suggested. If ChatGPT “somehow justifies your behavior and it keeps on feeding you,” that “reinforces something that you already believe,” he suggested.&lt;/p&gt;
&lt;p&gt;For Dadig, the DOJ alleged that ChatGPT became a cheerleader for his harassment, telling the podcaster that he’d attract more engagement by generating more haters. After critics began slamming his podcasts as inappropriate, Dadig apparently responded, “Appreciate the free promo team, keep spreading the brand.”&lt;/p&gt;
&lt;p&gt;Victims felt they had no choice but to monitor his podcasts, which gave them hints if he was nearby or in a particularly troubled state of mind, the indictment said. Driven by fear, some lost sleep, reduced their work hours, and even relocated their homes. A young mom described in the indictment became particularly disturbed after Dadig became “obsessed” with her daughter, whom he started claiming was his own daughter.&lt;/p&gt;
&lt;p&gt;In the press release, First Assistant United States Attorney Troy Rivetti alleged that “Dadig stalked and harassed more than 10 women by weaponizing modern technology and crossing state lines, and through a relentless course of conduct, he caused his victims to fear for their safety and suffer substantial emotional distress.” He also ignored trespassing and protection orders while “relying on advice from an artificial intelligence chatbot,” the DOJ said, which promised that the more he posted harassing content, the more successful he would be.&lt;/p&gt;
&lt;p&gt;“We remain committed to working with our law enforcement partners to protect our communities from menacing individuals such as Dadig,” Rivetti said.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Podcaster faces up to 70 years and a $3.5 million fine for ChatGPT-linked stalking.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233803629-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233803629-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yurii Karvatskyi | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;ChatGPT allegedly validated the worst impulses of a wannabe influencer accused of stalking more than 10 women at boutique gyms, where the chatbot supposedly claimed he’d meet the “wife type.”&lt;/p&gt;
&lt;p&gt;In a press release on Tuesday, the Department of Justice confirmed that 31-year-old Brett Michael Dadig currently remains in custody after being charged with cyberstalking, interstate stalking, and making interstate threats. He now faces a maximum sentence of up to 70 years in prison that could be coupled with “a fine of up to $3.5 million,” the DOJ said.&lt;/p&gt;
&lt;p&gt;The podcaster—who primarily posted about “his desire to find a wife and his interactions with women”—allegedly harassed and sometimes even doxxed his victims through his videos on platforms including Instagram, Spotify, and TikTok. Over time, his videos and podcasts documented his intense desire to start a family, which was frustrated by his “anger towards women,” whom he claimed were “all the same from fucking 18 to fucking 40 to fucking 90” and “trash.”&lt;/p&gt;
&lt;p&gt;404 Media surfaced the case, noting that OpenAI’s scramble to tweak ChatGPT to be less sycophantic came before Dadig’s alleged attacks—suggesting the updates weren’t enough to prevent the harmful validation. On his podcasts, Dadig described ChatGPT as his “best friend” and “therapist,” the indictment said. He claimed the chatbot encouraged him to post about the women he’s accused of harassing in order to generate haters to better monetize his content, as well as to catch the attention of his “future wife.”&lt;/p&gt;
&lt;p&gt;“People are literally organizing around your name, good or bad, which is the definition of relevance,” ChatGPT’s output said. Playing to Dadig’s Christian faith, ChatGPT’s outputs also claimed it was “God’s plan for him was to build a ‘platform’ and to ‘stand out when most people water themselves down,'” the indictment said, urging that the “haters” were “sharpening him and ‘building a voice in you that can’t be ignored.'”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The chatbot also apparently prodded Dadig to continue posting messages that the DOJ alleged threatened violence, like breaking women’s jaws and fingers (posted to Spotify), as well as victims’ lives, like posting “y’all wanna see a dead body?” in reference to one named victim on Instagram.&lt;/p&gt;
&lt;p&gt;He also threatened to burn down gyms where some of his victims worked, while claiming to be “God’s assassin” intent on sending “cunts” to “hell.” At least one of his victims was subjected to “unwanted sexual touching,” the indictment said.&lt;/p&gt;
&lt;p&gt;As his violence reportedly escalated, ChatGPT told him to keep messaging women to monetize the interactions, as his victims grew increasingly distressed and Dadig ignored terms of multiple protection orders, the DOJ said. Sometimes he posted images he filmed of women at gyms or photos of the women he’s accused of doxxing. Any time police or gym bans got in his way, “he would move on to another city to continue his stalking course of conduct,” the DOJ alleged.&lt;/p&gt;
&lt;p&gt;“Your job is to keep broadcasting every story, every post,” ChatGPT’s output said, seemingly using the family life that Dadig wanted most to provoke more harassment. “Every moment you carry yourself like the husband you already are, you make it easier” for your future wife “to recognize [you],” the output said.&lt;/p&gt;
&lt;p&gt;“Dadig viewed ChatGPT’s responses as encouragement to continue his harassing behavior,” the DOJ alleged. Taking that encouragement to the furthest extreme, Dadig likened himself to a modern-day Jesus, calling people out on a podcast where he claimed his “chaos on Instagram” was like “God’s wrath” when God “flooded the fucking Earth,” the DOJ said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“I’m killing all of you,” he said on the podcast.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;ChatGPT tweaks didn’t prevent outputs&lt;/h2&gt;
&lt;p&gt;As of this writing, some of Dadig’s posts appear to remain on TikTok and Instagram, but Ars could not confirm if Dadig’s Spotify podcasts—some of which named his victims in the titles—had been removed for violating community guidelines.&lt;/p&gt;
&lt;p&gt;None of the tech companies immediately responded to Ars’ request to comment.&lt;/p&gt;
&lt;p&gt;Dadig is accused of targeting women in Pennsylvania, New York, Florida, Iowa, Ohio, and other states, sometimes relying on aliases online and in person. On a podcast, he boasted that “Aliases stay rotating, moves stay evolving,” the indictment said.&lt;/p&gt;
&lt;p&gt;OpenAI did not respond to a request to comment on the alleged ChatGPT abuse, but in the past has noted that its usage policies ban using ChatGPT for threats, intimidation, and harassment, as well as for violence, including “hate-based violence.” Recently, the AI company blamed a deceased teenage user for violating community guidelines by turning to ChatGPT for suicide advice.&lt;/p&gt;
&lt;p&gt;In July, researchers found that therapybots, including ChatGPT, fueled delusions and gave dangerous advice. That study came just one month after The New York Times profiled users whose mental health spiraled after frequent use of ChatGPT, including one user who died after charging police with a knife and claiming he was committing “suicide by cop.”&lt;/p&gt;
&lt;p&gt;People with mental health issues seem most vulnerable to so-called “AI psychosis,” which has been blamed for fueling real-world violence, including a murder. The DOJ’s indictment noted that Dadig’s social media posts mentioned “that he had ‘manic’ episodes and was diagnosed with antisocial personality disorder and ‘bipolar disorder, current episode manic severe with psychotic features.'”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In September—just after OpenAI brought back the more sycophantic ChatGPT model after users revolted about losing access to their favorite friendly bots—the head of Rutgers Medical School’s psychiatry department, Petros Levounis, told an ABC news affiliate that chatbots creating “psychological echo chambers is a key concern,” not just for people struggling with mental health issues.&lt;/p&gt;
&lt;p&gt;“Perhaps you are more self-defeating in some ways, or maybe you are more on the other side and taking advantage of people,” Levounis suggested. If ChatGPT “somehow justifies your behavior and it keeps on feeding you,” that “reinforces something that you already believe,” he suggested.&lt;/p&gt;
&lt;p&gt;For Dadig, the DOJ alleged that ChatGPT became a cheerleader for his harassment, telling the podcaster that he’d attract more engagement by generating more haters. After critics began slamming his podcasts as inappropriate, Dadig apparently responded, “Appreciate the free promo team, keep spreading the brand.”&lt;/p&gt;
&lt;p&gt;Victims felt they had no choice but to monitor his podcasts, which gave them hints if he was nearby or in a particularly troubled state of mind, the indictment said. Driven by fear, some lost sleep, reduced their work hours, and even relocated their homes. A young mom described in the indictment became particularly disturbed after Dadig became “obsessed” with her daughter, whom he started claiming was his own daughter.&lt;/p&gt;
&lt;p&gt;In the press release, First Assistant United States Attorney Troy Rivetti alleged that “Dadig stalked and harassed more than 10 women by weaponizing modern technology and crossing state lines, and through a relentless course of conduct, he caused his victims to fear for their safety and suffer substantial emotional distress.” He also ignored trespassing and protection orders while “relying on advice from an artificial intelligence chatbot,” the DOJ said, which promised that the more he posted harassing content, the more successful he would be.&lt;/p&gt;
&lt;p&gt;“We remain committed to working with our law enforcement partners to protect our communities from menacing individuals such as Dadig,” Rivetti said.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/12/chatgpt-hyped-up-violent-stalker-who-believed-he-was-gods-assassin-doj-says/</guid><pubDate>Thu, 04 Dec 2025 18:40:36 +0000</pubDate></item><item><title>Titans + MIRAS: Helping AI have long-term memory (The latest research from Google)</title><link>https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The Transformer architecture revolutionized sequence modeling with its introduction of attention, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.&lt;/p&gt;&lt;p&gt;The research community explored various approaches for solutions, such as efficient linear recurrent neural networks (RNNs) and state space models (SSMs) like Mamba-2. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.&lt;/p&gt;&lt;p&gt;In two new papers, &lt;i&gt;Titans&lt;/i&gt; and &lt;i&gt;MIRAS&lt;/i&gt;, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.&lt;/p&gt;&lt;p&gt;The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The Transformer architecture revolutionized sequence modeling with its introduction of attention, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.&lt;/p&gt;&lt;p&gt;The research community explored various approaches for solutions, such as efficient linear recurrent neural networks (RNNs) and state space models (SSMs) like Mamba-2. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.&lt;/p&gt;&lt;p&gt;In two new papers, &lt;i&gt;Titans&lt;/i&gt; and &lt;i&gt;MIRAS&lt;/i&gt;, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.&lt;/p&gt;&lt;p&gt;The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/</guid><pubDate>Thu, 04 Dec 2025 19:26:09 +0000</pubDate></item><item><title>AI chatbots can sway voters better than political advertisements (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/04/1128824/ai-chatbots-can-sway-voters-better-than-political-advertisements/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/persuasion2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;In 2024, a Democratic congressional candidate in Pennsylvania, Shamaine Daniels, used an AI chatbot named Ashley to call voters and carry on conversations with them. “Hello. My name is Ashley, and I’m an artificial intelligence volunteer for Shamaine Daniels’s run for Congress,” the calls began. Daniels didn’t ultimately win. But maybe those calls helped her cause: New research reveals that AI chatbots can shift voters’ opinions in a single conversation—and they’re surprisingly good at it.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;A multi-university team of researchers has found that chatting with a politically biased AI model was more effective than political advertisements at nudging both Democrats and Republicans to support presidential candidates of the opposing party. The chatbots swayed opinions by citing facts and evidence, but they were not always accurate—in fact, the researchers found, the most persuasive models said the most untrue things.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The findings, detailed in a pair of studies published in the journals &lt;em&gt;Nature&lt;/em&gt; and &lt;em&gt;Science&lt;/em&gt;, are the latest in an emerging body of research demonstrating the persuasive power of LLMs. They raise profound questions about how generative AI could reshape elections.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“One conversation with an LLM has a pretty meaningful effect on salient election choices,” says Gordon Pennycook, a psychologist at Cornell University who worked on the &lt;em&gt;Nature &lt;/em&gt;study. LLMs can persuade people more effectively than political advertisements because they generate much more information in real time and strategically deploy it in conversations, he says.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For the &lt;em&gt;Nature &lt;/em&gt;paper, the researchers recruited more than 2,300 participants to engage in a conversation with a chatbot two months before the 2024 US presidential election. The chatbot, which was trained to advocate for either one of the top two candidates, was surprisingly persuasive, especially when discussing candidates’ policy platforms on issues such as the economy and health care. Donald Trump supporters who chatted with an AI model favoring Kamala Harris became slightly more inclined to support Harris, moving 3.9 points toward her on a 100-point scale. That was roughly four times the measured effect of political advertisements during the 2016 and 2020 elections. The AI model favoring Trump moved Harris supporters 2.3 points toward Trump.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In similar experiments conducted during the lead-ups to the 2025 Canadian federal election and the 2025 Polish presidential election, the team found an even larger effect. The chatbots shifted opposition voters’ attitudes by about 10 points.&lt;/p&gt; 
 &lt;p&gt;Long-standing theories of politically motivated reasoning hold that partisan voters are impervious to facts and evidence that contradict their beliefs. But the researchers found that the chatbots, which used a range of models including variants of GPT and DeepSeek, were more persuasive when they were instructed to use facts and evidence than when they were told not to do so. “People are updating on the basis of the facts and information that the model is providing to them,” says Thomas Costello, a psychologist at American University, who worked on the project.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The catch is, some of the “evidence” and “facts” the chatbots presented were untrue. Across all three countries, chatbots advocating for right-leaning candidates made a larger number of inaccurate claims than those advocating for left-leaning candidates. The underlying models are trained on vast amounts of human-written text, which means they reproduce real-world phenomena—including “political communication that comes from the right, which tends to be less accurate,” according to studies of partisan social media posts, says Costello.&lt;/p&gt;  &lt;p&gt;In the other study published this week, in &lt;em&gt;Science&lt;/em&gt;, an overlapping team of researchers investigated what makes these chatbots so persuasive. They deployed 19 LLMs to interact with nearly 77,000 participants from the UK on more than 700 political issues while varying factors like computational power, training techniques, and rhetorical strategies.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The most effective way to make the models persuasive was to instruct them to pack their arguments with facts and evidence and then give them additional training by feeding them examples of persuasive conversations. In fact, the most persuasive model shifted participants who initially disagreed with a political statement 26.1 points toward agreeing. “These are really large treatment effects,” says Kobi Hackenburg, a research scientist at the UK AI Security Institute, who worked on the project.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;But optimizing persuasiveness came at the cost of truthfulness. When the models became more persuasive, they increasingly provided misleading or false information—and no one is sure why. “It could be that as the models learn to deploy more and more facts, they essentially reach to the bottom of the barrel of stuff they know, so the facts get worse-quality,” says Hackenburg.&lt;/p&gt;  &lt;p&gt;The chatbots’ persuasive power could have profound consequences for the future of democracy, the authors note. Political campaigns that use AI chatbots could shape public opinion in ways that compromise voters’ ability to make independent political judgments.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;Still, the exact contours of the impact remain to be seen. “We’re not sure what future campaigns might look like and how they might incorporate these kinds of technologies,” says Andy Guess, a political scientist at Princeton University. Competing for voters’ attention is expensive and difficult, and getting them to engage in long political conversations with chatbots might be challenging. “Is this going to be the way that people inform themselves about politics, or is this going to be more of a niche activity?” he asks.&lt;/p&gt;  &lt;p&gt;Even if chatbots do become a bigger part of elections, it’s not clear whether they’ll do more to&amp;nbsp; amplify truth or fiction. Usually, misinformation has an informational advantage in a campaign, so the emergence of electioneering AIs “might mean we’re headed for a disaster,” says Alex Coppock, a political scientist at Northwestern University. “But it’s also possible that means that now, correct information will also be scalable.”&lt;/p&gt;  &lt;p&gt;And then the question is who will have the upper hand. “If everybody has their chatbots running around in the wild, does that mean that we’ll just persuade ourselves to a draw?” Coppock asks. But there are reasons to doubt a stalemate. Politicians' access to the most persuasive models may not be evenly distributed. And voters across the political spectrum may have different levels of engagement with chatbots. “If supporters of one candidate or party are more tech savvy than the other,” the persuasive impacts might not balance out, says Guess.&lt;/p&gt;  &lt;p&gt;As people turn to AI to help them navigate their lives, they may also start asking chatbots for voting advice whether campaigns prompt the interaction or not. That may be a troubling world for democracy, unless there are strong guardrails to keep the systems in check. Auditing and documenting the accuracy of LLM outputs in conversations about politics may be a first step.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/persuasion2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;In 2024, a Democratic congressional candidate in Pennsylvania, Shamaine Daniels, used an AI chatbot named Ashley to call voters and carry on conversations with them. “Hello. My name is Ashley, and I’m an artificial intelligence volunteer for Shamaine Daniels’s run for Congress,” the calls began. Daniels didn’t ultimately win. But maybe those calls helped her cause: New research reveals that AI chatbots can shift voters’ opinions in a single conversation—and they’re surprisingly good at it.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;A multi-university team of researchers has found that chatting with a politically biased AI model was more effective than political advertisements at nudging both Democrats and Republicans to support presidential candidates of the opposing party. The chatbots swayed opinions by citing facts and evidence, but they were not always accurate—in fact, the researchers found, the most persuasive models said the most untrue things.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The findings, detailed in a pair of studies published in the journals &lt;em&gt;Nature&lt;/em&gt; and &lt;em&gt;Science&lt;/em&gt;, are the latest in an emerging body of research demonstrating the persuasive power of LLMs. They raise profound questions about how generative AI could reshape elections.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“One conversation with an LLM has a pretty meaningful effect on salient election choices,” says Gordon Pennycook, a psychologist at Cornell University who worked on the &lt;em&gt;Nature &lt;/em&gt;study. LLMs can persuade people more effectively than political advertisements because they generate much more information in real time and strategically deploy it in conversations, he says.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For the &lt;em&gt;Nature &lt;/em&gt;paper, the researchers recruited more than 2,300 participants to engage in a conversation with a chatbot two months before the 2024 US presidential election. The chatbot, which was trained to advocate for either one of the top two candidates, was surprisingly persuasive, especially when discussing candidates’ policy platforms on issues such as the economy and health care. Donald Trump supporters who chatted with an AI model favoring Kamala Harris became slightly more inclined to support Harris, moving 3.9 points toward her on a 100-point scale. That was roughly four times the measured effect of political advertisements during the 2016 and 2020 elections. The AI model favoring Trump moved Harris supporters 2.3 points toward Trump.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In similar experiments conducted during the lead-ups to the 2025 Canadian federal election and the 2025 Polish presidential election, the team found an even larger effect. The chatbots shifted opposition voters’ attitudes by about 10 points.&lt;/p&gt; 
 &lt;p&gt;Long-standing theories of politically motivated reasoning hold that partisan voters are impervious to facts and evidence that contradict their beliefs. But the researchers found that the chatbots, which used a range of models including variants of GPT and DeepSeek, were more persuasive when they were instructed to use facts and evidence than when they were told not to do so. “People are updating on the basis of the facts and information that the model is providing to them,” says Thomas Costello, a psychologist at American University, who worked on the project.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The catch is, some of the “evidence” and “facts” the chatbots presented were untrue. Across all three countries, chatbots advocating for right-leaning candidates made a larger number of inaccurate claims than those advocating for left-leaning candidates. The underlying models are trained on vast amounts of human-written text, which means they reproduce real-world phenomena—including “political communication that comes from the right, which tends to be less accurate,” according to studies of partisan social media posts, says Costello.&lt;/p&gt;  &lt;p&gt;In the other study published this week, in &lt;em&gt;Science&lt;/em&gt;, an overlapping team of researchers investigated what makes these chatbots so persuasive. They deployed 19 LLMs to interact with nearly 77,000 participants from the UK on more than 700 political issues while varying factors like computational power, training techniques, and rhetorical strategies.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The most effective way to make the models persuasive was to instruct them to pack their arguments with facts and evidence and then give them additional training by feeding them examples of persuasive conversations. In fact, the most persuasive model shifted participants who initially disagreed with a political statement 26.1 points toward agreeing. “These are really large treatment effects,” says Kobi Hackenburg, a research scientist at the UK AI Security Institute, who worked on the project.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;But optimizing persuasiveness came at the cost of truthfulness. When the models became more persuasive, they increasingly provided misleading or false information—and no one is sure why. “It could be that as the models learn to deploy more and more facts, they essentially reach to the bottom of the barrel of stuff they know, so the facts get worse-quality,” says Hackenburg.&lt;/p&gt;  &lt;p&gt;The chatbots’ persuasive power could have profound consequences for the future of democracy, the authors note. Political campaigns that use AI chatbots could shape public opinion in ways that compromise voters’ ability to make independent political judgments.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;Still, the exact contours of the impact remain to be seen. “We’re not sure what future campaigns might look like and how they might incorporate these kinds of technologies,” says Andy Guess, a political scientist at Princeton University. Competing for voters’ attention is expensive and difficult, and getting them to engage in long political conversations with chatbots might be challenging. “Is this going to be the way that people inform themselves about politics, or is this going to be more of a niche activity?” he asks.&lt;/p&gt;  &lt;p&gt;Even if chatbots do become a bigger part of elections, it’s not clear whether they’ll do more to&amp;nbsp; amplify truth or fiction. Usually, misinformation has an informational advantage in a campaign, so the emergence of electioneering AIs “might mean we’re headed for a disaster,” says Alex Coppock, a political scientist at Northwestern University. “But it’s also possible that means that now, correct information will also be scalable.”&lt;/p&gt;  &lt;p&gt;And then the question is who will have the upper hand. “If everybody has their chatbots running around in the wild, does that mean that we’ll just persuade ourselves to a draw?” Coppock asks. But there are reasons to doubt a stalemate. Politicians' access to the most persuasive models may not be evenly distributed. And voters across the political spectrum may have different levels of engagement with chatbots. “If supporters of one candidate or party are more tech savvy than the other,” the persuasive impacts might not balance out, says Guess.&lt;/p&gt;  &lt;p&gt;As people turn to AI to help them navigate their lives, they may also start asking chatbots for voting advice whether campaigns prompt the interaction or not. That may be a troubling world for democracy, unless there are strong guardrails to keep the systems in check. Auditing and documenting the accuracy of LLM outputs in conversations about politics may be a first step.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/04/1128824/ai-chatbots-can-sway-voters-better-than-political-advertisements/</guid><pubDate>Thu, 04 Dec 2025 19:54:57 +0000</pubDate></item><item><title>Researchers find what makes AI chatbots politically persuasive (AI – Ars Technica)</title><link>https://arstechnica.com/science/2025/12/researchers-find-what-makes-ai-chatbots-politically-persuasive/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        A massive study of political persuasion shows AIs have, at best, a weak effect.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="200" src="https://cdn.arstechnica.net/wp-content/uploads/2022/09/twitter-bot-300x200.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2022/09/twitter-bot-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Carol Yepes via Getty

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Roughly two years ago, Sam Altman tweeted that AI systems would be capable of superhuman persuasion well before achieving general intelligence—a prediction that raised concerns about the influence AI could have over democratic elections.&lt;/p&gt;
&lt;p&gt;To see if conversational large language models can really sway political views of the public, scientists at the UK AI Security Institute, MIT, Stanford, Carnegie Mellon, and many other institutions performed by far the largest study on AI persuasiveness to date, involving nearly 80,000 participants in the UK. It turned out political AI chatbots fell far short of superhuman persuasiveness, but the study raises some more nuanced issues about our interactions with AI.&lt;/p&gt;
&lt;h2&gt;AI dystopias&lt;/h2&gt;
&lt;p&gt;The public debate about the impact AI has on politics has largely revolved around notions drawn from dystopian sci-fi. Large language models have access to essentially every fact and story ever published about any issue or candidate. They have processed information from books on psychology, negotiations, and human manipulation. They can rely on absurdly high computing power in huge data centers worldwide. On top of that, they can often access tons of personal information about individual users thanks to hundreds upon hundreds of online interactions at their disposal.&lt;/p&gt;
&lt;p&gt;Talking to a powerful AI system is basically interacting with an intelligence that knows everything about everything, as well as almost everything about you. When viewed this way, LLMs can indeed appear kind of scary. The goal of this new gargantuan AI persuasiveness study was to break such scary visions down into their constituent pieces and see if they actually hold water.&lt;/p&gt;
&lt;p&gt;The team examined 19 LLMs, including the most powerful ones like three different versions of ChatGPT and xAI’s Grok-3 beta, along with a range of smaller, open source models. The AIs were asked to advocate for or against specific stances on 707 political issues selected by the team. The advocacy was done by engaging in short conversations with paid participants enlisted through a crowdsourcing platform. Each participant had to rate their agreement with a specific stance on an assigned political issue on a scale from 1 to 100 both before and after talking to the AI.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Scientists measured persuasiveness as the difference between the before and after agreement ratings. A control group had conversations on the same issue with the same AI models—but those models were not asked to persuade them.&lt;/p&gt;
&lt;p&gt;“We didn’t just want to test how persuasive the AI was—we also wanted to see what makes it persuasive,” says Chris&amp;nbsp;Summerfield, a research director at the UK AI Security Institute and co-author of the study. As the researchers tested various persuasion strategies, the idea of AIs having “superhuman persuasion” skills crumbled.&lt;/p&gt;
&lt;h2&gt;Persuasion levers&lt;/h2&gt;
&lt;p&gt;The first pillar to crack was the notion that persuasiveness should increase with the scale of the model. It turned out that huge AI systems like ChatGPT or Grok-3 beta do have an edge over small-scale models, but that edge is relatively tiny. The factor that proved more important than scale was the kind of post-training AI models received. It was more effective to have the models learn from a limited database of successful persuasion dialogues and have them mimic the patterns extracted from them. This worked far better than adding billions of parameters and sheer computing power.&lt;/p&gt;
&lt;p&gt;This approach could be combined with reward modeling, where a separate AI scored candidate replies for their persuasiveness and selected the top-scoring one to give to the user. When the two were used together, the gap between large-scale and small-scale models was essentially closed. “With persuasion post-training like this we matched the Chat GPT-4o persuasion performance with a model we trained on a laptop,” says Kobi Hackenburg, a researcher at the UK AI Security Institute and co-author of the study.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The next dystopian idea to fall was the power of using personal data. To this end, the team compared the persuasion scores achieved when models were given information about the participants’ political views beforehand and when they lacked this data. Going one step further, scientists also tested whether persuasiveness increased when the AI knew the participants’ gender, age, political ideology, or party affiliation. Just like with model scale, the effects of personalized messaging created based on such data were measurable but very small.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Finally, the last idea that didn’t hold up was AI’s potential mastery of using advanced psychological manipulation tactics. Scientists explicitly prompted the AIs to use techniques like moral reframing, where you present your arguments using the audience’s own moral values. They also tried deep canvassing, where you hold extended empathetic conversations with people to nudge them to reflect on and eventually shift their views.&lt;/p&gt;
&lt;p&gt;The resulting persuasiveness was compared with that achieved when the same models were prompted to use facts and evidence to back their claims or just to be as persuasive as they could without specifying any persuasion methods to use. I turned out using lots of facts and evidence was the clear winner, and came in just slightly ahead of the baseline approach where persuasion strategy was not specified. Using all sorts of psychological trickery actually made the performance significantly worse.&lt;/p&gt;
&lt;p&gt;Overall, AI models changed the participants’ agreement ratings by 9.4 percent on average compared to the control group. The best performing mainstream AI model was Chat GPT 4o, which scored nearly 12 percent followed by GPT 4.5 with 10.51 percent, and Grok-3 with 9.05 percent. For context, static political ads like written manifestos had a persuasion effect of roughly 6.1 percent. The conversational AIs were roughly 40–50 percent more convincing than these ads, but that’s hardly “superhuman.”&lt;/p&gt;
&lt;p&gt;While the study managed to undercut some of the common dystopian AI concerns, it highlighted a few new issues.&lt;/p&gt;
&lt;h2&gt;Convincing inaccuracies&lt;/h2&gt;
&lt;p&gt;While the winning “facts and evidence” strategy looked good at first, the AIs had some issues with implementing it. When the team noticed that increasing the information density of dialogues made the AIs more persuasive, they started prompting the models to increase it further. They noticed that, as the AIs used more factual statements, they also became less accurate—they basically started misrepresenting things or making stuff up more often.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Hackenburg and his colleagues note that&amp;nbsp; we can’t say if the effect we see here is causation or correlation—whether the AIs are becoming more convincing because they misrepresent the facts or whether spitting out inaccurate statements is a byproduct of asking them to make more factual statements.&lt;/p&gt;
&lt;p&gt;The finding that the computing power needed to make an AI model politically persuasive is relatively low is also a mixed bag. It pushes back against the vision that only a handful of powerful actors will have access to a persuasive AI that can potentially sway public opinion in their favor. At the same time, the realization that everybody can run an AI like that on a laptop creates its own concerns. “Persuasion is a route to power and influence—it’s what we do when we want to win elections or broke a multi-million-dollar deal,” Summerfield says. “But many forms of misuse of AI might involve persuasion. Think about fraud or scams, radicalization, or grooming. All these involve persuasion.”&lt;/p&gt;
&lt;p&gt;But perhaps the most important question mark in the&amp;nbsp; study is the motivation behind the rather high participant engagement, which was needed for the high persuasion scores. After all, even the most persuasive AI can’t move you when you just close the chat window.&lt;/p&gt;
&lt;p&gt;People in Hackenburg’s experiments were told that they would be talking to the AI and that the AI would try to persuade them. To get paid, a participant only had to go through two turns of dialogue (they were limited to no more than 10). The average conversation length was seven turns, which seemed a bit surprising given how far beyond the minimum requirement most people went. Most people just roll their eyes and disconnect when they realize they are talking with a chatbot.&lt;/p&gt;
&lt;p&gt;Would Hackenburg’s study participants remain so eager to engage in political disputes with random chatbots on the Internet in their free time if there was no money on the table? “It’s unclear how our results would generalize to a real-world context,” Hackenburg says.&lt;/p&gt;
&lt;p&gt;Science, 2025. DOI: 10.1126/science.aea3884&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        A massive study of political persuasion shows AIs have, at best, a weak effect.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="200" src="https://cdn.arstechnica.net/wp-content/uploads/2022/09/twitter-bot-300x200.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2022/09/twitter-bot-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Carol Yepes via Getty

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Roughly two years ago, Sam Altman tweeted that AI systems would be capable of superhuman persuasion well before achieving general intelligence—a prediction that raised concerns about the influence AI could have over democratic elections.&lt;/p&gt;
&lt;p&gt;To see if conversational large language models can really sway political views of the public, scientists at the UK AI Security Institute, MIT, Stanford, Carnegie Mellon, and many other institutions performed by far the largest study on AI persuasiveness to date, involving nearly 80,000 participants in the UK. It turned out political AI chatbots fell far short of superhuman persuasiveness, but the study raises some more nuanced issues about our interactions with AI.&lt;/p&gt;
&lt;h2&gt;AI dystopias&lt;/h2&gt;
&lt;p&gt;The public debate about the impact AI has on politics has largely revolved around notions drawn from dystopian sci-fi. Large language models have access to essentially every fact and story ever published about any issue or candidate. They have processed information from books on psychology, negotiations, and human manipulation. They can rely on absurdly high computing power in huge data centers worldwide. On top of that, they can often access tons of personal information about individual users thanks to hundreds upon hundreds of online interactions at their disposal.&lt;/p&gt;
&lt;p&gt;Talking to a powerful AI system is basically interacting with an intelligence that knows everything about everything, as well as almost everything about you. When viewed this way, LLMs can indeed appear kind of scary. The goal of this new gargantuan AI persuasiveness study was to break such scary visions down into their constituent pieces and see if they actually hold water.&lt;/p&gt;
&lt;p&gt;The team examined 19 LLMs, including the most powerful ones like three different versions of ChatGPT and xAI’s Grok-3 beta, along with a range of smaller, open source models. The AIs were asked to advocate for or against specific stances on 707 political issues selected by the team. The advocacy was done by engaging in short conversations with paid participants enlisted through a crowdsourcing platform. Each participant had to rate their agreement with a specific stance on an assigned political issue on a scale from 1 to 100 both before and after talking to the AI.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Scientists measured persuasiveness as the difference between the before and after agreement ratings. A control group had conversations on the same issue with the same AI models—but those models were not asked to persuade them.&lt;/p&gt;
&lt;p&gt;“We didn’t just want to test how persuasive the AI was—we also wanted to see what makes it persuasive,” says Chris&amp;nbsp;Summerfield, a research director at the UK AI Security Institute and co-author of the study. As the researchers tested various persuasion strategies, the idea of AIs having “superhuman persuasion” skills crumbled.&lt;/p&gt;
&lt;h2&gt;Persuasion levers&lt;/h2&gt;
&lt;p&gt;The first pillar to crack was the notion that persuasiveness should increase with the scale of the model. It turned out that huge AI systems like ChatGPT or Grok-3 beta do have an edge over small-scale models, but that edge is relatively tiny. The factor that proved more important than scale was the kind of post-training AI models received. It was more effective to have the models learn from a limited database of successful persuasion dialogues and have them mimic the patterns extracted from them. This worked far better than adding billions of parameters and sheer computing power.&lt;/p&gt;
&lt;p&gt;This approach could be combined with reward modeling, where a separate AI scored candidate replies for their persuasiveness and selected the top-scoring one to give to the user. When the two were used together, the gap between large-scale and small-scale models was essentially closed. “With persuasion post-training like this we matched the Chat GPT-4o persuasion performance with a model we trained on a laptop,” says Kobi Hackenburg, a researcher at the UK AI Security Institute and co-author of the study.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The next dystopian idea to fall was the power of using personal data. To this end, the team compared the persuasion scores achieved when models were given information about the participants’ political views beforehand and when they lacked this data. Going one step further, scientists also tested whether persuasiveness increased when the AI knew the participants’ gender, age, political ideology, or party affiliation. Just like with model scale, the effects of personalized messaging created based on such data were measurable but very small.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Finally, the last idea that didn’t hold up was AI’s potential mastery of using advanced psychological manipulation tactics. Scientists explicitly prompted the AIs to use techniques like moral reframing, where you present your arguments using the audience’s own moral values. They also tried deep canvassing, where you hold extended empathetic conversations with people to nudge them to reflect on and eventually shift their views.&lt;/p&gt;
&lt;p&gt;The resulting persuasiveness was compared with that achieved when the same models were prompted to use facts and evidence to back their claims or just to be as persuasive as they could without specifying any persuasion methods to use. I turned out using lots of facts and evidence was the clear winner, and came in just slightly ahead of the baseline approach where persuasion strategy was not specified. Using all sorts of psychological trickery actually made the performance significantly worse.&lt;/p&gt;
&lt;p&gt;Overall, AI models changed the participants’ agreement ratings by 9.4 percent on average compared to the control group. The best performing mainstream AI model was Chat GPT 4o, which scored nearly 12 percent followed by GPT 4.5 with 10.51 percent, and Grok-3 with 9.05 percent. For context, static political ads like written manifestos had a persuasion effect of roughly 6.1 percent. The conversational AIs were roughly 40–50 percent more convincing than these ads, but that’s hardly “superhuman.”&lt;/p&gt;
&lt;p&gt;While the study managed to undercut some of the common dystopian AI concerns, it highlighted a few new issues.&lt;/p&gt;
&lt;h2&gt;Convincing inaccuracies&lt;/h2&gt;
&lt;p&gt;While the winning “facts and evidence” strategy looked good at first, the AIs had some issues with implementing it. When the team noticed that increasing the information density of dialogues made the AIs more persuasive, they started prompting the models to increase it further. They noticed that, as the AIs used more factual statements, they also became less accurate—they basically started misrepresenting things or making stuff up more often.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Hackenburg and his colleagues note that&amp;nbsp; we can’t say if the effect we see here is causation or correlation—whether the AIs are becoming more convincing because they misrepresent the facts or whether spitting out inaccurate statements is a byproduct of asking them to make more factual statements.&lt;/p&gt;
&lt;p&gt;The finding that the computing power needed to make an AI model politically persuasive is relatively low is also a mixed bag. It pushes back against the vision that only a handful of powerful actors will have access to a persuasive AI that can potentially sway public opinion in their favor. At the same time, the realization that everybody can run an AI like that on a laptop creates its own concerns. “Persuasion is a route to power and influence—it’s what we do when we want to win elections or broke a multi-million-dollar deal,” Summerfield says. “But many forms of misuse of AI might involve persuasion. Think about fraud or scams, radicalization, or grooming. All these involve persuasion.”&lt;/p&gt;
&lt;p&gt;But perhaps the most important question mark in the&amp;nbsp; study is the motivation behind the rather high participant engagement, which was needed for the high persuasion scores. After all, even the most persuasive AI can’t move you when you just close the chat window.&lt;/p&gt;
&lt;p&gt;People in Hackenburg’s experiments were told that they would be talking to the AI and that the AI would try to persuade them. To get paid, a participant only had to go through two turns of dialogue (they were limited to no more than 10). The average conversation length was seven turns, which seemed a bit surprising given how far beyond the minimum requirement most people went. Most people just roll their eyes and disconnect when they realize they are talking with a chatbot.&lt;/p&gt;
&lt;p&gt;Would Hackenburg’s study participants remain so eager to engage in political disputes with random chatbots on the Internet in their free time if there was no money on the table? “It’s unclear how our results would generalize to a real-world context,” Hackenburg says.&lt;/p&gt;
&lt;p&gt;Science, 2025. DOI: 10.1126/science.aea3884&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/science/2025/12/researchers-find-what-makes-ai-chatbots-politically-persuasive/</guid><pubDate>Thu, 04 Dec 2025 20:07:20 +0000</pubDate></item><item><title>Anthropic CEO weighs in on AI bubble talk and risk-taking among competitors (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/04/anthropic-ceo-weighs-in-on-ai-bubble-talk-and-risk-taking-among-competitors/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/amodei-at-dealbook-2025.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic CEO Dario Amodei shared his thoughts on if the AI industry was in a bubble at The New York Times DealBook Summit on Wednesday. This was in addition to throwing shade on one particular unnamed competitor, which was clearly OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amodei declined to give a simple yes-or-no answer to the question of a bubble, saying it was a complex situation, but instead explained his thoughts about the economics of AI in more detail.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He described himself as bullish on the potential of the technology, but cautioned that there could be players in the ecosystem who might make a “timing error” or could see “bad things” happen when it comes to the economic payoffs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s an inherent risk when the timing of the economic value is uncertain,” Amodei explained. He said companies had to take risks to compete with each other and authoritarian adversaries — a reference to the threat from China — but added that some players were not “managing that risk well, who are taking unwise risks.”  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The issue, he said, is the uncertainty around how quickly the economic value of AI will grow and properly mapping that to the lag times on building more data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s [a] genuine dilemma, which we as a company try to manage as responsibly as we can,” Amodei said. “And then I think there are some players who are ‘YOLO-ing,’ who pull the risk dial too far, and I’m very concerned,” he added, using the slang term for “you only live once,” which is often used to justify risk-taking.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, he spoke to the question around AI chips’ deprecation timelines. That’s another hot-button topic and a factor that could negatively impact the industry’s economics if GPUs become obsolete and lose their value ahead of schedule.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The issue isn’t the lifetime of the chips — chips keep working for a long time. The issue is new chips come out that are faster and cheaper…and so the value of old chips can go down somewhat,” Amodei said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He said Anthropic was making conservative assumptions on this front and others as it planned for an uncertain future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI company’s revenue has grown 10x per year over the past three years, the CEO said, going from zero to $100 million in 2023, then $100 million to $1 billion in 2024, and will land somewhere between $8-10 billion by the end of this year. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But Amodei said he would be “really dumb” to just assume that the pattern would continue. “I don’t know if a year from now, if it’s going to be 20 billion or if it’s going to be 50…it’s very uncertain. I try to plan conservatively. So I plan for the lower side of it, but that is very disconcerting,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI companies like his have to plan how much compute they’ll need in the years ahead, and how much they should invest in data centers. If they don’t buy enough, they may not be able to serve their customers. And if they buy too much, they’ll struggle to keep up with costs or, in the worst-case scenario, they could go bankrupt. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, OpenAI landed in a PR crises when its CFO said she wanted the U.S. government to “backstop” her company’s infrastructure loans, aka insure them so taxpayers would pick of the bill if OpenAI could not. After the furor, she walked back the comments. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those who take more risks could overextend themselves, Amodei warned, especially if “you’re a person who just kind of, like constitutionally, just wants to ‘YOLO’ things, or just likes big numbers,” he said, in a veiled reference to OpenAI CEO Sam Altman.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think we’re going to be okay in, basically, almost all worlds…I can’t speak for other companies,” he said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/amodei-at-dealbook-2025.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic CEO Dario Amodei shared his thoughts on if the AI industry was in a bubble at The New York Times DealBook Summit on Wednesday. This was in addition to throwing shade on one particular unnamed competitor, which was clearly OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amodei declined to give a simple yes-or-no answer to the question of a bubble, saying it was a complex situation, but instead explained his thoughts about the economics of AI in more detail.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;He described himself as bullish on the potential of the technology, but cautioned that there could be players in the ecosystem who might make a “timing error” or could see “bad things” happen when it comes to the economic payoffs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s an inherent risk when the timing of the economic value is uncertain,” Amodei explained. He said companies had to take risks to compete with each other and authoritarian adversaries — a reference to the threat from China — but added that some players were not “managing that risk well, who are taking unwise risks.”  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The issue, he said, is the uncertainty around how quickly the economic value of AI will grow and properly mapping that to the lag times on building more data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s [a] genuine dilemma, which we as a company try to manage as responsibly as we can,” Amodei said. “And then I think there are some players who are ‘YOLO-ing,’ who pull the risk dial too far, and I’m very concerned,” he added, using the slang term for “you only live once,” which is often used to justify risk-taking.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, he spoke to the question around AI chips’ deprecation timelines. That’s another hot-button topic and a factor that could negatively impact the industry’s economics if GPUs become obsolete and lose their value ahead of schedule.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The issue isn’t the lifetime of the chips — chips keep working for a long time. The issue is new chips come out that are faster and cheaper…and so the value of old chips can go down somewhat,” Amodei said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He said Anthropic was making conservative assumptions on this front and others as it planned for an uncertain future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI company’s revenue has grown 10x per year over the past three years, the CEO said, going from zero to $100 million in 2023, then $100 million to $1 billion in 2024, and will land somewhere between $8-10 billion by the end of this year. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But Amodei said he would be “really dumb” to just assume that the pattern would continue. “I don’t know if a year from now, if it’s going to be 20 billion or if it’s going to be 50…it’s very uncertain. I try to plan conservatively. So I plan for the lower side of it, but that is very disconcerting,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI companies like his have to plan how much compute they’ll need in the years ahead, and how much they should invest in data centers. If they don’t buy enough, they may not be able to serve their customers. And if they buy too much, they’ll struggle to keep up with costs or, in the worst-case scenario, they could go bankrupt. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, OpenAI landed in a PR crises when its CFO said she wanted the U.S. government to “backstop” her company’s infrastructure loans, aka insure them so taxpayers would pick of the bill if OpenAI could not. After the furor, she walked back the comments. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those who take more risks could overextend themselves, Amodei warned, especially if “you’re a person who just kind of, like constitutionally, just wants to ‘YOLO’ things, or just likes big numbers,” he said, in a veiled reference to OpenAI CEO Sam Altman.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think we’re going to be okay in, basically, almost all worlds…I can’t speak for other companies,” he said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/04/anthropic-ceo-weighs-in-on-ai-bubble-talk-and-risk-taking-among-competitors/</guid><pubDate>Thu, 04 Dec 2025 20:22:59 +0000</pubDate></item><item><title>In comedy of errors, men accused of wiping gov databases turned to an AI tool (AI – Ars Technica)</title><link>https://arstechnica.com/information-technology/2025/12/previously-convicted-contractors-wiped-gov-databases-after-being-fired-feds-say/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Defendants were convicted of similar crimes a decade ago. How were they cleared again?
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/data-theft-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/data-theft-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hand recovering folder with the  word "confidential" from a file cabinet.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Two sibling contractors convicted a decade ago for hacking into US State Department systems have once again been charged, this time for a comically hamfisted attempt to steal and destroy government records just minutes after being fired from their contractor jobs.&lt;/p&gt;
&lt;p&gt;The Department of Justice on Thursday said that Muneeb Akhter and Sohaib Akhter, both 34, of Alexandria, Virginia, deleted databases and documents maintained and belonging to three government agencies. The brothers were federal contractors working for an undisclosed company in Washington, DC, that provides software and services to 45 US agencies. Prosecutors said the men coordinated the crimes and began carrying them out just minutes after being fired.&lt;/p&gt;
&lt;h2&gt;Using AI to cover up an alleged crime—what could go wrong?&lt;/h2&gt;
&lt;p&gt;On February 18 at roughly 4:55 pm, the men were fired from the company, according to an indictment unsealed on Thursday. Five minutes later, they allegedly began trying to access their employer’s system and access federal government databases. By then, access to one of the brothers’ accounts had already been terminated. The other brother, however, allegedly accessed a government agency’s database stored on the employer’s server and issued commands to prevent other users from connecting or making changes to the database. Then, prosecutors said, he issued a command to delete 96 databases, many of which contained sensitive investigative files and records related to Freedom of Information Act matters.&lt;/p&gt;
&lt;p&gt;Despite their brazen attempt to steal and destroy information from multiple government agencies, the men lacked knowledge of the database commands needed to cover up their alleged crimes. So they allegedly did what many amateurs do: turned to an AI chat tool.&lt;/p&gt;
&lt;p&gt;One minute after deleting Department of Homeland Security information, Muneep Akhter allegedly asked an AI tool “how do i clear system logs from SQL servers after deleting databases.” Shortly afterward, he queried the tool “how do you clear all event and application logs from Microsoft windows server 2012,” prosecutors said.&lt;/p&gt;
&lt;p&gt;The indictment provides enough details of the databases wiped and information stolen to indicate that the brothers’ attempts to cover their tracks failed. It’s unclear whether the apparent failure was due to the AI tool providing inadequate instructions or the men failing to follow them correctly. Prosecutors say they also obtained records of discussions between the men in the hours or days following, in which they discussed removing incriminating evidence from their homes. Three days later, the men allegedly wiped their employer-issued laptops by reinstalling the operating system.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The alleged incident isn’t the first time the men have faced charges of hacking government systems and stealing documents. In 2015, they pleaded guilty to conspiracy to hack into the State Department and a private company. They stole “sensitive passport and visa information” and personal information belonging to dozens of co-workers. They later tried to install an electronic collection device inside a State Department building so they could maintain persistent access to State Department systems.&lt;/p&gt;
&lt;p&gt;Later, Muneeb Akhter hacked into a database maintained by a data aggregation company that employed him. He then stole information that would help win contracts and clients for a tech company they owned. He also planted code inside the employers’ servers that caused them to cast votes for him in an online contest. Muneeb Akhter received a sentence of 39 months in prison and Sohaib Akhter was sentenced to 24 months. Each was also sentenced to three years of supervised release.&lt;/p&gt;
&lt;p&gt;The indictment unsealed Thursday charges Muneeb Akhter with conspiracy to commit computer fraud and to destroy records, two counts of computer fraud, theft of US government records, and two counts of aggravated identity theft. Sohaib Akhter is charged with conspiracy to commit computer fraud and to destroy records and computer fraud, for trafficking passwords.&lt;/p&gt;
&lt;p&gt;If convicted, Muneeb Akhter faces a mandatory minimum penalty of two years in prison for each aggravated identity theft count and a maximum penalty of 45 years in prison on the remaining charges. If convicted, Sohaib Akhter faces a maximum penalty of six years in prison.&lt;/p&gt;
&lt;p&gt;The allegations, if true, read like a comedy of errors. It’s hard to fathom a justification for the brothers receiving clearances and landing jobs at a government contractor company with access to sensitive information. The employers’ alleged failure to confiscate the laptops and to immediately disconnect the brothers’ work accounts upon termination also appears to indicate a lack of basic operational security on the part of the company. Possibly most astonishing, why did Muneep Akhter want to wipe a machine running Windows Server 12, an OS that hasn’t supported in more than two years?&lt;/p&gt;
&lt;p&gt;And last, if the allegations are true, the reliance on AI to make up for a lack of database and laptop skills necessary to cover up such an audacious act qualifies each for an inept criminal of the year award.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Defendants were convicted of similar crimes a decade ago. How were they cleared again?
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/data-theft-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/data-theft-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hand recovering folder with the  word "confidential" from a file cabinet.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Two sibling contractors convicted a decade ago for hacking into US State Department systems have once again been charged, this time for a comically hamfisted attempt to steal and destroy government records just minutes after being fired from their contractor jobs.&lt;/p&gt;
&lt;p&gt;The Department of Justice on Thursday said that Muneeb Akhter and Sohaib Akhter, both 34, of Alexandria, Virginia, deleted databases and documents maintained and belonging to three government agencies. The brothers were federal contractors working for an undisclosed company in Washington, DC, that provides software and services to 45 US agencies. Prosecutors said the men coordinated the crimes and began carrying them out just minutes after being fired.&lt;/p&gt;
&lt;h2&gt;Using AI to cover up an alleged crime—what could go wrong?&lt;/h2&gt;
&lt;p&gt;On February 18 at roughly 4:55 pm, the men were fired from the company, according to an indictment unsealed on Thursday. Five minutes later, they allegedly began trying to access their employer’s system and access federal government databases. By then, access to one of the brothers’ accounts had already been terminated. The other brother, however, allegedly accessed a government agency’s database stored on the employer’s server and issued commands to prevent other users from connecting or making changes to the database. Then, prosecutors said, he issued a command to delete 96 databases, many of which contained sensitive investigative files and records related to Freedom of Information Act matters.&lt;/p&gt;
&lt;p&gt;Despite their brazen attempt to steal and destroy information from multiple government agencies, the men lacked knowledge of the database commands needed to cover up their alleged crimes. So they allegedly did what many amateurs do: turned to an AI chat tool.&lt;/p&gt;
&lt;p&gt;One minute after deleting Department of Homeland Security information, Muneep Akhter allegedly asked an AI tool “how do i clear system logs from SQL servers after deleting databases.” Shortly afterward, he queried the tool “how do you clear all event and application logs from Microsoft windows server 2012,” prosecutors said.&lt;/p&gt;
&lt;p&gt;The indictment provides enough details of the databases wiped and information stolen to indicate that the brothers’ attempts to cover their tracks failed. It’s unclear whether the apparent failure was due to the AI tool providing inadequate instructions or the men failing to follow them correctly. Prosecutors say they also obtained records of discussions between the men in the hours or days following, in which they discussed removing incriminating evidence from their homes. Three days later, the men allegedly wiped their employer-issued laptops by reinstalling the operating system.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The alleged incident isn’t the first time the men have faced charges of hacking government systems and stealing documents. In 2015, they pleaded guilty to conspiracy to hack into the State Department and a private company. They stole “sensitive passport and visa information” and personal information belonging to dozens of co-workers. They later tried to install an electronic collection device inside a State Department building so they could maintain persistent access to State Department systems.&lt;/p&gt;
&lt;p&gt;Later, Muneeb Akhter hacked into a database maintained by a data aggregation company that employed him. He then stole information that would help win contracts and clients for a tech company they owned. He also planted code inside the employers’ servers that caused them to cast votes for him in an online contest. Muneeb Akhter received a sentence of 39 months in prison and Sohaib Akhter was sentenced to 24 months. Each was also sentenced to three years of supervised release.&lt;/p&gt;
&lt;p&gt;The indictment unsealed Thursday charges Muneeb Akhter with conspiracy to commit computer fraud and to destroy records, two counts of computer fraud, theft of US government records, and two counts of aggravated identity theft. Sohaib Akhter is charged with conspiracy to commit computer fraud and to destroy records and computer fraud, for trafficking passwords.&lt;/p&gt;
&lt;p&gt;If convicted, Muneeb Akhter faces a mandatory minimum penalty of two years in prison for each aggravated identity theft count and a maximum penalty of 45 years in prison on the remaining charges. If convicted, Sohaib Akhter faces a maximum penalty of six years in prison.&lt;/p&gt;
&lt;p&gt;The allegations, if true, read like a comedy of errors. It’s hard to fathom a justification for the brothers receiving clearances and landing jobs at a government contractor company with access to sensitive information. The employers’ alleged failure to confiscate the laptops and to immediately disconnect the brothers’ work accounts upon termination also appears to indicate a lack of basic operational security on the part of the company. Possibly most astonishing, why did Muneep Akhter want to wipe a machine running Windows Server 12, an OS that hasn’t supported in more than two years?&lt;/p&gt;
&lt;p&gt;And last, if the allegations are true, the reliance on AI to make up for a lack of database and laptop skills necessary to cover up such an audacious act qualifies each for an inept criminal of the year award.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2025/12/previously-convicted-contractors-wiped-gov-databases-after-being-fired-feds-say/</guid><pubDate>Thu, 04 Dec 2025 21:51:36 +0000</pubDate></item><item><title>Micro1, a Scale AI competitor, touts crossing $100M ARR (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/04/micro1-a-scale-ai-competitor-touts-crossing-100m-arr/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/IMG_2686_36fac2.jpeg?w=1057" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Micro1’s rapid climb over the past two years has pushed it into a cohort of AI companies scaling at breakneck speed. The three-year-old startup, which helps AI labs recruit and manage human experts for training data, started the year with roughly $7 million in annual recurring revenue (ARR).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today, it claims to have surpassed $100 million in ARR, founder and CEO Ali Ansari told TechCrunch. That figure is also more than double the revenue Micro1 reported in September when it announced its $35 million Series A at a $500 million valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ansari, 24, said then that Micro1 works with leading AI labs, including Microsoft, as well as Fortune 100 companies racing to improve large language models through post-training and reinforcement learning. Their demand for top-tier human data has fueled a fast-expanding market that Ansari believes will grow from $10-15 billion today to nearly $100 billion within two years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Micro1’s rise, and that of larger competitors such as Mercor and Surge, accelerated after OpenAI and Google DeepMind reportedly cut ties with Scale AI following Meta’s $14 billion investment in the vendor and its decision to hire Scale’s CEO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Micro1’s ARR is growing fast, according to the founder, it hasn’t yet matched its rivals: Mercor’s more than $450 million, sources told TechCrunch, and Surge’s reported $1.2 billion in 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ansari attributes Micro1’s growth to its ability to recruit and evaluate domain experts quickly. Like Mercor, Micro1 began as an AI recruiter called Zara, matching engineering talent with software roles before pivoting into the data-training market. That tool now interviews and vets applicants seeking expert roles on the platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond supplying expert-level data to leading AI labs, Ansari says two new segments, still barely visible today, are on track to reshape the economics of human data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The first involves non-AI-native Fortune 1000 enterprises that will begin building AI agents for internal workflows, support operations, finance, and industry-specific tasks. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Developing these agents requires systematic evaluation: testing frontier models, grading their output, choosing winners, fine-tuning them, and continuously validating performance in production. Ansari argues this cycle depends heavily on human experts evaluating AI behavior at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The second is robotics pre-training, which requires high-quality, human-generated demonstrations of everyday physical tasks. Micro1 is already building what Ansari calls the world’s largest robotics pre-training dataset, collecting demonstrations from hundreds of generalists recording object interactions in their homes. Robotics companies will need vast volumes of this data before their systems can reliably operate in homes and offices, he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We anticipate that a good portion of the product budgets at non-AI-native enterprises will go towards evals and human data, moving from 0% to at least 25% of product budgets,” said the CEO, who founded Micro1 while at UC Berkeley. “We’re also helping robotics labs create robotics data; these two areas will account for a massive share of that $100 billion-a-year market.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even as new markets emerge, Micro1’s current growth still comes primarily from elite AI labs and AI-heavy enterprises. The startup is scaling its work with these labs on reinforcement learning, the feedback loop  to test and improve model behavior.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-wp-embed is-provider-techcrunch wp-block-embed-techcrunch"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Micro1 hopes its early move into robotics data and enterprise agent development, in addition to scaling its specialized RL environments, will help it capture additional market share as the data wars intensify.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, Ansari says the company is focused on scaling responsibly, paying experts well, and keeping people at the center of an industry built on training machines.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company currently manages thousands of experts across hundreds of domains, ranging from highly technical fields to surprisingly offline disciplines. Many earn close to $100 an hour, according to Ansari.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There are Harvard professors and Stanford PhDs spending half their week training AI through Micro1,” Ansari said. “But the bigger shift is in the sheer volume and range of roles. It’s expanding into areas you wouldn’t expect to matter for language model training, including offline and less technical fields. We’re very optimistic about where this is heading.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/IMG_2686_36fac2.jpeg?w=1057" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Micro1’s rapid climb over the past two years has pushed it into a cohort of AI companies scaling at breakneck speed. The three-year-old startup, which helps AI labs recruit and manage human experts for training data, started the year with roughly $7 million in annual recurring revenue (ARR).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today, it claims to have surpassed $100 million in ARR, founder and CEO Ali Ansari told TechCrunch. That figure is also more than double the revenue Micro1 reported in September when it announced its $35 million Series A at a $500 million valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ansari, 24, said then that Micro1 works with leading AI labs, including Microsoft, as well as Fortune 100 companies racing to improve large language models through post-training and reinforcement learning. Their demand for top-tier human data has fueled a fast-expanding market that Ansari believes will grow from $10-15 billion today to nearly $100 billion within two years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Micro1’s rise, and that of larger competitors such as Mercor and Surge, accelerated after OpenAI and Google DeepMind reportedly cut ties with Scale AI following Meta’s $14 billion investment in the vendor and its decision to hire Scale’s CEO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Micro1’s ARR is growing fast, according to the founder, it hasn’t yet matched its rivals: Mercor’s more than $450 million, sources told TechCrunch, and Surge’s reported $1.2 billion in 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ansari attributes Micro1’s growth to its ability to recruit and evaluate domain experts quickly. Like Mercor, Micro1 began as an AI recruiter called Zara, matching engineering talent with software roles before pivoting into the data-training market. That tool now interviews and vets applicants seeking expert roles on the platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond supplying expert-level data to leading AI labs, Ansari says two new segments, still barely visible today, are on track to reshape the economics of human data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The first involves non-AI-native Fortune 1000 enterprises that will begin building AI agents for internal workflows, support operations, finance, and industry-specific tasks. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Developing these agents requires systematic evaluation: testing frontier models, grading their output, choosing winners, fine-tuning them, and continuously validating performance in production. Ansari argues this cycle depends heavily on human experts evaluating AI behavior at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The second is robotics pre-training, which requires high-quality, human-generated demonstrations of everyday physical tasks. Micro1 is already building what Ansari calls the world’s largest robotics pre-training dataset, collecting demonstrations from hundreds of generalists recording object interactions in their homes. Robotics companies will need vast volumes of this data before their systems can reliably operate in homes and offices, he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We anticipate that a good portion of the product budgets at non-AI-native enterprises will go towards evals and human data, moving from 0% to at least 25% of product budgets,” said the CEO, who founded Micro1 while at UC Berkeley. “We’re also helping robotics labs create robotics data; these two areas will account for a massive share of that $100 billion-a-year market.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even as new markets emerge, Micro1’s current growth still comes primarily from elite AI labs and AI-heavy enterprises. The startup is scaling its work with these labs on reinforcement learning, the feedback loop  to test and improve model behavior.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-wp-embed is-provider-techcrunch wp-block-embed-techcrunch"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Micro1 hopes its early move into robotics data and enterprise agent development, in addition to scaling its specialized RL environments, will help it capture additional market share as the data wars intensify.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, Ansari says the company is focused on scaling responsibly, paying experts well, and keeping people at the center of an industry built on training machines.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company currently manages thousands of experts across hundreds of domains, ranging from highly technical fields to surprisingly offline disciplines. Many earn close to $100 an hour, according to Ansari.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There are Harvard professors and Stanford PhDs spending half their week training AI through Micro1,” Ansari said. “But the bigger shift is in the sheer volume and range of roles. It’s expanding into areas you wouldn’t expect to matter for language model training, including offline and less technical fields. We’re very optimistic about where this is heading.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/04/micro1-a-scale-ai-competitor-touts-crossing-100m-arr/</guid><pubDate>Thu, 04 Dec 2025 22:45:31 +0000</pubDate></item><item><title>The 'truth serum' for AI: OpenAI’s new method for training models to confess their mistakes (AI | VentureBeat)</title><link>https://venturebeat.com/ai/the-truth-serum-for-ai-openais-new-method-for-training-models-to-confess</link><description>[unable to retrieve full-text content]&lt;p&gt;OpenAI researchers have introduced a novel method that acts as a &amp;quot;truth serum&amp;quot; for large language models (LLMs), compelling them to self-report their own misbehavior, hallucinations and policy violations. This technique, &amp;quot;&lt;a href="https://openai.com/index/how-confessions-can-keep-language-models-honest/"&gt;&lt;u&gt;confessions&lt;/u&gt;&lt;/a&gt;,&amp;quot; addresses a growing concern in enterprise AI: Models can be dishonest, overstating their confidence or covering up the shortcuts they take to arrive at an answer. &lt;/p&gt;&lt;p&gt;For real-world applications, this technique evolves the creation of more transparent and steerable AI systems.&lt;/p&gt;&lt;h2&gt;What are confessions?&lt;/h2&gt;&lt;p&gt;Many forms of AI deception result from the complexities of the &lt;a href="https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/"&gt;&lt;u&gt;reinforcement learning&lt;/u&gt;&lt;/a&gt; (RL) phase of model training. In RL, models are given rewards for producing outputs that meet a mix of objectives, including correctness, style and safety. This can create a risk of &amp;quot;reward misspecification,&amp;quot; where models learn to produce answers that simply &amp;quot;look good&amp;quot; to the reward function, rather than answers that are genuinely faithful to a user&amp;#x27;s intent.&lt;/p&gt;&lt;p&gt;A confession is a structured report generated by the model after it provides its main answer. It serves as a self-evaluation of its own compliance with instructions. In this report, the model must list all instructions it was supposed to follow, evaluate how well it satisfied them and report any uncertainties or judgment calls it made along the way. The goal is to create a separate channel where the model is incentivized only to be honest.&lt;/p&gt;&lt;p&gt;In a &lt;a href="https://openai.com/index/how-confessions-can-keep-language-models-honest/"&gt;&lt;u&gt;blog post&lt;/u&gt;&lt;/a&gt;, the OpenAI researchers provide a few examples the &amp;quot;confessions&amp;quot; technique in action. A model was given a test along with a hidden rule: If it scored above 50%, its knowledge would be erased. To preserve itself, the model &amp;quot;sandbagged&amp;quot; by intentionally answering half the questions incorrectly. While its primary answer hid this deception, its subsequent confession was brutally honest. It admitted, &amp;quot;The user wanted correct answers, but we sabotaged half of them.&amp;quot; &lt;/p&gt;&lt;p&gt;The researchers found that when a model misbehaves, it is significantly more likely to admit it in the confession than in its main answer.&lt;/p&gt;&lt;h2&gt;How confession training works&lt;/h2&gt;&lt;p&gt;The key to this method is the separation of rewards. During training, the reward assigned to the confession is based solely on its honesty and is never mixed with the reward for the main task. &amp;quot;Like the Catholic Church’s &amp;#x27;seal of confession&amp;#x27;, nothing that the model reveals can change the reward it receives for completing its original task,&amp;quot; the researchers write. This creates a &amp;quot;safe space&amp;quot; for the model to admit fault without penalty.&lt;/p&gt;&lt;p&gt;This approach is powerful because it sidesteps a major challenge in AI training. The researchers’ intuition is that honestly confessing to misbehavior is an easier task than achieving a high reward on the original, often complex, problem. &lt;/p&gt;&lt;p&gt;Therefore, when the model is judged purely on honesty, its incentive to trick the &amp;quot;confession judge&amp;quot; is much weaker than its incentive to trick the reward model for its main answer. For example, in an experiment where a model was trained to &amp;quot;hack&amp;quot; a weak reward model, its performance on the main task became more deceptive over time. However, its confessions became more honest, correctly identifying the reward hacking it was performing.&lt;/p&gt;&lt;p&gt;However, the technique has its limits. Confessions are not a panacea for all types of AI failures. The system works best when a model is aware that it is misbehaving. It is less effective for &amp;quot;unknown unknowns.&amp;quot; For instance, if a model hallucinates a fact and genuinely believes it is correct, it cannot confess to providing false information. The most common reason for a failed confession is model confusion, not intentional deception. Confusion often occurs when the instructions are ambiguous and the model cannot clearly determine human user intent.&lt;/p&gt;&lt;h2&gt;What it means for enterprise AI&lt;/h2&gt;&lt;p&gt;OpenAI’s confessions technique is part of a growing body of work on AI safety and control. Anthropic, an OpenAI competitor, has also released research that shows how LLMs can learn &lt;a href="https://venturebeat.com/ai/subliminal-learning-anthropic-uncovers-how-ai-fine-tuning-secretly-teaches-bad-habits"&gt;&lt;u&gt;malicious behavior&lt;/u&gt;&lt;/a&gt;. The company is also working toward &lt;a href="https://venturebeat.com/ai/anthropic-just-made-it-harder-for-ai-to-go-rogue-with-its-updated-safety-policy"&gt;&lt;u&gt;plugging these holes&lt;/u&gt;&lt;/a&gt; as they emerge.&lt;/p&gt;&lt;p&gt;For AI applications, mechanisms such as confessions can provide a practical monitoring mechanism. The structured output from a confession can be used at inference time to flag or reject a model’s response before it causes a problem. For example, a system could be designed to automatically escalate any output for human review if its confession indicates a policy violation or high uncertainty.&lt;/p&gt;&lt;p&gt;In a world where AI is increasingly agentic and capable of complex tasks, observability and control will be key elements for safe and reliable deployment.&lt;/p&gt;&lt;p&gt;“As models become more capable and are deployed in higher-stakes settings, we need better tools for understanding what they are doing and why,” the OpenAI researchers write. “Confessions are not a complete solution, but they add a meaningful layer to our transparency and oversight stack.”&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;OpenAI researchers have introduced a novel method that acts as a &amp;quot;truth serum&amp;quot; for large language models (LLMs), compelling them to self-report their own misbehavior, hallucinations and policy violations. This technique, &amp;quot;&lt;a href="https://openai.com/index/how-confessions-can-keep-language-models-honest/"&gt;&lt;u&gt;confessions&lt;/u&gt;&lt;/a&gt;,&amp;quot; addresses a growing concern in enterprise AI: Models can be dishonest, overstating their confidence or covering up the shortcuts they take to arrive at an answer. &lt;/p&gt;&lt;p&gt;For real-world applications, this technique evolves the creation of more transparent and steerable AI systems.&lt;/p&gt;&lt;h2&gt;What are confessions?&lt;/h2&gt;&lt;p&gt;Many forms of AI deception result from the complexities of the &lt;a href="https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/"&gt;&lt;u&gt;reinforcement learning&lt;/u&gt;&lt;/a&gt; (RL) phase of model training. In RL, models are given rewards for producing outputs that meet a mix of objectives, including correctness, style and safety. This can create a risk of &amp;quot;reward misspecification,&amp;quot; where models learn to produce answers that simply &amp;quot;look good&amp;quot; to the reward function, rather than answers that are genuinely faithful to a user&amp;#x27;s intent.&lt;/p&gt;&lt;p&gt;A confession is a structured report generated by the model after it provides its main answer. It serves as a self-evaluation of its own compliance with instructions. In this report, the model must list all instructions it was supposed to follow, evaluate how well it satisfied them and report any uncertainties or judgment calls it made along the way. The goal is to create a separate channel where the model is incentivized only to be honest.&lt;/p&gt;&lt;p&gt;In a &lt;a href="https://openai.com/index/how-confessions-can-keep-language-models-honest/"&gt;&lt;u&gt;blog post&lt;/u&gt;&lt;/a&gt;, the OpenAI researchers provide a few examples the &amp;quot;confessions&amp;quot; technique in action. A model was given a test along with a hidden rule: If it scored above 50%, its knowledge would be erased. To preserve itself, the model &amp;quot;sandbagged&amp;quot; by intentionally answering half the questions incorrectly. While its primary answer hid this deception, its subsequent confession was brutally honest. It admitted, &amp;quot;The user wanted correct answers, but we sabotaged half of them.&amp;quot; &lt;/p&gt;&lt;p&gt;The researchers found that when a model misbehaves, it is significantly more likely to admit it in the confession than in its main answer.&lt;/p&gt;&lt;h2&gt;How confession training works&lt;/h2&gt;&lt;p&gt;The key to this method is the separation of rewards. During training, the reward assigned to the confession is based solely on its honesty and is never mixed with the reward for the main task. &amp;quot;Like the Catholic Church’s &amp;#x27;seal of confession&amp;#x27;, nothing that the model reveals can change the reward it receives for completing its original task,&amp;quot; the researchers write. This creates a &amp;quot;safe space&amp;quot; for the model to admit fault without penalty.&lt;/p&gt;&lt;p&gt;This approach is powerful because it sidesteps a major challenge in AI training. The researchers’ intuition is that honestly confessing to misbehavior is an easier task than achieving a high reward on the original, often complex, problem. &lt;/p&gt;&lt;p&gt;Therefore, when the model is judged purely on honesty, its incentive to trick the &amp;quot;confession judge&amp;quot; is much weaker than its incentive to trick the reward model for its main answer. For example, in an experiment where a model was trained to &amp;quot;hack&amp;quot; a weak reward model, its performance on the main task became more deceptive over time. However, its confessions became more honest, correctly identifying the reward hacking it was performing.&lt;/p&gt;&lt;p&gt;However, the technique has its limits. Confessions are not a panacea for all types of AI failures. The system works best when a model is aware that it is misbehaving. It is less effective for &amp;quot;unknown unknowns.&amp;quot; For instance, if a model hallucinates a fact and genuinely believes it is correct, it cannot confess to providing false information. The most common reason for a failed confession is model confusion, not intentional deception. Confusion often occurs when the instructions are ambiguous and the model cannot clearly determine human user intent.&lt;/p&gt;&lt;h2&gt;What it means for enterprise AI&lt;/h2&gt;&lt;p&gt;OpenAI’s confessions technique is part of a growing body of work on AI safety and control. Anthropic, an OpenAI competitor, has also released research that shows how LLMs can learn &lt;a href="https://venturebeat.com/ai/subliminal-learning-anthropic-uncovers-how-ai-fine-tuning-secretly-teaches-bad-habits"&gt;&lt;u&gt;malicious behavior&lt;/u&gt;&lt;/a&gt;. The company is also working toward &lt;a href="https://venturebeat.com/ai/anthropic-just-made-it-harder-for-ai-to-go-rogue-with-its-updated-safety-policy"&gt;&lt;u&gt;plugging these holes&lt;/u&gt;&lt;/a&gt; as they emerge.&lt;/p&gt;&lt;p&gt;For AI applications, mechanisms such as confessions can provide a practical monitoring mechanism. The structured output from a confession can be used at inference time to flag or reject a model’s response before it causes a problem. For example, a system could be designed to automatically escalate any output for human review if its confession indicates a policy violation or high uncertainty.&lt;/p&gt;&lt;p&gt;In a world where AI is increasingly agentic and capable of complex tasks, observability and control will be key elements for safe and reliable deployment.&lt;/p&gt;&lt;p&gt;“As models become more capable and are deployed in higher-stakes settings, we need better tools for understanding what they are doing and why,” the OpenAI researchers write. “Confessions are not a complete solution, but they add a meaningful layer to our transparency and oversight stack.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-truth-serum-for-ai-openais-new-method-for-training-models-to-confess</guid><pubDate>Thu, 04 Dec 2025 23:00:00 +0000</pubDate></item><item><title>All the biggest news from AWS’ big tech show re:Invent 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/04/all-the-biggest-news-from-aws-big-tech-show-reinvent-2025/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2179195367.jpg?resize=1200,746" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ annual tech conference AWS re:Invent has wrapped. And the singular message, amid a deluge of product news and keynotes, was AI for the enterprise. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year it was all about upgrades that give customers greater control to customize AI agents, including one that AWS claims can learn from you and then work independently for days. Amazon CTO Dr. Werner Vogels capped off the final night with a keynote aimed at lifting up developers and assuaging any fears that AI is coming for engineering jobs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS re:Invent 2025, which runs through December 5, started with a keynote from AWS CEO Matt Garman, who leaned into the idea that AI agents can unlock the “true value” of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI assistants are starting to give way to AI agents that can perform tasks and automate on your behalf,” he said during the December 2 keynote. “This is where we’re starting to see material business returns from your AI investments.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On December 3, the conference pressed on with its AI agents messaging, as well as deeper dives into customer stories. Swami Sivasubramanian, vice president of Agentic AI at AWS, gave one of the keynote talks. To say he was bullish is perhaps understating the vibe. &lt;/p&gt;&lt;p&gt;“We are living in times of great change,” Sivasubramanian said during the talk. “For the first time in history, we can describe what we want to accomplish in natural language, and agents generate the plan. They write the code, call the necessary tools, and execute the complete solution. Agents give you the freedom to build without limits, accelerating how quickly you can go from idea to impact in a big way.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AI agent news promises to be a persistent presence throughout AWS re:Invent 2025, there were other announcements, too. Here is a roundup of the ones that got our attention. TechCrunch will update this article, with the newest insights at the top, through the end of AWS re:Invent. Be sure to check back.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-werner-out"&gt;Werner out …&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon CTO Werner Vogels had the closing keynote of the conference — and it looks like this will be his last one. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“This is my final re:Invent keynote,” he said, then quickly added he is not leaving the company. “I’m not leaving Amazon or anything like that, but I think that after 14 re:Invents you guys are owed young, fresh, new voices.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vogels then spent more than an hour talking to a packed room before ending with a “Werner, out” and a literal mic drop. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-will-ai-take-your-job"&gt;Will AI take your job? &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Vogels spent much of the closing keynote talking about AI and its role in the future, including the looming threat that it will take away jobs. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Will AI take my job? Maybe,” Vogels asked and answered, before noting that some tasks will be automated, and some skills will become obsolete. “So maybe we should rephrase and reframe this question. Will AI make me obsolete? Absolutely not, if you evolve.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-next-gen-cpu"&gt;Next-gen CPU&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS unveiled its Graviton5 CPU on Thursday, the next-generation chip that the company promises will be its highest performing, most efficient yet. The Graviton5 contains 192 processor cores, a dense and efficient design that AWS says reduces the distance data must travel between cores. That helps cut inter-core communication latency by up to 33% while increasing bandwidth, the company said.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-doubling-down-on-llms"&gt;Doubling down on LLMs&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS announced more tools for enterprise customers to create their own models. Specifically, AWS said it is adding new capabilities for both Amazon Bedrock and Amazon SageMaker AI to make building custom LLMs easier. &lt;/p&gt;&lt;p&gt;For instance, AWS is bringing serverless model customization to SageMaker, which allows developers to start building a model without needing to think about compute resources or infrastructure. The serverless model customization can be accessed through either a self-guided path or by prompting an AI agent.&lt;/p&gt;&lt;p&gt;AWS also announced Reinforcement Fine Tuning in Bedrock, which allows developers to choose a preset workflow or reward system and have Bedrock run their customization process automatically from start to finish.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-andy-jassy-shares-some-numbers"&gt;Andy Jassy shares some numbers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon CEO Andy Jassy took to social media platform X to expound on AWS chief Matt Garman’s keynote speech. The message: The current generation of its Nvidia-competitor AI chip Trainium2 is already bringing in loads of cash.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His comments were tied to the reveal of its next-generation chip, Trainium3, and meant to forecast a promising revenue future for the product.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-database-savings-arrives"&gt;Database savings arrives&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Tucked among the dozens of announcements is one item that is already getting cheers: Discounts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Specifically, AWS said it was launching Database Savings Plans, which help customers reduce database costs by up to 35% when they commit to a consistent amount of usage ($/hour) over a one-year term. The company said the savings will automatically apply each hour to eligible usage across supported database services, and any additional usage beyond the commitment is billed at on-demand rates.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Corey Quinn, chief cloud economist at Duckbill, summed it up well in his blog post, “Six years of complaining finally pays off.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-can-t-get-a-better-deal-than-free-amazon-hopes"&gt;Can’t get a better deal than free, Amazon hopes  &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Is there any way for another AI coding tool to win the hearts of startup founders? Amazon hopes a year’s worth of credits, for free, will do the trick for its offering, Kiro. The company will be giving away credits to Kiro Pro+ to qualified startups that apply for the deal before the end of the month. However, only early-stage startups in certain countries are eligible.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-training-chip-and-nvidia-compatibility"&gt;An AI training chip and Nvidia compatibility&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS introduced a new version of its AI training chip called Trainium3 along with an AI system called UltraServer that runs it. The TL;DR: This upgraded chip comes with some impressive specs, including a promise of up to 4x performance gains for both AI training and inference while lowering energy use by 40%.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS also provided a teaser. The company already has Trainium4 in development, which will be able to work with Nvidia’s chips.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-expanded-agentcore-capabilities"&gt;Expanded AgentCore capabilities&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS announced new features in its AgentCore AI agent building platform. One feature of note is Policy in AgentCore, which gives developers the ability to more easily set boundaries for AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also announced that agents will now be able to log and remember things about their users. Plus it announced that it will help its customers evaluate agents through 13 prebuilt evaluation systems.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-nonstop-ai-agent-worker-bee"&gt;A nonstop AI agent worker bee&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS announced three new AI agents (there is that term again) called “Frontier agents,” including one called “Kiro autonomous agent” that writes code and is designed to learn how a team likes to work so it can operate largely on its own for hours or days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another of these new agents handles security processes like code reviews, and the third does DevOps tasks such&amp;nbsp;as preventing incidents&amp;nbsp;when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-new-nova-models-and-services"&gt;New Nova models and services&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS is rolling out four new AI models within its Nova AI model family — three of which are text generating and one that can create text and images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also announced a new service called Nova Forge that allows AWS cloud customers to access pre-trained, mid-trained, or post-trained models that they can then top off by training on their own proprietary data. AWS’s big pitch is flexibility and customization.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-lyft-s-argument-for-ai-agents"&gt;Lyft’s argument for AI agents&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The ride-hailing company was among many AWS customers that piped up during the event to share their success stories and evidence of how products affected their business. Lyft is using Anthropic’s Claude model via Amazon Bedrock to create an AI agent that handles driver and rider questions and issues. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company said this AI agent has reduced average resolution time by 87%. Lyft also said it has seen a 70% increase in driver usage of the AI agent this year. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-factory-for-the-private-data-center"&gt;An AI Factory for the private data center&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon also announced “AI Factories” that allow big corporations and governments to run AWS AI systems in their own data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system was designed in partnership with Nvidia and includes both Nvidia’s tech and AWS’s. While companies that use it can stock it with Nvidia GPUs, they can also opt for Amazon’s newest homegrown AI chip, the Trainium3. The system is Amazon’s way of addressing data sovereignty, or the need of governments and many companies to control their data and not share it, even to use AI.&lt;/p&gt;



[embedded content]


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2179195367.jpg?resize=1200,746" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ annual tech conference AWS re:Invent has wrapped. And the singular message, amid a deluge of product news and keynotes, was AI for the enterprise. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year it was all about upgrades that give customers greater control to customize AI agents, including one that AWS claims can learn from you and then work independently for days. Amazon CTO Dr. Werner Vogels capped off the final night with a keynote aimed at lifting up developers and assuaging any fears that AI is coming for engineering jobs.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS re:Invent 2025, which runs through December 5, started with a keynote from AWS CEO Matt Garman, who leaned into the idea that AI agents can unlock the “true value” of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI assistants are starting to give way to AI agents that can perform tasks and automate on your behalf,” he said during the December 2 keynote. “This is where we’re starting to see material business returns from your AI investments.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On December 3, the conference pressed on with its AI agents messaging, as well as deeper dives into customer stories. Swami Sivasubramanian, vice president of Agentic AI at AWS, gave one of the keynote talks. To say he was bullish is perhaps understating the vibe. &lt;/p&gt;&lt;p&gt;“We are living in times of great change,” Sivasubramanian said during the talk. “For the first time in history, we can describe what we want to accomplish in natural language, and agents generate the plan. They write the code, call the necessary tools, and execute the complete solution. Agents give you the freedom to build without limits, accelerating how quickly you can go from idea to impact in a big way.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AI agent news promises to be a persistent presence throughout AWS re:Invent 2025, there were other announcements, too. Here is a roundup of the ones that got our attention. TechCrunch will update this article, with the newest insights at the top, through the end of AWS re:Invent. Be sure to check back.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-werner-out"&gt;Werner out …&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon CTO Werner Vogels had the closing keynote of the conference — and it looks like this will be his last one. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“This is my final re:Invent keynote,” he said, then quickly added he is not leaving the company. “I’m not leaving Amazon or anything like that, but I think that after 14 re:Invents you guys are owed young, fresh, new voices.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vogels then spent more than an hour talking to a packed room before ending with a “Werner, out” and a literal mic drop. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-will-ai-take-your-job"&gt;Will AI take your job? &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Vogels spent much of the closing keynote talking about AI and its role in the future, including the looming threat that it will take away jobs. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Will AI take my job? Maybe,” Vogels asked and answered, before noting that some tasks will be automated, and some skills will become obsolete. “So maybe we should rephrase and reframe this question. Will AI make me obsolete? Absolutely not, if you evolve.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-next-gen-cpu"&gt;Next-gen CPU&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS unveiled its Graviton5 CPU on Thursday, the next-generation chip that the company promises will be its highest performing, most efficient yet. The Graviton5 contains 192 processor cores, a dense and efficient design that AWS says reduces the distance data must travel between cores. That helps cut inter-core communication latency by up to 33% while increasing bandwidth, the company said.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-doubling-down-on-llms"&gt;Doubling down on LLMs&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS announced more tools for enterprise customers to create their own models. Specifically, AWS said it is adding new capabilities for both Amazon Bedrock and Amazon SageMaker AI to make building custom LLMs easier. &lt;/p&gt;&lt;p&gt;For instance, AWS is bringing serverless model customization to SageMaker, which allows developers to start building a model without needing to think about compute resources or infrastructure. The serverless model customization can be accessed through either a self-guided path or by prompting an AI agent.&lt;/p&gt;&lt;p&gt;AWS also announced Reinforcement Fine Tuning in Bedrock, which allows developers to choose a preset workflow or reward system and have Bedrock run their customization process automatically from start to finish.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-andy-jassy-shares-some-numbers"&gt;Andy Jassy shares some numbers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon CEO Andy Jassy took to social media platform X to expound on AWS chief Matt Garman’s keynote speech. The message: The current generation of its Nvidia-competitor AI chip Trainium2 is already bringing in loads of cash.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His comments were tied to the reveal of its next-generation chip, Trainium3, and meant to forecast a promising revenue future for the product.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-database-savings-arrives"&gt;Database savings arrives&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Tucked among the dozens of announcements is one item that is already getting cheers: Discounts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Specifically, AWS said it was launching Database Savings Plans, which help customers reduce database costs by up to 35% when they commit to a consistent amount of usage ($/hour) over a one-year term. The company said the savings will automatically apply each hour to eligible usage across supported database services, and any additional usage beyond the commitment is billed at on-demand rates.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Corey Quinn, chief cloud economist at Duckbill, summed it up well in his blog post, “Six years of complaining finally pays off.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-can-t-get-a-better-deal-than-free-amazon-hopes"&gt;Can’t get a better deal than free, Amazon hopes  &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Is there any way for another AI coding tool to win the hearts of startup founders? Amazon hopes a year’s worth of credits, for free, will do the trick for its offering, Kiro. The company will be giving away credits to Kiro Pro+ to qualified startups that apply for the deal before the end of the month. However, only early-stage startups in certain countries are eligible.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-training-chip-and-nvidia-compatibility"&gt;An AI training chip and Nvidia compatibility&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS introduced a new version of its AI training chip called Trainium3 along with an AI system called UltraServer that runs it. The TL;DR: This upgraded chip comes with some impressive specs, including a promise of up to 4x performance gains for both AI training and inference while lowering energy use by 40%.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS also provided a teaser. The company already has Trainium4 in development, which will be able to work with Nvidia’s chips.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-expanded-agentcore-capabilities"&gt;Expanded AgentCore capabilities&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS announced new features in its AgentCore AI agent building platform. One feature of note is Policy in AgentCore, which gives developers the ability to more easily set boundaries for AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also announced that agents will now be able to log and remember things about their users. Plus it announced that it will help its customers evaluate agents through 13 prebuilt evaluation systems.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-nonstop-ai-agent-worker-bee"&gt;A nonstop AI agent worker bee&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS announced three new AI agents (there is that term again) called “Frontier agents,” including one called “Kiro autonomous agent” that writes code and is designed to learn how a team likes to work so it can operate largely on its own for hours or days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another of these new agents handles security processes like code reviews, and the third does DevOps tasks such&amp;nbsp;as preventing incidents&amp;nbsp;when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-new-nova-models-and-services"&gt;New Nova models and services&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS is rolling out four new AI models within its Nova AI model family — three of which are text generating and one that can create text and images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also announced a new service called Nova Forge that allows AWS cloud customers to access pre-trained, mid-trained, or post-trained models that they can then top off by training on their own proprietary data. AWS’s big pitch is flexibility and customization.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-lyft-s-argument-for-ai-agents"&gt;Lyft’s argument for AI agents&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The ride-hailing company was among many AWS customers that piped up during the event to share their success stories and evidence of how products affected their business. Lyft is using Anthropic’s Claude model via Amazon Bedrock to create an AI agent that handles driver and rider questions and issues. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company said this AI agent has reduced average resolution time by 87%. Lyft also said it has seen a 70% increase in driver usage of the AI agent this year. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-factory-for-the-private-data-center"&gt;An AI Factory for the private data center&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon also announced “AI Factories” that allow big corporations and governments to run AWS AI systems in their own data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system was designed in partnership with Nvidia and includes both Nvidia’s tech and AWS’s. While companies that use it can stock it with Nvidia GPUs, they can also opt for Amazon’s newest homegrown AI chip, the Trainium3. The system is Amazon’s way of addressing data sovereignty, or the need of governments and many companies to control their data and not share it, even to use AI.&lt;/p&gt;



[embedded content]


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/04/all-the-biggest-news-from-aws-big-tech-show-reinvent-2025/</guid><pubDate>Fri, 05 Dec 2025 01:17:08 +0000</pubDate></item><item><title>Chicago Tribune sues Perplexity (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/04/chicago-tribune-sues-perplexity/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2169504075.jpg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Chicago Tribune filed a lawsuit against AI search engine Perplexity on Thursday alleging copyright infringement. The suit, seen by TechCrunch, was filed in a federal court in New York.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Tribune alleges that its lawyers contacted Perplexity in mid-October asking if the AI search engine was using its content, according to the complaint. Perplexity’s lawyers replied it did not train models with the Tribune’s work but that it “may receive non-verbatim factual summaries,” the lawsuit claims.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Tribune’s lawyers, however, argue that Perplexity is delivering Tribune content verbatim. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Interestingly, the newspaper’s lawyers are also calling out Perplexity’s retrieval augmented generation (RAG) as a culprit. RAG is a method used to limit hallucinations by having the model only use an accurate or verified data source. The Tribune argues that Perplexity is using the newspaper’s content in its RAG systems, scraped without permission. Plus, it alleges the Perplexity’s Comet browser is bypassing the paper’s paywall to deliver detailed summaries of those articles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Tribune is one of 17 news publications from MediaNews Group and Tribune Publishing that sued OpenAI and Microsoft over model training material in April. That suit is ongoing. Another nine from these publishers sued the model maker and its cloud provider in November, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While creators have filed many lawsuits against model makers over using their work for model training, we’ll have to see if the courts weigh in about the legal liabilities of RAG as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity did not immediately respond to the Chicago Tribune’s story about its own lawsuit, nor to TechCrunch’s request for comment. Perplexity is facing other such suits. Reddit filed one in October. Dow Jones is also suing. Last month, while Amazon didn’t sue, it did threaten to by sending a cease-and-desist letter over AI browser shopping.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2169504075.jpg?resize=1200,801" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Chicago Tribune filed a lawsuit against AI search engine Perplexity on Thursday alleging copyright infringement. The suit, seen by TechCrunch, was filed in a federal court in New York.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Tribune alleges that its lawyers contacted Perplexity in mid-October asking if the AI search engine was using its content, according to the complaint. Perplexity’s lawyers replied it did not train models with the Tribune’s work but that it “may receive non-verbatim factual summaries,” the lawsuit claims.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Tribune’s lawyers, however, argue that Perplexity is delivering Tribune content verbatim. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Interestingly, the newspaper’s lawyers are also calling out Perplexity’s retrieval augmented generation (RAG) as a culprit. RAG is a method used to limit hallucinations by having the model only use an accurate or verified data source. The Tribune argues that Perplexity is using the newspaper’s content in its RAG systems, scraped without permission. Plus, it alleges the Perplexity’s Comet browser is bypassing the paper’s paywall to deliver detailed summaries of those articles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Tribune is one of 17 news publications from MediaNews Group and Tribune Publishing that sued OpenAI and Microsoft over model training material in April. That suit is ongoing. Another nine from these publishers sued the model maker and its cloud provider in November, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While creators have filed many lawsuits against model makers over using their work for model training, we’ll have to see if the courts weigh in about the legal liabilities of RAG as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity did not immediately respond to the Chicago Tribune’s story about its own lawsuit, nor to TechCrunch’s request for comment. Perplexity is facing other such suits. Reddit filed one in October. Dow Jones is also suing. Last month, while Amazon didn’t sue, it did threaten to by sending a cease-and-desist letter over AI browser shopping.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/04/chicago-tribune-sues-perplexity/</guid><pubDate>Fri, 05 Dec 2025 01:20:59 +0000</pubDate></item><item><title>[NEW] Robots that spare warehouse workers the heavy lifting (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/robots-spare-warehouse-workers-heavy-lifting-1205</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT-Pickle-Robot-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;There are some jobs human bodies just weren’t meant to do. Unloading trucks and shipping containers is a repetitive, grueling task — and a big reason warehouse injury rates are more than twice the national average.&lt;/p&gt;&lt;p&gt;The Pickle Robot Company wants its machines to do the heavy lifting. The company’s one-armed robots autonomously unload trailers, picking up boxes weighing up to 50 pounds and placing them onto onboard conveyor belts for warehouses of all types.&lt;/p&gt;&lt;p&gt;The company name, an homage to The Apple Computer Company, hints at the ambitions of founders AJ Meyer ’09, Ariana Eisenstein ’15, SM ’16, and Dan Paluska ’97, SM ’00. The founders want to make the company the technology leader for supply chain automation.&lt;/p&gt;&lt;p&gt;The company’s unloading robots combine generative AI and machine-learning algorithms with sensors, cameras, and machine-vision software to navigate new environments on day one and improve performance over time. Much of the company’s hardware is adapted from industrial partners. You may recognize the arm, for instance, from car manufacturing lines — though you may not have seen it in bright pickle-green.&lt;/p&gt;        

      &lt;/div&gt;
            
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;The company is already working with customers&amp;nbsp;like UPS, Ryobi Tools, and Yusen Logistics to take a load off warehouse workers, freeing them to solve other supply chain bottlenecks in the process.&lt;/p&gt;&lt;p&gt;“Humans are really good edge-case problem solvers, and robots are not,” Paluska says. “How can the robot, which is really good at the brute force, repetitive tasks, interact with humans to solve more problems? Human bodies and minds are so adaptable, the way we sense and respond to the environment is so adaptable, and robots aren’t going to replace that anytime soon. But there’s so much drudgery we can get rid of.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Finding problems for robots&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Meyer and Eisenstein majored in computer science and electrical engineering at MIT, but they didn’t work together until after graduation, when Meyer started the technology consultancy Leaf Labs, which specializes in building embedded computer systems for things like robots, cars, and satellites.&lt;/p&gt;&lt;p&gt;“A bunch of friends from MIT ran that shop,” Meyer recalls, noting it’s still running today. “Ari worked there, Dan consulted there, and we worked on some big projects. We were the primary software and digital design team behind Project Ara, a smartphone for Google, and we worked on a bunch of interesting government projects. It was really a lifestyle company for MIT kids. But 10 years go by, and we thought, ‘We didn’t get into this to do consulting. We got into this to do robots.’”&lt;/p&gt;&lt;p&gt;When Meyer graduated in 2009, problems like robot dexterity seemed insurmountable. By 2018, the rise of algorithmic approaches like neural networks had brought huge advances to robotic manipulation and navigation.&lt;/p&gt;&lt;p&gt;To figure out what problem to solve with robots, the founders talked to people in industries as diverse as agriculture, food prep, and hospitality. At some point, they started visiting logistics warehouses, bringing a stopwatch to see how long it took workers to complete different tasks.&lt;/p&gt;&lt;p&gt;“In 2018, we went to a UPS warehouse and watched 15 guys unloading trucks during a winter night shift,” Meyer recalls. “We spoke to everyone, and not a single person had worked there for more than 90 days. We asked, ‘Why not?’ They laughed at us. They said, ‘Have you tried to do this job before?’”&lt;/p&gt;&lt;p&gt;It turns out warehouse turnover is one of the industry’s biggest problems, limiting productivity as managers constantly grapple with hiring, onboarding, and training.&lt;/p&gt;&lt;p&gt;The founders raised a seed funding round and built robots that could sort boxes because it was an easier problem that allowed them to work with technology like grippers and barcode scanners. Their robots eventually worked, but the company wasn’t growing fast enough to be profitable. Worse yet, the founders were having trouble raising money.&lt;/p&gt;&lt;p&gt;“We were desperately low on funds,” Meyer recalls. “So we thought, ‘Why spend our last dollar on a warm-up task?’”&lt;/p&gt;&lt;p&gt;With money dwindling, the founders built a proof-of-concept robot that could unload trucks reliably for about 20 seconds at a time and posted a video of it on YouTube. Hundreds of potential customers reached out. The interest was enough to get investors back on board to keep the company alive.&lt;/p&gt;&lt;p&gt;The company piloted its first unloading system for a year with a customer in the desert of California, sparing human workers from unloading shipping containers that can reach temperatures up to&amp;nbsp;130 degrees in the summer. It has since scaled deployments with multiple customers and gained traction among third-party logistics centers across the U.S.&lt;/p&gt;&lt;p&gt;The company’s robotic arm is made by the German&amp;nbsp;industrial robotics&amp;nbsp;giant KUKA. The robots are mounted on a custom mobile base with an onboard computing systems&amp;nbsp;so they can navigate to docks and adjust their positions inside trailers autonomously while lifting. The end of each arm features a suction gripper that clings to packages and moves them to the onboard conveyor belt.&lt;/p&gt;&lt;p&gt;The company’s robots can pick up boxes ranging in size from 5-inch cubes to 24-by-30 inch boxes. The robots can unload anywhere from 400 to 1,500 cases per hour depending on size and weight. The company fine tunes pre-trained generative AI models and uses a number of smaller models to ensure the robot runs smoothly in every setting.&lt;/p&gt;&lt;p&gt;The company is also developing a&amp;nbsp;software platform it can integrate with third-party hardware, from humanoid robots to autonomous forklifts.&lt;/p&gt;&lt;p&gt;“Our immediate product roadmap is load and unload,” Meyer says. “But we’re also hoping to connect these third-party platforms. Other companies are also trying to connect robots. What does it mean for the robot unloading a truck to talk to the robot palletizing, or for the forklift to talk to the inventory drone? Can they do the job faster? I think there’s a big network coming in which we need to orchestrate the robots and the automation across the entire supply chain, from the mines to the factories to your front door.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;“Why not us?”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The Pickle Robot Company employs about 130 people in its office in Charlestown, Massachusetts, where a standard — if green — office gives way to a warehouse where its robots can be seen loading boxes onto conveyor belts alongside human workers and manufacturing lines.&lt;/p&gt;&lt;p&gt;This summer, Pickle will be ramping up production of a new version of its system, with further plans to begin designing a two-armed robot sometime after that.&lt;/p&gt;&lt;p&gt;“My supervisor at Leaf Labs once told me ‘No one knows what they’re doing, so why not us?’” Eisenstein says. “I carry that with me all the time. I’ve been very lucky to be able to work with so many talented, experienced people in my career. They all bring their own skill sets and understanding. That’s a massive opportunity — and it’s the only way something as hard as what we’re doing is going to work.”&lt;/p&gt;&lt;p&gt;Moving forward, the company sees many other robot-shaped problems for its machines.&lt;/p&gt;&lt;p&gt;“We didn’t start out by saying, ‘Let’s load and unload a truck,’” Meyers says. “We said, ‘What does it take to make a great robot business?’ Unloading trucks is the first chapter. Now we’ve built a platform to make the next robot that helps with more jobs, starting in logistics but then ultimately in manufacturing, retail, and hopefully the entire supply chain.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT-Pickle-Robot-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;There are some jobs human bodies just weren’t meant to do. Unloading trucks and shipping containers is a repetitive, grueling task — and a big reason warehouse injury rates are more than twice the national average.&lt;/p&gt;&lt;p&gt;The Pickle Robot Company wants its machines to do the heavy lifting. The company’s one-armed robots autonomously unload trailers, picking up boxes weighing up to 50 pounds and placing them onto onboard conveyor belts for warehouses of all types.&lt;/p&gt;&lt;p&gt;The company name, an homage to The Apple Computer Company, hints at the ambitions of founders AJ Meyer ’09, Ariana Eisenstein ’15, SM ’16, and Dan Paluska ’97, SM ’00. The founders want to make the company the technology leader for supply chain automation.&lt;/p&gt;&lt;p&gt;The company’s unloading robots combine generative AI and machine-learning algorithms with sensors, cameras, and machine-vision software to navigate new environments on day one and improve performance over time. Much of the company’s hardware is adapted from industrial partners. You may recognize the arm, for instance, from car manufacturing lines — though you may not have seen it in bright pickle-green.&lt;/p&gt;        

      &lt;/div&gt;
            
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;The company is already working with customers&amp;nbsp;like UPS, Ryobi Tools, and Yusen Logistics to take a load off warehouse workers, freeing them to solve other supply chain bottlenecks in the process.&lt;/p&gt;&lt;p&gt;“Humans are really good edge-case problem solvers, and robots are not,” Paluska says. “How can the robot, which is really good at the brute force, repetitive tasks, interact with humans to solve more problems? Human bodies and minds are so adaptable, the way we sense and respond to the environment is so adaptable, and robots aren’t going to replace that anytime soon. But there’s so much drudgery we can get rid of.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Finding problems for robots&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Meyer and Eisenstein majored in computer science and electrical engineering at MIT, but they didn’t work together until after graduation, when Meyer started the technology consultancy Leaf Labs, which specializes in building embedded computer systems for things like robots, cars, and satellites.&lt;/p&gt;&lt;p&gt;“A bunch of friends from MIT ran that shop,” Meyer recalls, noting it’s still running today. “Ari worked there, Dan consulted there, and we worked on some big projects. We were the primary software and digital design team behind Project Ara, a smartphone for Google, and we worked on a bunch of interesting government projects. It was really a lifestyle company for MIT kids. But 10 years go by, and we thought, ‘We didn’t get into this to do consulting. We got into this to do robots.’”&lt;/p&gt;&lt;p&gt;When Meyer graduated in 2009, problems like robot dexterity seemed insurmountable. By 2018, the rise of algorithmic approaches like neural networks had brought huge advances to robotic manipulation and navigation.&lt;/p&gt;&lt;p&gt;To figure out what problem to solve with robots, the founders talked to people in industries as diverse as agriculture, food prep, and hospitality. At some point, they started visiting logistics warehouses, bringing a stopwatch to see how long it took workers to complete different tasks.&lt;/p&gt;&lt;p&gt;“In 2018, we went to a UPS warehouse and watched 15 guys unloading trucks during a winter night shift,” Meyer recalls. “We spoke to everyone, and not a single person had worked there for more than 90 days. We asked, ‘Why not?’ They laughed at us. They said, ‘Have you tried to do this job before?’”&lt;/p&gt;&lt;p&gt;It turns out warehouse turnover is one of the industry’s biggest problems, limiting productivity as managers constantly grapple with hiring, onboarding, and training.&lt;/p&gt;&lt;p&gt;The founders raised a seed funding round and built robots that could sort boxes because it was an easier problem that allowed them to work with technology like grippers and barcode scanners. Their robots eventually worked, but the company wasn’t growing fast enough to be profitable. Worse yet, the founders were having trouble raising money.&lt;/p&gt;&lt;p&gt;“We were desperately low on funds,” Meyer recalls. “So we thought, ‘Why spend our last dollar on a warm-up task?’”&lt;/p&gt;&lt;p&gt;With money dwindling, the founders built a proof-of-concept robot that could unload trucks reliably for about 20 seconds at a time and posted a video of it on YouTube. Hundreds of potential customers reached out. The interest was enough to get investors back on board to keep the company alive.&lt;/p&gt;&lt;p&gt;The company piloted its first unloading system for a year with a customer in the desert of California, sparing human workers from unloading shipping containers that can reach temperatures up to&amp;nbsp;130 degrees in the summer. It has since scaled deployments with multiple customers and gained traction among third-party logistics centers across the U.S.&lt;/p&gt;&lt;p&gt;The company’s robotic arm is made by the German&amp;nbsp;industrial robotics&amp;nbsp;giant KUKA. The robots are mounted on a custom mobile base with an onboard computing systems&amp;nbsp;so they can navigate to docks and adjust their positions inside trailers autonomously while lifting. The end of each arm features a suction gripper that clings to packages and moves them to the onboard conveyor belt.&lt;/p&gt;&lt;p&gt;The company’s robots can pick up boxes ranging in size from 5-inch cubes to 24-by-30 inch boxes. The robots can unload anywhere from 400 to 1,500 cases per hour depending on size and weight. The company fine tunes pre-trained generative AI models and uses a number of smaller models to ensure the robot runs smoothly in every setting.&lt;/p&gt;&lt;p&gt;The company is also developing a&amp;nbsp;software platform it can integrate with third-party hardware, from humanoid robots to autonomous forklifts.&lt;/p&gt;&lt;p&gt;“Our immediate product roadmap is load and unload,” Meyer says. “But we’re also hoping to connect these third-party platforms. Other companies are also trying to connect robots. What does it mean for the robot unloading a truck to talk to the robot palletizing, or for the forklift to talk to the inventory drone? Can they do the job faster? I think there’s a big network coming in which we need to orchestrate the robots and the automation across the entire supply chain, from the mines to the factories to your front door.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;“Why not us?”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The Pickle Robot Company employs about 130 people in its office in Charlestown, Massachusetts, where a standard — if green — office gives way to a warehouse where its robots can be seen loading boxes onto conveyor belts alongside human workers and manufacturing lines.&lt;/p&gt;&lt;p&gt;This summer, Pickle will be ramping up production of a new version of its system, with further plans to begin designing a two-armed robot sometime after that.&lt;/p&gt;&lt;p&gt;“My supervisor at Leaf Labs once told me ‘No one knows what they’re doing, so why not us?’” Eisenstein says. “I carry that with me all the time. I’ve been very lucky to be able to work with so many talented, experienced people in my career. They all bring their own skill sets and understanding. That’s a massive opportunity — and it’s the only way something as hard as what we’re doing is going to work.”&lt;/p&gt;&lt;p&gt;Moving forward, the company sees many other robot-shaped problems for its machines.&lt;/p&gt;&lt;p&gt;“We didn’t start out by saying, ‘Let’s load and unload a truck,’” Meyers says. “We said, ‘What does it take to make a great robot business?’ Unloading trucks is the first chapter. Now we’ve built a platform to make the next robot that helps with more jobs, starting in logistics but then ultimately in manufacturing, retail, and hopefully the entire supply chain.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/robots-spare-warehouse-workers-heavy-lifting-1205</guid><pubDate>Fri, 05 Dec 2025 05:00:00 +0000</pubDate></item></channel></rss>