<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 01 Mar 2026 02:35:41 +0000</lastBuildDate><item><title>OpenAI’s Sam Altman announces Pentagon deal with ‘technical safeguards’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/28/openais-sam-altman-announces-pentagon-deal-with-technical-safeguards/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2236544077.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI CEO Sam Altman announced late on Friday that his company has reached an agreement allowing the Department of Defense to use its AI models in the department’s classified network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This follows a high-profile standoff between the DoD — also known under the Trump administration as the Department of War — and OpenAI’s rival Anthropic. The Pentagon pushed AI companies, including Anthropic, to allow their models to be used for “all lawful purposes,” while Anthropic sought to draw a red line around mass domestic surveillance and fully autonomous weapons.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a lengthy statement released Thursday, Anthropic CEO Dario Amodei said the company “never raised objections to particular military operations nor attempted to limit use of our technology in an &lt;em&gt;ad hoc&lt;/em&gt; manner,” but he argued that “in a narrow set of cases, we believe AI can undermine, rather than defend, democratic values.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;More than 60 OpenAI employees and 300 Google employees signed an open letter this week asking their employers to support Anthropic’s position.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After Anthropic and the Pentagon failed to reach an agreement, President Donald Trump criticized the “Leftwing nut jobs at Anthropic” in a social media post that also directed federal agencies to stop using the company’s products after a six-month phase-out period.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a separate post, Secretary of Defense Pete Hegseth claimed Anthropic was trying to “seize veto power over the operational decisions of the United States military.” Hegseth also said he is designating Anthropic as a supply-chain risk: “Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Friday, Anthropic said it had “not yet received direct communication from the Department of War or the White House on the status of our negotiations,” but insisted it would “challenge any supply chain risk designation in court.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco, CA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Surprisingly, Altman claimed in a post on X that OpenAI’s new defense contract includes protections addressing the same issues that became a flashpoint for Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Two of our most important safety principles are prohibitions on domestic mass surveillance and human responsibility for the use of force, including for autonomous weapon systems,” Altman said. “The DoW agrees with these principles, reflects them in law and policy, and we put them into our agreement.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman said OpenAI “will build technical safeguards to ensure our models behave as they should, which the DoW also wanted,” and it will deploy engineers with the Pentagon “to help with our models and to ensure their safety.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We are asking the DoW to offer these same terms to all AI companies, which in our opinion we think everyone should be willing to accept,” Altman added. “We have expressed our strong desire to see things de-escalate away from legal and governmental actions and towards reasonable agreements.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fortune’s Sharon Goldman reports that Altman told OpenAI employees at an all-hands meeting that the government will allow the company to build its own “safety stack” to prevent misuse and that “if the model refuses to do a task, then the government would not force OpenAI to make it do that task.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman’s post came shortly before news broke that the U.S. and Israeli governments have begun bombing Iran, with Trump calling for the overthrow of the Iranian government.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2236544077.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI CEO Sam Altman announced late on Friday that his company has reached an agreement allowing the Department of Defense to use its AI models in the department’s classified network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This follows a high-profile standoff between the DoD — also known under the Trump administration as the Department of War — and OpenAI’s rival Anthropic. The Pentagon pushed AI companies, including Anthropic, to allow their models to be used for “all lawful purposes,” while Anthropic sought to draw a red line around mass domestic surveillance and fully autonomous weapons.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a lengthy statement released Thursday, Anthropic CEO Dario Amodei said the company “never raised objections to particular military operations nor attempted to limit use of our technology in an &lt;em&gt;ad hoc&lt;/em&gt; manner,” but he argued that “in a narrow set of cases, we believe AI can undermine, rather than defend, democratic values.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;More than 60 OpenAI employees and 300 Google employees signed an open letter this week asking their employers to support Anthropic’s position.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After Anthropic and the Pentagon failed to reach an agreement, President Donald Trump criticized the “Leftwing nut jobs at Anthropic” in a social media post that also directed federal agencies to stop using the company’s products after a six-month phase-out period.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a separate post, Secretary of Defense Pete Hegseth claimed Anthropic was trying to “seize veto power over the operational decisions of the United States military.” Hegseth also said he is designating Anthropic as a supply-chain risk: “Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Friday, Anthropic said it had “not yet received direct communication from the Department of War or the White House on the status of our negotiations,” but insisted it would “challenge any supply chain risk designation in court.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco, CA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Surprisingly, Altman claimed in a post on X that OpenAI’s new defense contract includes protections addressing the same issues that became a flashpoint for Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Two of our most important safety principles are prohibitions on domestic mass surveillance and human responsibility for the use of force, including for autonomous weapon systems,” Altman said. “The DoW agrees with these principles, reflects them in law and policy, and we put them into our agreement.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman said OpenAI “will build technical safeguards to ensure our models behave as they should, which the DoW also wanted,” and it will deploy engineers with the Pentagon “to help with our models and to ensure their safety.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We are asking the DoW to offer these same terms to all AI companies, which in our opinion we think everyone should be willing to accept,” Altman added. “We have expressed our strong desire to see things de-escalate away from legal and governmental actions and towards reasonable agreements.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fortune’s Sharon Goldman reports that Altman told OpenAI employees at an all-hands meeting that the government will allow the company to build its own “safety stack” to prevent misuse and that “if the model refuses to do a task, then the government would not force OpenAI to make it do that task.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman’s post came shortly before news broke that the U.S. and Israeli governments have begun bombing Iran, with Trump calling for the overthrow of the Iranian government.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/28/openais-sam-altman-announces-pentagon-deal-with-technical-safeguards/</guid><pubDate>Sat, 28 Feb 2026 16:17:36 +0000</pubDate></item><item><title>[NEW] Trump moves to ban Anthropic from the US government (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/02/trump-moves-to-ban-anthropic-from-the-us-government/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Defense Department pressured Anthropic to drop restrictions on how its AI can be used by the military.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthropic CEO Dario Amodei sitting at a table and speaking into a microphone during a Senate committee hearing." class="absolute inset-0 w-full h-full object-cover hidden" height="190" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/getty-Dario-Amodei-300x190.jpg" width="300" /&gt;
                  &lt;img alt="Anthropic CEO Dario Amodei sitting at a table and speaking into a microphone during a Senate committee hearing." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/getty-Dario-Amodei-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic CEO Dario Amodei on Tuesday, July 25, 2023, during a hearing on AI held by the Senate Judiciary Subcommittee on Privacy, Technology, and the Law.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | Bloomberg

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;US President Donald Trump announced Friday that he was instructing every federal agency to “immediately cease” use of Anthropic’s AI tools. The move comes after Anthropic and top officials clashed for weeks over military applications of artificial intelligence.&lt;/p&gt;
&lt;p&gt;“The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War,” Trump said in a post on Truth Social.&lt;/p&gt;
&lt;p&gt;Trump said that there would be a “six month phase out period” for agencies using Anthropic, which could allow time for further negotiations between the government and the AI startup.&lt;/p&gt;
&lt;p&gt;The Pentagon and Anthropic did not immediately respond to requests for comment.&lt;/p&gt;
&lt;p&gt;The Department of Defense has sought to change the terms of a deal struck with Anthropic and other companies last July to eliminate restrictions on how AI can be deployed and instead permit “all lawful use” of the technology. Anthropic objected to the change, claiming that it could allow AI to be used to fully control lethal autonomous weapons or to conduct mass surveillance on US citizens.&lt;/p&gt;
&lt;p&gt;The Pentagon does not currently use AI in these ways, and has said it has no plans to do so. However, top Trump administration officials have voiced opposition to the idea of a civilian tech company dictating military use of such an important technology.&lt;/p&gt;
&lt;p&gt;Anthropic was the first major AI lab to work with the US military, through a $200 million deal signed with the Pentagon last year. It created several custom models known as Claude Gov that have fewer restrictions than its regular ones. Google, OpenAI, and xAI signed similar deals around the same time, but Anthropic is the only AI company currently working with classified systems.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Anthropic’s model is available through platforms provided by Palantir and Amazon’s cloud platform for classified military work. Claude Gov is currently largely used for run-of-the-mill tasks, like writing reports and summarizing documents, but it is also used for intelligence analysis and military planning, according to one source familiar with the situation who spoke to WIRED on condition of anonymity because they are not authorized to discuss the matter publicly.&lt;/p&gt;
&lt;p&gt;In recent years, Silicon Valley has gone from largely avoiding defense work to increasingly embracing it and eventually becoming full-blown military contractors. The fight between Anthropic and the Pentagon is now testing the limits of that shift. This week, several hundred workers from OpenAI and Google signed an open letter supporting Anthropic and criticizing their own companies’ decisions to remove restrictions on military use of AI.&lt;/p&gt;
&lt;p&gt;In a memo sent to OpenAI staff today, CEO Sam Altman said that the company agreed with Anthropic and also viewed mass surveillance and fully autonomous weapons as a “red line.” Altman added that the company would try to agree to a deal with the Pentagon that would let it continue working with the military, The Wall Street Journal reported.&lt;/p&gt;
&lt;p&gt;The public spat between the Pentagon and Anthropic began after Axios reported that US military leaders used Claude to assist in planning its operation to capture Venezuela’s president, Nicolás Maduro. After the operation, an employee at Palantir relayed concerns from an Anthropic staffer to US military leaders about how its models had been used. Anthropic has denied ever raising concerns or interfering with the Pentagon’s use of its technology.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The dispute between Anthropic and the Department of Defense has escalated in recent days, with officials publicly trading barbs with the AI company on social media.&lt;/p&gt;
&lt;p&gt;Defense Secretary Pete Hegseth met with Anthropic’s CEO, Dario Amodei, earlier this week. He gave the company until Friday to commit to changing the terms of its contract to allow “all lawful use” of its models. Hegseth praised Anthropic’s products during the meeting and said that the Department of Defense wanted to continue working with Anthropic, according to one source familiar with interaction who was not authorized to discuss it publicly.&lt;/p&gt;
&lt;p&gt;Some experts say that the dispute boils down to a clash over vibes rather than concrete disagreements over how artificial intelligence should be deployed. “This is such an unnecessary dispute in my opinion,” says Michael Horowitz, an expert on military use of AI and former Deputy Assistant Secretary for emerging technologies at the Pentagon. “It is about theoretical use cases that are not on the table for now.”&lt;/p&gt;
&lt;p&gt;Horowitz notes that Anthropic has supported all of the ways the Department of Defense has proposed using its technology thus far. “My sense is that the Pentagon and Anthropic agree at present about the use cases where the technology is not ready for prime time,” he adds.&lt;/p&gt;
&lt;p&gt;Anthropic was founded on the idea that AI should be built with safety at its core. In January, Amoedi penned a blog post about the risks of powerful artificial intelligence that touched upon the dangers of fully autonomous AI-controlled weapons.&lt;/p&gt;
&lt;p&gt;“These weapons also have legitimate uses in the defense of democracy,” Amodei wrote. “But they are a dangerous weapon to wield.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Additional reporting by Paresh Dave.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This story originally appeared at WIRED.com&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The Defense Department pressured Anthropic to drop restrictions on how its AI can be used by the military.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthropic CEO Dario Amodei sitting at a table and speaking into a microphone during a Senate committee hearing." class="absolute inset-0 w-full h-full object-cover hidden" height="190" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/getty-Dario-Amodei-300x190.jpg" width="300" /&gt;
                  &lt;img alt="Anthropic CEO Dario Amodei sitting at a table and speaking into a microphone during a Senate committee hearing." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/getty-Dario-Amodei-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic CEO Dario Amodei on Tuesday, July 25, 2023, during a hearing on AI held by the Senate Judiciary Subcommittee on Privacy, Technology, and the Law.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | Bloomberg

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;US President Donald Trump announced Friday that he was instructing every federal agency to “immediately cease” use of Anthropic’s AI tools. The move comes after Anthropic and top officials clashed for weeks over military applications of artificial intelligence.&lt;/p&gt;
&lt;p&gt;“The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War,” Trump said in a post on Truth Social.&lt;/p&gt;
&lt;p&gt;Trump said that there would be a “six month phase out period” for agencies using Anthropic, which could allow time for further negotiations between the government and the AI startup.&lt;/p&gt;
&lt;p&gt;The Pentagon and Anthropic did not immediately respond to requests for comment.&lt;/p&gt;
&lt;p&gt;The Department of Defense has sought to change the terms of a deal struck with Anthropic and other companies last July to eliminate restrictions on how AI can be deployed and instead permit “all lawful use” of the technology. Anthropic objected to the change, claiming that it could allow AI to be used to fully control lethal autonomous weapons or to conduct mass surveillance on US citizens.&lt;/p&gt;
&lt;p&gt;The Pentagon does not currently use AI in these ways, and has said it has no plans to do so. However, top Trump administration officials have voiced opposition to the idea of a civilian tech company dictating military use of such an important technology.&lt;/p&gt;
&lt;p&gt;Anthropic was the first major AI lab to work with the US military, through a $200 million deal signed with the Pentagon last year. It created several custom models known as Claude Gov that have fewer restrictions than its regular ones. Google, OpenAI, and xAI signed similar deals around the same time, but Anthropic is the only AI company currently working with classified systems.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Anthropic’s model is available through platforms provided by Palantir and Amazon’s cloud platform for classified military work. Claude Gov is currently largely used for run-of-the-mill tasks, like writing reports and summarizing documents, but it is also used for intelligence analysis and military planning, according to one source familiar with the situation who spoke to WIRED on condition of anonymity because they are not authorized to discuss the matter publicly.&lt;/p&gt;
&lt;p&gt;In recent years, Silicon Valley has gone from largely avoiding defense work to increasingly embracing it and eventually becoming full-blown military contractors. The fight between Anthropic and the Pentagon is now testing the limits of that shift. This week, several hundred workers from OpenAI and Google signed an open letter supporting Anthropic and criticizing their own companies’ decisions to remove restrictions on military use of AI.&lt;/p&gt;
&lt;p&gt;In a memo sent to OpenAI staff today, CEO Sam Altman said that the company agreed with Anthropic and also viewed mass surveillance and fully autonomous weapons as a “red line.” Altman added that the company would try to agree to a deal with the Pentagon that would let it continue working with the military, The Wall Street Journal reported.&lt;/p&gt;
&lt;p&gt;The public spat between the Pentagon and Anthropic began after Axios reported that US military leaders used Claude to assist in planning its operation to capture Venezuela’s president, Nicolás Maduro. After the operation, an employee at Palantir relayed concerns from an Anthropic staffer to US military leaders about how its models had been used. Anthropic has denied ever raising concerns or interfering with the Pentagon’s use of its technology.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The dispute between Anthropic and the Department of Defense has escalated in recent days, with officials publicly trading barbs with the AI company on social media.&lt;/p&gt;
&lt;p&gt;Defense Secretary Pete Hegseth met with Anthropic’s CEO, Dario Amodei, earlier this week. He gave the company until Friday to commit to changing the terms of its contract to allow “all lawful use” of its models. Hegseth praised Anthropic’s products during the meeting and said that the Department of Defense wanted to continue working with Anthropic, according to one source familiar with interaction who was not authorized to discuss it publicly.&lt;/p&gt;
&lt;p&gt;Some experts say that the dispute boils down to a clash over vibes rather than concrete disagreements over how artificial intelligence should be deployed. “This is such an unnecessary dispute in my opinion,” says Michael Horowitz, an expert on military use of AI and former Deputy Assistant Secretary for emerging technologies at the Pentagon. “It is about theoretical use cases that are not on the table for now.”&lt;/p&gt;
&lt;p&gt;Horowitz notes that Anthropic has supported all of the ways the Department of Defense has proposed using its technology thus far. “My sense is that the Pentagon and Anthropic agree at present about the use cases where the technology is not ready for prime time,” he adds.&lt;/p&gt;
&lt;p&gt;Anthropic was founded on the idea that AI should be built with safety at its core. In January, Amoedi penned a blog post about the risks of powerful artificial intelligence that touched upon the dangers of fully autonomous AI-controlled weapons.&lt;/p&gt;
&lt;p&gt;“These weapons also have legitimate uses in the defense of democracy,” Amodei wrote. “But they are a dangerous weapon to wield.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Additional reporting by Paresh Dave.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This story originally appeared at WIRED.com&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/02/trump-moves-to-ban-anthropic-from-the-us-government/</guid><pubDate>Sat, 28 Feb 2026 20:00:26 +0000</pubDate></item><item><title>[NEW] The billion-dollar infrastructure deals powering the AI boom (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/28/billion-dollar-infrastructure-deals-ai-boom-data-centers-openai-oracle-nvidia-microsoft-google-meta/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/08/GettyImages-1297856112.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It takes a lot of computing power to run an AI product — and as the tech industry races to tap the power of AI models, there’s a parallel race underway to build the infrastructure that will power them. On a recent earnings call, Nvidia CEO Jensen Huang estimated that between $3 trillion and $4 trillion will be spent on AI infrastructure by the end of the decade — with much of that money coming from AI companies. Along the way, they’re placing immense strain on power grids and pushing the industry’s building capacity to its limit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Below, we’ve laid out everything we know about the biggest AI infrastructure projects, including major spending from Meta, Oracle, Microsoft, Google, and OpenAI. We’ll keep it updated as the boom continues and the numbers climb even higher.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-microsoft-s-2019-investment-in-openai"&gt;Microsoft’s 2019 investment in OpenAI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This is arguably the deal that kicked off the whole contemporary AI boom: In 2019, Microsoft made a $1 billion investment in a buzzy non-profit called OpenAI, known mostly for its association with Elon Musk. Crucially, the deal made Microsoft the exclusive cloud provider for OpenAI — and as the demands of model training became more intense, more of Microsoft’s investment started to come in the form of Azure cloud credit rather than cash. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It was a great deal for both sides: Microsoft was able to claim more Azure sales, and OpenAI got more money for its biggest single expense. In the years that followed, Microsoft would build its investment up to nearly $14 billion — a move that is set to pay off enormously when OpenAI converts into a for-profit company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership between the two companies has unwound more recently. Last year, OpenAI announced it would no longer be using Microsoft’s cloud exclusively, instead giving the company a right of first refusal on future infrastructure demands but pursuing others if Azure couldn’t meet their needs. Microsoft has also begun exploring other foundation models to power its AI products, establishing even more independence from the AI giant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s arrangement with Microsoft was so successful that it’s become a common practice for AI services to sign on with a particular cloud provider. Anthropic has received $8 billion in investment from Amazon, while making kernel-level modifications on the company’s hardware to make it better suited for AI training. Google Cloud has also signed on smaller AI companies like Lovable and Windsurf as “primary computing partners,” although those deals did not involve any investment. And even OpenAI has gone back to the well, receiving a $100 billion investment from Nvidia in September, giving it capacity to buy even more of the company’s GPUs.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-the-rise-of-oracle"&gt;The rise of Oracle&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;On June 30, 2025, Oracle revealed in an SEC filing that it had signed a $30 billion cloud services deal with an unnamed partner; this is more than the company’s cloud revenues for all of the previous fiscal year. OpenAI was eventually revealed as the partner, securing Oracle a spot alongside Google as one of OpenAI’s string of post-Microsoft hosting partners. Unsurprisingly, the company’s stock went shooting up.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco, CA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A few months later, it happened again. On September 10, Oracle revealed a five-year, $300 billion deal for compute power, set to begin in 2027. Oracle’s stock climbed even higher, briefly making founder Larry Ellison the richest man in the world. The sheer scale of the deal is stunning: OpenAI does not have $300 billion to spend, so the figure presumes immense growth for both companies, and more than a little faith. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But before a single dollar is spent, the deal has already cemented Oracle as one of the leading AI infrastructure providers — and a financial force to be reckoned with.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-nvidia-s-investment-spree"&gt;Nvidia’s investment spree&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;As AI labs scramble to build infrastructure, they’re mostly buying GPUs from one company: Nvidia. That trade has made Nvidia flush with cash — and it’s been investing that cash back into the industry in increasingly unconventional ways. In September 2025, Nvidia bought a 4% stake in rival Intel for $5 billion — but even more surprising has been the deals with its own customers. One week after the Intel deal was revealed, the company announced a $100 billion investment in OpenAI, paid for with GPUs that would be used in OpenAI’s ongoing data center projects. Nvidia has since announced a similar deal with Elon Musk’s xAI, and OpenAI launched a separate GPU-for-stock arrangement with AMD.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If that seems circular, it’s because it is. Nvidia’s GPUs are valuable because they’re so scarce — and by trading them directly into an ever-inflating data center scheme, Nvidia is making sure they stay that way. You could say the same thing about OpenAI’s privately held stock, which is all the more valuable because it can’t be obtained through public markets. For now, OpenAI and Nvidia are riding high and nobody seems too worried — but if the momentum starts to flag, this sort of arrangement will get a lot more scrutiny.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-building-tomorrow-s-hyperscale-data-centers"&gt;Building tomorrow’s hyperscale data centers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;For companies like Meta that already have significant legacy infrastructure, the story is more complicated — although equally expensive. Meta CEO Mark Zuckerberg has said that the company plans to spend $600 billion on U.S. infrastructure through the end of 2028.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the first half of 2025, the company spent $30 billion more than the previous year, driven largely by the company’s growing AI ambitions. Some of that spending goes toward big ticket cloud contracts, like a recent $10 billion deal with Google Cloud, but even more resources are being poured into two massive new data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new 2,250-acre site in Louisiana, dubbed Hyperion, will cost an estimated $10 billion to build out and provide an estimated 5 gigawatts of compute power. Notably, the site includes an arrangement with a local nuclear power plant to handle the increased energy load. A smaller site in Ohio, called Prometheus, is expected to come online in 2026, powered by natural gas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That kind of buildout comes with real environmental costs. Elon Musk’s xAI built its own hybrid data center and power-generation plant in South Memphis, Tennessee. The plant has quickly become one of the county’s largest emitters of smog-producing chemicals, thanks to a string of natural gas turbines that experts say violate the Clean Air Act.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-the-stargate-moonshot"&gt;The Stargate moonshot&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Just two days after his second inauguration last January, President Trump announced a joint venture between SoftBank, OpenAI, and Oracle, meant to spend $500 billion building AI infrastructure in the United States. Named “Stargate” after the 1994 film, the project arrived with incredible amounts of hype, with Trump calling it “the largest AI infrastructure project in history.” OpenAI’s Sam Altman seemed to agree, saying, ​​”I think this will be the most important project of this era.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In broad strokes, the plan was for SoftBank to provide the funding, with Oracle handling the buildout with input from OpenAI. Overseeing it all was Trump, who promised to clear away any regulatory hurdles that might slow down the build. But there were doubts from the beginning, including from Elon Musk, Altman’s business rival, who claimed the project did not have the available funds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the hype has died down, the project has lost some momentum. In August, Bloomberg reported that the partners were failing to reach consensus. Nonetheless, the project has moved forward with the construction of eight data centers in Abilene, Texas, with construction on the final building set to be finished by the end of 2026.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-capex-crunch"&gt;The capex crunch&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;“Capital expenditures” are usually a pretty dry metric, referring to a company’s spending on physical assets. But as tech companies lined up to report their capex plans for 2026, the rush of data center spending made the figures a lot more interesting — and a lot bigger.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon was the capex leader, projecting $200 billion in 2026 spending (up from $131 billion in 2025), while Google was a close second with an estimate between $175 billion and $185 billion&amp;nbsp;(up from $91 billion in 2025). Meta estimated $115 billion to $135 billion (up from $71 billion the previous year), although that figure is a little deceptive because a lot of the data center projects have been kept off their books entirely. All told, hyperscalers are planning to spend nearly $700 billion on data center projects in 2026 alone.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It was enough money to spook some investors. The companies were mostly undeterred, however, explaining that AI infrastructure was vital to their companies’ future. It’s set up a strange dynamic. As you might expect, tech executives are more bullish on AI than their Wall Street counterparts — and the more tech companies spend, the more nervous their bankers get. Add in the huge amounts of debt many companies are taking on to fund those buildouts, and you start to hear CFOs across the valley grinding their teeth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That hasn’t put a damper on AI spending yet, but it will soon — unless of course, hyperscalers show they can make those investments pay off.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was first published on September 22.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/08/GettyImages-1297856112.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It takes a lot of computing power to run an AI product — and as the tech industry races to tap the power of AI models, there’s a parallel race underway to build the infrastructure that will power them. On a recent earnings call, Nvidia CEO Jensen Huang estimated that between $3 trillion and $4 trillion will be spent on AI infrastructure by the end of the decade — with much of that money coming from AI companies. Along the way, they’re placing immense strain on power grids and pushing the industry’s building capacity to its limit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Below, we’ve laid out everything we know about the biggest AI infrastructure projects, including major spending from Meta, Oracle, Microsoft, Google, and OpenAI. We’ll keep it updated as the boom continues and the numbers climb even higher.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-microsoft-s-2019-investment-in-openai"&gt;Microsoft’s 2019 investment in OpenAI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This is arguably the deal that kicked off the whole contemporary AI boom: In 2019, Microsoft made a $1 billion investment in a buzzy non-profit called OpenAI, known mostly for its association with Elon Musk. Crucially, the deal made Microsoft the exclusive cloud provider for OpenAI — and as the demands of model training became more intense, more of Microsoft’s investment started to come in the form of Azure cloud credit rather than cash. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It was a great deal for both sides: Microsoft was able to claim more Azure sales, and OpenAI got more money for its biggest single expense. In the years that followed, Microsoft would build its investment up to nearly $14 billion — a move that is set to pay off enormously when OpenAI converts into a for-profit company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership between the two companies has unwound more recently. Last year, OpenAI announced it would no longer be using Microsoft’s cloud exclusively, instead giving the company a right of first refusal on future infrastructure demands but pursuing others if Azure couldn’t meet their needs. Microsoft has also begun exploring other foundation models to power its AI products, establishing even more independence from the AI giant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s arrangement with Microsoft was so successful that it’s become a common practice for AI services to sign on with a particular cloud provider. Anthropic has received $8 billion in investment from Amazon, while making kernel-level modifications on the company’s hardware to make it better suited for AI training. Google Cloud has also signed on smaller AI companies like Lovable and Windsurf as “primary computing partners,” although those deals did not involve any investment. And even OpenAI has gone back to the well, receiving a $100 billion investment from Nvidia in September, giving it capacity to buy even more of the company’s GPUs.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-the-rise-of-oracle"&gt;The rise of Oracle&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;On June 30, 2025, Oracle revealed in an SEC filing that it had signed a $30 billion cloud services deal with an unnamed partner; this is more than the company’s cloud revenues for all of the previous fiscal year. OpenAI was eventually revealed as the partner, securing Oracle a spot alongside Google as one of OpenAI’s string of post-Microsoft hosting partners. Unsurprisingly, the company’s stock went shooting up.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco, CA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A few months later, it happened again. On September 10, Oracle revealed a five-year, $300 billion deal for compute power, set to begin in 2027. Oracle’s stock climbed even higher, briefly making founder Larry Ellison the richest man in the world. The sheer scale of the deal is stunning: OpenAI does not have $300 billion to spend, so the figure presumes immense growth for both companies, and more than a little faith. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But before a single dollar is spent, the deal has already cemented Oracle as one of the leading AI infrastructure providers — and a financial force to be reckoned with.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-nvidia-s-investment-spree"&gt;Nvidia’s investment spree&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;As AI labs scramble to build infrastructure, they’re mostly buying GPUs from one company: Nvidia. That trade has made Nvidia flush with cash — and it’s been investing that cash back into the industry in increasingly unconventional ways. In September 2025, Nvidia bought a 4% stake in rival Intel for $5 billion — but even more surprising has been the deals with its own customers. One week after the Intel deal was revealed, the company announced a $100 billion investment in OpenAI, paid for with GPUs that would be used in OpenAI’s ongoing data center projects. Nvidia has since announced a similar deal with Elon Musk’s xAI, and OpenAI launched a separate GPU-for-stock arrangement with AMD.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If that seems circular, it’s because it is. Nvidia’s GPUs are valuable because they’re so scarce — and by trading them directly into an ever-inflating data center scheme, Nvidia is making sure they stay that way. You could say the same thing about OpenAI’s privately held stock, which is all the more valuable because it can’t be obtained through public markets. For now, OpenAI and Nvidia are riding high and nobody seems too worried — but if the momentum starts to flag, this sort of arrangement will get a lot more scrutiny.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-building-tomorrow-s-hyperscale-data-centers"&gt;Building tomorrow’s hyperscale data centers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;For companies like Meta that already have significant legacy infrastructure, the story is more complicated — although equally expensive. Meta CEO Mark Zuckerberg has said that the company plans to spend $600 billion on U.S. infrastructure through the end of 2028.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the first half of 2025, the company spent $30 billion more than the previous year, driven largely by the company’s growing AI ambitions. Some of that spending goes toward big ticket cloud contracts, like a recent $10 billion deal with Google Cloud, but even more resources are being poured into two massive new data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A new 2,250-acre site in Louisiana, dubbed Hyperion, will cost an estimated $10 billion to build out and provide an estimated 5 gigawatts of compute power. Notably, the site includes an arrangement with a local nuclear power plant to handle the increased energy load. A smaller site in Ohio, called Prometheus, is expected to come online in 2026, powered by natural gas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That kind of buildout comes with real environmental costs. Elon Musk’s xAI built its own hybrid data center and power-generation plant in South Memphis, Tennessee. The plant has quickly become one of the county’s largest emitters of smog-producing chemicals, thanks to a string of natural gas turbines that experts say violate the Clean Air Act.&lt;/p&gt;

&lt;h2 class="wp-block-heading has-h-3-font-size" id="h-the-stargate-moonshot"&gt;The Stargate moonshot&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Just two days after his second inauguration last January, President Trump announced a joint venture between SoftBank, OpenAI, and Oracle, meant to spend $500 billion building AI infrastructure in the United States. Named “Stargate” after the 1994 film, the project arrived with incredible amounts of hype, with Trump calling it “the largest AI infrastructure project in history.” OpenAI’s Sam Altman seemed to agree, saying, ​​”I think this will be the most important project of this era.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In broad strokes, the plan was for SoftBank to provide the funding, with Oracle handling the buildout with input from OpenAI. Overseeing it all was Trump, who promised to clear away any regulatory hurdles that might slow down the build. But there were doubts from the beginning, including from Elon Musk, Altman’s business rival, who claimed the project did not have the available funds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the hype has died down, the project has lost some momentum. In August, Bloomberg reported that the partners were failing to reach consensus. Nonetheless, the project has moved forward with the construction of eight data centers in Abilene, Texas, with construction on the final building set to be finished by the end of 2026.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-capex-crunch"&gt;The capex crunch&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;“Capital expenditures” are usually a pretty dry metric, referring to a company’s spending on physical assets. But as tech companies lined up to report their capex plans for 2026, the rush of data center spending made the figures a lot more interesting — and a lot bigger.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon was the capex leader, projecting $200 billion in 2026 spending (up from $131 billion in 2025), while Google was a close second with an estimate between $175 billion and $185 billion&amp;nbsp;(up from $91 billion in 2025). Meta estimated $115 billion to $135 billion (up from $71 billion the previous year), although that figure is a little deceptive because a lot of the data center projects have been kept off their books entirely. All told, hyperscalers are planning to spend nearly $700 billion on data center projects in 2026 alone.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It was enough money to spook some investors. The companies were mostly undeterred, however, explaining that AI infrastructure was vital to their companies’ future. It’s set up a strange dynamic. As you might expect, tech executives are more bullish on AI than their Wall Street counterparts — and the more tech companies spend, the more nervous their bankers get. Add in the huge amounts of debt many companies are taking on to fund those buildouts, and you start to hear CFOs across the valley grinding their teeth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That hasn’t put a damper on AI spending yet, but it will soon — unless of course, hyperscalers show they can make those investments pay off.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was first published on September 22.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/28/billion-dollar-infrastructure-deals-ai-boom-data-centers-openai-oracle-nvidia-microsoft-google-meta/</guid><pubDate>Sat, 28 Feb 2026 20:41:55 +0000</pubDate></item><item><title>[NEW] Anthropic’s Claude rises to No. 2 in the App Store following Pentagon dispute (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/28/anthropics-claude-rises-to-no-2-in-the-app-store-following-pentagon-dispute/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/Claude-ad-e1733259907871.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic’s chatbot Claude seems to have benefited from the attention around the company’s fraught negotiations with the Pentagon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As first reported by CNBC, as of Saturday afternoon, Claude is currently ranked number two among free apps in Apple’s US App Store — the number one app is OpenAI’s ChatGPT, and number three is Google Gemini.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to data from SensorTower, Claude was just outside the top 100 at the end of January, and has spent most of February somewhere in the top 20. Its ranking has climbed in the last few days, from sixth on Wednesday to fourth on Thursday to second on Saturday (today).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After Anthropic attempted to negotiate for safeguards preventing the Department of Defense from using its AI models for mass domestic surveillance or fully autonomous weapons, President Donald Trump directed federal agencies to stop using all Anthropic products and Secretary of Defense Pete Hegseth said he’s designating the company a supply-chain threat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI subsequently announced its own agreement with the Pentagon, which CEO Sam Altman claimed includes safeguards related to domestic surveillance and autonomous weapons.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/Claude-ad-e1733259907871.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic’s chatbot Claude seems to have benefited from the attention around the company’s fraught negotiations with the Pentagon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As first reported by CNBC, as of Saturday afternoon, Claude is currently ranked number two among free apps in Apple’s US App Store — the number one app is OpenAI’s ChatGPT, and number three is Google Gemini.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to data from SensorTower, Claude was just outside the top 100 at the end of January, and has spent most of February somewhere in the top 20. Its ranking has climbed in the last few days, from sixth on Wednesday to fourth on Thursday to second on Saturday (today).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After Anthropic attempted to negotiate for safeguards preventing the Department of Defense from using its AI models for mass domestic surveillance or fully autonomous weapons, President Donald Trump directed federal agencies to stop using all Anthropic products and Secretary of Defense Pete Hegseth said he’s designating the company a supply-chain threat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI subsequently announced its own agreement with the Pentagon, which CEO Sam Altman claimed includes safeguards related to domestic surveillance and autonomous weapons.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/28/anthropics-claude-rises-to-no-2-in-the-app-store-following-pentagon-dispute/</guid><pubDate>Sat, 28 Feb 2026 21:05:06 +0000</pubDate></item><item><title>[NEW] The trap Anthropic built for itself (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/28/the-trap-anthropic-built-for-itself/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2261854833.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Friday afternoon, just as this interview was getting underway, a news alert flashed across my computer screen: the Trump administration was severing ties with Anthropic, the San Francisco AI company founded in 2021 by Dario Amodei. Defense Secretary Pete Hegseth had invoked a national security law to blacklist the company from doing business with the Pentagon after Amodei refused to allow Anthropic’s tech to be used for mass surveillance of U.S. citizens or for autonomous armed drones that could select and kill targets without human input.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It was a jaw-dropping sequence. Anthropic stands to lose a contract worth up to $200 million and will be barred from working with other defense contractors after President Trump posted on Truth Social directing every federal agency to “immediately cease all use of Anthropic technology.” (Anthropic has since said it will challenge the Pentagon in court.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Max Tegmark has spent the better part of a decade warning that the race to build ever-more-powerful AI systems is outpacing the world’s ability to govern them. The MIT physicist founded the Future of Life Institute in 2014 and helped organize an open letter — ultimately signed by more than 33,000 people, including Elon Musk — calling for a pause in advanced AI development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His view of the Anthropic crisis is unsparing: the company, like its rivals, has sown the seeds of its own predicament. Tegmark’s argument doesn’t begin with the Pentagon but with a decision made years earlier — a choice, shared across the industry, to resist binding regulation. Anthropic, OpenAI, Google DeepMind and others have long promised to govern themselves responsibly. Anthropic this week even dropped the central tenet of its own safety pledge — its promise not to release increasingly powerful AI systems until the company was confident they wouldn’t cause harm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, in the absence of rules, there’s not a lot to protect these players, says Tegmark. Here’s more from that interview, edited for length and clarity. You can hear the full conversation this coming week on TechCrunch’s StrictlyVC Download podcast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;When you saw this news just now about Anthropic, what was your first reaction?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The road to hell is paved with good intentions. It’s so interesting to think back a decade ago, when people were so excited about how we were going to make artificial intelligence to cure cancer, to grow the prosperity in America and make America strong. And here we are now where the U.S. government is pissed off at this company for not wanting AI to be used for domestic mass surveillance of Americans, and also not wanting to have killer robots that can autonomously — without any human input at all — decide who gets killed.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco, CA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Anthropic has staked its entire identity on being a safety-first AI company, and yet it was collaborating with defense and intelligence agencies [dating back to at least 2024]. Do you think that’s at all contradictory?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It is contradictory. If I can give a little cynical take on this — yes, Anthropic has been very good at marketing themselves as all about safety. But if you actually look at the facts rather than the claims, what you see is that Anthropic, OpenAI, Google DeepMind and xAI have all talked a lot about how they care about safety. None of them has come out supporting binding safety regulation the way we have in other industries. And all four of these companies have now broken their own promises. First we had Google — this big slogan, ‘Don’t be evil.’ Then they dropped that. Then they dropped another longer commitment that basically said they promised not to do harm with AI. They dropped that so they could sell AI for surveillance and weapons. OpenAI just dropped the word safety from their mission statement. xAI shut down their whole safety team. And now Anthropic, earlier in the week, dropped their most important safety commitment — the promise not to release powerful AI systems until they were sure they weren’t going to cause harm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;How did companies that made such prominent safety commitments end up in this position?&lt;/strong&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;All of these companies, especially OpenAI and Google DeepMind but to some extent also Anthropic, have persistently lobbied against regulation of AI, saying, ‘Just trust us, we’re going to regulate ourselves.’ And they’ve successfully lobbied. So we right now have less regulation on AI systems in America than on sandwiches. You know, if you want to open a sandwich shop and the health inspector finds 15 rats in the kitchen, he won’t let you sell any sandwiches until you fix it. But if you say, ‘Don’t worry, I’m not going to sell sandwiches, I’m going to sell AI girlfriends for 11-year-olds, and they’ve been linked to suicides in the past, and then I’m going to release something called superintelligence which might overthrow the U.S. government, but I have a good feeling about mine’ — the inspector has to say, ‘Fine, go ahead, just don’t sell sandwiches.’&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;There’s food safety regulation and no AI regulation.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;And this, I feel, all of these companies really share the blame for. Because if they had taken all these promises that they made back in the day for how they were going to be so safe and goody-goody, and gotten together, and then gone to the government and said, ‘Please take our voluntary commitments and turn them into U.S. law that binds even our most sloppy competitors’ — this would have happened instead. We’re in a complete regulatory vacuum. And we know what happens when there’s a complete corporate amnesty: you get thalidomide, you get tobacco companies pushing cigarettes on kids, you get asbestos causing lung cancer. So it’s sort of ironic that their own resistance to having laws saying what’s okay and not okay to do with AI is now coming back and biting them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is no law right now against building AI to kill Americans, so the government can just suddenly ask for it. If the companies themselves had earlier come out and said, ‘We want this law,’ they wouldn’t be in this pickle. They really shot themselves in the foot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;The companies’ counter-argument is always the race with China — if American companies don’t do this, Beijing will. Does that argument hold?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Let’s analyze that. The most common talking point from the lobbyists for the AI companies — they’re now better funded and more numerous than the lobbyists from the fossil fuel industry, the pharma industry and the military-industrial complex combined — is that whenever anyone proposes any kind of regulation, they say, ‘But China.’ So let’s look at that. China is in the process of banning AI girlfriends outright. Not just age limits — they’re looking at banning all anthropomorphic AI. Why? Not because they want to please America but because they feel this is screwing up Chinese youth and making China weak. Obviously, it’s making American youth weak, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And when people say we have to race to build superintelligence so we can win against China — when we don’t actually know how to control superintelligence, so that the default outcome is that humanity loses control of Earth to alien machines — guess what? The Chinese Communist Party really likes control. Who in their right mind thinks that Xi Jinping is going to tolerate some Chinese AI company building something that overthrows the Chinese government? No way. It’s clearly really bad for the American government too if it gets overthrown in a coup by the first American company to build superintelligence. This is a national security threat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;That’s compelling framing — superintelligence as a national security threat, not an asset. Do you see that view gaining traction in Washington? &lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think if people in the national security community listen to Dario Amodei describe his vision — he’s given a famous speech where he says we’ll soon have a country of geniuses in a data center — they might start thinking: wait, did Dario just use the word ‘country’? Maybe I should put that country of geniuses in a data center on the same threat list I’m keeping tabs on, because that sounds threatening to the U.S. government. And I think fairly soon, enough people in the U.S. national security community are going to realize that uncontrollable superintelligence is a threat, not a tool. This is totally analogous to the Cold War. There was a race for dominance — economic and military — against the Soviet Union. We Americans won that one without ever engaging in the second race, which was to see who could put the most nuclear craters in the other superpower. People realized that was just suicide. No one wins. The same logic applies here.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;What does all of this mean for the pace of AI development more broadly? How close do you think we are to the systems you’re describing?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Six years ago, almost every expert in AI I knew predicted we were decades away from having AI that could master language and knowledge at human level — maybe 2040, maybe 2050. They were all wrong, because we already have that now. We’ve seen AI progress quite rapidly from high school level to college level to PhD level to university professor level in some areas. Last year, AI won the gold medal at the International Mathematics Olympiad, which is about as difficult as human tasks get. I wrote a paper together with Yoshua Bengio, Dan Hendrycks, and other top AI researchers just a few months ago giving a rigorous definition of AGI. According to this, GPT-4 was 27% of the way there. GPT-5 was 57% of the way there. So we’re not there yet, but going from 27% to 57% that quickly suggests it might not be that long.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When I lectured to my students yesterday at MIT, I told them that even if it takes four years, that means when they graduate, they might not be able to get any jobs anymore. It’s certainly not too soon to start preparing for it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Anthropic is now blacklisted. I’m curious to see what happens next — will the other AI giants stand with them and say, we won’t do this either? Or does someone like xAI raise their hand and say, Anthropic didn’t want that contract, we’ll take it?&lt;/strong&gt; &lt;strong&gt;[Editor’s note: Hours after the interview, OpenAI announced its own deal with the Pentagon.]&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last night, Sam Altman came out and said he stands with Anthropic and has the same red lines. I admire him for the courage of saying that. Google, as of when we started this interview, had said nothing. If they just stay quiet, I think that’s incredibly embarrassing for them as a company, and a lot of their staff will feel the same. We haven’t heard anything from xAI yet either. So it’ll be interesting to see. Basically, there’s this moment where everybody has to show their true colors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Is there a version of this where the outcome is actually good?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yes, and this is why I’m actually optimistic in a strange way. There’s such an obvious alternative here. If we just start treating AI companies like any other companies — drop the corporate amnesty — they would clearly have to do something like a clinical trial before they released something this powerful, and demonstrate to independent experts that they know how to control it. Then we get a golden age with all the good stuff from AI, without the existential angst. That’s not the path we’re on right now. But it could be.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2261854833.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Friday afternoon, just as this interview was getting underway, a news alert flashed across my computer screen: the Trump administration was severing ties with Anthropic, the San Francisco AI company founded in 2021 by Dario Amodei. Defense Secretary Pete Hegseth had invoked a national security law to blacklist the company from doing business with the Pentagon after Amodei refused to allow Anthropic’s tech to be used for mass surveillance of U.S. citizens or for autonomous armed drones that could select and kill targets without human input.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It was a jaw-dropping sequence. Anthropic stands to lose a contract worth up to $200 million and will be barred from working with other defense contractors after President Trump posted on Truth Social directing every federal agency to “immediately cease all use of Anthropic technology.” (Anthropic has since said it will challenge the Pentagon in court.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Max Tegmark has spent the better part of a decade warning that the race to build ever-more-powerful AI systems is outpacing the world’s ability to govern them. The MIT physicist founded the Future of Life Institute in 2014 and helped organize an open letter — ultimately signed by more than 33,000 people, including Elon Musk — calling for a pause in advanced AI development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His view of the Anthropic crisis is unsparing: the company, like its rivals, has sown the seeds of its own predicament. Tegmark’s argument doesn’t begin with the Pentagon but with a decision made years earlier — a choice, shared across the industry, to resist binding regulation. Anthropic, OpenAI, Google DeepMind and others have long promised to govern themselves responsibly. Anthropic this week even dropped the central tenet of its own safety pledge — its promise not to release increasingly powerful AI systems until the company was confident they wouldn’t cause harm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, in the absence of rules, there’s not a lot to protect these players, says Tegmark. Here’s more from that interview, edited for length and clarity. You can hear the full conversation this coming week on TechCrunch’s StrictlyVC Download podcast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;When you saw this news just now about Anthropic, what was your first reaction?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The road to hell is paved with good intentions. It’s so interesting to think back a decade ago, when people were so excited about how we were going to make artificial intelligence to cure cancer, to grow the prosperity in America and make America strong. And here we are now where the U.S. government is pissed off at this company for not wanting AI to be used for domestic mass surveillance of Americans, and also not wanting to have killer robots that can autonomously — without any human input at all — decide who gets killed.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco, CA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Anthropic has staked its entire identity on being a safety-first AI company, and yet it was collaborating with defense and intelligence agencies [dating back to at least 2024]. Do you think that’s at all contradictory?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It is contradictory. If I can give a little cynical take on this — yes, Anthropic has been very good at marketing themselves as all about safety. But if you actually look at the facts rather than the claims, what you see is that Anthropic, OpenAI, Google DeepMind and xAI have all talked a lot about how they care about safety. None of them has come out supporting binding safety regulation the way we have in other industries. And all four of these companies have now broken their own promises. First we had Google — this big slogan, ‘Don’t be evil.’ Then they dropped that. Then they dropped another longer commitment that basically said they promised not to do harm with AI. They dropped that so they could sell AI for surveillance and weapons. OpenAI just dropped the word safety from their mission statement. xAI shut down their whole safety team. And now Anthropic, earlier in the week, dropped their most important safety commitment — the promise not to release powerful AI systems until they were sure they weren’t going to cause harm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;How did companies that made such prominent safety commitments end up in this position?&lt;/strong&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;All of these companies, especially OpenAI and Google DeepMind but to some extent also Anthropic, have persistently lobbied against regulation of AI, saying, ‘Just trust us, we’re going to regulate ourselves.’ And they’ve successfully lobbied. So we right now have less regulation on AI systems in America than on sandwiches. You know, if you want to open a sandwich shop and the health inspector finds 15 rats in the kitchen, he won’t let you sell any sandwiches until you fix it. But if you say, ‘Don’t worry, I’m not going to sell sandwiches, I’m going to sell AI girlfriends for 11-year-olds, and they’ve been linked to suicides in the past, and then I’m going to release something called superintelligence which might overthrow the U.S. government, but I have a good feeling about mine’ — the inspector has to say, ‘Fine, go ahead, just don’t sell sandwiches.’&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;There’s food safety regulation and no AI regulation.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;And this, I feel, all of these companies really share the blame for. Because if they had taken all these promises that they made back in the day for how they were going to be so safe and goody-goody, and gotten together, and then gone to the government and said, ‘Please take our voluntary commitments and turn them into U.S. law that binds even our most sloppy competitors’ — this would have happened instead. We’re in a complete regulatory vacuum. And we know what happens when there’s a complete corporate amnesty: you get thalidomide, you get tobacco companies pushing cigarettes on kids, you get asbestos causing lung cancer. So it’s sort of ironic that their own resistance to having laws saying what’s okay and not okay to do with AI is now coming back and biting them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is no law right now against building AI to kill Americans, so the government can just suddenly ask for it. If the companies themselves had earlier come out and said, ‘We want this law,’ they wouldn’t be in this pickle. They really shot themselves in the foot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;The companies’ counter-argument is always the race with China — if American companies don’t do this, Beijing will. Does that argument hold?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Let’s analyze that. The most common talking point from the lobbyists for the AI companies — they’re now better funded and more numerous than the lobbyists from the fossil fuel industry, the pharma industry and the military-industrial complex combined — is that whenever anyone proposes any kind of regulation, they say, ‘But China.’ So let’s look at that. China is in the process of banning AI girlfriends outright. Not just age limits — they’re looking at banning all anthropomorphic AI. Why? Not because they want to please America but because they feel this is screwing up Chinese youth and making China weak. Obviously, it’s making American youth weak, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And when people say we have to race to build superintelligence so we can win against China — when we don’t actually know how to control superintelligence, so that the default outcome is that humanity loses control of Earth to alien machines — guess what? The Chinese Communist Party really likes control. Who in their right mind thinks that Xi Jinping is going to tolerate some Chinese AI company building something that overthrows the Chinese government? No way. It’s clearly really bad for the American government too if it gets overthrown in a coup by the first American company to build superintelligence. This is a national security threat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;That’s compelling framing — superintelligence as a national security threat, not an asset. Do you see that view gaining traction in Washington? &lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think if people in the national security community listen to Dario Amodei describe his vision — he’s given a famous speech where he says we’ll soon have a country of geniuses in a data center — they might start thinking: wait, did Dario just use the word ‘country’? Maybe I should put that country of geniuses in a data center on the same threat list I’m keeping tabs on, because that sounds threatening to the U.S. government. And I think fairly soon, enough people in the U.S. national security community are going to realize that uncontrollable superintelligence is a threat, not a tool. This is totally analogous to the Cold War. There was a race for dominance — economic and military — against the Soviet Union. We Americans won that one without ever engaging in the second race, which was to see who could put the most nuclear craters in the other superpower. People realized that was just suicide. No one wins. The same logic applies here.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;What does all of this mean for the pace of AI development more broadly? How close do you think we are to the systems you’re describing?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Six years ago, almost every expert in AI I knew predicted we were decades away from having AI that could master language and knowledge at human level — maybe 2040, maybe 2050. They were all wrong, because we already have that now. We’ve seen AI progress quite rapidly from high school level to college level to PhD level to university professor level in some areas. Last year, AI won the gold medal at the International Mathematics Olympiad, which is about as difficult as human tasks get. I wrote a paper together with Yoshua Bengio, Dan Hendrycks, and other top AI researchers just a few months ago giving a rigorous definition of AGI. According to this, GPT-4 was 27% of the way there. GPT-5 was 57% of the way there. So we’re not there yet, but going from 27% to 57% that quickly suggests it might not be that long.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When I lectured to my students yesterday at MIT, I told them that even if it takes four years, that means when they graduate, they might not be able to get any jobs anymore. It’s certainly not too soon to start preparing for it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Anthropic is now blacklisted. I’m curious to see what happens next — will the other AI giants stand with them and say, we won’t do this either? Or does someone like xAI raise their hand and say, Anthropic didn’t want that contract, we’ll take it?&lt;/strong&gt; &lt;strong&gt;[Editor’s note: Hours after the interview, OpenAI announced its own deal with the Pentagon.]&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last night, Sam Altman came out and said he stands with Anthropic and has the same red lines. I admire him for the courage of saying that. Google, as of when we started this interview, had said nothing. If they just stay quiet, I think that’s incredibly embarrassing for them as a company, and a lot of their staff will feel the same. We haven’t heard anything from xAI yet either. So it’ll be interesting to see. Basically, there’s this moment where everybody has to show their true colors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Is there a version of this where the outcome is actually good?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yes, and this is why I’m actually optimistic in a strange way. There’s such an obvious alternative here. If we just start treating AI companies like any other companies — drop the corporate amnesty — they would clearly have to do something like a clinical trial before they released something this powerful, and demonstrate to independent experts that they know how to control it. Then we get a golden age with all the good stuff from AI, without the existential angst. That’s not the path we’re on right now. But it could be.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/28/the-trap-anthropic-built-for-itself/</guid><pubDate>Sun, 01 Mar 2026 00:08:58 +0000</pubDate></item></channel></rss>