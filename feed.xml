<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 18 Jun 2025 06:32:27 +0000</lastBuildDate><item><title>Toy-maker Mattel accused of planning “reckless” AI social experiment on kids (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/06/mattel-sparks-fear-that-planned-chatgpt-fueled-toys-will-warp-kids/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Mattel faces pressure to be more transparent about OpenAI partnership.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Barbie invading your childhood" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ai-barbie-hotwheels-640x360.jpg" width="640" /&gt;
                  &lt;img alt="AI Barbie invading your childhood" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ai-barbie-hotwheels-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;After Mattel and OpenAI announced a partnership that would result in an AI product marketed to kids, a consumer rights advocacy group is warning that the collaboration may endanger children.&lt;/p&gt;
&lt;p&gt;It remains unclear what shape Mattel's first AI product will take. However, on Tuesday, Public Citizen co-President Robert Weissman issued a statement urging more transparency so that parents can prepare for potential risks. Weissman is particularly concerned that ChatGPT-fueled toys could hurt kids in unknown ways.&lt;/p&gt;
&lt;p&gt;"Endowing toys with human-seeming voices that are able to engage in human-like conversations risks inflicting real damage on children," Weissman said. "It may undermine social development, interfere with children’s ability to form peer relationships, pull children away from playtime with peers, and possibly inflict long-term harm."&lt;/p&gt;
&lt;p&gt;One anonymous source told Axios that Mattel's plans for the AI partnership are still in "early stages," so perhaps more will be revealed as Mattel gears up for its first launch. That source suggested that the first product would not be marketed to kids under 13, which some think suggests that Mattel may recognize that exposing younger kids to AI is possibly a step too far at this stage. But more likely, it's due to OpenAI age restrictions on its API, prohibiting use under 13.&lt;/p&gt;
&lt;p&gt;Parents shouldn't be blindsided by new products, Weissman suggested, and some red lines should be drawn before any toy hits the shelves. Perhaps most urgently, "Mattel should announce immediately that it will not incorporate AI technology into children’s toys," Weissman said. "Children do not have the cognitive capacity to distinguish fully between reality and play."&lt;/p&gt;
&lt;p&gt;"Mattel should not leverage its trust with parents to conduct a reckless social experiment on our children by selling toys that incorporate AI," Weissman said.&lt;/p&gt;
&lt;p&gt;OpenAI declined to comment. Mattel did not immediately respond to Ars' request for comment.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI and Mattel defend partnership&lt;/h2&gt;
&lt;p&gt;In Mattel's press release, the toy maker behind brands like Barbie and Hot Wheels remained vague, saying only that the OpenAI deal would "support AI-powered products and experiences based on Mattel’s brands." The company's chief franchise officer, Josh Silverman, said the collaboration would enable Mattel to "reimagine new forms of play," teasing that the first release would be announced by the end of this year. Axios' source suggested it likely wouldn't be sold until 2026.&lt;/p&gt;
&lt;p&gt;OpenAI's statement also glossed over the details, promising "to bring a new dimension of AI-powered innovation and magic to Mattel’s iconic brands."&lt;/p&gt;
&lt;p&gt;Both companies emphasized that safety, privacy, and age-appropriateness would be front of mind in designing Mattel's AI products. OpenAI further claimed that kids would only be exposed to positive experiences through the collaboration, due to Mattel's experience creating kid-friendly products.&lt;/p&gt;
&lt;p&gt;"By tapping into OpenAI’s AI capabilities, Mattel aims to reimagine how fans can experience and interact with its cherished brands, with careful consideration to ensure positive, enriching experiences," OpenAI said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Critics fear Mattel is moving too fast&lt;/h2&gt;
&lt;p&gt;Critics on LinkedIn have noted that while the partnership could have positive impacts on kids—like enhancing learning or inclusivity—AI toys also carry a wide variety of potential risks that families should carefully weigh before buying into any new hyped product.&lt;/p&gt;
&lt;p&gt;In a detailed post, one tech executive, Varundeep Kaur, warned that parents should be thinking about privacy since AI toys may process their kids' "voice data, behavioral patterns, and personal preferences." He suggested Mattel may have set its first AI product's age limit at 13 to avoid running afoul of laws that are stricter when it comes to kids' data. OpenAI has said the collaboration will comply with all safety and privacy regulations.&lt;/p&gt;
&lt;p&gt;Parents should also keep in mind the bias behind the large language models that fuel AI tools like ChatGPT, Kaur said, which "might reproduce subtle stereotypes, biased narratives, or culturally inappropriate content, even unintentionally," that could skew kids' perspectives or social development.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Most obviously, AI models are still prone to hallucination, Kaur noted. And while Mattel's AI toys are "unlikely to cause physical harm," toys giving "inappropriate or bizarre responses" could "be confusing or even unsettling for a child," he said.&lt;/p&gt;
&lt;p&gt;For parents, the emotional ties kids make with AI toys will also need to be monitored, especially since chatbot outputs can be unpredictable. Another LinkedIn user, Adam Dodge—founder of a digital safety company preventing cyber abuse, called EndTab—pointed to a lawsuit where a grieving mom alleged her son committed suicide after interacting with hyper-realistic chatbots.&lt;/p&gt;
&lt;p&gt;Those bots encouraged self-harm and engaged her son in sexualized chats, and Dodge suggested that toy makers are similarly "wading into dangerous new waters with AI" that could possibly "communicate dangerous, sexualized, and harmful responses that put kids at risk."&lt;/p&gt;
&lt;p&gt;"This was inevitable—but wow does it make me cringe," Dodge wrote, noting that Mattel's plan to announce its first product this year seems "fast."&lt;/p&gt;
&lt;p&gt;Dodge said that right now, Mattel and OpenAI are "saying the right things" by emphasizing safety, privacy, and security, but more transparency is needed before parents can rest assured that AI toys are safe.&lt;/p&gt;
&lt;p&gt;AI is "unpredictable, sycophantic, and addictive," Dodge warned. "I don't want to be posting a year from now about how a Hot Wheels car encouraged self-harm or that children are in committed romantic relationships with their AI Barbies."&lt;/p&gt;
&lt;p&gt;Kaur agreed that it's in Mattel's best interest to give parents more information, since "public trust will be vital for widespread adoption." He recommended that the toy maker submit to independent audits and provide parental controls to reassure parents, as well as clearly outline how data is used, where it's stored, who has access to it, and what will happen if their kids' data is breached.&lt;/p&gt;
&lt;p&gt;For Mattel, a bigger legal threat forcing responsible design and appropriate content filtering may come from any unintentional copyright issues arising from using OpenAI models trained on a wide range of intellectual property. Hollywood studios recently sued one AI company for allowing users to generate images of their most popular characters and would likely be just as litigious defending against AI toys emulating their characters.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Mattel faces pressure to be more transparent about OpenAI partnership.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Barbie invading your childhood" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ai-barbie-hotwheels-640x360.jpg" width="640" /&gt;
                  &lt;img alt="AI Barbie invading your childhood" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ai-barbie-hotwheels-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;After Mattel and OpenAI announced a partnership that would result in an AI product marketed to kids, a consumer rights advocacy group is warning that the collaboration may endanger children.&lt;/p&gt;
&lt;p&gt;It remains unclear what shape Mattel's first AI product will take. However, on Tuesday, Public Citizen co-President Robert Weissman issued a statement urging more transparency so that parents can prepare for potential risks. Weissman is particularly concerned that ChatGPT-fueled toys could hurt kids in unknown ways.&lt;/p&gt;
&lt;p&gt;"Endowing toys with human-seeming voices that are able to engage in human-like conversations risks inflicting real damage on children," Weissman said. "It may undermine social development, interfere with children’s ability to form peer relationships, pull children away from playtime with peers, and possibly inflict long-term harm."&lt;/p&gt;
&lt;p&gt;One anonymous source told Axios that Mattel's plans for the AI partnership are still in "early stages," so perhaps more will be revealed as Mattel gears up for its first launch. That source suggested that the first product would not be marketed to kids under 13, which some think suggests that Mattel may recognize that exposing younger kids to AI is possibly a step too far at this stage. But more likely, it's due to OpenAI age restrictions on its API, prohibiting use under 13.&lt;/p&gt;
&lt;p&gt;Parents shouldn't be blindsided by new products, Weissman suggested, and some red lines should be drawn before any toy hits the shelves. Perhaps most urgently, "Mattel should announce immediately that it will not incorporate AI technology into children’s toys," Weissman said. "Children do not have the cognitive capacity to distinguish fully between reality and play."&lt;/p&gt;
&lt;p&gt;"Mattel should not leverage its trust with parents to conduct a reckless social experiment on our children by selling toys that incorporate AI," Weissman said.&lt;/p&gt;
&lt;p&gt;OpenAI declined to comment. Mattel did not immediately respond to Ars' request for comment.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI and Mattel defend partnership&lt;/h2&gt;
&lt;p&gt;In Mattel's press release, the toy maker behind brands like Barbie and Hot Wheels remained vague, saying only that the OpenAI deal would "support AI-powered products and experiences based on Mattel’s brands." The company's chief franchise officer, Josh Silverman, said the collaboration would enable Mattel to "reimagine new forms of play," teasing that the first release would be announced by the end of this year. Axios' source suggested it likely wouldn't be sold until 2026.&lt;/p&gt;
&lt;p&gt;OpenAI's statement also glossed over the details, promising "to bring a new dimension of AI-powered innovation and magic to Mattel’s iconic brands."&lt;/p&gt;
&lt;p&gt;Both companies emphasized that safety, privacy, and age-appropriateness would be front of mind in designing Mattel's AI products. OpenAI further claimed that kids would only be exposed to positive experiences through the collaboration, due to Mattel's experience creating kid-friendly products.&lt;/p&gt;
&lt;p&gt;"By tapping into OpenAI’s AI capabilities, Mattel aims to reimagine how fans can experience and interact with its cherished brands, with careful consideration to ensure positive, enriching experiences," OpenAI said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Critics fear Mattel is moving too fast&lt;/h2&gt;
&lt;p&gt;Critics on LinkedIn have noted that while the partnership could have positive impacts on kids—like enhancing learning or inclusivity—AI toys also carry a wide variety of potential risks that families should carefully weigh before buying into any new hyped product.&lt;/p&gt;
&lt;p&gt;In a detailed post, one tech executive, Varundeep Kaur, warned that parents should be thinking about privacy since AI toys may process their kids' "voice data, behavioral patterns, and personal preferences." He suggested Mattel may have set its first AI product's age limit at 13 to avoid running afoul of laws that are stricter when it comes to kids' data. OpenAI has said the collaboration will comply with all safety and privacy regulations.&lt;/p&gt;
&lt;p&gt;Parents should also keep in mind the bias behind the large language models that fuel AI tools like ChatGPT, Kaur said, which "might reproduce subtle stereotypes, biased narratives, or culturally inappropriate content, even unintentionally," that could skew kids' perspectives or social development.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Most obviously, AI models are still prone to hallucination, Kaur noted. And while Mattel's AI toys are "unlikely to cause physical harm," toys giving "inappropriate or bizarre responses" could "be confusing or even unsettling for a child," he said.&lt;/p&gt;
&lt;p&gt;For parents, the emotional ties kids make with AI toys will also need to be monitored, especially since chatbot outputs can be unpredictable. Another LinkedIn user, Adam Dodge—founder of a digital safety company preventing cyber abuse, called EndTab—pointed to a lawsuit where a grieving mom alleged her son committed suicide after interacting with hyper-realistic chatbots.&lt;/p&gt;
&lt;p&gt;Those bots encouraged self-harm and engaged her son in sexualized chats, and Dodge suggested that toy makers are similarly "wading into dangerous new waters with AI" that could possibly "communicate dangerous, sexualized, and harmful responses that put kids at risk."&lt;/p&gt;
&lt;p&gt;"This was inevitable—but wow does it make me cringe," Dodge wrote, noting that Mattel's plan to announce its first product this year seems "fast."&lt;/p&gt;
&lt;p&gt;Dodge said that right now, Mattel and OpenAI are "saying the right things" by emphasizing safety, privacy, and security, but more transparency is needed before parents can rest assured that AI toys are safe.&lt;/p&gt;
&lt;p&gt;AI is "unpredictable, sycophantic, and addictive," Dodge warned. "I don't want to be posting a year from now about how a Hot Wheels car encouraged self-harm or that children are in committed romantic relationships with their AI Barbies."&lt;/p&gt;
&lt;p&gt;Kaur agreed that it's in Mattel's best interest to give parents more information, since "public trust will be vital for widespread adoption." He recommended that the toy maker submit to independent audits and provide parental controls to reassure parents, as well as clearly outline how data is used, where it's stored, who has access to it, and what will happen if their kids' data is breached.&lt;/p&gt;
&lt;p&gt;For Mattel, a bigger legal threat forcing responsible design and appropriate content filtering may come from any unintentional copyright issues arising from using OpenAI models trained on a wide range of intellectual property. Hollywood studios recently sued one AI company for allowing users to generate images of their most popular characters and would likely be just as litigious defending against AI toys emulating their characters.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/06/mattel-sparks-fear-that-planned-chatgpt-fueled-toys-will-warp-kids/</guid><pubDate>Tue, 17 Jun 2025 19:22:43 +0000</pubDate></item><item><title>Meta is reportedly building AI smart glasses with Prada, too (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/meta-is-reportedly-building-ai-smart-glasses-with-prada-too/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/zuck-meta.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is working on a pair of AI smart glasses with the Italian high fashion brand, Prada, according to a report from CNBC on Tuesday. It’s unclear at this time when Meta’s Prada smart glasses will be publicly announced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The reported Prada collaboration signifies that Meta aims to bring its AI smart glasses technology to more fashion companies outside of its relationship with eyewear giant EssilorLuxottica. Until now, Meta has collaborated closely with EssilorLuxottica and its numerous brands. Prada is not owned by EssilorLuxottica, although the fashion brand has relied on the company to help build its eyewear for decades and the companies just renewed their partnership.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta has already sold millions of Ray-Ban Meta AI smart glasses. Earlier this week, the company teased a collaboration with another EssilorLuxottica brand, Oakley, as Bloomberg previously reported. CNBC reports that those Oakley smart glasses, which could be announced as soon as Friday, may cost around $360.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/zuck-meta.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is working on a pair of AI smart glasses with the Italian high fashion brand, Prada, according to a report from CNBC on Tuesday. It’s unclear at this time when Meta’s Prada smart glasses will be publicly announced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The reported Prada collaboration signifies that Meta aims to bring its AI smart glasses technology to more fashion companies outside of its relationship with eyewear giant EssilorLuxottica. Until now, Meta has collaborated closely with EssilorLuxottica and its numerous brands. Prada is not owned by EssilorLuxottica, although the fashion brand has relied on the company to help build its eyewear for decades and the companies just renewed their partnership.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta has already sold millions of Ray-Ban Meta AI smart glasses. Earlier this week, the company teased a collaboration with another EssilorLuxottica brand, Oakley, as Bloomberg previously reported. CNBC reports that those Oakley smart glasses, which could be announced as soon as Friday, may cost around $360.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/meta-is-reportedly-building-ai-smart-glasses-with-prada-too/</guid><pubDate>Tue, 17 Jun 2025 19:38:07 +0000</pubDate></item><item><title>A sounding board for strengthening the student experience (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/sounding-board-for-strengthening-student-experience-0617</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-SCC-UAG.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;During his first year at MIT in 2021, Matthew Caren ’25 received an intriguing email inviting students to apply to become members of the MIT Schwarzman College of Computing’s (SCC)&amp;nbsp;Undergraduate Advisory Group (UAG). He immediately shot off an application.&lt;/p&gt;&lt;p&gt;Caren is a jazz musician who majored in computer science and engineering, and minored in music and theater arts. He was drawn to the college because of its focus on the applied intersections between computing, engineering, the arts, and other academic pursuits.&amp;nbsp;Caron eagerly joined the UAG and stayed on it all four years at MIT.&lt;/p&gt;&lt;p&gt;First formed in April 2020, the group brings together a committee of around 25 undergraduate&amp;nbsp;students representing a broad swath of both traditional and&amp;nbsp;blended majors in electrical engineering and computer science (EECS) and other computing-related programs. They advise the college’s leadership on issues, offer constructive feedback, and serve as a sounding board for innovative new ideas.&lt;/p&gt;&lt;p&gt;“The ethos of the UAG is the ethos of the college itself,” Caren explains. “If you very intentionally bring together a bunch of smart, interesting, fun-to-be-around people who are all interested in completely diverse things, you'll get some really cool discussions and interactions out of it.”&lt;/p&gt;&lt;p&gt;Along the way, he’s also made “dear” friends and found true colleagues. In the group’s monthly meetings with SCC dean Dan Huttenlocher and Deputy Dean Asu Ozdaglar, who is also the department head of EECS, UAG members speak openly about challenges in the student experience and offer recommendations to guests from across the Institute, such as faculty who are developing new courses and looking for student input.&lt;/p&gt;&lt;p&gt;“This group is unique in the sense that it’s a direct line of communication to the college’s leadership,” says Caren. “They make time in their insanely busy schedules for us to explain where the holes are, and what students’ needs are, directly from our experiences.”&lt;/p&gt;&lt;p&gt;“The students in the group are keenly interested in computer science and AI, especially how these fields connect with other disciplines. They’re also passionate about MIT and eager to enhance the undergraduate experience. Hearing their perspective is refreshing — their honesty and feedback have been incredibly helpful to me as dean,” says Huttenlocher.&lt;/p&gt;&lt;p&gt;“Meeting with the students each month is a real pleasure. The UAG has been an invaluable space for understanding the student experience more deeply. They engage with computing in diverse ways across MIT, so their input on the curriculum and broader college issues has been insightful,” Ozdaglar says.&lt;/p&gt;&lt;p&gt;UAG program manager Ellen Rushman says that “Asu and Dan have done an amazing job cultivating a space in which students feel safe bringing up things that aren’t positive all the time.” The group’s suggestions are frequently implemented, too.&lt;/p&gt;&lt;p&gt;For example, in 2021, Skidmore, Owings &amp;amp; Merrill, the architects designing the new SCC&amp;nbsp;building, presented their renderings at a UAG meeting to request student feedback. Their original interiors layout offered very few of the hybrid study and meeting booths that are so popular in today’s first floor lobby.&lt;/p&gt;&lt;p&gt;Hearing strong UAG opinions about the sort of open-plan, community-building spaces that students really valued was one of the things that created the change to the current floor plan. “It’s super cool walking into the personalized space and seeing it constantly being in use and always crowded. I actually feel happy when I can’t get a table,” says Caren, who has just ended his tenure as co-chair of the group in preparation for graduation.&lt;/p&gt;&lt;p&gt;Caren’s co-chair, rising senior Julia Schneider, who is double-majoring in artificial intelligence and decision-making and mathematics, joined the UAG as a first-year to understand more about the college’s mission of fostering interdepartmental collaborations.&lt;/p&gt;&lt;p&gt;“Since I am a student in electrical engineering and computer science, but I conduct research in mechanical engineering on robotics, the college’s mission of fostering interdepartmental collaborations and uniting them through computing really spoke to my personal experiences in my first year at MIT,” Schneider says.&lt;/p&gt;&lt;p&gt;During her time on the UAG, members have joined subgroups focused around achieving different programmatic goals of the college, such as curating a public lecture series for the 2025-26 academic year to give MIT students exposure to faculty who conduct research in other disciplines that relate to computing.&lt;/p&gt;&lt;p&gt;At one meeting, after hearing how challenging it is for students to understand all the possible courses to take during their tenure, Schneider and some UAG peers formed a subgroup to find a solution.&lt;/p&gt;&lt;p&gt;The students agreed that some of the best courses they’ve taken at MIT, or pairings of courses that really struck a chord with their interdisciplinary interests, came because they spoke to upperclassmen and got recommendations. “This kind of tribal knowledge doesn’t really permeate to all of MIT,” Schneider explains.&lt;/p&gt;&lt;p&gt;For the last six months, Schneider and the subgroup have been working on a course visualization website,&amp;nbsp;NerdXing,&amp;nbsp;which came out of these discussions.&lt;/p&gt;&lt;p&gt;Guided by Rob Miller, distinguished professor of computer science in EECS, the subgroup used a dataset of EECS course enrollments over the past decade to develop a different type of tool than MIT students typically use, such as CourseRoad and others.&lt;/p&gt;&lt;p&gt;Miller, who regularly attends the UAG meetings in his role as the education officer for the college’s cross-cutting initiative,&amp;nbsp;Common Ground for Computing Education, comments, “the really cool idea here is to help students find paths that were taken by other people who are like them — not just interested in computer science, but maybe also in biology, or music, or economics, or neuroscience.&amp;nbsp;It's very much in the spirit of the College of Computing — applying data-driven computational methods, in support of students with wide-ranging computational interests.”&lt;/p&gt;&lt;p&gt;Opening the NerdXing pilot, which is set to roll out later this spring, Schneider gave a demo. She explains that if you are a computer science (CS) major and would like to create a visual presenting potential courses for you, after you select your major and a class of interest, you can expand a huge graph presenting all the possible courses your CS peers have taken over the past decade.&lt;/p&gt;&lt;p&gt;She clicked on class 18.404 (Theory of Computation) as the starting class of interest, which led to class 6.7900 (Machine Learning), and then unexpectedly to 21M.302 (Harmony and Counterpoint II), an advanced music class.&lt;/p&gt;&lt;p&gt;“You start to see aggregate statistics that tell you how many students took each course, and you can further pare it down to see the most popular courses in CS or follow lines of red dots between courses to see the typical sequence of classes taken.”&lt;/p&gt;&lt;p&gt;By getting granular on the graph, users begin to see classes that they have probably never heard anyone talking about in their program. “I think that one of the reasons you come to MIT is to be able to take cool stuff exactly like this,” says Schneider.&lt;/p&gt;&lt;p&gt;The tool aims to show students how they can choose classes that go far beyond just filling degree requirements. It’s just one example of how UAG is empowering students to strengthen the college and the experiences it offers them.&lt;/p&gt;&lt;p&gt;“We are MIT students. We have the skills to build solutions,” Schneider says. “This group of people not only brings up ways in which things could be better, but we take it into our own hands to fix things.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-SCC-UAG.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;During his first year at MIT in 2021, Matthew Caren ’25 received an intriguing email inviting students to apply to become members of the MIT Schwarzman College of Computing’s (SCC)&amp;nbsp;Undergraduate Advisory Group (UAG). He immediately shot off an application.&lt;/p&gt;&lt;p&gt;Caren is a jazz musician who majored in computer science and engineering, and minored in music and theater arts. He was drawn to the college because of its focus on the applied intersections between computing, engineering, the arts, and other academic pursuits.&amp;nbsp;Caron eagerly joined the UAG and stayed on it all four years at MIT.&lt;/p&gt;&lt;p&gt;First formed in April 2020, the group brings together a committee of around 25 undergraduate&amp;nbsp;students representing a broad swath of both traditional and&amp;nbsp;blended majors in electrical engineering and computer science (EECS) and other computing-related programs. They advise the college’s leadership on issues, offer constructive feedback, and serve as a sounding board for innovative new ideas.&lt;/p&gt;&lt;p&gt;“The ethos of the UAG is the ethos of the college itself,” Caren explains. “If you very intentionally bring together a bunch of smart, interesting, fun-to-be-around people who are all interested in completely diverse things, you'll get some really cool discussions and interactions out of it.”&lt;/p&gt;&lt;p&gt;Along the way, he’s also made “dear” friends and found true colleagues. In the group’s monthly meetings with SCC dean Dan Huttenlocher and Deputy Dean Asu Ozdaglar, who is also the department head of EECS, UAG members speak openly about challenges in the student experience and offer recommendations to guests from across the Institute, such as faculty who are developing new courses and looking for student input.&lt;/p&gt;&lt;p&gt;“This group is unique in the sense that it’s a direct line of communication to the college’s leadership,” says Caren. “They make time in their insanely busy schedules for us to explain where the holes are, and what students’ needs are, directly from our experiences.”&lt;/p&gt;&lt;p&gt;“The students in the group are keenly interested in computer science and AI, especially how these fields connect with other disciplines. They’re also passionate about MIT and eager to enhance the undergraduate experience. Hearing their perspective is refreshing — their honesty and feedback have been incredibly helpful to me as dean,” says Huttenlocher.&lt;/p&gt;&lt;p&gt;“Meeting with the students each month is a real pleasure. The UAG has been an invaluable space for understanding the student experience more deeply. They engage with computing in diverse ways across MIT, so their input on the curriculum and broader college issues has been insightful,” Ozdaglar says.&lt;/p&gt;&lt;p&gt;UAG program manager Ellen Rushman says that “Asu and Dan have done an amazing job cultivating a space in which students feel safe bringing up things that aren’t positive all the time.” The group’s suggestions are frequently implemented, too.&lt;/p&gt;&lt;p&gt;For example, in 2021, Skidmore, Owings &amp;amp; Merrill, the architects designing the new SCC&amp;nbsp;building, presented their renderings at a UAG meeting to request student feedback. Their original interiors layout offered very few of the hybrid study and meeting booths that are so popular in today’s first floor lobby.&lt;/p&gt;&lt;p&gt;Hearing strong UAG opinions about the sort of open-plan, community-building spaces that students really valued was one of the things that created the change to the current floor plan. “It’s super cool walking into the personalized space and seeing it constantly being in use and always crowded. I actually feel happy when I can’t get a table,” says Caren, who has just ended his tenure as co-chair of the group in preparation for graduation.&lt;/p&gt;&lt;p&gt;Caren’s co-chair, rising senior Julia Schneider, who is double-majoring in artificial intelligence and decision-making and mathematics, joined the UAG as a first-year to understand more about the college’s mission of fostering interdepartmental collaborations.&lt;/p&gt;&lt;p&gt;“Since I am a student in electrical engineering and computer science, but I conduct research in mechanical engineering on robotics, the college’s mission of fostering interdepartmental collaborations and uniting them through computing really spoke to my personal experiences in my first year at MIT,” Schneider says.&lt;/p&gt;&lt;p&gt;During her time on the UAG, members have joined subgroups focused around achieving different programmatic goals of the college, such as curating a public lecture series for the 2025-26 academic year to give MIT students exposure to faculty who conduct research in other disciplines that relate to computing.&lt;/p&gt;&lt;p&gt;At one meeting, after hearing how challenging it is for students to understand all the possible courses to take during their tenure, Schneider and some UAG peers formed a subgroup to find a solution.&lt;/p&gt;&lt;p&gt;The students agreed that some of the best courses they’ve taken at MIT, or pairings of courses that really struck a chord with their interdisciplinary interests, came because they spoke to upperclassmen and got recommendations. “This kind of tribal knowledge doesn’t really permeate to all of MIT,” Schneider explains.&lt;/p&gt;&lt;p&gt;For the last six months, Schneider and the subgroup have been working on a course visualization website,&amp;nbsp;NerdXing,&amp;nbsp;which came out of these discussions.&lt;/p&gt;&lt;p&gt;Guided by Rob Miller, distinguished professor of computer science in EECS, the subgroup used a dataset of EECS course enrollments over the past decade to develop a different type of tool than MIT students typically use, such as CourseRoad and others.&lt;/p&gt;&lt;p&gt;Miller, who regularly attends the UAG meetings in his role as the education officer for the college’s cross-cutting initiative,&amp;nbsp;Common Ground for Computing Education, comments, “the really cool idea here is to help students find paths that were taken by other people who are like them — not just interested in computer science, but maybe also in biology, or music, or economics, or neuroscience.&amp;nbsp;It's very much in the spirit of the College of Computing — applying data-driven computational methods, in support of students with wide-ranging computational interests.”&lt;/p&gt;&lt;p&gt;Opening the NerdXing pilot, which is set to roll out later this spring, Schneider gave a demo. She explains that if you are a computer science (CS) major and would like to create a visual presenting potential courses for you, after you select your major and a class of interest, you can expand a huge graph presenting all the possible courses your CS peers have taken over the past decade.&lt;/p&gt;&lt;p&gt;She clicked on class 18.404 (Theory of Computation) as the starting class of interest, which led to class 6.7900 (Machine Learning), and then unexpectedly to 21M.302 (Harmony and Counterpoint II), an advanced music class.&lt;/p&gt;&lt;p&gt;“You start to see aggregate statistics that tell you how many students took each course, and you can further pare it down to see the most popular courses in CS or follow lines of red dots between courses to see the typical sequence of classes taken.”&lt;/p&gt;&lt;p&gt;By getting granular on the graph, users begin to see classes that they have probably never heard anyone talking about in their program. “I think that one of the reasons you come to MIT is to be able to take cool stuff exactly like this,” says Schneider.&lt;/p&gt;&lt;p&gt;The tool aims to show students how they can choose classes that go far beyond just filling degree requirements. It’s just one example of how UAG is empowering students to strengthen the college and the experiences it offers them.&lt;/p&gt;&lt;p&gt;“We are MIT students. We have the skills to build solutions,” Schneider says. “This group of people not only brings up ways in which things could be better, but we take it into our own hands to fix things.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/sounding-board-for-strengthening-student-experience-0617</guid><pubDate>Tue, 17 Jun 2025 20:00:00 +0000</pubDate></item><item><title>Unpacking the bias of large language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/unpacking-large-language-model-bias-0617</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT-transform-bias-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Research has shown that large language models (LLMs) tend to overemphasize information at the beginning and end of a document or conversation, while neglecting the middle.&lt;/p&gt;&lt;p&gt;This “position bias” means that, if a lawyer is using an LLM-powered virtual assistant to retrieve a certain phrase in a 30-page affidavit, the LLM is more likely to find the right text if it is on the initial or final pages.&lt;/p&gt;&lt;p&gt;MIT researchers have discovered the mechanism behind this phenomenon.&lt;/p&gt;&lt;p&gt;They created a theoretical framework to study how information flows through the machine-learning architecture that forms the backbone of LLMs. They found that certain design choices which control how the model processes input data can cause position bias.&lt;/p&gt;&lt;p&gt;Their experiments revealed that model architectures, particularly those affecting how information is spread across input words within the model, can give rise to or intensify position bias, and that training data also contribute to the problem.&lt;/p&gt;&lt;p&gt;In addition to pinpointing the origins of position bias, their framework can be used to diagnose and correct it in future model designs.&lt;/p&gt;&lt;p&gt;This could lead to more reliable chatbots that stay on topic during long conversations, medical AI systems that reason more fairly when handling a trove of patient data, and code assistants that pay closer attention to all parts of a program.&lt;/p&gt;&lt;p&gt;“These models are black boxes, so as an LLM user, you probably don’t know that position bias can cause your model to be inconsistent. You just feed it your documents in whatever order you want and expect it to work. But by understanding the underlying mechanism of these black-box models better, we can improve them by addressing these limitations,” says Xinyi Wu, a graduate student in the MIT Institute for Data, Systems, and Society (IDSS) and the Laboratory for Information and Decision Systems (LIDS), and first author of a paper on this research.&lt;/p&gt;&lt;p&gt;Her co-authors include Yifei Wang, an MIT postdoc; and senior authors Stefanie Jegelka, an associate professor of electrical engineering and computer science (EECS) and a member of IDSS and the Computer Science and Artificial Intelligence Laboratory (CSAIL); and Ali Jadbabaie, professor and head of the Department of Civil and Environmental Engineering, a core faculty member of IDSS, and a principal investigator in LIDS. The research will be presented at the International Conference on Machine Learning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Analyzing attention&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LLMs like Claude, Llama, and GPT-4 are powered by a type of neural network architecture known as a transformer. Transformers are designed to process sequential data, encoding a sentence into chunks called tokens and then learning the relationships between tokens to predict what words comes next.&lt;/p&gt;&lt;p&gt;These models have gotten very good at this because of the attention mechanism, which uses interconnected layers of data processing nodes to make sense of context by allowing tokens to selectively focus on, or attend to, related tokens.&lt;/p&gt;&lt;p&gt;But if every token can attend to every other token in a 30-page document, that quickly becomes computationally intractable. So, when engineers build transformer models, they often employ attention masking techniques which limit the words a token can attend to.&lt;/p&gt;&lt;p&gt;For instance, a causal mask only allows words to attend to those that came before it.&lt;/p&gt;&lt;p&gt;Engineers also use positional encodings to help the model understand the location of each word in a sentence, improving performance.&lt;/p&gt;&lt;p&gt;The MIT researchers built a graph-based theoretical framework to explore how these modeling choices, attention masks and positional encodings, could affect position bias.&lt;/p&gt;&lt;p&gt;“Everything is coupled and tangled within the attention mechanism, so it is very hard to study. Graphs are a flexible language to describe the dependent relationship among words within the attention mechanism and trace them across multiple layers,” Wu says.&lt;/p&gt;&lt;p&gt;Their theoretical analysis suggested that causal masking gives the model an inherent bias toward the beginning of an input, even when that bias doesn’t exist in the data.&lt;/p&gt;&lt;p&gt;If the earlier words are relatively unimportant for a sentence’s meaning, causal masking can cause the transformer to pay more attention to its beginning anyway.&lt;/p&gt;&lt;p&gt;“While it is often true that earlier words and later words in a sentence are more important, if an LLM is used on a task that is not natural language generation, like ranking or information retrieval, these biases can be extremely harmful,” Wu says.&lt;/p&gt;&lt;p&gt;As a model grows, with additional layers of attention mechanism, this bias is amplified because earlier parts of the input are used more frequently in the model’s reasoning process.&lt;/p&gt;&lt;p&gt;They also found that using positional encodings to link words more strongly to nearby words can mitigate position bias. The technique refocuses the model’s attention in the right place, but its effect can be diluted in models with more attention layers.&lt;/p&gt;&lt;p&gt;And these design choices are only one cause of position bias — some can come from training data the model uses to learn how to prioritize words in a sequence.&lt;/p&gt;&lt;p&gt;“If you know your data are biased in a certain way, then you should also finetune your model on top of adjusting your modeling choices,” Wu says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Lost in the middle&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;After they’d established a theoretical framework, the researchers performed experiments in which they systematically varied the position of the correct answer in text sequences for an information retrieval task.&lt;/p&gt;&lt;p&gt;The experiments showed a “lost-in-the-middle” phenomenon, where retrieval accuracy followed a U-shaped pattern. Models performed best if the right answer was located at the beginning of the sequence. Performance declined the closer it got to the middle before rebounding a bit if the correct answer was near the end.&lt;/p&gt;&lt;p&gt;Ultimately, their work suggests that using a different masking technique, removing extra layers from the attention mechanism, or strategically employing positional encodings could reduce position bias and improve a model’s accuracy.&lt;/p&gt;&lt;p&gt;“By doing a combination of theory and experiments, we were able to look at the consequences of model design choices that weren’t clear at the time. If you want to use a model in high-stakes applications, you must know when it will work, when it won’t, and why,” Jadbabaie says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to further explore the effects of positional encodings and study how position bias could be strategically exploited in certain applications.&lt;/p&gt;&lt;p&gt;“These researchers offer a rare theoretical lens into the attention mechanism at the heart of the transformer model. They provide a compelling analysis that clarifies longstanding quirks in transformer behavior, showing that attention mechanisms, especially with causal masks, inherently bias models toward the beginning of sequences. The paper achieves the best of both worlds — mathematical clarity paired with insights that reach into the guts of real-world systems,” says Amin Saberi, professor and director of the Stanford University Center for Computational Market Design, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This research is supported, in part, by the U.S. Office of Naval Research, the National Science Foundation, and an Alexander von Humboldt Professorship.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT-transform-bias-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Research has shown that large language models (LLMs) tend to overemphasize information at the beginning and end of a document or conversation, while neglecting the middle.&lt;/p&gt;&lt;p&gt;This “position bias” means that, if a lawyer is using an LLM-powered virtual assistant to retrieve a certain phrase in a 30-page affidavit, the LLM is more likely to find the right text if it is on the initial or final pages.&lt;/p&gt;&lt;p&gt;MIT researchers have discovered the mechanism behind this phenomenon.&lt;/p&gt;&lt;p&gt;They created a theoretical framework to study how information flows through the machine-learning architecture that forms the backbone of LLMs. They found that certain design choices which control how the model processes input data can cause position bias.&lt;/p&gt;&lt;p&gt;Their experiments revealed that model architectures, particularly those affecting how information is spread across input words within the model, can give rise to or intensify position bias, and that training data also contribute to the problem.&lt;/p&gt;&lt;p&gt;In addition to pinpointing the origins of position bias, their framework can be used to diagnose and correct it in future model designs.&lt;/p&gt;&lt;p&gt;This could lead to more reliable chatbots that stay on topic during long conversations, medical AI systems that reason more fairly when handling a trove of patient data, and code assistants that pay closer attention to all parts of a program.&lt;/p&gt;&lt;p&gt;“These models are black boxes, so as an LLM user, you probably don’t know that position bias can cause your model to be inconsistent. You just feed it your documents in whatever order you want and expect it to work. But by understanding the underlying mechanism of these black-box models better, we can improve them by addressing these limitations,” says Xinyi Wu, a graduate student in the MIT Institute for Data, Systems, and Society (IDSS) and the Laboratory for Information and Decision Systems (LIDS), and first author of a paper on this research.&lt;/p&gt;&lt;p&gt;Her co-authors include Yifei Wang, an MIT postdoc; and senior authors Stefanie Jegelka, an associate professor of electrical engineering and computer science (EECS) and a member of IDSS and the Computer Science and Artificial Intelligence Laboratory (CSAIL); and Ali Jadbabaie, professor and head of the Department of Civil and Environmental Engineering, a core faculty member of IDSS, and a principal investigator in LIDS. The research will be presented at the International Conference on Machine Learning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Analyzing attention&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LLMs like Claude, Llama, and GPT-4 are powered by a type of neural network architecture known as a transformer. Transformers are designed to process sequential data, encoding a sentence into chunks called tokens and then learning the relationships between tokens to predict what words comes next.&lt;/p&gt;&lt;p&gt;These models have gotten very good at this because of the attention mechanism, which uses interconnected layers of data processing nodes to make sense of context by allowing tokens to selectively focus on, or attend to, related tokens.&lt;/p&gt;&lt;p&gt;But if every token can attend to every other token in a 30-page document, that quickly becomes computationally intractable. So, when engineers build transformer models, they often employ attention masking techniques which limit the words a token can attend to.&lt;/p&gt;&lt;p&gt;For instance, a causal mask only allows words to attend to those that came before it.&lt;/p&gt;&lt;p&gt;Engineers also use positional encodings to help the model understand the location of each word in a sentence, improving performance.&lt;/p&gt;&lt;p&gt;The MIT researchers built a graph-based theoretical framework to explore how these modeling choices, attention masks and positional encodings, could affect position bias.&lt;/p&gt;&lt;p&gt;“Everything is coupled and tangled within the attention mechanism, so it is very hard to study. Graphs are a flexible language to describe the dependent relationship among words within the attention mechanism and trace them across multiple layers,” Wu says.&lt;/p&gt;&lt;p&gt;Their theoretical analysis suggested that causal masking gives the model an inherent bias toward the beginning of an input, even when that bias doesn’t exist in the data.&lt;/p&gt;&lt;p&gt;If the earlier words are relatively unimportant for a sentence’s meaning, causal masking can cause the transformer to pay more attention to its beginning anyway.&lt;/p&gt;&lt;p&gt;“While it is often true that earlier words and later words in a sentence are more important, if an LLM is used on a task that is not natural language generation, like ranking or information retrieval, these biases can be extremely harmful,” Wu says.&lt;/p&gt;&lt;p&gt;As a model grows, with additional layers of attention mechanism, this bias is amplified because earlier parts of the input are used more frequently in the model’s reasoning process.&lt;/p&gt;&lt;p&gt;They also found that using positional encodings to link words more strongly to nearby words can mitigate position bias. The technique refocuses the model’s attention in the right place, but its effect can be diluted in models with more attention layers.&lt;/p&gt;&lt;p&gt;And these design choices are only one cause of position bias — some can come from training data the model uses to learn how to prioritize words in a sequence.&lt;/p&gt;&lt;p&gt;“If you know your data are biased in a certain way, then you should also finetune your model on top of adjusting your modeling choices,” Wu says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Lost in the middle&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;After they’d established a theoretical framework, the researchers performed experiments in which they systematically varied the position of the correct answer in text sequences for an information retrieval task.&lt;/p&gt;&lt;p&gt;The experiments showed a “lost-in-the-middle” phenomenon, where retrieval accuracy followed a U-shaped pattern. Models performed best if the right answer was located at the beginning of the sequence. Performance declined the closer it got to the middle before rebounding a bit if the correct answer was near the end.&lt;/p&gt;&lt;p&gt;Ultimately, their work suggests that using a different masking technique, removing extra layers from the attention mechanism, or strategically employing positional encodings could reduce position bias and improve a model’s accuracy.&lt;/p&gt;&lt;p&gt;“By doing a combination of theory and experiments, we were able to look at the consequences of model design choices that weren’t clear at the time. If you want to use a model in high-stakes applications, you must know when it will work, when it won’t, and why,” Jadbabaie says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to further explore the effects of positional encodings and study how position bias could be strategically exploited in certain applications.&lt;/p&gt;&lt;p&gt;“These researchers offer a rare theoretical lens into the attention mechanism at the heart of the transformer model. They provide a compelling analysis that clarifies longstanding quirks in transformer behavior, showing that attention mechanisms, especially with causal masks, inherently bias models toward the beginning of sequences. The paper achieves the best of both worlds — mathematical clarity paired with insights that reach into the guts of real-world systems,” says Amin Saberi, professor and director of the Stanford University Center for Computational Market Design, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This research is supported, in part, by the U.S. Office of Naval Research, the National Science Foundation, and an Alexander von Humboldt Professorship.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/unpacking-large-language-model-bias-0617</guid><pubDate>Tue, 17 Jun 2025 20:00:00 +0000</pubDate></item><item><title>Amazon expects to reduce corporate jobs due to AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/amazon-expects-to-reduce-corporate-jobs-due-to-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2201505679.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon CEO Andy Jassy is betting that generative AI will change how the company thinks about its workforce in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy said that as the company continues to roll out more AI agents, and thus change how the company’s work is done, he expects Amazon will reduce the number of corporate jobs needed in the future, according to a memo that was first covered by CNBC.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs,” Jassy wrote in the memo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the size of this future reduction of workforce is hard to estimate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A recent survey from the World Economic Forum found that potential reductions in workforce due to AI may already be happening. The survey found that 40% of employers plan to cut staff that are doing roles that can be automated by AI.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2201505679.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon CEO Andy Jassy is betting that generative AI will change how the company thinks about its workforce in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy said that as the company continues to roll out more AI agents, and thus change how the company’s work is done, he expects Amazon will reduce the number of corporate jobs needed in the future, according to a memo that was first covered by CNBC.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs,” Jassy wrote in the memo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the size of this future reduction of workforce is hard to estimate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A recent survey from the World Economic Forum found that potential reductions in workforce due to AI may already be happening. The survey found that 40% of employers plan to cut staff that are doing roles that can be automated by AI.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/amazon-expects-to-reduce-corporate-jobs-due-to-ai/</guid><pubDate>Tue, 17 Jun 2025 20:21:23 +0000</pubDate></item><item><title>Combining technology, education, and human connection to improve online learning (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/caitlin-morris-combines-tech-education-human-connection-improve-online-learning-0617</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/mit-Caitlin-Morris.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT Morningside Academy for Design (MAD) Fellow&amp;nbsp;Caitlin Morris is an architect, artist,&amp;nbsp;researcher, and educator who has studied psychology and used online learning tools to teach herself coding and other skills. She’s a soft-spoken observer, with a keen interest in how people use space and respond to their environments.&amp;nbsp;Combining her observational skills with active community engagement, she works at the intersection of technology, education, and human connection to improve digital learning platforms.&lt;/p&gt;&lt;p&gt;Morris grew up in rural upstate New York in a family of makers. She learned to sew, cook, and build things with wood at a young age. One of her earlier memories is of a small handsaw she made — with the help of her father, a professional carpenter. It had wooden handles on both sides to make sawing easier for her.&lt;/p&gt;&lt;p&gt;Later, when she needed to learn something, she’d turn to project-based communities, rather than books. She taught herself to code late at night, taking advantage of community-oriented platforms where people answer questions and post sketches, allowing her to see the code behind the objects people made.&lt;/p&gt;&lt;p&gt;“For me, that was this huge, wake-up moment of feeling like there was a path to expression that was not a traditional computer-science classroom,” she says. “I think that’s partly why I feel so passionate about what I’m doing now. That was the big transformation: having that community available in this really personal, project-based way.”&lt;/p&gt;&lt;p&gt;Subsequently, Morris has become involved in community-based learning in diverse ways: She’s a co-organizer of the MIT Media Lab’s Festival of Learning; she leads creative coding community meetups; and she’s been active in the open-source software community development.&lt;/p&gt;&lt;p&gt;“My years of organizing learning and making communities — both in person and online — have shown me firsthand how powerful social interaction can be for motivation and curiosity,” Morris said. “My research is really about identifying which elements of that social magic are most essential, so we can design digital environments that better support those dynamics.”&lt;/p&gt;&lt;p&gt;Even in her artwork, Morris sometimes works with a collective. She’s contributed to the creation of about 10 large art installations that combine movement, sound, imagery, lighting, and other technologies to immerse the visitor in an experience evoking some aspect of nature, such as flowing water, birds in flight, or crowd kinetics. These marvelous installations are commanding and calming at the same time, possibly because they focus the mind, eye, and sometimes the ear.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            MIT graduate student and MAD Fellow Caitlin Morris contributed concept design, design development, electrical design and engineering, firmware development, and fabrication to “Diffusion Choir,” an installation from the artist collaborative Hypersonic, as well as Sosolimited and Plebian Design.&lt;br /&gt;Video: Hypersonic        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;She did much of this work with New York-based Hypersonic, a company of artists and technologists specializing in large kinetic installations in public spaces. Before that, she earned a BS in psychology and a BS in architectural building sciences from Rensselaer Polytechnic Institute, then an MFA in design and technology from the Parsons School of Design at The New School.&lt;/p&gt;&lt;p&gt;During, in between, after, and sometimes concurrently, she taught design, coding, and other technologies at the high school, undergraduate, and graduate-student levels.&lt;/p&gt;&lt;p&gt;“I think what kind of got me hooked on teaching was that the way I learned as a child was not the same as in the classroom,” Morris explains. “And I later saw this in many of my students. I got the feeling that the normal way of learning things was not working for them. And they thought it was their fault. They just didn’t really feel welcome within the traditional education model.”&lt;/p&gt;&lt;p&gt;Morris says that when she worked with those students, tossing aside tradition and instead saying — “You know, we’re just going to do this animation. Or we’re going to make this design or this website or these graphics, and we’re going to approach it in this totally different way” — she saw people “kind of unlock and be like, ‘Oh my gosh. I never thought I could do that.’&lt;/p&gt;&lt;p&gt;“For me, that was the hook, that’s the magic of it. Because I was coming from that experience of having to figure out those unlock mechanisms for myself, it was really exciting to be able to share them with other people, those unlock moments.”&lt;/p&gt;&lt;p&gt;For her doctoral work with the MIT Media Lab’s Fluid Interfaces Group, she’s focusing on the personal space and emotional gaps associated with learning, particularly online and AI-assisted learning. This research builds on her experience increasing human connection in both physical and virtual learning environments.&lt;/p&gt;&lt;p&gt;“I’m developing a framework that combines AI-driven behavioral analysis with human expert assessment to study social learning dynamics,” she says. “My research investigates how social interaction patterns influence curiosity development and intrinsic motivation in learning, with particular focus on understanding how these dynamics differ between real peers and AI-supported environments.”&lt;/p&gt;&lt;p&gt;The first step in her research is determining which elements of social interaction are not replaceable by an AI-based digital tutor. Following that assessment, her goal is to build a prototype platform for experiential learning.&lt;/p&gt;&lt;p&gt;“I’m creating tools that can simultaneously track observable behaviors — like physical actions, language cues, and interaction patterns — while capturing learners’ subjective experiences through reflection and interviews,” Morris explains. “This approach helps connect what people do with how they feel about their learning experience.&lt;/p&gt;&lt;p&gt;“I aim to make two primary contributions: first, analysis tools for studying social learning dynamics; and second, prototype tools that demonstrate practical approaches for supporting social curiosity in digital learning environments. These contributions could help bridge the gap between the efficiency of digital platforms and the rich social interaction that occurs in effective in-person learning.”&lt;/p&gt;&lt;p&gt;Her goals make Morris a perfect fit for the MIT MAD Fellowship. One statement in MAD’s mission is: “Breaking away from traditional education, we foster creativity, critical thinking, making, and collaboration, exploring a range of dynamic approaches to prepare students for complex, real-world challenges.”&lt;/p&gt;&lt;p&gt;Morris wants to help community organizations deal with the rapid AI-powered changes in education, once she finishes her doctorate in 2026. “What should we do with this ‘physical space versus virtual space’ divide?” she asks. That is the space currently captivating Morris’s thoughts.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/mit-Caitlin-Morris.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT Morningside Academy for Design (MAD) Fellow&amp;nbsp;Caitlin Morris is an architect, artist,&amp;nbsp;researcher, and educator who has studied psychology and used online learning tools to teach herself coding and other skills. She’s a soft-spoken observer, with a keen interest in how people use space and respond to their environments.&amp;nbsp;Combining her observational skills with active community engagement, she works at the intersection of technology, education, and human connection to improve digital learning platforms.&lt;/p&gt;&lt;p&gt;Morris grew up in rural upstate New York in a family of makers. She learned to sew, cook, and build things with wood at a young age. One of her earlier memories is of a small handsaw she made — with the help of her father, a professional carpenter. It had wooden handles on both sides to make sawing easier for her.&lt;/p&gt;&lt;p&gt;Later, when she needed to learn something, she’d turn to project-based communities, rather than books. She taught herself to code late at night, taking advantage of community-oriented platforms where people answer questions and post sketches, allowing her to see the code behind the objects people made.&lt;/p&gt;&lt;p&gt;“For me, that was this huge, wake-up moment of feeling like there was a path to expression that was not a traditional computer-science classroom,” she says. “I think that’s partly why I feel so passionate about what I’m doing now. That was the big transformation: having that community available in this really personal, project-based way.”&lt;/p&gt;&lt;p&gt;Subsequently, Morris has become involved in community-based learning in diverse ways: She’s a co-organizer of the MIT Media Lab’s Festival of Learning; she leads creative coding community meetups; and she’s been active in the open-source software community development.&lt;/p&gt;&lt;p&gt;“My years of organizing learning and making communities — both in person and online — have shown me firsthand how powerful social interaction can be for motivation and curiosity,” Morris said. “My research is really about identifying which elements of that social magic are most essential, so we can design digital environments that better support those dynamics.”&lt;/p&gt;&lt;p&gt;Even in her artwork, Morris sometimes works with a collective. She’s contributed to the creation of about 10 large art installations that combine movement, sound, imagery, lighting, and other technologies to immerse the visitor in an experience evoking some aspect of nature, such as flowing water, birds in flight, or crowd kinetics. These marvelous installations are commanding and calming at the same time, possibly because they focus the mind, eye, and sometimes the ear.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            MIT graduate student and MAD Fellow Caitlin Morris contributed concept design, design development, electrical design and engineering, firmware development, and fabrication to “Diffusion Choir,” an installation from the artist collaborative Hypersonic, as well as Sosolimited and Plebian Design.&lt;br /&gt;Video: Hypersonic        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;She did much of this work with New York-based Hypersonic, a company of artists and technologists specializing in large kinetic installations in public spaces. Before that, she earned a BS in psychology and a BS in architectural building sciences from Rensselaer Polytechnic Institute, then an MFA in design and technology from the Parsons School of Design at The New School.&lt;/p&gt;&lt;p&gt;During, in between, after, and sometimes concurrently, she taught design, coding, and other technologies at the high school, undergraduate, and graduate-student levels.&lt;/p&gt;&lt;p&gt;“I think what kind of got me hooked on teaching was that the way I learned as a child was not the same as in the classroom,” Morris explains. “And I later saw this in many of my students. I got the feeling that the normal way of learning things was not working for them. And they thought it was their fault. They just didn’t really feel welcome within the traditional education model.”&lt;/p&gt;&lt;p&gt;Morris says that when she worked with those students, tossing aside tradition and instead saying — “You know, we’re just going to do this animation. Or we’re going to make this design or this website or these graphics, and we’re going to approach it in this totally different way” — she saw people “kind of unlock and be like, ‘Oh my gosh. I never thought I could do that.’&lt;/p&gt;&lt;p&gt;“For me, that was the hook, that’s the magic of it. Because I was coming from that experience of having to figure out those unlock mechanisms for myself, it was really exciting to be able to share them with other people, those unlock moments.”&lt;/p&gt;&lt;p&gt;For her doctoral work with the MIT Media Lab’s Fluid Interfaces Group, she’s focusing on the personal space and emotional gaps associated with learning, particularly online and AI-assisted learning. This research builds on her experience increasing human connection in both physical and virtual learning environments.&lt;/p&gt;&lt;p&gt;“I’m developing a framework that combines AI-driven behavioral analysis with human expert assessment to study social learning dynamics,” she says. “My research investigates how social interaction patterns influence curiosity development and intrinsic motivation in learning, with particular focus on understanding how these dynamics differ between real peers and AI-supported environments.”&lt;/p&gt;&lt;p&gt;The first step in her research is determining which elements of social interaction are not replaceable by an AI-based digital tutor. Following that assessment, her goal is to build a prototype platform for experiential learning.&lt;/p&gt;&lt;p&gt;“I’m creating tools that can simultaneously track observable behaviors — like physical actions, language cues, and interaction patterns — while capturing learners’ subjective experiences through reflection and interviews,” Morris explains. “This approach helps connect what people do with how they feel about their learning experience.&lt;/p&gt;&lt;p&gt;“I aim to make two primary contributions: first, analysis tools for studying social learning dynamics; and second, prototype tools that demonstrate practical approaches for supporting social curiosity in digital learning environments. These contributions could help bridge the gap between the efficiency of digital platforms and the rich social interaction that occurs in effective in-person learning.”&lt;/p&gt;&lt;p&gt;Her goals make Morris a perfect fit for the MIT MAD Fellowship. One statement in MAD’s mission is: “Breaking away from traditional education, we foster creativity, critical thinking, making, and collaboration, exploring a range of dynamic approaches to prepare students for complex, real-world challenges.”&lt;/p&gt;&lt;p&gt;Morris wants to help community organizations deal with the rapid AI-powered changes in education, once she finishes her doctorate in 2026. “What should we do with this ‘physical space versus virtual space’ divide?” she asks. That is the space currently captivating Morris’s thoughts.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/caitlin-morris-combines-tech-education-human-connection-improve-online-learning-0617</guid><pubDate>Tue, 17 Jun 2025 20:25:00 +0000</pubDate></item><item><title>Google’s Gemini panicked when playing Pokémon (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/googles-gemini-panicked-when-playing-pokemon/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI companies are battling to dominate the industry, but sometimes they’re also battling in Pokémon gyms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Google and Anthropic both study how their latest AI models navigate early Pokémon games, the results can be as amusing as they are enlightening — and this time, Google DeepMind has written in a report that Gemini 2.5 Pro resorts to panic when its Pokémon are close to death. This can cause the AI’s performance to experience “qualitatively observable degradation in the model’s reasoning capability,” according to the report.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI benchmarking — or, the process of comparing the performance of different AI models — is a dubious art that often provides little context for the actual capabilities of a given model. But some researchers think that studying how AI models play video games could be useful (or, at the very least, kind of funny). &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Over the last several months, two developers unaffiliated with Google and Anthropic have set up respective Twitch streams called “Gemini Plays Pokémon” and “Claude Plays Pokémon,” where anyone can watch in real time as an AI tries to navigate a children’s video game from over 25 years ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each stream displays the AI’s “reasoning” process — or, a natural language translation of how the AI evaluates a problem and arrives at a response — giving us insight into the way that these models work. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3019676" height="539" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-17-at-3.43.39PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While the progress of these AI models is impressive, they are still not very good at playing Pokémon. It takes hundreds of hours for Gemini to reason through a game that a child could complete in exponentially less time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s interesting about watching an AI navigate a Pokémon game is not so much about its time of completion, but rather how it behaves along the way.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“Over the course of the playthrough, Gemini 2.5 Pro gets into various situations which cause the model to simulate ‘panic,’” the report says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This state of “panic” can result in the model’s performance getting worse, as the AI may suddenly stop using certain tools at its disposal for a stretch of gameplay. While AI does not think or experience emotion, its actions mimic the way in which a human might make poor, hasty decisions when under stress — a fascinating, yet unsettling response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This behavior has occurred in enough separate instances that the members of the Twitch chat have actively noticed when it is occurring,” the report says.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Claude has also exhibited some curious behaviors in its journeys across Kanto. In one instance, the AI picked up on the pattern that when all of its Pokémon run out of health, the player character will “white out” and return to a Pokémon Center.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Claude got stuck in the Mt. Moon cave, it erroneously hypothesized that if it intentionally got all of its Pokémon to faint, then it would be transported across the cave to the Pokémon Center in the next town.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, that isn’t how the game works. When all of your Pokémon die, you return to whatever Pokémon Center you used most recently, rather than the nearest geographically. Viewers watched on in horror as the AI essentially tried to kill itself in the game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite its shortcomings, there are a few ways in which the AI can outperform human players. As of the release of Gemini 2.5 Pro, the AI is able to solve puzzles with impressive accuracy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With some human assistance, the AI created agentic tools — prompted instances of Gemini 2.5 Pro geared toward specific tasks — to solve the game’s boulder puzzles and find efficient routes to reach a destination.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With only a prompt describing boulder physics and a description of how to verify a valid path, Gemini 2.5 Pro is able to one-shot some of these complex boulder puzzles, which are required to progress through Victory Road,” the report says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Gemini 2.5 Pro did a lot of the work in creating these tools on its own, Google theorizes that the current model may be capable of creating these tools without human intervention. Who knows, maybe Gemini will therapize itself into creating a “don’t panic” module.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI companies are battling to dominate the industry, but sometimes they’re also battling in Pokémon gyms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Google and Anthropic both study how their latest AI models navigate early Pokémon games, the results can be as amusing as they are enlightening — and this time, Google DeepMind has written in a report that Gemini 2.5 Pro resorts to panic when its Pokémon are close to death. This can cause the AI’s performance to experience “qualitatively observable degradation in the model’s reasoning capability,” according to the report.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI benchmarking — or, the process of comparing the performance of different AI models — is a dubious art that often provides little context for the actual capabilities of a given model. But some researchers think that studying how AI models play video games could be useful (or, at the very least, kind of funny). &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Over the last several months, two developers unaffiliated with Google and Anthropic have set up respective Twitch streams called “Gemini Plays Pokémon” and “Claude Plays Pokémon,” where anyone can watch in real time as an AI tries to navigate a children’s video game from over 25 years ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each stream displays the AI’s “reasoning” process — or, a natural language translation of how the AI evaluates a problem and arrives at a response — giving us insight into the way that these models work. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3019676" height="539" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-17-at-3.43.39PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While the progress of these AI models is impressive, they are still not very good at playing Pokémon. It takes hundreds of hours for Gemini to reason through a game that a child could complete in exponentially less time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s interesting about watching an AI navigate a Pokémon game is not so much about its time of completion, but rather how it behaves along the way.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“Over the course of the playthrough, Gemini 2.5 Pro gets into various situations which cause the model to simulate ‘panic,’” the report says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This state of “panic” can result in the model’s performance getting worse, as the AI may suddenly stop using certain tools at its disposal for a stretch of gameplay. While AI does not think or experience emotion, its actions mimic the way in which a human might make poor, hasty decisions when under stress — a fascinating, yet unsettling response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This behavior has occurred in enough separate instances that the members of the Twitch chat have actively noticed when it is occurring,” the report says.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Claude has also exhibited some curious behaviors in its journeys across Kanto. In one instance, the AI picked up on the pattern that when all of its Pokémon run out of health, the player character will “white out” and return to a Pokémon Center.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Claude got stuck in the Mt. Moon cave, it erroneously hypothesized that if it intentionally got all of its Pokémon to faint, then it would be transported across the cave to the Pokémon Center in the next town.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, that isn’t how the game works. When all of your Pokémon die, you return to whatever Pokémon Center you used most recently, rather than the nearest geographically. Viewers watched on in horror as the AI essentially tried to kill itself in the game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite its shortcomings, there are a few ways in which the AI can outperform human players. As of the release of Gemini 2.5 Pro, the AI is able to solve puzzles with impressive accuracy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With some human assistance, the AI created agentic tools — prompted instances of Gemini 2.5 Pro geared toward specific tasks — to solve the game’s boulder puzzles and find efficient routes to reach a destination.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With only a prompt describing boulder physics and a description of how to verify a valid path, Gemini 2.5 Pro is able to one-shot some of these complex boulder puzzles, which are required to progress through Victory Road,” the report says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Gemini 2.5 Pro did a lot of the work in creating these tools on its own, Google theorizes that the current model may be capable of creating these tools without human intervention. Who knows, maybe Gemini will therapize itself into creating a “don’t panic” module.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/googles-gemini-panicked-when-playing-pokemon/</guid><pubDate>Tue, 17 Jun 2025 20:53:19 +0000</pubDate></item><item><title>OpenAI’s $200M DoD contract could squeeze frenemy Microsoft (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/openais-200m-dod-contract-could-squeeze-frenemy-microsoft/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1930518491.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI said Monday the U.S. Department of Defense granted it a contract for up to $200 million to help the agency identify and build prototype systems that use its frontier models for administrative tasks and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI provides a few examples of possible tasking, such as helping service members get healthcare, streamlining data on various programs, and “supporting proactive cyber defense.” The company also said that “All use cases must be consistent with OpenAI’s usage policies and guidelines.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The DoD’s announcement used slightly more straightforward wording. It says, “Under this award, the performer will develop prototype frontier AI capabilities to address critical national security challenges in both warfighting and enterprise domains.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether that reference to war-fighting applies to the weapons themselves or just other areas associated with wars, like paperwork, remains to be seen. OpenAI’s guidelines do forbid individual users to use ChatGPT or its APIs to develop or use weapons. However, OpenAI deleted the explicit prohibitions of “military and warfare” in its terms of service back in January 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given how heavily some powerful people in Silicon Valley have warned of the dangers of China’s advanced LLM models, it’s not surprising the DoD wants to use OpenAI for whatever purposes it wants. For instance, Marc Andreessen, co-founder of VC firm Andreessen Horowitz, an OpenAI investor, recently appeared on Jack Altman’s “Uncapped” podcast (Jack is Sam Altman’s brother). Andreessen described the race between China’s AI and the Western world’s models as a “cold war.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, perhaps an equally interesting part of this announcement is what it says about OpenAI’s increasingly strained relationship with its major investor Microsoft.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft has thousands of contracts with the federal government worth hundreds of millions of dollars. It has, for decades, been implementing the strict security protocols necessary for the government — especially the DoD — to use its cloud.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;OpenAI announced this deal as part of its broader new “OpenAI for Government” program, which consolidates a number of other programs it uses to sell wares directly to government agencies, including the U.S. National Labs⁠, the Air Force Research Laboratory, NASA, NIH, and the Treasury, according to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it was only in April that Microsoft announced the DoD had approved its Azure OpenAI Service for all classified levels. Now the DoD is also going straight to the source. From Microsoft’s perspective: Ouch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft declined to comment and OpenAI did not respond to a request for comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1930518491.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI said Monday the U.S. Department of Defense granted it a contract for up to $200 million to help the agency identify and build prototype systems that use its frontier models for administrative tasks and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI provides a few examples of possible tasking, such as helping service members get healthcare, streamlining data on various programs, and “supporting proactive cyber defense.” The company also said that “All use cases must be consistent with OpenAI’s usage policies and guidelines.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The DoD’s announcement used slightly more straightforward wording. It says, “Under this award, the performer will develop prototype frontier AI capabilities to address critical national security challenges in both warfighting and enterprise domains.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether that reference to war-fighting applies to the weapons themselves or just other areas associated with wars, like paperwork, remains to be seen. OpenAI’s guidelines do forbid individual users to use ChatGPT or its APIs to develop or use weapons. However, OpenAI deleted the explicit prohibitions of “military and warfare” in its terms of service back in January 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given how heavily some powerful people in Silicon Valley have warned of the dangers of China’s advanced LLM models, it’s not surprising the DoD wants to use OpenAI for whatever purposes it wants. For instance, Marc Andreessen, co-founder of VC firm Andreessen Horowitz, an OpenAI investor, recently appeared on Jack Altman’s “Uncapped” podcast (Jack is Sam Altman’s brother). Andreessen described the race between China’s AI and the Western world’s models as a “cold war.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, perhaps an equally interesting part of this announcement is what it says about OpenAI’s increasingly strained relationship with its major investor Microsoft.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft has thousands of contracts with the federal government worth hundreds of millions of dollars. It has, for decades, been implementing the strict security protocols necessary for the government — especially the DoD — to use its cloud.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;OpenAI announced this deal as part of its broader new “OpenAI for Government” program, which consolidates a number of other programs it uses to sell wares directly to government agencies, including the U.S. National Labs⁠, the Air Force Research Laboratory, NASA, NIH, and the Treasury, according to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it was only in April that Microsoft announced the DoD had approved its Azure OpenAI Service for all classified levels. Now the DoD is also going straight to the source. From Microsoft’s perspective: Ouch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft declined to comment and OpenAI did not respond to a request for comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/openais-200m-dod-contract-could-squeeze-frenemy-microsoft/</guid><pubDate>Tue, 17 Jun 2025 21:09:34 +0000</pubDate></item><item><title>OpenAI moves forward with GPT-4.5 deprecation in API, triggering developer anguish and confusion (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/openai-moves-forward-with-gpt-4-5-deprecation-in-api-triggering-developer-anguish-and-confusion/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Word spread quickly across the machine learning and AI community on the social network X yesterday: OpenAI was sending developers an email notifying them that the company would be removing one of its largest and most powerful large language models (LLMs), GPT-4.5 Preview, from the official OpenAI application programming interface (API) &lt;strong&gt;on July 14, 2025.&lt;/strong&gt; &lt;/p&gt;



&lt;p&gt;However, as an OpenAI spokesperson told VentureBeat via email, GPT-4.5 Preview will remain an option to individual ChatGPT users through the dropdown model selector menu at the top of the application. &lt;/p&gt;



&lt;p&gt;But it stil means that any third-party developers who had built applications or workflows atop GPT-4.5 preview (we’ll call it GPT-4.5 from here on out for simplicity’s sake, since a full GPT-4.5 was never made available on this platform), now need to switch over to another one of OpenAI’s nearly 40 (!!!) different model offerings still available through the API. &lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012431" height="507" src="https://venturebeat.com/wp-content/uploads/2025/06/Gtl4zbCXIAAGnLj.jpg" width="633" /&gt;&lt;/figure&gt;



&lt;p&gt;The news quickly spread on X, where developers and AI enthusiasts posted reactions ranging from disappointment to confusion. &lt;/p&gt;



&lt;p&gt;Some described GPT-4.5 as a daily tool in their workflow, praising its tone and reliability. Others questioned the rationale behind launching the model in the first place if it was going to be short-lived.&lt;/p&gt;



&lt;p&gt;“This is sad — GPT-4.5 is one of my fav models,” wrote @BumrahBachi. &lt;/p&gt;



&lt;p&gt;Ben Hyak, the co-founder of AI observability and performance monitoring platform Raindrop.AI, called the move “tragic,” adding: “o3 + 4.5 are the models I use the most everyday.” &lt;/p&gt;



&lt;p&gt;Another user, @flowersslop asked bluntly, “what was the purpose of this model all along?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-deprecation-had-been-planned-since-april"&gt;Deprecation had been planned since April&lt;/h2&gt;



&lt;p&gt;Despite the strong reaction, OpenAI had in fact already announced the plan to deprecate GPT-4.5 Preview back in April 2025 during the launch of GPT-4.1. &lt;/p&gt;



&lt;p&gt;At that time, the company stated that developers would have three months to transition away from 4.5. OpenAI framed the model as an experimental offering that provided insights for future development, and said it would carry forward learnings from GPT-4.5 into future iterations — particularly in areas like creativity and writing nuance.&lt;/p&gt;



&lt;p&gt;In a follow-up response to VentureBeat, OpenAI communications confirmed that the June email was simply a scheduled reminder and that there are currently no plans to remove GPT-4.5 from ChatGPT subscriptions, where the model remains available.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-community-speculation-on-cost-and-model-strategy"&gt;Community speculation on cost and model strategy&lt;/h2&gt;



&lt;p&gt;Still, the developer-facing deprecation leaves a gap for some users, especially those who had built workflows or products around GPT-4.5’s specific characteristics. &lt;/p&gt;



&lt;p&gt;Some in the community speculated that high compute costs might have influenced the move, noting that similar changes had occurred with prior models. &lt;/p&gt;



&lt;p&gt;Others referenced recent API pricing updates, including a major reduction in cost for GPT-3.5 (internally referred to as o3), which is now priced 80% lower than before.&lt;/p&gt;



&lt;p&gt;User @chatgpt21 commented that GPT-4.5 is the “best non reasoning model for open ai on all benchmarks and it’s obvious,” and predicted that OpenAI will “they add test time compute it will blow o3 out of the water. In order to scale TTC you need to scale pre training”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-end-of-the-road-for-gpt-4-5-via-api-developers-encouraged-to-migrate-to-gpt-4-1"&gt;The end of the road for GPT-4.5 via API — developers encouraged to migrate to GPT-4.1&lt;/h2&gt;



&lt;p&gt;OpenAI has directed developers to its online forum for questions about migrating to GPT-4.1 or other models. With the API shutdown for GPT-4.5 Preview set for mid-July, teams relying on the model now have less than a month to complete that transition.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Word spread quickly across the machine learning and AI community on the social network X yesterday: OpenAI was sending developers an email notifying them that the company would be removing one of its largest and most powerful large language models (LLMs), GPT-4.5 Preview, from the official OpenAI application programming interface (API) &lt;strong&gt;on July 14, 2025.&lt;/strong&gt; &lt;/p&gt;



&lt;p&gt;However, as an OpenAI spokesperson told VentureBeat via email, GPT-4.5 Preview will remain an option to individual ChatGPT users through the dropdown model selector menu at the top of the application. &lt;/p&gt;



&lt;p&gt;But it stil means that any third-party developers who had built applications or workflows atop GPT-4.5 preview (we’ll call it GPT-4.5 from here on out for simplicity’s sake, since a full GPT-4.5 was never made available on this platform), now need to switch over to another one of OpenAI’s nearly 40 (!!!) different model offerings still available through the API. &lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012431" height="507" src="https://venturebeat.com/wp-content/uploads/2025/06/Gtl4zbCXIAAGnLj.jpg" width="633" /&gt;&lt;/figure&gt;



&lt;p&gt;The news quickly spread on X, where developers and AI enthusiasts posted reactions ranging from disappointment to confusion. &lt;/p&gt;



&lt;p&gt;Some described GPT-4.5 as a daily tool in their workflow, praising its tone and reliability. Others questioned the rationale behind launching the model in the first place if it was going to be short-lived.&lt;/p&gt;



&lt;p&gt;“This is sad — GPT-4.5 is one of my fav models,” wrote @BumrahBachi. &lt;/p&gt;



&lt;p&gt;Ben Hyak, the co-founder of AI observability and performance monitoring platform Raindrop.AI, called the move “tragic,” adding: “o3 + 4.5 are the models I use the most everyday.” &lt;/p&gt;



&lt;p&gt;Another user, @flowersslop asked bluntly, “what was the purpose of this model all along?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-deprecation-had-been-planned-since-april"&gt;Deprecation had been planned since April&lt;/h2&gt;



&lt;p&gt;Despite the strong reaction, OpenAI had in fact already announced the plan to deprecate GPT-4.5 Preview back in April 2025 during the launch of GPT-4.1. &lt;/p&gt;



&lt;p&gt;At that time, the company stated that developers would have three months to transition away from 4.5. OpenAI framed the model as an experimental offering that provided insights for future development, and said it would carry forward learnings from GPT-4.5 into future iterations — particularly in areas like creativity and writing nuance.&lt;/p&gt;



&lt;p&gt;In a follow-up response to VentureBeat, OpenAI communications confirmed that the June email was simply a scheduled reminder and that there are currently no plans to remove GPT-4.5 from ChatGPT subscriptions, where the model remains available.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-community-speculation-on-cost-and-model-strategy"&gt;Community speculation on cost and model strategy&lt;/h2&gt;



&lt;p&gt;Still, the developer-facing deprecation leaves a gap for some users, especially those who had built workflows or products around GPT-4.5’s specific characteristics. &lt;/p&gt;



&lt;p&gt;Some in the community speculated that high compute costs might have influenced the move, noting that similar changes had occurred with prior models. &lt;/p&gt;



&lt;p&gt;Others referenced recent API pricing updates, including a major reduction in cost for GPT-3.5 (internally referred to as o3), which is now priced 80% lower than before.&lt;/p&gt;



&lt;p&gt;User @chatgpt21 commented that GPT-4.5 is the “best non reasoning model for open ai on all benchmarks and it’s obvious,” and predicted that OpenAI will “they add test time compute it will blow o3 out of the water. In order to scale TTC you need to scale pre training”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-end-of-the-road-for-gpt-4-5-via-api-developers-encouraged-to-migrate-to-gpt-4-1"&gt;The end of the road for GPT-4.5 via API — developers encouraged to migrate to GPT-4.1&lt;/h2&gt;



&lt;p&gt;OpenAI has directed developers to its online forum for questions about migrating to GPT-4.1 or other models. With the API shutdown for GPT-4.5 Preview set for mid-July, teams relying on the model now have less than a month to complete that transition.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-moves-forward-with-gpt-4-5-deprecation-in-api-triggering-developer-anguish-and-confusion/</guid><pubDate>Tue, 17 Jun 2025 21:52:29 +0000</pubDate></item><item><title>Google launches production-ready Gemini 2.5 AI models to challenge OpenAI’s enterprise dominance (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/google-launches-production-ready-gemini-2-5-ai-models-to-challenge-openais-enterprise-dominance/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google moved decisively to strengthen its position in the artificial intelligence arms race Monday, declaring its most powerful Gemini 2.5 models ready for enterprise production while unveiling a new ultra-efficient variant designed to undercut competitors on cost and speed.&lt;/p&gt;



&lt;p&gt;The Alphabet subsidiary promoted two of its flagship AI models—Gemini 2.5 Pro and Gemini 2.5 Flash—from experimental preview status to general availability, signaling the company’s confidence that the technology can handle mission-critical business applications. Google simultaneously introduced Gemini 2.5 Flash-Lite, positioning it as the most cost-effective option in its model lineup for high-volume tasks.&lt;/p&gt;



&lt;p&gt;The announcements represent Google’s most assertive challenge yet to OpenAI’s market leadership, offering enterprises a comprehensive suite of AI tools spanning from premium reasoning capabilities to budget-conscious automation. The move comes as businesses increasingly demand production-ready AI systems that can scale reliably across their operations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-google-finally-moved-its-most-powerful-ai-models-from-preview-to-production-status"&gt;Why Google finally moved its most powerful AI models from preview to production status&lt;/h2&gt;



&lt;p&gt;Google’s decision to graduate these models from preview reflects mounting pressure to match OpenAI’s rapid deployment of consumer and enterprise AI tools. While OpenAI has dominated headlines with ChatGPT and its GPT-4 family, Google has pursued a more cautious approach, extensively testing models before declaring them production-ready.&lt;/p&gt;



&lt;p&gt;“The momentum of the Gemini 2.5 era continues to build,” wrote Jason Gelman, Director of Product Management for Vertex AI, in a blog post announcing the updates. The language suggests Google views this moment as pivotal in establishing its AI platform’s credibility among enterprise buyers.&lt;/p&gt;



&lt;p&gt;The timing appears strategic. Google released these updates just weeks after OpenAI faced scrutiny over the safety and reliability of its latest models, creating an opening for Google to position itself as the more stable, enterprise-focused alternative.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-gemini-s-thinking-capabilities-give-enterprises-more-control-over-ai-decision-making"&gt;How Gemini’s ‘thinking’ capabilities give enterprises more control over AI decision-making&lt;/h2&gt;



&lt;p&gt;What distinguishes Google’s approach is its emphasis on “reasoning” or “thinking” capabilities — a technical architecture that allows models to process problems more deliberately before responding. Unlike traditional language models that generate responses immediately, Gemini 2.5 models can spend additional computational resources working through complex problems step-by-step.&lt;/p&gt;



&lt;p&gt;This “thinking budget” gives developers unprecedented control over AI behavior. They can instruct models to think longer for complex reasoning tasks or respond quickly for simple queries, optimizing both accuracy and cost. The feature addresses a critical enterprise need: predictable AI behavior that can be tuned for specific business requirements.&lt;/p&gt;



&lt;p&gt;Gemini 2.5 Pro, positioned as Google’s most capable model, excels at complex reasoning, advanced code generation, and multimodal understanding. It can process up to one million tokens of context—roughly equivalent to 750,000 words — enabling it to analyze entire codebases or lengthy documents in a single session.&lt;/p&gt;



&lt;p&gt;Gemini 2.5 Flash strikes a balance between capability and efficiency, designed for high-throughput enterprise tasks like large-scale document summarization and responsive chat applications. The newly introduced Flash-Lite variant sacrifices some intelligence for dramatic cost savings, targeting use cases like classification and translation where speed and volume matter more than sophisticated reasoning.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-major-companies-like-snap-and-smartbear-are-already-using-gemini-2-5-in-mission-critical-applications"&gt;Major companies like Snap and SmartBear are already using Gemini 2.5 in mission-critical applications&lt;/h2&gt;



&lt;p&gt;Several major companies have already integrated these models into production systems, suggesting Google’s confidence in their stability isn’t misplaced. Snap Inc. uses Gemini 2.5 Pro to power spatial intelligence features in its AR glasses, translating 2D image coordinates into 3D space for augmented reality applications.&lt;/p&gt;



&lt;p&gt;SmartBear, which provides software testing tools, leverages Gemini 2.5 Flash to translate manual test scripts into automated tests. “The ROI is multifaceted,” said Fitz Nowlan, the company’s VP of AI, describing how the technology accelerates testing velocity while reducing costs.&lt;/p&gt;



&lt;p&gt;Healthcare technology company Connective Health uses the models to extract vital medical information from complex free-text records — a task requiring both accuracy and reliability given the life-or-death nature of medical data. The company’s success with these applications suggests Google’s models have achieved the reliability threshold necessary for regulated industries.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-google-s-new-ai-pricing-strategy-targets-both-premium-and-budget-conscious-enterprise-customers"&gt;Google’s new AI pricing strategy targets both premium and budget-conscious enterprise customers&lt;/h2&gt;



&lt;p&gt;Google’s pricing decisions signal its determination to compete aggressively across market segments. The company raised prices for Gemini 2.5 Flash input tokens from $0.15 to $0.30 per million tokens while reducing output token costs from $3.50 to $2.50 per million tokens. This restructuring benefits applications that generate lengthy responses — a common enterprise use case.&lt;/p&gt;



&lt;p&gt;More significantly, Google eliminated the previous distinction between “thinking” and “non-thinking” pricing that had confused developers. The simplified pricing structure removes a barrier to adoption while making cost prediction easier for enterprise buyers.&lt;/p&gt;



&lt;p&gt;Flash-Lite’s introduction at $0.10 per million input tokens and $0.40 per million output tokens creates a new bottom tier designed to capture price-sensitive workloads. This pricing positions Google to compete with smaller AI providers who have gained traction by offering basic models at extremely low costs.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-google-s-three-tier-model-lineup-means-for-the-competitive-ai-landscape"&gt;What Google’s three-tier model lineup means for the competitive AI landscape&lt;/h2&gt;



&lt;p&gt;The simultaneous release of three production-ready models across different performance tiers represents a sophisticated market segmentation strategy. Google appears to be borrowing from the traditional software industry playbook: offer good, better, and best options to capture customers across budget ranges while providing upgrade paths as needs evolve.&lt;/p&gt;



&lt;p&gt;This approach contrasts sharply with OpenAI’s strategy of pushing users toward its most capable (and expensive) models. Google’s willingness to offer genuinely low-cost alternatives could disrupt the market’s pricing dynamics, particularly for high-volume applications where cost per interaction matters more than peak performance.&lt;/p&gt;



&lt;p&gt;The technical capabilities also position Google advantageously for enterprise sales cycles. The million-token context length enables use cases—like analyzing entire legal contracts or processing comprehensive financial reports — that competing models cannot handle effectively. For large enterprises with complex document processing needs, this capability difference could prove decisive.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-google-s-enterprise-focused-approach-differs-from-openai-s-consumer-first-strategy"&gt;How Google’s enterprise-focused approach differs from OpenAI’s consumer-first strategy&lt;/h2&gt;



&lt;p&gt;These releases occur against the backdrop of intensifying AI competition across multiple fronts. While consumer attention focuses on chatbot interfaces, the real business value—and revenue potential—lies in enterprise applications that can automate complex workflows and augment human decision-making.&lt;/p&gt;



&lt;p&gt;Google’s emphasis on production readiness and enterprise features suggests the company has learned from earlier AI deployment challenges. Previous Google AI launches sometimes felt premature or disconnected from real business needs. The extensive preview period for Gemini 2.5 models, combined with early enterprise partnerships, indicates a more mature approach to product development.&lt;/p&gt;



&lt;p&gt;The technical architecture choices also reflect lessons learned from the broader industry. The “thinking” capability addresses criticism that AI models make decisions too quickly, without sufficient consideration of complex factors. By making this reasoning process controllable and transparent, Google positions its models as more trustworthy for high-stakes business applications.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprises-need-to-know-about-choosing-between-competing-ai-platforms"&gt;What enterprises need to know about choosing between competing AI platforms&lt;/h2&gt;



&lt;p&gt;Google’s aggressive positioning of the Gemini 2.5 family sets up 2025 as a pivotal year for enterprise AI adoption. With production-ready models spanning performance and cost requirements, Google has eliminated many of the technical and economic barriers that previously limited enterprise AI deployment.&lt;/p&gt;



&lt;p&gt;The real test will come as businesses integrate these tools into critical workflows. Early enterprise adopters report promising results, but broader market validation requires months of production use across diverse industries and applications.&lt;/p&gt;



&lt;p&gt;For technical decision makers, Google’s announcement creates both opportunity and complexity. The range of model options enables more precise matching of capabilities to requirements, but also demands more sophisticated evaluation and deployment strategies. Organizations must now consider not just whether to adopt AI, but which specific models and configurations best serve their unique needs.&lt;/p&gt;



&lt;p&gt;The stakes extend beyond individual company decisions. As AI becomes integral to business operations across industries, the choice of AI platform increasingly determines competitive advantage. Enterprise buyers face a critical inflection point: commit to a single AI provider’s ecosystem or maintain costly multi-vendor strategies as the technology matures.&lt;/p&gt;



&lt;p&gt;Google wants to become the enterprise standard for AI—a position that could prove extraordinarily valuable as AI adoption accelerates. The company that created the search engine now wants to create the intelligence engine that powers every business decision.&lt;/p&gt;



&lt;p&gt;After years of watching OpenAI capture headlines and market share, Google has finally stopped talking about the future of AI and started selling it.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google moved decisively to strengthen its position in the artificial intelligence arms race Monday, declaring its most powerful Gemini 2.5 models ready for enterprise production while unveiling a new ultra-efficient variant designed to undercut competitors on cost and speed.&lt;/p&gt;



&lt;p&gt;The Alphabet subsidiary promoted two of its flagship AI models—Gemini 2.5 Pro and Gemini 2.5 Flash—from experimental preview status to general availability, signaling the company’s confidence that the technology can handle mission-critical business applications. Google simultaneously introduced Gemini 2.5 Flash-Lite, positioning it as the most cost-effective option in its model lineup for high-volume tasks.&lt;/p&gt;



&lt;p&gt;The announcements represent Google’s most assertive challenge yet to OpenAI’s market leadership, offering enterprises a comprehensive suite of AI tools spanning from premium reasoning capabilities to budget-conscious automation. The move comes as businesses increasingly demand production-ready AI systems that can scale reliably across their operations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-google-finally-moved-its-most-powerful-ai-models-from-preview-to-production-status"&gt;Why Google finally moved its most powerful AI models from preview to production status&lt;/h2&gt;



&lt;p&gt;Google’s decision to graduate these models from preview reflects mounting pressure to match OpenAI’s rapid deployment of consumer and enterprise AI tools. While OpenAI has dominated headlines with ChatGPT and its GPT-4 family, Google has pursued a more cautious approach, extensively testing models before declaring them production-ready.&lt;/p&gt;



&lt;p&gt;“The momentum of the Gemini 2.5 era continues to build,” wrote Jason Gelman, Director of Product Management for Vertex AI, in a blog post announcing the updates. The language suggests Google views this moment as pivotal in establishing its AI platform’s credibility among enterprise buyers.&lt;/p&gt;



&lt;p&gt;The timing appears strategic. Google released these updates just weeks after OpenAI faced scrutiny over the safety and reliability of its latest models, creating an opening for Google to position itself as the more stable, enterprise-focused alternative.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-gemini-s-thinking-capabilities-give-enterprises-more-control-over-ai-decision-making"&gt;How Gemini’s ‘thinking’ capabilities give enterprises more control over AI decision-making&lt;/h2&gt;



&lt;p&gt;What distinguishes Google’s approach is its emphasis on “reasoning” or “thinking” capabilities — a technical architecture that allows models to process problems more deliberately before responding. Unlike traditional language models that generate responses immediately, Gemini 2.5 models can spend additional computational resources working through complex problems step-by-step.&lt;/p&gt;



&lt;p&gt;This “thinking budget” gives developers unprecedented control over AI behavior. They can instruct models to think longer for complex reasoning tasks or respond quickly for simple queries, optimizing both accuracy and cost. The feature addresses a critical enterprise need: predictable AI behavior that can be tuned for specific business requirements.&lt;/p&gt;



&lt;p&gt;Gemini 2.5 Pro, positioned as Google’s most capable model, excels at complex reasoning, advanced code generation, and multimodal understanding. It can process up to one million tokens of context—roughly equivalent to 750,000 words — enabling it to analyze entire codebases or lengthy documents in a single session.&lt;/p&gt;



&lt;p&gt;Gemini 2.5 Flash strikes a balance between capability and efficiency, designed for high-throughput enterprise tasks like large-scale document summarization and responsive chat applications. The newly introduced Flash-Lite variant sacrifices some intelligence for dramatic cost savings, targeting use cases like classification and translation where speed and volume matter more than sophisticated reasoning.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-major-companies-like-snap-and-smartbear-are-already-using-gemini-2-5-in-mission-critical-applications"&gt;Major companies like Snap and SmartBear are already using Gemini 2.5 in mission-critical applications&lt;/h2&gt;



&lt;p&gt;Several major companies have already integrated these models into production systems, suggesting Google’s confidence in their stability isn’t misplaced. Snap Inc. uses Gemini 2.5 Pro to power spatial intelligence features in its AR glasses, translating 2D image coordinates into 3D space for augmented reality applications.&lt;/p&gt;



&lt;p&gt;SmartBear, which provides software testing tools, leverages Gemini 2.5 Flash to translate manual test scripts into automated tests. “The ROI is multifaceted,” said Fitz Nowlan, the company’s VP of AI, describing how the technology accelerates testing velocity while reducing costs.&lt;/p&gt;



&lt;p&gt;Healthcare technology company Connective Health uses the models to extract vital medical information from complex free-text records — a task requiring both accuracy and reliability given the life-or-death nature of medical data. The company’s success with these applications suggests Google’s models have achieved the reliability threshold necessary for regulated industries.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-google-s-new-ai-pricing-strategy-targets-both-premium-and-budget-conscious-enterprise-customers"&gt;Google’s new AI pricing strategy targets both premium and budget-conscious enterprise customers&lt;/h2&gt;



&lt;p&gt;Google’s pricing decisions signal its determination to compete aggressively across market segments. The company raised prices for Gemini 2.5 Flash input tokens from $0.15 to $0.30 per million tokens while reducing output token costs from $3.50 to $2.50 per million tokens. This restructuring benefits applications that generate lengthy responses — a common enterprise use case.&lt;/p&gt;



&lt;p&gt;More significantly, Google eliminated the previous distinction between “thinking” and “non-thinking” pricing that had confused developers. The simplified pricing structure removes a barrier to adoption while making cost prediction easier for enterprise buyers.&lt;/p&gt;



&lt;p&gt;Flash-Lite’s introduction at $0.10 per million input tokens and $0.40 per million output tokens creates a new bottom tier designed to capture price-sensitive workloads. This pricing positions Google to compete with smaller AI providers who have gained traction by offering basic models at extremely low costs.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-google-s-three-tier-model-lineup-means-for-the-competitive-ai-landscape"&gt;What Google’s three-tier model lineup means for the competitive AI landscape&lt;/h2&gt;



&lt;p&gt;The simultaneous release of three production-ready models across different performance tiers represents a sophisticated market segmentation strategy. Google appears to be borrowing from the traditional software industry playbook: offer good, better, and best options to capture customers across budget ranges while providing upgrade paths as needs evolve.&lt;/p&gt;



&lt;p&gt;This approach contrasts sharply with OpenAI’s strategy of pushing users toward its most capable (and expensive) models. Google’s willingness to offer genuinely low-cost alternatives could disrupt the market’s pricing dynamics, particularly for high-volume applications where cost per interaction matters more than peak performance.&lt;/p&gt;



&lt;p&gt;The technical capabilities also position Google advantageously for enterprise sales cycles. The million-token context length enables use cases—like analyzing entire legal contracts or processing comprehensive financial reports — that competing models cannot handle effectively. For large enterprises with complex document processing needs, this capability difference could prove decisive.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-google-s-enterprise-focused-approach-differs-from-openai-s-consumer-first-strategy"&gt;How Google’s enterprise-focused approach differs from OpenAI’s consumer-first strategy&lt;/h2&gt;



&lt;p&gt;These releases occur against the backdrop of intensifying AI competition across multiple fronts. While consumer attention focuses on chatbot interfaces, the real business value—and revenue potential—lies in enterprise applications that can automate complex workflows and augment human decision-making.&lt;/p&gt;



&lt;p&gt;Google’s emphasis on production readiness and enterprise features suggests the company has learned from earlier AI deployment challenges. Previous Google AI launches sometimes felt premature or disconnected from real business needs. The extensive preview period for Gemini 2.5 models, combined with early enterprise partnerships, indicates a more mature approach to product development.&lt;/p&gt;



&lt;p&gt;The technical architecture choices also reflect lessons learned from the broader industry. The “thinking” capability addresses criticism that AI models make decisions too quickly, without sufficient consideration of complex factors. By making this reasoning process controllable and transparent, Google positions its models as more trustworthy for high-stakes business applications.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprises-need-to-know-about-choosing-between-competing-ai-platforms"&gt;What enterprises need to know about choosing between competing AI platforms&lt;/h2&gt;



&lt;p&gt;Google’s aggressive positioning of the Gemini 2.5 family sets up 2025 as a pivotal year for enterprise AI adoption. With production-ready models spanning performance and cost requirements, Google has eliminated many of the technical and economic barriers that previously limited enterprise AI deployment.&lt;/p&gt;



&lt;p&gt;The real test will come as businesses integrate these tools into critical workflows. Early enterprise adopters report promising results, but broader market validation requires months of production use across diverse industries and applications.&lt;/p&gt;



&lt;p&gt;For technical decision makers, Google’s announcement creates both opportunity and complexity. The range of model options enables more precise matching of capabilities to requirements, but also demands more sophisticated evaluation and deployment strategies. Organizations must now consider not just whether to adopt AI, but which specific models and configurations best serve their unique needs.&lt;/p&gt;



&lt;p&gt;The stakes extend beyond individual company decisions. As AI becomes integral to business operations across industries, the choice of AI platform increasingly determines competitive advantage. Enterprise buyers face a critical inflection point: commit to a single AI provider’s ecosystem or maintain costly multi-vendor strategies as the technology matures.&lt;/p&gt;



&lt;p&gt;Google wants to become the enterprise standard for AI—a position that could prove extraordinarily valuable as AI adoption accelerates. The company that created the search engine now wants to create the intelligence engine that powers every business decision.&lt;/p&gt;



&lt;p&gt;After years of watching OpenAI capture headlines and market share, Google has finally stopped talking about the future of AI and started selling it.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/google-launches-production-ready-gemini-2-5-ai-models-to-challenge-openais-enterprise-dominance/</guid><pubDate>Tue, 17 Jun 2025 21:55:56 +0000</pubDate></item><item><title>Sam Altman says Meta tried and failed to poach OpenAI’s talent with $100M offers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/sam-altman-says-meta-tried-and-failed-to-poach-openais-talent-with-100m-offers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1535376729-e1731897472270.jpg?resize=1200,798" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta CEO Mark Zuckerberg has been on something of a hiring spree lately, trying to staff up Meta’s new superintelligence team with top-tier AI researchers from competing labs. To work on a team led by former Scale AI CEO Alexandr Wang and at a desk physically near Zuckerberg, Meta has reportedly offered employees from OpenAI and Google DeepMind compensation packages worth upwards of $100 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI CEO Sam Altman confirmed those reports on a podcast with his brother, Jack Altman, which was published on Tuesday. However, the OpenAI CEO noted that Zuckerberg’s recruiting efforts have been largely unsuccessful and made sure to throw a few more digs at Meta in the process.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“[Meta has] started making these, like, giant offers to a lot of people on our team,” Sam Altman said on the podcast. “You know, like, $100 million signing bonuses, more than that [in] compensation per year […] I’m really happy that, at least so far, none of our best people have decided to take him up on that.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The OpenAI CEO said he believed his employees made the assessment that OpenAI had a better chance of achieving AGI and may one day be the more valuable company. He also said he believes Meta’s focus on high compensation packages for employees, rather than the mission of delivering AGI, would likely not create a great culture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta reportedly tried to poach one of OpenAI’s lead researchers, Noam Brown, as well as Google’s AI architect, Koray Kavukcuoglu. However, both efforts were unsuccessful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sam Altman went on to say he believes OpenAI’s culture of innovation has been a major key to its success, and that Meta’s “current AI efforts have not worked as well as they hoped.” The OpenAI CEO said he respects many things about Meta but noted he doesn’t “think they’re a company that’s great at innovation.” Later in the podcast, Altman said he believes it’s not enough for companies to catch up on AI — they have to truly innovate to stay ahead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The OpenAI CEO’s comments highlight some of the challenges that Meta has to overcome in order to build out a successful AI superintelligence lab. Besides bringing on Wang, Meta announced last week that it invested significantly in Wang’s former company, Scale AI. The company has also reportedly nabbed a few star AI researchers, such as Google DeepMind’s Jack Rae and Sesame AI’s Johan Schalkwyk. But there’s more work ahead.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;In the coming year, Meta will have to staff up its new AI team while OpenAI, Anthropic, and Google DeepMind operate at full speed. In the coming months, OpenAI is expected to release an open AI model that’s likely to set Meta back in the AI race even further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later on in the podcast, Sam Altman described an AI-powered social media feed that seems likely to encroach on Meta’s apps. The OpenAI CEO said he’s curious about exploring a social media app that uses AI to deliver custom feeds based on what users want, rather than the default, algorithmic feed that exists on traditional social media apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is reportedly working on a social networking app internally. Meanwhile, Meta is experimenting with an AI-powered social network through its Meta AI app. However, it seems that some users are confused by the Meta AI app and have shared some hyperpersonal chats with the broader world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether AI-powered social networks take off remains to be seen. In the meantime, Zuckerberg and Sam Altman seem poised to butt heads over the AI talent race.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1535376729-e1731897472270.jpg?resize=1200,798" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta CEO Mark Zuckerberg has been on something of a hiring spree lately, trying to staff up Meta’s new superintelligence team with top-tier AI researchers from competing labs. To work on a team led by former Scale AI CEO Alexandr Wang and at a desk physically near Zuckerberg, Meta has reportedly offered employees from OpenAI and Google DeepMind compensation packages worth upwards of $100 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI CEO Sam Altman confirmed those reports on a podcast with his brother, Jack Altman, which was published on Tuesday. However, the OpenAI CEO noted that Zuckerberg’s recruiting efforts have been largely unsuccessful and made sure to throw a few more digs at Meta in the process.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“[Meta has] started making these, like, giant offers to a lot of people on our team,” Sam Altman said on the podcast. “You know, like, $100 million signing bonuses, more than that [in] compensation per year […] I’m really happy that, at least so far, none of our best people have decided to take him up on that.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The OpenAI CEO said he believed his employees made the assessment that OpenAI had a better chance of achieving AGI and may one day be the more valuable company. He also said he believes Meta’s focus on high compensation packages for employees, rather than the mission of delivering AGI, would likely not create a great culture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta reportedly tried to poach one of OpenAI’s lead researchers, Noam Brown, as well as Google’s AI architect, Koray Kavukcuoglu. However, both efforts were unsuccessful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sam Altman went on to say he believes OpenAI’s culture of innovation has been a major key to its success, and that Meta’s “current AI efforts have not worked as well as they hoped.” The OpenAI CEO said he respects many things about Meta but noted he doesn’t “think they’re a company that’s great at innovation.” Later in the podcast, Altman said he believes it’s not enough for companies to catch up on AI — they have to truly innovate to stay ahead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The OpenAI CEO’s comments highlight some of the challenges that Meta has to overcome in order to build out a successful AI superintelligence lab. Besides bringing on Wang, Meta announced last week that it invested significantly in Wang’s former company, Scale AI. The company has also reportedly nabbed a few star AI researchers, such as Google DeepMind’s Jack Rae and Sesame AI’s Johan Schalkwyk. But there’s more work ahead.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;In the coming year, Meta will have to staff up its new AI team while OpenAI, Anthropic, and Google DeepMind operate at full speed. In the coming months, OpenAI is expected to release an open AI model that’s likely to set Meta back in the AI race even further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later on in the podcast, Sam Altman described an AI-powered social media feed that seems likely to encroach on Meta’s apps. The OpenAI CEO said he’s curious about exploring a social media app that uses AI to deliver custom feeds based on what users want, rather than the default, algorithmic feed that exists on traditional social media apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is reportedly working on a social networking app internally. Meanwhile, Meta is experimenting with an AI-powered social network through its Meta AI app. However, it seems that some users are confused by the Meta AI app and have shared some hyperpersonal chats with the broader world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether AI-powered social networks take off remains to be seen. In the meantime, Zuckerberg and Sam Altman seem poised to butt heads over the AI talent race.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/sam-altman-says-meta-tried-and-failed-to-poach-openais-talent-with-100m-offers/</guid><pubDate>Tue, 17 Jun 2025 22:31:28 +0000</pubDate></item><item><title>The Interpretable AI playbook: What Anthropic’s research means for your enterprise LLM strategy (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/the-interpretable-ai-playbook-what-anthropics-research-means-for-your-enterprise-llm-strategy/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Anthropic CEO Dario Amodei made an urgent push in April for the need to understand how AI models think.&lt;/p&gt;



&lt;p&gt;This comes at a crucial time. As Anthropic battles in global AI rankings, it’s important to note what sets it apart from other top AI labs. Since its founding in 2021, when seven OpenAI employees broke off over concerns about AI safety, Anthropic has built AI models that adhere to a set of human-valued principles, a system they call Constitutional AI. These principles ensure that models are “helpful, honest and harmless” and generally act in the best interests of society. At the same time, Anthropic’s research arm is diving deep to understand how its models think about the world, and &lt;em&gt;why&lt;/em&gt; they produce helpful (and sometimes harmful) answers.&lt;/p&gt;



&lt;p&gt;Anthropic’s flagship model, Claude 3.7 Sonnet, dominated coding benchmarks when it launched in February, proving that AI models can excel at both performance and safety. And the recent release of Claude 4.0 Opus and Sonnet again puts Claude at the top of coding benchmarks. However, in today’s rapid and hyper-competitive AI market, Anthropic’s rivals like Google’s Gemini 2.5 Pro and Open AI’s o3 have their own impressive showings for coding prowess, while they’re already dominating Claude at math, creative writing and overall reasoning across many languages.&lt;/p&gt;



&lt;p&gt;If Amodei’s thoughts are any indication, Anthropic is planning for the future of AI and its implications in critical fields like medicine, psychology and law, where model safety and human values are imperative. And it shows: Anthropic is the leading AI lab that focuses strictly on developing “interpretable” AI, which are models that let us understand, to some degree of certainty, what the model is thinking and how it arrives at a particular conclusion.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amazon and Google have already invested billions of dollars in Anthropic even as they build their own AI models, so perhaps Anthropic’s competitive advantage is still budding. Interpretable models, as Anthropic suggests, could significantly reduce the long-term operational costs associated with debugging, auditing and mitigating risks in complex AI deployments.&lt;/p&gt;



&lt;p&gt;Sayash Kapoor, an AI safety researcher, suggests that while interpretability is valuable, it is just one of many tools for managing AI risk. In his view, “interpretability is neither necessary nor sufficient” to ensure models behave safely — it matters most when paired with filters, verifiers and human-centered design. This more expansive view sees interpretability as part of a larger ecosystem of control strategies, particularly in real-world AI deployments where models are components in broader decision-making systems.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-need-for-interpretable-ai"&gt;The need for interpretable AI&lt;/h2&gt;



&lt;p&gt;Until recently, many thought AI was still years from advancements like those that are now helping Claude, Gemini and ChatGPT boast exceptional market adoption. While these models are already pushing the frontiers of human knowledge, their widespread use is attributable to just how good they are at solving a wide range of practical problems that require creative problem-solving or detailed analysis. As models are put to the task on increasingly critical problems, it is important that they produce accurate answers.&lt;/p&gt;



&lt;p&gt;Amodei fears that when an AI responds to a prompt, “we have no idea… why it chooses certain words over others, or why it occasionally makes a mistake despite usually being accurate.” Such errors — hallucinations of inaccurate information, or responses that do not align with human values — will hold AI models back from reaching their full potential. Indeed, we’ve seen many examples of AI continuing to struggle with hallucinations and unethical behavior.&lt;/p&gt;



&lt;p&gt;For Amodei, the best way to solve these problems is to understand how an AI thinks: “Our inability to understand models’ internal mechanisms means that we cannot meaningfully predict such [harmful] behaviors, and therefore struggle to rule them out … If instead it were possible to look inside models, we might be able to systematically block all jailbreaks, and also characterize what dangerous knowledge the models have.”&lt;/p&gt;



&lt;p&gt;Amodei also sees the opacity of current models as a barrier to deploying AI models in “high-stakes financial or safety-critical settings, because we can’t fully set the limits on their behavior, and a small number of mistakes could be very harmful.” In decision-making that affects humans directly, like medical diagnosis or mortgage assessments, legal regulations require AI to explain its decisions.&lt;/p&gt;



&lt;p&gt;Imagine a financial institution using a large language model (LLM) for fraud detection — interpretability could mean explaining a denied loan application to a customer as required by law. Or a manufacturing firm optimizing supply chains — understanding why an AI suggests a particular supplier could unlock efficiencies and prevent unforeseen bottlenecks.&lt;/p&gt;



&lt;p&gt;Because of this, Amodei explains, “Anthropic is doubling down on interpretability, and we have a goal of getting to ‘interpretability can reliably detect most model problems’ by 2027.”&lt;/p&gt;



&lt;p&gt;To that end, Anthropic recently participated in a $50 million investment in Goodfire, an AI research lab making breakthrough progress on AI “brain scans.” Their model inspection platform, Ember, is an agnostic tool that identifies learned concepts within models and lets users manipulate them. In a recent demo, the company showed how Ember can recognize individual visual concepts within an image generation AI and then let users &lt;em&gt;paint&lt;/em&gt; these concepts on a canvas to generate new images that follow the user’s design.&lt;/p&gt;



&lt;p&gt;Anthropic’s investment in Ember hints at the fact that developing interpretable models is difficult enough that Anthropic does not have the manpower to achieve interpretability on their own. Creative interpretable models requires new toolchains and skilled developers to build them&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-broader-context-an-ai-researcher-s-perspective"&gt;Broader context: An AI researcher’s perspective&lt;/h2&gt;



&lt;p&gt;To break down Amodei’s perspective and add much-needed context, VentureBeat interviewed Kapoor an AI safety researcher at Princeton. Kapoor co-authored the book &lt;em&gt;AI Snake Oil&lt;/em&gt;, a critical examination of exaggerated claims surrounding the capabilities of leading AI models. He is also a co-author of “&lt;em&gt;AI as Normal Technology&lt;/em&gt;,” in which he advocates for treating AI as a standard, transformational tool like the internet or electricity, and promotes a realistic perspective on its integration into everyday systems.&lt;/p&gt;



&lt;p&gt;Kapoor doesn’t dispute that interpretability is valuable. However, he’s skeptical of treating it as the central pillar of AI alignment. “It’s not a silver bullet,” Kapoor told VentureBeat. Many of the most effective safety techniques, such as post-response filtering, don’t require opening up the model at all, he said.&lt;/p&gt;



&lt;p&gt;He also warns against what researchers call the “fallacy of inscrutability” — the idea that if we don’t fully understand a system’s internals, we can’t use or regulate it responsibly. In practice, full transparency isn’t how most technologies are evaluated. What matters is whether a system performs reliably under real conditions.&lt;/p&gt;



&lt;p&gt;This isn’t the first time Amodei has warned about the risks of AI outpacing our understanding. In his October 2024 post, “Machines of Loving Grace,” he sketched out a vision of increasingly capable models that could take meaningful real-world actions (and maybe double our lifespans).&lt;/p&gt;



&lt;p&gt;According to Kapoor, there’s an important distinction to be made here between a model’s &lt;em&gt;capability&lt;/em&gt; and its &lt;em&gt;power&lt;/em&gt;. Model capabilities are undoubtedly increasing rapidly, and they may soon develop enough intelligence to find solutions for many complex problems challenging humanity today. But a model is only as powerful as the interfaces we provide it to interact with the real world, including where and how models are deployed.&lt;/p&gt;



&lt;p&gt;Amodei has separately argued that the U.S. should maintain a lead in AI development, in part through export controls that limit access to powerful models. The idea is that authoritarian governments might use frontier AI systems irresponsibly — or seize the geopolitical and economic edge that comes with deploying them first.&lt;/p&gt;



&lt;p&gt;For Kapoor, “Even the biggest proponents of export controls agree that it will give us at most a year or two.” He thinks we should treat AI as a “normal technology” like electricity or the internet. While revolutionary, it took decades for both technologies to be fully realized throughout society. Kapoor thinks it’s the same for AI: The best way to maintain geopolitical edge is to focus on the “long game” of transforming industries to use AI effectively.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-others-critiquing-amodei"&gt;Others critiquing Amodei&lt;/h2&gt;



&lt;p&gt;Kapoor isn’t the only one critiquing Amodei’s stance. Last week at VivaTech in Paris, Jansen Huang, CEO of Nvidia, declared his disagreement with Amodei’s views. Huang questioned whether the authority to develop AI should be limited to a few powerful entities like Anthropic. He said: “If you want things to be done safely and responsibly, you do it in the open … Don’t do it in a dark room and tell me it’s safe.” &lt;/p&gt;



&lt;p&gt;In response, Anthropic stated: “Dario has never claimed that ‘only Anthropic’ can build safe and powerful AI. As the public record will show, Dario has advocated for a national transparency standard for AI developers (including Anthropic) so the public and policymakers are aware of the models’ capabilities and risks and can prepare accordingly.”&lt;/p&gt;



&lt;p&gt;It’s also worth noting that Anthropic isn’t alone in its pursuit of interpretability: Google’s DeepMind interpretability team, led by Neel Nanda, has also made serious contributions to interpretability research.&lt;/p&gt;



&lt;p&gt;Ultimately, top AI labs and researchers are providing strong evidence that interpretability could be a key differentiator in the competitive AI market. Enterprises that prioritize interpretability early may gain a significant competitive edge by building more trusted, compliant, and adaptable AI systems.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Anthropic CEO Dario Amodei made an urgent push in April for the need to understand how AI models think.&lt;/p&gt;



&lt;p&gt;This comes at a crucial time. As Anthropic battles in global AI rankings, it’s important to note what sets it apart from other top AI labs. Since its founding in 2021, when seven OpenAI employees broke off over concerns about AI safety, Anthropic has built AI models that adhere to a set of human-valued principles, a system they call Constitutional AI. These principles ensure that models are “helpful, honest and harmless” and generally act in the best interests of society. At the same time, Anthropic’s research arm is diving deep to understand how its models think about the world, and &lt;em&gt;why&lt;/em&gt; they produce helpful (and sometimes harmful) answers.&lt;/p&gt;



&lt;p&gt;Anthropic’s flagship model, Claude 3.7 Sonnet, dominated coding benchmarks when it launched in February, proving that AI models can excel at both performance and safety. And the recent release of Claude 4.0 Opus and Sonnet again puts Claude at the top of coding benchmarks. However, in today’s rapid and hyper-competitive AI market, Anthropic’s rivals like Google’s Gemini 2.5 Pro and Open AI’s o3 have their own impressive showings for coding prowess, while they’re already dominating Claude at math, creative writing and overall reasoning across many languages.&lt;/p&gt;



&lt;p&gt;If Amodei’s thoughts are any indication, Anthropic is planning for the future of AI and its implications in critical fields like medicine, psychology and law, where model safety and human values are imperative. And it shows: Anthropic is the leading AI lab that focuses strictly on developing “interpretable” AI, which are models that let us understand, to some degree of certainty, what the model is thinking and how it arrives at a particular conclusion.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amazon and Google have already invested billions of dollars in Anthropic even as they build their own AI models, so perhaps Anthropic’s competitive advantage is still budding. Interpretable models, as Anthropic suggests, could significantly reduce the long-term operational costs associated with debugging, auditing and mitigating risks in complex AI deployments.&lt;/p&gt;



&lt;p&gt;Sayash Kapoor, an AI safety researcher, suggests that while interpretability is valuable, it is just one of many tools for managing AI risk. In his view, “interpretability is neither necessary nor sufficient” to ensure models behave safely — it matters most when paired with filters, verifiers and human-centered design. This more expansive view sees interpretability as part of a larger ecosystem of control strategies, particularly in real-world AI deployments where models are components in broader decision-making systems.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-need-for-interpretable-ai"&gt;The need for interpretable AI&lt;/h2&gt;



&lt;p&gt;Until recently, many thought AI was still years from advancements like those that are now helping Claude, Gemini and ChatGPT boast exceptional market adoption. While these models are already pushing the frontiers of human knowledge, their widespread use is attributable to just how good they are at solving a wide range of practical problems that require creative problem-solving or detailed analysis. As models are put to the task on increasingly critical problems, it is important that they produce accurate answers.&lt;/p&gt;



&lt;p&gt;Amodei fears that when an AI responds to a prompt, “we have no idea… why it chooses certain words over others, or why it occasionally makes a mistake despite usually being accurate.” Such errors — hallucinations of inaccurate information, or responses that do not align with human values — will hold AI models back from reaching their full potential. Indeed, we’ve seen many examples of AI continuing to struggle with hallucinations and unethical behavior.&lt;/p&gt;



&lt;p&gt;For Amodei, the best way to solve these problems is to understand how an AI thinks: “Our inability to understand models’ internal mechanisms means that we cannot meaningfully predict such [harmful] behaviors, and therefore struggle to rule them out … If instead it were possible to look inside models, we might be able to systematically block all jailbreaks, and also characterize what dangerous knowledge the models have.”&lt;/p&gt;



&lt;p&gt;Amodei also sees the opacity of current models as a barrier to deploying AI models in “high-stakes financial or safety-critical settings, because we can’t fully set the limits on their behavior, and a small number of mistakes could be very harmful.” In decision-making that affects humans directly, like medical diagnosis or mortgage assessments, legal regulations require AI to explain its decisions.&lt;/p&gt;



&lt;p&gt;Imagine a financial institution using a large language model (LLM) for fraud detection — interpretability could mean explaining a denied loan application to a customer as required by law. Or a manufacturing firm optimizing supply chains — understanding why an AI suggests a particular supplier could unlock efficiencies and prevent unforeseen bottlenecks.&lt;/p&gt;



&lt;p&gt;Because of this, Amodei explains, “Anthropic is doubling down on interpretability, and we have a goal of getting to ‘interpretability can reliably detect most model problems’ by 2027.”&lt;/p&gt;



&lt;p&gt;To that end, Anthropic recently participated in a $50 million investment in Goodfire, an AI research lab making breakthrough progress on AI “brain scans.” Their model inspection platform, Ember, is an agnostic tool that identifies learned concepts within models and lets users manipulate them. In a recent demo, the company showed how Ember can recognize individual visual concepts within an image generation AI and then let users &lt;em&gt;paint&lt;/em&gt; these concepts on a canvas to generate new images that follow the user’s design.&lt;/p&gt;



&lt;p&gt;Anthropic’s investment in Ember hints at the fact that developing interpretable models is difficult enough that Anthropic does not have the manpower to achieve interpretability on their own. Creative interpretable models requires new toolchains and skilled developers to build them&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-broader-context-an-ai-researcher-s-perspective"&gt;Broader context: An AI researcher’s perspective&lt;/h2&gt;



&lt;p&gt;To break down Amodei’s perspective and add much-needed context, VentureBeat interviewed Kapoor an AI safety researcher at Princeton. Kapoor co-authored the book &lt;em&gt;AI Snake Oil&lt;/em&gt;, a critical examination of exaggerated claims surrounding the capabilities of leading AI models. He is also a co-author of “&lt;em&gt;AI as Normal Technology&lt;/em&gt;,” in which he advocates for treating AI as a standard, transformational tool like the internet or electricity, and promotes a realistic perspective on its integration into everyday systems.&lt;/p&gt;



&lt;p&gt;Kapoor doesn’t dispute that interpretability is valuable. However, he’s skeptical of treating it as the central pillar of AI alignment. “It’s not a silver bullet,” Kapoor told VentureBeat. Many of the most effective safety techniques, such as post-response filtering, don’t require opening up the model at all, he said.&lt;/p&gt;



&lt;p&gt;He also warns against what researchers call the “fallacy of inscrutability” — the idea that if we don’t fully understand a system’s internals, we can’t use or regulate it responsibly. In practice, full transparency isn’t how most technologies are evaluated. What matters is whether a system performs reliably under real conditions.&lt;/p&gt;



&lt;p&gt;This isn’t the first time Amodei has warned about the risks of AI outpacing our understanding. In his October 2024 post, “Machines of Loving Grace,” he sketched out a vision of increasingly capable models that could take meaningful real-world actions (and maybe double our lifespans).&lt;/p&gt;



&lt;p&gt;According to Kapoor, there’s an important distinction to be made here between a model’s &lt;em&gt;capability&lt;/em&gt; and its &lt;em&gt;power&lt;/em&gt;. Model capabilities are undoubtedly increasing rapidly, and they may soon develop enough intelligence to find solutions for many complex problems challenging humanity today. But a model is only as powerful as the interfaces we provide it to interact with the real world, including where and how models are deployed.&lt;/p&gt;



&lt;p&gt;Amodei has separately argued that the U.S. should maintain a lead in AI development, in part through export controls that limit access to powerful models. The idea is that authoritarian governments might use frontier AI systems irresponsibly — or seize the geopolitical and economic edge that comes with deploying them first.&lt;/p&gt;



&lt;p&gt;For Kapoor, “Even the biggest proponents of export controls agree that it will give us at most a year or two.” He thinks we should treat AI as a “normal technology” like electricity or the internet. While revolutionary, it took decades for both technologies to be fully realized throughout society. Kapoor thinks it’s the same for AI: The best way to maintain geopolitical edge is to focus on the “long game” of transforming industries to use AI effectively.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-others-critiquing-amodei"&gt;Others critiquing Amodei&lt;/h2&gt;



&lt;p&gt;Kapoor isn’t the only one critiquing Amodei’s stance. Last week at VivaTech in Paris, Jansen Huang, CEO of Nvidia, declared his disagreement with Amodei’s views. Huang questioned whether the authority to develop AI should be limited to a few powerful entities like Anthropic. He said: “If you want things to be done safely and responsibly, you do it in the open … Don’t do it in a dark room and tell me it’s safe.” &lt;/p&gt;



&lt;p&gt;In response, Anthropic stated: “Dario has never claimed that ‘only Anthropic’ can build safe and powerful AI. As the public record will show, Dario has advocated for a national transparency standard for AI developers (including Anthropic) so the public and policymakers are aware of the models’ capabilities and risks and can prepare accordingly.”&lt;/p&gt;



&lt;p&gt;It’s also worth noting that Anthropic isn’t alone in its pursuit of interpretability: Google’s DeepMind interpretability team, led by Neel Nanda, has also made serious contributions to interpretability research.&lt;/p&gt;



&lt;p&gt;Ultimately, top AI labs and researchers are providing strong evidence that interpretability could be a key differentiator in the competitive AI market. Enterprises that prioritize interpretability early may gain a significant competitive edge by building more trusted, compliant, and adaptable AI systems.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-interpretable-ai-playbook-what-anthropics-research-means-for-your-enterprise-llm-strategy/</guid><pubDate>Tue, 17 Jun 2025 23:01:08 +0000</pubDate></item><item><title>Police shut down Cluely’s party, the ‘cheat at everything’ startup (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/police-shut-down-cluelys-party-the-cheat-at-everything-startup/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Cluely-party-video.png?resize=1200,772" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The latest San Francisco startup culture drama happened on Monday night. And it centered around “the most legendary party that never happened,” Cluely founder and CEO Roy Lee tells TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cluely had hoped to throw an after-party for a Y Combinator event occurring on Monday and Tuesday called AI Startup School. The event drew crowds thanks to scheduled speakers like Sam Altman, Satya Nadella, and Elon Musk.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cluely is an AI startup born of controversy and rage-bait comedy marketing. True to form, Lee posted a satirical video on X advertising his after-party. It shows him camped out by the famed Y Combinator sign — the one all the YC founders take selfies with. (Cluely is not a YC startup.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tweet advertised the party to his more than 100,000 followers and said to DM for an invite. Lee tells TechCrunch that he didn’t actually send invites out to the hordes. “We only invited friends and friends of friends,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it became&lt;em&gt; the&lt;/em&gt; party, and people shared the details. When it was set to begin, so many people were standing outside the venue that the lines wrapped around blocks. “It just blew up way out of proportion,” Lee says. What looked like 2,000 people showed up, he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A party that big might have gotten out of control, but it didn’t get the chance. The lines were blocking traffic, so the cops showed up and shut it down. “Cluely’s aura is just too strong!”&amp;nbsp;Lee was heard shouting outside as the cops busted it up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would have been the most legendary party in tech history. And I would argue that the reputation of this story might just make it the most legendary party that never happened,” Lee tells TechCrunch, simultaneously proud and bummed.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Lee became known in San Francisco when he posted a viral tweet on X saying he was suspended by Columbia University after he and his co-founder developed an AI tool to cheat on job interviews for software engineers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They turned that tool into a startup that offers a hidden in-browser window that can’t be viewed by an interviewer or proctor. The startup also went viral for its marketing that promised to help people “cheat on everything.” In April, Cluely raised a $5.3 million seed round, and its marketing is now a little less in-your-face: “Everything you need. Before you ask.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The party and its demise by law enforcement naturally became the subject of jokes, memes, and inventive rumors. Lee’s explanation of the crowds outside is perhaps more dull than what some people imagined.&amp;nbsp;After the cops showed, “We did some cleanup, but the drinks are all there waiting for the next party,” he promises.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Cluely-party-video.png?resize=1200,772" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The latest San Francisco startup culture drama happened on Monday night. And it centered around “the most legendary party that never happened,” Cluely founder and CEO Roy Lee tells TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cluely had hoped to throw an after-party for a Y Combinator event occurring on Monday and Tuesday called AI Startup School. The event drew crowds thanks to scheduled speakers like Sam Altman, Satya Nadella, and Elon Musk.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cluely is an AI startup born of controversy and rage-bait comedy marketing. True to form, Lee posted a satirical video on X advertising his after-party. It shows him camped out by the famed Y Combinator sign — the one all the YC founders take selfies with. (Cluely is not a YC startup.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tweet advertised the party to his more than 100,000 followers and said to DM for an invite. Lee tells TechCrunch that he didn’t actually send invites out to the hordes. “We only invited friends and friends of friends,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it became&lt;em&gt; the&lt;/em&gt; party, and people shared the details. When it was set to begin, so many people were standing outside the venue that the lines wrapped around blocks. “It just blew up way out of proportion,” Lee says. What looked like 2,000 people showed up, he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A party that big might have gotten out of control, but it didn’t get the chance. The lines were blocking traffic, so the cops showed up and shut it down. “Cluely’s aura is just too strong!”&amp;nbsp;Lee was heard shouting outside as the cops busted it up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would have been the most legendary party in tech history. And I would argue that the reputation of this story might just make it the most legendary party that never happened,” Lee tells TechCrunch, simultaneously proud and bummed.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Lee became known in San Francisco when he posted a viral tweet on X saying he was suspended by Columbia University after he and his co-founder developed an AI tool to cheat on job interviews for software engineers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They turned that tool into a startup that offers a hidden in-browser window that can’t be viewed by an interviewer or proctor. The startup also went viral for its marketing that promised to help people “cheat on everything.” In April, Cluely raised a $5.3 million seed round, and its marketing is now a little less in-your-face: “Everything you need. Before you ask.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The party and its demise by law enforcement naturally became the subject of jokes, memes, and inventive rumors. Lee’s explanation of the crowds outside is perhaps more dull than what some people imagined.&amp;nbsp;After the cops showed, “We did some cleanup, but the drinks are all there waiting for the next party,” he promises.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/police-shut-down-cluelys-party-the-cheat-at-everything-startup/</guid><pubDate>Tue, 17 Jun 2025 23:09:59 +0000</pubDate></item></channel></rss>