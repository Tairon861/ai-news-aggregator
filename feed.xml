<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 04 Feb 2026 18:55:18 +0000</lastBuildDate><item><title>Exclusive: Positron raises $230M Series B to take on Nvidia’s AI chips (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/exclusive-positron-raises-230m-series-b-to-take-on-nvidias-ai-chips/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/positron.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Semiconductor startup Positron has secured $230 million in Series B funding, TechCrunch has exclusively learned. The outfit plans to use the capital to speed up deployment of its high-speed memory chips, a critical component for the chips used for AI workloads, sources familiar with the matter told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round, which brought Positron to a $1 billion valuation, was co-led by Arena Private Wealth, Jump Trading, and Unless, with strategic investment from Qatar Investment Authority (QIA), the country’s sovereign wealth fund, which has been increasingly focused on building out AI infrastructure, the sources said. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Reno-based startup’s Series B comes as hyperscalers and AI firms push to reduce their reliance on longstanding leader Nvidia. These firms include OpenAI, which, despite being one of Nvidia’s largest and most important customers, is reportedly unsatisfied with some of the firm’s latest AI chips and has been seeking alternatives since last year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Qatar, through QIA, has been accelerating a broader push into so-called “sovereign” AI infrastructure — a priority repeatedly underscored at Web Summit Qatar in Doha this week. Several sources told TechCrunch the country views compute capacity as critical to staying competitive on the global economic stage, and is positioning itself as a leading AI services hub in the Middle East, fueling interest in startups like Positron.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The strategy is already taking shape through major commitments, including a $20 billion AI infrastructure joint venture with Brookfield Asset Management that was announced in December.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Positron’s fundraise brings the three-year-old startup’s total capital raised to just over $300 million. The startup previously raised $75 million last year from investors including Valor Equity Partners, Atreides Management, DFJ Growth, Flume Ventures, and Resilience Reserve.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims its first-generation chip, Atlas, manufactured in Arizona, can match the performance of Nvidia’s H100 GPUs for less than a third of the power. Positron is focused on inference — computing needed to run AI models for real-world applications — rather than training large language models, positioning the company well as demand surges for inference hardware as businesses increasingly shift focus from building large models to deploying them at scale.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;With the funding, Positron aims to accelerate its roadmap to ship its next-generation Asimov silicon chip, targeting production in early 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sources tell TechCrunch that beyond its memory capabilities, Positron’s chips also perform strongly in high-frequency and video-processing workloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Positron for more information.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated to include updates from Positron about co-lead investors and valuation. &lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: A previous version of this article misstated the date of QIA’s partnership with Brookfield. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/positron.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Semiconductor startup Positron has secured $230 million in Series B funding, TechCrunch has exclusively learned. The outfit plans to use the capital to speed up deployment of its high-speed memory chips, a critical component for the chips used for AI workloads, sources familiar with the matter told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round, which brought Positron to a $1 billion valuation, was co-led by Arena Private Wealth, Jump Trading, and Unless, with strategic investment from Qatar Investment Authority (QIA), the country’s sovereign wealth fund, which has been increasingly focused on building out AI infrastructure, the sources said. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Reno-based startup’s Series B comes as hyperscalers and AI firms push to reduce their reliance on longstanding leader Nvidia. These firms include OpenAI, which, despite being one of Nvidia’s largest and most important customers, is reportedly unsatisfied with some of the firm’s latest AI chips and has been seeking alternatives since last year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Qatar, through QIA, has been accelerating a broader push into so-called “sovereign” AI infrastructure — a priority repeatedly underscored at Web Summit Qatar in Doha this week. Several sources told TechCrunch the country views compute capacity as critical to staying competitive on the global economic stage, and is positioning itself as a leading AI services hub in the Middle East, fueling interest in startups like Positron.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The strategy is already taking shape through major commitments, including a $20 billion AI infrastructure joint venture with Brookfield Asset Management that was announced in December.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Positron’s fundraise brings the three-year-old startup’s total capital raised to just over $300 million. The startup previously raised $75 million last year from investors including Valor Equity Partners, Atreides Management, DFJ Growth, Flume Ventures, and Resilience Reserve.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company claims its first-generation chip, Atlas, manufactured in Arizona, can match the performance of Nvidia’s H100 GPUs for less than a third of the power. Positron is focused on inference — computing needed to run AI models for real-world applications — rather than training large language models, positioning the company well as demand surges for inference hardware as businesses increasingly shift focus from building large models to deploying them at scale.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;With the funding, Positron aims to accelerate its roadmap to ship its next-generation Asimov silicon chip, targeting production in early 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sources tell TechCrunch that beyond its memory capabilities, Positron’s chips also perform strongly in high-frequency and video-processing workloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Positron for more information.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated to include updates from Positron about co-lead investors and valuation. &lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: A previous version of this article misstated the date of QIA’s partnership with Brookfield. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/exclusive-positron-raises-230m-series-b-to-take-on-nvidias-ai-chips/</guid><pubDate>Wed, 04 Feb 2026 08:18:51 +0000</pubDate></item><item><title>How Cisco builds smart systems for the AI era (AI News)</title><link>https://www.artificialintelligence-news.com/news/how-cisco-builds-smart-systems-for-the-ai-era/</link><description>&lt;p&gt; Among the big players in technology, Cisco is one of the sector’s leaders that’s advancing operational deployments of AI internally to its own operations, and the tools it sells to its customers around the world. As a large company, its activities encompass many areas of the typical IT stack, including infrastructure, services, security, and the design of entire enterprise-scale networks.&lt;/p&gt;&lt;p&gt; Cisco’s internal teams use a blend of machine learning and agentic AI to help them improve their own service delivery and personalise user experiences for its customers. It’s built a shared AI fabric built on patterns of compute and networking that are the product of years spent checking and validating its systems – battle-hardened solutions it then has the confidence to offer to customers. The infrastructure in play relies on high-performance GPUs, of course, but it’s not just raw horse-power. The detail is in the careful integration between compute and network stacks used in model training and the quite different demands from the ongoing load of inference.&lt;/p&gt;&lt;p&gt; Having made its name as the &lt;i&gt;de facto&lt;/i&gt; supplier of networking infrastructure for the enterprise, it comes as no shock that it’s in network automation that some of its better-known uses of AI finds their place. Automated configuration workflows and identity management combine into access solutions that are focused on rapid network deployments generated by natural language.&lt;/p&gt;&lt;p&gt; For organisations looking to develop into the next generation of AI users, Cisco has been rolling out hardware and orchestration tools that are aimed explicitly to support AI workloads. A recent collaboration with chip giant NVIDIA led to the emergence of a new line of switches and the Nexus Hyperfabric line of AI network controllers. These aim to simplify the deployment of the complex clusters needed for top-end, high-performance artificial intelligence clusters.&lt;/p&gt;&lt;p&gt; Cisco’s Secure AI Factory framework with partners like NVIDIA and Run:ai is aimed at production-grade AI pipelines. It uses distributed orchestration, GPU utilisation governance, Kubernetes microservice optimisation, and storage, under the umbrella product description Intersight. For more local deployments, Cisco Unified Edge brings all the necessary elements – compute, networking, security, and storage – close to where data gets generated and processed.&lt;/p&gt;&lt;p&gt; In environments where latency metrics are critically important, AI processing at the edge is the answer. But Cisco’s approach is not necessarily to offer dedicated IIoT-specific solutions. Instead, it tries to extend the operational models typically found in a data centre and applies the same technology (if not the same exact methodology) to edge sites. It’s like data centre-grade security policies and configurations available to remote installations. Having the same precepts and standards in cloud and edge mean that Cisco accredited engineers can manage and maintain data centres or small edge deployments using the same skills, accreditation, knowledge, and experience.&lt;/p&gt;&lt;p&gt; Security and risk management figure prominently in the Cisco AI narrative. Its Integrated AI Security and Safety Framework applies high standards of safety and security throughout the life-cycle of AI systems. It considers adversarial threats, supply chain weakness, the risk profiles of multi-agent interactions, and multi-modal vulnerabilities as issues that have to be addressed regardless of the nature or size of any deployment.&lt;/p&gt;&lt;p&gt; Cisco’s work on operational AI also reflects broader ecosystem conversations. The company markets products for organisations wanting to make the transition from generative to agentic AI, where autonomous software agents carry out operational tasks. In most cases, this requires new tooling and new operational protocols.&lt;/p&gt;&lt;p&gt; Cisco’s future AI plans include continuing its central work in infrastructure provision for AI workloads. It’s also pursuing broader adoption of AI-ready networks, including next-gen wireless and unified management systems that will control systems across campus, branch, and cloud environments. The company is also expanding its software and platform investments, including its most recent acquisition (NeuralFabric), to help it build a more comprehensive software stack and product portfolio.&lt;/p&gt;&lt;p&gt; In summary, Cisco’s AI deployment strategy combines hardware, software, and service elements that embed AI into operations, giving organisations a route to production-grade systems. Its work can be found in large-scale infrastructure, systems for unified management, risk mitigation, and anywhere that connects distributed, cloud, and edge computing.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt; Among the big players in technology, Cisco is one of the sector’s leaders that’s advancing operational deployments of AI internally to its own operations, and the tools it sells to its customers around the world. As a large company, its activities encompass many areas of the typical IT stack, including infrastructure, services, security, and the design of entire enterprise-scale networks.&lt;/p&gt;&lt;p&gt; Cisco’s internal teams use a blend of machine learning and agentic AI to help them improve their own service delivery and personalise user experiences for its customers. It’s built a shared AI fabric built on patterns of compute and networking that are the product of years spent checking and validating its systems – battle-hardened solutions it then has the confidence to offer to customers. The infrastructure in play relies on high-performance GPUs, of course, but it’s not just raw horse-power. The detail is in the careful integration between compute and network stacks used in model training and the quite different demands from the ongoing load of inference.&lt;/p&gt;&lt;p&gt; Having made its name as the &lt;i&gt;de facto&lt;/i&gt; supplier of networking infrastructure for the enterprise, it comes as no shock that it’s in network automation that some of its better-known uses of AI finds their place. Automated configuration workflows and identity management combine into access solutions that are focused on rapid network deployments generated by natural language.&lt;/p&gt;&lt;p&gt; For organisations looking to develop into the next generation of AI users, Cisco has been rolling out hardware and orchestration tools that are aimed explicitly to support AI workloads. A recent collaboration with chip giant NVIDIA led to the emergence of a new line of switches and the Nexus Hyperfabric line of AI network controllers. These aim to simplify the deployment of the complex clusters needed for top-end, high-performance artificial intelligence clusters.&lt;/p&gt;&lt;p&gt; Cisco’s Secure AI Factory framework with partners like NVIDIA and Run:ai is aimed at production-grade AI pipelines. It uses distributed orchestration, GPU utilisation governance, Kubernetes microservice optimisation, and storage, under the umbrella product description Intersight. For more local deployments, Cisco Unified Edge brings all the necessary elements – compute, networking, security, and storage – close to where data gets generated and processed.&lt;/p&gt;&lt;p&gt; In environments where latency metrics are critically important, AI processing at the edge is the answer. But Cisco’s approach is not necessarily to offer dedicated IIoT-specific solutions. Instead, it tries to extend the operational models typically found in a data centre and applies the same technology (if not the same exact methodology) to edge sites. It’s like data centre-grade security policies and configurations available to remote installations. Having the same precepts and standards in cloud and edge mean that Cisco accredited engineers can manage and maintain data centres or small edge deployments using the same skills, accreditation, knowledge, and experience.&lt;/p&gt;&lt;p&gt; Security and risk management figure prominently in the Cisco AI narrative. Its Integrated AI Security and Safety Framework applies high standards of safety and security throughout the life-cycle of AI systems. It considers adversarial threats, supply chain weakness, the risk profiles of multi-agent interactions, and multi-modal vulnerabilities as issues that have to be addressed regardless of the nature or size of any deployment.&lt;/p&gt;&lt;p&gt; Cisco’s work on operational AI also reflects broader ecosystem conversations. The company markets products for organisations wanting to make the transition from generative to agentic AI, where autonomous software agents carry out operational tasks. In most cases, this requires new tooling and new operational protocols.&lt;/p&gt;&lt;p&gt; Cisco’s future AI plans include continuing its central work in infrastructure provision for AI workloads. It’s also pursuing broader adoption of AI-ready networks, including next-gen wireless and unified management systems that will control systems across campus, branch, and cloud environments. The company is also expanding its software and platform investments, including its most recent acquisition (NeuralFabric), to help it build a more comprehensive software stack and product portfolio.&lt;/p&gt;&lt;p&gt; In summary, Cisco’s AI deployment strategy combines hardware, software, and service elements that embed AI into operations, giving organisations a route to production-grade systems. Its work can be found in large-scale infrastructure, systems for unified management, risk mitigation, and anywhere that connects distributed, cloud, and edge computing.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/how-cisco-builds-smart-systems-for-the-ai-era/</guid><pubDate>Wed, 04 Feb 2026 10:00:00 +0000</pubDate></item><item><title>Combing the Rackspace blogfiles for operational AI pointers (AI News)</title><link>https://www.artificialintelligence-news.com/news/combing-the-rackspace-blogfiles-for-operational-ai-pointers/</link><description>&lt;p&gt; In a recent blog output, Rackspace refers to the bottlenecks familiar to many readers: messy data, unclear ownership, governance gaps, and the cost of running models once they become part of production. The company frames them through the lens of service delivery, security operations, and cloud modernisation, which tells you where it is putting its own effort.&lt;/p&gt;&lt;p&gt; One of the clearest examples of operational AI inside Rackspace sits in its security business. In late January, the company described RAIDER (Rackspace Advanced Intelligence, Detection and Event Research) as a custom back-end platform built for its internal cyber defense centre. With security teams working amid many alerts and logs, standard detection engineering doesn’t scale if dependent on the manual writing of security rules. Rackspace says its RAIDER system unifies threat intelligence with detection engineering workflows and uses its AI Security Engine (RAISE) and LLMs to automate detection rule creation, generating detection criteria  it describes as “platform-ready” in line with known frameworks such as MITRE ATT&amp;amp;CK. The company claims it’s cut detection development time by more than half and reduced mean time to detect and respond. This is just the kind of internal process change that matters.&lt;/p&gt;&lt;p&gt; The company also positions agentic AI as a way of taking the friction out of complex engineering programmes. A January post on modernising VMware environments on AWS describes a model in which AI agents handle data-intensive analysis and many repeating tasks, yet it keeps “architectural judgement, governance and business decisions” remain in the human domain. Rackspace presents this workflow as stopping senior engineers being sidelined into migration projects. The article states the target is to keep day two operations in scope –  where many migration plans fail as teams discover they have modernised infrastructure but not operating practices.&lt;/p&gt;&lt;p&gt; Elsewhere the company sets out a picture of AI-supported operations where monitoring becomes more predictive, routine incidents are handled by bots and automation scripts, and telemetry (plus historical data) are used to spot patterns and, it turn, recommend fixes. This is conventional AIOps language, but it Rackspace is tying such language to managed services delivery, suggesting the company uses AI to reduce the cost of labour in operational pipelines in addition to the more familiar use of AI in customer-facing environments.&lt;/p&gt;&lt;p&gt; In a post describing AI-enabled operations, the company stresses the importance of focus strategy, governance and operating models. It specifies the machinery it needed to industrialise AI, such as choosing infrastructure based on whether workloads involve training, fine-tuning or inference. Many tasks are relatively lightweight and can run inference locally on existing hardware.&lt;/p&gt;&lt;p&gt; The company’s noted four recurring barriers to AI adoption, most notably that of fragmented and inconsistent data, and it recommends investment in integration and data management so models have consistent foundations. This is not an opinion unique to Rackspace, of course, but having it writ large by a technology-first, big player is illustrative of the issues faced by many enterprise-scale AI deployments.&lt;/p&gt;&lt;p&gt; A company of even greater size, Microsoft, is working to coordinate autonomous agents’ work across systems. Copilot has evolved into an orchestration layer, and in Microsoft’s ecosystem, multi-step task execution and broader model choice do exist. However, it’s noteworthy that Redmond is called out by Rackspace on the fact that productivity gains only arrive when identity, data access, and oversight are firmly ensconced into operations.&lt;/p&gt;&lt;p&gt; Rackspace’s near-term AI plan comprises of AI-assisted security engineering, agent-supported modernisation, and AI-augmented service management. Its future plans can perhaps be discerned in a January article published on the company’s blog that concerns private cloud AI trends. In it, the author  argues inference economics and governance will drive architecture decisions well into 2026. It anticipates ‘bursty’ exploration in public clouds, while moving inference tasks into private clouds on the grounds of cost stability, and compliance. That’s a roadmap for operational AI grounded in budget and audit requirements, not novelty.&lt;/p&gt;&lt;p&gt; For decision-makers trying to accelerate their own deployments, the useful takeaway is  that Rackspace has treats AI as an operational discipline. The concrete, published examples it gives are those that reduce cycle time in repeatable work. Readers may accept the company’s direction and still be wary of the company’s claimed metrics. The steps to take inside a growing business are to discover repeating processes, examine where strict oversight is necessary because of data  governance, and where inference costs might be reduced by bringing some processing in-house.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt; In a recent blog output, Rackspace refers to the bottlenecks familiar to many readers: messy data, unclear ownership, governance gaps, and the cost of running models once they become part of production. The company frames them through the lens of service delivery, security operations, and cloud modernisation, which tells you where it is putting its own effort.&lt;/p&gt;&lt;p&gt; One of the clearest examples of operational AI inside Rackspace sits in its security business. In late January, the company described RAIDER (Rackspace Advanced Intelligence, Detection and Event Research) as a custom back-end platform built for its internal cyber defense centre. With security teams working amid many alerts and logs, standard detection engineering doesn’t scale if dependent on the manual writing of security rules. Rackspace says its RAIDER system unifies threat intelligence with detection engineering workflows and uses its AI Security Engine (RAISE) and LLMs to automate detection rule creation, generating detection criteria  it describes as “platform-ready” in line with known frameworks such as MITRE ATT&amp;amp;CK. The company claims it’s cut detection development time by more than half and reduced mean time to detect and respond. This is just the kind of internal process change that matters.&lt;/p&gt;&lt;p&gt; The company also positions agentic AI as a way of taking the friction out of complex engineering programmes. A January post on modernising VMware environments on AWS describes a model in which AI agents handle data-intensive analysis and many repeating tasks, yet it keeps “architectural judgement, governance and business decisions” remain in the human domain. Rackspace presents this workflow as stopping senior engineers being sidelined into migration projects. The article states the target is to keep day two operations in scope –  where many migration plans fail as teams discover they have modernised infrastructure but not operating practices.&lt;/p&gt;&lt;p&gt; Elsewhere the company sets out a picture of AI-supported operations where monitoring becomes more predictive, routine incidents are handled by bots and automation scripts, and telemetry (plus historical data) are used to spot patterns and, it turn, recommend fixes. This is conventional AIOps language, but it Rackspace is tying such language to managed services delivery, suggesting the company uses AI to reduce the cost of labour in operational pipelines in addition to the more familiar use of AI in customer-facing environments.&lt;/p&gt;&lt;p&gt; In a post describing AI-enabled operations, the company stresses the importance of focus strategy, governance and operating models. It specifies the machinery it needed to industrialise AI, such as choosing infrastructure based on whether workloads involve training, fine-tuning or inference. Many tasks are relatively lightweight and can run inference locally on existing hardware.&lt;/p&gt;&lt;p&gt; The company’s noted four recurring barriers to AI adoption, most notably that of fragmented and inconsistent data, and it recommends investment in integration and data management so models have consistent foundations. This is not an opinion unique to Rackspace, of course, but having it writ large by a technology-first, big player is illustrative of the issues faced by many enterprise-scale AI deployments.&lt;/p&gt;&lt;p&gt; A company of even greater size, Microsoft, is working to coordinate autonomous agents’ work across systems. Copilot has evolved into an orchestration layer, and in Microsoft’s ecosystem, multi-step task execution and broader model choice do exist. However, it’s noteworthy that Redmond is called out by Rackspace on the fact that productivity gains only arrive when identity, data access, and oversight are firmly ensconced into operations.&lt;/p&gt;&lt;p&gt; Rackspace’s near-term AI plan comprises of AI-assisted security engineering, agent-supported modernisation, and AI-augmented service management. Its future plans can perhaps be discerned in a January article published on the company’s blog that concerns private cloud AI trends. In it, the author  argues inference economics and governance will drive architecture decisions well into 2026. It anticipates ‘bursty’ exploration in public clouds, while moving inference tasks into private clouds on the grounds of cost stability, and compliance. That’s a roadmap for operational AI grounded in budget and audit requirements, not novelty.&lt;/p&gt;&lt;p&gt; For decision-makers trying to accelerate their own deployments, the useful takeaway is  that Rackspace has treats AI as an operational discipline. The concrete, published examples it gives are those that reduce cycle time in repeatable work. Readers may accept the company’s direction and still be wary of the company’s claimed metrics. The steps to take inside a growing business are to discover repeating processes, examine where strict oversight is necessary because of data  governance, and where inference costs might be reduced by bringing some processing in-house.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: Pixabay)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/combing-the-rackspace-blogfiles-for-operational-ai-pointers/</guid><pubDate>Wed, 04 Feb 2026 10:01:00 +0000</pubDate></item><item><title>Accel doubles down on Fibr AI as agents turn static websites into one-to-one experiences (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/accel-doubles-down-on-fibr-ai-as-agents-turn-static-websites-into-one-to-one-experiences/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While advertising and targeting have become increasingly personalized, the website — the final destination for that traffic — has remained largely static. Fibr AI aims to bridge that gap by using AI agents to turn generic webpages into one-to-one experiences tailored to each visitor, a thesis that has prompted Accel to double down on the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Accel has led Fibr AI’s $5.7 million seed round following an earlier $1.8 million pre-seed investment in 2024. The fresh funding also included participation from WillowTree Ventures and MVP Ventures, alongside Fortune 100 operators joining as angel investors and advisors, bringing the startup’s total funding to $7.5 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For large companies, the gap between increasingly personalized ads and largely generic website experiences has traditionally been filled by a mix of personalization software, engineering teams, and marketing agencies — a model that is slow, expensive, and difficult to scale. While ads can be tailored instantly for different audiences, changing what happens once a visitor lands on a site often requires weeks of coordination and limits teams to running only a handful of experiments each year. Fibr AI argues that this human-heavy operating model no longer works. Instead, the startup uses autonomous AI agents to infer intent, generate variations, and continuously optimize pages in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fibr AI replaces the agency- and engineering-heavy model with autonomous systems that operate continuously, Ankur Goyal (pictured above, right), the co-founder and chief executive, said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are [the] software, and the agency is the workforce of agents we are deploying,” Goyal told TechCrunch, adding that this allows Fibr AI to run thousands of experiments in parallel rather than a few dozen each year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adoption was initially slow. Founded in early 2023 by Goyal and Pritam Roy (pictured above, left), Fibr AI had just one or two customers for much of its first two years as enterprises took time to evaluate the approach. That began to change last year, Goyal said, with adoption picking up among large U.S. companies, including banks and healthcare providers, bringing the total number of customers to 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are an infra afterthought layer,” Goyal told TechCrunch. “Once it’s set up, nobody wants to think about it again.” That dynamic, he added, has led Fibr AI to sign three- to five-year contracts with large enterprises, which tend to treat website infrastructure as something to standardize rather than continuously revisit.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;At a technical level, Fibr AI operates as a layer on top of an existing website, connecting to a company’s advertising, analytics, and customer data systems to understand how visitors arrive and what they are likely looking for. Its AI agents then assemble and adjust page content, such as copy, imagery, and layout, treating each URL as a system that learns and optimizes continuously rather than a fixed page. Instead of relying on manually configured rules or sequential A/B tests, the platform runs large numbers of micro-experiments in parallel and updates experiences systematically as traffic flows in from different channels.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3089322" height="1080" src="https://techcrunch.com/wp-content/uploads/2026/02/fibr-ai-personalization_b82c81.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Fibr AI personalizes webpages using AI agents&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Fibr AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That shift has direct cost implications for large enterprises. Traditional website personalization typically combines software licenses with agency retainers and engineering time, tying costs to people rather than outcomes. Goyal said enterprises are increasingly evaluating Fibr AI’s platform based on cost per experiment and conversion impact, rather than the number of tools or people involved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Accel, that operating model — rather than the AI buzz — was central to the decision to invest again. “Advertising today is one-to-one, but when users land on a website it becomes one-to-many,” said Prayank Swaroop, a partner at Accel. “You can create hundreds of ads for different audiences, but they all still land on the same page.” Fibr’s ability to turn that dynamic into one-to-one personalization, he said, stood out because it removed the agency and engineering bottlenecks that typically limit how far enterprises can push experimentation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Swaroop added that early enterprise adoption, particularly among banks and healthcare companies, helped validate the thesis. “These are regulated, conservative industries,” he said. “When they start saying, ‘We need this, and we’re willing to pay for it,’ that’s when we feel confident doubling down.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-future-proofing-for-the-agentic-commerce-era"&gt;Future-proofing for the agentic-commerce era&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While most of Fibr AI’s business today is driven by personalizing experiences for human visitors, Accel and Fibr AI also see potential in how AI agents are beginning to mediate online discovery. As users increasingly research, compare, and shortlist products using large language models and AI chatbots, including OpenAI’s ChatGPT, before visiting a website, Swaroop said, the ability for sites to adapt based on what a visitor — or an AI system acting on their behalf — already knows could become more important over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That part is still early,” Swaroop said, “but the companies building for today’s needs while being ready for that shift tomorrow are the ones we want to back.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3089266" height="984" src="https://techcrunch.com/wp-content/uploads/2026/02/fibr-ai-llm-optimization.jpg" width="1298" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Fibr AI’s dynamic experience for discovery through LLMs and AI chatbots&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Fibr AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new funding, Fibr AI plans to focus on expanding its sales and customer-facing teams in the U.S., while continuing to build out its technical base in India. The San Francisco-headquartered startup maintains an office in Bengaluru, with 17 of its roughly 23 employees based in India and the remaining six in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Goyal said the startup targets about $5 million in annual recurring revenue by the end of this year and around 50 enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fibr AI is entering a space long dominated by incumbents such as Adobe and Optimizely, which offer experimentation and personalization tools to large enterprises. But both Goyal and Swaroop argued that those platforms are constrained by how they are built and sold, typically relying on marketing agencies and engineering teams to configure and operate them. That model, they said, makes it difficult to move quickly or scale experimentation, even as customer acquisition and messaging have become increasingly dynamic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Incumbents have been slow in bringing out products,” Swaroop said, adding that even when new features arrive, they often come years after demand has shifted.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;While advertising and targeting have become increasingly personalized, the website — the final destination for that traffic — has remained largely static. Fibr AI aims to bridge that gap by using AI agents to turn generic webpages into one-to-one experiences tailored to each visitor, a thesis that has prompted Accel to double down on the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Accel has led Fibr AI’s $5.7 million seed round following an earlier $1.8 million pre-seed investment in 2024. The fresh funding also included participation from WillowTree Ventures and MVP Ventures, alongside Fortune 100 operators joining as angel investors and advisors, bringing the startup’s total funding to $7.5 million.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For large companies, the gap between increasingly personalized ads and largely generic website experiences has traditionally been filled by a mix of personalization software, engineering teams, and marketing agencies — a model that is slow, expensive, and difficult to scale. While ads can be tailored instantly for different audiences, changing what happens once a visitor lands on a site often requires weeks of coordination and limits teams to running only a handful of experiments each year. Fibr AI argues that this human-heavy operating model no longer works. Instead, the startup uses autonomous AI agents to infer intent, generate variations, and continuously optimize pages in real time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fibr AI replaces the agency- and engineering-heavy model with autonomous systems that operate continuously, Ankur Goyal (pictured above, right), the co-founder and chief executive, said in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are [the] software, and the agency is the workforce of agents we are deploying,” Goyal told TechCrunch, adding that this allows Fibr AI to run thousands of experiments in parallel rather than a few dozen each year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adoption was initially slow. Founded in early 2023 by Goyal and Pritam Roy (pictured above, left), Fibr AI had just one or two customers for much of its first two years as enterprises took time to evaluate the approach. That began to change last year, Goyal said, with adoption picking up among large U.S. companies, including banks and healthcare providers, bringing the total number of customers to 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are an infra afterthought layer,” Goyal told TechCrunch. “Once it’s set up, nobody wants to think about it again.” That dynamic, he added, has led Fibr AI to sign three- to five-year contracts with large enterprises, which tend to treat website infrastructure as something to standardize rather than continuously revisit.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;At a technical level, Fibr AI operates as a layer on top of an existing website, connecting to a company’s advertising, analytics, and customer data systems to understand how visitors arrive and what they are likely looking for. Its AI agents then assemble and adjust page content, such as copy, imagery, and layout, treating each URL as a system that learns and optimizes continuously rather than a fixed page. Instead of relying on manually configured rules or sequential A/B tests, the platform runs large numbers of micro-experiments in parallel and updates experiences systematically as traffic flows in from different channels.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3089322" height="1080" src="https://techcrunch.com/wp-content/uploads/2026/02/fibr-ai-personalization_b82c81.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Fibr AI personalizes webpages using AI agents&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Fibr AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That shift has direct cost implications for large enterprises. Traditional website personalization typically combines software licenses with agency retainers and engineering time, tying costs to people rather than outcomes. Goyal said enterprises are increasingly evaluating Fibr AI’s platform based on cost per experiment and conversion impact, rather than the number of tools or people involved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Accel, that operating model — rather than the AI buzz — was central to the decision to invest again. “Advertising today is one-to-one, but when users land on a website it becomes one-to-many,” said Prayank Swaroop, a partner at Accel. “You can create hundreds of ads for different audiences, but they all still land on the same page.” Fibr’s ability to turn that dynamic into one-to-one personalization, he said, stood out because it removed the agency and engineering bottlenecks that typically limit how far enterprises can push experimentation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Swaroop added that early enterprise adoption, particularly among banks and healthcare companies, helped validate the thesis. “These are regulated, conservative industries,” he said. “When they start saying, ‘We need this, and we’re willing to pay for it,’ that’s when we feel confident doubling down.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-future-proofing-for-the-agentic-commerce-era"&gt;Future-proofing for the agentic-commerce era&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While most of Fibr AI’s business today is driven by personalizing experiences for human visitors, Accel and Fibr AI also see potential in how AI agents are beginning to mediate online discovery. As users increasingly research, compare, and shortlist products using large language models and AI chatbots, including OpenAI’s ChatGPT, before visiting a website, Swaroop said, the ability for sites to adapt based on what a visitor — or an AI system acting on their behalf — already knows could become more important over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That part is still early,” Swaroop said, “but the companies building for today’s needs while being ready for that shift tomorrow are the ones we want to back.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3089266" height="984" src="https://techcrunch.com/wp-content/uploads/2026/02/fibr-ai-llm-optimization.jpg" width="1298" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Fibr AI’s dynamic experience for discovery through LLMs and AI chatbots&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Fibr AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new funding, Fibr AI plans to focus on expanding its sales and customer-facing teams in the U.S., while continuing to build out its technical base in India. The San Francisco-headquartered startup maintains an office in Bengaluru, with 17 of its roughly 23 employees based in India and the remaining six in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Goyal said the startup targets about $5 million in annual recurring revenue by the end of this year and around 50 enterprise customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fibr AI is entering a space long dominated by incumbents such as Adobe and Optimizely, which offer experimentation and personalization tools to large enterprises. But both Goyal and Swaroop argued that those platforms are constrained by how they are built and sold, typically relying on marketing agencies and engineering teams to configure and operate them. That model, they said, makes it difficult to move quickly or scale experimentation, even as customer acquisition and messaging have become increasingly dynamic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Incumbents have been slow in bringing out products,” Swaroop said, adding that even when new features arrive, they often come years after demand has shifted.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/accel-doubles-down-on-fibr-ai-as-agents-turn-static-websites-into-one-to-one-experiences/</guid><pubDate>Wed, 04 Feb 2026 13:00:00 +0000</pubDate></item><item><title>[NEW] The Download: the future of nuclear power plants, and social media-fueled AI hype (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/04/1132115/the-download-the-future-of-nuclear-power-plants-and-social-media-fueled-ai-hype/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why AI companies are betting on next-gen nuclear&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;AI is driving unprecedented investment for massive data centers and an energy supply that can support its huge computational appetite. One potential source of electricity for these facilities is next-generation nuclear power plants, which could be cheaper to construct and safer to operate than their predecessors.&lt;/p&gt;&lt;p&gt;We recently held a subscriber-exclusive Roundtables discussion on hyperscale AI data centers and next-gen nuclear—two featured technologies on the MIT Technology Review 10 Breakthrough Technologies of 2026 list. You can watch the conversation back here, and don’t forget to subscribe to make sure you catch future discussions as they happen.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How social media encourages the worst of AI boosterism&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Demis Hassabis, CEO of Google DeepMind, summed it up in three words: “This is embarrassing.”&lt;/p&gt;&lt;p&gt;Hassabis was replying on X to an overexcited post by Sébastien Bubeck, a research scientist at the rival firm OpenAI, announcing that two mathematicians had used OpenAI’s latest large language model, GPT-5, to find solutions to 10 unsolved problems in mathematics.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Put your math hats on for a minute, and let’s take a look at what this beef from mid-October was about. It’s a perfect example of what’s wrong with AI right now.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The paints, coatings, and chemicals making the world a cooler place&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;It’s getting harder to beat the heat. During the summer of 2025, heat waves knocked out power grids in North America, Europe, and the Middle East. Global warming means more people need air-­conditioning, which requires more power and strains grids.&lt;/p&gt;&lt;p&gt;But a millennia-old idea (plus 21st-century tech) might offer an answer: radiative cooling. Paints, coatings, and textiles can scatter sunlight and dissipate heat—no additional energy required. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—Becky Ferreira&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the most recent print issue of&lt;em&gt; MIT Technology Review&lt;/em&gt; magazine, which shines a light on the exciting innovations happening right now. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: China figured out how to sell EVs. Now it has to deal with their aging batteries.&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;As early electric cars age out, hundreds of thousands of used batteries are flooding the market, fueling a gray recycling economy even as Beijing and big manufacturers scramble to build a more orderly system.&lt;/p&gt;  &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Europe is edging closer towards banning social media for minors&lt;/strong&gt;&lt;br /&gt;Spain has become the latest country to consider it. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Elon Musk called the Spanish prime minister a “tyrant” in retaliation. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Other European nations considering restrictions include Greece, France and the UK. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Humans are infiltrating the social network for AI agents&lt;/strong&gt;&lt;br /&gt;It turns out role-playing as a bot is surprisingly fun. (Wired $)&lt;br /&gt;+ &lt;em&gt;Some of the most viral posts may actually be human-generated after all. &lt;/em&gt;(The Verge)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Russian spy spacecraft have intercepted Europe’s key satellites&lt;/strong&gt;&lt;br /&gt;Security officials are confident Moscow has tapped into unencrypted European comms. (FT $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 French authorities raided X’s Paris office&lt;/strong&gt;&lt;br /&gt;They’re investigating a range of potential charges against the company. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Elon Musk has been summoned to give evidence in April. &lt;/em&gt;(Reuters)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Jeffrey Epstein invested millions into crypto startup Coinbase&lt;/strong&gt;&lt;br /&gt;Which suggests he was still able to take advantage of Silicon Valley investment opportunities years after pleading guilty to soliciting sex from an underage girl. (WP $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 A group of crypto bros paid $300,000 for a gold statue of Trump&lt;/strong&gt;&lt;br /&gt;It’s destined to be installed on his Florida golf complex, apparently. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 OpenAI has appointed a “head of preparedness”&lt;br /&gt;&lt;/strong&gt;Dylan Scandinaro will earn a cool $555,000 for his troubles. (Bloomberg $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 The eternal promise of 3D-printed batteries&lt;br /&gt;&lt;/strong&gt;Traditional batteries are blocky and bulky. Printing them ourselves could help solve that. (IEEE Spectrum)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What snow can teach us about city design&lt;/strong&gt;&lt;br /&gt;When icy mounds refuse to melt, they show us what a less car-focused city could look like. (New Yorker $)&lt;br /&gt;+ &lt;em&gt;This startup thinks slime mold can help us design better cities. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;10 Please don’t use AI to talk to your friends&lt;/strong&gt;&lt;br /&gt;That’s what your brain is for. (The Atlantic $)&lt;br /&gt;+ &lt;em&gt;Therapists are secretly using ChatGPT. Clients are triggered. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Today, our children are exposed to a space they were never meant to navigate alone. We will no longer accept that.”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Spanish prime minister Pedro Sánchez proposes a social media ban for children aged under 16 in the country, following in Australia’s footsteps, AP News reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1132118" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_20e8ae.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;A brain implant changed her life. Then it was removed against her will.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Sticking an electrode inside a person’s brain can do more than treat a disease. Take the case of Rita Leggett, an Australian woman whose experimental brain implant designed to help people with epilepsy changed her sense of agency and self.&lt;/p&gt;&lt;p&gt;Leggett told researchers that she “became one” with her device. It helped her to control the unpredictable, violent seizures she routinely experienced, and allowed her to take charge of her own life. So she was devastated when, two years later, she was told she had to remove the implant because the company that made it had gone bust.&lt;/p&gt;&lt;p&gt;The removal of this implant, and others like it, might represent a breach of human rights, ethicists say in a paper published earlier this month. And the issue will only become more pressing as the brain implant market grows in the coming years and more people receive devices like Leggett’s. Read the full story.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Why Beethoven’s Ode to Joy is still such an undisputed banger.&lt;br /&gt;+ Did you know that one of the world’s most famous prisons actually served as a zoo and menagerie for over 600 years?&lt;br /&gt;+ Banana nut muffins sound like a fantastic way to start your day.&lt;br /&gt;+ 2026 is shaping up to be a blockbuster year for horror films.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why AI companies are betting on next-gen nuclear&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;AI is driving unprecedented investment for massive data centers and an energy supply that can support its huge computational appetite. One potential source of electricity for these facilities is next-generation nuclear power plants, which could be cheaper to construct and safer to operate than their predecessors.&lt;/p&gt;&lt;p&gt;We recently held a subscriber-exclusive Roundtables discussion on hyperscale AI data centers and next-gen nuclear—two featured technologies on the MIT Technology Review 10 Breakthrough Technologies of 2026 list. You can watch the conversation back here, and don’t forget to subscribe to make sure you catch future discussions as they happen.&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How social media encourages the worst of AI boosterism&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Demis Hassabis, CEO of Google DeepMind, summed it up in three words: “This is embarrassing.”&lt;/p&gt;&lt;p&gt;Hassabis was replying on X to an overexcited post by Sébastien Bubeck, a research scientist at the rival firm OpenAI, announcing that two mathematicians had used OpenAI’s latest large language model, GPT-5, to find solutions to 10 unsolved problems in mathematics.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Put your math hats on for a minute, and let’s take a look at what this beef from mid-October was about. It’s a perfect example of what’s wrong with AI right now.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Will Douglas Heaven&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The paints, coatings, and chemicals making the world a cooler place&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;It’s getting harder to beat the heat. During the summer of 2025, heat waves knocked out power grids in North America, Europe, and the Middle East. Global warming means more people need air-­conditioning, which requires more power and strains grids.&lt;/p&gt;&lt;p&gt;But a millennia-old idea (plus 21st-century tech) might offer an answer: radiative cooling. Paints, coatings, and textiles can scatter sunlight and dissipate heat—no additional energy required. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;—Becky Ferreira&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the most recent print issue of&lt;em&gt; MIT Technology Review&lt;/em&gt; magazine, which shines a light on the exciting innovations happening right now. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: China figured out how to sell EVs. Now it has to deal with their aging batteries.&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;As early electric cars age out, hundreds of thousands of used batteries are flooding the market, fueling a gray recycling economy even as Beijing and big manufacturers scramble to build a more orderly system.&lt;/p&gt;  &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Europe is edging closer towards banning social media for minors&lt;/strong&gt;&lt;br /&gt;Spain has become the latest country to consider it. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Elon Musk called the Spanish prime minister a “tyrant” in retaliation. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Other European nations considering restrictions include Greece, France and the UK. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Humans are infiltrating the social network for AI agents&lt;/strong&gt;&lt;br /&gt;It turns out role-playing as a bot is surprisingly fun. (Wired $)&lt;br /&gt;+ &lt;em&gt;Some of the most viral posts may actually be human-generated after all. &lt;/em&gt;(The Verge)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Russian spy spacecraft have intercepted Europe’s key satellites&lt;/strong&gt;&lt;br /&gt;Security officials are confident Moscow has tapped into unencrypted European comms. (FT $)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 French authorities raided X’s Paris office&lt;/strong&gt;&lt;br /&gt;They’re investigating a range of potential charges against the company. (WSJ $)&lt;br /&gt;+ &lt;em&gt;Elon Musk has been summoned to give evidence in April. &lt;/em&gt;(Reuters)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Jeffrey Epstein invested millions into crypto startup Coinbase&lt;/strong&gt;&lt;br /&gt;Which suggests he was still able to take advantage of Silicon Valley investment opportunities years after pleading guilty to soliciting sex from an underage girl. (WP $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 A group of crypto bros paid $300,000 for a gold statue of Trump&lt;/strong&gt;&lt;br /&gt;It’s destined to be installed on his Florida golf complex, apparently. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 OpenAI has appointed a “head of preparedness”&lt;br /&gt;&lt;/strong&gt;Dylan Scandinaro will earn a cool $555,000 for his troubles. (Bloomberg $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 The eternal promise of 3D-printed batteries&lt;br /&gt;&lt;/strong&gt;Traditional batteries are blocky and bulky. Printing them ourselves could help solve that. (IEEE Spectrum)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 What snow can teach us about city design&lt;/strong&gt;&lt;br /&gt;When icy mounds refuse to melt, they show us what a less car-focused city could look like. (New Yorker $)&lt;br /&gt;+ &lt;em&gt;This startup thinks slime mold can help us design better cities. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;10 Please don’t use AI to talk to your friends&lt;/strong&gt;&lt;br /&gt;That’s what your brain is for. (The Atlantic $)&lt;br /&gt;+ &lt;em&gt;Therapists are secretly using ChatGPT. Clients are triggered. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Today, our children are exposed to a space they were never meant to navigate alone. We will no longer accept that.”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Spanish prime minister Pedro Sánchez proposes a social media ban for children aged under 16 in the country, following in Australia’s footsteps, AP News reports.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1132118" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_20e8ae.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;A brain implant changed her life. Then it was removed against her will.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Sticking an electrode inside a person’s brain can do more than treat a disease. Take the case of Rita Leggett, an Australian woman whose experimental brain implant designed to help people with epilepsy changed her sense of agency and self.&lt;/p&gt;&lt;p&gt;Leggett told researchers that she “became one” with her device. It helped her to control the unpredictable, violent seizures she routinely experienced, and allowed her to take charge of her own life. So she was devastated when, two years later, she was told she had to remove the implant because the company that made it had gone bust.&lt;/p&gt;&lt;p&gt;The removal of this implant, and others like it, might represent a breach of human rights, ethicists say in a paper published earlier this month. And the issue will only become more pressing as the brain implant market grows in the coming years and more people receive devices like Leggett’s. Read the full story.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Why Beethoven’s Ode to Joy is still such an undisputed banger.&lt;br /&gt;+ Did you know that one of the world’s most famous prisons actually served as a zoo and menagerie for over 600 years?&lt;br /&gt;+ Banana nut muffins sound like a fantastic way to start your day.&lt;br /&gt;+ 2026 is shaping up to be a blockbuster year for horror films.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/04/1132115/the-download-the-future-of-nuclear-power-plants-and-social-media-fueled-ai-hype/</guid><pubDate>Wed, 04 Feb 2026 13:10:00 +0000</pubDate></item><item><title>[NEW] From guardrails to governance: A CEO’s guide for securing agentic systems (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/04/1131014/from-guardrails-to-governance-a-ceos-guide-for-securing-agentic-systems/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;Protegrity&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The previous article in this series, “Rules fail at the prompt, succeed at the boundary,” focused on the first AI-orchestrated espionage campaign and the failure of prompt-level control. This article is the prescription. The question every CEO is now getting from their board is some version of: &lt;em&gt;What do we do about agent risk?&lt;/em&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1131031" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/Protegrity-article-2-iStock-536974739.png" /&gt;&lt;/figure&gt;  &lt;p&gt;Across recent AI security guidance from standards bodies, regulators, and major providers, a simple idea keeps repeating: treat agents like powerful, semi-autonomous users, and enforce rules at the boundaries where they touch identity, tools, data, and outputs.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The following is an actionable eight-step plan one can ask teams to implement and report against: &amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1131740" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/steps-image-v6.png" /&gt;&lt;figcaption class="wp-element-caption"&gt;Eight controls, three pillars: govern agentic systems at the boundary. Source: Protegrity&lt;/figcaption&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Constrain capabilities&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;These steps help define identity and limit capabilities.&lt;/p&gt; 
 &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;1. Identity and scope: Make agents real users with narrow jobs&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Today, agents run under vague, over-privileged service identities. The fix is straightforward: treat each agent as a non-human principal with the same discipline applied to employees.&lt;/p&gt;&lt;p&gt;Every agent should run as the requesting user in the correct tenant, with permissions constrained to that user’s role and geography. Prohibit cross-tenant on-behalf-of shortcuts. Anything high-impact should require explicit human approval with a recorded rationale. That is how Google’s Secure AI Framework (SAIF) and NIST AI’s access-control guidance are meant to be applied in practice.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question: Can we show, today, a list of our agents and exactly what each is allowed to do?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. Tooling control: Pin, approve, and bound what agents can use&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;The Anthropic espionage framework worked because the attackers could wire Claude into a flexible suite of tools (e.g., scanners, exploit frameworks, data parsers) through Model Context Protocol, and those tools weren’t pinned or policy-gated.&lt;/p&gt;  &lt;p&gt;The defense is to treat toolchains like a supply chain:&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;Pin versions of remote tool servers.&lt;/li&gt;    &lt;li&gt;Require approvals for adding new tools, scopes, or data sources.&lt;/li&gt;    &lt;li&gt;Forbid automatic tool-chaining unless a policy explicitly allows it.&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;This is exactly what OWASP flags under excessive agency and what it recommends protecting against. Under the &lt;strong&gt;EU AI Act&lt;/strong&gt;&lt;strong&gt;,&lt;/strong&gt; designing for such cyber-resilience and misuse resistance is part of the Article 15 obligation to ensure robustness and cybersecurity.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question:&lt;/em&gt;&lt;em&gt; &lt;/em&gt;&lt;em&gt;Who signs off when an agent gains a new tool or a broader scope? How does one know?&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;3. Permissions by design: Bind tools to tasks, not to models&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;A common anti-pattern is to give the model a long-lived credential and hope prompts keep it polite. SAIF and NIST argue the opposite: credentials and scopes should be bound to tools and tasks, rotated regularly, and auditable. Agents then request narrowly scoped capabilities through those tools.&lt;/p&gt;  &lt;p&gt;In practice, that looks like: “finance-ops-agent may read, but not write, certain ledgers without CFO approval.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question:&lt;/em&gt;&lt;em&gt; &lt;/em&gt;&lt;em&gt;Can we revoke a specific capability from an agent without re-architecting the whole system?&lt;/em&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Control data and behavior&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;These steps gate inputs, outputs, and constrain behavior.&lt;/p&gt; 

 &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;4. Inputs, memory, and RAG: Treat external content as hostile until proven otherwise&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Most agent incidents start with sneaky data: a poisoned web page, PDF, email, or repository that smuggles adversarial instructions into the system. OWASP’s prompt-injection cheat sheet and OpenAI’s own guidance both insist on strict separation of system instructions from user content and on treating unvetted retrieval sources as untrusted.&lt;/p&gt;  &lt;p&gt;Operationally, gate before anything enters retrieval or long-term memory: new sources are reviewed, tagged, and onboarded; persistent memory is disabled when untrusted context is present; provenance is attached to each chunk.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question: &lt;/em&gt;&lt;em&gt;Can we enumerate every external content source our agents learn from, and who approved them?&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;5. Output handling and rendering: Nothing executes “just because the model said so”&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;In the Anthropic case, AI-generated exploit code and credential dumps flowed straight into action. Any output that can cause a side effect needs a validator between the agent and the real world. OWASP’s insecure output handling category is explicit on this point, as are browser security best practices around origin boundaries.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;The CEO question:&lt;/em&gt; &lt;em&gt;Where, in our architecture, are agent outputs assessed before they run or ship to customers?&lt;/em&gt;&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;6. Data privacy at runtime: Protect the data first, then the model&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Protect the data such that there is nothing dangerous to reveal by default. NIST and SAIF both lean toward “secure-by-default” designs where sensitive values are tokenized or masked and only re-hydrated for authorized users and use cases.&lt;/p&gt;  &lt;p&gt;In agentic systems, that means policy-controlled detokenization at the output boundary and logging every reveal. If an agent is fully compromised, the blast radius is bounded by what the policy lets it see.&lt;/p&gt;  &lt;p&gt;This is where the AI stack intersects not just with the EU AI Act but with GDPR and sector-specific regimes. The EU AI Act expects providers and deployers to manage AI-specific risk; runtime tokenization and policy-gated reveal are strong evidence that one is actively controlling those risks in production.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;The CEO question: &lt;/em&gt;&lt;em&gt;When our agents touch regulated data, is that protection enforced by architecture or by promises?&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Prove governance and resilience&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;For the final steps, it’s important to show controls work and keep working.&lt;/p&gt; 
 &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;7. Continuous evaluation: Don’t ship a one-time test, ship a test harness&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Anthropic’s research about sleeper agents should eliminate all fantasies about single test dreams and show how critical continuous evaluation is. This means instrumenting agents with deep observability, regularly red teaming with adversarial test suites, and backing everything with robust logging and evidence, so failures become both regression tests and enforceable policy updates.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question: Who works to break our agents every week, and how do their findings change policy?&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;&amp;nbsp;8. Governance, inventory, and audit: Keep score in one place&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;AI security frameworks emphasize inventory and evidence: enterprises must know which models, prompts, tools, datasets, and vector stores they have, who owns them, and what decisions were taken about risk.&lt;/p&gt;  &lt;p&gt;For agents, that means a living catalog and unified logs:&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;Which agents exist, on which platforms&lt;/li&gt;    &lt;li&gt;What scopes, tools, and data each is allowed&lt;/li&gt;    &lt;li&gt;Every approval, detokenization, and high-impact action, with who approved it and when&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;em&gt;The CEO question: &lt;/em&gt;&lt;em&gt;If asked how an agent made a specific decision, could we reconstruct the chain?&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;And don’t forget the system-level threat model: assume the threat actor GTG-1002 is already in your enterprise. To complete enterprise preparedness, zoom out and consider the MITRE ATLAS product, which exists precisely because adversaries attack systems, not models. Anthropic provides a case study of a state-based threat actor (GTG-1002) doing exactly that with an agentic framework.&lt;/p&gt; 
 &lt;p&gt;Taken together, these controls do not make agents magically safe. They do something more familiar and more reliable: they put AI, its access, and actions back inside the same security frame used for any powerful user or system.&lt;/p&gt;  &lt;p&gt;For boards and CEOs, the question is no longer “Do we have good AI guardrails?” It’s: Can we answer the CEO questions above with evidence, not assurances?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Protegrity. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;Provided by&lt;/span&gt;Protegrity&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The previous article in this series, “Rules fail at the prompt, succeed at the boundary,” focused on the first AI-orchestrated espionage campaign and the failure of prompt-level control. This article is the prescription. The question every CEO is now getting from their board is some version of: &lt;em&gt;What do we do about agent risk?&lt;/em&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1131031" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/Protegrity-article-2-iStock-536974739.png" /&gt;&lt;/figure&gt;  &lt;p&gt;Across recent AI security guidance from standards bodies, regulators, and major providers, a simple idea keeps repeating: treat agents like powerful, semi-autonomous users, and enforce rules at the boundaries where they touch identity, tools, data, and outputs.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The following is an actionable eight-step plan one can ask teams to implement and report against: &amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1131740" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/steps-image-v6.png" /&gt;&lt;figcaption class="wp-element-caption"&gt;Eight controls, three pillars: govern agentic systems at the boundary. Source: Protegrity&lt;/figcaption&gt;&lt;/figure&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Constrain capabilities&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;These steps help define identity and limit capabilities.&lt;/p&gt; 
 &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;1. Identity and scope: Make agents real users with narrow jobs&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Today, agents run under vague, over-privileged service identities. The fix is straightforward: treat each agent as a non-human principal with the same discipline applied to employees.&lt;/p&gt;&lt;p&gt;Every agent should run as the requesting user in the correct tenant, with permissions constrained to that user’s role and geography. Prohibit cross-tenant on-behalf-of shortcuts. Anything high-impact should require explicit human approval with a recorded rationale. That is how Google’s Secure AI Framework (SAIF) and NIST AI’s access-control guidance are meant to be applied in practice.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question: Can we show, today, a list of our agents and exactly what each is allowed to do?&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2. Tooling control: Pin, approve, and bound what agents can use&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;The Anthropic espionage framework worked because the attackers could wire Claude into a flexible suite of tools (e.g., scanners, exploit frameworks, data parsers) through Model Context Protocol, and those tools weren’t pinned or policy-gated.&lt;/p&gt;  &lt;p&gt;The defense is to treat toolchains like a supply chain:&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;Pin versions of remote tool servers.&lt;/li&gt;    &lt;li&gt;Require approvals for adding new tools, scopes, or data sources.&lt;/li&gt;    &lt;li&gt;Forbid automatic tool-chaining unless a policy explicitly allows it.&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;This is exactly what OWASP flags under excessive agency and what it recommends protecting against. Under the &lt;strong&gt;EU AI Act&lt;/strong&gt;&lt;strong&gt;,&lt;/strong&gt; designing for such cyber-resilience and misuse resistance is part of the Article 15 obligation to ensure robustness and cybersecurity.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question:&lt;/em&gt;&lt;em&gt; &lt;/em&gt;&lt;em&gt;Who signs off when an agent gains a new tool or a broader scope? How does one know?&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;3. Permissions by design: Bind tools to tasks, not to models&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;A common anti-pattern is to give the model a long-lived credential and hope prompts keep it polite. SAIF and NIST argue the opposite: credentials and scopes should be bound to tools and tasks, rotated regularly, and auditable. Agents then request narrowly scoped capabilities through those tools.&lt;/p&gt;  &lt;p&gt;In practice, that looks like: “finance-ops-agent may read, but not write, certain ledgers without CFO approval.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question:&lt;/em&gt;&lt;em&gt; &lt;/em&gt;&lt;em&gt;Can we revoke a specific capability from an agent without re-architecting the whole system?&lt;/em&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Control data and behavior&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;These steps gate inputs, outputs, and constrain behavior.&lt;/p&gt; 

 &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;4. Inputs, memory, and RAG: Treat external content as hostile until proven otherwise&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Most agent incidents start with sneaky data: a poisoned web page, PDF, email, or repository that smuggles adversarial instructions into the system. OWASP’s prompt-injection cheat sheet and OpenAI’s own guidance both insist on strict separation of system instructions from user content and on treating unvetted retrieval sources as untrusted.&lt;/p&gt;  &lt;p&gt;Operationally, gate before anything enters retrieval or long-term memory: new sources are reviewed, tagged, and onboarded; persistent memory is disabled when untrusted context is present; provenance is attached to each chunk.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question: &lt;/em&gt;&lt;em&gt;Can we enumerate every external content source our agents learn from, and who approved them?&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;5. Output handling and rendering: Nothing executes “just because the model said so”&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;In the Anthropic case, AI-generated exploit code and credential dumps flowed straight into action. Any output that can cause a side effect needs a validator between the agent and the real world. OWASP’s insecure output handling category is explicit on this point, as are browser security best practices around origin boundaries.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;The CEO question:&lt;/em&gt; &lt;em&gt;Where, in our architecture, are agent outputs assessed before they run or ship to customers?&lt;/em&gt;&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;6. Data privacy at runtime: Protect the data first, then the model&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Protect the data such that there is nothing dangerous to reveal by default. NIST and SAIF both lean toward “secure-by-default” designs where sensitive values are tokenized or masked and only re-hydrated for authorized users and use cases.&lt;/p&gt;  &lt;p&gt;In agentic systems, that means policy-controlled detokenization at the output boundary and logging every reveal. If an agent is fully compromised, the blast radius is bounded by what the policy lets it see.&lt;/p&gt;  &lt;p&gt;This is where the AI stack intersects not just with the EU AI Act but with GDPR and sector-specific regimes. The EU AI Act expects providers and deployers to manage AI-specific risk; runtime tokenization and policy-gated reveal are strong evidence that one is actively controlling those risks in production.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;The CEO question: &lt;/em&gt;&lt;em&gt;When our agents touch regulated data, is that protection enforced by architecture or by promises?&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Prove governance and resilience&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;For the final steps, it’s important to show controls work and keep working.&lt;/p&gt; 
 &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;7. Continuous evaluation: Don’t ship a one-time test, ship a test harness&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Anthropic’s research about sleeper agents should eliminate all fantasies about single test dreams and show how critical continuous evaluation is. This means instrumenting agents with deep observability, regularly red teaming with adversarial test suites, and backing everything with robust logging and evidence, so failures become both regression tests and enforceable policy updates.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The CEO question: Who works to break our agents every week, and how do their findings change policy?&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;&amp;nbsp;8. Governance, inventory, and audit: Keep score in one place&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;AI security frameworks emphasize inventory and evidence: enterprises must know which models, prompts, tools, datasets, and vector stores they have, who owns them, and what decisions were taken about risk.&lt;/p&gt;  &lt;p&gt;For agents, that means a living catalog and unified logs:&lt;/p&gt;  &lt;ul class="wp-block-list"&gt; &lt;li&gt;Which agents exist, on which platforms&lt;/li&gt;    &lt;li&gt;What scopes, tools, and data each is allowed&lt;/li&gt;    &lt;li&gt;Every approval, detokenization, and high-impact action, with who approved it and when&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;em&gt;The CEO question: &lt;/em&gt;&lt;em&gt;If asked how an agent made a specific decision, could we reconstruct the chain?&lt;/em&gt;&lt;em&gt;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;And don’t forget the system-level threat model: assume the threat actor GTG-1002 is already in your enterprise. To complete enterprise preparedness, zoom out and consider the MITRE ATLAS product, which exists precisely because adversaries attack systems, not models. Anthropic provides a case study of a state-based threat actor (GTG-1002) doing exactly that with an agentic framework.&lt;/p&gt; 
 &lt;p&gt;Taken together, these controls do not make agents magically safe. They do something more familiar and more reliable: they put AI, its access, and actions back inside the same security frame used for any powerful user or system.&lt;/p&gt;  &lt;p&gt;For boards and CEOs, the question is no longer “Do we have good AI guardrails?” It’s: Can we answer the CEO questions above with evidence, not assurances?&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Protegrity. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/04/1131014/from-guardrails-to-governance-a-ceos-guide-for-securing-agentic-systems/</guid><pubDate>Wed, 04 Feb 2026 14:00:00 +0000</pubDate></item><item><title>[NEW] Alexa+, Amazon’s AI assistant, is now available to everyone in the US (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/alexa-amazons-ai-assistant-is-now-available-to-everyone-in-the-u-s/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Alexa+, Amazon’s upgraded, generative AI-powered version of its Alexa assistant, is available to all U.S. customers as of Wednesday. The company said that the AI feature will be free to Prime members across devices. Meanwhile, anyone can use Alexa+ for free via the Alexa website or mobile app, with some limitations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have tens of millions of customers using Alexa+ now, and now we’re going to make it available to all Prime members…Prime members enjoy unlimited access — it’s basically a paid-tier level of access that we’re including in Prime now,” notes Daniel Rausch, VP of Alexa and Echo at Amazon, in an interview with TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Announced last year, Alexa+ is model agnostic — meaning it runs on a combination of Amazon’s own foundation models and those from other companies, allowing the assistant to do more than the basic tasks of its predecessor using whatever AI technology is best for the job. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3034655" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/alexa-devices.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As an AI assistant, Alexa+ can carry on natural language conversations that include follow-up questions and back-and-forth chat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to performing smart home tasks, scheduling timers, or offering news and weather, as before, the new assistant can do most things that other AI chatbots can do — like planning an itinerary for a trip, updating a shared calendar, finding and saving recipes to a library, making movie recommendations, helping with homework, exploring a topic, and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, integrations with services like Ticketmaster, Thumbtack, Uber, Angi,&amp;nbsp;Expedia,&amp;nbsp;Square,&amp;nbsp;Yelp, Fodor’s, OpenTable, and Suno will allow Alexa to perform more complex tasks, like scheduling a dinner reservation or requesting an Uber ride. Amazon has not yet shared user adoption numbers on this more “agentic” use case (where the AI acts autonomously to complete tasks) for the AI helper.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During its year-long beta testing period, Alexa customers had the option to try the AI feature or roll back to the prior version. The company tells TechCrunch the option to revert to the old Alexa will continue to be available, but couldn’t say for how long. Likely, Amazon wants a bit more time to improve the AI experience before making it a requirement for users. The percentage of those opting out is also a key metric to track, but Rausch notes that the figure is in the low single digits, suggesting that most customers are not so unhappy with Alexa+ that they’re giving up. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Alexa.com" class="wp-image-3079553" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Alexa.com_.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Amazon has had to work to resolve bugs and address user feedback ahead of this launch. Some beta testers complained that Alexa+ was too chatty, or interrupted at the wrong times, for instance. Others complained about Alexa’s new voice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon has taken in this feedback and made changes over time. For instance, the company revised the onboarding experience to have Alexa explain how to change her voice, as some preferred Alexa’s “OG” voice. (That voice is still available as Alexa+ voice No. 2, but it now uses AI to add more inflection.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Eventually, we had her use her new version of her old voice, and then switch back again, just to show customers,” says Rausch, describing the changes the team made to onboarding. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In another example, Amazon tried to make Alexa less prone to unwanted interruptions. Now, Alexa will ask, “Is that for me?” when the AI is unsure who is being addressed. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Alexa app" class="wp-image-3079554" height="680" src="https://techcrunch.com/wp-content/uploads/2026/01/IMG_0422.jpg?w=335" width="335" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Screenshot of the new Alexa app&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rausch points out that the overall experience is configurable, too. If customers don’t want the follow-on mode, for instance — which allows Alexa to continue listening after responding — they can turn it off.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked if users will be able to change the AI assistant’s personality, as in other AI chatbots, where the AI can be set to be personal, professional, quirky, nerdy, and more,  Rausch says simply, “Stay tuned.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the beta, Amazon reported positive adoption trends in terms of both usage and engagement, with few customers opting for rollbacks. Music streams increased by 25% after customers upgraded to Alexa+, and more customers are engaging deeply with recipes, a feature that has seen 5x growth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, customers are having two to three times more conversations with Alexa+, compared with the original Alexa.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Alexa+ will be free to U.S. Prime members, non-Prime customers could opt to pay $19.99/month for standalone access — a price that’s comparable to something like ChatGPT Plus. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon notes the free experience on web and mobile will have some limits, but these are mainly in place to protect against abuse. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think we’ve got some great, generous limits. We’re not talking about exactly what they are today, but…there are some [limits],” Rausch says. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Alexa+ experience in the U.S. will be available across Alexa devices, including Echo products, Fire TV, Alexa.com, the Alexa mobile app, and Alexa-enabled devices from partners including Samsung, Bose, and others, with more to come. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Alexa+, Amazon’s upgraded, generative AI-powered version of its Alexa assistant, is available to all U.S. customers as of Wednesday. The company said that the AI feature will be free to Prime members across devices. Meanwhile, anyone can use Alexa+ for free via the Alexa website or mobile app, with some limitations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have tens of millions of customers using Alexa+ now, and now we’re going to make it available to all Prime members…Prime members enjoy unlimited access — it’s basically a paid-tier level of access that we’re including in Prime now,” notes Daniel Rausch, VP of Alexa and Echo at Amazon, in an interview with TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Announced last year, Alexa+ is model agnostic — meaning it runs on a combination of Amazon’s own foundation models and those from other companies, allowing the assistant to do more than the basic tasks of its predecessor using whatever AI technology is best for the job. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3034655" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/alexa-devices.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As an AI assistant, Alexa+ can carry on natural language conversations that include follow-up questions and back-and-forth chat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to performing smart home tasks, scheduling timers, or offering news and weather, as before, the new assistant can do most things that other AI chatbots can do — like planning an itinerary for a trip, updating a shared calendar, finding and saving recipes to a library, making movie recommendations, helping with homework, exploring a topic, and more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, integrations with services like Ticketmaster, Thumbtack, Uber, Angi,&amp;nbsp;Expedia,&amp;nbsp;Square,&amp;nbsp;Yelp, Fodor’s, OpenTable, and Suno will allow Alexa to perform more complex tasks, like scheduling a dinner reservation or requesting an Uber ride. Amazon has not yet shared user adoption numbers on this more “agentic” use case (where the AI acts autonomously to complete tasks) for the AI helper.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During its year-long beta testing period, Alexa customers had the option to try the AI feature or roll back to the prior version. The company tells TechCrunch the option to revert to the old Alexa will continue to be available, but couldn’t say for how long. Likely, Amazon wants a bit more time to improve the AI experience before making it a requirement for users. The percentage of those opting out is also a key metric to track, but Rausch notes that the figure is in the low single digits, suggesting that most customers are not so unhappy with Alexa+ that they’re giving up. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Alexa.com" class="wp-image-3079553" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Alexa.com_.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Amazon has had to work to resolve bugs and address user feedback ahead of this launch. Some beta testers complained that Alexa+ was too chatty, or interrupted at the wrong times, for instance. Others complained about Alexa’s new voice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon has taken in this feedback and made changes over time. For instance, the company revised the onboarding experience to have Alexa explain how to change her voice, as some preferred Alexa’s “OG” voice. (That voice is still available as Alexa+ voice No. 2, but it now uses AI to add more inflection.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Eventually, we had her use her new version of her old voice, and then switch back again, just to show customers,” says Rausch, describing the changes the team made to onboarding. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In another example, Amazon tried to make Alexa less prone to unwanted interruptions. Now, Alexa will ask, “Is that for me?” when the AI is unsure who is being addressed. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Alexa app" class="wp-image-3079554" height="680" src="https://techcrunch.com/wp-content/uploads/2026/01/IMG_0422.jpg?w=335" width="335" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Screenshot of the new Alexa app&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rausch points out that the overall experience is configurable, too. If customers don’t want the follow-on mode, for instance — which allows Alexa to continue listening after responding — they can turn it off.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked if users will be able to change the AI assistant’s personality, as in other AI chatbots, where the AI can be set to be personal, professional, quirky, nerdy, and more,  Rausch says simply, “Stay tuned.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During the beta, Amazon reported positive adoption trends in terms of both usage and engagement, with few customers opting for rollbacks. Music streams increased by 25% after customers upgraded to Alexa+, and more customers are engaging deeply with recipes, a feature that has seen 5x growth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, customers are having two to three times more conversations with Alexa+, compared with the original Alexa.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Alexa+ will be free to U.S. Prime members, non-Prime customers could opt to pay $19.99/month for standalone access — a price that’s comparable to something like ChatGPT Plus. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon notes the free experience on web and mobile will have some limits, but these are mainly in place to protect against abuse. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think we’ve got some great, generous limits. We’re not talking about exactly what they are today, but…there are some [limits],” Rausch says. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Alexa+ experience in the U.S. will be available across Alexa devices, including Echo products, Fire TV, Alexa.com, the Alexa mobile app, and Alexa-enabled devices from partners including Samsung, Bose, and others, with more to come. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/alexa-amazons-ai-assistant-is-now-available-to-everyone-in-the-u-s/</guid><pubDate>Wed, 04 Feb 2026 14:00:00 +0000</pubDate></item><item><title>[NEW] Nemotron ColEmbed V2: Raising the Bar for Multimodal Retrieval with ViDoRe V3’s Top Model (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/nemotron-colembed-v2</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Ronay Ak's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/d9fa8404f258c96df1c500cffd10752f.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
Modern search systems are increasingly designed to process heterogeneous document images that may contain text, tables, charts, figures, and other visual components. In this context, accurately retrieving relevant information across these diverse modalities is a central challenge. Multimodal embedding models built on top of foundational vision–language models (VLMs) map diverse content types into a shared representation space, enabling unified retrieval over text, images, and structured visual elements. Although encoding an entire query and candidate document into a single vector is a common practice—exemplified by our recently released commercial-ready Llama-Nemotron-Embed-VL-1B which prioritizes efficiency and low storage—there is an increasing research direction on multi-vector, late-interaction style embedding architectures which provide fine-grained multi-vector interaction between queries and documents. By enabling richer token representations, these models better capture more detailed semantic relationships, and they have shown higher accuracy performance on various (multimodal) benchmarks.
&lt;p&gt;NVIDIA introduces the Nemotron ColEmbed V2 family, a set of late-interaction embedding models available in three sizes—3B, 4B, and 8B—designed for highly accurate multimodal retrieval. These models adopt a unified approach to text–image retrieval and achieve state-of-the-art performance on the ViDoRe V1, V2, and V3 benchmarks.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron ColEmbed V2 Highlights (TL;DR)
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2 are state-of-the-art late interaction embedding models that rank 1st, 3rd and 6th—the highest ranked models in each weight class, as of Feb 3, 2026, on the ViDoRe V3 benchmark: a comprehensive evaluation of visual document retrieval for enterprise use-case benchmark. &lt;/p&gt;
&lt;p&gt;&lt;img alt="late_interaction" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/A4KY3NSZrrxGEcO6mihU8.png" /&gt;&lt;/p&gt;
&lt;p&gt;The late interaction mechanism introduced by ColBERT for multi-vector embedding matching has been extended in our work to a multimodal setting, enabling fine-grained interactions between query and document tokens, whether textual or visual. As illustrated in the figure, each query token embedding interacts with all document token embeddings via the &lt;code&gt;MaxSim&lt;/code&gt; operator, which selects the maximum similarity for each query token and then sums these maxima to produce the final relevance score. This approach requires storing the token embeddings for the entire document corpus, whether textual or visual, thereby increasing storage requirements. During inference, query token embeddings are computed and matched against the stored document embeddings using the same MaxSim operation. &lt;/p&gt;
&lt;p&gt;Nemotron ColEmbed V2 family of models is intended for researchers exploring visual document retrieval applications where accuracy is paramount. This distinguishes it from our 1B single-vector model released last month, which was designed for commercial environments requiring minimal storage and high throughput. It is instrumental in multimodal RAG systems, where textual queries can be used to retrieve document images, such as pages, text, charts, tables, or infographics. The models output multi-vector embeddings for input queries and documents. Potential applications include multimedia search engines, cross-modal retrieval systems, and conversational AI with rich input understanding.&lt;/p&gt;
&lt;p&gt;As a new benchmark, ViDoRe V3 is designed to set an industry standard for multi-modal enterprise document retrieval. It tackles a key challenge in production RAG systems: accurately extracting information from complex, visually-rich documents.  With its strong multi-modal document retrieval capability, the nemotron-colembed-vl-8b-v2 model ranks &lt;strong&gt;#1&lt;/strong&gt; on the ViDoRe V3 leaderboard, setting a new standard for accuracy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Document Retrieval benchmark (page retrieval) – Avg NDCG@10 on ViDoRe V3 public and private tasks.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Models’ Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The llama-nemotron-colembed-vl-3b-v2 is a transformer-based multimodal embedding model built on top of a VLM based on google/siglip2-giant-opt-patch16-384 and meta-llama/Llama-3.2-3B.  The nemotron-colembed-vl-8b-v2 and nemotron-colembed-vl-4b-v2 multimodal encoder models were built from Qwen3-VL-8B-Instruct and Qwen3-VL-4B-Instruct, respectively. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Architecture modifications:
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Our models use bi-directional self-attention instead of the original uni-directional causal self-attention from the LLM decoder models. This allows the model to learn rich representations from the whole input sequence.&lt;/li&gt;
&lt;li&gt;ColBERT-style late interaction mechanism- for each input token, each model outputs an n-dimensional embedding vector of floating-point values, where n is determined by the model’s hidden size.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Training Methodology
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2 models were trained using a bi-encoder architecture, independently. This involves encoding a pair of sentences (for example, a query and a document) independently using the embedding model. Using contrastive learning, it is used to maximize the late interaction similarity between the query and the document that contains the answer, while minimizing the similarity between the query and sampled negative documents not useful to answer the question.&lt;/p&gt;
&lt;p&gt;The llama-nemotron-colembed-vl-3b-v2 model was trained in a two-stage pipeline:  it was first fine-tuned with 12.5M textQA pairs, and subsequently fine-tuned with text–image pairs. The nemotron-colembed-vl-8b-v2,  nemotron-colembed-vl-4b-v2 models were fine-tuned using only text-image pairs (2nd stage).&lt;/p&gt;
&lt;p&gt;Our training datasets contain both text-only and text-image pairs, and we apply hard negative mining following the positive-aware hard negative mining methods presented in the NV-Retriever paper to improve retrieval performance.&lt;/p&gt;
&lt;p&gt;✨ &lt;strong&gt;Key Improvements over V1:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;⚗️ Advanced Model Merging: Utilizes post-training model merging to combine the strengths of multiple fine-tuned checkpoints. This delivers the accuracy stability of an ensemble without any additional inference latency.&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;🌍 Enhanced Synthetic Data: We significantly enriched our training mixture with diverse multilingual synthetic data, improving semantic alignment across languages and complex document types.&lt;/p&gt;
&lt;p&gt;&lt;img alt="modelperfs_vidorev3" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/J7JIKCDriMjztO1ULw0k3.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Start Building with Nemotron ColEmbed V2
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron ColEmbed V2 models mark a major step forward in high-accuracy text–image retrieval, delivering state-of-the-art results on the ViDoRe V1, V2, and  V3 benchmarks. The availability of 3B, 4B and 8B model variants further establishes a solid foundation for future research and advanced experimentation in multimodal retrieval applications.&lt;/p&gt;
&lt;p&gt;Get started with Nemotron ColEmbed V2 models by downloading the models: nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2, available on Hugging Face. Learn more about the NVIDIA NeMo Retriever family of Nemotron RAG models on the product page, or access the microservice container from NVIDIA NGC. This is an excellent opportunity to explore state-of-the-art retrieval in your own applications and workflows.&lt;/p&gt;
&lt;p&gt;Try NVIDIA Enterprise RAG Blueprint, using the Nemotron RAG models that are powered by the same tech behind our ViDoRe V3 winning. &lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Ronay Ak's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/d9fa8404f258c96df1c500cffd10752f.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
Modern search systems are increasingly designed to process heterogeneous document images that may contain text, tables, charts, figures, and other visual components. In this context, accurately retrieving relevant information across these diverse modalities is a central challenge. Multimodal embedding models built on top of foundational vision–language models (VLMs) map diverse content types into a shared representation space, enabling unified retrieval over text, images, and structured visual elements. Although encoding an entire query and candidate document into a single vector is a common practice—exemplified by our recently released commercial-ready Llama-Nemotron-Embed-VL-1B which prioritizes efficiency and low storage—there is an increasing research direction on multi-vector, late-interaction style embedding architectures which provide fine-grained multi-vector interaction between queries and documents. By enabling richer token representations, these models better capture more detailed semantic relationships, and they have shown higher accuracy performance on various (multimodal) benchmarks.
&lt;p&gt;NVIDIA introduces the Nemotron ColEmbed V2 family, a set of late-interaction embedding models available in three sizes—3B, 4B, and 8B—designed for highly accurate multimodal retrieval. These models adopt a unified approach to text–image retrieval and achieve state-of-the-art performance on the ViDoRe V1, V2, and V3 benchmarks.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Nemotron ColEmbed V2 Highlights (TL;DR)
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2 are state-of-the-art late interaction embedding models that rank 1st, 3rd and 6th—the highest ranked models in each weight class, as of Feb 3, 2026, on the ViDoRe V3 benchmark: a comprehensive evaluation of visual document retrieval for enterprise use-case benchmark. &lt;/p&gt;
&lt;p&gt;&lt;img alt="late_interaction" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/A4KY3NSZrrxGEcO6mihU8.png" /&gt;&lt;/p&gt;
&lt;p&gt;The late interaction mechanism introduced by ColBERT for multi-vector embedding matching has been extended in our work to a multimodal setting, enabling fine-grained interactions between query and document tokens, whether textual or visual. As illustrated in the figure, each query token embedding interacts with all document token embeddings via the &lt;code&gt;MaxSim&lt;/code&gt; operator, which selects the maximum similarity for each query token and then sums these maxima to produce the final relevance score. This approach requires storing the token embeddings for the entire document corpus, whether textual or visual, thereby increasing storage requirements. During inference, query token embeddings are computed and matched against the stored document embeddings using the same MaxSim operation. &lt;/p&gt;
&lt;p&gt;Nemotron ColEmbed V2 family of models is intended for researchers exploring visual document retrieval applications where accuracy is paramount. This distinguishes it from our 1B single-vector model released last month, which was designed for commercial environments requiring minimal storage and high throughput. It is instrumental in multimodal RAG systems, where textual queries can be used to retrieve document images, such as pages, text, charts, tables, or infographics. The models output multi-vector embeddings for input queries and documents. Potential applications include multimedia search engines, cross-modal retrieval systems, and conversational AI with rich input understanding.&lt;/p&gt;
&lt;p&gt;As a new benchmark, ViDoRe V3 is designed to set an industry standard for multi-modal enterprise document retrieval. It tackles a key challenge in production RAG systems: accurately extracting information from complex, visually-rich documents.  With its strong multi-modal document retrieval capability, the nemotron-colembed-vl-8b-v2 model ranks &lt;strong&gt;#1&lt;/strong&gt; on the ViDoRe V3 leaderboard, setting a new standard for accuracy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Visual Document Retrieval benchmark (page retrieval) – Avg NDCG@10 on ViDoRe V3 public and private tasks.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Models’ Architecture
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The llama-nemotron-colembed-vl-3b-v2 is a transformer-based multimodal embedding model built on top of a VLM based on google/siglip2-giant-opt-patch16-384 and meta-llama/Llama-3.2-3B.  The nemotron-colembed-vl-8b-v2 and nemotron-colembed-vl-4b-v2 multimodal encoder models were built from Qwen3-VL-8B-Instruct and Qwen3-VL-4B-Instruct, respectively. &lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Architecture modifications:
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Our models use bi-directional self-attention instead of the original uni-directional causal self-attention from the LLM decoder models. This allows the model to learn rich representations from the whole input sequence.&lt;/li&gt;
&lt;li&gt;ColBERT-style late interaction mechanism- for each input token, each model outputs an n-dimensional embedding vector of floating-point values, where n is determined by the model’s hidden size.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Training Methodology
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2 models were trained using a bi-encoder architecture, independently. This involves encoding a pair of sentences (for example, a query and a document) independently using the embedding model. Using contrastive learning, it is used to maximize the late interaction similarity between the query and the document that contains the answer, while minimizing the similarity between the query and sampled negative documents not useful to answer the question.&lt;/p&gt;
&lt;p&gt;The llama-nemotron-colembed-vl-3b-v2 model was trained in a two-stage pipeline:  it was first fine-tuned with 12.5M textQA pairs, and subsequently fine-tuned with text–image pairs. The nemotron-colembed-vl-8b-v2,  nemotron-colembed-vl-4b-v2 models were fine-tuned using only text-image pairs (2nd stage).&lt;/p&gt;
&lt;p&gt;Our training datasets contain both text-only and text-image pairs, and we apply hard negative mining following the positive-aware hard negative mining methods presented in the NV-Retriever paper to improve retrieval performance.&lt;/p&gt;
&lt;p&gt;✨ &lt;strong&gt;Key Improvements over V1:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;⚗️ Advanced Model Merging: Utilizes post-training model merging to combine the strengths of multiple fine-tuned checkpoints. This delivers the accuracy stability of an ensemble without any additional inference latency.&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;🌍 Enhanced Synthetic Data: We significantly enriched our training mixture with diverse multilingual synthetic data, improving semantic alignment across languages and complex document types.&lt;/p&gt;
&lt;p&gt;&lt;img alt="modelperfs_vidorev3" src="https://cdn-uploads.huggingface.co/production/uploads/6814af6d59c3d8d6f9b09f2b/J7JIKCDriMjztO1ULw0k3.png" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Start Building with Nemotron ColEmbed V2
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron ColEmbed V2 models mark a major step forward in high-accuracy text–image retrieval, delivering state-of-the-art results on the ViDoRe V1, V2, and  V3 benchmarks. The availability of 3B, 4B and 8B model variants further establishes a solid foundation for future research and advanced experimentation in multimodal retrieval applications.&lt;/p&gt;
&lt;p&gt;Get started with Nemotron ColEmbed V2 models by downloading the models: nemotron-colembed-vl-8b-v2, nemotron-colembed-vl-4b-v2 and llama-nemotron-colembed-vl-3b-v2, available on Hugging Face. Learn more about the NVIDIA NeMo Retriever family of Nemotron RAG models on the product page, or access the microservice container from NVIDIA NGC. This is an excellent opportunity to explore state-of-the-art retrieval in your own applications and workflows.&lt;/p&gt;
&lt;p&gt;Try NVIDIA Enterprise RAG Blueprint, using the Nemotron RAG models that are powered by the same tech behind our ViDoRe V3 winning. &lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/nemotron-colembed-v2</guid><pubDate>Wed, 04 Feb 2026 15:00:40 +0000</pubDate></item><item><title>[NEW] ElevenLabs raises $500M from Sequoia at an $11 billion valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/elevenlabs-raises-500m-from-sequioia-at-a-11-billion-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/ElevenLabs-feat.jpg?resize=1200,669" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Voice AI company ElevenLabs said today it raised $500 million in a new funding round led by Sequoia Capital, which was an investor in the startup’s last secondary round through a tender. Sequoia partner Andrew Reed is joining the company’s board.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup is now valued at $11 billion, more than three times its valuation in its last round in January 2025. Earlier in the year, the Financial Times reported that the startup was looking to raise at that valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company said that existing investor a16z quadrupled its investment amount, and Iconiq, which led the last round, tripled it. Some prior investors, like BroadLight, NFDG, Valor Capital, AMP Coalition, and Smash Capital, also joined the round. New investors for the funding included Lightspeed Venture Partners, Evantic&lt;br /&gt;Capital, and Bond.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs said that it will disclose some investors later in February, which might be strategic partners. The company has raised over $781 million to date. It said that it will use the funding for research and product development, along with expansion in international markets like India, Japan, Singapore, Brazil, and Mexico. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s co-founder, Mati Staniszewski, indicated that ElevenLabs might work on agents beyond voice and incorporate video. In January, the company announced a partnership with LTX to produce audio-to-video content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The intersection of models and products is critical – and our team has proven, time and again, how to translate research into real-world experiences. This funding helps us go beyond voice alone to transform how we interact with technology altogether. We plan to expand our Creative offering – helping creators combine our best-in-class audio with video and Agents – enabling businesses to build agents that can talk, type, and take action,” he said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has seen good growth momentum as it closed the year at $330 million ARR. In an interview with Bloomberg earlier this year, Staniszewski said that it took ElevenLabs five months to reach $200 million to $300 million in ARR.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Voice AI model providers are an attractive target for investors and big tech companies. In January, rival Deepgram raised $130 million from AVP at a $1.3 billion valuation. Meanwhile, Google hired top talent from voice model company Hume AI, including CEO Alan Cowen.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/ElevenLabs-feat.jpg?resize=1200,669" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Voice AI company ElevenLabs said today it raised $500 million in a new funding round led by Sequoia Capital, which was an investor in the startup’s last secondary round through a tender. Sequoia partner Andrew Reed is joining the company’s board.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup is now valued at $11 billion, more than three times its valuation in its last round in January 2025. Earlier in the year, the Financial Times reported that the startup was looking to raise at that valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company said that existing investor a16z quadrupled its investment amount, and Iconiq, which led the last round, tripled it. Some prior investors, like BroadLight, NFDG, Valor Capital, AMP Coalition, and Smash Capital, also joined the round. New investors for the funding included Lightspeed Venture Partners, Evantic&lt;br /&gt;Capital, and Bond.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs said that it will disclose some investors later in February, which might be strategic partners. The company has raised over $781 million to date. It said that it will use the funding for research and product development, along with expansion in international markets like India, Japan, Singapore, Brazil, and Mexico. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s co-founder, Mati Staniszewski, indicated that ElevenLabs might work on agents beyond voice and incorporate video. In January, the company announced a partnership with LTX to produce audio-to-video content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The intersection of models and products is critical – and our team has proven, time and again, how to translate research into real-world experiences. This funding helps us go beyond voice alone to transform how we interact with technology altogether. We plan to expand our Creative offering – helping creators combine our best-in-class audio with video and Agents – enabling businesses to build agents that can talk, type, and take action,” he said in a statement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has seen good growth momentum as it closed the year at $330 million ARR. In an interview with Bloomberg earlier this year, Staniszewski said that it took ElevenLabs five months to reach $200 million to $300 million in ARR.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Voice AI model providers are an attractive target for investors and big tech companies. In January, rival Deepgram raised $130 million from AVP at a $1.3 billion valuation. Meanwhile, Google hired top talent from voice model company Hume AI, including CEO Alan Cowen.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/elevenlabs-raises-500m-from-sequioia-at-a-11-billion-valuation/</guid><pubDate>Wed, 04 Feb 2026 15:33:58 +0000</pubDate></item><item><title>[NEW] Nemotron Labs: How AI Agents Are Turning Documents Into Real-Time Business Intelligence (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-agents-intelligent-document-processing/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;Nemotron Labs&lt;/i&gt;&lt;i&gt; blog series, which explores how the latest open models, datasets and training techniques help businesses build specialized AI systems and applications on NVIDIA platforms. Each post highlights practical ways to use an open stack to deliver value in production — from transparent research copilots to scalable AI agents.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Businesses today face the challenge of uncovering valuable insights buried within a wide variety of documents — including reports, presentations, PDFs, web pages and spreadsheets.&lt;/p&gt;
&lt;p&gt;Often, teams piece together insights by manually reviewing files, copying data into spreadsheets, building dashboards and using basic search or template-based optical character recognition (OCR) tools that often miss important details in complex media.&lt;/p&gt;
&lt;p&gt;Intelligent document processing is an AI-powered workflow that automatically reads, understands and extracts insights from documents. It interprets rich formats inside those documents — including tables, charts, images and text — using AI agents and techniques like retrieval-augmented generation (RAG) to turn the multimodal content into insights that other multi-agent systems and people can easily use.&lt;/p&gt;
&lt;p&gt;With NVIDIA Nemotron open models and GPU-accelerated libraries, organizations can build AI-powered document intelligence systems for research, financial services, legal workflows and more.&lt;/p&gt;
&lt;p&gt;These open models, datasets and training recipes have powered strong results on leaderboards such as MTEB, MMTEB and ViDoRe V3, benchmarks for evaluating multilingual and multimodal retrieval models. Teams can choose from among the best models for tasks like search and question answering.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;How Document Processing Streamlines Business Intelligence&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Document intelligence systems that can pull meaning from complex layouts, scale to huge file libraries and show exactly where an answer came from are incredibly useful in high-stakes environments. These systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Understand rich document content&lt;/b&gt;, moving beyond simple text scraping to capture information from charts, tables, figures and mixed-language pages and treating documents as a human would by recognizing structure, relationships and context​​.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Handle large quantities of shifting data&lt;/b&gt;, ingesting and processing massive collections of documents in parallel, and keeping knowledge bases continuously up to date.​​&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Find exactly what users need&lt;/b&gt;, helping AI agents pinpoint the most relevant passages, tables or paragraphs to a query so they can respond with precision and accuracy.​​&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Show the evidence behind answers&lt;/b&gt; by providing citations to specific pages or charts so teams can gain transparency and auditability, which is critical in regulated industries.​​&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-medium wp-image-89600" height="384" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/nemotron-labs-infographic-960x384.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;The result is a shift from static document archives to living knowledge systems that directly power business intelligence, customer experiences and operational workflows.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Document Intelligence at Work&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Intelligent document processing systems built on NVIDIA Nemotron RAG models, Nemotron Parse and accelerated computing are already reshaping how organizations across industries gain insights from their documents.​​&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Justt: AI-Native Chargeback Management and Dispute Optimization&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;In financial services, payment disputes create significant revenue loss and operational complexity for merchants, largely because the evidence needed to handle them lives in unstructured formats. Transaction logs, customer communications and policy documents are often fragmented across systems and difficult to process at scale, making dispute handling slow, manual and costly.&lt;/p&gt;
&lt;p&gt;Justt.ai provides an AI-driven platform that automates the full chargeback lifecycle at scale. The platform connects directly to payment service providers and merchant data sources to ingest transaction data, customer interactions and policies, then automatically assembles dispute-specific evidence that aligns with card network and issuer requirements.&lt;/p&gt;
&lt;p&gt;The platform’s AI-powered dispute optimization, powered by Nemotron Parse, applies predictive analytics to determine which chargebacks to fight or accept, and how to optimize each response for maximum net recovery. Leading hospitality operators like HEI Hotels &amp;amp; Resorts use the platform to automate dispute handling across their properties, recapturing revenue while maintaining guest relationships.&lt;/p&gt;
&lt;p&gt;By pairing document-centric intelligence with decision automation, merchants can recapture a significant portion of revenue lost to illegitimate chargebacks while reducing manual review effort.​&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Docusign: Scaling Agreement Intelligence&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Docusign is the global leader in Intelligent Agreement Management, handling millions of transactions every day for more than 1.8 million customers and over 1 billion users.&lt;/p&gt;
&lt;p&gt;Agreements are the foundation of every business, but the critical information they contain are often buried inside pages of documents. To surface the information, Docusign needed high-fidelity extraction of tables, text and metadata from complex documents like PDFs so organizations could understand and act on obligations, risks and opportunities faster.&lt;/p&gt;
&lt;p&gt;Docusign is evaluating Nemotron Parse for deeper contract understanding at scale. Running on NVIDIA GPUs, the model combines advanced AI with layout detection and OCR. The system can reliably interpret complex tables and reconstruct tables with required information. This reduces the need for manual corrections and helps ensure that even the most complex contracts are processed with the speed and accuracy their customers expect.&lt;/p&gt;
&lt;p&gt;With this foundation, Docusign will transform agreement repositories into structured data that powers contract search, analysis and AI-driven workflows — turning agreements into business assets that help organizations and their teams improve visibility, reduce risk and make faster decisions.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Edison Scientific: Research Across Massive Literature Scale&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Edison Scientific’s Kosmos AI Scientist helps researchers navigate complex scientific landscapes to synthesize literature, identify connections and surface evidence.​&lt;/p&gt;
&lt;p&gt;Edison needed a way to rapidly and accurately extract structured information from large volumes of PDFs, including equations, tables and figures that traditional information parsing methods often mishandle.​&lt;/p&gt;
&lt;p&gt;By integrating the NVIDIA Nemotron Parse model into its PaperQA2 pipeline, Edison can decompose research papers, index key concepts and ground responses in specific passages, improving both throughput and answer quality for scientists.​​ This approach turns a sprawling research corpus into an interactive, queryable knowledge engine that accelerates hypothesis generation and literature review.​&lt;/p&gt;
&lt;p&gt;The high efficiency of Nemotron Parse enables cost-efficient serving at scale, allowing Edison’s team to unlock the whole multimodal pipeline.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Designing an Intelligent Document Processing Application With NVIDIA Technologies&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A robust, domain-specific document intelligence pipeline requires technologies that can handle data extraction, embedding and reranking, while keeping the data secure and compliant with regulations.​​&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Extraction:&lt;/b&gt; Nemotron extraction and OCR models rapidly ingest multimodal PDFs, text, tables, graphs and images to convert them into structured, machine-readable content while preserving layout and semantics.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Embedding:&lt;/b&gt; Nemotron embedding models convert passages, entities and visual elements into vector representations tuned for document retrieval, enabling semantically accurate search.​​&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Reranking:&lt;/b&gt; Nemotron reranking models evaluate candidate passages to ensure the most relevant content is surfaced as context for large language models (LLMs), improving answer fidelity and reducing hallucinations.​​&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Parsing:&lt;/b&gt; Nemotron Parse models decipher document semantics to extract text and tables with precise spatial grounding and correct reading flow. Overcoming layout variability, they turn unstructured documents into actionable data that enhances the accuracy of LLMs and agentic workflows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These capabilities are packaged as NVIDIA NIM microservices and foundation models that run efficiently on NVIDIA GPUs, allowing teams to scale from proof of concept to production while keeping sensitive data within their chosen cloud or data center environment.&lt;/p&gt;
&lt;p&gt;The most effective AI systems use a mix of frontier models and open source models like NVIDIA Nemotron, with an LLM router analyzing each task and automatically selecting the model best suited for it. This approach keeps performance strong while managing computing costs and improving efficiency.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Started With NVIDIA Nemotron&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Access a step-by-step tutorial on how to build a document processing pipeline with RAG capabilities. Explore how Nemotron RAG can power specialized agents tailored for different industries.​&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Plus, experiment with Nemotron RAG models and the NVIDIA NeMo Retriever open library, available on GitHub and Hugging Face, as well as Nemotron Parse on Hugging Face.&lt;/p&gt;
&lt;p&gt;Join the community of developers building with the NVIDIA Blueprint for Enterprise RAG — trusted by a dozen industry-leading AI Data Platform providers and available now on build.nvidia.com, GitHub and the NGC catalog.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, &lt;/i&gt;&lt;i&gt;NVIDIA Nemotron&lt;/i&gt;&lt;i&gt; and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA AI news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of the &lt;/i&gt;&lt;i&gt;Nemotron Labs&lt;/i&gt;&lt;i&gt; blog series, which explores how the latest open models, datasets and training techniques help businesses build specialized AI systems and applications on NVIDIA platforms. Each post highlights practical ways to use an open stack to deliver value in production — from transparent research copilots to scalable AI agents.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Businesses today face the challenge of uncovering valuable insights buried within a wide variety of documents — including reports, presentations, PDFs, web pages and spreadsheets.&lt;/p&gt;
&lt;p&gt;Often, teams piece together insights by manually reviewing files, copying data into spreadsheets, building dashboards and using basic search or template-based optical character recognition (OCR) tools that often miss important details in complex media.&lt;/p&gt;
&lt;p&gt;Intelligent document processing is an AI-powered workflow that automatically reads, understands and extracts insights from documents. It interprets rich formats inside those documents — including tables, charts, images and text — using AI agents and techniques like retrieval-augmented generation (RAG) to turn the multimodal content into insights that other multi-agent systems and people can easily use.&lt;/p&gt;
&lt;p&gt;With NVIDIA Nemotron open models and GPU-accelerated libraries, organizations can build AI-powered document intelligence systems for research, financial services, legal workflows and more.&lt;/p&gt;
&lt;p&gt;These open models, datasets and training recipes have powered strong results on leaderboards such as MTEB, MMTEB and ViDoRe V3, benchmarks for evaluating multilingual and multimodal retrieval models. Teams can choose from among the best models for tasks like search and question answering.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;How Document Processing Streamlines Business Intelligence&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Document intelligence systems that can pull meaning from complex layouts, scale to huge file libraries and show exactly where an answer came from are incredibly useful in high-stakes environments. These systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Understand rich document content&lt;/b&gt;, moving beyond simple text scraping to capture information from charts, tables, figures and mixed-language pages and treating documents as a human would by recognizing structure, relationships and context​​.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Handle large quantities of shifting data&lt;/b&gt;, ingesting and processing massive collections of documents in parallel, and keeping knowledge bases continuously up to date.​​&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Find exactly what users need&lt;/b&gt;, helping AI agents pinpoint the most relevant passages, tables or paragraphs to a query so they can respond with precision and accuracy.​​&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Show the evidence behind answers&lt;/b&gt; by providing citations to specific pages or charts so teams can gain transparency and auditability, which is critical in regulated industries.​​&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-medium wp-image-89600" height="384" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/nemotron-labs-infographic-960x384.jpg" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;The result is a shift from static document archives to living knowledge systems that directly power business intelligence, customer experiences and operational workflows.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Document Intelligence at Work&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Intelligent document processing systems built on NVIDIA Nemotron RAG models, Nemotron Parse and accelerated computing are already reshaping how organizations across industries gain insights from their documents.​​&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Justt: AI-Native Chargeback Management and Dispute Optimization&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;In financial services, payment disputes create significant revenue loss and operational complexity for merchants, largely because the evidence needed to handle them lives in unstructured formats. Transaction logs, customer communications and policy documents are often fragmented across systems and difficult to process at scale, making dispute handling slow, manual and costly.&lt;/p&gt;
&lt;p&gt;Justt.ai provides an AI-driven platform that automates the full chargeback lifecycle at scale. The platform connects directly to payment service providers and merchant data sources to ingest transaction data, customer interactions and policies, then automatically assembles dispute-specific evidence that aligns with card network and issuer requirements.&lt;/p&gt;
&lt;p&gt;The platform’s AI-powered dispute optimization, powered by Nemotron Parse, applies predictive analytics to determine which chargebacks to fight or accept, and how to optimize each response for maximum net recovery. Leading hospitality operators like HEI Hotels &amp;amp; Resorts use the platform to automate dispute handling across their properties, recapturing revenue while maintaining guest relationships.&lt;/p&gt;
&lt;p&gt;By pairing document-centric intelligence with decision automation, merchants can recapture a significant portion of revenue lost to illegitimate chargebacks while reducing manual review effort.​&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Docusign: Scaling Agreement Intelligence&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Docusign is the global leader in Intelligent Agreement Management, handling millions of transactions every day for more than 1.8 million customers and over 1 billion users.&lt;/p&gt;
&lt;p&gt;Agreements are the foundation of every business, but the critical information they contain are often buried inside pages of documents. To surface the information, Docusign needed high-fidelity extraction of tables, text and metadata from complex documents like PDFs so organizations could understand and act on obligations, risks and opportunities faster.&lt;/p&gt;
&lt;p&gt;Docusign is evaluating Nemotron Parse for deeper contract understanding at scale. Running on NVIDIA GPUs, the model combines advanced AI with layout detection and OCR. The system can reliably interpret complex tables and reconstruct tables with required information. This reduces the need for manual corrections and helps ensure that even the most complex contracts are processed with the speed and accuracy their customers expect.&lt;/p&gt;
&lt;p&gt;With this foundation, Docusign will transform agreement repositories into structured data that powers contract search, analysis and AI-driven workflows — turning agreements into business assets that help organizations and their teams improve visibility, reduce risk and make faster decisions.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Edison Scientific: Research Across Massive Literature Scale&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Edison Scientific’s Kosmos AI Scientist helps researchers navigate complex scientific landscapes to synthesize literature, identify connections and surface evidence.​&lt;/p&gt;
&lt;p&gt;Edison needed a way to rapidly and accurately extract structured information from large volumes of PDFs, including equations, tables and figures that traditional information parsing methods often mishandle.​&lt;/p&gt;
&lt;p&gt;By integrating the NVIDIA Nemotron Parse model into its PaperQA2 pipeline, Edison can decompose research papers, index key concepts and ground responses in specific passages, improving both throughput and answer quality for scientists.​​ This approach turns a sprawling research corpus into an interactive, queryable knowledge engine that accelerates hypothesis generation and literature review.​&lt;/p&gt;
&lt;p&gt;The high efficiency of Nemotron Parse enables cost-efficient serving at scale, allowing Edison’s team to unlock the whole multimodal pipeline.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Designing an Intelligent Document Processing Application With NVIDIA Technologies&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;A robust, domain-specific document intelligence pipeline requires technologies that can handle data extraction, embedding and reranking, while keeping the data secure and compliant with regulations.​​&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Extraction:&lt;/b&gt; Nemotron extraction and OCR models rapidly ingest multimodal PDFs, text, tables, graphs and images to convert them into structured, machine-readable content while preserving layout and semantics.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Embedding:&lt;/b&gt; Nemotron embedding models convert passages, entities and visual elements into vector representations tuned for document retrieval, enabling semantically accurate search.​​&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Reranking:&lt;/b&gt; Nemotron reranking models evaluate candidate passages to ensure the most relevant content is surfaced as context for large language models (LLMs), improving answer fidelity and reducing hallucinations.​​&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Parsing:&lt;/b&gt; Nemotron Parse models decipher document semantics to extract text and tables with precise spatial grounding and correct reading flow. Overcoming layout variability, they turn unstructured documents into actionable data that enhances the accuracy of LLMs and agentic workflows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These capabilities are packaged as NVIDIA NIM microservices and foundation models that run efficiently on NVIDIA GPUs, allowing teams to scale from proof of concept to production while keeping sensitive data within their chosen cloud or data center environment.&lt;/p&gt;
&lt;p&gt;The most effective AI systems use a mix of frontier models and open source models like NVIDIA Nemotron, with an LLM router analyzing each task and automatically selecting the model best suited for it. This approach keeps performance strong while managing computing costs and improving efficiency.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Started With NVIDIA Nemotron&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Access a step-by-step tutorial on how to build a document processing pipeline with RAG capabilities. Explore how Nemotron RAG can power specialized agents tailored for different industries.​&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Plus, experiment with Nemotron RAG models and the NVIDIA NeMo Retriever open library, available on GitHub and Hugging Face, as well as Nemotron Parse on Hugging Face.&lt;/p&gt;
&lt;p&gt;Join the community of developers building with the NVIDIA Blueprint for Enterprise RAG — trusted by a dozen industry-leading AI Data Platform providers and available now on build.nvidia.com, GitHub and the NGC catalog.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, &lt;/i&gt;&lt;i&gt;NVIDIA Nemotron&lt;/i&gt;&lt;i&gt; and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA AI news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-agents-intelligent-document-processing/</guid><pubDate>Wed, 04 Feb 2026 16:00:36 +0000</pubDate></item><item><title>[NEW] AI Expo 2026 Day 1: Governance and data readiness enable the agentic enterprise (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-expo-2026-day-1-governance-data-readiness-enable-agentic-enterprise/</link><description>&lt;p&gt;While the prospect of AI acting as a digital co-worker dominated the day one agenda at the co-located AI &amp;amp; Big Data Expo and Intelligent Automation Conference, the technical sessions focused on the infrastructure to make it work.&lt;/p&gt;&lt;p&gt;A primary topic on the exhibition floor was the progression from passive automation to “agentic” systems. These tools reason, plan, and execute tasks rather than following rigid scripts. Amal Makwana from Citi detailed how these systems act across enterprise workflows. This capability separates them from earlier robotic process automation (RPA).&lt;/p&gt;&lt;p&gt;Scott Ivell and Ire Adewolu of DeepL described this development as closing the “automation gap”. They argued that agentic AI functions as a digital co-worker rather than a simple tool. Real value is unlocked by reducing the distance between intent and execution. Brian Halpin from SS&amp;amp;C Blue Prism noted that organisations typically must master standard automation before they can deploy agentic AI.&lt;/p&gt;&lt;p&gt;This change requires governance frameworks capable of handling non-deterministic outcomes. Steve Holyer of Informatica, alongside speakers from MuleSoft and Salesforce, argued that architecting these systems requires strict oversight. A governance layer must control how agents access and utilise data to prevent operational failure.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-data-quality-blocks-deployment"&gt;Data quality blocks deployment&lt;/h3&gt;&lt;p&gt;The output of an autonomous system relies on the quality of its input. Andreas Krause from SAP stated that AI fails without trusted, connected enterprise data. For GenAI to function in a corporate context, it must access data that is both accurate and contextually-relevant.&lt;/p&gt;&lt;p&gt;Meni Meller of Gigaspaces addressed the technical challenge of “hallucinations” in LLMs. He advocated for the use of eRAG (retrieval-augmented generation) combined with semantic layers to fix data access issues. This approach allows models to retrieve factual enterprise data in real-time.&lt;/p&gt;&lt;p&gt;Storage and analysis also present challenges. A panel featuring representatives from Equifax, British Gas, and Centrica discussed the necessity of cloud-native, real-time analytics. For these organisations, competitive advantage comes from the ability to execute analytics strategies that are scalable and immediate.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-physical-safety-and-observability"&gt;Physical safety and observability&lt;/h3&gt;&lt;p&gt;The integration of AI extends into physical environments, introducing safety risks that differ from software failures. A panel including Edith-Clare Hall from ARIA and Matthew Howard from IEEE RAS examined how embodied AI is deployed in factories, offices, and public spaces. Safety protocols must be established &lt;em&gt;before&lt;/em&gt; robots interact with humans.&lt;/p&gt;&lt;p&gt;Perla Maiolino from the Oxford Robotics Institute provided a technical perspective on this challenge. Her research into Time-of-Flight (ToF) sensors and electronic skin aims to give robots both self-awareness and environmental awareness. For industries such as manufacturing and logistics, these integrated perception systems prevent accidents.&lt;/p&gt;&lt;p&gt;In software development, observability remains a parallel concern. Yulia Samoylova from Datadog highlighted how AI changes the way teams build and troubleshoot software. As systems become more autonomous, the ability to observe their internal state and reasoning processes becomes necessary for reliability.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-infrastructure-and-adoption-barriers"&gt;Infrastructure and adoption barriers&lt;/h3&gt;&lt;p&gt;Implementation demands reliable infrastructure and a receptive culture. Julian Skeels from Expereo argued that networks must be designed specifically for AI workloads. This involves building sovereign, secure, and “always-on” network fabrics capable of handling high throughput.&lt;/p&gt;&lt;p&gt;Of course, the human element remains unpredictable. Paul Fermor from IBM Automation warned that traditional automation thinking often underestimates the complexity of AI adoption. He termed this the “illusion of AI readiness”. Jena Miller reinforced this point, noting that strategies must be human-centred to ensure adoption. If the workforce does not trust the tools, the technology yields no return.&lt;/p&gt;&lt;p&gt;Ravi Jay from Sanofi suggested that leaders need to ask operational and ethical questions early on in the process. Success depends on deciding where to build proprietary solutions versus where to buy established platforms.&lt;/p&gt;&lt;p&gt;The sessions from day one of the co-located events indicate that, while technology is moving toward autonomous agents, deployment requires a solid data foundation.&lt;/p&gt;&lt;p&gt;CIOs should focus on establishing data governance frameworks that support retrieval-augmented generation. Network infrastructure must be evaluated to ensure it supports the latency requirements of agentic workloads. Finally, cultural adoption strategies must run parallel to technical implementation.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p&gt;While the prospect of AI acting as a digital co-worker dominated the day one agenda at the co-located AI &amp;amp; Big Data Expo and Intelligent Automation Conference, the technical sessions focused on the infrastructure to make it work.&lt;/p&gt;&lt;p&gt;A primary topic on the exhibition floor was the progression from passive automation to “agentic” systems. These tools reason, plan, and execute tasks rather than following rigid scripts. Amal Makwana from Citi detailed how these systems act across enterprise workflows. This capability separates them from earlier robotic process automation (RPA).&lt;/p&gt;&lt;p&gt;Scott Ivell and Ire Adewolu of DeepL described this development as closing the “automation gap”. They argued that agentic AI functions as a digital co-worker rather than a simple tool. Real value is unlocked by reducing the distance between intent and execution. Brian Halpin from SS&amp;amp;C Blue Prism noted that organisations typically must master standard automation before they can deploy agentic AI.&lt;/p&gt;&lt;p&gt;This change requires governance frameworks capable of handling non-deterministic outcomes. Steve Holyer of Informatica, alongside speakers from MuleSoft and Salesforce, argued that architecting these systems requires strict oversight. A governance layer must control how agents access and utilise data to prevent operational failure.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-data-quality-blocks-deployment"&gt;Data quality blocks deployment&lt;/h3&gt;&lt;p&gt;The output of an autonomous system relies on the quality of its input. Andreas Krause from SAP stated that AI fails without trusted, connected enterprise data. For GenAI to function in a corporate context, it must access data that is both accurate and contextually-relevant.&lt;/p&gt;&lt;p&gt;Meni Meller of Gigaspaces addressed the technical challenge of “hallucinations” in LLMs. He advocated for the use of eRAG (retrieval-augmented generation) combined with semantic layers to fix data access issues. This approach allows models to retrieve factual enterprise data in real-time.&lt;/p&gt;&lt;p&gt;Storage and analysis also present challenges. A panel featuring representatives from Equifax, British Gas, and Centrica discussed the necessity of cloud-native, real-time analytics. For these organisations, competitive advantage comes from the ability to execute analytics strategies that are scalable and immediate.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-physical-safety-and-observability"&gt;Physical safety and observability&lt;/h3&gt;&lt;p&gt;The integration of AI extends into physical environments, introducing safety risks that differ from software failures. A panel including Edith-Clare Hall from ARIA and Matthew Howard from IEEE RAS examined how embodied AI is deployed in factories, offices, and public spaces. Safety protocols must be established &lt;em&gt;before&lt;/em&gt; robots interact with humans.&lt;/p&gt;&lt;p&gt;Perla Maiolino from the Oxford Robotics Institute provided a technical perspective on this challenge. Her research into Time-of-Flight (ToF) sensors and electronic skin aims to give robots both self-awareness and environmental awareness. For industries such as manufacturing and logistics, these integrated perception systems prevent accidents.&lt;/p&gt;&lt;p&gt;In software development, observability remains a parallel concern. Yulia Samoylova from Datadog highlighted how AI changes the way teams build and troubleshoot software. As systems become more autonomous, the ability to observe their internal state and reasoning processes becomes necessary for reliability.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-infrastructure-and-adoption-barriers"&gt;Infrastructure and adoption barriers&lt;/h3&gt;&lt;p&gt;Implementation demands reliable infrastructure and a receptive culture. Julian Skeels from Expereo argued that networks must be designed specifically for AI workloads. This involves building sovereign, secure, and “always-on” network fabrics capable of handling high throughput.&lt;/p&gt;&lt;p&gt;Of course, the human element remains unpredictable. Paul Fermor from IBM Automation warned that traditional automation thinking often underestimates the complexity of AI adoption. He termed this the “illusion of AI readiness”. Jena Miller reinforced this point, noting that strategies must be human-centred to ensure adoption. If the workforce does not trust the tools, the technology yields no return.&lt;/p&gt;&lt;p&gt;Ravi Jay from Sanofi suggested that leaders need to ask operational and ethical questions early on in the process. Success depends on deciding where to build proprietary solutions versus where to buy established platforms.&lt;/p&gt;&lt;p&gt;The sessions from day one of the co-located events indicate that, while technology is moving toward autonomous agents, deployment requires a solid data foundation.&lt;/p&gt;&lt;p&gt;CIOs should focus on establishing data governance frameworks that support retrieval-augmented generation. Network infrastructure must be evaluated to ensure it supports the latency requirements of agentic workloads. Finally, cultural adoption strategies must run parallel to technical implementation.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-expo-2026-day-1-governance-data-readiness-enable-agentic-enterprise/</guid><pubDate>Wed, 04 Feb 2026 16:33:34 +0000</pubDate></item><item><title>[NEW] Roblox’s 4D creation feature is now available in open beta (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/robloxs-4d-creation-feature-is-now-available-in-open-beta/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last year, Roblox launched an open-source AI model that could generate 3D objects on the platform, helping users quickly design digital items such as furniture, vehicles, and accessories. The company claims the tool, called Cube 3D, has so far helped users generate over 1.8 million 3D objects since it was rolled out last March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, the company launched the open beta for its anticipated 4D creation feature that lets creators make not just static 3D models, but fully functional and interactive objects. The feature has been in early access since November.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Roblox says 4D creation adds an important new layer: interactivity. With this technology, users can design items that can move and react to players in the game.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3088899" height="408" src="https://techcrunch.com/wp-content/uploads/2026/02/Roblox-4D-Creations.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Roblox&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;​At launch, there are two types of object templates (called schemas) that creators can try out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first is the “Car-5” schema, which is used to create a car made of five separate parts: the main body and four wheels. Previously, cars were a single, solid 3D object that couldn’t move. The new system breaks down objects into parts and assigns behaviors to each so that they function individually within the virtual world. The AI therefore can generate cars with spinning wheels, making them more realistic and interactive. ​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The second is called “Body-1,” which can generate any object made from a single piece, like a simple box or sculpture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first experience with 4D generation is a game called Wish Master, where players can generate cars they can drive, planes they can fly, and even dragons.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;​In the future, Roblox plans to let creators make their own schemas so they’ll have more freedom to define how objects behave. The company says it is also developing new technology that could use a reference image to create a detailed 3D model that matches the image’s style (example below.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3088898" height="292" src="https://techcrunch.com/wp-content/uploads/2026/02/Upsample-3D-Models.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Roblox&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it is developing more ways to help people create games and experiences using AI, including a project it has dubbed “real-time dreaming.” Roblox CEO David Baszucki last month explained that this project would let creators build new worlds using “keyboard navigation and sharing real-time text prompts.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The open beta comes on the heels of Roblox’s recent implementation of mandatory facial verification for users to access chat features in the game, following lawsuits and investigations related to child safety.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Last year, Roblox launched an open-source AI model that could generate 3D objects on the platform, helping users quickly design digital items such as furniture, vehicles, and accessories. The company claims the tool, called Cube 3D, has so far helped users generate over 1.8 million 3D objects since it was rolled out last March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, the company launched the open beta for its anticipated 4D creation feature that lets creators make not just static 3D models, but fully functional and interactive objects. The feature has been in early access since November.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Roblox says 4D creation adds an important new layer: interactivity. With this technology, users can design items that can move and react to players in the game.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3088899" height="408" src="https://techcrunch.com/wp-content/uploads/2026/02/Roblox-4D-Creations.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Roblox&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;​At launch, there are two types of object templates (called schemas) that creators can try out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first is the “Car-5” schema, which is used to create a car made of five separate parts: the main body and four wheels. Previously, cars were a single, solid 3D object that couldn’t move. The new system breaks down objects into parts and assigns behaviors to each so that they function individually within the virtual world. The AI therefore can generate cars with spinning wheels, making them more realistic and interactive. ​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The second is called “Body-1,” which can generate any object made from a single piece, like a simple box or sculpture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first experience with 4D generation is a game called Wish Master, where players can generate cars they can drive, planes they can fly, and even dragons.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;​In the future, Roblox plans to let creators make their own schemas so they’ll have more freedom to define how objects behave. The company says it is also developing new technology that could use a reference image to create a detailed 3D model that matches the image’s style (example below.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3088898" height="292" src="https://techcrunch.com/wp-content/uploads/2026/02/Upsample-3D-Models.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Roblox&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it is developing more ways to help people create games and experiences using AI, including a project it has dubbed “real-time dreaming.” Roblox CEO David Baszucki last month explained that this project would let creators build new worlds using “keyboard navigation and sharing real-time text prompts.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The open beta comes on the heels of Roblox’s recent implementation of mandatory facial verification for users to access chat features in the game, following lawsuits and investigations related to child safety.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/robloxs-4d-creation-feature-is-now-available-in-open-beta/</guid><pubDate>Wed, 04 Feb 2026 17:00:00 +0000</pubDate></item><item><title>[NEW] 3 Questions: Using AI to accelerate the discovery and design of therapeutic drugs (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/3-questions-using-ai-to-accelerate-discovery-design-therapeutic-drugs-0204</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/jim-collins-mit-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;em&gt;In the pursuit of solutions to complex global challenges including disease, energy demands, and climate change, scientific researchers, including at MIT, have turned to artificial intelligence, and to quantitative analysis and modeling, to design and construct engineered cells with novel properties. The engineered cells can be programmed to become new therapeutics — battling, and perhaps eradicating, diseases.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;James J. Collins&lt;/em&gt;&lt;em&gt; is one of the founders of the field of synthetic biology, and is also a leading researcher in systems biology, the interdisciplinary approach that uses mathematical analysis and modeling of complex systems to better understand biological systems. His research has led to the development of new classes of diagnostics and therapeutics, including in the detection and treatment of pathogens like Ebola, Zika, SARS-CoV-2, and antibiotic-resistant bacteria. Collins, the Termeer Professor of Medical Engineering and Science and professor of biological engineering at MIT, is a core faculty member of the Institute for Medical Engineering and Science (IMES), the director of the MIT Abdul Latif Jameel Clinic for Machine Learning in Health, as well as an institute member of the Broad Institute of MIT and Harvard, and core founding faculty at the Wyss Institute for Biologically Inspired Engineering, Harvard.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;In this Q&amp;amp;A, Collins speaks about his latest work and goals for this research.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;You’re known for collaborating with colleagues across MIT, and at other institutions. How have these collaborations and affiliations helped you with your research?&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;Collaboration has been central to the work in my lab. At the MIT Jameel Clinic for Machine Learning in Health, I formed a collaboration with Regina Barzilay [the Delta Electronics Professor in the MIT Department of Electrical Engineering and Computer Science and affiliate faculty member at IMES] and Tommi Jaakkola [the Thomas Siebel Professor of Electrical Engineering and Computer Science and the Institute for Data, Systems, and Society] to use deep learning to discover new antibiotics. This effort combined our expertise in artificial intelligence, network biology, and systems microbiology, leading to the discovery of halicin, a potent new antibiotic effective against a broad range of multidrug-resistant bacterial pathogens. Our results were published in &lt;em&gt;Cell&lt;/em&gt; in 2020 and showcased the power of bringing together complementary skill sets to tackle a global health challenge.&lt;/p&gt;&lt;p&gt;At the Wyss Institute, I’ve worked closely with Donald Ingber [the Judah Folkman Professor of Vascular Biology at Harvard Medical School and the Vascular Biology Program at Boston Children’s Hospital, and Hansjörg Wyss Professor of Biologically Inspired Engineering at Harvard], leveraging his organs-on-chips technology to test the efficacy of AI-discovered and AI-generated antibiotics. These platforms allow us to study how drugs behave in human tissue-like environments, complementing traditional animal experiments and providing a more nuanced view of their therapeutic potential.&lt;/p&gt;&lt;p&gt;The common thread across our many collaborations is the ability to combine computational predictions with cutting-edge experimental platforms, accelerating the path from ideas to validated new therapies.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;Your research has led to many advances in designing novel antibiotics, using generative AI and deep learning. Can you talk about some of the advances you’ve been a part of in the development of drugs that can battle multi-drug-resistant pathogens, and what you see on the horizon for breakthroughs in this arena?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;In 2025, our lab published a study in&amp;nbsp;&lt;em&gt;Cell&lt;/em&gt; demonstrating how generative AI can be used to design completely new antibiotics from scratch. We used genetic algorithms and variational autoencoders to generate millions of candidate molecules, exploring both fragment-based designs and entirely unconstrained chemical space. After computational filtering, retrosynthetic modeling, and medicinal chemistry review, we synthesized 24 compounds and tested them experimentally. Seven showed selective antibacterial activity. One lead, NG1, was highly narrow-spectrum, eradicating multi-drug-resistant&amp;nbsp;&lt;em&gt;Neisseria gonorrhoeae&lt;/em&gt;, including strains resistant to first-line therapies, while sparing commensal species. Another, DN1, targeted methicillin-resistant &lt;em&gt;Staphylococcus aureus&lt;/em&gt; (MRSA) and cleared infections in mice through broad membrane disruption. Both were non-toxic and showed low rates of resistance.&lt;/p&gt;&lt;p&gt;Looking ahead, we are using deep learning to design antibiotics with drug-like properties that make them stronger candidates for clinical development. By integrating AI with high-throughput biological testing, we aim to accelerate the discovery and design of antibiotics that are novel, safe, and effective, ready for real-world therapeutic use. This approach could transform how we respond to drug-resistant bacterial pathogens, moving from a reactive to a proactive strategy in antibiotic development.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;You’re a co-founder of Phare Bio, a nonprofit organization that uses AI to discover new antibiotics, and the Collins Lab has helped to launch the Antibiotics-AI Project in collaboration with Phare Bio. Can you tell us more about what you hope to accomplish with these collaborations, and how they tie back to your research goals?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;We founded Phare Bio as a nonprofit to take the most promising antibiotic candidates emerging from the Antibiotics-AI Project at MIT and advance them toward the clinic. The idea is to bridge the gap between discovery and development by collaborating with biotech companies, pharmaceutical partners, AI companies, philanthropies, other nonprofits, and even nation states. Akhila Kosaraju has been doing a brilliant job leading Phare Bio, coordinating these efforts and moving candidates forward efficiently.&lt;/p&gt;&lt;p&gt;Recently, we received a grant from ARPA-H to use generative AI to design 15 new antibiotics and develop them as pre-clinical candidates. This project builds directly on our lab’s research, combining computational design with experimental testing to create novel antibiotics that are ready for further development. By integrating generative AI, biology, and translational partnerships, we hope to create a pipeline that can respond more rapidly to the global threat of antibiotic resistance, ultimately delivering new therapies to patients who need them most.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/jim-collins-mit-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;em&gt;In the pursuit of solutions to complex global challenges including disease, energy demands, and climate change, scientific researchers, including at MIT, have turned to artificial intelligence, and to quantitative analysis and modeling, to design and construct engineered cells with novel properties. The engineered cells can be programmed to become new therapeutics — battling, and perhaps eradicating, diseases.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;James J. Collins&lt;/em&gt;&lt;em&gt; is one of the founders of the field of synthetic biology, and is also a leading researcher in systems biology, the interdisciplinary approach that uses mathematical analysis and modeling of complex systems to better understand biological systems. His research has led to the development of new classes of diagnostics and therapeutics, including in the detection and treatment of pathogens like Ebola, Zika, SARS-CoV-2, and antibiotic-resistant bacteria. Collins, the Termeer Professor of Medical Engineering and Science and professor of biological engineering at MIT, is a core faculty member of the Institute for Medical Engineering and Science (IMES), the director of the MIT Abdul Latif Jameel Clinic for Machine Learning in Health, as well as an institute member of the Broad Institute of MIT and Harvard, and core founding faculty at the Wyss Institute for Biologically Inspired Engineering, Harvard.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;In this Q&amp;amp;A, Collins speaks about his latest work and goals for this research.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;You’re known for collaborating with colleagues across MIT, and at other institutions. How have these collaborations and affiliations helped you with your research?&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;Collaboration has been central to the work in my lab. At the MIT Jameel Clinic for Machine Learning in Health, I formed a collaboration with Regina Barzilay [the Delta Electronics Professor in the MIT Department of Electrical Engineering and Computer Science and affiliate faculty member at IMES] and Tommi Jaakkola [the Thomas Siebel Professor of Electrical Engineering and Computer Science and the Institute for Data, Systems, and Society] to use deep learning to discover new antibiotics. This effort combined our expertise in artificial intelligence, network biology, and systems microbiology, leading to the discovery of halicin, a potent new antibiotic effective against a broad range of multidrug-resistant bacterial pathogens. Our results were published in &lt;em&gt;Cell&lt;/em&gt; in 2020 and showcased the power of bringing together complementary skill sets to tackle a global health challenge.&lt;/p&gt;&lt;p&gt;At the Wyss Institute, I’ve worked closely with Donald Ingber [the Judah Folkman Professor of Vascular Biology at Harvard Medical School and the Vascular Biology Program at Boston Children’s Hospital, and Hansjörg Wyss Professor of Biologically Inspired Engineering at Harvard], leveraging his organs-on-chips technology to test the efficacy of AI-discovered and AI-generated antibiotics. These platforms allow us to study how drugs behave in human tissue-like environments, complementing traditional animal experiments and providing a more nuanced view of their therapeutic potential.&lt;/p&gt;&lt;p&gt;The common thread across our many collaborations is the ability to combine computational predictions with cutting-edge experimental platforms, accelerating the path from ideas to validated new therapies.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;Your research has led to many advances in designing novel antibiotics, using generative AI and deep learning. Can you talk about some of the advances you’ve been a part of in the development of drugs that can battle multi-drug-resistant pathogens, and what you see on the horizon for breakthroughs in this arena?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;In 2025, our lab published a study in&amp;nbsp;&lt;em&gt;Cell&lt;/em&gt; demonstrating how generative AI can be used to design completely new antibiotics from scratch. We used genetic algorithms and variational autoencoders to generate millions of candidate molecules, exploring both fragment-based designs and entirely unconstrained chemical space. After computational filtering, retrosynthetic modeling, and medicinal chemistry review, we synthesized 24 compounds and tested them experimentally. Seven showed selective antibacterial activity. One lead, NG1, was highly narrow-spectrum, eradicating multi-drug-resistant&amp;nbsp;&lt;em&gt;Neisseria gonorrhoeae&lt;/em&gt;, including strains resistant to first-line therapies, while sparing commensal species. Another, DN1, targeted methicillin-resistant &lt;em&gt;Staphylococcus aureus&lt;/em&gt; (MRSA) and cleared infections in mice through broad membrane disruption. Both were non-toxic and showed low rates of resistance.&lt;/p&gt;&lt;p&gt;Looking ahead, we are using deep learning to design antibiotics with drug-like properties that make them stronger candidates for clinical development. By integrating AI with high-throughput biological testing, we aim to accelerate the discovery and design of antibiotics that are novel, safe, and effective, ready for real-world therapeutic use. This approach could transform how we respond to drug-resistant bacterial pathogens, moving from a reactive to a proactive strategy in antibiotic development.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Q.&amp;nbsp;&lt;/strong&gt;You’re a co-founder of Phare Bio, a nonprofit organization that uses AI to discover new antibiotics, and the Collins Lab has helped to launch the Antibiotics-AI Project in collaboration with Phare Bio. Can you tell us more about what you hope to accomplish with these collaborations, and how they tie back to your research goals?&lt;/p&gt;&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;We founded Phare Bio as a nonprofit to take the most promising antibiotic candidates emerging from the Antibiotics-AI Project at MIT and advance them toward the clinic. The idea is to bridge the gap between discovery and development by collaborating with biotech companies, pharmaceutical partners, AI companies, philanthropies, other nonprofits, and even nation states. Akhila Kosaraju has been doing a brilliant job leading Phare Bio, coordinating these efforts and moving candidates forward efficiently.&lt;/p&gt;&lt;p&gt;Recently, we received a grant from ARPA-H to use generative AI to design 15 new antibiotics and develop them as pre-clinical candidates. This project builds directly on our lab’s research, combining computational design with experimental testing to create novel antibiotics that are ready for further development. By integrating generative AI, biology, and translational partnerships, we hope to create a pipeline that can respond more rapidly to the global threat of antibiotic resistance, ultimately delivering new therapies to patients who need them most.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/3-questions-using-ai-to-accelerate-discovery-design-therapeutic-drugs-0204</guid><pubDate>Wed, 04 Feb 2026 18:00:00 +0000</pubDate></item><item><title>[NEW] Tinder looks to AI to help fight ‘swipe fatigue’ and dating app burnout (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/04/tinder-looks-to-ai-to-help-fight-swipe-fatigue-and-dating-app-burnout/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/tinder-featured.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Tinder is turning to a new AI-powered feature, Chemistry, to help it reduce so-called “swipe fatigue,” a growing problem among online dating users who are feeling burned out and are in search of better outcomes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced last quarter, the Match-owned dating app said that Chemistry leverages AI to get to know users through questions and, with permission, accesses their Camera Roll on their phone to learn more about their interests and personality.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Match’s Q4 2026 earnings call, one analyst from Morgan Stanley asked for an update on the product’s success so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Match CEO Spencer Rascoff noted that Chemistry was still only being tested in Australia for the time being, but said that the feature offered users an “AI way to interact with Tinder.” He explained that users could choose to answer questions to then “get just a single drop or two, rather than swiping through many, many profiles.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to Chemistry’s Q&amp;amp;A and Camera Roll features, the company plans to use the AI feature in other ways going forward, the CEO also hinted.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most importantly, Rascoff said the feature is designed to combat swipe fatigue — a complaint from users who say they have to swipe through too many profiles to find a potential match. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s turn toward AI comes as Tinder and other dating apps have been experiencing paying subscriber declines, user burnout, and declines in new sign-ups.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the fourth quarter, new registrations on Tinder were still down 5% year-over-year, and its monthly active users were down 9%. These numbers show some slight improvements over prior quarters, which Match attributes to AI-driven recommendations that change the order of profiles shown to women, and other product experiments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Match said that this year, it aims to address common Gen Z pain points, including better relevance, authenticity and trust. To do so, the company said it is redesigning discovery to make it less repetitive and is using other features, like Face Check — a facial recognition verification system — to cut down on bad actors. On Tinder, the latter led to a more than 50% reduction in interactions with bad actors, Match noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tinder’s decision to start moving away from the swipe toward more targeted, AI-powered recommendations could have a significant impact on the dating app. Today, the swipe method, which was popularized by Tinder, encourages users to think that they’re choosing a match from an endless number of profiles. But in reality, the app presents the illusion of choice, since matches have to be two-way to connect, and even then, a spark is not guaranteed.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company delivered an earnings beat in the fourth quarter, with revenue of $878 million and EPS of 83 cents per share above Wall Street estimates. But weak guidance saw the stock decline on Tuesday, before rising again in premarket trading on Wednesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond AI, Match will also increase its product marketing to help boost Tinder engagement. The company is committing to $50 million in Tinder marketing spend, which will include creator campaigns on TikTok and Instagram, where users will make claims that “Tinder is cool again,” Rascoff noted. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/tinder-featured.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Tinder is turning to a new AI-powered feature, Chemistry, to help it reduce so-called “swipe fatigue,” a growing problem among online dating users who are feeling burned out and are in search of better outcomes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced last quarter, the Match-owned dating app said that Chemistry leverages AI to get to know users through questions and, with permission, accesses their Camera Roll on their phone to learn more about their interests and personality.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Match’s Q4 2026 earnings call, one analyst from Morgan Stanley asked for an update on the product’s success so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Match CEO Spencer Rascoff noted that Chemistry was still only being tested in Australia for the time being, but said that the feature offered users an “AI way to interact with Tinder.” He explained that users could choose to answer questions to then “get just a single drop or two, rather than swiping through many, many profiles.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to Chemistry’s Q&amp;amp;A and Camera Roll features, the company plans to use the AI feature in other ways going forward, the CEO also hinted.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Most importantly, Rascoff said the feature is designed to combat swipe fatigue — a complaint from users who say they have to swipe through too many profiles to find a potential match. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s turn toward AI comes as Tinder and other dating apps have been experiencing paying subscriber declines, user burnout, and declines in new sign-ups.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the fourth quarter, new registrations on Tinder were still down 5% year-over-year, and its monthly active users were down 9%. These numbers show some slight improvements over prior quarters, which Match attributes to AI-driven recommendations that change the order of profiles shown to women, and other product experiments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Match said that this year, it aims to address common Gen Z pain points, including better relevance, authenticity and trust. To do so, the company said it is redesigning discovery to make it less repetitive and is using other features, like Face Check — a facial recognition verification system — to cut down on bad actors. On Tinder, the latter led to a more than 50% reduction in interactions with bad actors, Match noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tinder’s decision to start moving away from the swipe toward more targeted, AI-powered recommendations could have a significant impact on the dating app. Today, the swipe method, which was popularized by Tinder, encourages users to think that they’re choosing a match from an endless number of profiles. But in reality, the app presents the illusion of choice, since matches have to be two-way to connect, and even then, a spark is not guaranteed.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company delivered an earnings beat in the fourth quarter, with revenue of $878 million and EPS of 83 cents per share above Wall Street estimates. But weak guidance saw the stock decline on Tuesday, before rising again in premarket trading on Wednesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond AI, Match will also increase its product marketing to help boost Tinder engagement. The company is committing to $50 million in Tinder marketing spend, which will include creator campaigns on TikTok and Instagram, where users will make claims that “Tinder is cool again,” Rascoff noted. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/04/tinder-looks-to-ai-to-help-fight-swipe-fatigue-and-dating-app-burnout/</guid><pubDate>Wed, 04 Feb 2026 18:08:00 +0000</pubDate></item><item><title>[NEW] Antonio Torralba, three MIT alumni named 2025 ACM fellows (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/antonio-torralba-three-mit-alumni-named-acm-fellows-0204</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202601/mit-eecs-Torralba-Antonio.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Antonio Torralba, Delta Electronics Professor of Electrical Engineering and Computer Science and faculty head of artificial intelligence and decision-making at MIT, has been named to the 2025 cohort of Association for Computing Machinery (ACM) Fellows. He shares the honor of an ACM Fellowship with three MIT alumni: Eytan Adar ’97, MEng ’98; George Candea ’97, MEng ’98; and Gookwon Edward Suh SM ’01, PhD ’05.&lt;/p&gt;&lt;p dir="ltr"&gt;A principal investigator within both the Computer Science and Artificial Intelligence Laboratory and the Center for Brains, Minds, and Machines, Torralba received his BS in telecommunications engineering from Telecom BCN, Spain, in 1994, and a PhD in signal, image, and speech processing from the Institut National Polytechnique de Grenoble, France, in 2000. At different points in his MIT career, he has been director of both the MIT Quest for Intelligence (now the MIT Siegel Family Quest for Intelligence) and the MIT-IBM Watson AI Lab.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Torralba’s research focuses on computer vision, machine learning, and human visual perception; as he puts it, “I am interested in building systems that can perceive the world like humans do.” Alongside Phillip Isola and William Freeman, he recently co-authored&amp;nbsp;“Foundations of Computer Vision,” an 800-plus page textbook exploring the foundations and core principles of the field.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Among other awards and recognitions, he is the recipient of the 2008 National Science Foundation Career award; the 2010 J. K. Aggarwal Prize from the International Association for Pattern Recognition; the 2017 Frank Quick Faculty Research Innovation Fellowship; the Louis D. Smullin (’39) Award for Teaching Excellence; and the 2020 PAMI Mark Everingham Prize. In 2021, he was awarded the inaugural Thomas Huang Memorial Prize by the Pattern Analysis and Machine Intelligence Technical Committee and was named a fellow of the Association for the Advancement of Artificial Intelligence. In 2022, he received an honorary doctoral degree from the Universitat Politècnica de Catalunya — BarcelonaTech (UPC).&amp;nbsp;&lt;/p&gt;&lt;p&gt;ACM fellows, the highest honor bestowed by the professional organization, are registered members of the society selected by their peers for&amp;nbsp;outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202601/mit-eecs-Torralba-Antonio.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Antonio Torralba, Delta Electronics Professor of Electrical Engineering and Computer Science and faculty head of artificial intelligence and decision-making at MIT, has been named to the 2025 cohort of Association for Computing Machinery (ACM) Fellows. He shares the honor of an ACM Fellowship with three MIT alumni: Eytan Adar ’97, MEng ’98; George Candea ’97, MEng ’98; and Gookwon Edward Suh SM ’01, PhD ’05.&lt;/p&gt;&lt;p dir="ltr"&gt;A principal investigator within both the Computer Science and Artificial Intelligence Laboratory and the Center for Brains, Minds, and Machines, Torralba received his BS in telecommunications engineering from Telecom BCN, Spain, in 1994, and a PhD in signal, image, and speech processing from the Institut National Polytechnique de Grenoble, France, in 2000. At different points in his MIT career, he has been director of both the MIT Quest for Intelligence (now the MIT Siegel Family Quest for Intelligence) and the MIT-IBM Watson AI Lab.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Torralba’s research focuses on computer vision, machine learning, and human visual perception; as he puts it, “I am interested in building systems that can perceive the world like humans do.” Alongside Phillip Isola and William Freeman, he recently co-authored&amp;nbsp;“Foundations of Computer Vision,” an 800-plus page textbook exploring the foundations and core principles of the field.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Among other awards and recognitions, he is the recipient of the 2008 National Science Foundation Career award; the 2010 J. K. Aggarwal Prize from the International Association for Pattern Recognition; the 2017 Frank Quick Faculty Research Innovation Fellowship; the Louis D. Smullin (’39) Award for Teaching Excellence; and the 2020 PAMI Mark Everingham Prize. In 2021, he was awarded the inaugural Thomas Huang Memorial Prize by the Pattern Analysis and Machine Intelligence Technical Committee and was named a fellow of the Association for the Advancement of Artificial Intelligence. In 2022, he received an honorary doctoral degree from the Universitat Politècnica de Catalunya — BarcelonaTech (UPC).&amp;nbsp;&lt;/p&gt;&lt;p&gt;ACM fellows, the highest honor bestowed by the professional organization, are registered members of the society selected by their peers for&amp;nbsp;outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/antonio-torralba-three-mit-alumni-named-acm-fellows-0204</guid><pubDate>Wed, 04 Feb 2026 18:15:00 +0000</pubDate></item><item><title>[NEW] Brian Hedden named co-associate dean of Social and Ethical Responsibilities of Computing (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/brian-hedden-named-co-associate-dean-serc-0204</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/brian-hedden-mit-00.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Brian Hedden PhD ’12 has been appointed co-associate dean of the Social and Ethical Responsibilities of Computing (SERC) at MIT, a cross-cutting initiative in the MIT Schwarzman College of Computing, effective Jan. 16.&lt;/p&gt;&lt;p&gt;Hedden is a professor in the Department of Linguistics and Philosophy, holding an MIT Schwarzman College of Computing shared position with the Department of Electrical Engineering and Computer Science (EECS). He joined the MIT faculty last fall from the Australian National University and the University of Sydney, where he previously served as a faculty member. He earned his BA from Princeton University and his PhD from MIT, both in philosophy.&lt;/p&gt;&lt;p&gt;“Brian is a natural and compelling choice for SERC, as a philosopher whose work speaks directly to the intellectual challenges facing education and research today, particularly in computing and AI. His expertise in epistemology, decision theory, and ethics addresses questions that have become increasingly urgent in an era defined by information abundance and artificial intelligence. His scholarship exemplifies the kind of interdisciplinary inquiry that SERC exists to advance,” says Dan Huttenlocher, dean of the MIT Schwarzman College of Computing and the Henry Ellis Warren Professor of Electrical Engineering and Computer Science.&lt;/p&gt;&lt;p&gt;Hedden’s research focuses on how we ought to form beliefs and make decisions, and it explores how philosophical thinking about rationality can yield insights into contemporary ethical issues, including ethics of AI. He is the author of “Reasons without Persons: Rationality, Identity, and Time” (Oxford University Press, 2015) and articles on topics such as collective action problems, legal standards of proof, algorithmic fairness, and political polarization.&lt;/p&gt;&lt;p&gt;Joining co-associate dean Nikos Trichakis, the J.C. Penney Professor of Management at the MIT Sloan School of Management, Hedden will help lead SERC and advance the initiative’s ongoing research, teaching, and engagement efforts. He succeeds professor of philosophy Caspar Hare, who stepped down at the conclusion of his three-year term on Sept. 1, 2025.&lt;/p&gt;&lt;p&gt;Since its inception in 2020, SERC has launched a range of programs and activities designed to cultivate responsible “habits of mind and action” among those who create and deploy computing technologies, while fostering the development of technologies in the public interest.&lt;/p&gt;&lt;p&gt;The SERC Scholars Program invites undergraduate and graduate students to work alongside postdoctoral mentors to explore interdisciplinary ethical challenges in computing. The initiative also hosts an annual prize competition that challenges MIT students to envision the future of computing, publishes a twice-yearly series of case studies, and collaborates on coordinated curricular materials, including active-learning projects, homework assignments, and in-class demonstrations. In 2024, SERC introduced a new seed grant program to support MIT researchers investigating ethical technology development; to date, two rounds of grants have been awarded to 24 projects.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/brian-hedden-mit-00.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Brian Hedden PhD ’12 has been appointed co-associate dean of the Social and Ethical Responsibilities of Computing (SERC) at MIT, a cross-cutting initiative in the MIT Schwarzman College of Computing, effective Jan. 16.&lt;/p&gt;&lt;p&gt;Hedden is a professor in the Department of Linguistics and Philosophy, holding an MIT Schwarzman College of Computing shared position with the Department of Electrical Engineering and Computer Science (EECS). He joined the MIT faculty last fall from the Australian National University and the University of Sydney, where he previously served as a faculty member. He earned his BA from Princeton University and his PhD from MIT, both in philosophy.&lt;/p&gt;&lt;p&gt;“Brian is a natural and compelling choice for SERC, as a philosopher whose work speaks directly to the intellectual challenges facing education and research today, particularly in computing and AI. His expertise in epistemology, decision theory, and ethics addresses questions that have become increasingly urgent in an era defined by information abundance and artificial intelligence. His scholarship exemplifies the kind of interdisciplinary inquiry that SERC exists to advance,” says Dan Huttenlocher, dean of the MIT Schwarzman College of Computing and the Henry Ellis Warren Professor of Electrical Engineering and Computer Science.&lt;/p&gt;&lt;p&gt;Hedden’s research focuses on how we ought to form beliefs and make decisions, and it explores how philosophical thinking about rationality can yield insights into contemporary ethical issues, including ethics of AI. He is the author of “Reasons without Persons: Rationality, Identity, and Time” (Oxford University Press, 2015) and articles on topics such as collective action problems, legal standards of proof, algorithmic fairness, and political polarization.&lt;/p&gt;&lt;p&gt;Joining co-associate dean Nikos Trichakis, the J.C. Penney Professor of Management at the MIT Sloan School of Management, Hedden will help lead SERC and advance the initiative’s ongoing research, teaching, and engagement efforts. He succeeds professor of philosophy Caspar Hare, who stepped down at the conclusion of his three-year term on Sept. 1, 2025.&lt;/p&gt;&lt;p&gt;Since its inception in 2020, SERC has launched a range of programs and activities designed to cultivate responsible “habits of mind and action” among those who create and deploy computing technologies, while fostering the development of technologies in the public interest.&lt;/p&gt;&lt;p&gt;The SERC Scholars Program invites undergraduate and graduate students to work alongside postdoctoral mentors to explore interdisciplinary ethical challenges in computing. The initiative also hosts an annual prize competition that challenges MIT students to envision the future of computing, publishes a twice-yearly series of case studies, and collaborates on coordinated curricular materials, including active-learning projects, homework assignments, and in-class demonstrations. In 2024, SERC introduced a new seed grant program to support MIT researchers investigating ethical technology development; to date, two rounds of grants have been awarded to 24 projects.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/brian-hedden-named-co-associate-dean-serc-0204</guid><pubDate>Wed, 04 Feb 2026 18:25:00 +0000</pubDate></item></channel></rss>