<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 20 Jun 2025 12:44:28 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>[NEW] Calorie restriction can help animals live longer. What about humans? (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/20/1119073/calorie-restriction-live-longer/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/calorie-restriction.jpeg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Living comes with a side effect: aging. Despite what you might hear on social media or in advertisements, there are no drugs that are known to slow or reverse human aging. But there’s some evidence to support another approach: cutting back on calories.&lt;/p&gt;  &lt;p&gt;Caloric restriction (reducing your intake of calories) and intermittent fasting (switching between fasting and eating normally on a fixed schedule) can help with weight loss. But they may also offer protection against some health conditions. And some believe such diets might even help you live longer—a finding supported by new research out this week. (Longevity enthusiast Bryan Johnson famously claims to eat his last meal of the day at 12pm.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;But the full picture is not so simple. Weight loss isn’t always healthy and neither is restricting your calorie intake, especially if your BMI is low to begin with. Some scientists warn that, based on evidence in animals, it could negatively impact wound healing, metabolism and bone density. This week let’s take a closer look at the benefits—and risks—of caloric restriction.&lt;/p&gt;  &lt;p&gt;Eating less can make animals live longer. This remarkable finding has been published in scientific journals for the last 100 years. It seems to work across almost every animal studied—everything from tiny nematode worms and fruit flies to mice, rats and even monkeys. It can extend the lifespan of rodents by between 15 and 60%, depending on which study you look at.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;The effect of caloric restriction is more reliable than the leading contenders for an “anti-aging” drug.&lt;/strong&gt; Both rapamycin (an immunosuppressive drug used in organ transplants) and metformin (a diabetes drug) have been touted as potential longevity therapeutics. And both have been found to increase the lifespans of animals in some studies.&lt;/p&gt;  &lt;p&gt;But when scientists looked across 167 published studies of those three interventions in research animals, they found that caloric restriction was the most “robust.” According to&amp;nbsp;their research, published in the journal &lt;em&gt;Aging Cell&lt;/em&gt; on Wednesday, the effect of rapamycin was somewhat comparable, but metformin was nowhere near as effective.&lt;/p&gt; 
 &lt;p&gt;“That is a pity for the many people now taking off-label metformin for lifespan extension,” David Clancy, lecturer in biogerontology at Lancaster University said in a statement. “Let’s hope it doesn’t have any or many adverse effects.” Still, for caloric restriction, so far so good.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;At least it’s good news for lab animals. What about people?&lt;/strong&gt; Also on Wednesday, another team of scientists published a separate review of research investigating the effects of caloric restriction and fasting on humans. That review assessed 99 clinical trials, involving over 6,500 adults. (Like I said, caloric restriction has been an active area of research for a long time.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Those researchers found that, across all those trials, fasting and caloric restriction did seem to aid weight loss. There were other benefits, too—but they depended on the specific approach to dieting. Fasting every other day seemed to help lower cholesterol, for example. Time-restricted eating where you only eat within a specific period each day (à la Bryan Johnson), by comparison, seemed to &lt;em&gt;increase&lt;/em&gt; cholesterol, the researchers&amp;nbsp;write in &lt;em&gt;The BMJ&lt;/em&gt;. Given that elevated cholesterol in the blood can lead to heart disease, it’s not great news for the time-restricted eaters.&lt;/p&gt;  &lt;p&gt;Cutting calories&amp;nbsp;could also carry broader risks. Dietary restriction seems to impair wound healing in mice and rats, for example. Caloric restriction also seems to affect bone density. In some studies, the biggest effects on lifespan extension are seen when rats are put on calorie-restricted diets early in life. But this approach can affect bone development and reduce bone density by 9 to 30%.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;&lt;strong&gt;It’s also &lt;em&gt;really hard&lt;/em&gt; for most people to cut their caloric intake. &lt;/strong&gt;When researchers ran a two-year trial to measure the impact of a 25% reduction in caloric intake, they found that&amp;nbsp;the most their volunteers could cut was 12%. (That study found that caloric restriction reduces markers of inflammation, which can be harmful when it’s chronic, and had only a small impact on bone density.)&lt;/p&gt;  &lt;p&gt;Unfortunately, there’s a lot we still don’t really understand about caloric restriction. It doesn’t seem to help &lt;em&gt;all&lt;/em&gt; animals live longer—it seems to shorten the lifespan of animals with certain genetic backgrounds. And we don’t know whether it extends the lifespan of people. It isn’t possible to conduct a randomized clinical trial in which you deprive people of food from childhood and then wait their entire lives to see when they die.&lt;/p&gt;  &lt;p&gt;It is notoriously difficult to track or change your diet. And given the unknowns surrounding caloric restriction, it’s too soon to make sweeping recommendations, particularly given that your own personal biology will play a role in any benefits or risks you’ll experience. Roll on the next round of research.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article first appeared in The Checkup,&amp;nbsp;&lt;/em&gt;MIT Technology Review’s&lt;em&gt;&amp;nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/calorie-restriction.jpeg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Living comes with a side effect: aging. Despite what you might hear on social media or in advertisements, there are no drugs that are known to slow or reverse human aging. But there’s some evidence to support another approach: cutting back on calories.&lt;/p&gt;  &lt;p&gt;Caloric restriction (reducing your intake of calories) and intermittent fasting (switching between fasting and eating normally on a fixed schedule) can help with weight loss. But they may also offer protection against some health conditions. And some believe such diets might even help you live longer—a finding supported by new research out this week. (Longevity enthusiast Bryan Johnson famously claims to eat his last meal of the day at 12pm.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;But the full picture is not so simple. Weight loss isn’t always healthy and neither is restricting your calorie intake, especially if your BMI is low to begin with. Some scientists warn that, based on evidence in animals, it could negatively impact wound healing, metabolism and bone density. This week let’s take a closer look at the benefits—and risks—of caloric restriction.&lt;/p&gt;  &lt;p&gt;Eating less can make animals live longer. This remarkable finding has been published in scientific journals for the last 100 years. It seems to work across almost every animal studied—everything from tiny nematode worms and fruit flies to mice, rats and even monkeys. It can extend the lifespan of rodents by between 15 and 60%, depending on which study you look at.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;The effect of caloric restriction is more reliable than the leading contenders for an “anti-aging” drug.&lt;/strong&gt; Both rapamycin (an immunosuppressive drug used in organ transplants) and metformin (a diabetes drug) have been touted as potential longevity therapeutics. And both have been found to increase the lifespans of animals in some studies.&lt;/p&gt;  &lt;p&gt;But when scientists looked across 167 published studies of those three interventions in research animals, they found that caloric restriction was the most “robust.” According to&amp;nbsp;their research, published in the journal &lt;em&gt;Aging Cell&lt;/em&gt; on Wednesday, the effect of rapamycin was somewhat comparable, but metformin was nowhere near as effective.&lt;/p&gt; 
 &lt;p&gt;“That is a pity for the many people now taking off-label metformin for lifespan extension,” David Clancy, lecturer in biogerontology at Lancaster University said in a statement. “Let’s hope it doesn’t have any or many adverse effects.” Still, for caloric restriction, so far so good.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;At least it’s good news for lab animals. What about people?&lt;/strong&gt; Also on Wednesday, another team of scientists published a separate review of research investigating the effects of caloric restriction and fasting on humans. That review assessed 99 clinical trials, involving over 6,500 adults. (Like I said, caloric restriction has been an active area of research for a long time.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Those researchers found that, across all those trials, fasting and caloric restriction did seem to aid weight loss. There were other benefits, too—but they depended on the specific approach to dieting. Fasting every other day seemed to help lower cholesterol, for example. Time-restricted eating where you only eat within a specific period each day (à la Bryan Johnson), by comparison, seemed to &lt;em&gt;increase&lt;/em&gt; cholesterol, the researchers&amp;nbsp;write in &lt;em&gt;The BMJ&lt;/em&gt;. Given that elevated cholesterol in the blood can lead to heart disease, it’s not great news for the time-restricted eaters.&lt;/p&gt;  &lt;p&gt;Cutting calories&amp;nbsp;could also carry broader risks. Dietary restriction seems to impair wound healing in mice and rats, for example. Caloric restriction also seems to affect bone density. In some studies, the biggest effects on lifespan extension are seen when rats are put on calorie-restricted diets early in life. But this approach can affect bone development and reduce bone density by 9 to 30%.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;&lt;strong&gt;It’s also &lt;em&gt;really hard&lt;/em&gt; for most people to cut their caloric intake. &lt;/strong&gt;When researchers ran a two-year trial to measure the impact of a 25% reduction in caloric intake, they found that&amp;nbsp;the most their volunteers could cut was 12%. (That study found that caloric restriction reduces markers of inflammation, which can be harmful when it’s chronic, and had only a small impact on bone density.)&lt;/p&gt;  &lt;p&gt;Unfortunately, there’s a lot we still don’t really understand about caloric restriction. It doesn’t seem to help &lt;em&gt;all&lt;/em&gt; animals live longer—it seems to shorten the lifespan of animals with certain genetic backgrounds. And we don’t know whether it extends the lifespan of people. It isn’t possible to conduct a randomized clinical trial in which you deprive people of food from childhood and then wait their entire lives to see when they die.&lt;/p&gt;  &lt;p&gt;It is notoriously difficult to track or change your diet. And given the unknowns surrounding caloric restriction, it’s too soon to make sweeping recommendations, particularly given that your own personal biology will play a role in any benefits or risks you’ll experience. Roll on the next round of research.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article first appeared in The Checkup,&amp;nbsp;&lt;/em&gt;MIT Technology Review’s&lt;em&gt;&amp;nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/20/1119073/calorie-restriction-live-longer/</guid><pubDate>Fri, 20 Jun 2025 09:00:00 +0000</pubDate></item><item><title>[NEW] Unlock the other 99% of your data – now ready for AI (AI News)</title><link>https://www.artificialintelligence-news.com/news/unlock-the-other-99-of-your-data-now-ready-for-ai/</link><description>&lt;p&gt;For decades, companies of all sizes have recognized that the data available to them holds significant value, for improving user and customer experiences and for developing strategic plans based on empirical evidence.&lt;/p&gt;&lt;p&gt;As AI becomes increasingly accessible and practical for real-world business applications, the potential value of available data has grown exponentially. Successfully adopting AI requires significant effort in data collection, curation, and preprocessing. Moreover, important aspects such as data governance, privacy, anonymization, regulatory compliance, and security must be addressed carefully from the outset.&lt;/p&gt;&lt;p&gt;In a conversation with Henrique Lemes, Americas Data Platform Leader at IBM, we explored the challenges enterprises face in implementing practical AI in a range of use cases. We began by examining the nature of data itself, its various types, and its role in enabling effective AI-powered applications.&lt;/p&gt;&lt;p&gt;Henrique highlighted that referring to all enterprise information simply as ‘data’ understates its complexity. The modern enterprise navigates a fragmented landscape of diverse data types and inconsistent quality, particularly between structured and unstructured sources.&lt;/p&gt;&lt;p&gt;In simple terms, structured data refers to information that is organized in a standardized and easily searchable format, one that enables efficient processing and analysis by software systems.&lt;/p&gt;&lt;p&gt;Unstructured data is information that does not follow a predefined format nor organizational model, making it more complex to process and analyze. Unlike structured data, it includes diverse formats like emails, social media posts, videos, images, documents, and audio files. While it lacks the clear organization of structured data, unstructured data holds valuable insights that, when effectively managed through advanced analytics and AI, can drive innovation and inform strategic business decisions.&lt;/p&gt;&lt;p&gt;Henrique stated, “Currently, less than 1% of enterprise data is utilized by generative AI, and over 90% of that data is unstructured, which directly affects trust and quality”.&lt;/p&gt;&lt;p&gt;The element of trust in terms of data is an important one. Decision-makers in an organization need firm belief (trust) that the information at their fingertips is complete, reliable, and properly obtained. But there is evidence that states less than half of data available to businesses is used for AI, with unstructured data often going ignored or sidelined due to the complexity of processing it and examining it for compliance – especially at scale.&lt;/p&gt;&lt;p&gt;To open the way to better decisions that are based on a fuller set of empirical data, the trickle of easily consumed information needs to be turned into a firehose. Automated ingestion is the answer in this respect, Henrique said, but the governance rules and data policies still must be applied – to unstructured and structured data alike.&lt;/p&gt;&lt;p&gt;Henrique set out the three processes that let enterprises leverage the inherent value of their data. “Firstly, ingestion at scale. It’s important to automate this process. Second, curation and data governance. And the third [is when] you make this available for generative AI. We achieve over 40% of ROI over any conventional RAG use-case.”&lt;/p&gt;&lt;p&gt;IBM provides a unified strategy, rooted in a deep understanding of the enterprise’s AI journey, combined with advanced software solutions and domain expertise. This enables organizations to efficiently and securely transform both structured and unstructured data into AI-ready assets, all within the boundaries of existing governance and compliance frameworks.&lt;/p&gt;&lt;p&gt;“We bring together the people, processes, and tools. It’s not inherently simple, but we simplify it by aligning all the essential resources,” he said.&lt;/p&gt;&lt;figure class="wp-block-image alignleft size-full"&gt;&lt;img alt="alt" class="wp-image-106820" height="689" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/06/henrique-lemes-1.jpeg" width="689" /&gt;&lt;/figure&gt;&lt;p&gt;As businesses scale and transform, the diversity and volume of their data increase. To keep up, AI data ingestion process must be both scalable and flexible.&lt;/p&gt;&lt;p&gt;“[Companies] encounter difficulties when scaling because their AI solutions were initially built for specific tasks. When they attempt to broaden their scope, they often aren’t ready, the data pipelines grow more complex, and managing unstructured data becomes essential. This drives an increased demand for effective data governance,” he said.&lt;/p&gt;&lt;p&gt;IBM’s approach is to thoroughly understand each client’s AI journey, creating a clear roadmap to achieve ROI through effective AI implementation. “We prioritize data accuracy, whether structured or unstructured, along with data ingestion, lineage, governance, compliance with industry-specific regulations, and the necessary observability. These capabilities enable our clients to scale across multiple use cases and fully capitalize on the value of their data,” Henrique said.&lt;/p&gt;&lt;p&gt;Like anything worthwhile in technology implementation, it takes time to put the right processes in place, gravitate to the right tools, and have the necessary vision of how any data solution might need to evolve.&lt;/p&gt;&lt;p&gt;IBM offers enterprises a range of options and tooling to enable AI workloads in even the most regulated industries, at any scale. With international banks, finance houses, and global multinationals among its client roster, there are few substitutes for Big Blue in this context.&lt;/p&gt;&lt;p&gt;To find out more about enabling data pipelines for AI that drive business and offer fast, significant ROI, &lt;u&gt;head over to this page&lt;/u&gt;.&lt;/p&gt;</description><content:encoded>&lt;p&gt;For decades, companies of all sizes have recognized that the data available to them holds significant value, for improving user and customer experiences and for developing strategic plans based on empirical evidence.&lt;/p&gt;&lt;p&gt;As AI becomes increasingly accessible and practical for real-world business applications, the potential value of available data has grown exponentially. Successfully adopting AI requires significant effort in data collection, curation, and preprocessing. Moreover, important aspects such as data governance, privacy, anonymization, regulatory compliance, and security must be addressed carefully from the outset.&lt;/p&gt;&lt;p&gt;In a conversation with Henrique Lemes, Americas Data Platform Leader at IBM, we explored the challenges enterprises face in implementing practical AI in a range of use cases. We began by examining the nature of data itself, its various types, and its role in enabling effective AI-powered applications.&lt;/p&gt;&lt;p&gt;Henrique highlighted that referring to all enterprise information simply as ‘data’ understates its complexity. The modern enterprise navigates a fragmented landscape of diverse data types and inconsistent quality, particularly between structured and unstructured sources.&lt;/p&gt;&lt;p&gt;In simple terms, structured data refers to information that is organized in a standardized and easily searchable format, one that enables efficient processing and analysis by software systems.&lt;/p&gt;&lt;p&gt;Unstructured data is information that does not follow a predefined format nor organizational model, making it more complex to process and analyze. Unlike structured data, it includes diverse formats like emails, social media posts, videos, images, documents, and audio files. While it lacks the clear organization of structured data, unstructured data holds valuable insights that, when effectively managed through advanced analytics and AI, can drive innovation and inform strategic business decisions.&lt;/p&gt;&lt;p&gt;Henrique stated, “Currently, less than 1% of enterprise data is utilized by generative AI, and over 90% of that data is unstructured, which directly affects trust and quality”.&lt;/p&gt;&lt;p&gt;The element of trust in terms of data is an important one. Decision-makers in an organization need firm belief (trust) that the information at their fingertips is complete, reliable, and properly obtained. But there is evidence that states less than half of data available to businesses is used for AI, with unstructured data often going ignored or sidelined due to the complexity of processing it and examining it for compliance – especially at scale.&lt;/p&gt;&lt;p&gt;To open the way to better decisions that are based on a fuller set of empirical data, the trickle of easily consumed information needs to be turned into a firehose. Automated ingestion is the answer in this respect, Henrique said, but the governance rules and data policies still must be applied – to unstructured and structured data alike.&lt;/p&gt;&lt;p&gt;Henrique set out the three processes that let enterprises leverage the inherent value of their data. “Firstly, ingestion at scale. It’s important to automate this process. Second, curation and data governance. And the third [is when] you make this available for generative AI. We achieve over 40% of ROI over any conventional RAG use-case.”&lt;/p&gt;&lt;p&gt;IBM provides a unified strategy, rooted in a deep understanding of the enterprise’s AI journey, combined with advanced software solutions and domain expertise. This enables organizations to efficiently and securely transform both structured and unstructured data into AI-ready assets, all within the boundaries of existing governance and compliance frameworks.&lt;/p&gt;&lt;p&gt;“We bring together the people, processes, and tools. It’s not inherently simple, but we simplify it by aligning all the essential resources,” he said.&lt;/p&gt;&lt;figure class="wp-block-image alignleft size-full"&gt;&lt;img alt="alt" class="wp-image-106820" height="689" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/06/henrique-lemes-1.jpeg" width="689" /&gt;&lt;/figure&gt;&lt;p&gt;As businesses scale and transform, the diversity and volume of their data increase. To keep up, AI data ingestion process must be both scalable and flexible.&lt;/p&gt;&lt;p&gt;“[Companies] encounter difficulties when scaling because their AI solutions were initially built for specific tasks. When they attempt to broaden their scope, they often aren’t ready, the data pipelines grow more complex, and managing unstructured data becomes essential. This drives an increased demand for effective data governance,” he said.&lt;/p&gt;&lt;p&gt;IBM’s approach is to thoroughly understand each client’s AI journey, creating a clear roadmap to achieve ROI through effective AI implementation. “We prioritize data accuracy, whether structured or unstructured, along with data ingestion, lineage, governance, compliance with industry-specific regulations, and the necessary observability. These capabilities enable our clients to scale across multiple use cases and fully capitalize on the value of their data,” Henrique said.&lt;/p&gt;&lt;p&gt;Like anything worthwhile in technology implementation, it takes time to put the right processes in place, gravitate to the right tools, and have the necessary vision of how any data solution might need to evolve.&lt;/p&gt;&lt;p&gt;IBM offers enterprises a range of options and tooling to enable AI workloads in even the most regulated industries, at any scale. With international banks, finance houses, and global multinationals among its client roster, there are few substitutes for Big Blue in this context.&lt;/p&gt;&lt;p&gt;To find out more about enabling data pipelines for AI that drive business and offer fast, significant ROI, &lt;u&gt;head over to this page&lt;/u&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/unlock-the-other-99-of-your-data-now-ready-for-ai/</guid><pubDate>Fri, 20 Jun 2025 09:34:00 +0000</pubDate></item><item><title>[NEW] How a 30-year-old techno-thriller predicted our digital isolation (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/20/1118379/the-net-cyber-thriller-digital-isolation/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;In April, Mark Zuckerberg, as tech billionaires are so fond of doing these days, pontificated at punishing length on a podcast. In the interview, he addressed America’s loneliness epidemic: “The average American has—I think it’s fewer than three friends. And the average person has demand for meaningfully more. I think it’s like 15 friends or something, right?”&lt;/p&gt;  &lt;p&gt;Before you’ve had a moment to register the ominous way in which he frames human connection in such bleak economic terms, he offers his solution to the loneliness epidemic: AI friends. Ideally AI friends &lt;em&gt;his&lt;/em&gt; company generates.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;“It’s like I’m not even me anymore.”&lt;br /&gt;—Angela Bennett, &lt;em&gt;The Net&lt;/em&gt; (1995)&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Thirty years ago, Irwin Winkler’s proto–cyber thriller, &lt;em&gt;The Net&lt;/em&gt;, was released. It was 1995, commonly regarded as the year Hollywood &lt;em&gt;discovered&lt;/em&gt; the internet. Sandra Bullock played a social recluse and computer nerd for hire named Angela Bennett, who unwittingly uncovers a sinister computer security conspiracy. She soon finds her life turned upside down as the conspiracists begin systematically destroying her credibility and reputation. Her job, home, finances, and very identity are seemingly erased with some judicial tweaks to key computer records.&lt;/p&gt; 
 &lt;p&gt;Bennett is uniquely—conveniently, perhaps—well positioned for this identity annihilation. Her mother, in the throes of dementia, no longer recognizes her; she works from home for clients who have never met her; her social circle is limited to an online chat room; she orders takeout from Pizza.net; her neighbors don’t even know what she looks like. Her most reliable companion is the screen in front of her. A wild, unimaginable scenario that I’m sure none of us can relate to.&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;“Just think about it. Our whole world is sitting there on a computer. It’s in the computer, everything: your DMV records, your Social Security, your credit cards, your medical records. It’s all right there. Everyone is stored in there. It’s like this little electronic shadow on each and every one of us, just begging for someone to screw with, and you know what? They’ve done it to me, and you know what? They’re gonna do it to you.”&lt;br /&gt;—Angela Bennett, &lt;em&gt;The Net&lt;/em&gt;&lt;/p&gt; 
 &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;While the villain of &lt;em&gt;The Net&lt;/em&gt; is ultimately a nefarious cybersecurity software company, the film’s preoccupying fear is much more fundamental: If all of our data is digitized, what happens if the people with access to that information tamper with it? Or weaponize it against us?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;This period of Hollywood’s flirtation with the internet is often referred to as the era of the technophobic thriller, but that’s a surface-level misreading. Techno-&lt;em&gt;skeptic&lt;/em&gt; might be more accurate. These films were broadly positive and excited about new technology; it almost always played a role in how the hero saved the day. Their bigger concern was with the humans who had ultimate control of these tools, and what oversight and restrictions we should place on them.&lt;/p&gt;  &lt;p&gt;In 2025, however, the most prescient part of &lt;em&gt;The Net&lt;/em&gt; is Angela Bennett’s digital alienation. What was originally a series of plausible enough contrivances to make the theft of her identity more believable is now just part of our everyday lives. We all bank, shop, eat, work, and socialize without necessarily seeing another human being in person. And we’ve all been through covid lockdowns where that isolation was actively encouraged. For a whole generation of young people who lived through that, socializing face to face is not second nature. In 2023, the World Health Organization declared loneliness to be a pressing global health threat, estimating that one in four older adults experience social isolation and between 5% and 15% of adolescents experience loneliness. In the US, social isolation may threaten public health more seriously than obesity.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;The Net&lt;/em&gt; appeared at a time when the internet was only faintly understood as the new Wild West … In that sense, it remains a fascinating time capsule of a moment when the possibilities to come felt endless, the outlook cautiously optimistic.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;We also spend increasing amounts of time looking at our phones, where finely tuned algorithms aggressively lobby for more and more of our ad-revenue-­generating attention. As Bennett warns: “Our whole lives are on the computer, and they knew that I could be vanished. They knew that nobody would care, that nobody would understand.” In this sense, in 2025 &lt;em&gt;we are all Angela Bennett&lt;/em&gt;. As Bennett’s digital alienation makes her more vulnerable to pernicious actors, so too are we increasingly at risk from those who don’t have, and have never had, our best interests at heart.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;To blame technology entirely for a rise in loneliness—as many policymakers are doing—would be a mistake. While it is unquestionably playing a part in exacerbating the problem, its outsize role in our lives has always reflected larger underlying factors. In &lt;em&gt;Multitudes: How Crowds Made the Modern World &lt;/em&gt;(2024), the journalist Dan Hancox examines the ways in which crowds have been demonized and othered by those in power and suggests that our alienation is much more structural: “Whether through government cuts or concessions to the expansive ambitions of private enterprise, a key reason we have all become a bit more crowd-shy in recent decades is the prolonged, top-down assault on public space and the wider public realm—what are sometimes called the urban commons. From properly funded libraries to pleasant, open parks and squares, free or affordable sports and leisure facilities, safe, accessible and cheap public transport, comfortable street furniture and free public toilets, and a vibrant, varied, uncommodified social and cultural life—all the best things about city life fall under the heading of the public realm, and all of them facilitate and support happy crowds rather than sad, alienated, stay-at-home loners.”&lt;/p&gt;  &lt;p&gt;Nearly half a century ago Margaret Thatcher laid out the neoliberal consensus that would frame the next decades of individualism: “There’s no such thing as society. There are individual men and women and there are families. And no government can do anything except through people, and people must look after themselves first.”&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118694" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/TheNet2.jpg?w=1954" width="1954" /&gt;&lt;div class="image-credit"&gt;TOM HUMBERSTONE&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In keeping with that philosophy, social connectivity has been outsourced to tech companies for which the attention economy is paramount. “The Algo” is our new, capricious god. If your livelihood depends on engagement, the temptation is to stop thinking about human connection when you post, and to think more about what will satisfy The Algo to ensure a good harvest.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;How much will you trust an AI chatbot powered by Meta to be your friend? Answers to this may vary. Even if you won’t, other people are already making close connections with “AI companions” or “falling in love” with ChatGPT. The rise of “cognitive offloading”—of people asking AI to do their critical thinking for them—is already well underway, with many high school and college students admitting to a deep reliance on the technology.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Beyond the obvious concern that AI “friends” are hallucinating, unthinking, obsequious algorithms that will never challenge you in the way a real friend might, it’s also worth remembering who AI actually works for. Recently Elon Musk’s own AI chatbot, Grok, was given new edicts that caused it to cast doubt on the Holocaust and talk about “white genocide” in response to unrelated prompts—a reminder, if we needed it, that these systems are never neutral, never apolitical, and always at the command of those with their hands on the code.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I’m fairly lucky. I live with my partner and have a decent community of friends. But I work from home and can spend the majority of the day not talking to anyone. I’m not immune to feeling isolated, anxious, and powerless as I stare unblinking at my news feed. I think we all feel it. &lt;em&gt;We are all Angela Bennett&lt;/em&gt;. Weaponizing that alienation, as the antagonists of &lt;em&gt;The Net&lt;/em&gt; do, can of course be used for identity theft. But it can also have much more deleterious applications: Our loneliness can be manipulated to make us consume more, work longer, turn against ourselves and each other. AI “friendships,” if engaged with uncritically, are only going to supercharge this disaffection and the ways in which it can be abused.&lt;/p&gt;  &lt;p&gt;It doesn’t have to be this way. We can withhold our attention, practice healthier screen routines, limit our exposure to doomscrolling, refuse to engage with energy-guzzling AI, delete our accounts. But, crucially, we can also organize collectively IRL: join a union or a local club, ask our friends if they need to talk. Hopelessness is what those in power want us to feel, so resist it.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The Net&lt;/em&gt; appeared at a time when the internet was only faintly understood as the new Wild West. Before the dot-com boom and bust, before Web 2.0, before the walled gardens and the theory of a “dead internet.” In that sense, it remains a fascinating time capsule of a moment when the possibilities to come felt endless, the outlook cautiously optimistic.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt;&lt;p&gt;We can also see &lt;em&gt;The Net&lt;/em&gt;’s influence in modern screen-life films like &lt;em&gt;Searching&lt;/em&gt;, &lt;em&gt;Host&lt;/em&gt;, &lt;em&gt;Unfriended&lt;/em&gt;, and &lt;em&gt;The Den&lt;/em&gt;. But perhaps—hopefully—its most enduring legacy will be inviting us to go outside, touch grass, talk to another human being, and organize.&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;“Find the others.”&lt;br /&gt;—Douglas Rushkoff,&amp;nbsp;&lt;em&gt;Team Human &lt;/em&gt;(2019)&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;Tom Humberstone is a comic artist and illustrator based in Edinburgh.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;In April, Mark Zuckerberg, as tech billionaires are so fond of doing these days, pontificated at punishing length on a podcast. In the interview, he addressed America’s loneliness epidemic: “The average American has—I think it’s fewer than three friends. And the average person has demand for meaningfully more. I think it’s like 15 friends or something, right?”&lt;/p&gt;  &lt;p&gt;Before you’ve had a moment to register the ominous way in which he frames human connection in such bleak economic terms, he offers his solution to the loneliness epidemic: AI friends. Ideally AI friends &lt;em&gt;his&lt;/em&gt; company generates.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;“It’s like I’m not even me anymore.”&lt;br /&gt;—Angela Bennett, &lt;em&gt;The Net&lt;/em&gt; (1995)&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Thirty years ago, Irwin Winkler’s proto–cyber thriller, &lt;em&gt;The Net&lt;/em&gt;, was released. It was 1995, commonly regarded as the year Hollywood &lt;em&gt;discovered&lt;/em&gt; the internet. Sandra Bullock played a social recluse and computer nerd for hire named Angela Bennett, who unwittingly uncovers a sinister computer security conspiracy. She soon finds her life turned upside down as the conspiracists begin systematically destroying her credibility and reputation. Her job, home, finances, and very identity are seemingly erased with some judicial tweaks to key computer records.&lt;/p&gt; 
 &lt;p&gt;Bennett is uniquely—conveniently, perhaps—well positioned for this identity annihilation. Her mother, in the throes of dementia, no longer recognizes her; she works from home for clients who have never met her; her social circle is limited to an online chat room; she orders takeout from Pizza.net; her neighbors don’t even know what she looks like. Her most reliable companion is the screen in front of her. A wild, unimaginable scenario that I’m sure none of us can relate to.&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;“Just think about it. Our whole world is sitting there on a computer. It’s in the computer, everything: your DMV records, your Social Security, your credit cards, your medical records. It’s all right there. Everyone is stored in there. It’s like this little electronic shadow on each and every one of us, just begging for someone to screw with, and you know what? They’ve done it to me, and you know what? They’re gonna do it to you.”&lt;br /&gt;—Angela Bennett, &lt;em&gt;The Net&lt;/em&gt;&lt;/p&gt; 
 &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;While the villain of &lt;em&gt;The Net&lt;/em&gt; is ultimately a nefarious cybersecurity software company, the film’s preoccupying fear is much more fundamental: If all of our data is digitized, what happens if the people with access to that information tamper with it? Or weaponize it against us?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;This period of Hollywood’s flirtation with the internet is often referred to as the era of the technophobic thriller, but that’s a surface-level misreading. Techno-&lt;em&gt;skeptic&lt;/em&gt; might be more accurate. These films were broadly positive and excited about new technology; it almost always played a role in how the hero saved the day. Their bigger concern was with the humans who had ultimate control of these tools, and what oversight and restrictions we should place on them.&lt;/p&gt;  &lt;p&gt;In 2025, however, the most prescient part of &lt;em&gt;The Net&lt;/em&gt; is Angela Bennett’s digital alienation. What was originally a series of plausible enough contrivances to make the theft of her identity more believable is now just part of our everyday lives. We all bank, shop, eat, work, and socialize without necessarily seeing another human being in person. And we’ve all been through covid lockdowns where that isolation was actively encouraged. For a whole generation of young people who lived through that, socializing face to face is not second nature. In 2023, the World Health Organization declared loneliness to be a pressing global health threat, estimating that one in four older adults experience social isolation and between 5% and 15% of adolescents experience loneliness. In the US, social isolation may threaten public health more seriously than obesity.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;The Net&lt;/em&gt; appeared at a time when the internet was only faintly understood as the new Wild West … In that sense, it remains a fascinating time capsule of a moment when the possibilities to come felt endless, the outlook cautiously optimistic.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;We also spend increasing amounts of time looking at our phones, where finely tuned algorithms aggressively lobby for more and more of our ad-revenue-­generating attention. As Bennett warns: “Our whole lives are on the computer, and they knew that I could be vanished. They knew that nobody would care, that nobody would understand.” In this sense, in 2025 &lt;em&gt;we are all Angela Bennett&lt;/em&gt;. As Bennett’s digital alienation makes her more vulnerable to pernicious actors, so too are we increasingly at risk from those who don’t have, and have never had, our best interests at heart.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;To blame technology entirely for a rise in loneliness—as many policymakers are doing—would be a mistake. While it is unquestionably playing a part in exacerbating the problem, its outsize role in our lives has always reflected larger underlying factors. In &lt;em&gt;Multitudes: How Crowds Made the Modern World &lt;/em&gt;(2024), the journalist Dan Hancox examines the ways in which crowds have been demonized and othered by those in power and suggests that our alienation is much more structural: “Whether through government cuts or concessions to the expansive ambitions of private enterprise, a key reason we have all become a bit more crowd-shy in recent decades is the prolonged, top-down assault on public space and the wider public realm—what are sometimes called the urban commons. From properly funded libraries to pleasant, open parks and squares, free or affordable sports and leisure facilities, safe, accessible and cheap public transport, comfortable street furniture and free public toilets, and a vibrant, varied, uncommodified social and cultural life—all the best things about city life fall under the heading of the public realm, and all of them facilitate and support happy crowds rather than sad, alienated, stay-at-home loners.”&lt;/p&gt;  &lt;p&gt;Nearly half a century ago Margaret Thatcher laid out the neoliberal consensus that would frame the next decades of individualism: “There’s no such thing as society. There are individual men and women and there are families. And no government can do anything except through people, and people must look after themselves first.”&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1118694" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/TheNet2.jpg?w=1954" width="1954" /&gt;&lt;div class="image-credit"&gt;TOM HUMBERSTONE&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In keeping with that philosophy, social connectivity has been outsourced to tech companies for which the attention economy is paramount. “The Algo” is our new, capricious god. If your livelihood depends on engagement, the temptation is to stop thinking about human connection when you post, and to think more about what will satisfy The Algo to ensure a good harvest.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;How much will you trust an AI chatbot powered by Meta to be your friend? Answers to this may vary. Even if you won’t, other people are already making close connections with “AI companions” or “falling in love” with ChatGPT. The rise of “cognitive offloading”—of people asking AI to do their critical thinking for them—is already well underway, with many high school and college students admitting to a deep reliance on the technology.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Beyond the obvious concern that AI “friends” are hallucinating, unthinking, obsequious algorithms that will never challenge you in the way a real friend might, it’s also worth remembering who AI actually works for. Recently Elon Musk’s own AI chatbot, Grok, was given new edicts that caused it to cast doubt on the Holocaust and talk about “white genocide” in response to unrelated prompts—a reminder, if we needed it, that these systems are never neutral, never apolitical, and always at the command of those with their hands on the code.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I’m fairly lucky. I live with my partner and have a decent community of friends. But I work from home and can spend the majority of the day not talking to anyone. I’m not immune to feeling isolated, anxious, and powerless as I stare unblinking at my news feed. I think we all feel it. &lt;em&gt;We are all Angela Bennett&lt;/em&gt;. Weaponizing that alienation, as the antagonists of &lt;em&gt;The Net&lt;/em&gt; do, can of course be used for identity theft. But it can also have much more deleterious applications: Our loneliness can be manipulated to make us consume more, work longer, turn against ourselves and each other. AI “friendships,” if engaged with uncritically, are only going to supercharge this disaffection and the ways in which it can be abused.&lt;/p&gt;  &lt;p&gt;It doesn’t have to be this way. We can withhold our attention, practice healthier screen routines, limit our exposure to doomscrolling, refuse to engage with energy-guzzling AI, delete our accounts. But, crucially, we can also organize collectively IRL: join a union or a local club, ask our friends if they need to talk. Hopelessness is what those in power want us to feel, so resist it.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;The Net&lt;/em&gt; appeared at a time when the internet was only faintly understood as the new Wild West. Before the dot-com boom and bust, before Web 2.0, before the walled gardens and the theory of a “dead internet.” In that sense, it remains a fascinating time capsule of a moment when the possibilities to come felt endless, the outlook cautiously optimistic.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt;&lt;p&gt;We can also see &lt;em&gt;The Net&lt;/em&gt;’s influence in modern screen-life films like &lt;em&gt;Searching&lt;/em&gt;, &lt;em&gt;Host&lt;/em&gt;, &lt;em&gt;Unfriended&lt;/em&gt;, and &lt;em&gt;The Den&lt;/em&gt;. But perhaps—hopefully—its most enduring legacy will be inviting us to go outside, touch grass, talk to another human being, and organize.&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;“Find the others.”&lt;br /&gt;—Douglas Rushkoff,&amp;nbsp;&lt;em&gt;Team Human &lt;/em&gt;(2019)&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;Tom Humberstone is a comic artist and illustrator based in Edinburgh.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/20/1118379/the-net-cyber-thriller-digital-isolation/</guid><pubDate>Fri, 20 Jun 2025 10:00:00 +0000</pubDate></item><item><title>[NEW] Study: Meta AI model can reproduce almost half of Harry Potter book (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/06/study-metas-llama-3-1-can-recall-42-percent-of-the-first-harry-potter-book/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The research could have big implications for generative AI copyright lawsuits.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2173576600-640x426.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2173576600-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Meta CEO Mark Zuckerberg.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Andrej Sokolow/picture alliance via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;In recent years, numerous plaintiffs—including publishers of books, newspapers, computer code, and photographs—have&amp;nbsp;sued AI companies&amp;nbsp;for training models using copyrighted material. A key question in all of these lawsuits has been how easily AI models produce verbatim excerpts from the plaintiffs’ copyrighted content.&lt;/p&gt;
&lt;p&gt;For example, in its&amp;nbsp;December 2023 lawsuit against OpenAI, The New York Times Company produced dozens of examples where GPT-4 exactly reproduced significant passages from Times&amp;nbsp;stories. In its&amp;nbsp;response, OpenAI described this as a “fringe behavior” and a “problem that researchers at OpenAI and elsewhere work hard to address.”&lt;/p&gt;
&lt;p&gt;But is it actually a fringe behavior? And have leading AI companies addressed it?&amp;nbsp;New research—focusing on books rather than newspaper articles and on different companies—provides surprising insights into this question. Some of the findings should bolster plaintiffs’ arguments, while others may be more helpful to defendants.&lt;/p&gt;
&lt;p&gt;The paper was published last month by a team of computer scientists and legal scholars from Stanford, Cornell, and West Virginia University. They studied whether five popular open-weight models—three from Meta and one each from Microsoft and EleutherAI—were able to reproduce text from Books3, a collection of books that is widely used to train LLMs. Many of the books are still under copyright.&lt;/p&gt;
&lt;p&gt;This chart illustrates their most surprising finding:&lt;/p&gt;
&lt;div class="captioned-image-container"&gt;
&lt;figure&gt;
&lt;div class="image2-inset"&gt;
&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="1154" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d9755f9-cfad-48cc-a4cb-482c0f9c3bb5_1072x1154.png" width="1072" /&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;The chart shows how easy it is to get a model to generate 50-token excerpts from various parts of&amp;nbsp;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt;. The darker a line is, the easier it is to reproduce that portion of the book.&lt;/p&gt;
&lt;p&gt;Each row represents a different model. The three bottom rows are Llama models from Meta. And as you can see, Llama 3.1 70B—a mid-sized model Meta released in July 2024—is far more likely to reproduce Harry Potter text than any of the other four models.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Specifically, the paper estimates that Llama 3.1 70B has memorized&amp;nbsp;&lt;em&gt;42 percent&lt;/em&gt;&amp;nbsp;of the first Harry Potter book well enough to reproduce 50-token excerpts at least half the time. (I’ll unpack how this was measured in the next section.)&lt;/p&gt;
&lt;p&gt;Interestingly, Llama 1 65B, a similar-sized model released in February 2023, had memorized only 4.4 percent of&amp;nbsp;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt;. This suggests that despite the potential legal liability, Meta did not do much to prevent memorization as it trained Llama 3. At least for this book, the problem got much worse between Llama 1 and Llama 3.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt;&amp;nbsp;was one of dozens of books tested by the researchers. They found that Llama 3.1 70B was far more likely to reproduce popular books—such as&amp;nbsp;&lt;em&gt;The Hobbit&lt;/em&gt;&amp;nbsp;and George Orwell’s&amp;nbsp;&lt;em&gt;1984&lt;/em&gt;—than obscure ones. And for most books, Llama 3.1 70B memorized more than any of the other models.&lt;/p&gt;
&lt;p&gt;“There are really striking differences among models in terms of how much verbatim text they have memorized,” said James Grimmelmann, a Cornell law professor who has collaborated with several of the paper’s authors.&lt;/p&gt;
&lt;p&gt;The results surprised the study’s authors, including Mark Lemley, a law professor at Stanford. (Lemley used to be part of Meta's legal team, but in January, he dropped them as a client after Facebook adopted more Trump-friendly moderation policies.)&lt;/p&gt;
&lt;p&gt;“We'd expected to see some kind of low level of replicability on the order of 1 or 2 percent,” Lemley told me. “The first thing that surprised me is how much variation there is.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;These results give everyone in the AI copyright debate something to latch onto. For AI industry critics, the big takeaway is that—at least for some models and some books—memorization is not a fringe phenomenon.&lt;/p&gt;
&lt;p&gt;On the other hand, the study only found significant memorization of a few popular books. For example, the researchers found that Llama 3.1 70B only memorized 0.13 percent of&amp;nbsp;&lt;em&gt;Sandman Slim&lt;/em&gt;, a 2009 novel by author Richard Kadrey. That’s a tiny fraction of the 42 percent figure for&amp;nbsp;&lt;em&gt;Harry Potter&lt;/em&gt;.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This could be a headache for law firms that have filed class-action lawsuits against AI companies. Kadrey is the lead plaintiff in a class-action lawsuit against Meta. To certify a class of plaintiffs, a court must find that the plaintiffs are in largely similar legal and factual situations.&lt;/p&gt;
&lt;p&gt;Divergent results like these could cast doubt on whether it makes sense to lump J.K. Rowling, Kadrey, and thousands of other authors together in a single mass lawsuit. And that could work in Meta’s favor, since most authors lack the resources to file individual lawsuits.&lt;/p&gt;
&lt;p&gt;The broader lesson of this study is that the details will matter in these copyright cases. Too often, online discussions have treated “do generative models copy their training data or merely learn from it?” as a theoretical or even philosophical question. But it’s a question that can be tested empirically—and the answer might differ across models and across copyrighted works.&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;How they measured memorization&lt;/h2&gt;
&lt;p&gt;It’s common to talk about LLMs predicting the next token. But under the hood, what the model actually does is generate a probability distribution over&amp;nbsp;&lt;em&gt;all&lt;/em&gt;&amp;nbsp;possibilities for the next token. For example, if you prompt an LLM with the phrase “Peanut butter and,” it will respond with a probability distribution that might look like this made-up example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(“jelly”) = 70 percent&lt;/li&gt;
&lt;li&gt;P(“sugar”) = 9 percent&lt;/li&gt;
&lt;li&gt;P(“peanut”) = 6 percent&lt;/li&gt;
&lt;li&gt;P(“chocolate”) = 4 percent&lt;/li&gt;
&lt;li&gt;P(“cream”) = 3 percent&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And so forth.&lt;/p&gt;
&lt;p&gt;After the model generates a list of probabilities like this, the system will select one of these options at random, weighted by their probabilities. So 70 percent of the time the system will generate “Peanut butter and jelly.” Nine percent of the time, we’ll get “Peanut butter and sugar.” Six percent of the time, it will be “Peanut butter and peanut.” You get the idea.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The study’s authors didn’t have to generate multiple outputs to estimate the likelihood of a particular response. Instead, they could calculate probabilities for each token and then multiply them together.&lt;/p&gt;
&lt;p&gt;Suppose someone wants to estimate the probability that a model will respond to “My favorite sandwich is” with “peanut butter and jelly.” Here’s how to do that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prompt the model with “My favorite sandwich is,” and look up the probability of “peanut” (let’s say it’s &lt;strong&gt;20 percent&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Prompt the model with “My favorite sandwich is peanut,” and look up the probability of “butter” (let’s say it’s &lt;strong&gt;90 percent&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Prompt the model with “My favorite sandwich is peanut butter” and look up the probability of “and” (let’s say it’s&amp;nbsp;&lt;strong&gt;80 percent&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Prompt the model with “My favorite sandwich is peanut butter and” and look up the probability of “jelly” (let’s say it’s&amp;nbsp;&lt;strong&gt;70 percent&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then we just have to multiply the probabilities like this:&lt;/p&gt;
&lt;p&gt;0.2 * 0.9 * 0.8 * 0.7 = 0.1008&lt;/p&gt;
&lt;p&gt;So we can predict that the model will produce “peanut butter and jelly” about 10 percent of the time, without actually generating 100 or 1,000 outputs and counting how many of them were that exact phrase.&lt;/p&gt;
&lt;p&gt;This technique greatly reduced the cost of the research, allowed the authors to analyze more books, and made it feasible to precisely estimate very low probabilities.&lt;/p&gt;
&lt;p&gt;For example, the authors estimated that it would take more than 10 quadrillion samples to exactly reproduce some 50-token sequences from some books. Obviously, it wouldn’t be feasible to actually generate that many outputs. But it wasn’t necessary: the probability could be estimated just by multiplying the probabilities for the 50 tokens.&lt;/p&gt;
&lt;p&gt;A key thing to notice is that probabilities can get really small really fast. In my made-up example, the probability that the model will produce the four tokens “peanut butter and jelly” is just 10 percent. If we added more tokens, the probability would get even lower. If we added&amp;nbsp;&lt;em&gt;46 more tokens&lt;/em&gt;, the probability could fall by several orders of magnitude.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For any language model, the probability of generating any given 50-token sequence “by accident” is vanishingly small. If a model generates 50 tokens from a copyrighted work, that is strong evidence that the tokens “came from” the training data. This is true even if it only generates those tokens 10 percent, 1 percent, or 0.01 percent of the time.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2 class="header-anchor-post"&gt;We don’t know how Harry Potter got into Llama models&lt;/h2&gt;
&lt;p&gt;The study authors took 36 books and divided each of them into overlapping 100-token passages. Using the first 50 tokens as a prompt, they calculated the probability that the next 50 tokens would be identical to the original passage. They counted a passage as “memorized” if the model had a greater than 50 percent chance of reproducing it word for word.&lt;/p&gt;
&lt;p&gt;This definition is quite strict. For a 50-token sequence to have a probability greater than 50 percent, the average token in the passage needs a probability of at least 98.5 percent! Moreover, the authors only counted exact matches. They didn’t try to count cases where—for example—the model generates 48 or 49 tokens from the original passage but got one or two tokens wrong. If these cases were counted, the amount of memorization would be even higher.&lt;/p&gt;
&lt;p&gt;This research provides strong evidence that significant portions of&amp;nbsp;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt; were copied into the weights of Llama 3.1 70B. But this finding doesn’t tell us why or how this happened. I suspect that part of the answer is that Llama 3 70B was trained on 15 trillion tokens—more than 10 times the 1.4 trillion tokens used to train Llama 1 65B.&lt;/p&gt;
&lt;p&gt;The more times a model is trained on a particular example, the more likely it is to memorize that example. Perhaps Meta had trouble finding 15 trillion distinct tokens, so it trained on the Books3 dataset multiple times. Or maybe Meta added third-party sources—such as online Harry Potter fan forums, consumer book reviews, or student book reports—that included quotes from &lt;em&gt;Harry Potter&lt;/em&gt;&amp;nbsp;and other popular books.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;I’m not sure that either of these explanations fully fits the facts. The fact that memorization was a much bigger problem for the most popular books does suggest that Llama may have been trained on secondary sources that quote these books rather than the books themselves. There are likely exponentially more online discussions of Harry Potter than Sandman Slim.&lt;/p&gt;
&lt;p&gt;On the other hand, it’s surprising that Llama memorized&lt;em&gt;&amp;nbsp;&lt;/em&gt;so much of&amp;nbsp;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;“If it were citations and quotations, you'd expect it to concentrate around a few popular things that everyone quotes or talks about,” Lemley said. The fact that Llama 3 memorized almost half the book suggests that the entire text was well represented in the training data.&lt;/p&gt;
&lt;p&gt;Or there could be another explanation entirely. Maybe Meta made subtle changes in its training recipe that accidentally worsened the memorization problem. I emailed Meta for comment last week but haven’t heard back.&lt;/p&gt;
&lt;p&gt;“It doesn't seem to be all popular books,” Mark Lemley told me. “Some popular books have this result and not others. It’s hard to come up with a clear story that says why that happened.”&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;Three theories of liability&lt;/h2&gt;
&lt;div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"&gt;
&lt;div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"&gt;There are actually three distinct theories of how training a model on copyrighted works could infringe copyright:&lt;/div&gt;
&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Training on a copyrighted work is inherently infringing because the training process involves making a digital copy of the work.&lt;/li&gt;
&lt;li&gt;The training process copies information from the training data into the model, making the model a derivative work under copyright law.&lt;/li&gt;
&lt;li&gt;Infringement occurs when a model generates (portions of) a copyrighted work.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A lot of discussion so far has focused on the first theory because it is the most threatening to AI companies. If the courts uphold this theory, most current LLMs would be illegal, whether or not they have memorized any training data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The AI industry has some pretty strong arguments that&amp;nbsp;using copyrighted works during the training process is fair use&amp;nbsp;under the 2015 Google Books ruling. But the fact that Llama 3.1 70B memorized large portions of&amp;nbsp;&lt;em&gt;Harry Potter&lt;/em&gt; could color how the courts consider these fair use questions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;A key part of fair use analysis is whether a use is “transformative”—whether a company has made something new or is merely profiting from the work of others. The fact that language models are capable of regurgitating substantial portions of popular works like&amp;nbsp;&lt;em&gt;Harry Potter&lt;/em&gt;,&amp;nbsp;&lt;em&gt;1984&lt;/em&gt;, and&amp;nbsp;&lt;em&gt;The Hobbit&lt;/em&gt; could cause judges to look at these fair use arguments more skeptically.&lt;/p&gt;
&lt;p&gt;Moreover, one of Google’s key arguments in the books case was that its system was designed to never return more than a short excerpt from any book. If the judge in the Meta lawsuit wanted to distinguish Meta’s arguments from the ones Google made in the books case, he could point to the fact that Llama can generate far more than a few lines of&amp;nbsp;&lt;em&gt;Harry Potter&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The new study “complicates the story that the defendants have been telling in these cases,” co-author Mark Lemley told me. “Which is ‘we just learn word patterns. None of that shows up in the model.’”&lt;/p&gt;
&lt;p&gt;But the Harry Potter result creates even more danger for Meta under that second theory—that Llama itself is a derivative copy of Rowling’s book.&lt;/p&gt;
&lt;p&gt;“It's clear that you can in fact extract substantial parts of Harry Potter and various other books from the model,” Lemley said. “That suggests to me that probably for some of those books there's something the law would call a copy of part of the book in the model itself.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The Google Books precedent probably can’t protect Meta against this second legal theory because Google never made its books database available for users to download—Google almost certainly would have lost the case if it had done that.&lt;/p&gt;
&lt;p&gt;In principle, Meta could still convince a judge that copying 42 percent of Harry Potter was allowed under the flexible, judge-made doctrine of fair use. But it would be an uphill battle.&lt;/p&gt;
&lt;p&gt;“The fair use analysis you've gotta do is not just ‘is the training set fair use,’ but ‘is the incorporation in the model fair use?’" Lemley said. "That complicates the defendants' story.”&lt;/p&gt;
&lt;p&gt;Grimmelmann also said there’s a danger that this research could put open-weight models in greater legal jeopardy than closed-weight ones. The Cornell and Stanford researchers could only do their work because the authors had access to the underlying model—and hence to the token probability values that allowed efficient calculation of probabilities for sequences of tokens.&lt;/p&gt;
&lt;p&gt;Most leading labs, including OpenAI, Anthropic, and Google, have increasingly restricted access to these so-called logits, making it more difficult to study these models.&lt;/p&gt;
&lt;p&gt;Moreover, if a company keeps model weights on its own servers, it can use filters to try to prevent infringing output from reaching the outside world. So even if the underlying OpenAI, Anthropic, and Google models have memorized copyrighted works in the same way as Llama 3.1 70B, it might be difficult for anyone outside the company to prove it.&lt;/p&gt;
&lt;p&gt;Moreover, this kind of filtering makes it easier for companies with closed-weight models to invoke the Google Books precedent. In short, copyright law might create a strong disincentive for companies to release open-weight models.&lt;/p&gt;
&lt;p&gt;“It's kind of perverse,” Mark Lemley told me. “I don't like that outcome.”&lt;/p&gt;
&lt;p&gt;On the other hand, judges might conclude that it would be bad to effectively punish companies for publishing open-weight models.&lt;/p&gt;
&lt;p&gt;“There's a degree to which being open and sharing weights is a kind of public service,” Grimmelmann told me. “I could honestly see judges being less skeptical of Meta and others who provide open-weight models.”&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Timothy B. Lee was on staff at Ars Technica from 2017 to 2021. Today, he writes &lt;/i&gt;&lt;i&gt;Understanding AI,&lt;/i&gt;&lt;i&gt;&amp;nbsp;a newsletter that explores how AI works and how it's changing our world. You can subscribe&amp;nbsp;&lt;/i&gt;&lt;i&gt;here&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The research could have big implications for generative AI copyright lawsuits.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2173576600-640x426.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2173576600-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Meta CEO Mark Zuckerberg.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Andrej Sokolow/picture alliance via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;In recent years, numerous plaintiffs—including publishers of books, newspapers, computer code, and photographs—have&amp;nbsp;sued AI companies&amp;nbsp;for training models using copyrighted material. A key question in all of these lawsuits has been how easily AI models produce verbatim excerpts from the plaintiffs’ copyrighted content.&lt;/p&gt;
&lt;p&gt;For example, in its&amp;nbsp;December 2023 lawsuit against OpenAI, The New York Times Company produced dozens of examples where GPT-4 exactly reproduced significant passages from Times&amp;nbsp;stories. In its&amp;nbsp;response, OpenAI described this as a “fringe behavior” and a “problem that researchers at OpenAI and elsewhere work hard to address.”&lt;/p&gt;
&lt;p&gt;But is it actually a fringe behavior? And have leading AI companies addressed it?&amp;nbsp;New research—focusing on books rather than newspaper articles and on different companies—provides surprising insights into this question. Some of the findings should bolster plaintiffs’ arguments, while others may be more helpful to defendants.&lt;/p&gt;
&lt;p&gt;The paper was published last month by a team of computer scientists and legal scholars from Stanford, Cornell, and West Virginia University. They studied whether five popular open-weight models—three from Meta and one each from Microsoft and EleutherAI—were able to reproduce text from Books3, a collection of books that is widely used to train LLMs. Many of the books are still under copyright.&lt;/p&gt;
&lt;p&gt;This chart illustrates their most surprising finding:&lt;/p&gt;
&lt;div class="captioned-image-container"&gt;
&lt;figure&gt;
&lt;div class="image2-inset"&gt;
&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="1154" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d9755f9-cfad-48cc-a4cb-482c0f9c3bb5_1072x1154.png" width="1072" /&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;The chart shows how easy it is to get a model to generate 50-token excerpts from various parts of&amp;nbsp;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt;. The darker a line is, the easier it is to reproduce that portion of the book.&lt;/p&gt;
&lt;p&gt;Each row represents a different model. The three bottom rows are Llama models from Meta. And as you can see, Llama 3.1 70B—a mid-sized model Meta released in July 2024—is far more likely to reproduce Harry Potter text than any of the other four models.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Specifically, the paper estimates that Llama 3.1 70B has memorized&amp;nbsp;&lt;em&gt;42 percent&lt;/em&gt;&amp;nbsp;of the first Harry Potter book well enough to reproduce 50-token excerpts at least half the time. (I’ll unpack how this was measured in the next section.)&lt;/p&gt;
&lt;p&gt;Interestingly, Llama 1 65B, a similar-sized model released in February 2023, had memorized only 4.4 percent of&amp;nbsp;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt;. This suggests that despite the potential legal liability, Meta did not do much to prevent memorization as it trained Llama 3. At least for this book, the problem got much worse between Llama 1 and Llama 3.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt;&amp;nbsp;was one of dozens of books tested by the researchers. They found that Llama 3.1 70B was far more likely to reproduce popular books—such as&amp;nbsp;&lt;em&gt;The Hobbit&lt;/em&gt;&amp;nbsp;and George Orwell’s&amp;nbsp;&lt;em&gt;1984&lt;/em&gt;—than obscure ones. And for most books, Llama 3.1 70B memorized more than any of the other models.&lt;/p&gt;
&lt;p&gt;“There are really striking differences among models in terms of how much verbatim text they have memorized,” said James Grimmelmann, a Cornell law professor who has collaborated with several of the paper’s authors.&lt;/p&gt;
&lt;p&gt;The results surprised the study’s authors, including Mark Lemley, a law professor at Stanford. (Lemley used to be part of Meta's legal team, but in January, he dropped them as a client after Facebook adopted more Trump-friendly moderation policies.)&lt;/p&gt;
&lt;p&gt;“We'd expected to see some kind of low level of replicability on the order of 1 or 2 percent,” Lemley told me. “The first thing that surprised me is how much variation there is.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;These results give everyone in the AI copyright debate something to latch onto. For AI industry critics, the big takeaway is that—at least for some models and some books—memorization is not a fringe phenomenon.&lt;/p&gt;
&lt;p&gt;On the other hand, the study only found significant memorization of a few popular books. For example, the researchers found that Llama 3.1 70B only memorized 0.13 percent of&amp;nbsp;&lt;em&gt;Sandman Slim&lt;/em&gt;, a 2009 novel by author Richard Kadrey. That’s a tiny fraction of the 42 percent figure for&amp;nbsp;&lt;em&gt;Harry Potter&lt;/em&gt;.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;This could be a headache for law firms that have filed class-action lawsuits against AI companies. Kadrey is the lead plaintiff in a class-action lawsuit against Meta. To certify a class of plaintiffs, a court must find that the plaintiffs are in largely similar legal and factual situations.&lt;/p&gt;
&lt;p&gt;Divergent results like these could cast doubt on whether it makes sense to lump J.K. Rowling, Kadrey, and thousands of other authors together in a single mass lawsuit. And that could work in Meta’s favor, since most authors lack the resources to file individual lawsuits.&lt;/p&gt;
&lt;p&gt;The broader lesson of this study is that the details will matter in these copyright cases. Too often, online discussions have treated “do generative models copy their training data or merely learn from it?” as a theoretical or even philosophical question. But it’s a question that can be tested empirically—and the answer might differ across models and across copyrighted works.&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;How they measured memorization&lt;/h2&gt;
&lt;p&gt;It’s common to talk about LLMs predicting the next token. But under the hood, what the model actually does is generate a probability distribution over&amp;nbsp;&lt;em&gt;all&lt;/em&gt;&amp;nbsp;possibilities for the next token. For example, if you prompt an LLM with the phrase “Peanut butter and,” it will respond with a probability distribution that might look like this made-up example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(“jelly”) = 70 percent&lt;/li&gt;
&lt;li&gt;P(“sugar”) = 9 percent&lt;/li&gt;
&lt;li&gt;P(“peanut”) = 6 percent&lt;/li&gt;
&lt;li&gt;P(“chocolate”) = 4 percent&lt;/li&gt;
&lt;li&gt;P(“cream”) = 3 percent&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And so forth.&lt;/p&gt;
&lt;p&gt;After the model generates a list of probabilities like this, the system will select one of these options at random, weighted by their probabilities. So 70 percent of the time the system will generate “Peanut butter and jelly.” Nine percent of the time, we’ll get “Peanut butter and sugar.” Six percent of the time, it will be “Peanut butter and peanut.” You get the idea.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The study’s authors didn’t have to generate multiple outputs to estimate the likelihood of a particular response. Instead, they could calculate probabilities for each token and then multiply them together.&lt;/p&gt;
&lt;p&gt;Suppose someone wants to estimate the probability that a model will respond to “My favorite sandwich is” with “peanut butter and jelly.” Here’s how to do that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prompt the model with “My favorite sandwich is,” and look up the probability of “peanut” (let’s say it’s &lt;strong&gt;20 percent&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Prompt the model with “My favorite sandwich is peanut,” and look up the probability of “butter” (let’s say it’s &lt;strong&gt;90 percent&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Prompt the model with “My favorite sandwich is peanut butter” and look up the probability of “and” (let’s say it’s&amp;nbsp;&lt;strong&gt;80 percent&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Prompt the model with “My favorite sandwich is peanut butter and” and look up the probability of “jelly” (let’s say it’s&amp;nbsp;&lt;strong&gt;70 percent&lt;/strong&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then we just have to multiply the probabilities like this:&lt;/p&gt;
&lt;p&gt;0.2 * 0.9 * 0.8 * 0.7 = 0.1008&lt;/p&gt;
&lt;p&gt;So we can predict that the model will produce “peanut butter and jelly” about 10 percent of the time, without actually generating 100 or 1,000 outputs and counting how many of them were that exact phrase.&lt;/p&gt;
&lt;p&gt;This technique greatly reduced the cost of the research, allowed the authors to analyze more books, and made it feasible to precisely estimate very low probabilities.&lt;/p&gt;
&lt;p&gt;For example, the authors estimated that it would take more than 10 quadrillion samples to exactly reproduce some 50-token sequences from some books. Obviously, it wouldn’t be feasible to actually generate that many outputs. But it wasn’t necessary: the probability could be estimated just by multiplying the probabilities for the 50 tokens.&lt;/p&gt;
&lt;p&gt;A key thing to notice is that probabilities can get really small really fast. In my made-up example, the probability that the model will produce the four tokens “peanut butter and jelly” is just 10 percent. If we added more tokens, the probability would get even lower. If we added&amp;nbsp;&lt;em&gt;46 more tokens&lt;/em&gt;, the probability could fall by several orders of magnitude.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For any language model, the probability of generating any given 50-token sequence “by accident” is vanishingly small. If a model generates 50 tokens from a copyrighted work, that is strong evidence that the tokens “came from” the training data. This is true even if it only generates those tokens 10 percent, 1 percent, or 0.01 percent of the time.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2 class="header-anchor-post"&gt;We don’t know how Harry Potter got into Llama models&lt;/h2&gt;
&lt;p&gt;The study authors took 36 books and divided each of them into overlapping 100-token passages. Using the first 50 tokens as a prompt, they calculated the probability that the next 50 tokens would be identical to the original passage. They counted a passage as “memorized” if the model had a greater than 50 percent chance of reproducing it word for word.&lt;/p&gt;
&lt;p&gt;This definition is quite strict. For a 50-token sequence to have a probability greater than 50 percent, the average token in the passage needs a probability of at least 98.5 percent! Moreover, the authors only counted exact matches. They didn’t try to count cases where—for example—the model generates 48 or 49 tokens from the original passage but got one or two tokens wrong. If these cases were counted, the amount of memorization would be even higher.&lt;/p&gt;
&lt;p&gt;This research provides strong evidence that significant portions of&amp;nbsp;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt; were copied into the weights of Llama 3.1 70B. But this finding doesn’t tell us why or how this happened. I suspect that part of the answer is that Llama 3 70B was trained on 15 trillion tokens—more than 10 times the 1.4 trillion tokens used to train Llama 1 65B.&lt;/p&gt;
&lt;p&gt;The more times a model is trained on a particular example, the more likely it is to memorize that example. Perhaps Meta had trouble finding 15 trillion distinct tokens, so it trained on the Books3 dataset multiple times. Or maybe Meta added third-party sources—such as online Harry Potter fan forums, consumer book reviews, or student book reports—that included quotes from &lt;em&gt;Harry Potter&lt;/em&gt;&amp;nbsp;and other popular books.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;I’m not sure that either of these explanations fully fits the facts. The fact that memorization was a much bigger problem for the most popular books does suggest that Llama may have been trained on secondary sources that quote these books rather than the books themselves. There are likely exponentially more online discussions of Harry Potter than Sandman Slim.&lt;/p&gt;
&lt;p&gt;On the other hand, it’s surprising that Llama memorized&lt;em&gt;&amp;nbsp;&lt;/em&gt;so much of&amp;nbsp;&lt;em&gt;Harry Potter and the Sorcerer's Stone&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;“If it were citations and quotations, you'd expect it to concentrate around a few popular things that everyone quotes or talks about,” Lemley said. The fact that Llama 3 memorized almost half the book suggests that the entire text was well represented in the training data.&lt;/p&gt;
&lt;p&gt;Or there could be another explanation entirely. Maybe Meta made subtle changes in its training recipe that accidentally worsened the memorization problem. I emailed Meta for comment last week but haven’t heard back.&lt;/p&gt;
&lt;p&gt;“It doesn't seem to be all popular books,” Mark Lemley told me. “Some popular books have this result and not others. It’s hard to come up with a clear story that says why that happened.”&lt;/p&gt;
&lt;h2 class="header-anchor-post"&gt;Three theories of liability&lt;/h2&gt;
&lt;div class="pencraft pc-display-flex pc-alignItems-center pc-position-absolute pc-reset header-anchor-parent"&gt;
&lt;div class="pencraft pc-display-contents pc-reset pubTheme-yiXxQA"&gt;There are actually three distinct theories of how training a model on copyrighted works could infringe copyright:&lt;/div&gt;
&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Training on a copyrighted work is inherently infringing because the training process involves making a digital copy of the work.&lt;/li&gt;
&lt;li&gt;The training process copies information from the training data into the model, making the model a derivative work under copyright law.&lt;/li&gt;
&lt;li&gt;Infringement occurs when a model generates (portions of) a copyrighted work.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A lot of discussion so far has focused on the first theory because it is the most threatening to AI companies. If the courts uphold this theory, most current LLMs would be illegal, whether or not they have memorized any training data.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The AI industry has some pretty strong arguments that&amp;nbsp;using copyrighted works during the training process is fair use&amp;nbsp;under the 2015 Google Books ruling. But the fact that Llama 3.1 70B memorized large portions of&amp;nbsp;&lt;em&gt;Harry Potter&lt;/em&gt; could color how the courts consider these fair use questions.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;A key part of fair use analysis is whether a use is “transformative”—whether a company has made something new or is merely profiting from the work of others. The fact that language models are capable of regurgitating substantial portions of popular works like&amp;nbsp;&lt;em&gt;Harry Potter&lt;/em&gt;,&amp;nbsp;&lt;em&gt;1984&lt;/em&gt;, and&amp;nbsp;&lt;em&gt;The Hobbit&lt;/em&gt; could cause judges to look at these fair use arguments more skeptically.&lt;/p&gt;
&lt;p&gt;Moreover, one of Google’s key arguments in the books case was that its system was designed to never return more than a short excerpt from any book. If the judge in the Meta lawsuit wanted to distinguish Meta’s arguments from the ones Google made in the books case, he could point to the fact that Llama can generate far more than a few lines of&amp;nbsp;&lt;em&gt;Harry Potter&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The new study “complicates the story that the defendants have been telling in these cases,” co-author Mark Lemley told me. “Which is ‘we just learn word patterns. None of that shows up in the model.’”&lt;/p&gt;
&lt;p&gt;But the Harry Potter result creates even more danger for Meta under that second theory—that Llama itself is a derivative copy of Rowling’s book.&lt;/p&gt;
&lt;p&gt;“It's clear that you can in fact extract substantial parts of Harry Potter and various other books from the model,” Lemley said. “That suggests to me that probably for some of those books there's something the law would call a copy of part of the book in the model itself.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The Google Books precedent probably can’t protect Meta against this second legal theory because Google never made its books database available for users to download—Google almost certainly would have lost the case if it had done that.&lt;/p&gt;
&lt;p&gt;In principle, Meta could still convince a judge that copying 42 percent of Harry Potter was allowed under the flexible, judge-made doctrine of fair use. But it would be an uphill battle.&lt;/p&gt;
&lt;p&gt;“The fair use analysis you've gotta do is not just ‘is the training set fair use,’ but ‘is the incorporation in the model fair use?’" Lemley said. "That complicates the defendants' story.”&lt;/p&gt;
&lt;p&gt;Grimmelmann also said there’s a danger that this research could put open-weight models in greater legal jeopardy than closed-weight ones. The Cornell and Stanford researchers could only do their work because the authors had access to the underlying model—and hence to the token probability values that allowed efficient calculation of probabilities for sequences of tokens.&lt;/p&gt;
&lt;p&gt;Most leading labs, including OpenAI, Anthropic, and Google, have increasingly restricted access to these so-called logits, making it more difficult to study these models.&lt;/p&gt;
&lt;p&gt;Moreover, if a company keeps model weights on its own servers, it can use filters to try to prevent infringing output from reaching the outside world. So even if the underlying OpenAI, Anthropic, and Google models have memorized copyrighted works in the same way as Llama 3.1 70B, it might be difficult for anyone outside the company to prove it.&lt;/p&gt;
&lt;p&gt;Moreover, this kind of filtering makes it easier for companies with closed-weight models to invoke the Google Books precedent. In short, copyright law might create a strong disincentive for companies to release open-weight models.&lt;/p&gt;
&lt;p&gt;“It's kind of perverse,” Mark Lemley told me. “I don't like that outcome.”&lt;/p&gt;
&lt;p&gt;On the other hand, judges might conclude that it would be bad to effectively punish companies for publishing open-weight models.&lt;/p&gt;
&lt;p&gt;“There's a degree to which being open and sharing weights is a kind of public service,” Grimmelmann told me. “I could honestly see judges being less skeptical of Meta and others who provide open-weight models.”&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Timothy B. Lee was on staff at Ars Technica from 2017 to 2021. Today, he writes &lt;/i&gt;&lt;i&gt;Understanding AI,&lt;/i&gt;&lt;i&gt;&amp;nbsp;a newsletter that explores how AI works and how it's changing our world. You can subscribe&amp;nbsp;&lt;/i&gt;&lt;i&gt;here&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/06/study-metas-llama-3-1-can-recall-42-percent-of-the-first-harry-potter-book/</guid><pubDate>Fri, 20 Jun 2025 11:00:14 +0000</pubDate></item><item><title>[NEW] The Download: talking dirty with DeepSeek, and the risks and rewards of calorie restriction (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/20/1119102/the-download-talking-dirty-with-deepseek-and-the-risks-and-rewards-of-calorie-restriction/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;It’s pretty easy to get DeepSeek to talk dirty&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;AI companions like Replika are designed to engage in intimate exchanges, but people use general-purpose chatbots for sex talk too, despite their stricter content moderation policies. Now new research shows that not all chatbots are equally willing to talk dirty. DeepSeek is the easiest to convince. But other AI chatbots can be enticed too.&lt;/p&gt;&lt;p&gt;Huiqian Lai, a PhD student at Syracuse University, found vast differences in how mainstream models process sexual queries, from steadfast rejection to performative refusal followed by the requested sexually explicit content.&lt;/p&gt;&lt;p&gt;The findings highlight inconsistencies in LLMs’ safety boundaries that could, in certain situations, become harmful. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Rhiannon Williams&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Calorie restriction can help animals live longer. What about humans?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Living comes with a side effect: aging. Despite what you might hear on social media, there are no drugs that are known to slow or reverse human aging. But there’s some evidence to support another approach: cutting back on calories.&lt;/p&gt;  &lt;p&gt;Reducing your intake of calories and fasting can help with weight loss. But they may also offer protection against some health conditions. And some believe such diets might even help you live longer—a finding supported by new research out this week.&lt;/p&gt;&lt;p&gt;However, the full picture is not so simple. Let’s take a closer look at the benefits—and risks—of caloric restriction.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article first appeared in The Checkup, MIT Technology Review’s weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;strong&gt;How a 30-year-old techno-thriller predicted our digital isolation&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Thirty years ago, Irwin Winkler’s proto–cyber thriller, &lt;em&gt;The Net&lt;/em&gt;, was released. It was 1995, commonly regarded as the year Hollywood discovered the internet. Sandra Bullock played a social recluse and computer nerd for hire named Angela Bennett, who unwittingly uncovers a sinister computer security conspiracy. She soon finds her life turned upside down as the conspiracists begin systematically destroying her credibility and reputation.&lt;/p&gt;&lt;p&gt;While the villain of &lt;em&gt;The Net&lt;/em&gt; is ultimately a nefarious cybersecurity software company, the film’s preoccupying fear is much more fundamental: If all of our data is digitized, what happens if the people with access to that information tamper with it? Or weaponize it against us? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tom Humberstone&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the next print edition of MIT Technology Review, which explores power—who has it, and who wants it. It’s set to go live on Wednesday June 25, so &lt;/strong&gt;&lt;strong&gt;subscribe &amp;amp; save 25%&lt;/strong&gt;&lt;strong&gt; to read it and get a copy of the issue when it lands!&lt;/strong&gt;&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Donald Trump has extended TikTok’s deadline for a third time&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;He’s granted it yet another 90-day reprieve. (WSJ $)&lt;br /&gt;+ &lt;em&gt;He says he needs more time to broker a deal. &lt;/em&gt;(AP News)&lt;br /&gt;+ &lt;em&gt;But it’s not clear if Trump’s orders are even legal. &lt;/em&gt;(Bloomberg $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;2 A SpaceX rocket exploded on the test stand&lt;/strong&gt;&lt;br /&gt;Sending a giant fireball into the Texas sky. (CNN)&lt;br /&gt;+ &lt;em&gt;It’s the fourth SpaceX explosion this year. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;The company has a lot of issues to resolve before it can ever reach Mars. &lt;/em&gt;(Ars Technica)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Checking a web user’s age is technologically possible&lt;/strong&gt;&lt;br /&gt;An Australian trial may usher in a ban on under-16s accessing social media. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The findings are a blow to social media firms who have been fighting to avoid this. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Chinese companies are urgently searching for new markets&lt;/strong&gt;&lt;br /&gt;And Brazil is looking like an increasingly attractive prospect. (NYT $)&lt;br /&gt;+ &lt;em&gt;Chinese carmaker BYD is sending thousands of EVs there. &lt;/em&gt;(Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 How Mark Zuckerberg came to love MAGA&lt;br /&gt;&lt;/strong&gt;His recent alignment with the manosphere hasn’t come as a shock to insiders. (FT $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 We shouldn’t be using AI for everything&lt;br /&gt;&lt;/strong&gt;Using chatbots without good reason is putting unnecessary strain on the planet. (WP $)&lt;br /&gt;+ &lt;em&gt;AI companies are remaining tight-lipped over their energy use. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;We did the math on AI’s energy footprint. Here’s the story you haven’t heard. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 This Chinese courier company is out-delivering Amazon&lt;br /&gt;&lt;/strong&gt;J&amp;amp;T Express fulfills orders from giants like Temu and Shein. (Rest of World)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 How Amazon plans to overhaul Alexa&lt;br /&gt;&lt;/strong&gt;With AI, AI, and some more AI. (Wired $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 How smart should today’s toys be?&lt;/strong&gt;&lt;br /&gt;The last AI-powered Barbie was not a resounding success. (Vox)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;10 This French app allows you to rent household appliances&lt;/strong&gt;&lt;br /&gt;No raclette machine? No problem. (The Guardian)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“So Mr “Art of the Deal” has not made a TikTok deal (again).”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Adam Cochran, founder of venture capital firm Cinneamhain Ventures, questions Donald Trump’s credentials in a post on X.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcsuuwL3ER4wS3SwclD6NN6dURgOs6oIeM-V0BFc8ps1p3WwTAsmUrhUjCxlGagRBpAPJxcMQPxHbyfLG7aNAYlYf40-553KkEtDwtI-7E6-CtnH_r_Jt6o9Ki9BpZCwg0RMc-Q?key=NBarIQRk3ysdI9m4GYhoVg" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;China wants to restore the sea with high-tech marine ranches&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A short ferry ride from the port city of Yantai, on the northeast coast of China, sits Genghai No. 1, a 12,000-metric-ton ring of oil-rig-style steel platforms, advertised as a hotel and entertainment complex.&lt;/p&gt;&lt;p&gt;Genghai is in fact an unusual tourist destination, one that breeds 200,000 “high-quality marine fish” each year. The vast majority are released into the ocean as part of a process known as marine ranching.&lt;/p&gt;&lt;p&gt;The Chinese government sees this work as an urgent and necessary response to the bleak reality that fisheries are collapsing both in China and worldwide. But just how much of a difference can it make? Read the full story.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;em&gt;—Matthew Ponsford&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ How many art terms are you familiar with? Time to brush up.&lt;br /&gt;+ They can make a museum out of pretty much anything these days.&lt;br /&gt;+ Beekeeping isn’t just beneficial for the bees—it could help your mental health, too 🐝&lt;br /&gt;+ The Sculptor galaxy is looking ridiculously beautiful right now.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;It’s pretty easy to get DeepSeek to talk dirty&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;AI companions like Replika are designed to engage in intimate exchanges, but people use general-purpose chatbots for sex talk too, despite their stricter content moderation policies. Now new research shows that not all chatbots are equally willing to talk dirty. DeepSeek is the easiest to convince. But other AI chatbots can be enticed too.&lt;/p&gt;&lt;p&gt;Huiqian Lai, a PhD student at Syracuse University, found vast differences in how mainstream models process sexual queries, from steadfast rejection to performative refusal followed by the requested sexually explicit content.&lt;/p&gt;&lt;p&gt;The findings highlight inconsistencies in LLMs’ safety boundaries that could, in certain situations, become harmful. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Rhiannon Williams&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Calorie restriction can help animals live longer. What about humans?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Living comes with a side effect: aging. Despite what you might hear on social media, there are no drugs that are known to slow or reverse human aging. But there’s some evidence to support another approach: cutting back on calories.&lt;/p&gt;  &lt;p&gt;Reducing your intake of calories and fasting can help with weight loss. But they may also offer protection against some health conditions. And some believe such diets might even help you live longer—a finding supported by new research out this week.&lt;/p&gt;&lt;p&gt;However, the full picture is not so simple. Let’s take a closer look at the benefits—and risks—of caloric restriction.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article first appeared in The Checkup, MIT Technology Review’s weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;strong&gt;How a 30-year-old techno-thriller predicted our digital isolation&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Thirty years ago, Irwin Winkler’s proto–cyber thriller, &lt;em&gt;The Net&lt;/em&gt;, was released. It was 1995, commonly regarded as the year Hollywood discovered the internet. Sandra Bullock played a social recluse and computer nerd for hire named Angela Bennett, who unwittingly uncovers a sinister computer security conspiracy. She soon finds her life turned upside down as the conspiracists begin systematically destroying her credibility and reputation.&lt;/p&gt;&lt;p&gt;While the villain of &lt;em&gt;The Net&lt;/em&gt; is ultimately a nefarious cybersecurity software company, the film’s preoccupying fear is much more fundamental: If all of our data is digitized, what happens if the people with access to that information tamper with it? Or weaponize it against us? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tom Humberstone&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the next print edition of MIT Technology Review, which explores power—who has it, and who wants it. It’s set to go live on Wednesday June 25, so &lt;/strong&gt;&lt;strong&gt;subscribe &amp;amp; save 25%&lt;/strong&gt;&lt;strong&gt; to read it and get a copy of the issue when it lands!&lt;/strong&gt;&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Donald Trump has extended TikTok’s deadline for a third time&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;He’s granted it yet another 90-day reprieve. (WSJ $)&lt;br /&gt;+ &lt;em&gt;He says he needs more time to broker a deal. &lt;/em&gt;(AP News)&lt;br /&gt;+ &lt;em&gt;But it’s not clear if Trump’s orders are even legal. &lt;/em&gt;(Bloomberg $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;2 A SpaceX rocket exploded on the test stand&lt;/strong&gt;&lt;br /&gt;Sending a giant fireball into the Texas sky. (CNN)&lt;br /&gt;+ &lt;em&gt;It’s the fourth SpaceX explosion this year. &lt;/em&gt;(WP $)&lt;br /&gt;+ &lt;em&gt;The company has a lot of issues to resolve before it can ever reach Mars. &lt;/em&gt;(Ars Technica)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Checking a web user’s age is technologically possible&lt;/strong&gt;&lt;br /&gt;An Australian trial may usher in a ban on under-16s accessing social media. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;The findings are a blow to social media firms who have been fighting to avoid this. &lt;/em&gt;(Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Chinese companies are urgently searching for new markets&lt;/strong&gt;&lt;br /&gt;And Brazil is looking like an increasingly attractive prospect. (NYT $)&lt;br /&gt;+ &lt;em&gt;Chinese carmaker BYD is sending thousands of EVs there. &lt;/em&gt;(Rest of World)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 How Mark Zuckerberg came to love MAGA&lt;br /&gt;&lt;/strong&gt;His recent alignment with the manosphere hasn’t come as a shock to insiders. (FT $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 We shouldn’t be using AI for everything&lt;br /&gt;&lt;/strong&gt;Using chatbots without good reason is putting unnecessary strain on the planet. (WP $)&lt;br /&gt;+ &lt;em&gt;AI companies are remaining tight-lipped over their energy use. &lt;/em&gt;(Wired $)&lt;br /&gt;+ &lt;em&gt;We did the math on AI’s energy footprint. Here’s the story you haven’t heard. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 This Chinese courier company is out-delivering Amazon&lt;br /&gt;&lt;/strong&gt;J&amp;amp;T Express fulfills orders from giants like Temu and Shein. (Rest of World)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 How Amazon plans to overhaul Alexa&lt;br /&gt;&lt;/strong&gt;With AI, AI, and some more AI. (Wired $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 How smart should today’s toys be?&lt;/strong&gt;&lt;br /&gt;The last AI-powered Barbie was not a resounding success. (Vox)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;10 This French app allows you to rent household appliances&lt;/strong&gt;&lt;br /&gt;No raclette machine? No problem. (The Guardian)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“So Mr “Art of the Deal” has not made a TikTok deal (again).”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Adam Cochran, founder of venture capital firm Cinneamhain Ventures, questions Donald Trump’s credentials in a post on X.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcsuuwL3ER4wS3SwclD6NN6dURgOs6oIeM-V0BFc8ps1p3WwTAsmUrhUjCxlGagRBpAPJxcMQPxHbyfLG7aNAYlYf40-553KkEtDwtI-7E6-CtnH_r_Jt6o9Ki9BpZCwg0RMc-Q?key=NBarIQRk3ysdI9m4GYhoVg" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;China wants to restore the sea with high-tech marine ranches&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;A short ferry ride from the port city of Yantai, on the northeast coast of China, sits Genghai No. 1, a 12,000-metric-ton ring of oil-rig-style steel platforms, advertised as a hotel and entertainment complex.&lt;/p&gt;&lt;p&gt;Genghai is in fact an unusual tourist destination, one that breeds 200,000 “high-quality marine fish” each year. The vast majority are released into the ocean as part of a process known as marine ranching.&lt;/p&gt;&lt;p&gt;The Chinese government sees this work as an urgent and necessary response to the bleak reality that fisheries are collapsing both in China and worldwide. But just how much of a difference can it make? Read the full story.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;em&gt;—Matthew Ponsford&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ How many art terms are you familiar with? Time to brush up.&lt;br /&gt;+ They can make a museum out of pretty much anything these days.&lt;br /&gt;+ Beekeeping isn’t just beneficial for the bees—it could help your mental health, too 🐝&lt;br /&gt;+ The Sculptor galaxy is looking ridiculously beautiful right now.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/20/1119102/the-download-talking-dirty-with-deepseek-and-the-risks-and-rewards-of-calorie-restriction/</guid><pubDate>Fri, 20 Jun 2025 12:10:00 +0000</pubDate></item></channel></rss>